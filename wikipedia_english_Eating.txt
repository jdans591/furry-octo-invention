A pièce montée (pronounced [pjɛs mɔ̃te]; from French, literally "assembled piece" or "mounted piece", plural pièces montées) is a kind of decorative confectionery centerpiece in an architectural or sculptural form used for formal banquets and made of such ingredients as confectioner’s paste, nougat, marzipan, and spun sugar. Although the ingredients are typically edible, their purpose is purely decorative, and they are often not really meant to be consumed. They are associated with classical French chefs, such as Carême. Carême had studied architecture and is credited with saying, referring to pièces montées, that architecture was the most noble of the arts and that pastry was the highest form of architecture.[1]		The term pièce montée is sometimes used to refer to the dessert also known as croquembouche, an assemblage of cream puffs (or occasionally other kinds of candy or pastry) stuck together with caramel or spun sugar into a tall, usually conical shape. Unlike the type of pièce montée described above, it is meant to be eaten. It is traditionally served at parties celebrating weddings and baptisms in France.		In The Great British Bake Off, the confectionery centrepiece was used as the final show stop in series five of the programme.				
Barbecue or barbeque (informally BBQ or barby/barbies) is both a cooking method and an apparatus. Barbecuing is done slowly over low, indirect heat and the food is flavored by the smoking process, while grilling, a related process, is generally done quickly over moderate-to-high direct heat that produces little smoke.		The word "barbecue" when used as a noun can refer to: the cooking method itself, the meat cooked this way, the cooking apparatus used (the "barbecue grill" or simply "barbecue"), or to a type of social event featuring this type of cooking. The term is also used as a verb, i.e. "barbecuing" is usually done outdoors by smoking the meat over wood or charcoal. Restaurant barbecue may be cooked in large, specially-designed brick or metal ovens. Barbecue is practiced in many areas of the world and there are numerous regional variations.						The English word "barbecue" and its cognates in other languages come from the Spanish word barbacoa. Etymologists believe this to be derived from barabicu found in the language of the Arawak people of the Caribbean and the Timucua of Florida;[1][page needed] it has entered some European languages in the form of barbacoa. The Oxford English Dictionary (OED) traces the word to Haiti and translates it as a "framework of sticks set upon posts".[2] Gonzalo Fernández De Oviedo y Valdés, a Spanish explorer, was the first to use the word "barbecoa" in print in Spain in 1526 in the Diccionario de la Lengua Española (2nd Edition) of the Real Academia Española. After Columbus landed in the Americas in 1492, the Spaniards apparently found indigenous Haitians roasting meat over a grill consisting of a wooden framework resting on sticks above a fire. The flames and smoke rose and enveloped the meat, giving it a certain flavor.[3]		Traditional barbacoa involves digging a hole in the ground and placing some meat—usually a whole lamb—above a pot so the juices can be used to make a broth. It is then covered with maguey leaves and coal, and set alight. The cooking process takes a few hours. Olaudah Equiano, an African abolitionist, described this method of roasting alligators among the Mosquito People (Miskito people) on his journeys to Cabo Gracias a Dios in his narrative The Interesting Narrative of the Life of Olaudah Equiano.[4]		Linguists have suggested the word barbacoa migrated from the Caribbean and into other languages and cultures; it moved from Caribbean dialects into Spanish, then Portuguese, French, and English. According to the OED, the first recorded use of the word in English was a verb in 1661, in Edmund Hickeringill's Jamaica Viewed: "Some are slain, And their flesh forthwith Barbacu'd and eat".[2] The word barbecue was published in English in 1672 as a verb from the writings of John Lederer, following his travels in the North American southeast in 1669-70.[5] The first known use of the word as a noun was in 1697 by the British buccaneer William Dampier. In his New Voyage Round the World, Dampier wrote, " ... and lay there all night, upon our Borbecu's, or frames of Sticks, raised about 3 foot from the Ground".[6]		Samuel Johnson's 1756 dictionary gave the following definitions:[7]		While the standard modern English spelling of the word is barbecue, variations including barbeque and truncations such as bar-b-q or BBQ may also be found.[8] The spelling barbeque is given in Merriam-Webster and the Oxford Dictionaries as a variant.[9][10] In the southeastern United States, the word barbecue is used predominantly as a noun referring to roast pork, while in the southwestern states cuts of beef are often cooked.[11][page needed]		Because the word barbecue came from native groups, Europeans gave it "savage connotations."[12] This association with barbarians and "savages" is strengthened by Edmund Hickeringill's work Jamaica Viewed: with All the Ports, Harbours, and their Several Soundings, Towns, and Settlements through its descriptions of cannibalism. However, according to Andrew Warnes, there is very little proof that Hickeringill's tale of cannibalism in the Caribbean is even remotely true.[13] Another notable false depiction of cannibalistic barbecues appears in Theodor de Bry's Great Voyages, which in Warnes's eyes, "present smoke cookery as a custom quintessential to an underlying savagery ... that everywhere contains within it a potential for cannibalistic violence."[14] Today, those in the U.S. associate barbecue with "classic Americana."[15]		British usage, barbecuing refers to a fast cooking process done directly over high heat, while grilling refers to cooking under a source of direct, moderate-to-high heat—known in the United States as broiling. In American English usage, grilling refers to a fast process over high heat while barbecuing refers to a slow process using indirect heat or hot smoke, similar to some forms of roasting. In a typical U.S. home grill, food is cooked on a grate directly over hot charcoal, while in a U.S. barbecue the coals are dispersed to the sides or at a significant distance from the grate. Its South American versions are the southern Brazilian churrasco and the Argentine asado.[16]		In the southern United States, barbecues initially involved the cooking of pork. During the 19th century, pigs were a low-maintenance food source that could be released to forage in woodlands. When food or meat supplies were low, these semi-wild pigs could then be caught and eaten.[citation needed]		According to estimates, prior to the American Civil War, Southerners ate around five pounds of pork for every pound of beef they consumed.[17] Because of the effort to capture and cook these wild hogs, pig slaughtering became a time for celebration and the neighborhood would be invited to share in the largesse. In Cajun culture, these feats are called boucheries or "pig pickin's". The traditional Southern barbecue grew out of these gatherings.[citation needed]		Each Southern locale has its own variety of barbecue, particularly sauces. North Carolina sauces vary by region; eastern North Carolina uses a vinegar-based sauce, the center of the state uses Lexington-style barbecue, with a combination of ketchup and vinegar as their base, and western North Carolina uses a heavier ketchup base. Lexington calls itself "The Barbecue Capital of the World"; it has more than one BBQ restaurant per 1,000 residents.[18] South Carolina is the only state that traditionally includes all four recognized barbecue sauces, including mustard-based, vinegar-based, and light and heavy tomato-based sauces. Memphis barbecue is best known for tomato- and vinegar-based sauces. In some Memphis establishments and in Kentucky, meat is rubbed with dry seasoning (dry rubs) and smoked over hickory wood without sauce. The finished barbecue is then served with barbecue sauce on the side.[citation needed]		The barbecue of Alabama, Georgia, and Tennessee is almost always pork served with a sweet tomato-based sauce. Several regional variations exist. Alabama is known for its distinctive white sauce—a mayonnaise- and vinegar-based sauce originating in northern Alabama, used predominantly on chicken and pork. A popular item in North Carolina and Memphis is the pulled pork sandwich served on a bun and often topped with coleslaw. Pulled pork is prepared by shredding the pork after it has been barbecued.[citation needed]		Kansas City-style barbecue is characterized by its use of different types of meat, including pulled pork, pork ribs, burnt ends, smoked sausage, beef brisket, beef ribs, smoked/grilled chicken, smoked turkey, and sometimes fish—a variety attributable to Kansas City's history as a center for meat packing. Hickory is the primary wood used for smoking in Kansas City, while the sauces are typically tomato based with sweet, spicy, and tangy flavors.[citation needed]		Pit-beef prevails in Maryland and is often enjoyed at large outdoor "bull roasts", which are commonly fundraising events for clubs and associations. Maryland-style pit-beef is not the product of barbecue cookery in the strictest sense; the meat is not smoked but grilled over a high heat. The meat is typically served rare with a strong horseradish sauce as the preferred condiment.[19]		The state of Kentucky, particularly Western Kentucky, is unusual in its barbecue cooking; the preferred meat is mutton.[20] This kind of mutton barbecue is often used in communal events in Kentucky, such as political rallies, county fairs, and church fund-raising events.[citation needed]		In the midwest, Chicago-style is popular; this involves seasoning the meat with a dry rub, searing it over a hot grill, and cooking it slowly in an oven. The meat, typically ribs, is then finished with a sweet and tangy sauce.[citation needed]		Barbecue remains one of the most traditional foods in the United States. While many festive foods, such as roasted turkey or ham, are usually served on particular days or holidays, barbecue can be served on any day. Barbecue is often served on the Fourth of July, however, it is not only confined to that day. Barbecues tend to bring people together and serve as a bonding experience at any time of the year. It brings people back to their roots, providing a cooking experience that is often an escape from civilization and closer to nature.[21] Barbecues are traditionally held outside. They could be small informal gatherings with a few of people in a backyard or a formal event that could last all day, typically held for larger amounts of people. Barbecue has been a tradition in the United States beginning with Native Americans. As author Andrew Warnes states, "its mythology of savagery and freedom, of pleasure, masculinity and strength" is part of what makes barbecues so popular to date.[22] By the 19th century barbecues became one of the main forms of United States public celebration, especially in celebration of July 4.[23]		As barbecues continued to be held through the times of U.S. expansion the traditions began to migrate with the people. Today, barbecues held in different regions of the country vary in cuisine but the cuisines all hold the same concept of cooking outside and over a fire.[24] Barbecues today have taken on new meaning yet again with the emergence of competitive barbecue. Competitive barbecue competitions are held throughout the country in which people will compete by cooking barbecue and having it judged by the events judges. The constraints of what one may barbecue and the qualities that are judged vary by competition. Usually competitions are held in big open areas where spectators will be admitted as well and barbecue is served to all.[25][26]		Barbecuing encompasses four or five distinct types of cooking techniques. The original technique is cooking using smoke at low temperatures—usually around 240–280 °F or 115–145 °C—and significantly longer cooking times (several hours), known as smoking. Another technique, known as baking, used a masonry oven or baking oven that uses convection to cook meats and starches with moderate temperatures for an average cooking time of about an hour. Braising combines direct, dry heat charbroiling on a ribbed surface with a broth-filled pot for moist heat. Using this technique, cooking occurs at various speeds, starting fast, slowing down, then speeding up again, lasting for a few hours.[27][28]		Grilling is done over direct, dry heat, usually over a hot fire over 500 °F (260 °C) for a few minutes. Grilling may be done over wood, charcoal, gas, or electricity. The time difference between barbecuing and grilling is because of the temperature difference; at low temperatures used for barbecuing, meat takes several hours to reach the desired internal temperature.[29][30]		Smoking is the process of flavoring, cooking, and/or preserving food by exposing it to smoke from burning or smoldering material, most often wood. Meat and fish are the most common smoked foods, though cheeses, vegetables, nuts, and ingredients used to make beverages such as beer or smoked beer are also smoked.[31][full citation needed]		The masonry oven is similar to a smoke pit; it allows for an open flame but cooks more quickly and uses convection to cook. Barbecue-baking can also be done in traditional stove-ovens. It can be used to cook meats, breads and other starches, casseroles, and desserts. It uses direct and indirect heat to surround the food with hot air to cook, and can be basted in much the same manner as grilled foods.[32]		It is possible to braise meats and vegetables in a pot on top of a grill. A gas or electric charbroil grill are the best choices for barbecue-braising, combining dry heat charbroil-grilling directly on a ribbed surface and braising in a broth-filled pot for moist heat. The pot is placed on top of the grill, covered, and allowed to simmer for a few hours. There are two advantages to barbecue-braising; it allows browning of the meat directly on the grill before the braising. It also allows for glazing of meat with sauce and finishing it directly over the fire after the braising. This effectively cooks the meat three times, which results in a soft, textured product that falls off the bone. The time needed for braising varies depending on whether a slow cooker or pressure cooker is used; it is generally slower than regular grilling or baking, but quicker than pit-smoking.[citation needed]		The term barbecue is also used to designate a flavor added to food items, the most prominent of which are potato chips.[33]		
Swallowing, sometimes called deglutition in scientific contexts, is the process in the human or animal body that makes something pass from the mouth, to the pharynx, and into the esophagus, while shutting the epiglottis. Swallowing is an important part of eating and drinking. If the process fails and the material (such as food, drink, or medicine) goes through the trachea, then choking or pulmonary aspiration can occur. In the human body the automatic temporary closing of the epiglottis is controlled by the swallowing reflex.		The portion of food, drink, or other material that will move through the neck in one swallow is called a bolus.						Eating and swallowing are complex neuromuscular activities consisting essentially of three phases, an oral, pharyngeal and esophageal phase. Each phase is controlled by a different neurological mechanism. The oral phase, which is entirely voluntary, is mainly controlled by the medial temporal lobes and limbic system of the cerebral cortex with contributions from the motor cortex and other cortical areas. The pharyngeal swallow is started by the oral phase and subsequently is co-ordinated by the swallowing center on the medulla oblongata and pons. The reflex is initiated by touch receptors in the pharynx as a bolus of food is pushed to the back of the mouth by the tongue, or by stimulation of the palate (palatal reflex).		Swallowing is a complex mechanism using both skeletal muscle (tongue) and smooth muscles of the pharynx and esophagus. The autonomic nervous system (ANS) coordinates this process in the pharyngeal and esophageal phases.		Prior to the following stages of the oral phase, the mandible depresses and the lips abduct to allow food or liquid to enter the oral cavity. Upon entering the oral cavity, the mandible elevates and the lips adduct to assist in oral containment of the food and liquid. The following stages describe the normal and necessary actions to form the bolus, which is defined as the state of the food in which it is ready to be swallowed.		1) Moistening		Food is moistened by saliva from the salivary glands (parasympathetic).		2) Mastication		Food is mechanically broken down by the action of the teeth controlled by the muscles of mastication (V3) acting on the temporomandibular joint. This results in a bolus which is moved from one side of the oral cavity to the other by the tongue. Buccinator (VII) helps to contain the food against the occlusal surfaces of the teeth. The bolus is ready for swallowing when it is held together by (largely mucus) saliva (VII—chorda tympani and IX—lesser petrosal), sensed by the lingual nerve of the tongue (V3). Any food that is too dry to form a bolus will not be swallowed.		3) Trough formation		A trough is then formed at the back of the tongue by the intrinsic muscles (XII). The trough obliterates against the hard palate from front to back, forcing the bolus to the back of the tongue. The intrinsic muscles of the tongue (XII) contract to make a trough (a longitudinal concave fold) at the back of the tongue. The tongue is then elevated to the roof of the mouth (by the mylohyoid (mylohyoid nerve—V3), genioglossus, styloglossus and hyoglossus (the rest XII)) such that the tongue slopes downwards posteriorly. The contraction of the genioglossus and styloglossus (both XII) also contributes to the formation of the central trough.		4) Movement of the bolus posteriorly		At the end of the oral preparatory phase, the food bolus has been formed and is ready to be propelled posteriorly into the pharynx. In order for anterior to posterior transit of the bolus to occur, orbicularis oris contracts and adducts the lips to form a tight seal of the oral cavity. Next, the superior longitudinal muscle elevates the apex of the tongue to make contact with the hard palate and the bolus is propelled to the posterior portion of the oral cavity. Once the bolus reaches the palatoglossal arch of the oropharynx, the pharyngeal phase, which is reflex and involuntary, then begins. Receptors initiating this reflex are proprioceptive (afferent limb of reflex is IX and efferent limb is the pharyngeal plexus- IX and X). They are scattered over the base of the tongue, the palatoglossal and palatopharyngeal arches, the tonsillar fossa, uvula and posterior pharyngeal wall. Stimuli from the receptors of this phase then provoke the pharyngeal phase. In fact, it has been shown that the swallowing reflex can be initiated entirely by peripheral stimulation of the internal branch of the superior laryngeal nerve. This phase is voluntary and involves important cranial nerves: V (trigeminal), VII (facial) and XII (hypoglossal).		For the pharyngeal phase to work properly all other egress from the pharynx must be occluded—this includes the nasopharynx and the larynx. When the pharyngeal phase begins, other activities such as chewing, breathing, coughing and vomiting are concomitantly inhibited.		5) Closure of the nasopharynx		The soft palate is tensed by tensor palatini (Vc), and then elevated by levator palatini (pharyngeal plexus—IX, X) to close the nasopharynx. There is also the simultaneous approximation of the walls of the pharynx to the posterior free border of the soft palate, which is carried out by the palatopharyngeus (pharyngeal plexus—IX, X) and the upper part of the superior constrictor (pharyngeal plexus—IX, X).		6) The pharynx prepares to receive the bolus		The pharynx is pulled upwards and forwards by the suprahyoid and longitudinal pharyngeal muscles – stylopharyngeus (IX), salpingopharyngeus (pharyngeal plexus—IX, X) and palatopharyngeus (pharyngeal plexus—IX, X) to receive the bolus. The palatopharyngeal folds on each side of the pharynx are brought close together through the superior constrictor muscles, so that only a small bolus can pass.		7) Opening of the auditory tube		The actions of the levator palatini (pharyngeal plexus—IX, X), tensor palatini (Vc) and salpingopharyngeus (pharyngeal plexus—IX, X) in the closure of the nasopharynx and elevation of the pharynx opens the auditory tube, which equalises the pressure between the nasopharynx and the middle ear. This does not contribute to swallowing, but happens as a consequence of it.		8) Closure of the oropharynx		The oropharynx is kept closed by palatoglossus (pharyngeal plexus—IX, X), the intrinsic muscles of tongue (XII) and styloglossus (XII).		9) Laryngeal closure		It is true vocal fold closure that is the primary laryngopharyngeal protective mechanism to prevent aspiration during swallowing. The adduction of the vocal cords is effected by the contraction of the lateral cricoarytenoids and the oblique and transverse arytenoids (all recurrent laryngeal nerve of vagus). Since the true vocal folds adduct during the swallow, a finite period of apnea (swallowing apnea) must necessarily take place with each swallow. When relating swallowing to respiration, it has been demonstrated that swallowing occurs most often during expiration, even at full expiration a fine air jet is expired probably to clear the upper larynx from food remnants or liquid. The clinical significance of this finding is that patients with a baseline of compromised lung function will, over a period of time, develop respiratory distress as a meal progresses. Subsequently, false vocal fold adduction, adduction of the aryepiglottic folds and retroversion of the epiglottis take place. The aryepiglotticus (recurrent laryngeal nerve of vagus) contracts, causing the arytenoids to appose each other (closes the laryngeal aditus by bringing the aryepiglottic folds together), and draws the epiglottis down to bring its lower half into contact with arytenoids, thus closing the aditus. Retroversion of the epiglottis, while not the primary mechanism of protecting the airway from laryngeal penetration and aspiration, acts to anatomically direct the food bolus laterally towards the piriform fossa. Additionally, the larynx is pulled up with the pharynx under the tongue by stylopharyngeus (IX), salpingopharyngeus (pharyngeal plexus—IX, X), palatopharyngeus (pharyngeal plexus—IX, X) and inferior constrictor (pharyngeal plexus—IX, X).This phase is passively controlled reflexively and involves cranial nerves V, X (vagus), XI (accessory) and XII (hypoglossal). The respiratory center of the medulla is directly inhibited by the swallowing center for the very brief time that it takes to swallow. This means that it is briefly impossible to breathe during this phase of swallowing and the moment where breathing is prevented is known as deglutition apnea.		10) Hyoid elevation		The hyoid is elevated by digastric (V & VII) and stylohyoid (VII), lifting the pharynx and larynx up even further.		11) Bolus transits pharynx		The bolus moves down towards the esophagus by pharyngeal peristalsis which takes place by sequential contraction of the superior, middle and inferior pharyngeal constrictor muscles (pharyngeal plexus—IX, X). The lower part of the inferior constrictor (cricopharyngeus) is normally closed and only opens for the advancing bolus. Gravity plays only a small part in the upright position—in fact, it is possible to swallow solid food even when standing on one’s head. The velocity through the pharynx depends on a number of factors such as viscosity and volume of the bolus. In one study, bolus velocity in healthy adults was measured to be approximately 30–40 cm/s.[1]		12) Esophageal peristalsis		Like the pharyngeal phase of swallowing, the esophageal phase of swallowing is under involuntary neuromuscular control. However, propagation of the food bolus is significantly slower than in the pharynx. The bolus enters the esophagus and is propelled downwards first by striated muscle (recurrent laryngeal, X) then by the smooth muscle (X) at a rate of 3–5 cm/s. The upper esophageal sphincter relaxes to let food pass, after which various striated constrictor muscles of the pharynx as well as peristalsis and relaxation of the lower esophageal sphincter sequentially push the bolus of food through the esophagus into the stomach.		13) Relaxation phase		Finally the larynx and pharynx move down with the hyoid mostly by elastic recoil. Then the larynx and pharynx move down from the hyoid to their relaxed positions by elastic recoil. Swallowing therefore depends on coordinated interplay between many various muscles, and although the initial part of swallowing is under voluntary control, once the deglutition process is started, it is quite hard to stop it.		Swallowing becomes a great concern for the elderly since strokes and Alzheimer's disease can interfere with the autonomic nervous system. Speech Pathologists commonly diagnose and treat this condition since the speech process uses the same neuromuscular structures as swallowing. Diagnostic procedures commonly performed by a Speech Pathologist to evaluate dysphagia include Fiberoptic Endoscopic Evaluation of Swallowing and Modified Barium Swallow Study. Occupational Therapists may also offer swallowing rehabilitation services as well as prescribing modified feeding techniques and utensils. Consultation with a dietician is essential, in order to ensure that the individual with dysphagia is able to consume sufficient calories and nutrients to maintain health. In terminally ill patients, a failure of the reflex to swallow leads to a build-up of mucus or saliva in the throat and airways, producing a noise known as a death rattle (not to be confused with agonal respiration, which is an abnormal pattern of breathing due to cerebral ischemia or hypoxia).		Abnormalities of the pharynx and/or oral cavity may lead to oropharyngeal dysphagia. Abnormalities of the esophagus may lead to esophageal dysphagia. The failure of the lower esophagous sphincter to respond properly to swallowing is called achalasia.		In many birds, the esophagus is largely a mere gravity chute, and in such events as a seagull swallowing a fish or a stork swallowing a frog, swallowing consists largely of the bird lifting its head with its beak pointing up and guiding the prey with tongue and jaws so that the prey slides inside and down.		In fish, the tongue is largely bony and much less mobile and getting the food to the back of the pharynx is helped by pumping water in its mouth and out of its gills.		In snakes, the work of swallowing is done by raking with the lower jaw until the prey is far enough back to be helped down by body undulations.		
In dining, a course is a specific set of food items that are served together during a meal, all at the same time. A course may include multiple dishes or only one, and often includes items with some variety of flavors. For instance, a hamburger served with fries would be considered a single course, and most likely the entire meal. Likewise, an extended banquet might include many courses, such as a course where a soup is served by itself, a course where cordon bleu is served at the same time as its garnish and perhaps a side dish, and later a dessert such as a pumpkin pie. Courses may vary in size as well as number depending on the culture where the meal takes place.[1]		Meals are composed of one or more courses,[2] which in turn are composed of one or more dishes.		When dishes are served mostly in a single course, this is called service à la française; when dishes are served mostly in separate courses, this is called service à la russe.		The word is derived from the French word cours (run), and came into English in the 14th century.[3] It came to be used perhaps because the food in a banquet serving had to be brought at speed from a remote kitchen – in the 1420 cookbook Du fait de cuisine the word "course" is used interchangeably with the word for serving.[4]		Food portal		
A cuisine (/kwɪˈziːn/ kwi-ZEEN; from French [kɥizin], in turn from Latin coquere "to cook") is a style of cooking characterized by distinctive ingredients, techniques and dishes, and usually associated with a specific culture or geographic region.[1][2][3] A cuisine is primarily influenced by the ingredients that are available locally or through trade. Religious food laws, such as Hindu, Islamic and Jewish dietary laws, can also exercise a strong influence on cuisine. Regional food preparation traditions, customs and ingredients often combine to create dishes unique to a particular region.[4]						Some factors that have an influence on a region's cuisine include the area's climate, the trade among different countries, religiousness or sumptuary laws and culinary culture exchange. For example, a Tropical diet may be based more on fruits and vegetables, while a polar diet might rely more on meat and fish.		The area's climate, in large measure, determines the native foods that are available. In addition, climate influences food preservation. For example, foods preserved for winter consumption by smoking, curing, and pickling have remained significant in world cuisines for their altered gustatory properties.		The trade among different countries also largely affects a region's cuisine. Dating back to the ancient spice trade, seasonings such as cinnamon, cassia, cardamom, ginger, and turmeric were important items of commerce in the earliest evolution of trade. Cinnamon and cassia found their way to the Middle East at least 4,000 years ago.[5]		Certain foods and food preparations are required or proscribed by the religiousness or sumptuary laws, such as Islamic dietary laws and Jewish dietary laws.		Culinary culture exchange is also an important factor for cuisine in many regions: Japan’s first substantial and direct exposure to the West came with the arrival of European missionaries in the second half of the 16th century. At that time, the combination of Spanish and Portuguese game frying techniques with a Chinese method for cooking vegetables in oil led to the development of tempura, the popular Japanese dish in which seafood and many different types of vegetables are coated with batter and deep fried.[6]		Cuisine dates back to the Antiquity. Rome was known for its cuisine, wealthy families would dine in the Triclinium on a variety of dishes; their diet consisted of eggs, cheese, bread, meat and honey.		Cuisines evolve continually, and new cuisines are created by innovation and cultural interaction. One recent example is fusion cuisine, which combines elements of various culinary traditions while not being categorized per any one cuisine style, and generally refers to the innovations in many contemporary restaurant cuisines since the 1970s.[7] Nouvelle cuisine (New cuisine) is an approach to cooking and food presentation in French cuisine that was popularized in the 1960s by the food critics Henri Gault, who invented the phrase, and his colleagues André Gayot and Christian Millau in a new restaurant guide, the Gault-Millau, or Le Nouveau Guide. Molecular gastronomy, is a modern style of cooking which takes advantage of many technical innovations from the scientific disciplines. The term was coined in 1988 by late Oxford physicist Nicholas Kurti and the French INRA chemist Hervé This.[8] It is also named as multi sensory cooking, modernist cuisine, culinary physics, and experimental cuisine by some chefs.[9] Besides, international trade brings new foodstuffs including ingredients to existing cuisines and leads to changes. The introduction of hot pepper to China from South America around the end of the 17th century, greatly influencing Sichuan cuisine, which combines the original taste with the taste of introduced hot pepper and creates a unique flavor of both spicy and pungent.[10]		A global cuisine is a cuisine that is practiced around the world, and can be categorized according to the common use of major foodstuffs, including grains, produce and cooking fats.		Regional cuisines may vary based upon food availability and trade, cooking traditions and practices, and cultural differences.[3] For example, in Central and South America, corn (maize), both fresh and dried, is a staple food. In northern Europe, wheat, rye, and fats of animal origin predominate, while in southern Europe olive oil is ubiquitous and rice is more prevalent. In Italy the cuisine of the north, featuring butter and rice, stands in contrast to that of the south, with its wheat pasta and olive oil. China likewise can be divided into rice regions and noodle & bread regions. Throughout the Middle East and Mediterranean there is a common thread marking the use of lamb, olive oil, lemons, peppers, and rice. The vegetarianism practiced in much of India has made pulses (crops harvested solely for the dry seed) such as chickpeas and lentils as significant as wheat or rice. From India to Indonesia the use of spices is characteristic; coconuts and seafood are used throughout the region both as foodstuffs and as seasonings.		African cuisines use a combination of locally available fruits, cereal grains and vegetables, as well as milk and meat products. In some parts of the continent, the traditional diet features a preponderance of milk, curd and whey products. In much of tropical Africa, however, cow's milk is rare and cannot be produced locally (owing to various diseases that affect livestock). The continent's diverse demographic makeup is reflected in the many different eating and drinking habits, dishes, and preparation techniques of its manifold populations.[11]		Typical Ethiopian and Eritrean cuisine: Injera (pancake-like bread) and several kinds of wat (stew)		A Ramadan dinner in Tanzania		Yassa is a popular dish throughout West Africa prepared with chicken or fish. Chicken yassa is pictured.		Spices at central market in Agadir, Morocco		Asian cuisines are many and varied. Ingredients common to many cultures in the east and Southeast regions of the continent include rice, ginger, garlic, sesame seeds, chilies, dried onions, soy, and tofu. Stir frying, steaming, and deep frying are common cooking methods. While rice is common to most Asian cuisines, different varieties are popular in the various regions; Basmati rice is popular in the South Asia, Jasmine is often found across the southeast, while long-grain rice is popular in China and short-grain in Japan and Korea.[12] Curry is also a common dish found in southern and eastern Asia, however they are not as popular in western Asian cuisines. Those curry dishes with origins in India and other South Asian countries usually have a yogurt base while Southeastern and Eastern curries generally use coconut milk as their foundation.[13]		A market stall at Thanin market in Chiang Mai, Thailand selling ready cooked food		Due to Guangdong's location on the southern coast of China, fresh live seafood is a specialty in Cantonese cuisine.		A Tajik feast		Typical Assyrian cuisine		European cuisine (alternatively, "Western cuisine") include the cuisines of Europe and other Western countries. European cuisine includes that of Europe and to some extent Russia, as well as non-indigenous cuisines of North America, Australasia, Oceania, and Latin America. The term is used by East Asians to contrast with Asian styles of cooking.[14] This is analogous to Westerners referring collectively to the cuisines of Asian countries as Asian cuisine. When used by Westerners, the term may refer more specifically to cuisine in Europe; in this context, a synonym is Continental cuisine, especially in British English.		An English Sunday roast with roast beef, roast potatoes, vegetables and Yorkshire pudding		A variety of tapas: appetizers or snacks in Spanish cuisine		German sausages and cheese		Oceanian cuisines include Australian cuisine, New Zealand cuisine, Tasmanian cuisine, and the cuisines from many other islands or island groups throughout Oceania. Australian cuisine consists of immigrant European cuisine, and Bushfood prepared and eaten by native Aboriginal Australian peoples, and various newer Asian influences. New Zealand cuisine also consists of European inspired dishes, such as Pavlova, and native Maori cuisine. Across Oceania, staples include the Kumura (Sweet potato) and Taro, which was/is a staple from Papua New Guinea to the South Pacific. On most islands in the south pacific, Fish are widely consumed because of the proximity to the ocean.		Bush Tucker (bush foods) harvested at Alice Springs Desert Park in Australia		A Hāngi being prepared, a New Zealand Māori method of cooking food for special occasions using hot rocks buried in a pit oven.		Samoan umu, an oven of hot rocks above ground		The cuisines of the Americas are found across North and South America, and are based on the cuisines of the countries from which the immigrant people came, primarily Europe. However, the traditional European cuisine has been adapted by the addition of many local and native ingredients, and many techniques have been added to traditional foods as well. Native American cuisine is prepared by indigenous populations across the continent, and its influences can be seen on multi-ethnic Latin American cuisine. Many staple foods eaten across the continent, such as Corn, Beans, and Potatoes have native origins. The regional cuisines are North American cuisine, Mexican cuisine, Central American cuisine, South American cuisine, and Caribbean cuisine.		A New England clam bake		Québécois poutine is made with french fries, curds and gravy.		A sirloin steak dinner		
A healthy diet is one that helps to maintain or improve overall health.		A healthy diet provides the body with essential nutrition: fluid, adequate essential amino acids from protein,[1] essential fatty acids, vitamins, minerals, and adequate calories. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods. A healthy diet supports energy needs and provides for human nutrition without exposure to toxicity or excessive weight gain from consuming excessive amounts. Where lack of calories is not an issue, a properly balanced diet (in addition to exercise) is also thought to be important for lowering health risks, such as obesity, heart disease, type 2 diabetes, hypertension and cancer.[2]		Various nutrition guides are published by medical and governmental institutions to educate the public on what they should be eating to promote health. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.		The idea of dietary therapy (using dietary choices to maintain health and improve poor health) is quite old and thus has both modern scientific forms (medical nutrition therapy) and prescientific forms (such as dietary therapy in traditional Chinese medicine).						Healthy eating is simple, according to Marion Nestle, who expresses the mainstream view among scientists who study nutrition:[3]:10		The basic principles of good diets are so simple that I can summarize them in just ten words: eat less, move more, eat lots of fruits and vegetables. For additional clarification, a five-word modifier helps: go easy on junk foods. Follow these precepts and you will go a long way toward preventing the major diseases of our overfed society—coronary heart disease, certain cancers, diabetes, stroke, osteoporosis, and a host of others.... These precepts constitute the bottom line of what seem to be the far more complicated dietary recommendations of many health organizations and national and international governments—the forty-one “key recommendations” of the 2005 Dietary Guidelines, for example. ... Although you may feel as though advice about nutrition is constantly changing, the basic ideas behind my four precepts have not changed in half a century. And they leave plenty of room for enjoying the pleasures of food.[4]:22		David L. Katz, who reviewed the most prevalent popular diets in 2014, noted:		The weight of evidence strongly supports a theme of healthful eating while allowing for variations on that theme. A diet of minimally processed foods close to nature, predominantly plants, is decisively associated with health promotion and disease prevention and is consistent with the salient components of seemingly distinct dietary approaches. Efforts to improve public health through diet are forestalled not for want of knowledge about the optimal feeding of Homo sapiens but for distractions associated with exaggerated claims, and our failure to convert what we reliably know into what we routinely do. Knowledge in this case is not, as of yet, power; would that it were so.[5]		The World Health Organization (WHO) makes the following 5 recommendations with respect to both populations and individuals:[6]		Other recommendations include:		WHO recommends an intake of less than 5 grams of salt per day for the prevention of cardiovascular disease.[8]		The Dietary Guidelines for Americans by the United States Department of Agriculture (USDA) recommends three healthy patterns of diet, summarized in table below, for a 2000 kcal diet.[9]		It emphasizes both health and environmental sustainability and a flexible approach: the committee that drafted it wrote: "The major findings regarding sustainable diets were that a diet higher in plant-based foods, such as vegetables, fruits, whole grains, legumes, nuts, and seeds, and lower in calories and animal-based foods is more health promoting and is associated with less environmental impact than is the current U.S. diet. This pattern of eating can be achieved through a variety of dietary patterns, including the “Healthy U.S.-style Pattern,” the “Healthy Mediterranean-style Pattern,” and the “Healthy Vegetarian Pattern”.[10] Food group amounts per day, unless noted per week.		The American Heart Association, World Cancer Research Fund, and American Institute for Cancer Research recommend a diet that consists mostly of unprocessed plant foods, with emphasis a wide range of whole grains, legumes, and non-starchy vegetables and fruits. This healthy diet is full of a wide range of various non-starchy vegetables and fruits, that provide different colors including red, green, yellow, white, purple, and orange. They note that tomato cooked with oil, allium vegetables like garlic, and cruciferous vegetables like cauliflower, provide some protection against cancer. This healthy diet is low in energy density, which may protect against weight gain and associated diseases. Finally, limiting consumption of sugary drinks, limiting energy rich foods, including “fast foods” and red meat, and avoiding processed meats improves health and longevity. Overall, researchers and medical policy conclude that this healthy diet can reduce the risk of chronic disease and cancer.[11][12]		In children, consuming less than 25 grams of added sugar (100 calories) is recommended per day.[13] Other recommendations include no extra sugars in those under 2 years old and less than one soft drink per week.[13] As of 2017, decreasing total fat is no longer recommended, but instead, the recommendation to lower risk of cardiovascular disease is to increase consumption of monounsaturated fats and polyunsaturated fats, while decreasing consumption of saturated fats.[14]		The Nutrition Source of Harvard School of Public Health makes the following 10 recommendations for a healthy diet:[15]		Other than nutrition, the guide recommends frequent physical exercise and maintaining a healthy body weight.[15]		In addition to dietary recommendations for the general population, there are many specific diets that have primarily been developed to promote better health in specific population groups, such as people with high blood pressure (as in low sodium diets or the more specific DASH diet), or people who are overweight or obese (in weight control diets). However, some of them may have more or less evidence for beneficial effects in normal people as well.		A low sodium diet is beneficial for people with high blood pressure. A Cochrane review published in 2008 concluded that a long term (more than 4 weeks) low sodium diet has a useful effect to reduce blood pressure, both in people with hypertension and in people with normal blood pressure.[21]		The DASH diet (Dietary Approaches to Stop Hypertension) is a diet promoted by the National Heart, Lung, and Blood Institute (part of the NIH, a United States government organization) to control hypertension. A major feature of the plan is limiting intake of sodium,[22] and it also generally encourages the consumption of nuts, whole grains, fish, poultry, fruits and vegetables while lowering the consumption of red meats, sweets, and sugar. It is also "rich in potassium, magnesium, and calcium, as well as protein".		The Mediterranean diet has also been shown to improve cardiovascular outcomes.[23]		Weight control diets aim to maintain a controlled weight. In most cases dieting is used in combination with physical exercise to lose weight in those who are overweight or obese.		Diets to promote weight loss are divided into four categories: low-fat, low-carbohydrate, low-calorie, and very low calorie.[24] A meta-analysis of six randomized controlled trials found no difference between the main diet types (low calorie, low carbohydrate, and low fat), with a 2–4 kilogram weight loss in all studies.[24] At two years, all calorie-reduced diet types cause equal weight loss irrespective of the macronutrients emphasized.[25]		There may be a relationship between lifestyle including food consumption and potentially lowering the risk of cancer or other chronic diseases. A diet high in fruits and vegetables appears to decrease the risk of cardiovascular disease and death but not cancer.[26]		A healthy diet may consist mostly of whole plant foods, with limited consumption of energy dense foods, red meat, alcoholic drinks and salt while reducing consumption of sugary drinks, and processed meat.[27] A healthy diet may contain non-starchy vegetables and fruits, including those with red, green, yellow, white, purple or orange pigments. Tomato cooked with oil, allium vegetables like garlic, and cruciferous vegetables like cauliflower "probably" contain compounds which are under research for their possible anti-cancer activity.[11][12]		A healthy diet is low in energy density, lowering caloric content, thereby possibly inhibiting weight gain and lowering risk against chronic diseases.[11][12][28] Chronic Western diseases are associated with pathologically increased IGF-1 levels. Findings in molecular biology and epidemiologic data suggest that milk consumption is a promoter of chronic diseases of Western nations, including atherosclerosis, carcinogenesis and neurodegenerative diseases.[29]		The Western pattern diet which is typically eaten by Americans and increasingly adapted by people in the developing world as they leave poverty is unhealthy: it is "rich in red meat, dairy products, processed and artificially sweetened foods, and salt, with minimal intake of fruits, vegetables, fish, legumes, and whole grains."[30]		An unhealthy diet is a major risk factor for a number of chronic diseases including: high blood pressure, diabetes, abnormal blood lipids, overweight/obesity, cardiovascular diseases, and cancer.[31]		The WHO estimates that 2.7 million deaths are attributable to a diet low in fruits and vegetables every year.[31] Globally it is estimated to cause about 19% of gastrointestinal cancer, 31% of ischaemic heart disease, and 11% of strokes,[2] thus making it one of the leading preventable causes of death worldwide.[32]		Popular diets, often referred to as fad diets, make promises of weight loss or other health advantages such as longer life without backing by solid science, and in many cases are characterized by highly restrictive or unusual food choices.[4]:296[33] Celebrity endorsements (including celebrity doctors) are frequently associated with popular diets, and the individuals who develop and promote these programs often profit handsomely.[3]:11–12[34]		Fears of high cholesterol were frequently voiced up until the mid-1990s. However, more recent research has shown that the distinction between high- and low-density lipoprotein ('good' and 'bad' cholesterol, respectively) must be addressed when speaking of the potential ill effects of cholesterol. Different types of dietary fat have different effects on blood levels of cholesterol. For example, polyunsaturated fats tend to decrease both types of cholesterol; monounsaturated fats tend to lower LDL and raise HDL; saturated fats tend to either raise HDL, or raise both HDL and LDL;[35][36] and trans fat tend to raise LDL and lower HDL.		While dietary cholesterol is only found in animal products such as meat, eggs, and dairy, studies have not found a link between eating cholesterol and blood levels of cholesterol.[37]		Vending machines in particular have come under fire as being avenues of entry into schools for junk food promoters. However, there is little in the way of regulation and it is difficult for most people to properly analyze the real merits of a company referring to itself as "healthy." Recently, the Committee of Advertising Practice in the United Kingdom launched a proposal to limit media advertising for food and soft drink products high in fat, salt or sugar.[38] The British Heart Foundation released its own government-funded advertisements, labeled "Food4Thought", which were targeted at children and adults to discourage unhealthy habits of consuming junk food.[39]		From a psychological and cultural perspective, a healthier diet may be difficult to achieve for people with poor eating habits.[40] This may be due to tastes acquired in childhood and preferences for sugary, salty and/or fatty foods.[41]		
A fork, in cutlery or kitchenware, is a tool consisting of a handle with several narrow tines on one end. The usually metal utensil is used to lift food to the mouth or to hold ingredients in place while they are being cut by a knife. Food can be lifted either by spearing it on the tines or by holding it on top of the tines, which are often curved slightly.						The word fork comes from the Latin furca, meaning "pitchfork". Some of the earliest known uses of forks with food occurred in Ancient Egypt, where large forks were used as cooking utensils.[1]		Bone forks had been found in archaeological sites of the Bronze Age Qijia culture (2400–1900 BC), the Shang dynasty (c. 1600–c. 1050 BC), as well as later Chinese dynasties.[2] A stone carving from an Eastern Han tomb (in Ta-kua-liang, Suide County, Shaanxi) depicts three hanging two-pronged forks in a dining scene.[2] Conversely, similar forks has also been depicted on top of a stove in a scene at another Eastern Han tomb (in Suide County, Shaanxi).[2]		In the Roman Empire, bronze and silver forks were used, and indeed many examples are displayed in museums around Europe.[3][4] The use varied according to local customs, social class and the nature of food, but forks of the earlier periods were mostly used as cooking and serving utensils. The personal table fork was most likely invented in the Eastern Roman (Byzantine) Empire, where they were in common use by the 4th century (its origin may even go back to Ancient Greece, before the Roman period).[5][6] Records show that by the 9th century a similar utensil known as a barjyn was in limited use in Persia within some elite circles.[7] By the 10th century, the table fork was in common use throughout the Middle East.[1]		The first recorded introduction of the fork to Europe, as recorded by the theologian and cardinal Peter Damian,[8] was by Theophano Sklereina the Byzantine wife of Holy Roman Emperor Otto II, who nonchalantly wielded one at an Imperial banquet in 972, astonishing her Western hosts.[9] By the 11th century, the table fork had become increasingly prevalent in the Italian peninsula. It gained a following in Italy before any other European region because of historical ties with Byzantium, and continued to gain popularity due to the increasing presence of pasta in the Italian diet.[10] At first, pasta was consumed using a long wooden spike, but this eventually evolved into three spikes, a design better suited to gathering the noodles.[11] In Italy, it became commonplace by the 14th century and was almost universally used by the merchant and upper classes by 1600. It was proper for a guest to arrive with his own fork and spoon enclosed in a box called a cadena; this usage was introduced to the French court with Catherine de' Medici's entourage. In Portugal, forks were first used at the time of Infanta Beatrice, Duchess of Viseu, King Manuel I of Portugal's mother[12] around 1450. However, forks were not commonly used in Southern Europe until the 16th century when they became part of Italian etiquette.[13] The utensil had also gained some currency in Spain by this time,[14] Its use gradually spread to France. Nevertheless, most of Europe did not adopt use of the fork until the 18th century.[5]		Polish deputy minister has said in 2016, that Poland "taught the French how to use a fork".[15] However the truth is less easy, Henri III was the one to actually introduce proper usage of the fork after a travel to Poland, however on the way home he apparently went to Venice to visit his mother homeland and that's where he supposedly discovered the little pitchfork.		Long after the personal table fork had become commonplace in France, at the supper celebrating the marriage of the duc de Chartres to Louis XIV's natural daughter in 1692, the seating was described in the court memoirs of Saint-Simon: "King James having his Queen on his right hand and the King on his left, and each with their cadenas." In Perrault's contemporaneous fairy tale of La Belle au bois dormant (1697), each of the fairies invited for the christening is presented with a splendid "fork holder".		The fork's adoption in northern Europe was slower. Its use was first described in English by Thomas Coryat in a volume of writings on his Italian travels (1611), but for many years it was viewed as an unmanly Italian affectation.[16] Some writers of the Roman Catholic Church expressly disapproved of its use, St. Peter Damian seeing it as "excessive delicacy":[11] It was not until the 18th century that the fork became commonly used in Great Britain,[17] although some sources say that forks were common in France, England and Sweden already by the early 17th century.[18][19][dubious – discuss]		The fork did not become popular in North America until near the time of the American Revolution.[1] The curved fork used in most parts of the world today was developed in Germany in the mid 18th century while the standard four-tine design became current in the early 19th century. The fork was important in Germany because they believed that eating with the fingers was rude and disrespectful. The fork led to family dinners and sit-down meals, which are important features of German culture.[20]				
Meze or mezze (/ˈmɛzeɪ/, also spelled mazzeh or mazze; Persian: مزه‎, translit. maze‎; Turkish: meze; Greek: μεζές, translit. mezés; Serbian: мезе; Bulgarian: мезе; Arabic: مقبلات‎, translit. maqabālāt‎; Albanian: Meze) is a selection of small dishes served to accompany alcoholic drinks in the Near East, the Balkans, and parts of Central Asia. In Levantine, Caucasian, and Balkan cuisines, meze is often served at the beginning of multi-course meals.[1]						The word is found in all the cuisines of the former Ottoman Empire and comes from Persian مزه (mazze "taste, snack" < mazīdan "to taste").[2]		In Turkey, meze often consist of beyaz peynir (literally "white cheese"), kavun (sliced ripe melon), acılı ezme (hot pepper paste often with walnuts), haydari (thick strained yogurt with herbs), patlıcan salatası (cold eggplant salad), beyin salatası (brain salad), kalamar tava (fried calamari or squid), midye dolma and midye tava (stuffed or fried mussels), enginar (artichokes), cacık (yogurt with cucumber and garlic), pilaki (foods cooked in a special sauce), dolma or sarma (rice-stuffed vine leaves or other stuffed vegetables, such as bell peppers), arnavut ciğeri (a liver dish, served cold), octopus salad, and çiğ köfte (raw meatballs with bulgur).		In Greece, Cyprus and the Balkans, mezé, mezés, or mezédhes (plural) are small dishes, hot or cold, spicy or savory. Seafood dishes such as grilled octopus may be included, along with salads, sliced hard-boiled eggs, garlic bread, kalamata olives, fava beans, fried vegetables, melitzanosalata (eggplant salad), taramosalata, fried or grilled cheeses called saganaki, and sheep, goat, or cow cheeses.		Popular meze dishes include:		In Lebanon and Cyprus, meze is often a meal in its own right. There are vegetarian, meat or fish mezes. Groups of dishes arrive at the table about 4 or 5 at a time (usually between five and ten groups). There is a set pattern to the dishes: typically olives, tahini, salad and yogurt will be followed by dishes with vegetables and eggs, then small meat or fish dishes alongside special accompaniments, and finally more substantial dishes such as whole fish or meat stews and grills. Establishments will offer their own specialities, but the pattern remains the same. Naturally the dishes served will reflect the seasons. For example, in late autumn, snails will be prominent. As so much food is offered, it is not expected that every dish be finished, but rather shared at will and served at ease. Eating a Cypriot meze is a social event.		In the Balkans, meze is very similar to Mediterranean antipasti in the sense that cured cold-cuts, cheese and salads are dominant ingredients and that it typically doesn't include cooked meals. In Serbia, Croatia, Bosnia and Montenegro it includes hard or creamy cheeses, kajmak (clotted cream) or smetana cream, salami, ham and other forms of "suho/suvo meso" (cured pork or beef), kulen (paprika flavoured, cured sausage), cured bacon, ajvar, and various pastry; In Bosnia and Herzegovina, depending on religious food restrictions one obeys, meze excludes pork products and replaces them with sudžuk (dry, spicy sausage) and pastrami-like cured beef. In southern Croatia, Herzegovina and Montenegro more mediterranean forms of cured meat like pršut and panceta and regional products like olives are common. Albanian-style meze platters typically include prosciutto ham, salami and brined cheese, accompanied with roasted bell peppers (capsicum) or green olives marinated in olive oil with garlic. In Bulgaria, popular mezes are lukanka (a spicy sausage), soujouk (a dry and spicy sausage), sirene (a white brine cheese), and Shopska salad made with tomatoes, cucumbers, onion, roasted peppers and sirene. In Romania, mezelic means quick appetizer and includes Zacuscă, cheeses and salamis, often accompanied by Țuică.		Meze is generally accompanied by the distilled drinks rakı, arak, ouzo, Aragh Sagi, rakia, mastika, or tsipouro. It may also be consumed with beer, wine and other alcoholic drinks. Cyprus Brandy (served neat) is a favourite drink to accompany meze in Cyprus, although lager or wine are popular with some.		The same dishes, served without alcoholic drinks, are termed "muqabbilat" (starters) in Arabic.		In Bulgaria, meze is served primarily at consumption of wine but also as an appetizer for rakia and mastika. In Greece, meze is served in restaurants called mezedopoleíon and tsipourádiko or ouzerí, a type of café that serves ouzo or tsipouro. A tavérna (tavern) or estiatório (restaurant) offer a mezé as an orektikó (appetiser). Many restaurants offer their house poikilía ("variety") — a platter with a smorgasbord of mezédhes that can be served immediately to customers looking for a quick or light meal. Hosts commonly serve mezédhes to their guests at informal or impromptu get-togethers, as they are easy to prepare on short notice. Krasomezédhes (literally "wine-meze") is a meze that goes well with wine; ouzomezédhes are meze that goes with ouzo.		
Dieting is the practice of eating food in a regulated and supervised fashion to decrease, maintain, or increase body weight. In other words, it is conscious control or restriction of the diet. A restricted diet is often used by those who are overweight or obese, sometimes in combination with physical exercise, to reduce body weight. Some people follow a diet to gain weight (usually in the form of muscle). Diets can also be used to maintain a stable body weight and improve health. In particular, diets can be designed to prevent or treat diabetes.		Diets to promote weight loss can be categorized as: low-fat, low-carbohydrate, low-calorie, very low calorie and more recently flexible dieting.[1] A meta-analysis of six randomized controlled trials found no difference between low-calorie, low-carbohydrate, and low-fat diets, with a 2–4 kilogram weight loss over 12–18 months in all studies.[1] At two years, all calorie-reduced diet types cause equal weight loss irrespective of the macronutrients emphasized.[2] In general, the most effective diet is any which reduces calorie consumption.[3]		A study published in American Psychologist found that short-term dieting involving "severe restriction of calorie intake" does not lead to "sustained improvements in weight and health for the majority of individuals".[4] Other studies have found that the average individual maintains some weight loss after dieting.[5] Weight loss by dieting, while of benefit to those classified as unhealthy, may slightly increase the mortality rate for individuals who are otherwise healthy.[6][7][8]		The first popular diet was "Banting", named after William Banting. In his 1863 pamphlet, Letter on Corpulence, Addressed to the Public, he outlined the details of a particular low-carbohydrate, low-calorie diet that had led to his own dramatic weight loss.[9]						One of the first dietitians was the English doctor George Cheyne. He himself was tremendously overweight and would constantly eat large quantities of rich food and drink. He began a meatless diet, taking only milk and vegetables, and soon regained his health. He began publicly recommending his diet for everyone suffering from obesity. In 1724, he wrote An Essay of Health and Long Life, in which he advises exercise and fresh air and avoiding luxury foods.[10]		The Scottish military surgeon, John Rollo, published Notes of a Diabetic Case in 1797. It described the benefits of a meat diet for those suffering from diabetes, basing this recommendation on Matthew Dobson's discovery of glycosuria in diabetes mellitus.[11] By means of Dobson's testing procedure (for glucose in the urine) Rollo worked out a diet that had success for what is now called type 2 diabetes.[12]		The first popular diet was "Banting", named after the English undertaker William Banting. In 1863, he wrote a booklet called Letter on Corpulence, Addressed to the Public, which contained the particular plan for the diet he had successfully followed. His own diet was four meals per day, consisting of meat, greens, fruits, and dry wine. The emphasis was on avoiding sugar, sweet foods, starch, beer, milk and butter. Banting’s pamphlet was popular for years to come, and would be used as a model for modern diets.[13] The pamphlet's popularity was such that the question "Do you bant?" referred to his method, and eventually to dieting in general.[14] His booklet remains in print as of 2007.[9][15][16]		The first weight-loss book to promote calorie counting, and the first weight-loss book to become a bestseller, was the 1918 Diet and Health: With Key to the Calories by American physician and columnist Lulu Hunt Peters.[17]		The Atkins Diet was suggested by the American nutritionist Robert Atkins in 1958, in a research paper titled "Weight Reduction". Atkins used the study to resolve his own overweight condition and went on to popularize the method in a series of books, starting with Dr. Atkins' Diet Revolution in 1972. In his second book, Dr. Atkins' New Diet Revolution (1992), he modified parts of the diet but did not alter the original concepts.		Low-fat diets involve the reduction of the percentage of fat in one's diet. Calorie consumption is reduced because less fat is consumed. Diets of this type include NCEP Step I and II. A meta-analysis of 16 trials of 2–12 months' duration found that low-fat diets (without intentional restriction of caloric intake) resulted in average weight loss of 3.2 kg (7.1 lb) over habitual eating.[1]		Low-carbohydrate diets such as Atkins and Protein Power are relatively high in protein and fats. Low-carbohydrate diets are sometimes ketogenic (i.e., they restrict carbohydrate intake sufficiently to cause ketosis).		Low-calorie diets usually produce an energy deficit of 500–1,000 calories per day, which can result in a 0.5 to 1 kilogram (1.1 to 2.2 pounds) weight loss per week. Some of the most commonly used low-calorie diets include DASH diet and Weight Watchers. The National Institutes of Health reviewed 34 randomized controlled trials to determine the effectiveness of low-calorie diets. They found that these diets lowered total body mass by 8% in the short term, over 3–12 months.[1] Women doing low-calorie diets should have at least 1,200 calories per day. Men should have at least 1,800 calories per day.		Very low calorie diets provide 200–800 calories per day, maintaining protein intake but limiting calories from both fat and carbohydrates. They subject the body to starvation and produce an average loss of 1.5–2.5 kg (3.3–5.5 lb) per week. "2-4-6-8", a popular diet of this variety, follows a four-day cycle in which only 200 calories are consumed the first day, 400 the second day, 600 the third day, 800 the fourth day, and then totally fasting, after which the cycle repeats. These diets are not recommended for general use as they are associated with adverse side effects such as loss of lean muscle mass, increased risks of gout, and electrolyte imbalances. People attempting these diets must be monitored closely by a physician to prevent complications.[1]		Detox diets claim to eliminate "toxins" from the human body rather than claiming to cause weight loss. Many of these use herbs or celery and other juicy low-calorie vegetables.		Religious prescription may be a factor in motivating people to adopt a specific restrictive diet.[18] For example, the Biblical Book of Daniel (1:2-20, and 10:2-3) refers to a 10- or 21-day avoidance of foods (Daniel Fast) declared unclean by God in the laws of Moses.[18][19] In modern versions of the Daniel Fast, food choices may be limited to whole grains, fruits, vegetables, pulses, nuts, seeds and oil. The Daniel Fast resembles the vegan diet in that it excludes foods of animal origin.[19] The passages strongly suggest that the Daniel Fast will promote good health and mental performance.[18]		Fasting is practiced in various religions. Examples include Lent in Christianity; Yom Kippur, Tisha B'av, Fast of Esther, Tzom Gedalia, the Seventeenth of Tamuz, and the Tenth of Tevet in Judaism.[20] Muslims refrain from eating during the hours of daytime for one entire month, Ramadan, every year.		Details of fasting practices differ. Eastern Orthodox Christians fast during specified fasting seasons of the year, which include not only the better-known Great Lent, but also fasts on every Wednesday and Friday (except on special holidays), together with extended fasting periods before Christmas (the Nativity Fast), after Easter (the Apostles Fast) and in early August (the Dormition Fast). Members of The Church of Jesus Christ of Latter-day Saints (Mormons) generally fast for 24 hours on the first Sunday of each month. Like Muslims, they refrain from all drinking and eating unless they are children or are physically unable to fast. Fasting is also a feature of ascetic traditions in religions such as Hinduism and Buddhism. Mahayana traditions that follow the Brahma's Net Sutra may recommend that the laity fast "during the six days of fasting each month and the three months of fasting each year" [Brahma's Net Sutra, minor precept 30]. Members of the Baha'i Faith observe a Nineteen Day Fast from sunrise to sunset during March each year.		Weight loss diets that manipulate the proportion of macronutrients (low-fat, low-carbohydrate, etc.) have been shown to be more effective than diets that maintain a typical mix of foods with smaller portions and perhaps some substitutions (e.g. low-fat milk, or less salad dressing).[21] Extreme diets may, in some cases, lead to malnutrition.		Nutritionists also agree on the importance of avoiding fats, especially saturated fats, to reduce weight and to be healthier. They also agree on the importance of reducing salt intake because foods including snacks, biscuits, and bread already contain ocean-salt, contributing to an excess of salt daily intake.[22]		MyPyramid Food Guidance System is the result of extensive research performed by the United States Department of Agriculture to revise the original Food Guide Pyramid. It offers a wide array of personalized options to help individuals make healthy food choices. It also provides advice on physical activity.[23]		One of the most important things to take into consideration when either trying to lose or put on weight is output versus input. It is important to know the amount of energy your body is using every day, so that your intake fits the needs of one's personal weight goal. Someone wanting to lose weight would want a smaller energy intake than what they put out. There is increasing research-based evidence that low-fat vegetarian diets consistently lead to healthy weight loss and management, a decrease in diabetic symptoms[24] as well as improved cardiac health.[25]		When the body is expending more energy than it is consuming (e.g. when exercising), the body's cells rely on internally stored energy sources, such as complex carbohydrates and fats, for energy. The first source to which the body turns is glycogen (by glycogenolysis). Glycogen is a complex carbohydrate, 65% of which is stored in skeletal muscles and the remainder in the liver (totaling about 2,000 kcal in the whole body). It is created from the excess of ingested macronutrients, mainly carbohydrates. When glycogen is nearly depleted, the body begins lipolysis, the mobilization and catabolism of fat stores for energy. In this process, fats, obtained from adipose tissue, or fat cells, are broken down into glycerol and fatty acids, which can be used to generate energy.[26] The primary by-products of metabolism are carbon dioxide and water; carbon dioxide is expelled through the respiratory system.		Some weight loss groups aim to make money, others work as charities. The former include Weight Watchers and Peertrainer. The latter include Overeaters Anonymous and groups run by local organizations.		These organizations' customs and practices differ widely. Some groups are modelled on twelve-step programs, while others are quite informal. Some groups advocate certain prepared foods or special menus, while others train dieters to make healthy choices from restaurant menus and while grocery-shopping and cooking.		A 2008 study published in the American Journal of Preventive Medicine showed that dieters who kept a daily food diary (or diet journal), lost twice as much weight as those who did not keep a food log, suggesting that if you record your eating, you wouldn't eat as many calories.[27]		A 2009 review found that existing limited evidence suggested that encouraging water consumption and substituting energy-free beverages for energy-containing beverages (i.e., reducing caloric intake) may facilitate weight management. A 2009 article found that drinking 500 ml of water prior to meals for a 12-week period resulted in increased long-term weight reduction. (References given in main article.)		Lengthy fasting can be dangerous due to the risk of malnutrition and should be carried out only under medical supervision. During prolonged fasting or very low calorie diets the reduction of blood glucose, the preferred energy source of the brain, causes the body to deplete its glycogen stores. Once glycogen is depleted the body begins to fuel the brain using ketones, while also metabolizing body protein (including but not limited to skeletal muscle) to be used to synthesize sugars for use as energy by the rest of the body. Most experts believe that a prolonged fast can lead to muscle wasting, although some dispute this. The use of short-term fasting, or various forms of intermittent fasting have been used as a form of dieting to circumvent this issue.		While there are studies that show the health and medical benefits of weight loss, a study in 2005 of around 3000 Finns over an 18-year period showed that weight loss from dieting can result in increased mortality, while those who maintained their weight fared the best.[6][8][28] Similar conclusion is drawn by other studies,[6][29] and although other studies suggest that intentional weight loss has a small benefit for individuals classified as unhealthy, it is associated with slightly increased mortality for healthy individuals and the slightly overweight but not obese.[7] This may reflect the loss of subcutaneous fat and beneficial mass from organs and muscle in addition to visceral fat when there is a sudden and dramatic weight loss.[8]		Many studies have focused on diets that reduce calories via a low-carbohydrate (Atkins diet, Scarsdale diet, Zone diet) diet versus a low-fat diet (LEARN diet, Ornish diet). The Nurses' Health Study, an observational cohort study, found that low carbohydrate diets based on vegetable sources of fat and protein are associated with less coronary heart disease.[30] The same study also found no correlation (with multivariate adjustment) between animal fat intake and coronary heart disease (table 4). A long term study that monitored 43,396 Swedish women however suggests that a low carbohydrate-high protein diet, used on a regular basis and without consideration of the nature of carbohydrates or the source of proteins, is associated with increased risk of cardiovascular disease.[31]		A meta-analysis of randomized controlled trials by the international Cochrane Collaboration in 2002 concluded[32] that fat-restricted diets are no better than calorie-restricted diets in achieving long term weight loss in overweight or obese people. A more recent meta-analysis that included randomized controlled trials published after the Cochrane review[33][34][35] found that low-carbohydrate, non-energy-restricted diets appear to be at least as effective as low-fat, energy-restricted diets in inducing weight loss for up to 1 year. These results can be understood because weight loss is mainly governed by daily caloric deficit and not by the particular foods eaten.[36] However, when low-carbohydrate diets to induce weight loss are considered, potential favorable changes in triglyceride and high-density lipoprotein cholesterol values should be weighed against potential unfavorable changes in low-density lipoprotein cholesterol values."[37]		The Women's Health Initiative Randomized Controlled Dietary Modification Trial[38] found that a diet of total fat to 20% of energy and increasing consumption of vegetables and fruit to at least 5 servings daily and grains to at least 6 servings daily resulted in:		Additional randomized controlled trials found that:		The American Diabetes Association recommended a low carbohydrate diet to reduce weight for those with or at risk of Type 2 diabetes in its January 2008 Clinical Practice Recommendations.[46]		"The glycemic index (GI) factor is a ranking of foods based on their overall effect on blood sugar levels. The diet based around this research is called the Low GI diet. Low glycemic index foods, such as lentils, provide a slower, more consistent source of glucose to the bloodstream, thereby stimulating less insulin release than high glycemic index foods, such as white bread."[47][48]		The glycemic load is "the mathematical product of the glycemic index and the carbohydrate amount".[49]		In a randomized controlled trial that compared four diets that varied in carbohydrate amount and glycemic index found complicated results:[50]		Diets 2 and 3 lost the most weight and fat mass; however, low density lipoprotein fell in Diet 2 and rose in Diet 3. Thus the authors concluded that the high-carbohydrate, low-glycemic index diet was the most favorable.		A meta-analysis by the Cochrane Collaboration concluded that low glycemic index or low glycemic load diets led to more weight loss and better lipid profiles. However, the Cochrane Collaboration grouped low glycemic index and low glycemic load diets together and did not try to separate the effects of the load versus the index.[51]		
Housekeeping refers to the management of duties and chores involved in the running of a household, such as cleaning, cooking, home maintenance, shopping, laundry and bill pay. These tasks may be performed by any of the household members, or by other persons hired to perform these tasks. The term is also used to refer to the money allocated for such use.[1] By extension, an office or organization, as well as the maintenance of computer storage systems.[2]		A housekeeper is a person employed to manage a household,[3] and the domestic staff. According to the Victorian Era Mrs Beeton's Book of Household Management, the housekeeper is second in command in the house and "except in large establishments, where there is a house steward, the housekeeper must consider his/herself as the immediate representative of her mistress".[4]						It includes activities such as housecleaning, that is, disposing of rubbish, cleaning dirty surfaces, dusting and vacuuming. It may also involve some outdoor chores, such as removing leaves from rain gutters, washing windows and sweeping doormats. The term housecleaning is often used also figuratively in politics and business, for the removal of unwanted personnel, methods or policies in an effort at reform or improvement.[5]		Housecleaning is done to make the home look and smell better and be safer and easier to live in. Without housecleaning lime scale can build up on taps, mold grows in wet areas, smudges on glass surfaces, dust forms on surfaces, bacterial action make the garbage disposal and toilet smell and cobwebs accumulate. Tools used in housecleaning include vacuums, brooms, mops and sponges, together with cleaning products such as detergents, disinfectants and bleach.		Disposal of rubbish is an important aspect of house cleaning. Plastic bags are designed and manufactured specifically for the collection of litter. Many are sized to fit common waste baskets and trash cans. Paper bags are made to carry aluminum cans, glass jars and other things although most people use plastic bins for glass since it could break and tear through the bag. Recycling is possible with some kinds of litter.[6]		Over time dust accumulates on household surfaces. As well as making the surfaces dirty, when dust is disturbed it can become suspended in the air, causing sneezing and breathing trouble. It can also transfer from furniture to clothing, making it unclean. Various tools have been invented for dust removal; Feather and lamb’s wool dusters, cotton and polyester dust cloths, furniture spray, disposable paper "dust cloths", dust mops for smooth floors and vacuum cleaners. Vacuum cleaners often have a variety of tools to enable them to remove dirt not just from carpets and rugs, but from hard surfaces and upholstery.[7]		Examples of dirt or "soil" are detritus and common spills and stains that exists in the home. Equipment used with a cleaner might be a bucket and sponge or a rag. A modern tool is the spray bottle, but the principle is the same.		Various household cleaning products have been developed to facilitate the removal of dust and dirt, for surface maintenance, and for disinfection.[8] Products are available in powder, liquid or spray form. The basic ingredients determine the type of cleaning tasks for which they are suitable. Some are packaged as general purpose cleaning materials while others are targeted at specific cleaning tasks such as drain clearing, oven cleaning, lime scale removal and polishing furniture. Household cleaning products provide aesthetic and hygiene benefits but are also associated with health risks for the users, and building occupants.[9] The US Department of Health and Human Services offers the public access to the Household Products Database. This database provides consumer information for over 4,000 products based on information provided by the manufacturer through the material safety data sheet.[10]		Surfactants lower the surface tension of water, making it able to flow into smaller tiny cracks and crevices in soils making removal easier. Alkaline chemicals break down known soils such as grease and mud. Acids break down soils such as lime scale, soap scum, and stains of mustard, coffee, tea, and alcoholic beverages. Some solvent-based products are flammable and some can dissolve paint and varnish. Disinfectants stop smell and stains caused by bacteria.		When multiple chemicals are applied to the same surface without full removal of the earlier substance, the chemicals may interact. This interaction may result in a reduction of the efficiency of the chemicals applied (such as a change in pH value caused by mixing alkalis and acids) and in cases may even emit toxic fumes. An example of this is the mixing of ammonia-based cleaners (or acid-based cleaners) and bleach.[11] This causes the production of chloramines that volatilize (become gaseous) causing acute inflammation of the lungs (toxic pneumonitis), long-term respiratory damage, and potential death.[12]		Residue from cleaning products and cleaning activity (dusting, vacuuming, sweeping) have been shown to impact indoor air quality (IAQ) by redistributing particulate matter (dust, dirt, human skin cells, organic matter, animal dander, particles from combustion, fibers from insulation, pollen, and polycyclic aromatic hydrocarbons) that gaseous or liquid particles become adsorbed to. The particulate matter and chemical residual will of be highest concentrations right after cleaning but will decrease over time depending upon levels of contaminants, air exchange rate, and other sources of chemical residual.[11] Of most concern are the family of chemicals called VOCs such as formaldehyde, toluene, and limonene.[13]		Volatile organic compounds (VOCs) are released from many household cleaning products such as disinfectants, polishes, floor waxes, air-freshening sprays, all purpose cleaning sprays, and glass cleaner. These products have been shown to emit irritating vapors.[8][14][15] VOCs are of most concern due to their tendency to evaporate and be inhaled into the lungs or adsorbed to existing dust, which can also be inhaled.[8] It has been found that aerosolized (spray) cleaning products are important risk factors and may aggravate symptoms of adult asthma,[15] respiratory irritation,[8] childhood asthma, wheeze, bronchitis, and allergy.[14]		Other modes of exposure to potentially harmful household cleaning chemicals include absorption through the skin (dermis), accidental ingestion, and accidental splashing into the eyes. Products for the application and safe use of the chemicals are also available, such as nylon scrub sponge and rubber gloves. [16] It is up to the consumer to keep themselves safe while using these chemicals. Reading and comprehending the labels is important.		Chemicals used for cleaning toilet, sinks, and bathtubs can find their way into our sewage water and can often not be effectively removed or filtered.		There is a growing consumer and governmental interest in natural cleaning products and green cleaning methods. The use of nontoxic household chemicals is growing as consumers become more informed of the health effects of many household chemicals, and municipalities are having to deal with the expensive disposal of household hazardous waste (HHW).[17][18]		Brooms remove debris from floors and dustpans carry dust and debris swept into them, buckets hold cleaning and rinsing solutions, vacuum cleaners and carpet sweepers remove surface dust and debris, chamois leather and squeegees are used for window-cleaning, and mops are used for washing floors.[19] Protective gear including rubber gloves, face covers, and protective eyewear is also used when dealing with cleaning products.[citation needed]		A home's yard and exterior are sometimes subject to cleaning. Exterior cleaning also occurs for safety, upkeep and usefulness. It includes removal of paper litter and grass growing in sidewalk cracks.		House chores, or chores are components of housekeeping, and are usually in reference to specific tasks to be completed. Examples of house chores are: washing dishes; taking out trash after dinner.		While housekeeping can be seen as an objective activity that can be done by either men or women, some people have argued that housekeeping is a site of historical oppression and gender division between traditionally gendered men and women.[20] Housekeeping also has a role in maintaining certain parts of the capitalist economy, including the division of home and work life, as well as industries that sell chemicals and household goods.		A survey conducted by the U.S. Bureau of Labor Statistics in 2014 came to the result that approximately 43 percent of men did food preparation or cleanup on any given day, compared with approximately 70 percent of women. In addition, 20 percent of men did housekeeping chores (including cleaning and laundry) on any given day, compared to approximately 50 percent of women.[21]		Wikibooks has The Housework Manual as well as books on these subjects:		
Fika (Swedish pronunciation: [²fiːka]) is a concept in Swedish culture with the basic meaning "to have coffee", often accompanied with pastries, cookies or pie.[1] A more contemporary generalised meaning of the word, where the coffee may be replaced by tea or even juice, lemonade or saft for children, has become widespread. In some social circles, even just a sandwich or a small meal may be denoted a fika similar to the English concept of afternoon tea. In Sweden pastries in general (for example cinnamon buns) are often referred to as fikabröd ("fika bread").		The Swedish tradition is spreading through Swedish businesses around the world.[2]						The word "fika" is an example of the back slang used in the 19th century, in which syllables of a word were reversed, deriving fika from kaffi, an earlier variant of the Swedish word kaffe ("coffee").[3] From fika also comes the word fik (a colloquial term for "café") through a process of back-formation.[4]		An older meaning of fika is "to strive (for)" in Swedish,[5][6] and fika therefore becomes a combination of the reversed syllables of kaffe (coffee) and the Swedish verb word (fika)[citation needed].		Fika is considered a social institution in Sweden; it means having a break, most often a coffee break, with one's colleagues, friends, date or family. The word fika can be used as both verb and a noun. You can fika at work by taking a "coffee break", fika with someone like a "coffee date", or just drink a cup of coffee, tea or other non-alcoholic beverage. As such, the word has quite ambiguous connotations, but almost always includes something to eat, such as biscuits, cakes and even sweets, accompanied with the drink. This practice of taking a break, often with a cinnamon roll, biscuits, or cookies, and perhaps with fruit on the side, is central to Swedish life.[7]		Although the word may in itself imply "taking a break from work", this is often emphasized using the word fikapaus ("fika pause") or fikarast ("fika break"), with kaffepaus and kafferast, respectively, as near synonyms. Fika may also mean having coffee or other beverages at a café or konditori (a "patisserie-based coffeehouse").[8]		Traditionally, fika requires sweet, baked goods, especially cinnamon rolls.[9]		Kaffebröd ("coffee bread"; sometimes called fikabröd, "fika bread") is a collective name for all kinds of biscuits, cookies, buns, etc. that are traditionally eaten with coffee. Non-sweetened breads are normally not included in this term (even though these may sometimes be consumed with coffee). Used as a noun fika also refers to kaffebröd and coffee combined.		Fika is a common practice at workplaces in Sweden where it constitutes at least one break during a normal workday. Often, two fikor are taken in a day at around 9:00 in the morning and 3:00 in the afternoon. The work fika is an important social event where employees can gather and socialize to discuss private and professional matters. It is not uncommon for management to join employees and to some extent it can even be considered impolite not to join one's colleagues at fika.[10] The practice is not limited to any specific sector of the labor market and is considered normal practice even in government administration.[11]		
Conveyor belt sushi (Japanese: 回転寿司, Hepburn: kaiten-zushi), literally "rotation sushi", also called sushi-go-round (くるくる寿司, kuru kuru sushi), is a form of sushi restaurant common in Japan. In Australia, it is also known as a sushi train.		Kaiten-zushi is a sushi restaurant where the plates with the sushi are placed on a rotating conveyor belt or moat that winds through the restaurant and moves past every table, counter and seat. Customers may place special orders, but most simply pick their selections from a steady stream of fresh sushi moving along the conveyor belt. The final bill is based on the number and type of plates of the consumed sushi. Some restaurants use a fancier presentation such as miniature wooden "sushi boats" traveling small canals or miniature locomotive cars.						The most remarkable feature of conveyor belt sushi is the stream of plates winding through the restaurant. The selection is usually not limited to sushi; it may also include drinks, fruits, desserts, soups, and other foods.		Some restaurants have RFID tags or other systems in place to remove sushi that has rotated for too long.		If customers cannot find their desired sushi, they can make special orders. Sometimes speaker phones are available for this purpose above the conveyor belt. If a small quantity of sushi is ordered, it is placed on the conveyor belt but marked so other customers know that this dish was ordered by someone. Usually, the plate with the sushi sits on a labeled cylindrical stand to indicate that this is a special order. For large orders the sushi may also be brought to the customer by the attendants. Some restaurants in Japan also have touch screen displays for ordering specific dishes which might be delivered on a separate conveyor belt or by waiters.		Condiments and tools are usually found near the seats, for example pickled ginger, chopsticks, soy sauce, and small dishes for the soy sauce. Wasabi may be either at the seat or on the conveyor belt. Self-served tea and ice water is usually complimentary, with cups stacked on a shelf above the conveyor belt and teabags or green tea powder in a storage container on the table. There is also a hot water faucet at the tables to make tea. On the shelves are usually wet paper towels and plastic boxes to store sushi for take-out customers.		The bill is calculated by counting the number and type of plates of the consumed sushi. Plates with different colors, patterns, or shapes have different prices, usually ranging from 100 yen to 500 yen. The cost of each plate is shown on signboards or posters in the restaurant. In general, cheap items come on plain plates, and the level of plate decoration is related to the price. The most expensive items tend to come on gold colored plates. Expensive items may be placed on two plates, with the price being the sum of the prices of the individual plates. Some conveyor belt sushi restaurant chains, such as Kappa Sushi or Otaru Zushi, have a fixed price of 100 yen for every plate. This is similar to the phenomenon of 100-yen shops. A button above the conveyor belt can be used to call the attendants to count the plates. Some restaurants have a counting machine where the customer drops the plates to be counted automatically.		The sushi conveyor consists of a thin, narrow conveyor designed to fit within the tight confines of a sushi restaurant. Virtually 100% of sushi conveyors made in Japan are manufactured in Ishikawa Prefecture.[1]		The standard conveyor uses a specially designed plastic crescent top chain. The chain actually runs on its side (on its link plates), with the crescent plate attached to the other side plate by means of a snap pin. This gives the chain a very small bending radius and allows the conveyor to make the tight corners found in most conveyor belt sushi restaurants. Further, the horizontal layout means that there is no return side of the chain, which not only eliminates chain sag and sliding with the roller, but allows for a much shallower design.[2]		Major chain companies can offer different pin materials (stainless steel being common), plate shapes, surface treatments, and so on depending on the individual application. Many customers are also turning to sushi conveyor manufacturers for custom designed plates to go with their conveyor. Innovations in sushi conveyors include chainless designs for quieter operation and design/layout freedom, multi-tiered conveyors to allow for more sushi to be displayed in limited spaces, and high speed lanes for custom orders.[3]		Conveyor belt sushi was invented by Yoshiaki Shiraishi (1914–2001), who had problems staffing his small sushi restaurant and had difficulties managing the restaurant by himself. He got the idea of a conveyor belt sushi after watching beer bottles on a conveyor belt in an Asahi brewery.[4] After five years of development, including the design of the conveyor belt and the speed of operations, Shiraishi opened the first conveyor belt sushi Mawaru Genroku Sushi in Higashiosaka in 1958, eventually expanding to up to 250 restaurants all over Japan. However, by 2001, his company had just 11 restaurants.[5] Shiraishi also invented a robotic sushi, served by robots, but this idea has not had commercial success.		Initially in a conveyor belt sushi restaurant, all customers were seated to face the conveyor belt, but this was not popular with groups. Subsequently, tables were added at right angles to the conveyor belt, allowing up to six people to sit at one table. This also reduced the length of conveyor belt needed to serve a certain number of people.		A conveyor belt sushi boom started in 1970 after a conveyor belt sushi restaurant served sushi at the Osaka World Expo.[6][7] Another boom started in 1980, when eating out became more popular, and finally in the late 1990s, when inexpensive restaurants became popular after the burst of the economic bubble. Recently, Akindo Sushiro became the most famous brand in 2010 in Japan.[8]		A new variant of conveyor belt sushi has a touch screen monitor at every table, showing a virtual aquarium with many fish. The customer can order the sushi by touching the type of fish, which is then brought to the table by conveyor belt.		
A disease is a particular abnormal condition that affects part or all of an organism and that consists of a disorder of a structure or function. The study of disease is called pathology, which includes the study of cause. Disease is often construed as a medical condition associated with specific symptoms and signs.[1] It may be caused by external factors such as pathogens or by internal dysfunctions, particularly of the immune system, such as an immunodeficiency, or by a hypersensitivity, including allergies and autoimmunity.		When caused by pathogens (e.g. malaria by Plasmodium ssp.), the term disease is often misleadingly used even in the scientific literature in place of its causal agent, the pathogen. This language habit can cause confusion in the communication of the cause-effect principle in epidemiology, and as such it should be strongly discouraged.[2]		In humans, disease is often used more broadly to refer to any condition that causes pain, dysfunction, distress, social problems, or death to the person afflicted, or similar problems for those in contact with the person. In this broader sense, it sometimes includes injuries, disabilities, disorders, syndromes, infections, isolated symptoms, deviant behaviors, and atypical variations of structure and function, while in other contexts and for other purposes these may be considered distinguishable categories. Diseases can affect people not only physically, but also emotionally, as contracting and living with a disease can alter the affected person's perspective on life.[citation needed]		Death due to disease is called death by natural causes. There are four main types of disease: infectious diseases, deficiency diseases, genetic diseases (both hereditary and non-hereditary), and physiological diseases. Diseases can also be classified as communicable and non-communicable. The deadliest diseases in humans are coronary artery disease (blood flow obstruction), followed by cerebrovascular disease and lower respiratory infections.[3]						In many cases, terms such as disease, disorder, morbidity, sickness and illness are used interchangeably.[4] There are situations, however, when specific terms are considered preferable.		In an infectious disease, the incubation period is the time between infection and the appearance of symptoms. The latency period is the time between infection and the ability of the disease to spread to another person, which may precede, follow, or be simultaneous with the appearance of symptoms. Some viruses also exhibit a dormant phase, called viral latency, in which the virus hides in the body in an inactive state. For example, varicella zoster virus causes chickenpox in the acute phase; after recovery from chickenpox, the virus may remain dormant in nerve cells for many years, and later cause herpes zoster (shingles).		Diseases may be classified by cause, pathogenesis (mechanism by which the disease is caused), or by symptom(s). Alternatively, diseases may be classified according to the organ system involved, though this is often complicated since many diseases affect more than one organ.		A chief difficulty in nosology is that diseases often cannot be defined and classified clearly, especially when cause or pathogenesis are unknown. Thus diagnostic terms often only reflect a symptom or set of symptoms (syndrome).		Classical classification of human disease derives from observational correlation between pathological analysis and clinical syndromes. Today it is preferred to classify them by their cause if it is known.[19]		The most known and used classification of diseases is the World Health Organization's ICD. This is periodically updated. Currently the last publication is the ICD-10.		Only some diseases such as influenza are contagious and commonly believed infectious. The micro-organisms that cause these diseases are known as pathogens and include varieties of bacteria, viruses, protozoa and fungi. Infectious diseases can be transmitted, e.g. by hand-to-mouth contact with infectious material on surfaces, by bites of insects or other carriers of the disease, and from contaminated water or food (often via fecal contamination), etc.[20] In addition, there are sexually transmitted diseases. In some cases, microorganisms that are not readily spread from person to person play a role, while other diseases can be prevented or ameliorated with appropriate nutrition or other lifestyle changes.		Some diseases, such as most (but not all) forms of cancer, heart disease, and mental disorders, are non-infectious diseases. Many non-infectious diseases have a partly or completely genetic basis (see genetic disorder) and may thus be transmitted from one generation to another.		Social determinants of health are the social conditions in which people live that determine their health. Illnesses are generally related to social, economic, political, and environmental circumstances. Social determinants of health have been recognized by several health organizations such as the Public Health Agency of Canada and the World Health Organization to greatly influence collective and personal well-being. The World Health Organization's Social Determinants Council also recognizes Social determinants of health in poverty.		When the cause of a disease is poorly understood, societies tend to mythologize the disease or use it as a metaphor or symbol of whatever that culture considers evil. For example, until the bacterial cause of tuberculosis was discovered in 1882, experts variously ascribed the disease to heredity, a sedentary lifestyle, depressed mood, and overindulgence in sex, rich food, or alcohol—all the social ills of the time.[21]		Many diseases and disorders can be prevented through a variety of means. These include sanitation, proper nutrition, adequate exercise, vaccinations and other self-care and public health measures.		Medical therapies or treatments are efforts to cure or improve a disease or other health problem. In the medical field, therapy is synonymous with the word treatment. Among psychologists, the term may refer specifically to psychotherapy or "talk therapy". Common treatments include medications, surgery, medical devices, and self-care. Treatments may be provided by an organized health care system, or informally, by the patient or family members.		Preventive healthcare is a way to avoid an injury, sickness, or disease in the first place. A treatment or cure is applied after a medical problem has already started. A treatment attempts to improve or remove a problem, but treatments may not produce permanent cures, especially in chronic diseases. Cures are a subset of treatments that reverse diseases completely or end medical problems permanently. Many diseases that cannot be completely cured are still treatable. Pain management (also called pain medicine) is that branch of medicine employing an interdisciplinary approach to the relief of pain and improvement in the quality of life of those living with pain.[22]		Treatment for medical emergencies must be provided promptly, often through an emergency department or, in less critical situations, through an urgent care facility.		Epidemiology is the study of the factors that cause or encourage diseases. Some diseases are more common in certain geographic areas, among people with certain genetic or socioeconomic characteristics, or at different times of the year.		Epidemiology is considered a cornerstone methodology of public health research, and is highly regarded in evidence-based medicine for identifying risk factors for disease. In the study of communicable and non-communicable diseases, the work of epidemiologists ranges from outbreak investigation to study design, data collection and analysis including the development of statistical models to test hypotheses and the documentation of results for submission to peer-reviewed journals. Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic. Epidemiologists rely on a number of other scientific disciplines such as biology (to better understand disease processes), biostatistics (the current raw information available), Geographic Information Science (to store data and map disease patterns) and social science disciplines (to better understand proximate and distal risk factors). Epidemiology can help identify causes as well as guide prevention efforts.		In studying diseases, epidemiology faces the challenge of defining them. Especially for poorly understood diseases, different groups might use significantly different definitions. Without an agreed-on definition, different researchers may report different numbers of cases and characteristics of the disease.[23]		Some morbidity databases are compiled with data supplied by states and territories health authorities, at national level (National hospital morbidity database (NHMD), for example[24][25]), or at European scale (European Hospital Morbidity Database or HMDB[26]) but not yet at world scale.		Disease burden is the impact of a health problem in an area measured by financial cost, mortality, morbidity, or other indicators.		There are several measures used to quantify the burden imposed by diseases on people. The years of potential life lost (YPLL) is a simple estimate of the number of years that a person's life was shortened due to a disease. For example, if a person dies at the age of 65 from a disease, and would probably have lived until age 80 without that disease, then that disease has caused a loss of 15 years of potential life. YPLL measurements do not account for how disabled a person is before dying, so the measurement treats a person who dies suddenly and a person who died at the same age after decades of illness as equivalent. In 2004, the World Health Organization calculated that 932 million years of potential life were lost to premature death.[27]		The quality-adjusted life year (QALY) and disability-adjusted life year (DALY) metrics are similar, but take into account whether the person was healthy after diagnosis. In addition to the number of years lost due to premature death, these measurements add part of the years lost to being sick. Unlike YPLL, these measurements show the burden imposed on people who are very sick, but who live a normal lifespan. A disease that has high morbidity, but low mortality, has a high DALY and a low YPLL. In 2004, the World Health Organization calculated that 1.5 billion disability-adjusted life years were lost to disease and injury.[27] In the developed world, heart disease and stroke cause the most loss of life, but neuropsychiatric conditions like major depressive disorder cause the most years lost to being sick.		How a society responds to diseases is the subject of medical sociology.		A condition may be considered a disease in some cultures or eras but not in others. For example, obesity can represent wealth and abundance, and is a status symbol in famine-prone areas and some places hard-hit by HIV/AIDS.[29] Epilepsy is considered a sign of spiritual gifts among the Hmong people.[30]		Sickness confers the social legitimization of certain benefits, such as illness benefits, work avoidance, and being looked after by others. The person who is sick takes on a social role called the sick role. A person who responds to a dreaded disease, such as cancer, in a culturally acceptable fashion may be publicly and privately honored with higher social status.[31] In return for these benefits, the sick person is obligated to seek treatment and work to become well once more. As a comparison, consider pregnancy, which is not interpreted as a disease or sickness, even if the mother and baby may both benefit from medical care.		Most religions grant exceptions from religious duties to people who are sick. For example, one whose life would be endangered by fasting on Yom Kippur or during Ramadan is exempted from the requirement, or even forbidden from participating. People who are sick are also exempted from social duties. For example, ill health is the only socially acceptable reason for an American to refuse an invitation to the White House.[32]		The identification of a condition as a disease, rather than as simply a variation of human structure or function, can have significant social or economic implications. The controversial recognitions as diseases of repetitive stress injury (RSI) and post-traumatic stress disorder (also known as "Soldier's heart", "shell shock", and "combat fatigue") has had a number of positive and negative effects on the financial and other responsibilities of governments, corporations and institutions towards individuals, as well as on the individuals themselves. The social implication of viewing aging as a disease could be profound, though this classification is not yet widespread.		Lepers were people who were historically shunned because they had an infectious disease, and the term "leper" still evokes social stigma. Fear of disease can still be a widespread social phenomenon, though not all diseases evoke extreme social stigma.		Social standing and economic status affect health. Diseases of poverty are diseases that are associated with poverty and low social status; diseases of affluence are diseases that are associated with high social and economic status. Which diseases are associated with which states varies according to time, place, and technology. Some diseases, such as diabetes mellitus, may be associated with both poverty (poor food choices) and affluence (long lifespans and sedentary lifestyles), through different mechanisms. The term lifestyle diseases describes diseases associated with longevity and that are more common among older people. For example, cancer is far more common in societies in which most members live until they reach the age of 80 than in societies in which most members die before they reach the age of 50.		An illness narrative is a way of organizing a medical experience into a coherent story that illustrates the sick individual's personal experience.		People use metaphors to make sense of their experiences with disease. The metaphors move disease from an objective thing that exists to an affective experience. The most popular metaphors draw on military concepts: Disease is an enemy that must be feared, fought, battled, and routed. The patient or the healthcare provider is a warrior, rather than a passive victim or bystander. The agents of communicable diseases are invaders; non-communicable diseases constitute internal insurrection or civil war. Because the threat is urgent, perhaps a matter of life and death, unthinkably radical, even oppressive, measures are society's and the patient's moral duty as they courageously mobilize to struggle against destruction. The War on Cancer is an example of this metaphorical use of language.[33] This language is empowering to some patients, but leaves others feeling like they are failures.[34]		Another class of metaphors describes the experience of illness as a journey: The person travels to or from a place of disease, and changes himself, discovers new information, or increases his experience along the way. He may travel "on the road to recovery" or make changes to "get on the right track" or choose "pathways".[33][34] Some are explicitly immigration-themed: the patient has been exiled from the home territory of health to the land of the ill, changing identity and relationships in the process.[35] This language is more common among British healthcare professionals than the language of physical aggression.[34]		Some metaphors are disease-specific. Slavery is a common metaphor for addictions: The alcoholic is enslaved by drink, and the smoker is captive to nicotine. Some cancer patients treat the loss of their hair from chemotherapy as a metonymy or metaphor for all the losses caused by the disease.[33]		Some diseases are used as metaphors for social ills: "Cancer" is a common description for anything that is endemic and destructive in society, such as poverty, injustice, or racism. AIDS was seen as a divine judgment for moral decadence, and only by purging itself from the "pollution" of the "invader" could society become healthy again.[33] More recently, when AIDS seemed less threatening, this type of emotive language was applied to avian flu and type 2 diabetes mellitus.[36] Authors in the 19th century commonly used tuberculosis as a symbol and a metaphor for transcendence. Victims of the disease were portrayed in literature as having risen above daily life to become ephemeral objects of spiritual or artistic achievement. In the 20th century, after its cause was better understood, the same disease became the emblem of poverty, squalor, and other social problems.[35]		Medical sign Symptom Syndrome		Medical diagnosis Differential diagnosis Prognosis		Acute Chronic Cure/Remission		Disease Eponymous disease Acronym or abbreviation		
Metabolism (from Greek: μεταβολή metabolē, "change") is the set of life-sustaining chemical transformations within the cells of living organisms. The three main purposes of metabolism are the conversion of food/fuel to energy to run cellular processes, the conversion of food/fuel to building blocks for proteins, lipids, nucleic acids, and some carbohydrates, and the elimination of nitrogenous wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. The word metabolism can also refer to the sum of all chemical reactions that occur in living organisms, including digestion and the transport of substances into and between different cells, in which case the set of reactions within the cells is called intermediary metabolism or intermediate metabolism.		Metabolism is usually divided into two categories: catabolism, the breaking down of organic matter for example, the breaking down of glucose to pyruvate, by cellular respiration, and anabolism, the building up of components of cells such as proteins and nucleic acids. Usually, breaking down releases energy and building up consumes energy.		The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, by a sequence of enzymes. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts that allow the reactions to proceed more rapidly. Enzymes also allow the regulation of metabolic pathways in response to changes in the cell's environment or to signals from other cells.		The metabolic system of a particular organism determines which substances it will find nutritious and which poisonous. For example, some prokaryotes use hydrogen sulfide as a nutrient, yet this gas is poisonous to animals.[1] The speed of metabolism, the metabolic rate, influences how much food an organism will require, and also affects how it is able to obtain that food.		A striking feature of metabolism is the similarity of the basic metabolic pathways and components between even vastly different species.[2] For example, the set of carboxylic acids that are best known as the intermediates in the citric acid cycle are present in all known organisms, being found in species as diverse as the unicellular bacterium Escherichia coli and huge multicellular organisms like elephants.[3] These striking similarities in metabolic pathways are likely due to their early appearance in evolutionary history, and their retention because of their efficacy.[4][5]						Most of the structures that make up animals, plants and microbes are made from three basic classes of molecule: amino acids, carbohydrates and lipids (often called fats). As these molecules are vital for life, metabolic reactions either focus on making these molecules during the construction of cells and tissues, or by breaking them down and using them as a source of energy, by their digestion. These biochemicals can be joined together to make polymers such as DNA and proteins, essential macromolecules of life.		Proteins are made of amino acids arranged in a linear chain joined together by peptide bonds. Many proteins are enzymes that catalyze the chemical reactions in metabolism. Other proteins have structural or mechanical functions, such as those that form the cytoskeleton, a system of scaffolding that maintains the cell shape.[6] Proteins are also important in cell signaling, immune responses, cell adhesion, active transport across membranes, and the cell cycle.[7] Amino acids also contribute to cellular energy metabolism by providing a carbon source for entry into the citric acid cycle (tricarboxylic acid cycle),[8] especially when a primary source of energy, such as glucose, is scarce, or when cells undergo metabolic stress.[9]		Lipids are the most diverse group of biochemicals. Their main structural uses are as part of biological membranes both internal and external, such as the cell membrane, or as a source of energy.[7] Lipids are usually defined as hydrophobic or amphipathic biological molecules but will dissolve in organic solvents such as benzene or chloroform.[10] The fats are a large group of compounds that contain fatty acids and glycerol; a glycerol molecule attached to three fatty acid esters is called a triacylglyceride.[11] Several variations on this basic structure exist, including alternate backbones such as sphingosine in the sphingolipids, and hydrophilic groups such as phosphate as in phospholipids. Steroids such as cholesterol are another major class of lipids.[12]		Carbohydrates are aldehydes or ketones, with many hydroxyl groups attached, that can exist as straight chains or rings. Carbohydrates are the most abundant biological molecules, and fill numerous roles, such as the storage and transport of energy (starch, glycogen) and structural components (cellulose in plants, chitin in animals).[7] The basic carbohydrate units are called monosaccharides and include galactose, fructose, and most importantly glucose. Monosaccharides can be linked together to form polysaccharides in almost limitless ways.[13]		The two nucleic acids, DNA and RNA, are polymers of nucleotides. Each nucleotide is composed of a phosphate attached to a ribose or deoxyribose sugar group which is attached to a nitrogenous base. Nucleic acids are critical for the storage and use of genetic information, and its interpretation through the processes of transcription and protein biosynthesis.[7] This information is protected by DNA repair mechanisms and propagated through DNA replication. Many viruses have an RNA genome, such as HIV, which uses reverse transcription to create a DNA template from its viral RNA genome.[14] RNA in ribozymes such as spliceosomes and ribosomes is similar to enzymes as it can catalyze chemical reactions. Individual nucleosides are made by attaching a nucleobase to a ribose sugar. These bases are heterocyclic rings containing nitrogen, classified as purines or pyrimidines. Nucleotides also act as coenzymes in metabolic-group-transfer reactions.[15]		Metabolism involves a vast array of chemical reactions, but most fall under a few basic types of reactions that involve the transfer of functional groups of atoms and their bonds within molecules.[16] This common chemistry allows cells to use a small set of metabolic intermediates to carry chemical groups between different reactions.[15] These group-transfer intermediates are called coenzymes. Each class of group-transfer reactions is carried out by a particular coenzyme, which is the substrate for a set of enzymes that produce it, and a set of enzymes that consume it. These coenzymes are therefore continuously made, consumed and then recycled.[17]		One central coenzyme is adenosine triphosphate (ATP), the universal energy currency of cells. This nucleotide is used to transfer chemical energy between different chemical reactions. There is only a small amount of ATP in cells, but as it is continuously regenerated, the human body can use about its own weight in ATP per day.[17] ATP acts as a bridge between catabolism and anabolism. Catabolism breaks down molecules, and anabolism puts them together. Catabolic reactions generate ATP, and anabolic reactions consume it. It also serves as a carrier of phosphate groups in phosphorylation reactions.		A vitamin is an organic compound needed in small quantities that cannot be made in cells. In human nutrition, most vitamins function as coenzymes after modification; for example, all water-soluble vitamins are phosphorylated or are coupled to nucleotides when they are used in cells.[18] Nicotinamide adenine dinucleotide (NAD+), a derivative of vitamin B3 (niacin), is an important coenzyme that acts as a hydrogen acceptor. Hundreds of separate types of dehydrogenases remove electrons from their substrates and reduce NAD+ into NADH. This reduced form of the coenzyme is then a substrate for any of the reductases in the cell that need to reduce their substrates.[19] Nicotinamide adenine dinucleotide exists in two related forms in the cell, NADH and NADPH. The NAD+/NADH form is more important in catabolic reactions, while NADP+/NADPH is used in anabolic reactions.		Inorganic elements play critical roles in metabolism; some are abundant (e.g. sodium and potassium) while others function at minute concentrations. About 99% of a mammal's mass is made up of the elements carbon, nitrogen, calcium, sodium, chlorine, potassium, hydrogen, phosphorus, oxygen and sulfur.[20] Organic compounds (proteins, lipids and carbohydrates) contain the majority of the carbon and nitrogen; most of the oxygen and hydrogen is present as water.[20]		The abundant inorganic elements act as ionic electrolytes. The most important ions are sodium, potassium, calcium, magnesium, chloride, phosphate and the organic ion bicarbonate. The maintenance of precise ion gradients across cell membranes maintains osmotic pressure and pH.[21] Ions are also critical for nerve and muscle function, as action potentials in these tissues are produced by the exchange of electrolytes between the extracellular fluid and the cell's fluid, the cytosol.[22] Electrolytes enter and leave cells through proteins in the cell membrane called ion channels. For example, muscle contraction depends upon the movement of calcium, sodium and potassium through ion channels in the cell membrane and T-tubules.[23]		Transition metals are usually present as trace elements in organisms, with zinc and iron being most abundant of those.[24][25] These metals are used in some proteins as cofactors and are essential for the activity of enzymes such as catalase and oxygen-carrier proteins such as hemoglobin.[26] Metal cofactors are bound tightly to specific sites in proteins; although enzyme cofactors can be modified during catalysis, they always return to their original state by the end of the reaction catalyzed. Metal micronutrients are taken up into organisms by specific transporters and bind to storage proteins such as ferritin or metallothionein when not in use.[27][28]		Catabolism is the set of metabolic processes that break down large molecules. These include breaking down and oxidizing food molecules. The purpose of the catabolic reactions is to provide the energy and components needed by anabolic reactions which build molecules. The exact nature of these catabolic reactions differ from organism to organism, and organisms can be classified based on their sources of energy and carbon (their primary nutritional groups), as shown in the table below. Organic molecules are used as a source of energy by organotrophs, while lithotrophs use inorganic substrates, and phototrophs capture sunlight as chemical energy. However, all these different forms of metabolism depend on redox reactions that involve the transfer of electrons from reduced donor molecules such as organic molecules, water, ammonia, hydrogen sulfide or ferrous ions to acceptor molecules such as oxygen, nitrate or sulfate.[29] In animals, these reactions involve complex organic molecules that are broken down to simpler molecules, such as carbon dioxide and water. In photosynthetic organisms, such as plants and cyanobacteria, these electron-transfer reactions do not release energy but are used as a way of storing energy absorbed from sunlight.[30]		The most common set of catabolic reactions in animals can be separated into three main stages. In the first stage, large organic molecules, such as proteins, polysaccharides or lipids, are digested into their smaller components outside cells. Next, these smaller molecules are taken up by cells and converted to smaller molecules, usually acetyl coenzyme A (acetyl-CoA), which releases some energy. Finally, the acetyl group on the CoA is oxidised to water and carbon dioxide in the citric acid cycle and electron transport chain, releasing the energy that is stored by reducing the coenzyme nicotinamide adenine dinucleotide (NAD+) into NADH.		Macromolecules such as starch, cellulose or proteins cannot be rapidly taken up by cells and must be broken into their smaller units before they can be used in cell metabolism. Several common classes of enzymes digest these polymers. These digestive enzymes include proteases that digest proteins into amino acids, as well as glycoside hydrolases that digest polysaccharides into simple sugars known as monosaccharides.		Microbes simply secrete digestive enzymes into their surroundings,[31][32] while animals only secrete these enzymes from specialized cells in their guts.[33] The amino acids or sugars released by these extracellular enzymes are then pumped into cells by active transport proteins.[34][35]		Carbohydrate catabolism is the breakdown of carbohydrates into smaller units. Carbohydrates are usually taken into cells once they have been digested into monosaccharides.[36] Once inside, the major route of breakdown is glycolysis, where sugars such as glucose and fructose are converted into pyruvate and some ATP is generated.[37] Pyruvate is an intermediate in several metabolic pathways, but the majority is converted to acetyl-CoA through aerobic (with oxygen) glycolysis and fed into the citric acid cycle. Although some more ATP is generated in the citric acid cycle, the most important product is NADH, which is made from NAD+ as the acetyl-CoA is oxidized. This oxidation releases carbon dioxide as a waste product. In anaerobic conditions, glycolysis produces lactate, through the enzyme lactate dehydrogenase re-oxidizing NADH to NAD+ for re-use in glycolysis. An alternative route for glucose breakdown is the pentose phosphate pathway, which reduces the coenzyme NADPH and produces pentose sugars such as ribose, the sugar component of nucleic acids.		Fats are catabolised by hydrolysis to free fatty acids and glycerol. The glycerol enters glycolysis and the fatty acids are broken down by beta oxidation to release acetyl-CoA, which then is fed into the citric acid cycle. Fatty acids release more energy upon oxidation than carbohydrates because carbohydrates contain more oxygen in their structures. Steroids are also broken down by some bacteria in a process similar to beta oxidation, and this breakdown process involves the release of significant amounts of acetyl-CoA, propionyl-CoA, and pyruvate, which can all be used by the cell for energy. M. tuberculosis can also grow on the lipid cholesterol as a sole source of carbon, and genes involved in the cholesterol use pathway(s) have been validated as important during various stages of the infection lifecycle of M. tuberculosis.[38]		Amino acids are either used to synthesize proteins and other biomolecules, or oxidized to urea and carbon dioxide as a source of energy.[39] The oxidation pathway starts with the removal of the amino group by a transaminase. The amino group is fed into the urea cycle, leaving a deaminated carbon skeleton in the form of a keto acid. Several of these keto acids are intermediates in the citric acid cycle, for example the deamination of glutamate forms α-ketoglutarate.[40] The glucogenic amino acids can also be converted into glucose, through gluconeogenesis (discussed below).[41]		In oxidative phosphorylation, the electrons removed from organic molecules in areas such as the protagon acid cycle are transferred to oxygen and the energy released is used to make ATP. This is done in eukaryotes by a series of proteins in the membranes of mitochondria called the electron transport chain. In prokaryotes, these proteins are found in the cell's inner membrane.[42] These proteins use the energy released from passing electrons from reduced molecules like NADH onto oxygen to pump protons across a membrane.[43]		Pumping protons out of the mitochondria creates a proton concentration difference across the membrane and generates an electrochemical gradient.[44] This force drives protons back into the mitochondrion through the base of an enzyme called ATP synthase. The flow of protons makes the stalk subunit rotate, causing the active site of the synthase domain to change shape and phosphorylate adenosine diphosphate – turning it into ATP.[17]		Chemolithotrophy is a type of metabolism found in prokaryotes where energy is obtained from the oxidation of inorganic compounds. These organisms can use hydrogen,[45] reduced sulfur compounds (such as sulfide, hydrogen sulfide and thiosulfate),[1] ferrous iron (FeII)[46] or ammonia[47] as sources of reducing power and they gain energy from the oxidation of these compounds with electron acceptors such as oxygen or nitrite.[48] These microbial processes are important in global biogeochemical cycles such as acetogenesis, nitrification and denitrification and are critical for soil fertility.[49][50]		The energy in sunlight is captured by plants, cyanobacteria, purple bacteria, green sulfur bacteria and some protists. This process is often coupled to the conversion of carbon dioxide into organic compounds, as part of photosynthesis, which is discussed below. The energy capture and carbon fixation systems can however operate separately in prokaryotes, as purple bacteria and green sulfur bacteria can use sunlight as a source of energy, while switching between carbon fixation and the fermentation of organic compounds.[51][52]		In many organisms the capture of solar energy is similar in principle to oxidative phosphorylation, as it involves the storage of energy as a proton concentration gradient. This proton motive force then drives ATP synthesis.[17] The electrons needed to drive this electron transport chain come from light-gathering proteins called photosynthetic reaction centres or rhodopsins. Reaction centers are classed into two types depending on the type of photosynthetic pigment present, with most photosynthetic bacteria only having one type, while plants and cyanobacteria have two.[53]		In plants, algae, and cyanobacteria, photosystem II uses light energy to remove electrons from water, releasing oxygen as a waste product. The electrons then flow to the cytochrome b6f complex, which uses their energy to pump protons across the thylakoid membrane in the chloroplast.[30] These protons move back through the membrane as they drive the ATP synthase, as before. The electrons then flow through photosystem I and can then either be used to reduce the coenzyme NADP+, for use in the Calvin cycle, which is discussed below, or recycled for further ATP generation.[54]		Anabolism is the set of constructive metabolic processes where the energy released by catabolism is used to synthesize complex molecules. In general, the complex molecules that make up cellular structures are constructed step-by-step from small and simple precursors. Anabolism involves three basic stages. First, the production of precursors such as amino acids, monosaccharides, isoprenoids and nucleotides, secondly, their activation into reactive forms using energy from ATP, and thirdly, the assembly of these precursors into complex molecules such as proteins, polysaccharides, lipids and nucleic acids.		Organisms differ according to the number of constructed molecules in their cells. Autotrophs such as plants can construct the complex organic molecules in cells such as polysaccharides and proteins from simple molecules like carbon dioxide and water. Heterotrophs, on the other hand, require a source of more complex substances, such as monosaccharides and amino acids, to produce these complex molecules. Organisms can be further classified by ultimate source of their energy: photoautotrophs and photoheterotrophs obtain energy from light, whereas chemoautotrophs and chemoheterotrophs obtain energy from inorganic oxidation reactions.		Photosynthesis is the synthesis of carbohydrates from sunlight and carbon dioxide (CO2). In plants, cyanobacteria and algae, oxygenic photosynthesis splits water, with oxygen produced as a waste product. This process uses the ATP and NADPH produced by the photosynthetic reaction centres, as described above, to convert CO2 into glycerate 3-phosphate, which can then be converted into glucose. This carbon-fixation reaction is carried out by the enzyme RuBisCO as part of the Calvin – Benson cycle.[55] Three types of photosynthesis occur in plants, C3 carbon fixation, C4 carbon fixation and CAM photosynthesis. These differ by the route that carbon dioxide takes to the Calvin cycle, with C3 plants fixing CO2 directly, while C4 and CAM photosynthesis incorporate the CO2 into other compounds first, as adaptations to deal with intense sunlight and dry conditions.[56]		In photosynthetic prokaryotes the mechanisms of carbon fixation are more diverse. Here, carbon dioxide can be fixed by the Calvin – Benson cycle, a reversed citric acid cycle,[57] or the carboxylation of acetyl-CoA.[58][59] Prokaryotic chemoautotrophs also fix CO2 through the Calvin – Benson cycle, but use energy from inorganic compounds to drive the reaction.[60]		In carbohydrate anabolism, simple organic acids can be converted into monosaccharides such as glucose and then used to assemble polysaccharides such as starch. The generation of glucose from compounds like pyruvate, lactate, glycerol, glycerate 3-phosphate and amino acids is called gluconeogenesis. Gluconeogenesis converts pyruvate to glucose-6-phosphate through a series of intermediates, many of which are shared with glycolysis.[37] However, this pathway is not simply glycolysis run in reverse, as several steps are catalyzed by non-glycolytic enzymes. This is important as it allows the formation and breakdown of glucose to be regulated separately, and prevents both pathways from running simultaneously in a futile cycle.[61][62]		Although fat is a common way of storing energy, in vertebrates such as humans the fatty acids in these stores cannot be converted to glucose through gluconeogenesis as these organisms cannot convert acetyl-CoA into pyruvate; plants do, but animals do not, have the necessary enzymatic machinery.[63] As a result, after long-term starvation, vertebrates need to produce ketone bodies from fatty acids to replace glucose in tissues such as the brain that cannot metabolize fatty acids.[64] In other organisms such as plants and bacteria, this metabolic problem is solved using the glyoxylate cycle, which bypasses the decarboxylation step in the citric acid cycle and allows the transformation of acetyl-CoA to oxaloacetate, where it can be used for the production of glucose.[63][65]		Polysaccharides and glycans are made by the sequential addition of monosaccharides by glycosyltransferase from a reactive sugar-phosphate donor such as uridine diphosphate glucose (UDP-glucose) to an acceptor hydroxyl group on the growing polysaccharide. As any of the hydroxyl groups on the ring of the substrate can be acceptors, the polysaccharides produced can have straight or branched structures.[66] The polysaccharides produced can have structural or metabolic functions themselves, or be transferred to lipids and proteins by enzymes called oligosaccharyltransferases.[67][68]		Fatty acids are made by fatty acid synthases that polymerize and then reduce acetyl-CoA units. The acyl chains in the fatty acids are extended by a cycle of reactions that add the acyl group, reduce it to an alcohol, dehydrate it to an alkene group and then reduce it again to an alkane group. The enzymes of fatty acid biosynthesis are divided into two groups: in animals and fungi, all these fatty acid synthase reactions are carried out by a single multifunctional type I protein,[69] while in plant plastids and bacteria separate type II enzymes perform each step in the pathway.[70][71]		Terpenes and isoprenoids are a large class of lipids that include the carotenoids and form the largest class of plant natural products.[72] These compounds are made by the assembly and modification of isoprene units donated from the reactive precursors isopentenyl pyrophosphate and dimethylallyl pyrophosphate.[73] These precursors can be made in different ways. In animals and archaea, the mevalonate pathway produces these compounds from acetyl-CoA,[74] while in plants and bacteria the non-mevalonate pathway uses pyruvate and glyceraldehyde 3-phosphate as substrates.[73][75] One important reaction that uses these activated isoprene donors is steroid biosynthesis. Here, the isoprene units are joined together to make squalene and then folded up and formed into a set of rings to make lanosterol.[76] Lanosterol can then be converted into other steroids such as cholesterol and ergosterol.[76][77]		Organisms vary in their ability to synthesize the 20 common amino acids. Most bacteria and plants can synthesize all twenty, but mammals can only synthesize eleven nonessential amino acids, so nine essential amino acids must be obtained from food.[7] Some simple parasites, such as the bacteria Mycoplasma pneumoniae, lack all amino acid synthesis and take their amino acids directly from their hosts.[78] All amino acids are synthesized from intermediates in glycolysis, the citric acid cycle, or the pentose phosphate pathway. Nitrogen is provided by glutamate and glutamine. Amino acid synthesis depends on the formation of the appropriate alpha-keto acid, which is then transaminated to form an amino acid.[79]		Amino acids are made into proteins by being joined together in a chain of peptide bonds. Each different protein has a unique sequence of amino acid residues: this is its primary structure. Just as the letters of the alphabet can be combined to form an almost endless variety of words, amino acids can be linked in varying sequences to form a huge variety of proteins. Proteins are made from amino acids that have been activated by attachment to a transfer RNA molecule through an ester bond. This aminoacyl-tRNA precursor is produced in an ATP-dependent reaction carried out by an aminoacyl tRNA synthetase.[80] This aminoacyl-tRNA is then a substrate for the ribosome, which joins the amino acid onto the elongating protein chain, using the sequence information in a messenger RNA.[81]		Nucleotides are made from amino acids, carbon dioxide and formic acid in pathways that require large amounts of metabolic energy.[82] Consequently, most organisms have efficient systems to salvage preformed nucleotides.[82][83] Purines are synthesized as nucleosides (bases attached to ribose).[84] Both adenine and guanine are made from the precursor nucleoside inosine monophosphate, which is synthesized using atoms from the amino acids glycine, glutamine, and aspartic acid, as well as formate transferred from the coenzyme tetrahydrofolate. Pyrimidines, on the other hand, are synthesized from the base orotate, which is formed from glutamine and aspartate.[85]		All organisms are constantly exposed to compounds that they cannot use as foods and would be harmful if they accumulated in cells, as they have no metabolic function. These potentially damaging compounds are called xenobiotics.[86] Xenobiotics such as synthetic drugs, natural poisons and antibiotics are detoxified by a set of xenobiotic-metabolizing enzymes. In humans, these include cytochrome P450 oxidases,[87] UDP-glucuronosyltransferases,[88] and glutathione S-transferases.[89] This system of enzymes acts in three stages to firstly oxidize the xenobiotic (phase I) and then conjugate water-soluble groups onto the molecule (phase II). The modified water-soluble xenobiotic can then be pumped out of cells and in multicellular organisms may be further metabolized before being excreted (phase III). In ecology, these reactions are particularly important in microbial biodegradation of pollutants and the bioremediation of contaminated land and oil spills.[90] Many of these microbial reactions are shared with multicellular organisms, but due to the incredible diversity of types of microbes these organisms are able to deal with a far wider range of xenobiotics than multicellular organisms, and can degrade even persistent organic pollutants such as organochloride compounds.[91]		A related problem for aerobic organisms is oxidative stress.[92] Here, processes including oxidative phosphorylation and the formation of disulfide bonds during protein folding produce reactive oxygen species such as hydrogen peroxide.[93] These damaging oxidants are removed by antioxidant metabolites such as glutathione and enzymes such as catalases and peroxidases.[94][95]		Living organisms must obey the laws of thermodynamics, which describe the transfer of heat and work. The second law of thermodynamics states that in any closed system, the amount of entropy (disorder) cannot decrease. Although living organisms' amazing complexity appears to contradict this law, life is possible as all organisms are open systems that exchange matter and energy with their surroundings. Thus living systems are not in equilibrium, but instead are dissipative systems that maintain their state of high complexity by causing a larger increase in the entropy of their environments.[96] The metabolism of a cell achieves this by coupling the spontaneous processes of catabolism to the non-spontaneous processes of anabolism. In thermodynamic terms, metabolism maintains order by creating disorder.[97]		As the environments of most organisms are constantly changing, the reactions of metabolism must be finely regulated to maintain a constant set of conditions within cells, a condition called homeostasis.[98][99] Metabolic regulation also allows organisms to respond to signals and interact actively with their environments.[100] Two closely linked concepts are important for understanding how metabolic pathways are controlled. Firstly, the regulation of an enzyme in a pathway is how its activity is increased and decreased in response to signals. Secondly, the control exerted by this enzyme is the effect that these changes in its activity have on the overall rate of the pathway (the flux through the pathway).[101] For example, an enzyme may show large changes in activity (i.e. it is highly regulated) but if these changes have little effect on the flux of a metabolic pathway, then this enzyme is not involved in the control of the pathway.[102]		There are multiple levels of metabolic regulation. In intrinsic regulation, the metabolic pathway self-regulates to respond to changes in the levels of substrates or products; for example, a decrease in the amount of product can increase the flux through the pathway to compensate.[101] This type of regulation often involves allosteric regulation of the activities of multiple enzymes in the pathway.[103] Extrinsic control involves a cell in a multicellular organism changing its metabolism in response to signals from other cells. These signals are usually in the form of soluble messengers such as hormones and growth factors and are detected by specific receptors on the cell surface.[104] These signals are then transmitted inside the cell by second messenger systems that often involved the phosphorylation of proteins.[105]		A very well understood example of extrinsic control is the regulation of glucose metabolism by the hormone insulin.[106] Insulin is produced in response to rises in blood glucose levels. Binding of the hormone to insulin receptors on cells then activates a cascade of protein kinases that cause the cells to take up glucose and convert it into storage molecules such as fatty acids and glycogen.[107] The metabolism of glycogen is controlled by activity of phosphorylase, the enzyme that breaks down glycogen, and glycogen synthase, the enzyme that makes it. These enzymes are regulated in a reciprocal fashion, with phosphorylation inhibiting glycogen synthase, but activating phosphorylase. Insulin causes glycogen synthesis by activating protein phosphatases and producing a decrease in the phosphorylation of these enzymes.[108]		The central pathways of metabolism described above, such as glycolysis and the citric acid cycle, are present in all three domains of living things and were present in the last universal common ancestor.[3][109] This universal ancestral cell was prokaryotic and probably a methanogen that had extensive amino acid, nucleotide, carbohydrate and lipid metabolism.[110][111] The retention of these ancient pathways during later evolution may be the result of these reactions having been an optimal solution to their particular metabolic problems, with pathways such as glycolysis and the citric acid cycle producing their end products highly efficiently and in a minimal number of steps.[4][5] The first pathways of enzyme-based metabolism may have been parts of purine nucleotide metabolism, while previous metabolic pathways were a part of the ancient RNA world.[112]		Many models have been proposed to describe the mechanisms by which novel metabolic pathways evolve. These include the sequential addition of novel enzymes to a short ancestral pathway, the duplication and then divergence of entire pathways as well as the recruitment of pre-existing enzymes and their assembly into a novel reaction pathway.[113] The relative importance of these mechanisms is unclear, but genomic studies have shown that enzymes in a pathway are likely to have a shared ancestry, suggesting that many pathways have evolved in a step-by-step fashion with novel functions created from pre-existing steps in the pathway.[114] An alternative model comes from studies that trace the evolution of proteins' structures in metabolic networks, this has suggested that enzymes are pervasively recruited, borrowing enzymes to perform similar functions in different metabolic pathways (evident in the MANET database)[115] These recruitment processes result in an evolutionary enzymatic mosaic.[116] A third possibility is that some parts of metabolism might exist as "modules" that can be reused in different pathways and perform similar functions on different molecules.[117]		As well as the evolution of new metabolic pathways, evolution can also cause the loss of metabolic functions. For example, in some parasites metabolic processes that are not essential for survival are lost and preformed amino acids, nucleotides and carbohydrates may instead be scavenged from the host.[118] Similar reduced metabolic capabilities are seen in endosymbiotic organisms.[119]		Classically, metabolism is studied by a reductionist approach that focuses on a single metabolic pathway. Particularly valuable is the use of radioactive tracers at the whole-organism, tissue and cellular levels, which define the paths from precursors to final products by identifying radioactively labelled intermediates and products.[120] The enzymes that catalyze these chemical reactions can then be purified and their kinetics and responses to inhibitors investigated. A parallel approach is to identify the small molecules in a cell or tissue; the complete set of these molecules is called the metabolome. Overall, these studies give a good view of the structure and function of simple metabolic pathways, but are inadequate when applied to more complex systems such as the metabolism of a complete cell.[121]		An idea of the complexity of the metabolic networks in cells that contain thousands of different enzymes is given by the figure showing the interactions between just 43 proteins and 40 metabolites to the right: the sequences of genomes provide lists containing anything up to 45,000 genes.[122] However, it is now possible to use this genomic data to reconstruct complete networks of biochemical reactions and produce more holistic mathematical models that may explain and predict their behavior.[123] These models are especially powerful when used to integrate the pathway and metabolite data obtained through classical methods with data on gene expression from proteomic and DNA microarray studies.[124] Using these techniques, a model of human metabolism has now been produced, which will guide future drug discovery and biochemical research.[125] These models are now used in network analysis, to classify human diseases into groups that share common proteins or metabolites.[126][127]		Bacterial metabolic networks are a striking example of bow-tie[128][129][130] organization, an architecture able to input a wide range of nutrients and produce a large variety of products and complex macromolecules using a relatively few intermediate common currencies.		A major technological application of this information is metabolic engineering. Here, organisms such as yeast, plants or bacteria are genetically modified to make them more useful in biotechnology and aid the production of drugs such as antibiotics or industrial chemicals such as 1,3-propanediol and shikimic acid.[131] These genetic modifications usually aim to reduce the amount of energy used to produce the product, increase yields and reduce the production of wastes.[132]		The term metabolism is derived from the Greek Μεταβολισμός – "Metabolismos" for "change", or "overthrow".[133]		Aristotle's The Parts of Animals sets out enough details of his views on metabolism for an open flow model to be made. He believed that at each stage of the process, materials from food were transformed, with heat being released as the classical element of fire, and residual materials being excreted as urine, bile, or faeces.[134]		Ibn al-Nafis described metabolism in his 1260 AD work titled Al-Risalah al-Kamiliyyah fil Siera al-Nabawiyyah (The Treatise of Kamil on the Prophet's Biography) which included the following phrase "Both the body and its parts are in a continuous state of dissolution and nourishment, so they are inevitably undergoing permanent change."[135] The history of the scientific study of metabolism spans several centuries and has moved from examining whole animals in early studies, to examining individual metabolic reactions in modern biochemistry. The first controlled experiments in human metabolism were published by Santorio Santorio in 1614 in his book Ars de statica medicina.[136] He described how he weighed himself before and after eating, sleep, working, sex, fasting, drinking, and excreting. He found that most of the food he took in was lost through what he called "insensible perspiration".		In these early studies, the mechanisms of these metabolic processes had not been identified and a vital force was thought to animate living tissue.[137] In the 19th century, when studying the fermentation of sugar to alcohol by yeast, Louis Pasteur concluded that fermentation was catalyzed by substances within the yeast cells he called "ferments". He wrote that "alcoholic fermentation is an act correlated with the life and organization of the yeast cells, not with the death or putrefaction of the cells."[138] This discovery, along with the publication by Friedrich Wöhler in 1828 of a paper on the chemical synthesis of urea,[139] and is notable for being the first organic compound prepared from wholly inorganic precursors. This proved that the organic compounds and chemical reactions found in cells were no different in principle than any other part of chemistry.		It was the discovery of enzymes at the beginning of the 20th century by Eduard Buchner that separated the study of the chemical reactions of metabolism from the biological study of cells, and marked the beginnings of biochemistry.[140] The mass of biochemical knowledge grew rapidly throughout the early 20th century. One of the most prolific of these modern biochemists was Hans Krebs who made huge contributions to the study of metabolism.[141] He discovered the urea cycle and later, working with Hans Kornberg, the citric acid cycle and the glyoxylate cycle.[142][65] Modern biochemical research has been greatly aided by the development of new techniques such as chromatography, X-ray diffraction, NMR spectroscopy, radioisotopic labelling, electron microscopy and molecular dynamics simulations. These techniques have allowed the discovery and detailed analysis of the many molecules and metabolic pathways in cells.		Introductory		Advanced		General information		Human metabolism		Databases		Metabolic pathways						
Ramadan (/ˌræməˈdɑːn/; Arabic: رمضان‎‎ Ramaḍān, IPA: [ramaˈdˤaːn];[note 1] also romanized as Ramazan, Ramadhan, or Ramathan) is the ninth month of the Islamic calendar,[3] and is observed by Muslims worldwide as a month of fasting (Sawm) to commemorate the first revelation of the Quran to Muhammad according to Islamic belief.[4][5] This annual observance is regarded as one of the Five Pillars of Islam.[6] The month lasts 29–30 days based on the visual sightings of the crescent moon, according to numerous biographical accounts compiled in the hadiths.[7][8]		The word Ramadan comes from the Arabic root ramiḍa or ar-ramaḍ, which means scorching heat or dryness.[9] Fasting is fardh (obligatory) for adult Muslims, except those who are suffering from an illness, travelling, are elderly, pregnant, breastfeeding, diabetic, chronically ill or menstruating.[10] Fasting the month of Ramadan was made obligatory (wājib) during the month of Sha'ban, in the second year after the Muslims migrated from Mecca to Medina. Fatwas have been issued declaring that Muslims who live in regions with a natural phenomenon such as the midnight sun or polar night should follow the timetable of Mecca,[11] but the more commonly accepted opinion is that Muslims in those areas should follow the timetable of the closest country to them in which night can be distinguished from day.[12][13][14]		While fasting from dawn until sunset, Muslims refrain from consuming food, drinking liquids, smoking, and engaging in sexual relations. Muslims are also instructed to refrain from sinful behavior that may negate the reward of fasting, such as false speech (insulting, backbiting, cursing, lying, etc.) and fighting except in self-defense.[15][16] Food and drinks are served daily, before dawn and after sunset, referred to as Suhoor and Iftar respectively.[17][18] Spiritual rewards (thawab) for fasting are also believed to be multiplied within the month of Ramadan.[19] Fasting for Muslims during Ramadan typically includes the increased offering of salat (prayers), recitation of the Quran[20][21] and an increase of doing good deeds and charity.						Chapter 2, Verse 185, of the Quran states:		The month of Ramadan is that in which was revealed the Quran; a guidance for mankind, and clear proofs of the guidance, and the criterion (of right and wrong). And whosoever of you is present, let him fast the month, and whosoever of you is sick or on a journey, a number of other days. Allah desires for you ease; He desires not hardship for you; and that you should complete the period, and that you should magnify Allah for having guided you, and that perhaps you may be thankful.[Quran 2:185]		It is believed that the Quran was first revealed to Muhammad during the month of Ramadan which has been referred to as the "best of times". The first revelation was sent down on Laylat al-Qadr (The night of Power) which is one of the five odd nights of the last ten days of Ramadan.[22] According to hadith, all holy scriptures were sent down during Ramadan. The tablets of Ibrahim, the Torah, the Psalms, the Gospel and the Quran were sent down on 1st, 6th, 12th, 13th[note 2] and 24th Ramadan respectively.[23]		According to the Quran, fasting was also obligatory for prior nations, and is a way to attain taqwa, fear of God.[24][Quran 2:183] God proclaimed to Muhammad that fasting for His sake was not a new innovation in monotheism, but rather an obligation practiced by those truly devoted to the oneness of God.[25] The pagans of Mecca also fasted, but only on tenth day of Muharram to expiate sins and avoid droughts.[26]		The ruling to observe fasting during Ramadan was sent down 18 months after Hijra, during the month of Sha'ban in the second year of Hijra in 624 CE.[23]		Abu Zanad, an Arabic writer from Iraq who lived after the founding of Islam, in around 747 CE, wrote that at least one Mandaean community located in al-Jazira (modern northern Iraq) observed Ramadan before converting to Islam.[27][not in citation given]		According to historian Philip Jenkins, Ramadan comes "from the strict Lenten discipline of the Syrian Churches", a postulation corroborated by other scholars, such as the theologian Paul-Gordon Chandler.[28][29] This suggestion is based on the orientalist idea that the Qur'an itself has Syriac Christian origins, a claim to which some Muslim academics such as M. Al-Azami, object.[30]		The beginning and end of Ramadan are determined by the lunar Islamic calendar.		Hilāl (the crescent) is typically a day (or more) after the astronomical new moon. Since the new moon marks the beginning of the new month, Muslims can usually safely estimate the beginning of Ramadan.[31] However, to many Muslims, this is not in accordance with authenticated Hadiths stating that visual confirmation per region is recommended. The consistent variations of a day have existed since the time of Muhammad.[32]		The Arabic Laylat al-Qadr, translated to English is "the night of power" or "the night of decree", is considered the holiest night of the year.[33][34] This is the night in which Muslims believe the first revelation of the Quran was sent down to Muhammad stating that this night was "better than one thousand months [of proper worship], as stated in Chapter 97:3 of the Qu'ran.		Also, generally, Laylat al-Qadr is believed to have occurred on an odd-numbered night during the last ten days of Ramadan, i.e., the night of the 21st, 23rd, 25th, 27th or 29th. The Dawoodi Bohra Community believe that the 23rd night is laylat al Qadr.[35] [36]		The holiday of Eid al-Fitr (Arabic:عيد الفطر) marks the end of Ramadan and the beginning of the next lunar month, Shawwal. This first day of the following month is declared after another crescent new moon has been sighted or the completion of 30 days of fasting if no visual sighting is possible due to weather conditions. This first day of Shawwal is called Eid al-Fitr. Eid al-Fitr may also be a reference towards the festive nature of having endured the month of fasting successfully and returning to the more natural disposition (fitra) of being able to eat, drink and resume intimacy with spouses during the day..[37]		The common practice during Ramadan is fasting from dawn to sunset. The pre-dawn meal before the fast is called the suhur, while the meal at sunset that breaks the fast is the iftar. Considering the high diversity of the global Muslim population, it is impossible to describe typical suhur or iftar meals.		Muslims also engage in increased prayer and charity during Ramadan. Ramadan is also a month where Muslims try to practice increased self-discipline. This is motivated by the Hadith, especially in Al-Bukhari[38] and Muslim,[39] that "When Ramadan arrives, the gates of Paradise are opened and the gates of hell are locked up and devils are put in chains."[40]		Ramadan is a time of spiritual reflection, improvement and increased devotion and worship. Muslims are expected to put more effort into following the teachings of Islam. The fast (sawm) begins at dawn and ends at sunset. In addition to abstaining from eating and drinking, Muslims also increase restraint, such as abstaining from sexual relations[2] and generally sinful speech and behavior. The act of fasting is said to redirect the heart away from worldly activities, its purpose being to cleanse the soul by freeing it from harmful impurities. Ramadan also teaches Muslims how to better practice self-discipline, self-control,[41] sacrifice, and empathy for those who are less fortunate; thus encouraging actions of generosity and compulsory charity (zakat).[42]		It becomes compulsory for Muslims to start fasting when they reach puberty, so long as they are healthy and sane, and have no disabilities or illnesses. Many children endeavour to complete as many fasts as possible as practice for later life.		Exemptions to fasting are travel, menstruation, severe illness, pregnancy, and breastfeeding. However, many Muslims with medical conditions insist on fasting to satisfy their spiritual needs, although it is not recommended by the hadith. Professionals should closely monitor such individuals who decide to persist with fasting.[43] Those who were unable to fast still must make up the days missed later.[44]		Each day, before dawn, Muslims observe a pre-fast meal called the suhur. After stopping a short time before dawn, Muslims begin the first prayer of the day, Fajr.[45][46] At sunset, families hasten for the fast-breaking meal known as iftar.		In the evening, dates are usually the first food to break the fast; according to tradition, Muhammad broke fast with three dates. Following that, Muslims generally adjourn for the Maghrib prayer, the fourth of the five daily prayers, after which the main meal is served.[47]		Social gatherings, many times in a buffet style, are frequent at iftar. Traditional dishes are often highlighted, including traditional desserts, and particularly those made only during Ramadan. Water is usually the beverage of choice, but juice and milk are also often available, as are soft drinks and caffeinated beverages.[43]		In the Middle East, the iftar meal consists of water, juices, dates, salads and appetizers, one or more main dishes, and various kinds of desserts. Usually, the dessert is the most important part during iftar. Typical main dishes are lamb stewed with wheat berries, lamb kebabs with grilled vegetables, or roast chicken served with chickpea-studded rice pilaf. A rich dessert, such as luqaimat, baklava or kunafeh (a buttery, syrup-sweetened kadaifi noodle pastry filled with cheese), concludes the meal.[48]		Over time, iftar has grown into banquet festivals. This is a time of fellowship with families, friends and surrounding communities, but may also occupy larger spaces at masjid or banquet halls for 100 or more diners.[49]		Charity is very important in Islam, and even more so during Ramadan. Zakāt, often translated as "the poor-rate", is obligatory as one of the pillars of Islam; a fixed percentage of the person's savings is required to be given to the poor. Sadaqah is voluntary charity in giving above and beyond what is required from the obligation of zakāt. In Islam, all good deeds are more handsomely rewarded during Ramadan than in any other month of the year. Consequently, many will choose this time to give a larger portion, if not all, of the zakāt that they are obligated to give. In addition, many will also use this time to give a larger portion of sadaqah in order to maximize the reward that will await them at the Last Judgment.[citation needed]		Tarawih (Arabic: تراويح‎‎) refers to extra prayers performed by Muslims at night in the Islamic month of Ramadan. Contrary to popular belief, they are not compulsory.[50] However, many Muslims pray these prayers in the evening during Ramadan. Some scholars[who?] maintain that Tarawih is neither fard or a Sunnah, but is the preponed Tahajjud (night prayer) prayer shifted to post-Isha' for the ease of believers. But a majority of Sunni scholars regard the Tarawih prayers as Sunnat al-Mu'akkadah, a salaat that was performed by the Islamic prophet Muhammad very consistently.		In addition to fasting, Muslims are encouraged to read the entire Quran. Some Muslims perform the recitation of the entire Quran by means of special prayers, called Tarawih. These voluntary prayers are held in the mosques every night of the month, during which a whole section of the Quran (juz', which is 1/30 of the Quran) is recited. Therefore, the entire Quran would be completed at the end of the month. Although it is not required to read the whole Quran in the Tarawih prayers, it is common.		In some Muslim countries today, lights are strung up in public squares, and across city streets, to add to the festivities of the month. Lanterns have become symbolic decorations welcoming the month of Ramadan. In a growing number of countries, they are hung on city streets.[51][52][53] The tradition of lanterns as a decoration becoming associated with Ramadan is believed to have originated during the Fatimid Caliphate primarily centered in Egypt, where Caliph al-Mu'izz li-Din Allah was greeted by people holding lanterns to celebrate his ruling. From that time, lanterns were used to light mosques and houses throughout the capital city of Cairo. Shopping malls, places of business, and people's homes can be seen with stars and crescents and various lighting effects, as well.		As the nation with the world's largest Muslim population, Indonesia has diverse Ramadan traditions. On the island of Java, many Javanese Indonesians bathe in holy springs to prepare for fasting, a ritual known as Padusan. The city of Semarang marks the beginning of Ramadan with the Dugderan carnival, which involves parading the Warak ngendog, a horse-dragon hybrid creature allegedly inspired by the Buraq. In the Chinese-influenced capital city of Jakarta, fire crackers were traditionally used to wake people up for morning prayer, until the 19th century. Towards the end of Ramadan, most employees receive a one-month bonus known as Tunjangan Hari Raya. Certain kinds of food are especially popular during Ramadan, such as beef in Aceh, and snails in Central Java. The iftar meal is announced every evening by striking the bedug, a giant drum, in the mosque.		Common greetings during Ramadan are "Ramadan Mubarak" or "Ramadan Kareem", which wish the recipient a blessed or generous Ramadan.[54]		In some Muslim countries, failing to fast during Ramadan is considered a crime and is prosecuted as such. For instance, in Algeria, in October 2008 the court of Biskra condemned six people to four years in prison and heavy fines.[55]		In Kuwait, according to law number 44 of 1968, the penalty is a fine of no more than 100 Kuwaiti dinars, (about US$330, GB£260 in May 2017) or jail for no more than one month, or both penalties, for those seen eating, drinking or smoking during Ramadan daytime.[56][57] In some places in the U.A.E., eating or drinking in public during the daytime of Ramadan is considered a minor offence and would be punished by up to 150 hours of community service.[58] In neighbouring Saudi Arabia, described by The Economist as taking Ramadan "more seriously than anywhere else",[59] there are harsher punishments including flogging, imprisonment and, for foreigners, deportation.[60] In Malaysia, however, there are no such punishments.		In 2014 in Kermanshah, Iran, a non-Muslim was sentenced to having his lips burnt with a cigarette and five Muslims were publicly flogged with 70 stripes for eating during Ramadan.[61][unreliable source?]		Some countries have laws that amend work schedules during Ramadan. Under U.A.E. labor law, the maximum working hours are to be 6 hours per day and 36 hours per week. Qatar, Oman, Bahrain and Kuwait have similar laws.[62]		In Egypt, alcohol sales are banned during Ramadan.[63]		Schools all over the world criticize Ramadan. Not eating or drinking can lead to concentration problems and bad grades.[64][65]		Ramadan fasting is safe for healthy people, but those with medical conditions should seek medical advice.[66] The fasting period is usually associated with modest weight loss, but the weight tends to return afterwards.[67]		A review of the literature by an Iranian group suggested fasting during Ramadan might produce renal injury in patients with moderate (GFR <60 ml/min) or worse kidney disease, but was not injurious to renal transplant patients with good function or most stone-forming patients.[68]		The correlation of Ramadan with crime rates is mixed: some statistics show that crime rates drop during Ramadan, while others show that it rises. Decreases in crime rates have been reported by the police in some cities in Turkey (Istanbul[69] and Konya[70]) and the Eastern province of Saudi Arabia.[71] A 2012 study showed that crime rates decreased in Iran during Ramadan, and that the decrease was statistically significant.[72] A 2005 study found that there was a decrease in assault, robbery and alcohol-related crimes during Ramadan in Saudi Arabia, but only the decrease in alcohol-related crimes was statistically significant.[73] Increases in crime rates during Ramadan have been reported in Turkey,[74] Jakarta,[75][76][77] parts of Algeria,[78] Yemen[79] and Egypt.[80]		Various mechanisms have been proposed for the effect of Ramadan on crime:		The length of the dawn to sunset time varies in different parts of the world according to summer or winter solstices of the sun. Most Muslims fast for 11–16 hours during Ramadan. However, in polar regions, the period between dawn and sunset may exceed 22 hours in summers. For example, in 2014, Muslims in Reykjavik, Iceland, and Trondheim, Norway, fasted almost 22 hours, while Muslims in Sydney, Australia, fasted for only about 11 hours. Muslims in areas where continuous night or day is observed during Ramadan follow the fasting hours in the nearest city where fasting is observed at dawn and sunset. Alternatively, Muslims may follow Mecca time.[12][13][14]		Muslims will continue to work during Ramadan. The prophet Muhammad said that it is important to keep a balance between worship and work. In some Muslim countries, such as Oman, however, working hours are shortened during Ramadan.[82][83] It is often recommended that working Muslims inform their employers if they are fasting, given the potential for the observance to impact performance at work.[84] The extent to which Ramadan observers are protected by religious accommodation varies by country. Policies putting them at a disadvantage compared to other employees have been met with discrimination claims in the UK and the US.[85][86][87]		(federal) = federal holidays, (state) = state holidays, (religious) = religious holidays, (week) = weeklong holidays, (month) = monthlong holidays, (36) = Title 36 Observances and Ceremonies Bold indicates major holidays commonly celebrated in the United States, which often represent the major celebrations of the month.		
Blue-plate special or blue plate special is a term used in the United States by restaurants, especially diners and cafes. It refers to a low-priced meal that usually changes daily.		The term was very common from the 1920s through the 1950s. As of 2007[update], there are still a few restaurants and diners that offer blue-plate specials under that name, sometimes on blue plates, but it is a vanishing tradition. The phrase itself, however, is still a common American colloquialism.[citation needed]		A web collection of 1930s prose gives this definition: "A Blue Plate Special is a low-priced daily diner special: a main course with all the fixins, a daily combo, a square for two bits."[1]						The origin and explanation of the phrase are unclear. Kevin Reed says that "during the Depression, a manufacturer started making plates with separate sections for each part of a meal—like a frozen dinner tray—it seems that for whatever reason they were only available in the color blue." Michael Quinion cites a dictionary entry indicating that the blue plates were, more specifically, inexpensive divided plates that were decorated with a "blue willow" or similar blue pattern, such as those popularized by Spode and Wedgwood. One of his correspondents says that the first known use of the term is on an October 22, 1892 Fred Harvey Company restaurant menu and implies that blue-plate specials were regular features at Harvey Houses.[2]		The term became common starting in the late 1920s. A May 27, 1926 advertisement in The New York Times for "The Famous Old Sea Grill Lobster and Chop House" at 141 West 45th Street promised "A La Carte All Hours", "Moderate Prices", and "Blue Plate Specials". A December 2, 1928 article, lamenting the rise in prices that had made it difficult to "dine on a dime," praised an Ann Street establishment where one could still get "a steak-and-lots-of-onion sandwich for a dime" and a "big blue-plate special, with meat course and three vegetables, is purchasable for a quarter, just as it has been for the last ten years." The first book publication of Damon Runyon's story, "Little Miss Marker," was in a 1934 collection entitled Damon Runyon's Blue Plate Special.[citation needed] A Hollywood columnist wrote in 1940, "Every time Spencer Tracy enters the Metro commissary, executives and minor geniuses look up from their blue plate specials to look at the actor and marvel." In the 1953 The Honeymooners episode "Suspense," Ralph, suspecting that Alice plans to murder him with a carving knife, says to Norton, "Did you hear that, pal? She wants to borrow a carving knife. I never thought I'd end up a blue-plate special."		"No substitutions" was a common policy on blue-plate specials. One 1947 Candid Microphone episode features Allen Funt ordering a blue-plate special and trying to talk the waiter into making various changes, such as replacing the vegetable soup with consommé, while the polite but increasingly annoyed waiter tries in vain to explain to Funt that "no substitutions" means what it says. Our Man in Havana (1958) by Graham Greene has the following exchange regarding an "American blue-plate lunch":		"Surely you know what a blue-plate is, man? They shove the whole meal at you under your nose, already dished up on your plate  – roast turkey, cranberry sauce, sausages and carrots and French Fried. I can't bear French fried but there's no pick and choose with a blue-plate." "No pick and choose?" "You eat what you're given. That's democracy, man."		In contemporary usage, a "blue-plate special" can be any inexpensive full meal, any daily selection, or merely a whimsical phrasing. Travel columnist Wayne Curtis says that a Portland, Maine eatery offers "budget blue-plate specials along with more refined fare."[3]		Boston Children's Museum presents a participatory-theater show, sponsored by health insurer Blue Cross, which teaches good nutrition; the show is called Blue Plate Special.[4]		Workman's Blue Plate Special is "a monthly eCookbook Club, bringing you specially discounted and free eBooks from an award-winning collection of cookbooks".[5]		The WDVX Blue Plate Special is an almost-daily lunch-time concert at the Knoxville Visitor's Center, broadcasting on the East Tennessee radio station WDVX.[6]		The Turner South cable channel calls a daily movie selection, scheduled at lunchtime, its "blue-plate special".[citation needed] In the 1973 film 'The Sting', Robert Redford's character Johnny "Kelly" Hooker orders the Blue-Plate Special from (Unknown to him) the Assassin Salino.		Richard Bernstein titled his New York Times review of Andrew Hurley's book Diners, Bowling Alleys, and Trailer Parks (2001),[7] "The Red, White and Blue Plate Special".[8]		Mystery writer Abigail Padgett's second novel about amateur sleuth Blue McCarron is titled The Last Blue Plate Special (2001); no meals here, the blue plates are part of the decor at a clinic where patients are dying mysteriously.[9]		Road food experts Jane and Michael Stern titled their guidebook Blue Plate Specials and Blue Ribbon Chefs: The Heart And Soul of America's Great Roadside Restaurants (2001).[10]		
2JXZ,%%s1LS7		796		n/a		ENSG00000110680		n/a		P01258 P06881		n/a		NM_001033952 NM_001033953 NM_001741		n/a		NP_001029124.1 NP_001732.1 NP_001029125.1		n/a		Calcitonin (also known as thyrocalcitonin) is a 32-amino acid linear polypeptide hormone that is produced in humans primarily by the parafollicular cells (also known as C-cells) of the thyroid, and in many other animals in the ultimopharyngeal body.[2] It acts to reduce blood calcium (Ca2+), opposing the effects of parathyroid hormone (PTH).[3]		Calcitonin has been found in fish, reptiles, birds, and mammals. Its importance in humans has not been as well established as its importance in other animals, as its function is usually not significant in the regulation of normal calcium homeostasis.[4] It belongs to the calcitonin-like protein family.						Calcitonin is formed by the proteolytic cleavage of a larger prepropeptide, which is the product of the CALC1 gene (CALCA). The CALC1 gene belongs to a superfamily of related protein hormone precursors including islet amyloid precursor protein, calcitonin gene-related peptide, and the precursor of adrenomedullin.		Secretion of calcitonin is stimulated by:		The hormone participates in calcium (Ca2+) and phosphorus metabolism. In many ways, calcitonin counteracts parathyroid hormone (PTH).		More specifically, calcitonin lowers blood Ca2+ levels in two ways:		High concentrations of calcitonin may be able to increase urinary excretion of calcium and phosphate, via actions on the kidney tubules.[10] However, this is a minor effect with no physiological significance in humans. It is also a short-lived effect because the kidneys become resistant to calcitonin, as demonstrated by the kidney's unaffected excretion of calcium in patients with thyroid tumors that secrete excessive calcitonin.[11]		In its skeleton-preserving actions, calcitonin protects against calcium loss from skeleton during periods of calcium mobilization, such as pregnancy and, especially, lactation. The protective mechanisms include the direct inhibition of bone resorption and the indirect effect through the inhibition of the release of prolactin from the pituitary gland. The reason provided is that prolactin induces the release of PTH related peptide which enhances bone resorption, but is still under investigation., [12][13][14]		Other effects are in preventing postprandial hypercalcemia resulting from absorption of Ca2+. Also, calcitonin inhibits food intake in rats and monkeys, and may have CNS action involving the regulation of feeding and appetite.		Calcitonin lowers blood calcium and phosphorus mainly through its inhibition of osteoclasts. Osteoblasts do not have calcitonin receptors and are therefore not directly affected by calcitonin levels. However, since bone resorption and bone formation are coupled processes, eventually calcitonin's inhibition of osteoclastic activity leads to decreased osteoblastic activity (as an indirect effect).[11]		The calcitonin receptor, found on osteoclasts,[15] and in the kidney and regions of the brain, is a G protein-coupled receptor, which is coupled by Gs to adenylate cyclase and thereby to the generation of cAMP in target cells. It may also affect the ovaries in women and the testes in men.		Calcitonin was purified in 1962 by Copp and Cheney.[16] While it was initially considered a secretion of the parathyroid glands, it was later identified as the secretion of the C-cells of the thyroid gland.[17]		A malignancy of the parafollicular cells, i.e. Medullary thyroid cancer, typically produces an elevated serum calcitonin level.		Salmon calcitonin is used for the treatment of:		It has been investigated as a possible non-operative treatment for spinal stenosis.[19]		The following information is from the UK Electronic Medicines Compendium[20]		Salmon calcitonin is rapidly absorbed and eliminated. Peak plasma concentrations are attained within the first hour of administration.		Animal studies have shown that calcitonin is primarily metabolised via proteolysis in the kidney following parenteral administration. The metabolites lack the specific biological activity of calcitonin. Bioavailability following subcutaneous and intramuscular injection in humans is high and similar for the two routes of administration (71% and 66%, respectively).		Calcitonin has short absorption and elimination half-lives of 10–15 minutes and 50–80 minutes, respectively. Salmon calcitonin is primarily and almost exclusively degraded in the kidneys, forming pharmacologically inactive fragments of the molecule. Therefore, the metabolic clearance is much lower in patients with end-stage renal failure than in healthy subjects. However, the clinical relevance of this finding is not known. Plasma protein binding is 30% to 40%.		There is a relationship between the subcutaneous dose of calcitonin and peak plasma concentrations. Following parenteral administration of 100 IU calcitonin, peak plasma concentration lies between about 200 and 400 pg/ml. Higher blood levels may be associated with increased incidence of nausea, vomiting, and secretory diarrhea.		Conventional long-term toxicity, reproduction, mutagenicity, and carcinogenicity studies have been performed in laboratory animals. Salmon calcitonin is devoid of embryotoxic, teratogenic, and mutagenic potential.		An increased incidence of pituitary adenomas has been reported in rats given synthetic salmon calcitonin for 1 year. This is considered a species-specific effect and of no clinical relevance.[21] Salmon calcitonin does not cross the placental barrier.		In lactating animals given calcitonin, suppression of milk production has been observed. Calcitonin is secreted into the milk.		Calcitonin was extracted from the ultimobranchial glands (thyroid-like glands) of fish, particularly salmon. Salmon calcitonin resembles human calcitonin, but is more active. At present, it is produced either by recombinant DNA technology or by chemical peptide synthesis. The pharmacological properties of the synthetic and recombinant peptides have been demonstrated to be qualitatively and quantitatively equivalent.[20]		Calcitonin can be used therapeutically for the treatment of hypercalcemia or osteoporosis.[22] In a recent clinical study, subcutaneous injections of calcitonin have reduced the incidence of fractures and reduced the decrease in bone mass in women with type 2 diabetes complicated with osteoporosis.[23]		Subcutaneous injections of calcitonin in patients suffering from mania resulted in significant decreases in irritability, euphoria and hyperactivity and hence calcitonin holds promise for treating bipolar disorder.[24] However no further work on this potential application of calcitonin has been reported.		It may be used diagnostically as a tumor marker for medullary thyroid cancer, in which high calcitonin levels may be present and elevated levels after surgery may indicate recurrence. It may even be used on biopsy samples from suspicious lesions (e.g., lymph nodes that are swollen) to establish whether they are metastases of the original cancer.		Cutoffs for calcitonin to distinguish cases with medullary thyroid cancer have been suggested to be as follows, with a higher value increasing the suspicion of medullary thyroid cancer:[25]		When over 3 years of age, adult cutoffs may be used		Increased levels of calcitonin have also been reported for various other conditions. They include: C-cell hyperplasia, Nonthyroidal oat cell carcinoma, Nonthyroidal small cell carcinoma and other nonthyroidal malignancies, acute and chronic renal failure, hypercalcemia, hypergastrinemia and other gastrointestinal disorders, and pulmonary disease.[26]		Calcitonin is a polypeptide hormone of 32 amino acids, with a molecular weight of 3454.93 daltons. Its structure comprises a single alpha helix.[27] Alternative splicing of the gene coding for calcitonin produces a distantly related peptide of 37 amino acids, called calcitonin gene-related peptide (CGRP), beta type.[28]		The following are the amino acid sequences of salmon and human calcitonin:[citation needed]		Compared to salmon calcitonin, human calcitonin differs at 16 residues.		
Mastication or chewing is the process by which food is crushed and ground by teeth. It is the first step of digestion, and it increases the surface area of foods to allow a more efficient break down by enzymes. During the mastication process, the food is positioned by the cheek and tongue between the teeth for grinding. The muscles of mastication move the jaws to bring the teeth into intermittent contact, repeatedly occluding and opening. As chewing continues, the food is made softer and warmer, and the enzymes in saliva begin to break down carbohydrates in the food. After chewing, the food (now called a bolus) is swallowed. It enters the esophagus and via peristalsis continues on to the stomach, where the next step of digestion occurs.[1]		Premastication is sometimes performed by human parents for infants who are unable to do so for themselves. The food is masticated in the mouth of the parent into a bolus and then transferred to the infant for consumption.[2] (Some other animals also premasticate.)		Cattle and some other animals, called ruminants, chew food more than once to extract more nutrients. After the first round of chewing, this food is called cud.						Mastication is primarily an unconscious (semi-autonomic) act, but can be mediated by higher conscious input. The motor program for mastication is a hypothesized central nervous system function by which the complex patterns governing mastication are created and controlled.		It is thought that feedback from proprioceptive nerves in teeth and the temporomandibular joints govern the creation of neural pathways, which in turn determine duration and force of individual muscle activation (and in some cases muscle fiber groups as in the masseter and temporalis).		This motor program continuously adapts to changes in food type or occlusion.[3] This adaptation is a learned skill that may sometimes require relearning to adapt to loss of teeth or to dental appliances such as dentures.		It is thought that conscious mediation is important in the limitation of parafunctional habits as most commonly, the motor program can be excessively engaged during periods of sleep and times of stress. It is also theorized that excessive input to the motor program from myofascial pain or occlusal imbalance can contribute to parafunctional habits.		A study found that unchewed meat and vegetables were not digested, while tallow, cheese, fish, eggs, and grains did not need to be chewed.[4] Mastication stimulates saliva production and increases sensory perception of the food being eaten, controlling when the food is swallowed.[5] Avoiding mastication, by choice or due to medical reasons as tooth loss, is known as a soft diet. Such a diet may lead to inadequate nutrition due to a reduction in fruit and vegetable intake.[6]		Mastication also stimulates the hippocampus and is necessary to maintain its normal function.[7]		Chewing is largely an adaptation for mammalian herbivory. Carnivores generally chew very little or swallow their food whole or in chunks. This act of gulping food (or medicine pills) without chewing has inspired the English idiom "wolfing it down".		Ornithopods, a group of dinosaurs including the Hadrosaurids ("duck-bills"), developed teeth analogous to mammalian molars and incisors during the Cretaceous period; this advanced, cow-like dentition allowed the creatures to obtain more nutrients from the tough plant life. This may have given them the advantage needed to usurp the formidable sauropods, who depended on gastroliths for grinding food, from their ecological niches. They eventually became some of the most successful animals on the planet until the Cretaceous–Paleogene extinction event wiped them out.		The process of mastication has, by analogy, been applied to machinery. The U.S. Forest Service uses a machine called a masticator (also called a forestry mulching machine) to "chew" through brush and timber in order to clear firelines in advance of a wildfire.[8]		
An airline meal, airline food, plane food or in-flight meal is a meal served to passengers on board a commercial airliner. These meals are prepared by specialist airline catering services.		These meals vary widely in quality and quantity across different airline companies and classes of travel. They range from a simple snack or beverage in short-haul economy class to a seven-course gourmet meal in a first class long-haul flight. When ticket prices were regulated in the American domestic market, food was the primary means airlines differentiated themselves.[1]						The first airline meals were served by Handley Page Transport, an airline company founded in 1919, to serve the London–Paris route in October of that year.[2] Passengers could choose from a selection of sandwiches and fruit.[3]		The type of food varies depending upon the airline company and class of travel. Meals may be served on one tray or in multiple courses with no tray and with a tablecloth, metal cutlery, and glassware (generally in first and business classes). Often the food is reflective of the culture of the country the airline is based in.		The airline dinner typically includes meat (most commonly chicken or beef), fish, or pasta; a salad or vegetable; a small bread roll; and a dessert. Condiments (typically salt, pepper, and sugar) are supplied in small sachets or shakers.		Caterers usually produce alternative meals for passengers with restrictive diets. These must usually be ordered in advance, sometimes when buying the ticket. Some of the more common examples include:		For several Islamic airlines (e.g. EgyptAir, Emirates, Etihad Airways, Garuda Indonesia, Batik Air, Malindo Air, Gulf Air, Iran Air, Mahan Air, Iran Aseman Airlines, Oman Air, Yemenia, Kuwait Airways, Iraqi Airways, Qatar Airways, Saudia, Biman Bangladesh Airlines, Malaysia Airlines, Royal Brunei Airlines, Royal Air Maroc, Libyan Airlines, Afriqiyah Airways, Tunisair, Air Algérie and Turkish Airlines), in accordance with Islamic customs, all classes and dishes on the plane are served a Muslim meal with Halal certification – without pork and alcohol. While Emirates, Etihad, and Qatar are still providing bottles of wine to non-Muslim passengers, the cabin crew does not deliver alcoholic beverages lest to violate Islamic customs, unless those non-Muslim passengers request it. Because Iran and Saudi Arabia apply strict Sharia regulations, those countries' airlines do not deliver pork or alcoholic beverages, and all airlines flying to or from Iran or Saudi Arabia are prohibited from serving either.[citation needed] However, Garuda Indonesia is still serving alcoholic beverages (whiskey, beer, champagne and wine) to non-Muslim passengers.		In the case the Israeli airlines El Al, Arkia and Israir, all meals served are kosher-certified by Rabbis. Even destinations outside Israel, sky chefs must be supervised by rabbis to make kosher meals and load their planes.[citation needed]		Before the September 11 attacks in 2001, first class passengers were often provided with full sets of metal cutlery. Afterward, common household items were evaluated more closely for their potential use as weapons on aircraft, and both first class and coach class passengers were restricted to plastic utensils. Some airlines switched from metal to all-plastic or plastic-handled cutlery during the SARS outbreak in 2003, since the SARS virus transfers from person to person easily, and plastic cutlery can be thrown away after use. Many airlines later switched back to metal cutlery. However, Singapore Airlines continue to use metal utensils even in economy class as of 2017.		In May 2010, concerns were raised in Australia and New Zealand over their respective flag carriers, Qantas and Air New Zealand, reusing their plastic cutlery for international flights between 10 and 30 times before replacement. Both airlines cited cost saving, international quarantine, and environmental as the reasons for the choice. Both have also said that the plastic cutlery is commercially washed and sterilized before reuse.[4][5][6] Reusing plastic tablewares though is a regular practice among many airliners and food caterers.		For cleanliness, most meals come with a napkin and a moist towelette. First and business class passengers are often provided with hot towels.		During morning flights a cooked breakfast or smaller continental-style may be served. On long haul flights (and short/medium haul flights within Asia) breakfast normally includes an entrée of pancakes or eggs, traditional fried breakfast foods such as sausages and grilled tomatoes, and often muffins or pastries, fruits and breakfast cereal on the side. On shorter flights a continental-style breakfast, generally including a miniature box of breakfast cereal, fruits and either a muffin, pastry, or bagel. Coffee and tea are offered as well, and sometimes hot chocolate.		Food on board a flight is usually free on full-service Asian airlines and on almost all long-distance flights, while they might cost extra on low-cost airlines or European full-service airline flights. Quality may also fluctuate due to shifts in the economics of the airline industry.		On long-haul international flights in first class and business class, most Asian and European airlines serve gourmet meals, while legacy carriers based in the US tend to serve multicourse meals including a cocktail snack, appetizer, soup, salad, entrée (chicken, beef, fish, or pasta), cheeses with fruit, and ice cream. Some long-haul flights in first and business class offer such delicacies as caviar, champagne, and sorbet (intermezzo).		The cost and availability of meals on US airlines has changed considerably in recent years, as financial pressures have forced some airlines to either begin charging for meals, or abandon them altogether in favor of small snacks, as in the case of Southwest Airlines. Eliminating free pretzels saved Northwest $2 million annually.[citation needed] Nowadays, the main US legacy carriers (American, Delta and United) have discontinued full meal service in economy class on short-haul US domestic and North American flights, while retaining it on most intercontinental routes;[7][8][9] and at least one European carrier, Icelandair, follows this policy on intercontinental runs as well.[10]		As of 2016, all 4 major U.S. legacy airlines now offer free snacks on board in economy class. United Airlines re-introduced free snacks in February 2016.[11] Starting in April 2016, American Airlines will fully restore free snacks on all domestic flights in economy class.[12] Free meals will also be available on certain domestic routes.[13] Delta and Southwest have already been offering free snacks for years.[14]		Hawaiian Airlines is the only remaining major US airline that offers complimentary in–flight meals on its domestic flights.		Air China has reported that each domestic flight's meal requires RMB50 (US$7.30) while international flights require RMB70 (US$10).[15] However, this figure varies from airline to airline, as some have reported costs to be as low as US$3.50.[16] Air China is also minimizing costs by loading only 95% of all meals to reduce leftovers and storing non-perishable foods for emergencies.		In 1958 Pan Am and several European airlines entered into a legal dispute over whether certain airline food sandwiches counted as a "meal".[17]		Meals must generally be prepared on the ground before takeoff. Guillaume de Syon, a history professor at Albright College who wrote about the history of airline meals,[18] said that the higher altitudes alter the taste of the food and the function of the taste buds (although that's not case on Dreamliner or A350); according to de Syon the food may taste "dry and flavorless" as a result of the pressurization and passengers, feeling thirsty due to pressurization, many drink alcohol when they ought to drink water.[19] Tests have shown that the perception of saltiness and sweetness drops 30% at high altitudes. The low humidity in airline cabins also dries out the nose which decreases olfactory sensors which are essential for tasting flavor in dishes.[20]		Food safety is paramount in the airline catering industry. A case of mass food poisoning amongst the passengers on an airliner could have disastrous consequences. For example, on February 20, 1992, shrimp tainted with cholera was served on Aerolíneas Argentinas Flight 386. An elderly passenger died and other passengers fell ill. For this reason catering firms and airlines have worked together to provide a set of industry guidelines specific to the needs of airline catering. The World Food Safety Guidelines for Airline Catering is offered free of charge by the International Flight Service Association.[21]		
Enterostatin is a pentapeptide[1] derived from a proenzyme in the gastrointestinal tract called procolipase. It reduces food intake, in particular fat intake,[2] when given peripherally or into the brain.[3]						Enterostatin is created in the intestine by pancreatic procolipase, the other colipase serving as an obligatory cofactor for pancreatic lipase during fat digestion. Enterostatin can be created in the gastric mucosa and the mucosal epithelia in the small intestine. An increased high fat diets will cause the procolipase gene transcription and enterostatin to release into the gastrointestinal lumen. Enterostatin appears in the lymph and circulation after a meal. Enterostatin has been shown to selectively reduce fat intake during a normal meal. The testing has been successful with different species.		The signaling pathway of the peripheral mechanism uses afferent vagal to hypothalamic centers. The central responses are mediated through a pathway including serotonergic and opioidergic components. Inveterately, enterostatin cuts fat intake, bodyweight, and body fat. This reaction may involve multiple metabolic effects of enterostatin, which include a decrease of insulin secretion,[4] a growth in sympathetic drive to brown adipose tissue, and the stimulation of adrenal corticosteroid secretion. A possible pathophysiological role is indicated by studies that have associated low enterostatin output and/or responsiveness to breeds of rat that become obese and prefer dietary fat. Humans with obesity also exhibit a lower secretion from pancreatic procolipase after a test meal, compared with persons of normal weight.[3]		Its effects include a reduction of insulin secretion, an increase in sympathetic drive to brown adipose tissue, and the stimulation of adrenal corticosteroid secretion. At the end level, it initiates a sensation of fullness of stomach which could be the reason for its role in regulation of fat intake and reduction of body weight. For enterostatin to be utilized it needs the presence of CCK A receptors. Studies based on rats who lack these receptors have found them to be un-responsive to enterostatin.[5]		When rats have been injected with high doses of enterostatin into the brain the rats ate progressively less food as the dose was increased.[6]:969 In rats, examination of experiments involving the effects of peripheral or intracerebroventricular administration of enterostatin show this selectively slows down fat consumption.[7]:8		Although enterostatin-like immunoreactivities exist in blood, brain, and gut, and exogenous enterostatins decrease fat appetite and insulin secretion in rats, the roles of these peptides in human obesity remain to be examined.,[8]		
Disability is an impairment that may be cognitive, developmental, intellectual, mental, physical, sensory, or some combination of these. It substantially affects a person's life activities and may be present from birth or occur during a person's lifetime.		Disabilities is an umbrella term, covering impairments, activity limitations, and participation restrictions. An impairment is a problem in body function or structure; an activity limitation is a difficulty encountered by an individual in executing a task or action; while a participation restriction is a problem experienced by an individual in involvement in life situations. Disability is thus not just a health problem. It is a complex phenomenon, reflecting the interaction between features of a person’s body and features of the society in which he or she lives.		Disability is a contested concept, with different meanings for different communities.[2] It may be used to refer to physical or mental attributes that some institutions, particularly medicine, view as needing to be fixed (the medical model). It may refer to limitations imposed on people by the constraints of an ableist society (the social model). Or the term may serve to refer to the identity of people with disabilities.		The discussion over disability's definition arose out of disability activism in the U.S. and U.K. in the 1970s, which challenged how the medical concept of disability dominated perception and discourse about disabilities. Debates about proper terminology and their implied politics continue in disability communities and the academic field of disability studies. In some countries, the law requires that disabilities are documented by a healthcare provider in order to assess qualifications for disability benefits.						Contemporary understandings of disability derive from concepts that arose during the West's scientific Enlightenment; prior to the Enlightenment, physical differences were viewed through a different lens.		During the Middle Ages, madness and other conditions were thought to be caused by demons. They were also thought to be part of the natural order, especially during and in the fallout of the Plague, which wrought impairments throughout the general population.[3] In the early modern period there was a shift to seeking biological causes for physical and mental differences, as well as heightened interest in demarcating categories: for example, Ambroise Pare, in the sixteenth century, wrote of "monsters", "prodigies", and "the maimed".[4] The European Enlightenment's emphases on knowledge derived from reason and on the value of natural science to human progress helped spawn the birth of institutions and associated knowledge systems that observed and categorized human beings; among these, the ones significant to the development of today's concepts of disability were asylums, clinics, and, prisons.[3]		Contemporary concepts of disability are rooted in eighteenth- and nineteenth-century developments. Foremost among these was the development of clinical medical discourse, which made the human body visible as a thing to be manipulated, studied, and transformed. These worked in tandem with scientific discourses that sought to classify and categorize and, in so doing, became methods of normalization.[5]		The concept of the "norm" developed in this time period, and is signaled in the work of the French statistician Alphonse Quetelet, who wrote in the 1830s of l'homme moyen – the average man. Quetelet postulated that one could take the sum of all people's attributes in a given population (such as their height or weight) and find their average, and that this figure should serve as a norm toward which all should aspire.		This idea of a statistical norm threads through the rapid take up of statistics gathering by Britain, United States, and the Western European states during this time period, and it is tied to the rise of eugenics. Disability, as well as other concepts including: abnormal, non-normal, and normalcy came from this.[6] The circulation of these concepts is evident in the popularity of the freak show, where showmen profited from exhibiting people who deviated from those norms.[7]		With the rise of eugenics in the latter part of the nineteenth century, such deviations were viewed as dangerous to the health of entire populations. With disability viewed as part of a person's biological make-up and thus their genetic inheritance, scientists turned their attention to notions of weeding such "deviations" out of the gene pool. Various metrics for assessing a person's genetic fitness, which were then used to deport, sterilize, or institutionalize those deemed unfit. At the end of the Second World War, with the example of Nazi eugenics, eugenics faded from public discourse, and increasingly disability cohered into a set of attributes that medicine could attend to – whether through augmentation, rehabilitation, or treatment. In both contemporary and modern history, disability was often viewed as a by-product of incest between first-degree relatives or second-degree relatives.[8]		In the early 1970s, disability activists began to challenge how society treated people with disabilities and the medical approach to disability. Due to this work, physical barriers to access were identified. These conditions functionally disabled them, and what is now known as the social model of disability emerged. Coined by Mike Oliver in 1983, this phrase distinguishes between the medical model of disability – under which an impairment needs to be fixed – and the social model of disability – under which the society that limits a person needs to be fixed.[9]		Different terms have been used for people with disabilities in different times and places. Disability or impairment are commonly used, as are more specific terms, such as blind (to describe having no vision at all) or visually impaired (to describe having limited vision).		Handicap has been disparaged as a result of false folk etymology that says it is a reference to begging. It is actually derived from an old game, Hand-i'-cap, in which two players trade possessions and a third, neutral person judges the difference of value between the possessions.[10] The concept of a neutral person evening up the odds was extended to handicap racing in the mid-18th century. In handicap racing, horses carry different weights based on the umpire's estimation of what would make them run equally. The use of the term to describe a person with a disability—by extension from handicap racing, a person carrying a heavier burden than normal—appeared in the early 20th century.[11]		People-first language is one way to talk about disability that some people who have disabilities prefer. Using people-first language is said to put the person before their disability, so those individuals who prefer people-first language, prefer to be called, "a person with a disability". Some people prefer person-first phrasing, while others prefer disability-first phrasing.		For people-first guidelines, check out, "Cerebral Palsy: A Guide for Care" at the University of Delaware:[12]		"The American Psychological Association style guide states that, when identifying a person with a disability, the person's name or pronoun should come first, and descriptions of the disability should be used so that the disability is identified, but is not modifying the person. Acceptable examples included "a woman with Down syndrome" or "a man who has schizophrenia". It also states that a person's adaptive equipment should be described functionally as something that assists a person, not as something that limits a person, for example, "a woman who uses a wheelchair" rather than "a woman in/confined to a wheelchair".		A similar kind of "people-first" terminology is also used in the UK, but more often in the form "people with impairments" (such as "people with visual impairments"). However, in the UK, the term "disabled people" is generally preferred to "people with disabilities". It is argued under the social model that while someone's impairment (for example, having a spinal cord injury) is an individual property, "disability" is something created by external societal factors such as a lack of accessibility.[13] This distinction between the individual property of impairment and the social property of disability is central to the social model. The term "disabled people" as a political construction is also widely used by international organisations of disabled people, such as Disabled Peoples' International (DPI).		The use of “people-first” terminology has given rise to the use of the acronym PWD to refer to person(s) (or people) with disabilities (or disability).[14][15][16]		To a certain degree, physical impairments and changing mental states are almost ubiquitously experienced by people as they age. Aging populations are often stigmatized for having a high prevalence of disability. Kathleen Woodward, writing in Key Words for Disability Studies, explains the phenomenon as follows:		Aging is invoked rhetorically-at times ominously-as a pressing reason why disability should be of crucial interest to all of us (we are all getting older, we will all be disabled eventually), thereby inadvertently reinforcing the damaging and dominant stereotype of aging as solely an experience of decline and deterioration. But little attention has been given to the imbrication of aging and disability.[17]		As stated above, studies have illustrated a correlation between disabilities and poverty. Notably, jobs offered to people with disabilities are scarce. For global demographic data on unemployment rates for the disabled, see Disability and poverty. However, there are current programs in place that aid people with intellectual disabilities (ID) to acquire skills they need in the workforce.[18] Such programs include sheltered workshops and adult day care programs. Sheltered programs consist of daytime activities such as, gardening, manufacturing, and assembling. These activities facilitate routine-oriented tasks that in turn allow people with ID to gain experience before entering the workforce. Similarly, adult day care programs also include day time activities. However, these activities are based in an educational environment where people with ID are able to engage in educational, physical, and communication based tasks. This educational based environment helps facilitate communication, memory, and general living skills. In addition, adult day care programs arrange opportunities for their students to engage in community activities. Such opportunities are arranged by scheduling field trips to public places (i.e. Disneyland, Zoo, and Movie Theater). Despite, both programs providing essential skills for people with ID prior to entering the workforce researchers have found that people with ID prefer to be involved with community-integrated employment.[18] Community-integrated employment are job opportunities offered to people with ID at minimum wage or a higher rate depending on the position. Community-integrated employment comes in a variety of occupations ranging from customer service, clerical, janitorial, hospitality and manufacturing positions. Within their daily tasks community-integrated employees work alongside employees who do not have disabilities, but who are able to assist them with training. All three options allow people with ID to develop and exercise social skills that are vital to everyday life. However, it is not guaranteed that community-integrated employees receive the same treatment as employees that do not have ID. According to Lindstrom, Hirano, McCarthy, and Alverson, community-integrated employees are less likely to receiving raises. In addition, studies conducted in 2013 illustrated only 26% of employees with ID retained full-time status.[19]		Furthermore, many with disabilities, intellectual and (or) psychical, finding a stable workforce poses many challenges. According to a study conducted by JARED (Journal of Applied Research and Intellectual Disability, indicates that although finding a job may be difficult for an intellectually disabled individual, stabilizing a job is even harder.[20] This is largely due to two main factors: production skills and effective social skills. This idea is supported by Chadsey-Rusch who claims that securing employment for the intellectually disabled, requires adequate production skills and effective social skills.[20] However, other underlying factors for job loss include, structural factors and the integration between worker and workplace. As stated by Kilsby, limited structural factors can effect a multitude of factors in a job. Factors such as a restricted amount of hours an intellectually disabled person is allowed to work. This in return according to Fabian, Wistow, and Schneider leads to a lack of opportunity to develop relationships with coworkers and a chance to better integrate within the workplace. Nevertheless, those who are unable to stabilize a job often are left discouraged. According to the same study conducted by JARED, many who had participated, found that they had made smaller incomes when compared to their co-workers, had an excess of time throughout their days, because they did not have work. They also, had feelings of hopelessness and failure. According to the NOD ( National Organization On Disability), not only do the (ID) face constant discouragement but many live below the poverty line, because they are unable to find or stabilize employment and (or) because of employee restricting factors placed on ID workers.[19] This then causes the (ID) the incapacity to provide for themselves basic necessities one needs. Items such as, food, medical care, transportation, and housing.		There is a global correlation between disability and poverty, produced by a variety of factors. Disability and poverty may form a vicious circle, in which physical barriers and stigma of disability make it more difficult to get income, which in turn diminishes access to health care and other necessities for a healthy life.[21] The World report on disability indicates that half of all people with disabilities cannot afford health care, compared to a third of people without disabilities.[22] In countries without public services for adults with disabilities, their families may be impoverished.[23]		There is limited research knowledge, but many anecdotal reports, on what happens when disasters impact people with disabilities.[24][25] Individuals with disabilities are greatly affected by disasters.[24][26] Those with physical disabilities can be at risk when evacuating if assistance is not available. Individuals with cognitive impairments may struggle with understanding instructions that must be followed in the event a disaster occurs.[26][27][28] All of these factors can increase the degree of variation of risk in disaster situations with disabled individuals.[29]		Research studies have consistently found discrimination against individuals with disabilities during all phases of a disaster cycle.[24] The most common limitation is that people cannot physically access buildings or transportation, as well as access disaster-related services.[24] The exclusion of these individuals is caused in part by the lack of disability-related training provided to emergency planners and disaster relief personnel.[30]		The International Classification of Functioning, Disability and Health (ICF), produced by the World Health Organization, distinguishes between body functions (physiological or psychological, such as vision) and body structures (anatomical parts, such as the eye and related structures). Impairment in bodily structure or function is defined as involving an anomaly, defect, loss or other significant deviation from certain generally accepted population standards, which may fluctuate over time. Activity is defined as the execution of a task or action. The ICF lists 9 broad domains of functioning which can be affected:		In concert with disability scholars, the introduction to the ICF states that a variety of conceptual models has been proposed to understand and explain disability and functioning, which it seeks to integrate. These models include the following:		The medical model views disability as a problem of the person, directly caused by disease, trauma, or other health conditions which therefore requires sustained medical care in the form of individual treatment by professionals. In the medical model, management of the disability is aimed at a "cure", or the individual’s adjustment and behavioral change that would lead to an "almost-cure" or effective cure. In the medical model, medical care is viewed as the main issue, and at the political level, the principal response is that of modifying or reforming healthcare policy.[31][32]		The social model of disability sees "disability" as a socially created problem and a matter of the full integration of individuals into society. In this model, disability is not an attribute of an individual, but rather a complex collection of conditions, created by the social environment. The management of the problem requires social action and it is the collective responsibility of society to create a society in which limitations for people with disabilities are minimal. Disability is both cultural and ideological in creation. According to the social model, equal access for someone with an impairment/disability is a human rights concern.[33][32] The social model of disability has come under criticism. While recognizing the importance played by the social model in stressing the responsibility of society, scholars, including Tom Shakespeare, point out the limits of the model, and urge the need for a new model that will overcome the "medical vs. social" dichotomy.[34]		Some say medical humanities is a fruitful field where the gap between the medical and the social model of disability might be bridged.[35]		The social construction of disability is the idea that disability is constructed by social expectations and institutions rather than biological differences. Highlighting the ways society and institutions construct disability is one of the main focuses of this idea.[36] In the same way that race and gender are not biologically fixed, neither is disability.		Around the early 1970s, sociologists, notably Eliot Friedson, began to argue that labeling theory and social deviance could be applied to disability studies. This led to the creation of the social construction of disability theory. The social construction of disability is the idea that disability is constructed as the social response to a deviance from the norm. The medical industry is the creator of the ill and disabled social role. Medical professionals and institutions, who wield expertise over health, have the ability to define health and physical and mental norms. When an individual has a feature that creates an impairment, restriction, or limitation from reaching the social definition of health, the individual is labeled as disabled. Under this idea, disability is not defined by the physical features of the body but by a deviance from the social convention of health.[37]		Social construction of disability would argue that the medical model of disability's view that a disability is an impairment, restriction, or limitation is wrong. Instead what is seen as a disability is just a difference in the individual from what is considered "normal" in society.[38]		In contexts where their differences are visible, persons with disabilities often face stigma. People frequently react to disabled presence with fear, pity, patronization, intrusive gazes, revulsion, or disregard. These reactions can, and often do, exclude persons with disabilities from accessing social spaces along with the benefits and resources these spaces provide.[51] Disabled writer/researcher Jenny Morris describes how stigma functions to marginalize persons with disabilities:[52]		“Going out in public so often takes courage. How many of us find that we can't dredge up the strength to do it day after day, week after week, year after year, a lifetime of rejection and revulsion? It is not only physical limitations that restrict us to our homes and those whom we know. It is the knowledge that each entry into the public world will be dominated by stares, by condescension, by pity and by hostility.”		Additionally, facing stigma can cause harm to psycho-emotional well-being of the person being stigmatized. One of the ways in which the psycho-emotional health of persons with disabilities is adversely affected is through the internalization of the oppression they experience, which can lead to feeling that they are weak, crazy, worthless, or any number of other negative attributes that may be associated with their conditions. Internalization of oppression damages the self-esteem of the person affected and shapes their behaviors in ways that are compliant with nondisabled dominance.[51] Ableist ideas are frequently internalized when disabled people are pressured by the people and institutions around them to hide and downplay their disabled difference, or, "pass". According to writer Simi Linton, the act of passing takes a deep emotional toll by causing disabled individuals to experience loss of community, anxiety and self-doubt.[53] The media play a significant role in creating and reinforcing stigma associated with disability. Media portrayals of disability usually cast disabled presence as necessarily marginal within society at large. These portrayals simultaneously reflect and influence popular perception of disabled difference.		There are distinct tactics that the media frequently employ in representing disabled presence. These common ways of framing disability are heavily criticized for being dehumanizing and failing to place importance on the perspectives of persons with disabilities.		Inspiration porn refers to portrayals of persons with disabilities in which they are presented as being inspiring simply because the person has a disability. These portrayals are criticized because they are created with the intent of making able-bodied viewers feel better about themselves in comparison to the individual portrayed. Rather than recognizing the humanity of persons with disabilities, inspiration porn turns them into objects of inspiration for a nondisabled audience.[54]		The supercrip trope refers to instances when the media report on or portray a disabled individual who has made a noteworthy achievement but center on the person's disabled difference rather than what they actually did. They are portrayed as awe-inspiring for their ability to be exceptional in spite of their condition. This trope is widely used in reporting on disabled sports as well as in portrayals of autistic savants.[55][56] Persons with disabilities denounce these representations as reducing people to their condition rather than viewing them as full people. Furthermore, supercrip portrayals are criticized for creating the unrealistic expectation that disability should be accompanied by some type of special talent, genius, or insight.		Characters in fiction that bear physical or mental markers of difference are frequently positioned as villains within a text. Lindsey Row-Heyveld notes that it isn’t difficult to figure out, for instance, “that villainous pirates are scraggly, wizened, and inevitably kitted out with a peg leg or hook hand, whereas heroic pirates look like Johnny Depp.”[57] The use of disabled difference to evoke fear in audiences perpetuates the view that persons with disabilities are a threat to public interests and well-being.		One of the key ways that people with disabilities have resisted marginalization is through the creation and promotion of the social model in opposition to the medical model. By doing this they shift criticism away from their bodies and various impairments towards the social institutions that oppress them. Disability activism that demands a wide variety of grievances be addressed, such as lack of accessibility, poor representation in media, general disrespect, and lack of recognition, can be said to originate from a social model framework. Furthermore, embracing disability as a positive identity by becoming involved in disability communities and participating in disability cultures can be an effective way to combat internalized oppression and challenge dominant narratives about disability.[58]		The experiences that disabled people have navigating social institutions vary greatly as a function of what other social categories they may belong to. The categories that intersect with disabled difference to create unique experiences of ableism include, but aren’t limited to, race and gender.		Disabled individuals who are non-white generally have less access to support and are more vulnerable to violent discrimination. For example, in the United States people of color who are mentally ill are more frequently victims of police brutality than their white counterparts. Camille A. Nelson, writing for the Berkeley Journal of Criminal Law, notes that for “people who are negatively racialized, that is people who are perceived as being non-white, and for whom mental illness is either known or assumed, interaction with police is precarious and potentially dangerous.”[59]		The marginalization of disabled difference can leave persons with disabilities unable to actualize what society expects of gendered existence. This lack of recognition for their gender identity can leave persons with disabilities with feelings of inadequacy. Thomas J. Gerschick of Illinois State University describes why this denial of gendered identity occurs:[60]		"Bodies operate socially as canvases on which gender is displayed and kinesthetically as the mechanisms by which it is physically enacted. Thus, the bodies of people with disabilities make them vulnerable to being denied recognition as women and men."		To the extent that women and men with disabilities are gendered, the interactions of these two identities lead to different experiences. Disabled women face a sort of “double stigmatization” in which their membership to both of these marginalized categories simultaneously exacerbates the negative stereotypes associated with each as they are ascribed to them. As Rosemarie Garland-Thomson puts it, “Women with disabilities, even more intensely than women in general, have been cast in the collective cultural imagination as inferior, lacking, excessive, incapable, unfit, and useless.”[61]		Assistive Technology is a generic term for devices and modifications (for a person or within a society) that help overcome or remove a disability. The first recorded example of the use of a prosthesis dates to at least 1800 BC.[62] The wheelchair dates from the 17th century.[63] The curb cut is a related structural innovation. Other examples are standing frames, text telephones, accessible keyboards, large print, Braille, & speech recognition software. People with disabilities often develop personal or community adaptations, such as strategies to suppress tics in public (for example in Tourette's syndrome), or sign language in deaf communities.		As the personal computer has become more ubiquitous, various organizations have formed to develop software and hardware to make computers more accessible for people with disabilities. Some software and hardware, such as Voice Finger, Freedom Scientific's JAWS, the Free and Open Source alternative Orca etc. have been specifically designed for people with disabilities while other software and hardware, such as Nuance's Dragon NaturallySpeaking, were not developed specifically for people with disabilities, but can be used to increase accessibility.[64] The LOMAK keyboard was designed in New Zealand specifically for persons with disabilities.[65] The World Wide Web consortium recognised a need for International Standards for Web Accessibility for persons with disabilities and created the Web Accessibility Initiative (WAI).[66] As at Dec 2012 the standard is WCAG 2.0 (WCAG = Web Content Accessibility Guidelines).[67]		The Paralympic Games (meaning "alongside the Olympics") are held after the (Summer and Winter) Olympics. The Paralympic Games include athletes with a wide range of physical disabilities. In member countries organizations exist to organize competition in the Paralympic sports on levels ranging from recreational to elite (for example, Disabled Sports USA and BlazeSports America in the United States).		The Paralympics developed from a rehabilitation programme for British war veterans with spinal injuries. In 1948, Sir Ludwig Guttman, a neurologist working with World War II veterans with spinal injuries at Stoke Mandeville Hospital in Aylesbury in the UK, began using sport as part of the rehabilitation programmes of his patients.		In 2006, the Extremity Games was formed for people with physical disabilities, specifically limb loss or limb difference, to be able to compete in extreme sports.[68]		The disability rights movement aims to secure equal opportunities and equal rights for people with disabilities. The specific goals and demands of the movement are accessibility and safety in transportation, architecture, and the physical environment; equal opportunities in independent living, employment, education, and housing; and freedom from abuse, neglect, and violations of patients' rights.[69] Effective civil rights legislation is sought to secure these opportunities and rights.[69][70]		The early disability rights movement was dominated by the medical model of disability, where emphasis was placed on curing or treating people with disabilities so that they would adhere to the social norm, but starting in the 1960s, rights groups began shifting to the social model of disability, where disability is interpreted as an issue of discrimination, thereby paving the way for rights groups to achieve equality through legal means.[71]		On December 13, 2006, the United Nations formally agreed on the Convention on the Rights of Persons with Disabilities, the first human rights treaty of the 21st century, to protect and enhance the rights and opportunities of the world's estimated 650 million disabled people.[72] As of April 2011, 99 of the 147 signatories had ratified the Convention.[73] Countries that sign the convention are required to adopt national laws, and remove old ones, so that persons with disabilities will, for example, have equal rights to education, employment, and cultural life; to the right to own and inherit property; to not be discriminated against in marriage, etc.; and to not be unwilling subjects in medical experiments. UN officials, including the High Commissioner for Human Rights, have characterized the bill as representing a paradigm shift in attitudes toward a more rights-based view of disability in line with the social model.[72]		In 1976, the United Nations began planning for its International Year for Disabled Persons (1981),[74] later renamed the International Year of Disabled Persons. The UN Decade of Disabled Persons (1983–1993) featured a World Programme of Action Concerning Disabled Persons. In 1979, Frank Bowe was the only person with a disability representing any country in the planning of IYDP-1981. Today, many countries have named representatives who are themselves individuals with disabilities. The decade was closed in an address before the General Assembly by Robert Davila. Both Bowe and Davila are deaf. In 1984, UNESCO accepted sign language for use in education of deaf children and youth.		In the United States, the Department of Labor's new (2014) rules for federal contractors, defined as companies that make more than $50,000/year from the federal government, require them to have as a goal that 7% of their workforce must be people with disabilities.[75] In schools, the ADA says that all classrooms must be wheelchair accessible.[76] The U.S. Architectural and Transportation Barriers Compliance Board, commonly referred to as the Access Board, created the Rehabilitation Act of 1973 to help offer guidelines for transportation and accessibility for the physically disabled.[77]		About 12.6% of the U.S population are individuals who suffer from a mental or physical disability. Many are unemployed because of prejudiced assumptions that a person with disabilities is unable to complete tasks that are commonly required in the workforce. This became a major Human rights issue because of the discrimination that this group faced when trying to apply for jobs in the U.S. Many advocacy groups protested against such discrimination, asking the federal government to implement laws and policies that would help individuals with disabilities.		The Rehabilitation Act of 1973 was enacted with the purpose of protecting individuals with disabilities from prejudicial treatment by government funded programs, employers, and agencies. The Rehabilitation Act of 1973 has not only helped protect U.S citizens from being discriminated against but it has also created confidence amongst individuals to feel more comfortable with their disability. There are many sections within The Rehabilitation Act of 1973, that contains detailed information about what is covered in this policy.		The federal government enacted The Americans with Disabilities Act of 1990, which was created to allow equal opportunity for jobs, access to private and government funded facilities, and transportation for people with disabilities. This act was created with the purpose to ensure that employers would not discriminate against any individual despite their disability. In 1990, data was gathered to show the percentage of people with disabilities who worked in the U.S. Out of the 13% who filled out the survey, only 53% percent of individuals with disabilities worked while 90% of this group population did not, the government wanted to change this, they wanted Americans with disabilities to have the same opportunities as those who did not have a disability. The ADA not only required corporations to hire disabled people but that they also accommodate them and their needs.		In the UK, the Department for Work and Pension is a government department responsible for promoting disability awareness and among its aims is to increase the understanding of disability and removal of barriers for people with disabilities in the workplace. According to a news report, a people survey conducted in the UK shows a 23% increase in reported discrimination and harassment in the workplace at The Department for Work and Pension. The survey shows the number of reports for discrimination due to disability was in majority compared to discrimination due to gender, ethnicity or age. DWP received criticism for the survey results. As a department responsible for tackling discrimination at work, the DWP results may indicate room for improvement from within. A DWP spokesperson said the survey results do not necessarily indicate an increase in the number of reports, but rather reflecting the outcomes of efforts to encourage people to come forward.[78]		Political rights, social inclusion and citizenship have come to the fore in developed and some developing countries. The debate has moved beyond a concern about the perceived cost of maintaining dependent people with disabilities to finding effective ways to ensure that people with disabilities can participate in and contribute to society in all spheres of life.		In developing nations, where the vast bulk of the estimated 650 million people with disabilities reside, a great deal of work is needed to address concerns ranging from accessibility and education to self-empowerment, self-supporting employment, and beyond.		In the past few years, disability rights activists have focused on obtaining full citizenship for the disabled.		There are obstacles in some countries in getting full employment; public perception of disabled people may vary.		Disability abuse happens when a person is abused physically, financially, verbally or mentally due to the person having a disability. As many disabilities are not visible (for example, asthma, learning disabilities) some abusers cannot rationalize the non-physical disability with a need for understanding, support, and so on.		As the prevalence of disability and the cost of supporting disability increases with medical advancement and longevity in general, this aspect of society becomes of greater political importance. How political parties treat their disabled constituents may become a measure of a political party's understanding of disability, particularly in the social model of disability.[79]		Disability benefit, or disability pension, is a major kind of disability insurance that is provided by government agencies to people who are temporarily or permanently unable to work due to a disability. In the U.S., disability benefit is provided in the category of Supplemental Security Income. In Canada, it is within the Canada Pension Plan. In other countries, disability benefit may be provided under social security systems.		Costs of disability pensions are steadily growing in Western countries, mainly in Europe and the United States. It was reported that, in the UK, expenditure on disability pensions accounted for 0.9% of gross domestic product (GDP) in 1980; two decades later it had reached 2.6% of GDP.[80][81] Several studies have reported a link between increased absence from work due to sickness and elevated risk of future disability pension.[82]		A study by researchers in Denmark suggests that information on self-reported days of absence due to sickness can be used to effectively identify future potential groups for disability pension.[81] These studies may provide useful information for policy makers, case managing authorities, employers, and physicians.		Private, for-profit disability insurance plays a role in providing incomes to disabled people, but the nationalized programs are the safety net that catch most claimants.		Estimates of worldwide and country-wide numbers of individuals with disabilities are problematic. The varying approaches taken to defining disability notwithstanding, demographers agree that the world population of individuals with disabilities is very large. For example, in 2012, the World Health Organization estimated a world population of 6.5 billion people. Of those, nearly 650 million people, or 10%, were estimated to be moderately or severely disabled.[83]		According to the U.S. Census Bureau, as of 2010, there were some 56.7 million disabled people, or 19% (by comparison, African Americans are the largest racial minority in the U.S., but only constitute 12.6% of the U.S. population).[84]		Disabled individuals make up one of the most inclusive minority groups in the United States.[85] According to the 2014 Disability status report of the Cornell University Yang Tan Institute the prevalence rate of individuals with disabilities in the United States was 12.6% in that year. As of 2014 ambulatory disability had the highest prevalence (7.1%) in the United States. By contrast, visual disability had the lowest prevalence (2.3%). Additionally, 3.6% of people in the United States were reported to have had an auditory disability in the same year.[86]		5.8% of individuals ages 16–20 reported having any disability, physical and/ or cognitive. Adults 21 to 64 had a prevalence of 10.8% with over half of these (5.5%) being ambulatory disabilities. Ambulatory disability prevalence raised to 15.8% in adults 65–74 years of age. Adults 75 years and older comprised the highest prevalence with any disability at 50.3%.		Female individuals across all ages reported a total 0.4% higher prevalence rate than males who reported 12.4%.		In the U.S. 17.9% of Native American peoples reported having a disability while 4.5% reporting were of Asian descent, these were the two opposing poles of the prevalence rate within race as of 2014.[86]		Although there are acts that have been imposed in order to prevent the discrimination of individuals with disabilities in the workplace, there is still an employment gap that can be seen between those with and without disabilities. In regards to employment the institute’s status report accounts that 34.6% of people with any disability reported being employed. By comparison; 77.6% of individuals, who did not report having a disability, reported having a full-time job in 2014.[86]		For those employed full-time, individuals with disabilities on average earned $5,100 less than employees without a disability who were also employed full-time. Those affected the most by these differences were people with intellectual disabilities.[86]		There is widespread agreement among experts[who?] in the field that disability is more common in developing than in developed nations. The connection between disability and poverty is thought to be part of a "vicious cycle" in which these constructs are mutually reinforcing.[87]		Nearly 8 million European men were permanently disabled in World War I.[88] About 150,000 Vietnam veterans came home wounded, and at least 21,000 were permanently disabled.[89] As of 2008, there were 2.9 million disabled veterans in the United States, an increase of 25 percent over 2001.[90]		After years of war in Afghanistan, there are more than 1 million disabled people.[91] Afghanistan has one of the highest incidences of people with disabilities in the world.[92] An estimated 80,000 Afghans are missing limbs, usually from landmine explosions.[93]		In Australia, 18.5% of the population reported having a disability in a 2009 survey.[94]		Note: this file is approximately 18.3 megabytes		
Surgery (from the Greek: χειρουργική cheirourgikē (composed of χείρ, "hand", and ἔργον, "work"), via Latin: chirurgiae, meaning "hand work") is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as a disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum).		An act of performing surgery may be called a "surgical procedure", "operation", or simply "surgery". In this context, the verb "operate" means to perform surgery. The adjective "surgical" means pertaining to surgery; e.g. surgical instruments or surgical nurse. The patient or subject on which the surgery is performed can be a person or an animal. A surgeon is a person who practices surgery and a surgeon's assistant is a person who practices surgical assistance. A surgical team is made up of surgeon, surgeon's assistant, anesthesia provider, circulating nurse and surgical technologist. Surgery usually spans minutes to hours, but it is typically not an ongoing or periodic type of treatment. The term "surgery" can also refer to the place where surgery is performed, or simply the office of a physician, dentist, or veterinarian.						Surgery is a technology consisting of a physical intervention on tissues.		As a general rule, a procedure is considered surgical when it involves cutting of a patient's tissues or closure of a previously sustained wound. Other procedures that do not necessarily fall under this rubric, such as angioplasty or endoscopy, may be considered surgery if they involve "common" surgical procedure or settings, such as use of a sterile environment, anesthesia, antiseptic conditions, typical surgical instruments, and suturing or stapling. All forms of surgery are considered invasive procedures; so-called "noninvasive surgery" usually refers to an excision that does not penetrate the structure being excised (e.g. laser ablation of the cornea) or to a radiosurgical procedure (e.g. irradiation of a tumor).		Surgical procedures are commonly categorized by urgency, type of procedure, body system involved, degree of invasiveness, and special instrumentation.		At a hospital, modern surgery is often performed in an operating theater using surgical instruments, an operating table for the patient, and other equipment. Among United States hospitalizations for nonmaternal and nonneonatal conditions in 2012, more than one-fourth of stays and half of hospital costs involved stays that included operating room (OR) procedures.[1] The environment and procedures used in surgery are governed by the principles of aseptic technique: the strict separation of "sterile" (free of microorganisms) things from "unsterile" or "contaminated" things. All surgical instruments must be sterilized, and an instrument must be replaced or re-sterilized if it becomes contaminated (i.e. handled in an unsterile manner, or allowed to touch an unsterile surface). Operating room staff must wear sterile attire (scrubs, a scrub cap, a sterile surgical gown, sterile latex or non-latex polymer gloves and a surgical mask), and they must scrub hands and arms with an approved disinfectant agent before each procedure. There is moderate-quality evidence that usage of two layers of gloves compared to single gloving during surgery reduces perforations and blood stains on the skin, indicating a decrease in percutaneous exposure incidents.[2]		Prior to surgery, the patient is given a medical examination, receives certain pre-operative tests, and their physical status is rated according to the ASA physical status classification system. If these results are satisfactory, the patient signs a consent form and is given a surgical clearance. If the procedure is expected to result in significant blood loss, an autologous blood donation may be made some weeks prior to surgery. If the surgery involves the digestive system, the patient may be instructed to perform a bowel prep by drinking a solution of polyethylene glycol the night before the procedure. Patients are also instructed to abstain from food or drink (an NPO order after midnight on the night before the procedure), to minimize the effect of stomach contents on pre-operative medications and reduce the risk of aspiration if the patient vomits during or after the procedure.		Some medical systems have a practice of routinely performing chest x-rays before surgery. The premise behind this practice is that the physician might discover some unknown medical condition which would complicate the surgery, and that upon discovering this with the chest x-ray, the physician would adapt the surgery practice accordingly.[3] In fact, medical specialty professional organizations recommend against routine pre-operative chest x-rays for patients who have an unremarkable medical history and presented with a physical exam which did not indicate a chest x-ray.[3] Routine x-ray examination is more likely to result in problems like misdiagnosis, overtreatment, or other negative outcomes than it is to result in a benefit to the patient.[3] Likewise, other tests including complete blood count, prothrombin time, partial thromboplastin time, basic metabolic panel, and urinalysis should not be done unless the results of these tests can help evaluate surgical risk.[4]		In the pre-operative holding area, the patient changes out of his or her street clothes and is asked to confirm the details of his or her surgery. A set of vital signs are recorded, a peripheral IV line is placed, and pre-operative medications (antibiotics, sedatives, etc.) are given. When the patient enters the operating room, the skin surface to be operated on, called the operating field, is cleaned and prepared by applying an antiseptic such as chlorhexidine gluconate or povidone-iodine to reduce the possibility of infection. If hair is present at the surgical site, it is clipped off prior to prep application. The patient is assisted by an anesthesiologist or resident to make a specific surgical position, then sterile drapes are used to cover the surgical site or at least a wide area surrounding the operating field; the drapes are clipped to a pair of poles near the head of the bed to form an "ether screen", which separates the anesthetist/anesthesiologist's working area (unsterile) from the surgical site (sterile).[5]		Anesthesia is administered to prevent pain from incision, tissue manipulation and suturing. Based on the procedure, anesthesia may be provided locally or as general anesthesia. Spinal anesthesia may be used when the surgical site is too large or deep for a local block, but general anesthesia may not be desirable. With local and spinal anesthesia, the surgical site is anesthetized, but the patient can remain conscious or minimally sedated. In contrast, general anesthesia renders the patient unconscious and paralyzed during surgery. The patient is intubated and is placed on a mechanical ventilator, and anesthesia is produced by a combination of injected and inhaled agents. Choice of surgical method and anaesthetic technique aims to reduce risk of complications, shorten time needed for recovery and minimise the surgical stress response.		An incision is made to access the surgical site. Blood vessels may be clamped or cauterized to prevent bleeding, and retractors may be used to expose the site or keep the incision open. The approach to the surgical site may involve several layers of incision and dissection, as in abdominal surgery, where the incision must traverse skin, subcutaneous tissue, three layers of muscle and then the peritoneum. In certain cases, bone may be cut to further access the interior of the body; for example, cutting the skull for brain surgery or cutting the sternum for thoracic (chest) surgery to open up the rib cage. Whilst in surgery aseptic technique is used to prevent infection or further spreading of the disease. The surgeons' and assistants' hands, wrists and forearms are washed thoroughly for at least 4 minutes to prevent germs getting into the operative field, then sterile gloves are placed onto their hands. An antiseptic solution is applied to the area of the patient's body that will be operated on.Sterile drapes are placed around the operative site. Surgical masks are worn by the surgical team to avoid germs on droplets of liquid from their mouths and noses from contaminating the operative site.		Work to correct the problem in body then proceeds. This work may involve:		Blood or blood expanders may be administered to compensate for blood lost during surgery. Once the procedure is complete, sutures or staples are used to close the incision. Once the incision is closed, the anesthetic agents are stopped or reversed, and the patient is taken off ventilation and extubated (if general anesthesia was administered).[7]		After completion of surgery, the patient is transferred to the post anesthesia care unit and closely monitored. When the patient is judged to have recovered from the anesthesia, he/she is either transferred to a surgical ward elsewhere in the hospital or discharged home. During the post-operative period, the patient's general function is assessed, the outcome of the procedure is assessed, and the surgical site is checked for signs of infection. There are several risk factors associated with postoperative complications, such as immune deficienty and obesity. Obesity has long been considered a risk factor for adverse post-surgical outcomes. It has been linked to many disorders such as obesity hypoventilation syndrome, atelectasis and pulmonary embolism, adverse cardiovascular effects, and wound healing complications.[8] If removable skin closures are used, they are removed after 7 to 10 days post-operatively, or after healing of the incision is well under way.		It is not uncommon for surgical drains (see Drain (surgery)) to be required to remove blood or fluid from the surgical wound during recovery. Mostly these drains stay in until the volume tapers off, then they are removed. These drains can become clogged, leading to retained blood complications or abscess.		Postoperative therapy may include adjuvant treatment such as chemotherapy, radiation therapy, or administration of medication such as anti-rejection medication for transplants. Other follow-up studies or rehabilitation may be prescribed during and after the recovery period.		The use of topical antibiotics on surgical wounds to reduce infection rates has been questioned.[9] Antibiotic ointments are likely to irritate the skin, slow healing, and could increase risk of developing contact dermatitis and antibiotic resistance.[9] It has been also been suggested that topical antibiotics should only be used when a person shows signs of infection and not as a preventative.[9] A systematic review published by Cochrane (organisation) in 2016, though, concluded that topical antibiotics applied over certain types of surgical wounds reduce the risk of surgical site infections, when compared to no treatment or use of Antiseptics.[10] The review also did not find conclusive evidence to suggest that topical antibiotics increased the risk of local skin reactions or antibiotic resistance.		Through a retrospective analysis of national administrative data, the association between mortality and day of elective surgical procedure suggests a higher risk in procedures carried out later in the working week and on weekends. The odds of death were 44% and 82% higher respectively when comparing procedures on a Friday to a weekend procedure. This “weekday effect” has been postulated to be from several factors including poorer availability of services on a weekend, and also, decrease number and level of experience over a weekend.[11]		in 2011, of the 38.6 million hospital stays in U.S. hospitals, 29% included at least one operating room procedure. These stays accounted for 48% of the total $387 billion in hospital costs.[12]		The overall number of procedures remained stable from 2001 to 2011. In 2011, over 15 million operating room procedures were performed in U.S. hospitals. [13]		Data from 2003 to 2011 showed that U.S. hospital costs were highest for the surgical service line; the surgical service line costs were $17,600 in 2003 and projected to be $22,500 in 2013.[14] For hospital stays in 2012 in the United States, private insurance had the highest percentage of surgical expenditure.[15] in 2012, mean hospital costs in the United States were highest for surgical stays.[15]		Older adults have widely varying physical health. Frail elderly people are at significant risk of post-surgical complications and the need for extended care. Assessment of older patients before elective surgery can accurately predict the patients' recovery trajectories.[16] One frailty scale uses five items: unintentional weight loss, muscle weakness, exhaustion, low physical activity, and slowed walking speed. A healthy person scores 0; a very frail person scores 5. Compared to non-frail elderly people, people with intermediate frailty scores (2 or 3) are twice as likely to have post-surgical complications, spend 50% more time in the hospital, and are three times as likely to be discharged to a skilled nursing facility instead of to their own homes.[16] Frail elderly patients (score of 4 or 5) have even worse outcomes, with the risk of being discharged to a nursing home rising to twenty times the rate for non-frail elderly people.		Surgery on children requires considerations which are not common in adult surgery. Children and adolescents are still developing physically and mentally making it difficult for them to make informed decisions and give consent for surgical treatments. Bariatric surgery in youth is among the controversial topics related to surgery in children.		A person with a debilitating medical condition may have special needs during a surgery which a typical patient would not.		Doctors perform surgery with the consent of the patient. Some patients are able to give better informed consent than others. Populations such as incarcerated persons, the mentally incompetent, persons subject to coercion, and other people who are not able to make decisions with the same authority as a typical patient have special needs when making decisions about their personal healthcare, including surgery.		In 2014, the Lancet Commission on Global Surgery was launched to examine the case for surgery as an integral component of global health care and to provide recommendations regarding the delivery of surgical and anesthesia services in low and middle income countries.[17] The primary conclusions of the study were as follows:		Surgical treatments date back to the prehistoric era. The oldest for which there is evidence is trepanation,[18] in which a hole is drilled or scraped into the skull, thus exposing the dura mater in order to treat health problems related to intracranial pressure and other diseases. Prehistoric surgical techniques are seen in Ancient Egypt, where a mandible dated to approximately 2650 BC shows two perforations just below the root of the first molar, indicating the draining of an abscessed tooth. Surgical texts from ancient Egypt date back about 3500 years ago. Surgical operations were performed by priests, specialized in medical treatments similar to today.,[19] and used sutures to close wounds.[20] Infections were treated with honey.[21]		Remains from the early Harappan periods of the Indus Valley Civilization (c. 3300 BC) show evidence of teeth having been drilled dating back 9,000 years.[22] Susruta[23] was an ancient Indian surgeon commonly credited as the author of the treatise Sushruta Samhita. He is dubbed as the "founding father of surgery" and his period is usually placed between the period of 1200 BC – 600 BC.[24] One of the earliest known mention of the name is from the Bower Manuscript where Sushruta is listed as one of the ten sages residing in the Himalayas.[25][25] Texts also suggest that he learned surgery at Kasi from Lord Dhanvantari, the god of medicine in Hindu mythology.[26] It is one of the oldest known surgical texts and it describes in detail the examination, diagnosis, treatment, and prognosis of numerous ailments, as well as procedures on performing various forms of cosmetic surgery, plastic surgery and rhinoplasty.[27]		Instruments resembling surgical tools have also been found in the archaeological sites of Bronze Age China dating from the Shang Dynasty, along with seeds likely used for herbalism.[28]		In ancient Greece, temples dedicated to the healer-god Asclepius, known as Asclepieia (Greek: Ασκληπιεία, sing. Asclepieion Ασκληπιείον), functioned as centers of medical advice, prognosis, and healing.[29] In the Asclepieion of Epidaurus, some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place.[7] The Greek Galen was one of the greatest surgeons of the ancient world and performed many audacious operations—including brain and eye surgery—that were not tried again for almost two millennia.		In the Middle East, surgery was developed to a high degree in the Islamic world. Abulcasis (Abu al-Qasim Khalaf ibn al-Abbas Al-Zahrawi), an Andalusian-Arab physician and scientist who practised in the Zahra suburb of Córdoba, wrote medical texts that influenced European surgical procedures.[citation needed]		In Europe, the demand grew for surgeons to formally study for many years before practicing; universities such as Montpellier, Padua and Bologna were particularly renowned. In the 12th century, Rogerius Salernitanus composed his Chirurgia, laying the foundation for modern Western surgical manuals. Barber-surgeons generally had a bad reputation that was not to improve until the development of academic surgery as a specialty of medicine, rather than an accessory field.[30] Basic surgical principles for asepsis etc., are known as Halsteads principles.		There were some important advances to the art of surgery during this period. The professor of anatomy at the University of Padua, Andreas Vesalius, was a pivotal figure in the Renaissance transition from classical medicine and anatomy based on the works of Galen, to an empirical approach of 'hands-on' dissection. In his anatomic treatis, De humani corporis fabrica, he exposed the many anatomical errors in Galen and advocated that all surgeons should train by engaging in practical dissections themselves.		The second figure of importance in this era was Ambroise Paré (sometimes spelled "Ambrose"[31]), a French army surgeon from the 1530s until his death in 1590. The practice for cauterizing gunshot wounds on the battlefield had been to use boiling oil; an extremely dangerous and painful procedure. Paré began to employ a less irritating emollient, made of egg yolk, rose oil and turpentine. He also described more efficient techniques for the effective ligation of the blood vessels during an amputation.		The discipline of surgery was put on a sound, scientific footing during the Age of Enlightenment in Europe. An important figure in this regard was the English surgical scientist, John Hunter, generally regarded as the father of modern scientific surgery.[32] He brought an empirical and experimental approach to the science and was renowned around Europe for the quality of his research and his written works. Hunter reconstructed surgical knowledge from scratch; refusing to rely on the testimonies of others, he conducted his own surgical experiments to determine the truth of the matter. To aid comparative analysis, he built up a collection of over 13,000 specimens of separate organ systems, from the simplest plants and animals to humans.		He greatly advanced knowledge of venereal disease and introduced many new techniques of surgery, including new methods for repairing damage to the Achilles tendon and a more effective method for applying ligature of the arteries in case of an aneurysm.[33] He was also one of the first to understand the importance of pathology, the danger of the spread of infection and how the problem of inflammation of the wound, bone lesions and even tuberculosis often undid any benefit that was gained from the intervention. He consequently adopted the position that all surgical procedures should be used only as a last resort.[34]		Other important 18th- and early 19th-century surgeons included Percival Pott (1713–1788) who described tuberculosis on the spine and first demonstrated that a cancer may be caused by an environmental carcinogen – (he noticed a connection between chimney sweep's exposure to soot and their high incidence of scrotal cancer). Astley Paston Cooper (1768–1841) first performed a successful ligation of the abdominal aorta, and James Syme (1799–1870) pioneered the Symes Amputation for the ankle joint and successfully carried out the first hip disarticulation.		Modern pain control through anesthesia was discovered in the mid-19th century. Before the advent of anesthesia, surgery was a traumatically painful procedure and surgeons were encouraged to be as swift as possible to minimize patient suffering. This also meant that operations were largely restricted to amputations and external growth removals. Beginning in the 1840s, surgery began to change dramatically in character with the discovery of effective and practical anaesthetic chemicals such as ether, first used by the American surgeon Crawford Long, and chloroform, discovered by James Young Simpson and later pioneered by John Snow, physician to Queen Victoria.[35] In addition to relieving patient suffering, anaesthesia allowed more intricate operations in the internal regions of the human body. In addition, the discovery of muscle relaxants such as curare allowed for safer applications.		Unfortunately, the introduction of anesthetics encouraged more surgery, which inadvertently caused more dangerous patient post-operative infections. The concept of infection was unknown until relatively modern times. The first progress in combating infection was made in 1847 by the Hungarian doctor Ignaz Semmelweis who noticed that medical students fresh from the dissecting room were causing excess maternal death compared to midwives. Semmelweis, despite ridicule and opposition, introduced compulsory handwashing for everyone entering the maternal wards and was rewarded with a plunge in maternal and fetal deaths, however the Royal Society dismissed his advice.		Until the pioneering work of British surgeon Joseph Lister in the 1860s, most medical men believed that chemical damage from exposures to bad air (see "miasma") was responsible for infections in wounds, and facilities for washing hands or a patient's wounds were not available.[36] Lister became aware of the work of French chemist Louis Pasteur, who showed that rotting and fermentation could occur under anaerobic conditions if micro-organisms were present. Pasteur suggested three methods to eliminate the micro-organisms responsible for gangrene: filtration, exposure to heat, or exposure to chemical solutions. Lister confirmed Pasteur's conclusions with his own experiments and decided to use his findings to develop antiseptic techniques for wounds. As the first two methods suggested by Pasteur were inappropriate for the treatment of human tissue, Lister experimented with the third, spraying carbolic acid on his instruments. He found that this remarkably reduced the incidence of gangrene and he published his results in The Lancet. [37] Later, on 9 August 1867, he read a paper before the British Medical Association in Dublin, on the Antiseptic Principle of the Practice of Surgery, which was reprinted in The British Medical Journal.[38][39][40] His work was groundbreaking and laid the foundations for a rapid advance in infection control that saw modern antiseptic operating theatres widely used within 50 years.		Lister continued to develop improved methods of antisepsis and asepsis when he realised that infection could be better avoided by preventing bacteria from getting into wounds in the first place. This led to the rise of sterile surgery. Lister introduced the Steam Steriliser to sterilize equipment, instituted rigorous hand washing and later implemented the wearing of rubber gloves. These three crucial advances – the adoption of a scientific methodology toward surgical operations, the use of anaesthetic and the introduction of sterilised equipment – laid the groundwork for the modern invasive surgical techniques of today.		The use of X-rays as an important medical diagnostic tool began with their discovery in 1895 by German physicist Wilhelm Röntgen. He noticed that these rays could penetrate the skin, allowing the skeletal structure to be captured on a specially treated photographic plate.		
A value menu (not to be confused with a value meal) is a group of menu items at a fast food restaurant that are designed to be the least expensive items available. In the US, the items are usually priced between $0.99 and $1.49. The portion size, and number of items included with the food, are typically related to the price.						Arby's announced the launch of their value menu on April 9, 2010.[1] Items on the value menu vary based on location, but typically include small or value size roast beef sandwiches, curly fries, milkshakes, chicken sandwiches, ham and cheddar sandwiches, and turnovers.[2]		Burger King added a value menu in 1998 with items priced at 99¢ (USD).[3] In 2002[4] and 2006, BK revamped its value menu, adding and removing products at 99¢, and later increasing some prices to $1.39.[5] Many of these items have since been discontinued, modified or relegated to a regional menu option.[6] The Burger King Whopper was the very first 99 cent burger and it revolutionized the 99 cent menu in the fast food industry.[7]		After numerous attempts beginning in 1991,[8] experimenting with a variety of menus and pricing strategies,[9] McDonald's launched its first national value menu, the Dollar Menu, in late 2002.[10]		In 1988, Taco Bell lowered the prices of all new items and launched the first three-tiered pricing strategy and free drink refills.[11] In 2010, Taco Bell introduced the $2 Meal Deals menu, featuring a menu item (i.e., a chicken burrito, a beefy 5-layer burrito, a double decker taco, or a Gordita supreme), a bag of Doritos, and a medium drink.[12] In August 18, 2014, Taco Bell launched a new value menu called Dollar Cravings that included eleven food items each priced a $1.[13][14][15][16]		Wendy's is generally credited with being the first fast food chain to offer a value menu in October 1989, with every item priced at $0.99.[17][18][19]		
A cafeteria is a type of food service location in which there is little or no waiting staff table service, whether a restaurant or within an institution such as a large office building or school; a school dining location is also referred to as a dining hall or canteen (in British English).[1] Cafeterias are different from coffeehouses, although the English term came from Latin American Spanish, where it had and still has the meaning "coffeehouse".		Instead of table service, there are food-serving counters/stalls, either in a line or allowing arbitrary walking paths. Customers take the food that they desire as they walk along, placing it on a tray. In addition, there are often stations where customers order food and wait while it is prepared, particularly for items such as hamburgers or tacos which must be served hot and can be immediately prepared. Alternatively, the patron is given a number and the item is brought to his table. For some food items and drinks, such as sodas, water, or the like, customers collect an empty container, pay at the check-out, and fill the container after the check-out. Free unlimited second servings are often allowed under this system. For legal purposes (and the consumption patterns of customers), this system is rarely, if at all, used for alcoholic beverages in the US.		Customers are either charged a flat rate for admission (as in a buffet) or pay at the check-out for each item. Some self-service cafeterias charge by the weight of items on a patron's plate. In universities and colleges, some students pay for three meals a day by making a single large payment for the entire semester.		As cafeterias require few employees, they are often found within a larger institution, catering to the clientele of that institution. For example, schools, colleges and their residence halls, department stores, hospitals, museums, places of worship, military bases, prisons, factories, and office buildings often have cafeterias. Although some of such institutions self-operate their cafeterias, many outsource their cafeterias to a food service management company or lease space to independent businesses to operate food service facilities. The three largest food service management companies servicing institutions are Aramark, Compass Group, and Sodexo.[2]		At one time, upscale cafeteria-style restaurants dominated the culture of the Southern United States, and to a lesser extent the Midwest. There were numerous prominent chains of them: Bickford's, Morrison's Cafeteria, Piccadilly Cafeteria, S&W Cafeteria, Apple House, Luby's, K&W, Britling, Wyatt's Cafeteria, and Blue Boar among them. Currently two Midwestern chains still exist, Sloppy Jo's Lunchroom and Manny's, which are both located in Illinois. There were also a number of smaller chains, usually located in and around a single city. These institutions, with the exception of K&W, went into a decline in the 1960s with the rise of fast food and were largely finished off in the 1980s by the rise of "casual dining". A few chains — particularly Luby's and Piccadilly Cafeterias (which took over the Morrison's chain in 1998) — continue to fill some of the gap left by the decline of the older chains. Some of the smaller Midwestern chains, such as MCL Cafeterias centered on Indianapolis, are still very much in business.						Perhaps the first self-service restaurant (not necessarily a cafeteria) in the US was the Exchange Buffet in New York City, opened September 4, 1885, which catered to an exclusively male clientele. Food was purchased at a counter, and patrons ate standing up.[3] This represents the predecessor of two formats: the cafeteria, described below, and the automat.		During the 1893 World's Columbian Exposition in Chicago, entrepreneur John Kruger built an American version of the smörgåsbords he had seen while traveling in Sweden. Emphasizing the simplicity and light fare, he called it the 'Cafeteria' - Spanish for 'coffee shop'. The exposition attracted over 27 million visitors (half the US population at the time) in six months, and because of Kruger's operation that America first heard the term and experienced the self-service dining format.[4][5]		Meanwhile, in mid-scale America, the chain of Childs Restaurants quickly grew from about 10 locations in New York City in 1890 to hundreds across the US and Canada by 1920. Childs is credited with the innovation of adding trays and a "tray line" to the self-service format, introduced in 1898 at their 130 Broadway location.[4][5] Childs did not change its format of sit-down dining, however. This was soon the standard design for most Childs Restaurants, and many ultimately the dominant design for cafeterias.		It has been conjectured that the 'cafeteria craze' started in May 1905, when Helen Mosher opened a downtown L.A. restaurant where people chose their food at a long counter and carried their trays to their tables.[6] California has a long history in the cafeteria format - notably the Boos Brothers Cafeterias, and the Clifton's, and Schaber's. The facts do not warrant the characterization that some have ascribed to the region. The earliest cafeterias in California were opened at least 12 years after Kruger's Cafeteria, and Childs already had many locations around the country. Horn & Hardart, an automat format chain (different from cafeterias), was well established in the mid-Atlantic region before 1900.		Between 1960 and 1981, the popularity of cafeterias was overcome by the fast food restaurant and fast casual restaurant formats.		Outside of the United States, the development of cafeterias can be observed in France as early as 1881 with the passing of the Ferry Law. This law mandated that public school education be available to all children. Accordingly, the government also encouraged schools to provide meals for students in need, thus resulting in the conception of cafeterias or cantine (in French). According to Abramson, prior to the creation of cafeterias, only some students were able to bring home-cooked meals and able to be properly fed in schools.		As cafeterias in France became more popular, their use spread beyond schools and into the workforce. Thus, due to pressure from workers and eventually new labor laws, sizable businesses had to, at minimum, provide established eating areas for its workers. Support for this practice was also reinforced by the effects of World War II when the importance of national health and nutrition came under great attention.[7]		A cafeteria in a U.S. military installation is known as a chow hall, a mess hall, a galley, mess decks or, more formally, a dining facility, often abbreviated to DFAC, whereas in common British Armed Forces parlance, it is known as a cookhouse or mess. Students in the USA often refer to cafeterias as lunchrooms, though breakfast as well as lunch is often eaten there.[citation needed] Some school cafeterias in the US have stages and movable seating that allow use as auditoriums. These rooms are known as cafetoriums. Cafeterias serving university dormitories are sometimes called dining halls or dining commons. A food court is a type of cafeteria found in many shopping malls and airports featuring multiple food vendors or concessions, although a food court could equally be styled as a type of restaurant as well, being more aligned with public, rather than institutionalised, dining.		Some monasteries, boarding schools, and older universities refer to their cafeteria as a refectory. Modern-day British cathedrals and abbeys, notably in the Church of England, often use the phrase refectory to describe a cafeteria open to the public. Historically, the refectory was generally only used by monks and priests. For example, although the original 800-year-old refectory at Gloucester Cathedral (the stage setting for dining scenes in the Harry Potter movies) is now mostly used as a choir practice area, the relatively modern 300-year-old extension, now used as a cafeteria by staff and public alike, is today referred to as the refectory.[8]		A cafeteria located in a TV studio is often called a commissary. NBC's commissary, The Hungry Peacock, was often joked about by Johnny Carson on The Tonight Show.		In American English, a college cafeteria is a cafeteria intended for college students. In British English it is often called the refectory. These cafeterias can be a part of a residence hall or in a separate building.[citation needed] Many of these colleges employ their own students to work in the cafeteria.[citation needed] The number of meals served to students varies from school to school, but is normally around 21 meals per week.[citation needed] Like normal cafeterias, a person will have a tray to select the food that he or she wants, but (at some campuses) instead of paying money, pays beforehand by purchasing a meal plan.[citation needed]		The method of payment for college cafeterias is commonly in the form of a meal plan, whereby the patron pays a certain amount at the start of the semester and details of the plan are stored on a computer system. Student ID cards are then used to access the meal plan. Meal plans can vary widely in their details and are often not necessary to eat at a college cafeteria. Typically, the college tracks students' usage of their plan by counting the number of predefined meal servings, points, dollars, or number of buffet dinners. The plan may give the student a certain number of any of the above per week or semester and they may or may not roll over to the next week or semester.[9]		Many schools offer several different options for using their meal plans. The main cafeteria is usually where most of the meal plan is used but smaller cafeterias, cafés, restaurants, bars, or even fast food chains located on campus, on nearby streets, or in the surrounding town or city may accept meal plans. A college cafeteria system often has a virtual monopoly on the students due to an isolated location or a requirement that residence contracts include a full meal plan.[citation needed]		
A platter is a meal or course served on a platter.		In restaurant terminology, a platter is often a main dish served on a platter with one or more side dishes, such as a salad or french fries.		Notable platters includes the Colombian bandeja paisa, Indian thali or Arabic mixed-meat platters.		
An entremet or entremets (/ˈɑːntrəmeɪ/; French: [ɑ̃tʁəmɛ]; from Old French, literally meaning "between servings") is in modern French cuisine a small dish served between courses or simply a dessert. Originally it was an elaborate form of entertainment dish common among the nobility and upper middle class in Europe during the later part of the Middle Ages and the early modern period. An entremet marked the end of a serving of courses and could be anything from a simple frumenty (a type of wheat porridge) that was brightly colored and flavored with exotic and expensive spices to elaborate models of castles complete with wine fountains, musicians, and food modeled into allegorical scenes. By the end of the Middle Ages, it had evolved almost entirely into dinner entertainment in the form of inedible ornaments or acted performances, often packed with symbolism of power and regality. In English it was more commonly known as a subtlety (also sotelty or soteltie) and did not include acted entertainment.		For modern pastry chefs, an entremet is a multi-layered mousse-based cake with various complementary flavors and varying textural contrasts.[1][clarification needed]						Dishes that were intended to be eaten as well as entertain can be traced back at least to the early Roman Empire. In his Satyricon, the Roman writer Petronius describes a dish consisting of a rabbit dressed to look like the mythical horse Pegasus.[2] The function of the entremet was to mark the end of a course, met (a serving of several dishes), of which there could be several at a banquet. It punctuated each stage of a banquet, prepared the diners for the next serving and functioned as a conversation piece. The earliest recipe for an entremet can be found in an edition of Le Viandier, a medieval recipe collection from the early 14th century. It described a comparatively simple dish: boiled and fried chicken liver with chopped giblet, ground ginger, cinnamon, cloves, wine, verjuice, beef bouillon and egg yolks, served with cinnamon on top, and was supposed to be of a bright yellow color. An even simpler dish, like millet boiled in milk and seasoned with saffron, was also considered to be an entremet.[3]		The most noticeable trait of the early entremets was the focus on vivid colors. Later on, the entremets would take the shape of various types of illusion foods, such as peacocks or swans that were skinned, cooked, seasoned and then redressed in their original plumage (or filled with the meat of tastier fowl) or even scenes depicting contemporary human activities, such as a knight in the form of a grilled capon equipped with a paper helmet and lance, sitting on the back of a roast piglet.[4] Elaborate models of castles made from edible material was a popular theme. At a feast in 1343 dedicated to Pope Clement VI, one of the Avignon popes, one of the entremets was a castle with walls made from roast birds, populated with cooked and redressed deer, wild boar, goat, hare and rabbit.[5]		In the 14th century entremets began to involve not just eye-catching displays of amusing haute cuisine, but also more prominent and often highly symbolic forms of inedible entertainment. In 1306, the knighting of the son of Edward I included performances of chansons de geste in what has been assumed to be part of the entremets.[6] During the course of the 14th century they would often take on the character of theatrical displays, complete with props, actors, singers, mummers and dancers. At a banquet held in 1378 by Charles V of France in honor of Emperor Charles IV, a huge wooden model of the city of Jerusalem was rolled in before the high table. Actors portraying the crusader Godfrey of Bouillon and his knights then sailed into the hall on a miniature ship and reenacted the capture of Jerusalem in 1099.[7]		From the late 14th century on in England, entremets are referred to as subtleties. This English term was derived from an older meaning of "subtle" as "clever" or "surprising".[8] The meaning of "subtlety" did not include entertainment involving actors; those were referred to as pageants.[9] The "four and twenty blackbirds baked in a pie", in the nursery rhyme "Sing a Song of Sixpence", has its genesis in an entremet presented to amuse banquet guests in the 14th century. This extravaganza of hospitality was related by an Italian cook of the era.[10] “Live birds were slipped into a baked pie shell through a hole cut in its bottom.” The unwary guest would release the flapping birds once the upper crust was cut into.[11]		At the end of the Middle Ages, the level of refinement among the noble and royal courts of Europe had increased considerably, and the demands of powerful hosts and their rich dinner guests resulted in ever more complicated and elaborate creations. Chiquart, cook to Amadeus VIII, Duke of Savoy, described an entremet entitled Castle of Love in his 15th-century culinary treatise Du fait de cuisine ("On cookery"). It consisted of a giant castle model with four towers, carried in by four men. The castle contained, among other things, a roast piglet, a swan cooked and redressed in its own plumage, a roast boar's head and a pike cooked and sauced in three different ways without having been cut into pieces, all of them breathing fire.[12] The battlements of the castle were adorned with the banners of the Duke and his guests, manned by miniature archers, and inside the castle there was a fountain that gushed rosewater and spiced wine.[13] Noteworthy for its entertainment value to the assembled nobles of the time, the 17th century provided a memorable banquet event courtesy of the host, the Duke of Buckingham. In honor of his royal guests, Charles I and Queen Henrietta Maria, a pie was prepared concealing a human being—famous dwarf of the era, Jeffrey Hudson. [14]		Entremets also made an effective tool for political displays. One of the most famous examples is the so-called Feast of the Pheasant, arranged by Philip the Good of Burgundy in 1454. The theme of the banquet was the fall of Constantinople to the Ottoman Turks in 1453, and included a vow by Philip and his guests to retake the city in a crusade, though this was never realized. There were several spectacular displays at the banquet referred to by contemporary witnesses as entremets. Guests were entertained by a wide range of extravagant displays of automatons in the form of fountains and pies containing musicians. At the end of the banquet, an actor representing the Holy Church rode in on an elephant and read a poem about the plight of Eastern Christianity under Ottoman rule.[9]		
Hunger and satiety are sensations. Hunger represents the physiological need to eat food. Satiety is the absence of hunger; it is the sensation of feeling full.[1]		Appetite is another sensation experienced with eating; it is the desire to eat food. There are several theories about how the feeling of hunger arises. A healthy, well-nourished individual can survive for weeks without food intake, with claims ranging from three to ten weeks.[2][3] The sensation of hunger typically manifests after only a few hours without eating and is generally considered to be unpleasant. Satiety occurs between 5 and 20 minutes after eating.[4]		Hunger is also the most commonly used term to describe the condition of people who suffer from a chronic lack of sufficient food and constantly or frequently experience the sensation of hunger.						When hunger contractions start to occur in the stomach, they are informally referred to as hunger pangs. Hunger pangs usually do not begin until 12 to 24 hours after the last ingestion of food. A single hunger contraction lasts about 30 seconds, and pangs continue for around 30 to 45 minutes, then hunger subsides for around 30 to 150 minutes.[5] Individual contractions are separated at first, but are almost continuous after a certain amount of time.[5] Emotional states (anger, joy etc.) may inhibit hunger contractions.[5] Levels of hunger are increased by lower blood sugar levels, and are higher in diabetics.[5] They reach their greatest intensity in three to four days and may weaken in the succeeding days[citation needed], although research suggests that hunger never disappears.[6] Hunger contractions are most intense in young, healthy people who have high degrees of gastrointestinal tonus. Periods between contractions increase with old age.[5]		The fluctuation of leptin and ghrelin hormone levels results in the motivation of an organism to consume food. When an organism eats, adipocytes trigger the release of leptin into the body.[citation needed] Increasing levels of leptin result in a reduction of one's motivation to eat.[citation needed] After hours of non-consumption, leptin levels drop significantly. These low levels of leptin cause the release of a secondary hormone, ghrelin, which in turn reinitiates the feeling of hunger.		Some studies have suggested that an increased production of ghrelin may enhance appetite evoked by the sight of food, while an increase in stress may also influence the hormone's production.[7] These findings may help to explain why hunger can prevail even in stressful situations.		The state achieved when the need for food has been satisfied is called satiety. The satiety center in animals is located in the ventromedial nucleus of the hypothalamus.		Short-term regulation of hunger and food intake involves neural signals from the GI tract, blood levels of nutrients, GI tract hormones, and psychological factors.		One method that the brain uses to evaluate the contents of the gut is through vagal nerve fibers that carry signals between the brain and the gastrointestinal tract (GI tract). Studies have shown that through these vagal nerve fibers, the brain can sense a difference between different macronutrients. Stretch receptors work to inhibit appetite upon distention of the GI tract by sending signals along the vagus nerve afferent pathway and inhibiting the hunger center.[8]		Blood levels of glucose, amino acids, and fatty acids provide a constant flow of information to the brain that may be linked to regulating hunger and energy intake. Nutrient signals that indicate fullness, and therefore inhibit hunger include the following:[9]		The hormones insulin and cholecystokinin (CCK) are released from the GI tract during food absorption and act to suppress feeling of hunger. CCK is key in suppressing hunger because of its role in inhibiting neuropeptide Y. Glucagon and epinephrine levels rise during fasting and stimulate hunger. Ghrelin, a hormone produced by the stomach, is a hunger stimulant.[8]		Psychological states appear to play a role in short-term food intake. Merely repeatedly imagining the consumption of a food, for example, can reduce the subsequent actual consumption of that food by reducing the motivation to consume it.[10] Two psychological processes appear to be involved in regulating short-term food intake: liking and wanting. Liking refers to the palatability or taste of the food, which is reduced by repeated consumption. Wanting is the motivation to consume the food, which is also reduced by repeated consumption of a food[11] and may be due to change in memory-related processes.[12] Wanting can be triggered by a variety of psychological processes. Thoughts of a food may intrude on consciousness and be elaborated on, for instance, as when one sees a commercial or smells a desirable food.[13] Eating one food can induce a craving for its complements, foods that are perceived to add pleasure to the consumption of that food, by priming a goal to consume those foods.[14] Participants who drank a sip of cola, for example, were subsequently willing to pay more for a voucher for a cheeseburger that they could redeem later than controls who did not drink the cola.		Leptin, a hormone secreted exclusively by adipose cells in response to an increase in body fat mass, is an important component in the regulation of long term hunger and food intake. Leptin serves as the brain's indicator of the body's total energy stores. When leptin levels rise in the bloodstream they bind to receptors in ARC. The functions of leptin are to:		Though rising blood levels of leptin do promote weight loss to some extent, its main role is to protect the body against weight loss in times of nutritional deprivation. Other factors also have been shown to effect long-term hunger and food intake regulation including insulin.[8]		The decision to eat a food again in the future appears to be driven by several psychological factors generally related to memory of the previous consumption episode. People have a better memory for their enjoyment of a food at the end than at the beginning of a past consumption episode - they better remember the last bite than the first. As a consequence, their enjoyment of the last bite of a food (which is usually less enjoyable than the first) is the best predictor of how soon again they wish to eat that food in the future.[15] This greater weighting of the last bite than the first bite is due to an interference in memory, whereby memories for later bites make it harder to remember enjoyment of earlier bites of the food.[16]		The set-point theories of hunger and eating are a group of theories developed in the 1940s and 1950s that operate under the assumption that hunger is the result of an energy deficit and that eating is a means by which energy resources are returned to their optimal level, or energy set-point. According to this assumption, a person's energy resources are thought to be at or near their set-point soon after eating, and are thought to decline after that. Once the person's energy levels fall below a certain threshold, the sensation of hunger is experienced, which is the body's way of motivating the person to eat again. The set-point assumption is a negative feedback mechanism.[17] Two popular set-point theories include the glucostatic set-point theory and the lipostatic set-point theory.		The set-point theories of hunger and eating present a number of weaknesses.[18]		The positive-incentive perspective is an umbrella term for a set of theories presented as an alternative to the set-point theories of hunger and eating.[22] The central assertion to the positive-incentive perspective is the idea that humans and other animals are not normally motivated to eat by energy deficits, but are instead motivated to eat by the anticipated pleasure of eating, or the positive-incentive value.[23] According to this perspective, eating is controlled in much the same way as sexual behavior. Humans engage in sexual behavior, not because of an internal deficit, but instead because they have evolved to crave it. Similarly, the evolutionary pressures of unexpected food shortages have shaped humans and all other warm blooded animals to take advantage of food when it is present. It is the presence of good food, or the mere anticipation of it that makes one hungry.[19]		Prior to consuming a meal, the body's energy reserves are in reasonable homeostatic balance. However, when a meal is consumed, there is a homeostasis-disturbing influx of fuels into the bloodstream. When the usual mealtime approaches, the body takes steps to soften the impact of the homeostasis-disturbing influx of fuels by releasing insulin into the blood, and lowering the blood glucose levels. It is this lowering of blood glucose levels that causes premeal hunger, and not necessarily an energy deficit.[24][25][26]		Hunger appears to increase activity and movement in many animals - for example, an experiment on spiders showed increased activity and predation in starved spiders, resulting in larger weight gain.[27] This pattern is seen in many animals, including humans while sleeping.[28] It even occurs in rats with their cerebral cortex or stomachs completely removed.[29] Increased activity on hamster wheels occurred when rats were deprived not only of food, but also water or B vitamins such as thiamine.[30] This response may increase the animal's chance of finding food, though it has also been speculated the reaction relieves pressure on the home population.[28] There is also a difference between the neurological responses in human males and females in response to hunger and satiety.[31][32]		A food craving is an intense desire to consume a specific food, as opposed to general hunger. Similarly, thirst is the craving for water.		
Dessert (/dᵻˈzɜːrt/) is a confectionery course that concludes a main meal. The course usually consists of sweet foods and beverages, such as dessert wine or liqueurs, but may include coffee, cheeses, nuts, or other savory items. In some parts of the world, such as much of central and western Africa, and most parts of China, there is no tradition of a dessert course to conclude a meal.		The term "dessert" can apply to many confections, such as cakes, tarts, cookies, biscuits, gelatins, pastries, ice creams, pies, puddings, custards, and sweet soups. Fruit is also commonly found in dessert courses because of its naturally occurring sweetness. Some cultures sweeten foods that are more commonly savory to create desserts.						The word "dessert" originated from the French word desservir, meaning "to clear the table."[1] Its first known use was in 1600, in a health education manual entitled Naturall and artificial Directions for Health, which was written by William Vaughan.[2][3] In his A History of Dessert (2013), Michael Krondl explains it refers to the fact dessert was served after the table had been cleared of other dishes.[4] The term dates from the 14th century but attained its current meaning around the beginning of the 20th century when "service à la française" (setting a variety of dishes on the table at the same time) was replaced with "service à la russe" (presenting a meal in courses.)"[4]		The word "dessert" is most commonly used for this course in the United States, Canada, Australia, New Zealand and Ireland whilst "pudding" is more commonly used in the United Kingdom. Alternatives such as "sweets" or "afters" are also used in the United Kingdom[5] and some other Commonwealth countries, including Hong Kong, and India.[citation needed]		Sweets were fed to the gods in ancient Mesopotamia[6]:6 and India[6]:16 and other ancient civilizations.[7] Dried fruit and honey were probably the first sweeteners used in most of the world, but the spread of sugarcane around the world was essential to the development of dessert.[6]:13		Sugarcane was grown and refined in India before 500 BCE[6]:26 and was crystallized, making it easy to transport, by 500 CE. Sugar and sugarcane were traded, making sugar available to Macedonia by 300 BCE and China by 600 CE. In South Asia, the Middle East and China, sugar has been a staple of cooking and desserts for over a thousand years. Sugarcane and sugar were little known and rare in Europe until the twelfth century or later, when the Crusades and then colonialization spread its use.		Herodotus mentions that, as opposed to the Greeks, the main Persian meal was simple, but they would eat many desserts afterwards.[8][9]		Europeans began to manufacture sugar in the Middle Ages, and more sweet desserts became available.[10] Even then sugar was so expensive usually only the wealthy could indulge on special occasions. The first apple pie recipe was published in 1381.[11] The earliest documentation of the term cupcake was in "Seventy-five Receipts for Pastry, Cakes, and Sweetmeats" in 1828 in Eliza Leslie's Receipts cookbook.[12]		The Industrial Revolution in America and Europe caused desserts (and food in general) to be mass-produced, processed, preserved, canned, and packaged. Frozen foods, including desserts, became very popular starting in the 1920s when freezing emerged. These processed foods became a large part of diets in many industrialized nations. Many countries have desserts and foods distinctive to their nations or region.[13]		Sweet desserts usually contain cane sugar, palm sugar, honey or some types of syrup such as molasses, maple syrup, treacle, or corn syrup. Other common ingredients in Western-style desserts are flour or other starches, Cooking fats such as butter or lard, dairy, eggs, salt, acidic ingredients such as lemon juice, and spices and other flavoring agents such as chocolate, peanut butter, fruits, and nuts. The proportions of these ingredients, along with the preparation methods, play a major part in the consistency, texture, and flavor of the end product.		Sugars contribute moisture and tenderness to baked goods. Flour or starch components serves as a protein and gives the dessert structure. Fats contribute moisture and can enable the development of flaky layers in pastries and pie crusts. The dairy products in baked goods keep the desserts moist. Many desserts also contain eggs, in order to form custard or to aid in the rising and thickening of a cake-like substance. Egg yolks specifically contribute to the richness of desserts. Egg whites can act as a leavening agent[14] or provide structure. Further innovation in the healthy eating movement has led to more information being available about vegan and gluten-free substitutes for the standard ingredients, as well as replacements for refined sugar. Desserts can contain many spices and extracts to add a variety of flavors. Salt and acids are added to desserts to balance sweet flavors and create a contrast in flavors.		Some desserts are made with coffee,a coffee-flavoured version of a dessert can be made, for example an iced coffee soufflé or coffee biscuits.[15] Alcohol can also be used as an ingredient, to make alcoholic desserts.[16]		Dessert consist of variations of flavors, textures, and appearances. Desserts can be defined as a usually sweeter course that concludes a meal.[1] This definition includes a range of courses ranging from fruits or dried nuts to multi-ingredient cakes and pies. Many cultures have different variations of dessert. In modern times the variations of desserts have usually been passed down or come from geographical regions. This is one cause for the variation of desserts. These are some major categories in which desserts can be placed.[4]		Biscuits, (from the Old French word bescuit originally meaning twice-baked in Latin,[17][n 1] also known as "cookies" in North America, are flattish bite-sized or larger short pastries generally intended to be eaten out of the hand. Biscuits can have a texture that is crispy, chewy, or soft. Examples include layered bars, crispy meringues, and soft chocolate chip cookies.		Cakes are sweet tender breads made with sugar and delicate flour. Cakes can vary from light, airy sponge cakes to dense cakes with less flour. Common flavourings include dried, candied or fresh fruit, nuts, cocoa or extracts. They may be filled with fruit preserves or dessert sauces (like pastry cream), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit. Cake is often served as a celebratory dish on ceremonial occasions, for example weddings, anniversaries, and birthdays. Small-sized cakes have become popular, in the form of cupcakes and petits fours.		Chocolate is a typically sweet, usually brown, food preparation of Theobroma cacao seeds, roasted, ground, and often flavored. Pure, unsweetened chocolate contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate currently consumed is in the form of sweet chocolate, combining chocolate with sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk, but no cocoa solids. Dark chocolate is produced by adding fat and sugar to the cacao mixture, with no milk or much less than milk chocolate.		Candy, also called sweets or lollies, is a confection that features sugar as a principal ingredient. Many candies involve the crystallization of sugar which varies the texture of sugar crystals. Candies comprise many forms including caramel, marshmallows, and taffy.		These kinds of desserts usually include a thickened dairy base. Custards are cooked and thickened with eggs. Baked custards include crème brûlée and flan. Puddings are thickened with starches such as cornstarch or tapioca.[18] Custards and puddings are often used as ingredients in other desserts, for instance as a filling for pastries or pies.		Many cuisines include a dessert made of deep-fried starch-based batter or dough. In many countries a doughnut is a flour-based batter that has been deep-fried. It is sometimes filled with custard or jelly. Fritters are fruit pieces in a thick batter that have been deep fried. Gulab jamun is an Indian dessert made of milk solids kneaded into a dough, deep-fried, and soaked in honey. Churros are a deep-fried and sugared dough that is eaten as dessert or a snack in many countries. Doughnuts are most famous for being a trademark favorite of fictional character Homer Simpson from the animated television series The Simpsons.[19]		Ice cream, gelato, sorbet and shaved-ice desserts fit into this category. Ice cream is a cream base that is churned as it is frozen to create a creamy consistency. Gelato uses a milk base and has less air whipped in than ice cream, making it denser. Sorbet is made from churned fruit and is not dairy based. Shaved-ice desserts are made by shaving a block of ice and adding flavored syrup or juice to the ice shavings.		Jellied desserts are made with a sweetened liquid thickened with gelatin or another thickening agent. They are traditional in many cultures. Grass jelly and annin tofu are Chinese jellied desserts. Yōkan is a Japanese jellied dessert. In English-speaking countries, many dessert recipes are based on gelatin with fruit or whipped cream added.		Pastries are sweet baked pastry products. Pastries can either take the form of light and flaky bread with an airy texture, such as a croissant or unleavened dough with a high fat content and crispy texture, such as shortbread. Pastries are often flavored or filled with fruits, chocolate, nuts, and spices. Pastries are sometimes eaten with tea or coffee as a breakfast food.		Pies and cobblers are a crust with a filling. The crust can be either made from either a pastry or crumbs. Pie fillings range from fruits to puddings; cobbler fillings are generally fruit-based. Clafoutis are a batter with fruit-based filling poured over the top before baking.		Tong sui, literally translated as "sugar water" and also known as tim tong, is a collective term for any sweet, warm soup or custard served as a dessert at the end of a meal in Cantonese cuisine. Tong sui are a Cantonese specialty and are rarely found in other regional cuisines of China. Outside of Cantonese-speaking communities, soupy desserts generally are not recognized as a distinct category, and the term tong sui is not used.		Dessert wines are sweet wines typically served with dessert. There is no simple definition of a dessert wine. In the UK, a dessert wine is considered to be any sweet wine drunk with a meal, as opposed to the white[20] fortified wines (fino and amontillado sherry) drunk before the meal, and the red fortified wines (port and madeira) drunk after it. Thus, most fortified wines are regarded as distinct from dessert wines, but some of the less strong fortified white wines, such as Pedro Ximénez sherry and Muscat de Beaumes-de-Venise, are regarded as honorary dessert wines. In the United States, by contrast, a dessert wine is legally defined as any wine over 14% alcohol by volume, which includes all fortified wines - and is taxed at higher rates as a result. Examples include Sauternes and Tokaji Aszú.		Baked Alaska, ice cream and cake topped with browned meringue		Baklava, a pastry comprising layers of filo with chopped nuts, sweetened and held together with syrup or honey		Bananas Foster, made from bananas and vanilla ice cream with a sauce made from butter, brown sugar, cinnamon, dark rum and banana liqueur		Cheesecake, a type of dessert with a layer of a mixture of soft, fresh cheese, eggs and sugar		Chocolate mousse, a chocolate variety of a dessert incorporating air bubbles to give it a light and airy texture		Coconut bar, made with coconut milk and set with either tang flour and corn starch, or agar agar and gelatin		Preparation of crème brûlée, a rich custard base topped with a contrasting layer of hard caramel		Egg custard tarts, a pastry originating from Guangzhou, China.		Gyeongju bread, a small pastry with a filling of red bean paste		Hotteok (a variety of filled Korean pancake) with edible seeds, sugar, and cinnamon		Kkultarae, fine strands of honey and maltose, often with a sweet nut filling		Jell-o cream cheese square		Gulab jamun decorated with silver foil and almond chips.		Lemon tart, a pastry shell with a lemon-flavored filling		An assortment of pastries		Rum cake, a type of cake containing rum		Spotted Dick		Dessert Dish		Throughout much of central and western Africa, there is no tradition of a dessert course following a meal.[21][22] Fruit or fruit salad would be eaten instead, which may be spiced, or sweetened with a sauce. In some former colonies in the region, the colonial power has influenced desserts – for example, the Angolian cocada amarela (yellow coconut) resembles baked desserts in Portugal.[22]		In Asia, desserts are often eaten between meals as snacks rather than as a concluding course. There is widespread use of rice flour in East Asian desserts, which often include local ingredients such as coconut milk, palm sugar, and tropical fruit.[23] In India, where sugarcane has been grown and refined since before 500 BCE, desserts have been an important part of the diet for thousands of years; types of desserts include burfis, halvahs, jalebis, and laddus.[6]:37		In Ukraine and Russia, breakfast foods such as nalysnyky or blintz or oladi (pancakes), and syrniki are served with honey and jam as desserts.		European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles. The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants.		Dulce de leche is a very common confection in Argentina.[24] In Bolivia, sugarcane, honey and coconut are traditionally used in desserts.[25] Tawa tawa is a Bolivian sweet fritter prepared using sugar cane, and helado de canela is a dessert that is similar to sherbet which is prepared with cane sugar and cinnamon.[25] Coconut tarts, puddings cookies and candies are also consumed in Bolivia.[25] Brazil has a variety of candies such as brigadeiros (chocolate fudge balls), cocada (a coconut sweet), beijinhos (coconut truffles and clove) and romeu e julieta (cheese with a guava jam known as goiabada). Peanuts are used to make paçoca, rapadura and pé-de-moleque. Local common fruits are turned in juices and used to make chocolates, popsicles and ice cream.[26] In Chile, kuchen has been described as a "trademark dessert."[27] Several desserts in Chile are prepared with manjar, (caramelized milk), including alfajor, flan, cuchufli and arroz con leche.[27] Desserts consumed in Colombia include dulce de leche, waffle cookies,[28] puddings, nougat, coconut with syrup and thickened milk with sugarcane syrup.[29] Desserts in Ecuador tend to be simple, and desserts are a moderate part of the cuisine.[30] Desserts consumed in Ecuador include tres leches cake, flan, candies and various sweets.[30]		Desserts are typically eaten in Australia, and most daily meals "end with simple desserts," which can include various fruits.[31] More complex desserts include cakes, pies and cookies, which are sometimes served during special occasions.[31]		The market for desserts has grown over the last few decades, which was greatly increased by the commercialism of baking desserts and the rise of food productions. Desserts are present in most restaurants as the popularity has increased. Many commercial stores have been established as solely desserts stores. Ice cream parlors have been around since before 1800.[32] Many businesses started advertising campaigns focusing solely on desserts. The tactics used to market desserts are very different depending on the audience for example desserts can be advertised with popular movie characters to target children.[33] The rise of companies like Food Network has marketed many shows which feature dessert and their creation. Shows like these have displayed extreme desserts and made a game show atmosphere which made desserts a more competitive field.[34]		Desserts are a standard staple in restaurant menus, with different degrees of variety. Pie and cheesecake were among the most popular dessert courses ordered in U.S. restaurants in 2012.[35]		Dessert foods often contain relatively high amounts of sugar and fats and, as a result, higher calorie counts per gram than other foods. Fresh or cooked fruit with minimal added sugar or fat is an exception.[36]		
A serving size or portion size is the amount of a food or drink that is generally served. Serving size is a significant consideration in nutrition and weight control.[1]		A distinction is made between a portion size as determined by an external agent, such as a food manufacturer, chef, or restaurant, and a 'self selected portion size' in which an individual has control over the portion in a meal or snack.[2] Self-selected portion size is determined by several factors such as the palatability of a food and the extent to which it is expected to reduce hunger and to generate fullness (see expected satiety).		Evidence from a systematic review of 72 randomized controlled trials indicates that people consistently eat more food when offered larger portion, package, or tableware sizes rather than smaller size alternatives.[3]						Bulk products, such as sugar, generally have sizes in common units of measurement, such as the cup or tablespoon. Commonly divided products, such as pie or cake, have a serving size given in a fraction of the whole product (e.g., 1/8 cake). Products which are sliced beforehand or are bought in distinct, grouped units (such as olives), are listed in the approximate number of units corresponding to the reference amount. For example, if the reference amount for olives were 30 g, and one olive weighed 10 g, the serving size would probably be listed as three olives.		It is found both on the Food Pyramid and its successor program MyPlate and on Nutrition Labels and has two related but differing meanings. The USDA Center for Nutrition Policy and Promotion sets the standards for My Plate and related guidelines. The FDA defines the "Reference Amounts Customarily Consumed" (RACC) tables used by food manufacturers to determine the serving size on the Nutrition Facts Panel, and the USDA Food Safety and Inspection Services labels.[4]		The nutrition facts label is designed to give consumers important nutritional information about a product and allow comparisons with other food. The serving size indicates the amount of food for which the nutrition information is shown. RACCs were established by regulation in 1993 in response to the Nutrition Labeling and Education Act and were based on how much food people typically eat, balanced with desired portion size. Ice cream is the classic example where the RACC is 1/2 cup, but people more often consume more.		
An entrée (/ˈɑːntreɪ/ /ˈɒntreɪ/ AHN-tray; French for "entrance", pronounced [ɑ̃tʁe]) refers to types of dishes.		In French cuisine, as well as in the English-speaking world (save for the United States and parts of Canada), it is a dish served before the main course of a meal.[1][2]		In North American English, it is the main dish of a meal.[1][3][4]						Marie-Antoine Carême explained for a French readership the order of courses in the state dinner à la russe served for Tsar Alexander I's review of his troops in 1815, at an isolated location far from Paris, under trying circumstances:		Russian service is carried out rapidly and warmly; first, oysters are served; after the soup, hors d'oeuvres; then the large joint of meat; then the entrées of fish, fowl, game, meat, and the entremets of vegetables; then the roast meat with salad. The service ends with the desserts: jellies, creams and soufflés.[5]		In Mrs Beeton's Book of Household Management, bills of fare for a grand dinner for eighteen[6] follow two kinds of fish and two kinds of soup with four entrées: ris de veau, poulet à la Marengo, côtelettes de porc, and ragoût of lobster. Guests were not expected to eat of each dish, for the entrées were followed by a second course and a third course, of game and fruit.		In 1961 Julia Child and her co-authors[7] outlined the character of such entrées, which—when they did not precede a roast—might serve as the main course of a luncheon, in a chapter of "Entrées and Luncheon Dishes" that included quiches, tarts and gratins, soufflés and timbales, gnocchi, quenelles, and crêpes.		In 1970, Richard Olney, an American living in Paris, gave the place of the entrée in a French full menu: "A dinner that begins with a soup and runs through a fish course, an entrée, a sorbet, a roast, salad, cheese and dessert, and that may be accompanied by from three to six wines, presents a special problem of orchestration".[8]		An entrée is more substantial than hors d'œuvres and better thought of as a half-sized version of a main course. Restaurant menus will sometimes offer the same dish in different-sized servings as both entrée and main course.		Entrée (or entree) is generally used in the United States and Canada (except Quebec) as the name of the main course.[9] English-speaking Québécois follow the French use of the term.		According to linguist Dan Jurafsky, North American usage retains the original French meaning of a substantial meat course.[10]		The word entrée in French originally denoted the "entry" of the dishes from the kitchens into the dining hall. In the illustration from a French fifteenth-century illuminated manuscript of the Histoire d'Olivier de Castille et d'Artus d'Algarbe, a fanfare from trumpeters in the musicians' gallery announces the processional entrée of a series of dishes preceded by a covered cup that is the ancestor of the tureen, carried by the maître d'hôtel. The entrée will be shown round the hall but served only to the high table (though it does not stand on a dais in this hall), where the guests are set apart by a gold-and-crimson damask canopy of estate.		In traditional French haute cuisine, the entrée preceded a larger dish known as the relevé, which "replaces" or "relieves" it, an obsolete term in modern cooking, but still used as late as 1921 in Escoffier's Le Guide Culinaire.		In France, the modern restaurant menu meaning of "entrée" is the course that precedes the main course in a three-course meal,[11] i.e. the course which in British usage is often called the "starter" and in American usage the "appetizer". Thus a typical modern French three-course meal in a restaurant consists of "entrée" (first course, starter (UK), appetizer (U.S.)) followed by the "plat" or "plat principal" (the main course) and then dessert or cheese. This procession is commonly found in prix fixe menus.		
A crop is "a plant or animal product that can be grown and harvested extensively for profit or subsistence."[1] Crop may refer either to the harvested parts or to the harvest in a more refined state (husked, shelled, etc.). Most crops are cultivated in agriculture or aquaculture. A crop is usually expanded to include macroscopic fungus (e.g. mushrooms), or alga (algaculture).		Most crops are harvested as food for humans or livestock (fodder crops). Some crops are gathered from the wild (including intensive gathering, e.g. ginseng).		Important non-food crops include horticulture, floriculture and industrial crops. Horticulture crops include plants used for other crops (e.g. fruit trees). Floriculture crops include bedding plants, houseplants, flowering garden and pot plants, cut cultivated greens, and cut flowers. Industrial crops are produced for clothing (fiber crops), biofuel (energy crops, algae fuel), or medicine (medicinal plants).		Animals and microbes (fungi, bacteria or viruses) are rarely referred to as crops. Animals raised for human or animal consumption are referred to as livestock and microbes as microbiological cultures. Microbes are not typically grown for food itself, but are rather used to alter food (e.g., producing citric acid, fermenting yogurt, soy sauce, or sauerkraut).						The importance of a crop varies greatly by region. Globally, the following crops contribute most to human food supply (values of kcal/person/day for 2013 given in parentheses): Rice (541 kcal), wheat (527 kcal), sugarcane and other sugar crops (200 kcal), maize (corn) (147 kcal), soybean oil (82 kcal), other vegetables (74 kcal), potatoes (64 kcal), palm oil (52 kcal), cassava (37 kcal), legume pulses (37 kcal), sunflowerseed oil (35 kcal), rape and mustard oil (34 kcal), other fruits, (31 kcal), sorghum (28 kcal), millet (27 kcal), groundnuts (25 kcal), beans (23 kcal), sweet potatoes (22 kcal), bananas (21 kcal), various nuts (16 kcal), soyabeans (14 kcal), cottonseed oil (13 kcal), groundnut oil (13 kcal), yams (13 kcal).[2] Note that many of the globally apparently minor crops are regionally very important. For example in Africa, roots & tubers dominate with 421 kcal/person/day, and sorghum and millet contribute 135 kcal and 90 kcal, respectively.[2]		In terms of produced weight, the following crops are the most important ones (global production in thousand metric tonnes):[3]		Media related to Crops at Wikimedia Commons		
A buffet (IPA: [ˈbʊfeɪ] in the UK, IPA: [bəˈfeɪ] in the US, from French: sideboard) is a system of serving meals in which food is placed in a public area where the diners generally serve themselves.[1] Buffets are offered at various places including hotels, restaurants and many social events. Buffet restaurants normally offer all-you-can-eat (AYCE) food for a set price. Buffets usually have some hot dishes, so the term cold buffet (see Smörgåsbord) has been developed to describe formats lacking hot food. Hot or cold buffets usually involve dishware and utensils, but a finger buffet is an array of foods that are designed to be small and easily consumed only by hand, including cupcakes, slices of pizza, foods on cocktail sticks, etc.		The essential feature of the various buffet formats is that the diners can directly view the food and immediately select which dishes they wish to consume, and usually also can decide how much food they take. Buffets are effective for serving large numbers of people at once, and are often seen in institutional settings, such as business conventions or large parties.						Since a buffet involves diners serving themselves, it has in the past been considered an informal form of dining, less formal than table service. In recent years, however, buffet meals are increasingly popular among hosts of home dinner parties, especially in homes where limited space complicates the serving of individual table places.		The buffet table originates from the brännvinsbord (Swedish schnapps, or shot of alcoholic beverage)[2] table from the middle of 16th century. This custom had its prime during the early 18th century, and was developed into the more modern buffet around the beginning of 19th century. The smörgåsbord buffet did not increase in popularity until the expansion of the railroads throughout Europe.[citation needed]		The smörgåsbord table was originally a meal where guests gathered before dinner for a pre-dinner drink, and was not part of the formal dinner that followed. The smörgåsbord buffet was often held in separate rooms for men and women before the dinner was served.[3]		Smörgåsbord became internationally known as "smorgasbord" at the 1939 New York World's Fair exhibition, as the Swedes had to invent a new way of showcasing the best of Swedish food to large numbers of visitors.[citation needed]		The term buffet originally referred to the French sideboard furniture where the food was served, but eventually became applied to the serving format. The word "buffet" became popular in the English-speaking world in the second half of the 20th century after the Swedes had popularized the "smorgasbord" in New York. The word is now fully accepted into the English language.		While the possession of gold and silver has been a measure of solvency of a regime, the display of it, in the form of plates and vessels, is more a political act and a gesture of conspicuous consumption. The 16th-century French term buffet applied both to the display itself and to the furniture on which it was mounted, often draped with rich textiles, but more often as the century advanced the word described an elaborately carved cupboard surmounted by tiers of shelves. In England such a buffet was called a court cupboard. Prodigal displays of plate were probably first revived at the fashionable court of Burgundy and adopted in France. The Baroque displays of silver and gold that were affected by Louis XIV of France were immortalized in paintings by Alexandre-François Desportes and others, before Louis' plate and his silver furniture had to be sent to the mint to pay for the wars at the end of his reign.[citation needed]		During the 18th century more subtle demonstrations of wealth were preferred. The buffet was revived in England and France at the end of the century, when new ideals of privacy made a modicum of self-service at breakfast-time appealing, even among those who could have had a footman servant behind each chair. In The Cabinet Dictionary of 1803, Thomas Sheraton presented a neoclassical design and observed that "a buffet may, with some propriety, be restored to modern use, and prove ornamental to a modern breakfast-room, answering as the china cabinet/repository of a tea equipage".		In a 1922 housekeeping book entitled How to Prepare and Serve a Meal, Lillian B. Lansdown wrote:		The concept of eating a buffet arose in mid 17th century France, when gentleman callers would arrive at the homes of ladies they wanted to woo unexpectedly. Their surprise arrival would throw the kitchen staff into a panic and the only food that could be served was a selection of what was found in the cold room.		The informal luncheon or lunch — originally the light meal eaten between breakfast and dinner, but now often taking the place of dinner, the fashionable hour being one (or half after if cards are to follow) — is of two kinds. The "buffet" luncheon, at which the guests eat standing; and the luncheon served at small tables, at which the guests are seated....		The knife is tabooed at the "buffet" lunch, hence all the food must be such as can be eaten with fork or spoon. As a rule, friends of the hostess serve... The following dishes cover the essentials of a "buffet" luncheon. Beverages: punch, coffee, chocolate (poured from urn, or filled cups brought from pantry on tray); hot entrées of various sorts (served from chafing dish or platter) preceded by hot bouillon; cold entrées, salads, lobster, potatoes, chicken, shrimp, with heavy dressings; hot rolls, wafer-cut sandwiches (lettuce, tomato, deviled ham, etc.); small cakes, frozen creams and ices.[4]		The informal luncheon at small tables calls for service by a number of maids, hence the "buffet" plan is preferable.[citation needed]		There are many different ways of offering diners a selection of foods which are called "buffet" style meals. Some buffets are "single pass only", but most buffets allow a diner to first take small samples of unfamiliar foods, and then to return for more servings if desired. To avoid misunderstandings in commercial eating establishments, the rules and charges are often posted on signs near the buffet serving tables.		As a compromise between self-service and full table service, a staffed buffet may be offered: diners carry their own plate or tray along the buffet line and are given a portion by a server at each station, which may be selected or skipped by the diner. This method is prevalent at catered meetings where diners are not paying specifically for their meal.		Alternatively, diners may serve themselves for most prepared selections, but a carvery station for roasted meats is staffed. Some buffet formats also feature staffed stations where crepes, omelettes, noodle soups, barbecued meats, or sushi are custom prepared at the request of individual diners.		The "all-you-can-eat" buffet has been ascribed to Herbert "Herb" Cobb McDonald, a Las Vegas publicity and entertainment manager who introduced the idea in 1946.[5][6] In his 1965 novel The Muses of Ruin, William Pearson wrote, of the buffet:		At midnight every self-respecting casino premières its buffet—the eighth wonder of the world, the one true art form this androgynous harlot of cities has delivered herself of.... We marvel at the Great Pyramids, but they were built over decades; the midnight buffet is built daily. Crushed-ice castles and grottoes chill the shrimp and lobster. Sculptured aspic is scrolled with Paisley arabesques. They are, laid out with reverent artistry: hors d'oeuvres, relish, salads, and sauces; crab, herring oyster, sturgeon, octopus, and salmon; turkey, ham, roast beef, casseroles, fondues, and curries; cheeses, fruits and pastries. How many times you go through the line is a private matter between you and your capacity, and then between your capacity and the chef's evil eye.[7]		Many boarding schools, colleges, and universities offer optional or mandatory "meal plans", especially in connection with dormitories for students. These are often in an "all-you-can-eat" buffet format, sometimes called "all-you-care-to-eat" to encourage dietary moderation.[8] The format may also be used in other institutional settings, such as military bases, large factories, cruise ships, or medium-security prisons.		In 2007, the first all-you-can-eat seating section in Major League Baseball was introduced at Dodger Stadium.[9] The trend spread to 19 of the 30 major league parks by 2010[10] and numerous minor league parks by 2012.[11] The basic menu includes traditional ballpark food such as hot dogs, nachos, peanuts, popcorn, and soft drinks.[12][13] In 2008 AYCE seats were also inaugurated in numerous NBA and NHL arenas.[14]		A 2011 study showed that the actual amount of food consumed increases with the price charged for the buffet.[15]		In Australia, buffet chains such as Sizzler serve a large number of patrons with carvery meats, seafood, salads and desserts. Buffets are also common in Returned and Services League of Australia (RSL) clubs and some motel restaurants.		In Brazil, comida a quilo or comida por quilo — literally, "food by [the] kilo" — restaurants are common. This is a cafeteria style buffet in which diners are billed by the weight of the food selected, excluding the weight of the plate. Brazilian cuisine's rodízio style is all-you-can-eat, having both non-self-service and self-service variations.		In Japan, a buffet or smorgasbord is known as a viking (バイキング - baikingu). It is said that this originated from the restaurant "Imperial Viking" in the Imperial Hotel, Tokyo, which was the first restaurant in Japan to serve buffet-style meals. Dessert Vikings are very popular in Japan, where one can eat from a buffet full of desserts.		In Sweden, a traditional form of buffet is the smörgåsbord, which literally means "table of sandwiches".		In the US, there are numerous Chinese-American cuisine-inspired buffet restaurants, and well as those serving primarily traditional American fare.[16] Also, South Asian cuisine is increasingly available in the buffet format,[citation needed] and sushi has become more popular at buffets.[16] In some regions of the US, Brazilian-style rodízio churrascaria barbecue buffets are available.[citation needed]		Las Vegas and Atlantic City are famous for all-you-can-eat buffets with a very wide range of foods on offer (similar ones have also become common in casinos elsewhere in the US).[17]		Ovation Brands (formerly Buffets, Inc.) is a corporation which owns Country Buffet, Fire Mountain, HomeTown Buffet, Old Country Buffet, Ryan's Steakhouse, and other smaller buffet chains. HomeTown Buffet popularized the "scatter buffet", which refers to the layout of separate food pavilions. Other American restaurant chains well known for their buffets include America's Incredible Pizza Company, Chuck-A-Rama, Cici's Pizza, Fresh Choice, Furr's Family Dining, Gatti's Pizza, Golden Corral (which features food products presented in pans), John's Incredible Pizza, Pancho's Mexican Buffet, Ponderosa Steakhouse, Shakey's Pizza, Sizzler, Sweet Tomatoes (known in particular for its soups and salads), and Western Sizzlin'.		
A telephone, or phone, is a telecommunications device that permits two or more users to conduct a conversation when they are too far apart to be heard directly. A telephone converts sound, typically and most efficiently the human voice, into electronic signals suitable for transmission via cables or other transmission media over long distances, and replays such signals simultaneously in audible form to its user.		In 1876, Scottish emigrant Alexander Graham Bell was the first to be granted a United States patent for a device that produced clearly intelligible replication of the human voice. This instrument was further developed by many others. The telephone was the first device in history that enabled people to talk directly with each other across large distances. Telephones rapidly became indispensable to businesses, government, and households, and are today some of the most widely used small appliances.		The essential elements of a telephone are a microphone (transmitter) to speak into and an earphone (receiver) which reproduces the voice in a distant location. In addition, most telephones contain a ringer which produces a sound to announce an incoming telephone call, and a dial or keypad used to enter a telephone number when initiating a call to another telephone. Until approximately the 1970s most telephones used a rotary dial, which was superseded by the modern DTMF push-button dial, first introduced to the public by AT&T in 1963.[1] The receiver and transmitter are usually built into a handset which is held up to the ear and mouth during conversation. The dial may be located either on the handset, or on a base unit to which the handset is connected. The transmitter converts the sound waves to electrical signals which are sent through the telephone network to the receiving phone. The receiving telephone converts the signals into audible sound in the receiver, or sometimes a loudspeaker. Telephones permit duplex communication, meaning they allow the people on both ends to talk simultaneously.		The first telephones were directly connected to each other from one customer's office or residence to another customer's location. Being impractical beyond just a few customers, these systems were quickly replaced by manually operated centrally located switchboards. This gave rise to landline telephone service in which each telephone is connected by a pair of dedicated wires to a local central office switching system, which developed into fully automated systems starting in the early 1900s. For greater mobility, various radio systems were developed for transmission between mobile stations on ships and automobiles in the middle 20th century. Hand-held mobile phones were introduced for personal service starting in 1973. By the late 1970s several mobile telephone networks operated around the world. In 1983, the Advanced Mobile Phone System (AMPS) was launched, offering a standardized technology providing portability for users far beyond the personal residence or office. These analog cellular system evolved into digital networks with better security, greater capacity, better regional coverage, and lower cost. The public switched telephone network, with its hierarchical system of many switching centers, interconnects telephones around the world for communication with each other. With the standardized international numbering system, E.164, each telephone line has an identifying telephone number, that may be called from any authorized telephone on the network.		Although originally designed for simple voice communications, convergence has enabled most modern cell phones to have many additional capabilities. They may be able to record spoken messages, send and receive text messages, take and display photographs or video, play music or games, surf the Internet, do road navigation or immerse the user in virtual reality. Since 1999, the trend for mobile phones is smartphones that integrate all mobile communication and computing needs.						A traditional landline telephone system, also known as plain old telephone service (POTS), commonly carries both control and audio signals on the same twisted pair (C in diagram) of insulated wires, the telephone line. The control and signaling equipment consists of three components, the ringer, the hookswitch, and a dial. The ringer, or beeper, light or other device (A7), alerts the user to incoming calls. The hookswitch signals to the central office that the user has picked up the handset to either answer a call or initiate a call. A dial, if present, is used by the subscriber to transmit a telephone number to the central office when initiating a call. Until the 1960s dials used almost exclusively the rotary technology, which was replaced by dual-tone multi-frequency signaling (DTMF) with pushbutton telephones (A4).		A major expense of wire-line telephone service is the outside wire plant. Telephones transmit both the incoming and outgoing speech signals on a single pair of wires. A twisted pair line rejects electromagnetic interference (EMI) and crosstalk better than a single wire or an untwisted pair. The strong outgoing speech signal from the microphone (transmitter) does not overpower the weaker incoming speaker (receiver) signal with sidetone because a hybrid coil (A3) and other components compensate the imbalance. The junction box (B) arrests lightning (B2) and adjusts the line's resistance (B1) to maximize the signal power for the line length. Telephones have similar adjustments for inside line lengths (A8). The line voltages are negative compared to earth, to reduce galvanic corrosion. Negative voltage attracts positive metal ions toward the wires.		The landline telephone contains a switchhook (A4) and an alerting device, usually a ringer (A7), that remains connected to the phone line whenever the phone is "on hook" (i.e. the switch (A4) is open), and other components which are connected when the phone is "off hook". The off-hook components include a transmitter (microphone, A2), a receiver (speaker, A1), and other circuits for dialing, filtering (A3), and amplification.		A calling party wishing to speak to another party will pick up the telephone's handset, thereby operating a lever which closes the switchhook (A4), which powers the telephone by connecting the transmitter (microphone), receiver (speaker), and related audio components to the line. The off-hook circuitry has a low resistance (less than 300 ohms) which causes a direct current (DC), which comes down the line (C) from the telephone exchange. The exchange detects this current, attaches a digit receiver circuit to the line, and sends a dial tone to indicate readiness. On a modern push-button telephone, the caller then presses the number keys to send the telephone number of the called party. The keys control a tone generator circuit (not shown) that makes DTMF tones that the exchange receives. A rotary-dial telephone uses pulse dialing, sending electrical pulses, that the exchange can count to get the telephone number (as of 2010 many exchanges were still equipped to handle pulse dialing). If the called party's line is available, the exchange sends an intermittent ringing signal (about 75 volts alternating current (AC) in North America and UK and 60 volts in Germany) to alert the called party to an incoming call. If the called party's line is in use, the exchange returns a busy signal to the calling party. However, if the called party's line is in use but has call waiting installed, the exchange sends an intermittent audible tone to the called party to indicate an incoming call.		The ringer of a telephone (A7) is connected to the line through a capacitor (A6), which blocks direct current but passes the alternating current of the ringing signal. The telephone draws no current when it is on hook, while a DC voltage is continually applied to the line. Exchange circuitry (D2) can send an AC current down the line to activate the ringer and announce an incoming call. When there is no automatic exchange, telephones have hand-cranked magnetos to generate a ringing voltage back to the exchange or any other telephone on the same line. When a landline telephone is inactive (on hook), the circuitry at the telephone exchange detects the absence of direct current to indicate that the line is not in use. When a party initiates a call to this line, the exchange sends the ringing signal. When the called party picks up the handset, they actuate a double-circuit switchhook (not shown) which may simultaneously disconnects the alerting device and connects the audio circuitry to the line. This, in turn, draws direct current through the line, confirming that the called phone is now active. The exchange circuitry turns off the ring signal, and both telephones are now active and connected through the exchange. The parties may now converse as long as both phones remain off hook. When a party hangs up, placing the handset back on the cradle or hook, direct current ceases in that line, signaling the exchange to disconnect the call.		Calls to parties beyond the local exchange are carried over trunk lines which establish connections between exchanges. In modern telephone networks, fiber-optic cable and digital technology are often employed in such connections. Satellite technology may be used for communication over very long distances.		In most landline telephones, the transmitter and receiver (microphone and speaker) are located in the handset, although in a speakerphone these components may be located in the base or in a separate enclosure. Powered by the line, the microphone (A2) produces a modulated electric current which varies its frequency and amplitude in response to the sound waves arriving at its diaphragm. The resulting current is transmitted along the telephone line to the local exchange then on to the other phone (via the local exchange or via a larger network), where it passes through the coil of the receiver (A3). The varying current in the coil produces a corresponding movement of the receiver's diaphragm, reproducing the original sound waves present at the transmitter.		Along with the microphone and speaker, additional circuitry is incorporated to prevent the incoming speaker signal and the outgoing microphone signal from interfering with each other. This is accomplished through a hybrid coil (A3). The incoming audio signal passes through a resistor (A8) and the primary winding of the coil (A3) which passes it to the speaker (A1). Since the current path A8 – A3 has a far lower impedance than the microphone (A2), virtually all of the incoming signal passes through it and bypasses the microphone.		At the same time the DC voltage across the line causes a DC current which is split between the resistor-coil (A8-A3) branch and the microphone-coil (A2-A3) branch. The DC current through the resistor-coil branch has no effect on the incoming audio signal. But the DC current passing through the microphone is turned into AC current (in response to voice sounds) which then passes through only the upper branch of the coil's (A3) primary winding, which has far fewer turns than the lower primary winding. This causes a small portion of the microphone output to be fed back to the speaker, while the rest of the AC current goes out through the phone line.		A lineman's handset is a telephone designed for testing the telephone network, and may be attached directly to aerial lines and other infrastructure components.		Before the development of the electric telephone, the term "telephone" was applied to other inventions, and not all early researchers of the electrical device called it "telephone". A communication device for sailing vessels The Telephone was the invention of a captain John Taylor in 1844. This instrument used four air horns to communicate with vessels in foggy weather.[2] Later, c. 1860, Johann Philipp Reis used the term in reference to his Reis telephone, his device appears to be the first such device based on conversion of sound into electrical impulses, the term telephone was adopted into the vocabulary of many languages. It is derived from the Greek: τῆλε, tēle, "far" and φωνή, phōnē, "voice", together meaning "distant voice".		Credit for the invention of the electric telephone is frequently disputed. As with other influential inventions such as radio, television, the light bulb, and the computer, several inventors pioneered experimental work on voice transmission over a wire and improved on each other's ideas. New controversies over the issue still arise from time to time. Charles Bourseul, Antonio Meucci, Johann Philipp Reis, Alexander Graham Bell, and Elisha Gray, amongst others, have all been credited with the invention of the telephone.[3]		Alexander Graham Bell was the first to be awarded a patent for the electric telephone by the United States Patent and Trademark Office (USPTO) in March 1876.[4] The Bell patents were forensically victorious and commercially decisive. That first patent by Bell was the master patent of the telephone, from which other patents for electric telephone devices and features flowed.[5]		In 1876, shortly after the telephone was invented, Hungarian engineer Tivadar Puskás invented the telephone switch, which allowed for the formation of telephone exchanges, and eventually networks.[6]		Early telephones were technically diverse. Some used a liquid transmitter, some had a metal diaphragm that induced current in an electromagnet wound around a permanent magnet, and some were "dynamic" - their diaphragm vibrated a coil of wire in the field of a permanent magnet or the coil vibrated the diaphragm. The sound-powered dynamic kind survived in small numbers through the 20th century in military and maritime applications, where its ability to create its own electrical power was crucial. Most, however, used the Edison/Berliner carbon transmitter, which was much louder than the other kinds, even though it required an induction coil which was an impedance matching transformer to make it compatible with the impedance of the line. The Edison patents kept the Bell monopoly viable into the 20th century, by which time the network was more important than the instrument.		Early telephones were locally powered, using either a dynamic transmitter or by the powering of a transmitter with a local battery. One of the jobs of outside plant personnel was to visit each telephone periodically to inspect the battery. During the 20th century, "common battery" operation came to dominate, powered by "talk battery" from the telephone exchange over the same wires that carried the voice signals.		Early telephones used a single wire for the subscriber's line, with ground return used to complete the circuit (as used in telegraphs). The earliest dynamic telephones also had only one port opening for sound, with the user alternately listening and speaking (or rather, shouting) into the same hole. Sometimes the instruments were operated in pairs at each end, making conversation more convenient but also more expensive.		At first, the benefits of a telephone exchange were not exploited. Instead telephones were leased in pairs to a subscriber, who had to arrange for a telegraph contractor to construct a line between them, for example between a home and a shop. Users who wanted the ability to speak to several different locations would need to obtain and set up three or four pairs of telephones. Western Union, already using telegraph exchanges, quickly extended the principle to its telephones in New York City and San Francisco, and Bell was not slow in appreciating the potential.		Signalling began in an appropriately primitive manner. The user alerted the other end, or the exchange operator, by whistling into the transmitter. Exchange operation soon resulted in telephones being equipped with a bell in a ringer box, first operated over a second wire, and later over the same wire, but with a condenser (capacitor) in series with the bell coil to allow the AC ringer signal through while still blocking DC (keeping the phone "on hook"). Telephones connected to the earliest Strowger automatic exchanges had seven wires, one for the knife switch, one for each telegraph key, one for the bell, one for the push-button and two for speaking. Large wall telephones in the early 20th century usually incorporated the bell, and separate bell boxes for desk phones dwindled away in the middle of the century.		Rural and other telephones that were not on a common battery exchange had a magneto hand-cranked generator to produce a high voltage alternating signal to ring the bells of other telephones on the line and to alert the operator. Some local farming communities that were not connected to the main networks set up barbed wire telephone lines that exploited the existing system of field fences to transmit the signal.		In the 1890s a new smaller style of telephone was introduced, packaged in three parts. The transmitter stood on a stand, known as a "candlestick" for its shape. When not in use, the receiver hung on a hook with a switch in it, known as a "switchhook". Previous telephones required the user to operate a separate switch to connect either the voice or the bell. With the new kind, the user was less likely to leave the phone "off the hook". In phones connected to magneto exchanges, the bell, induction coil, battery and magneto were in a separate bell box or "ringer box".[7] In phones connected to common battery exchanges, the ringer box was installed under a desk, or other out of the way place, since it did not need a battery or magneto.		Cradle designs were also used at this time, having a handle with the receiver and transmitter attached, now called a handset, separate from the cradle base that housed the magneto crank and other parts. They were larger than the "candlestick" and more popular.		Disadvantages of single wire operation such as crosstalk and hum from nearby AC power wires had already led to the use of twisted pairs and, for long distance telephones, four-wire circuits. Users at the beginning of the 20th century did not place long distance calls from their own telephones but made an appointment to use a special soundproofed long distance telephone booth furnished with the latest technology.		What turned out to be the most popular and longest lasting physical style of telephone was introduced in the early 20th century, including Bell's 202-type desk set. A carbon granule transmitter and electromagnetic receiver were united in a single molded plastic handle, which when not in use sat in a cradle in the base unit. The circuit diagram of the model 202 shows the direct connection of the transmitter to the line, while the receiver was induction coupled. In local battery configurations, when the local loop was too long to provide sufficient current from the exchange, the transmitter was powered by a local battery and inductively coupled, while the receiver was included in the local loop.[8] The coupling transformer and the ringer were mounted in a separate enclosure, called the subscriber set. The dial switch in the base interrupted the line current by repeatedly but very briefly disconnecting the line 1 to 10 times for each digit, and the hook switch (in the center of the circuit diagram) disconnected the line and the transmitter battery while the handset was on the cradle.		In the 1930s, telephone sets were developed that combined the bell and induction coil with the desk set, obviating a separate ringer box. The rotary dial becoming commonplace in the 1930s in many areas enabled customer-dialed service, but some magneto systems remained even into the 1960s. After World-War II, the telephone networks saw rapid expansion and more efficient telephone sets, such as the model 500 telephone in the United States, were developed that permitted larger local networks centered around central offices. A breakthrough new technology was the introduction of Touch-Tone signaling using push-button telephones by American Telephone & Telegraph Company (AT&T) in 1963.		Ericsson DBH 1001 (ca. 1931), the first combined telephone made with a Bakelite housing and handset.		Telephone used by American soldiers (WWII, Minalin, Pampanga, Philippines)		Video shows the operation of an Ericofon		AT&T push button telephone made by Western Electric model 2500 DMG black 1980		A candlestick phone		Modern sound-powered emergency telephone		A mobile phone, also called a cell phone		The invention of the transistor in 1947 dramatically changed the technology used in telephone systems and in the long-distance transmission networks. With the development of electronic switching systems in the 1960s, telephony gradually evolved towards digital telephony which improved the capacity, quality, and cost of the network.		The development of digital data communications method, such as the protocols used for the Internet, it became possible to digitize voice and transmit it as real-time data across computer networks, giving rise to the field of Internet Protocol (IP) telephony, also known as voice over Internet Protocol (VoIP), a term that reflects the methodology memorably. VoIP has proven to be a disruptive technology that is rapidly replacing traditional telephone network infrastructure.		As of January 2005, up to 10% of telephone subscribers in Japan and South Korea have switched to this digital telephone service. A January 2005 Newsweek article suggested that Internet telephony may be "the next big thing."[9] As of 2006 many VoIP companies offer service to consumers and businesses.		From a customer perspective, IP telephony uses a high-bandwidth Internet connection and specialized customer premises equipment to transmit telephone calls via the Internet, or any modern private data network. The customer equipment may be an analog telephone adapter (ATA) which interfaces a conventional analog telephone to the IP networking equipment, or it may be an IP Phone that has the networking and interface technology built into the desk-top set and provides the traditional, familiar parts of a telephone, the handset, the dial or keypad, and a ringer in a package that usually resembles a standard telephone set.		In addition, many computer software vendors and telephony operators provide softphone application software that emulates a telephone by use of an attached microphone and audio headset, or loud speaker.		Despite the new features and conveniences of IP telephones, some may have notable disadvantages compared to traditional telephones. Unless the IP telephone's components are backed up with an uninterruptible power supply or other emergency power source, the phone ceases to function during a power outage as can occur during an emergency or disaster when the phone is most needed. Traditional phones connected to the older PSTN network do not experience that problem since they are powered by the telephone company's battery supply, which will continue to function even if there is a prolonged power outage. Another problem in Internet-based services is the lack of a fixed physical location, impacting the provisioning of emergency services such as police, fire or ambulance, should someone call for them. Unless the registered user updates the IP phone's physical address location after moving to a new residence, emergency services can be, and have been, dispatched to the wrong location.		Graphic symbols used to designate telephone service or phone-related information in print, signage, and other media include ℡ (U+2121), ☎ (U+260E), ☏ (U+260F), ✆ (U+2706), and ⌕ (U+2315).		By the end of 2009, there were a total of nearly 6 billion mobile and fixed-line telephone subscribers worldwide. This included 1.26 billion fixed-line subscribers and 4.6 billion mobile subscribers.[10]		
The hypothalamus (from Greek ὑπό, "under" and θάλαμος, thalamus) is a portion of the brain that contains a number of small nuclei with a variety of functions. One of the most important functions of the hypothalamus is to link the nervous system to the endocrine system via the pituitary gland (hypophysis).		The hypothalamus is located below the thalamus and is part of the limbic system.[1] In the terminology of neuroanatomy, it forms the ventral part of the diencephalon. All vertebrate brains contain a hypothalamus. In humans, it is the size of an almond.		The hypothalamus is responsible for the regulation of certain metabolic processes and other activities of the autonomic nervous system. It synthesizes and secretes certain neurohormones, called releasing hormones or hypothalamic hormones, and these in turn stimulate or inhibit the secretion of pituitary hormones. The hypothalamus controls body temperature, hunger, important aspects of parenting and attachment behaviours, thirst,[2] fatigue, sleep, and circadian rhythms.						The hypothalamus is a brain structure made up of distinct nuclei as well as less anatomically distinct areas. It is found in all vertebrate nervous systems. In mammals, magnocellular neurosecretory cells in the paraventricular nucleus and the supraoptic nucleus of the hypothalamus produce neurohypophysial hormones, oxytocin and vasopressin. These hormones are released into the blood in the posterior pituitary.[3] Much smaller parvocellular neurosecretory cells, neurons of the paraventricular nucleus, release corticotropin-releasing hormone and other hormones into the hypophyseal portal system, where these hormones diffuse to the anterior pituitary.		The hypothalamic nuclei include the following:[4][5][6]		See also: ventrolateral preoptic nucleus, periventricular nucleus.		A cross section of the monkey hypothalamus displays 2 of the major hypothalamic nuclei on either side of the fluid-filled 3rd ventricle.		Hypothalamic nuclei		Hypothalamic nuclei on one side of the hypothalamus, shown in a 3-D computer reconstruction[10]		The hypothalamus is highly interconnected with other parts of the central nervous system, in particular the brainstem and its reticular formation. As part of the limbic system, it has connections to other limbic structures including the amygdala and septum, and is also connected with areas of the autonomous nervous system.		The hypothalamus receives many inputs from the brainstem, the most notable from the nucleus of the solitary tract, the locus coeruleus, and the ventrolateral medulla.		Most nerve fibres within the hypothalamus run in two ways (bidirectional).		Several hypothalamic nuclei are sexually dimorphic; i.e., there are clear differences in both structure and function between males and females.[citation needed]		Some differences are apparent even in gross neuroanatomy: most notable is the sexually dimorphic nucleus within the preoptic area. However most of the differences are subtle changes in the connectivity and chemical sensitivity of particular sets of neurons.[citation needed]		The importance of these changes can be recognized by functional differences between males and females. For instance, males of most species prefer the odor and appearance of females over males, which is instrumental in stimulating male sexual behavior. If the sexually dimorphic nucleus is lesioned, this preference for females by males diminishes. Also, the pattern of secretion of growth hormone is sexually dimorphic, and this is one reason why in many species, adult males are much larger than females.[citation needed]		Other striking functional dimorphisms are in the behavioral responses to ovarian steroids of the adult. Males and females respond to ovarian steroids in different ways, partly because the expression of estrogen-sensitive neurons in the hypothalamus is sexually dimorphic; i.e., estrogen receptors are expressed in different sets of neurons.[citation needed]		Estrogen and progesterone can influence gene expression in particular neurons or induce changes in cell membrane potential and kinase activation, leading to diverse non-genomic cellular functions. Estrogen and progesterone bind to their cognate nuclear hormone receptors, which translocate to the cell nucleus and interact with regions of DNA known as hormone response elements (HREs) or get tethered to another transcription factor's binding site. Estrogen receptor (ER) has been shown to transactivate other transcription factors in this manner, despite the absence of an estrogen response element (ERE) in the proximal promoter region of the gene. In general, ERs and progesterone receptors (PRs) are gene activators, with increased mRNA and subsequent protein synthesis following hormone exposure.[citation needed]		Male and female brains differ in the distribution of estrogen receptors, and this difference is an irreversible consequence of neonatal steroid exposure. Estrogen receptors (and progesterone receptors) are found mainly in neurons in the anterior and mediobasal hypothalamus, notably:[citation needed]		In neonatal life, gonadal steroids influence the development of the neuroendocrine hypothalamus. For instance, they determine the ability of females to exhibit a normal reproductive cycle, and of males and females to display appropriate reproductive behaviors in adult life.[citation needed]		In primates, the developmental influence of androgens is less clear, and the consequences are less understood. Within the brain, testosterone is aromatized to (estradiol), which is the principal active hormone for developmental influences. The human testis secretes high levels of testosterone from about week 8 of fetal life until 5–6 months after birth (a similar perinatal surge in testosterone is observed in many species), a process that appears to underlie the male phenotype. Estrogen from the maternal circulation is relatively ineffective, partly because of the high circulating levels of steroid-binding proteins in pregnancy.[citation needed]		Sex steroids are not the only important influences upon hypothalamic development; in particular, pre-pubertal stress in early life (of rats) determines the capacity of the adult hypothalamus to respond to an acute stressor.[11] Unlike gonadal steroid receptors, glucocorticoid receptors are very widespread throughout the brain; in the paraventricular nucleus, they mediate negative feedback control of CRF synthesis and secretion, but elsewhere their role is not well understood.		The hypothalamus has a central neuroendocrine function, most notably by its control of the anterior pituitary, which in turn regulates various endocrine glands and organs. Releasing hormones (also called releasing factors) are produced in hypothalamic nuclei then transported along axons to either the median eminence or the posterior pituitary, where they are stored and released as needed.[12]		In the hypothalamic–adenohypophyseal axis, releasing hormones, also known as hypophysiotropic or hypothalamic hormones, are released from the median eminence, a prolongation of the hypothalamus, into the hypophyseal portal system, which carries them to the anterior pituitary where they exert their regulatory functions on the secretion of adenohypophyseal hormones.[13] These hypophysiotropic hormones are stimulated by parvocellular neurosecretory cells located in the periventricular area of the hypothalamus. After their release into the capillaries of the third ventricle, the hypophysiotropic hormones travel through what is known as the hypothalamo-pituitary portal circulation. Once they reach their destination in the anterior pituitary, these hormones bind to specific receptors located on the surface of pituitary cells. Depending on which cells are activated through this binding, the pituitary will either begin secreting or stop secreting hormones into the rest of the bloodstream. (Bear, Mark F. "Hypothalamic Control of the Anterior Pituitary." Neuroscience: Exploring the Brain. 4th ed. Philadelphia: Wolters Kluwer, 2016. 528. Print.)		Other hormones secreted from the median eminence include vasopressin, oxytocin, and neurotensin.[15][16][17][18]		In the hypothalamic-neurohypophyseal axis, neurohypophysial hormones are released from the posterior pituitary, which is actually a prolongation of the hypothalamus, into the circulation.		It is also known that hypothalamic-pituitary-adrenal axis (HPA) hormones are related to certain skin diseases and skin homeostasis. There is evidence linking hyperactivity of HPA hormones to stress-related skin diseases and skin tumors.[19]		The hypothalamus coordinates many hormonal and behavioural circadian rhythms, complex patterns of neuroendocrine outputs, complex homeostatic mechanisms, and important behaviours. The hypothalamus must, therefore, respond to many different signals, some of which generated externally and some internally. Delta wave signalling arising either in the thalamus or in the cortex influences the secretion of releasing hormones; GHRH and prolactin are stimulated whilst TRH is inhibited.		The hypothalamus is responsive to:		Olfactory stimuli are important for sexual reproduction and neuroendocrine function in many species. For instance if a pregnant mouse is exposed to the urine of a 'strange' male during a critical period after coitus then the pregnancy fails (the Bruce effect). Thus, during coitus, a female mouse forms a precise 'olfactory memory' of her partner that persists for several days. Pheromonal cues aid synchronization of oestrus in many species; in women, synchronized menstruation may also arise from pheromonal cues, although the role of pheromones in humans is disputed.		Peptide hormones have important influences upon the hypothalamus, and to do so they must pass through the blood–brain barrier. The hypothalamus is bounded in part by specialized brain regions that lack an effective blood–brain barrier; the capillary endothelium at these sites is fenestrated to allow free passage of even large proteins and other molecules. Some of these sites are the sites of neurosecretion - the neurohypophysis and the median eminence. However, others are sites at which the brain samples the composition of the blood. Two of these sites, the SFO (subfornical organ) and the OVLT (organum vasculosum of the lamina terminalis) are so-called circumventricular organs, where neurons are in intimate contact with both blood and CSF. These structures are densely vascularized, and contain osmoreceptive and sodium-receptive neurons that control drinking, vasopressin release, sodium excretion, and sodium appetite. They also contain neurons with receptors for angiotensin, atrial natriuretic factor, endothelin and relaxin, each of which important in the regulation of fluid and electrolyte balance. Neurons in the OVLT and SFO project to the supraoptic nucleus and paraventricular nucleus, and also to preoptic hypothalamic areas. The circumventricular organs may also be the site of action of interleukins to elicit both fever and ACTH secretion, via effects on paraventricular neurons.[citation needed]		It is not clear how all peptides that influence hypothalamic activity gain the necessary access. In the case of prolactin and leptin, there is evidence of active uptake at the choroid plexus from the blood into the cerebrospinal fluid (CSF). Some pituitary hormones have a negative feedback influence upon hypothalamic secretion; for example, growth hormone feeds back on the hypothalamus, but how it enters the brain is not clear. There is also evidence for central actions of prolactin.[citation needed]		Findings have suggested that thyroid hormone (T4) is taken up by the hypothalamic glial cells in the infundibular nucleus/ median eminence, and that it is here converted into T3 by the type 2 deiodinase (D2). Subsequent to this, T3 is transported into the thyrotropin-releasing hormone (TRH)-producing neurons in the paraventricular nucleus. Thyroid hormone receptors have been found in these neurons, indicating that they are indeed sensitive to T3 stimuli. In addition, these neurons expressed MCT8, a thyroid hormone transporter, supporting the theory that T3 is transported into them. T3 could then bind to the thyroid hormone receptor in these neurons and affect the production of thyrotropin-releasing hormone, thereby regulating thyroid hormone production.[20]		The hypothalamus functions as a type of thermostat for the body.[21] It sets a desired body temperature, and stimulates either heat production and retention to raise the blood temperature to a higher setting or sweating and vasodilation to cool the blood to a lower temperature. All fevers result from a raised setting in the hypothalamus; elevated body temperatures due to any other cause are classified as hyperthermia.[21] Rarely, direct damage to the hypothalamus, such as from a stroke, will cause a fever; this is sometimes called a hypothalamic fever. However, it is more common for such damage to cause abnormally low body temperatures.[21]		The hypothalamus contains neurons that react strongly to steroids and glucocorticoids – (the steroid hormones of the adrenal gland, released in response to ACTH). It also contains specialized glucose-sensitive neurons (in the arcuate nucleus and ventromedial hypothalamus), which are important for appetite. The preoptic area contains thermosensitive neurons; these are important for TRH secretion.		Oxytocin secretion in response to suckling or vagino-cervical stimulation is mediated by some of these pathways; vasopressin secretion in response to cardiovascular stimuli arising from chemoreceptors in the carotid body and aortic arch, and from low-pressure atrial volume receptors, is mediated by others. In the rat, stimulation of the vagina also causes prolactin secretion, and this results in pseudo-pregnancy following an infertile mating. In the rabbit, coitus elicits reflex ovulation. In the sheep, cervical stimulation in the presence of high levels of estrogen can induce maternal behavior in a virgin ewe. These effects are all mediated by the hypothalamus, and the information is carried mainly by spinal pathways that relay in the brainstem. Stimulation of the nipples stimulates release of oxytocin and prolactin and suppresses the release of LH and FSH.		Cardiovascular stimuli are carried by the vagus nerve. The vagus also conveys a variety of visceral information, including for instance signals arising from gastric distension or emptying, to suppress or promote feeding, by signalling the release of leptin or gastrin, respectively. Again this information reaches the hypothalamus via relays in the brainstem.		In addition hypothalamic function is responsive to—and regulated by—levels of all three classical monoamine neurotransmitters, noradrenaline, dopamine, and serotonin (5-hydroxytryptamine), in those tracts from which it receives innervation. For example, noradrenergic inputs arising from the locus coeruleus have important regulatory effects upon corticotropin-releasing hormone (CRH) levels.		The extreme lateral part of the ventromedial nucleus of the hypothalamus is responsible for the control of food intake. Stimulation of this area causes increased food intake. Bilateral lesion of this area causes complete cessation of food intake. Medial parts of the nucleus have a controlling effect on the lateral part. Bilateral lesion of the medial part of the ventromedial nucleus causes hyperphagia and obesity of the animal. Further lesion of the lateral part of the ventromedial nucleus in the same animal produces complete cessation of food intake.		There are different hypotheses related to this regulation:[23]		The medial zone of hypothalamus is part of a circuitry that controls motivated behaviors, like defensive behaviors.[24] Analyses of Fos-labeling showed that a series of nuclei in the "behavioral control column" is important in regulating the expression of innate and conditioned defensive behaviors.[25]		Exposure to a predator (such as a cat) elicits defensive behaviors in laboratory rodents, even when the animal has never been exposed to a cat.[26] In the hypothalamus, this exposure causes an increase in Fos-labeled cells in the anterior hypothalamic nucleus, the dorsomedial part of the ventromedial nucleus, and in the ventrolateral part of the premammillary nucleus (PMDvl).[27] The premammillary nucleus has an important role in expression of defensive behaviors towards a predator, since lesions in this nucleus abolish defensive behaviors, like freezing and flight.[27][28] The PMD does not modulate defensive behavior in other situations, as lesions of this nucleus had minimal effects on post-shock freezing scores.[28] The PMD has important connections to the dorsal periaqueductal gray, an important structure in fear expression.[29][30] In addition, animals display risk assessment behaviors to the environment previously associated with the cat. Fos-labeled cell analysis showed that the PMDvl is the most activated structure in the hypothalamus, and inactivation with muscimol prior to exposure to the context abolishes the defensive behavior.[27] Therefore, the hypothalamus, mainly the PMDvl, has an important role in expression of innate and conditioned defensive behaviors to a predator.		Likewise, the hypothalamus has a role in social defeat: Nuclei in medial zone are also mobilized during an encounter with an aggressive conspecific. The defeated animal has an increase in Fos levels in sexually dimorphic structures, such as the medial pre-optic nucleus, the ventrolateral part of ventromedial nucleus, and the ventral premammilary nucleus.[31] Such structures are important in other social behaviors, such as sexual and aggressive behaviors. Moreover, the premammillary nucleus also is mobilized, the dorsomedial part but not the ventrolateral part.[31] Lesions in this nucleus abolish passive defensive behavior, like freezing and the "on-the-back" posture.[31]		According to D. F. Swaab, writing in a July 2008 paper, "Neurobiological research related to sexual orientation in humans is only just gathering momentum, but the evidence already shows that humans have a vast array of brain differences, not only in relation to gender, but also in relation to sexual orientation."[32]		Swaab first reported on the relationship between sexual orientation in males and the hypothalamus's "clock", the suprachiasmatic nucleus (SCN). In 1990, Swaab and Hofman[33] reported that the suprachiasmatic nucleus in homosexual men was significantly larger than in heterosexual men. Then in 1995, Swaab et al.[34] linked brain development to sexual orientation by treating male rats both pre- and postnatally with ATD, an aromatase blocker in the brain. This produced an enlarged SCN and bisexual behavior in the adult male rats. In 1991, LeVay showed that part of the sexually dimorphic nucleus (SDN) known as the 3rd interstitial nucleus of the anterior hypothalamus (INAH 3), is nearly twice as large (in terms of volume) in heterosexual men than in homosexual men and heterosexual women. However, a study in 1992 has shown that the sexually dimorph nucleus of the preoptic area, which include the INAH3, are of similar size in homosexual males who died of AIDS to heterosexual males, and therefore larger than female. This clearly contradicts the hypothesis that homosexual males have a female hypothalamus. Furthermore, the SCN of homosexual males is extremely large (both the volume and the number of neurons are twice as many as in heterosexual males). These areas of the hypothalamus have not yet been explored in homosexual females nor bisexual males nor females. Although the functional implications of such findings still haven't been examined in detail, they cast serious doubt over the widely accepted Dörner hypothesis that homosexual males have a "female hypothalamus" and that the key mechanism of differentiating the "male brain from originally female brain" is the epigenetic influence of testosterone during prenatal development.[35][36]		In 2004 and 2006, two studies by Berglund, Lindström, and Savic[37][38] used positron emission tomography (PET) to observe how the hypothalamus responds to smelling common odors, the scent of testosterone found in male sweat, and the scent of estrogen found in female urine. These studies showed that the hypothalamus of heterosexual men and homosexual women both respond to estrogen. Also, the hypothalamus of homosexual men and heterosexual women both respond to testosterone. The hypothalamus of all four groups did not respond to the common odors, which produced a normal olfactory response in the brain.		Human brain left dissected midsagittal view		Location of the hypothalamus		Bear, Mark F. "Hypothalamic Control of the Anterior Pituitary." Neuroscience: Exploring the Brain. 4th ed. Philadelphia: Wolters Kluwer, 2016. 528. Print.		
A heterotroph (/ˈhɛtərəˌtroʊf, -ˌtrɒf/;[1] trophe = "nutrition") is an organism that cannot fix carbon from inorganic sources (such as carbon dioxide) but uses organic carbon for growth.[2][3] Heterotrophs can be further divided based on how they obtain energy; if the heterotroph uses light for energy, then it is considered a photoheterotroph, while if the heterotroph uses chemical energy, it is considered a chemoheterotroph.		Heterotrophs contrast with autotrophs, such as plants and algae, which can use energy from sunlight (photoautotrophs) or inorganic compounds (lithoautotrophs) to produce organic compounds such as carbohydrates, fats, and proteins from inorganic carbon dioxide. These reduced carbon compounds can be used as an energy source by the autotroph and provide the energy in food consumed by heterotrophs. Ninety-five percent or more of all types of living organisms are heterotrophic, including all animals and fungi and most bacteria and protists.[4]						Organotrophs exploit reduced carbon compounds as energy sources, like carbohydrates, fats, and proteins from plants and animals. Photoorganoheterotrophs such as Rhodospirillaceae and purple non-sulfur bacteria synthesize organic compounds by utilization of sunlight coupled with oxidation of inorganic substances, including hydrogen sulfide, elemental sulfur, thiosulfate, and molecular hydrogen. They use organic compounds to build structures. They do not fix carbon dioxide and apparently do not have the Calvin cycle.[5] Chemolithoheterotrophs can be distinguished from mixotrophs (or facultative chemolithotroph), which can utilize either carbon dioxide or organic carbon as the carbon source.[6][7]		Heterotrophs, by consuming reduced carbon compounds, are able to use all the energy that they obtain from food for growth and reproduction, unlike autotrophs, which must use some of their energy for carbon fixation.[5] Both heterotrophs and autotrophs alike are usually dependent on the metabolic activities of other organisms for nutrients other than carbon, including nitrogen, phosphorus, and sulfur, and can die from lack of food that supplies these nutrients.[8] This applies not only to animals and fungi but also to bacteria.[5]		Most heterotrophs are chemoorganoheterotrophs (or simply organotrophs) who utilize organic compounds both as a carbon source and an energy source. The term "heterotroph" very often refers to chemoorganoheterotrophs. Heterotrophs function as consumers in food chains: they obtain organic carbon by eating autotrophs or other heterotrophs. They break down complex organic compounds (e.g., carbohydrates, fats, and proteins) produced by autotrophs into simpler compounds (e.g., carbohydrates into glucose, fats into fatty acids and glycerol, and proteins into amino acids). They release energy by oxidizing carbon and hydrogen atoms present in carbohydrates, lipids, and proteins to carbon dioxide and water, respectively.		Most opisthokonts and prokaryotes are heterotrophic; in particular, all animals and fungi are heterotrophs.[4] Some animals, such as corals, form symbiotic relationships with autotrophs and obtain organic carbon in this way. Furthermore, some parasitic plants have also turned fully or partially heterotrophic, while carnivorous plants consume animals to augment their nitrogen supply while remaining autotrophic.		Animals are heterotrophs by ingestion, fungi are heterotrophs by absorption.		
A meal delivery service (MDS) is a service that sends customers fresh, prepared meals delivered to their homes.[1] These services individually package pre-portioned meals to assist with eating a healthy diet. These services cook and prepare meals for customers. Meals may come in small tupperware containers and are often labeled with nutritional information. There are also many options for specific diet types like vegetarian and vegan. These services often operate on a subscription business model rather than by individual order as in pizza delivery.		An alternate type of meal delivery service is a meal kit, which distributes ingredients and recipes[2] that customers prepare themselves.[3]		
Catering is the business of providing food service at a remote site or a site such as a hotel, public house (pub), or other location. Catering has evolved to become an artisanal affair. Caterers now create an experience that involves the senses.				The earliest account of major services being catered in the United States is a 1778 ball in Philadelphia catered by Caesar Cranshell to celebrate the departure of British General William Howe.[1] Catering business began to form around 1820, centering in Philadelphia.[1][2] Catering being a respectable and profitable business. The early catering industry was disproportionately founded by African-Americans.[1][2][3]		The industry began to professionalize under the reigns of Robert Bogle who is recognized as "the originator of catering."[2] By 1840, the second generation of Philadelphia black caterers formed, who began to combine their catering businesses with restaurants they owned.[2] Common usage of the word "caterer" came about in the 1880s at which point local directories began listing numerous caterers.[1] White businessmen eventually moved into the industry and by the 1930s, the black businesses had virtually disappeared.[1]		In the 1930s, the Soviet Union, creating more simple menus, began developing state public catering establishments as part of its collectivization policies.[4] A rationing system was implemented during World War II, and people became used to public catering. After the Second World War, many businessmen embraced catering as an alternative way of staying in business after the war.[5] By the 1960s, the home-made food was overtaken by eating in public catering establishments.[4]		By the 2000s, personal chef services started gaining popularity[6], with more women entering the workforce. People between 15 and 24 years of age spent as little as 11-17 minutes daily on food preparation and clean-up activities in 2006-2016, according to figures revealed by the American Time Use Survey conducted by the US Bureau of Labor Statistics[7].		A mobile caterer serves food directly from a vehicle, cart or truck which is designed for the purpose. Mobile catering is common at outdoor events (such as concerts), workplaces, and downtown business districts.		A wedding caterer provides food to the wedding party. The wedding caterer can be hired independently or can be part of a package designed by the venue.		Merchant ships often carry Catering Officers – especially ferries, cruise liners and large cargo ships. In fact, the term "catering" was in use in the world of the merchant marine long before it became established as a land-bound business.[citation needed]		
Fasting is a willing abstinence or reduction from some or all food, drink, or both, for a period of time. An absolute fast or dry fasting is normally defined as abstinence from all food and liquid for a defined period, usually a period of 24 hours, or a number of days. Water fasting allows drinking water but nothing else. Other fasts may be partially restrictive, limiting only particular foods or substances. A fast may also be intermittent in nature. Fasting practices may preclude intercourse and other activities as well as food.		In a physiological context, fasting may refer to the metabolic status of a person who has not eaten overnight, or to the metabolic state achieved after complete digestion and absorption of a meal. Several metabolic adjustments occur during fasting, and some diagnostic tests are used to determine a fasting state. For example, a person is assumed to be fasting after 8–12 hours from their last meal. Metabolic changes toward the fasting state begin after absorption of a meal (typically 3–5 hours after a meal); "post-absorptive state" is synonymous with this usage, in contrast to the postprandial state of ongoing digestion.		A diagnostic fast refers to prolonged fasting (from 8–72 hours depending on age) conducted under observation for investigation of a problem, usually hypoglycemia. Many people may also fast as part of a medical procedure or check-up such as a colonoscopy.		Fasting is also a part of many religious rituals.						Fasting is often practiced prior to surgery or other procedures that require general anesthetics because of the risk of pulmonary aspiration of gastric contents after induction of anesthesia (i.e., vomiting and inhaling the vomit, causing life-threatening aspiration pneumonia).[1][2][3] Additionally, certain medical tests, such as cholesterol testing (lipid panel) or certain blood glucose measurements require fasting for several hours so that a baseline can be established. In the case of a lipid panel, failure to fast for a full 12 hours (including vitamins) will guarantee an elevated triglyceride measurement.[4]		Fasting is of no help in either preventing or treating cancer.[5]		The American Cancer Society recommends that people undergoing chemotherapy increase their intake of protein and calories; current research around dietary restriction in these circumstances are inconclusive – there is weak evidence that a short-term period of fasting may have benefits during treatment.[6]		Fasting can help alleviate some symptoms of depression.[7] However the psychological effects may also include anxiety and depression.[8]		Some scientists have indicated that a fast will cause white blood cells to break down during the fasting, resulting in new ones needing to be built when the fast is broken, resulting in the replacement of old damaged ones.[9]		Although fasting will lead to weight loss,[8][10] using fasting for weight loss is considered unnecessary.[8][11]		Fasting is one of the alternatives proved to reduce the DPP-4 level and activate the Dipeptidyl peptidase 4 inhibitors and so, prevent osteoporosis[12].		It has been argued that fasting makes one better appreciate food.[8][11][13][14]		In rare occurrences,[15] fasting can lead to refeeding syndrome.[16]		Fasting is often used as a tool to make a political statement, to protest, or to bring awareness to a cause. A hunger strike is a method of non-violent resistance in which participants fast as an act of political protest, or to provoke feelings of guilt, or to achieve a goal such as a policy change. A spiritual fast incorporates personal spiritual beliefs with the desire to express personal principles, sometimes in the context of a social injustice.[17]		Notable annual fasts include the famine events (such as the 30 Hour Famine) coordinated by World Vision to bring awareness to world poverty and hunger.		Activists have also used fasting to bring attention to a cause and to pressure authority or government to act.		In British India, Jatin Das (who fasted to death) and Bhagat Singh who were on 116th day of their fast, on October 5, 1929 that Bhagat Singh and Dutt gave up their strike (surpassing the 97-day world record for hunger strikes which was set by an Irish revolutionary). The political and religious leader Mohandas K. Gandhi undertook several long fasts as political and social protests. Gandhi's fasts had a significant impact on the British Raj and the Indian population generally.		In Northern Ireland in 1981, a prisoner, Bobby Sands, was part of the 1981 Irish hunger strike, protesting for better rights in prison. Sands had just been elected to the British Parliament and died after 66 days of not eating. His funeral was attended by 100,000 people and the strike ended only after 9 other men died. In all, ten men survived without food for 46 to 73 days,[18] taking only water and salt.		César Chávez undertook a number of spiritual fasts, including a 25-day fast in 1968 promoting the principle of nonviolence, and a fast of 'thanksgiving and hope' to prepare for pre-arranged civil disobedience by farm workers.[17][19] Chávez regarded a spiritual fast as "a personal spiritual transformation".[20] Other progressive campaigns have adopted the tactic.[21]		In the Bahá'í Faith, fasting is observed from sunrise to sunset during the Bahá'í month of 'Ala' (March 1 or 2 – March 19 or 20).[22] Bahá'u'lláh established the guidelines in the Kitáb-i-Aqdas. It is the complete abstaining from both food and drink during daylight hours (including abstaining from smoking). Consumption of prescribed medications is not restricted. Observing the fast is an individual obligation and is binding on Bahá'ís between 15 years (considered the age of maturity) and 70 years old.[22] Exceptions to fasting include individuals younger than 15 or older than 70; those suffering illness; women who are pregnant, nursing, or menstruating; travellers who meet specific criteria; individuals whose profession involves heavy labor and those who are very sick, where fasting would be considered dangerous. For those involved in heavy labor, they are advised to eat in private and generally to have simpler or smaller meals than are normal.		Along with obligatory prayer, it is one of the greatest obligations of a Bahá'í.[22] In the first half of the 20th century, Shoghi Effendi, explains: "It is essentially a period of meditation and prayer, of spiritual recuperation, during which the believer must strive to make the necessary readjustments in his inner life, and to refresh and reinvigorate the spiritual forces latent in his soul. Its significance and purpose are, therefore, fundamentally spiritual in character. Fasting is symbolic, and a reminder of abstinence from selfish and carnal desires."[23]		Buddhist monks and nuns following the Vinaya rules commonly do not eat each day after the noon meal.[24] This is not considered a fast but rather a disciplined regimen aiding in meditation and good health.		Once when the Buddha was touring in the region of Kasi together with a large sangha of monks he addressed them saying: I, monks, do not eat a meal in the evening. Not eating a meal in the evening I, monks, am aware of good health and of being without illness and of buoyancy and strength and living in comfort. Come, do you too, monks, not eat a meal in the evening. Not eating a meal in the evening you too, monks, will be aware of good health and..... living in comfort.[25]		Fasting is practiced by lay Buddhists during times of intensive meditation, such as during a retreat. The Middle Path refers to avoiding extremes of indulgence on the one hand and self-mortification on the other. Prior to attaining Buddhahood, prince Siddhartha practiced a short regime of strict austerity—following years of serenity meditation under two teachers—during which he consumed very little food. These austerities with five other ascetics did not lead to progress in meditation, liberation (moksha), or the ultimate goal of nirvana. Henceforth, prince Siddhartha practiced moderation in eating which he later advocated for his disciples. However, on Uposatha days (roughly once a week) lay Buddhists are instructed to observe the eight precepts[26] which includes refraining from eating after noon until the following morning.[26] The eight precepts closely resemble the ten vinaya precepts for novice monks and nuns. The novice precepts are the same with an added prohibition against handling money.[27]		The Vajrayana practice of Nyung Ne is based on the tantric practice of Chenrezig.[28][29][30] It is said that Chenrezig appeared to an Indian nun[28] who had contracted leprosy and was on the verge of death. Chenrezig taught her the method of Nyung Ne[28] in which one keeps the eight precepts on the first day, then refrains from both food and water on the second. Although seemingly against the Middle Way, this practice is to experience the negative karma of both oneself and all other sentient beings and, as such is seen to be of benefit. Other self-inflicted harm is discouraged.[31][32]		The "acceptable fast" is discussed in the biblical Book of Isaiah, chapter 58:6–7. In this chapter, the nation of Israel is rebuked for their fasting, and given this exhortation:		(verse 6) "Is not this the fast that I choose:    to loose the bonds of wickedness,    to undo the thongs of the yoke,    to let the oppressed go free,    and to break every yoke? (7) Is it not to share your bread with the hungry    and bring the homeless poor into your house;    when you see the naked, to cover them,    and not to hide yourself from your own kin?" (8) Then your light shall break forth like the dawn,    and your healing shall spring up quickly;    your vindicator go before you,    and the glory of the Lord shall be your rear guard. (9) Then you shall call, and the Lord will answer;    you will cry for help, and he will say: Here am I."[33]		This passage indicates that the acceptable fast is not merely abstinence from food or water, but a decision to fully obey God's commands to care for the poor and oppressed. Zechariah, chapter 7:5–10, also repeats this message. The opening chapter of the Book of Daniel, vv. 8–16, describes a partial Daniel Fast and its effects on the health of its observers.		Fasting is a practice in several Christian denominations or other churches. Some denominations do not practice it, considering it an external observance, but many individual believers choose to observe fasts at various times at their own behest.[34] The Lenten fast observed in the Catholic Church and the Eastern Orthodox Church is a forty-day partial fast to commemorate the fast observed by Christ during his temptation in the desert. This is similar to the partial fasting within the Ethiopian Orthodox Church (abstaining from meat and milk) which takes place during certain times of the year and lasts for weeks.		For Roman Catholics, fasting, taken as a technical term, is the reduction of one's intake of food to one full meal (which may not contain meat on Ash Wednesday and Fridays throughout Lent) and two small meals (known liturgically as collations, taken in the morning and the evening), both of which together should not equal the large meal. Eating solid food between meals is not permitted. Fasting is required of the faithful between the ages of 18 and 59 on specified days. Complete abstinence of meat for the day is required of those 14 and older. Partial abstinence prescribes that meat be taken only once during the course of the day. Meat is understood not to include fish or cold-blooded animals.		Pope Pius XII had initially relaxed some of the regulations concerning fasting in 1956. In 1966, Pope Paul VI in his apostolic constitution Paenitemini, changed the strictly regulated Roman Catholic fasting requirements. He recommended that fasting be appropriate to the local economic situation, and that all Catholics voluntarily fast and abstain. In the United States, there are only two obligatory days of fast – Ash Wednesday and Good Friday. The Fridays of Lent are days of abstinence: eating meat is not allowed. Pastoral teachings since 1966 have urged voluntary fasting during Lent and voluntary abstinence on the other Fridays of the year. The regulations concerning such activities do not apply when the ability to work or the health of a person would be negatively affected.		Prior to the changes made by Pius XII and Paul VI, fasting and abstinence were more strictly regulated. The church had prescribed that Roman Catholics observe fasting or abstinence on a number of days throughout the year.		In addition to the fasts mentioned above, Roman Catholics must also observe the Eucharistic Fast, which involves taking nothing but water and medicines into the body for one hour before receiving the Eucharist. The ancient practice was to fast from midnight until Mass that day, but as Masses after noon and in the evening became common, this was soon modified to fasting for three hours. Current law requires merely one hour of eucharistic fast, although some Roman Catholics still abide by the older rules.		Colloquially, fasting, abstinence, the Eucharistic Fast, and personal sacrificial disciplines (such as abnegation of sweets for Lent or the like) are altogether referred to as fasting.		The Catholic Church has also promoted a Black Fast, in which in addition to water, bread is consumed. Typically, this form of fasting was used only by monks and other religious individuals who practice mortifications and asceticism, but all Catholics are invited to take part in it with the advice and consent of their Spiritual Director.		The Book of Common Prayer prescribes certain days as days for fasting and abstinence, "consisting of the 40 days of Lent, the ember days, the three rogation days (the Monday to Wednesday following the Sunday after Ascension Day), and all Fridays in the year (except Christmas, if it falls on a Friday)":[35]		A Table of the Vigils, Fasts, and Days of Abstinence, to be Observed in the Year.		Saint Augustine's Prayer Book defines "Fasting, usually meaning not more than a light breakfast, one full meal, and one half meal, on the forty days of Lent."[36] Abstinence, according to Saint Augustine's Prayer Book, "means to refrain from some particular type of food or drink. One traditional expression of abstinence is to avoid meat on Fridays in Lent or through the entire year, except in the seasons of Christmas and Easter. It is common to undertake some particular act of abstinence during the entire season of Lent. This self-discipline may be helpful at other times, as an act of solidarity with those who are in need or as a bodily expression of prayer."[37]		In the process of revising the Book of Common Prayer in various provinces of the Anglican Communion the specification of abstinence or fast for certain days has been retained. Generally Lent and Fridays are set aside, though Fridays during Christmastide and Eastertide are sometimes avoided. Often the Ember Days or Rogation Days are also specified, and the eves (vigils) of certain feasts.		For Eastern Orthodox Christians, fasting is an important spiritual discipline, found in both the Old Testament and the New, and is tied to the principle in Orthodox theology of the synergy between the body (Greek: soma) and the soul (pnevma). That is to say, Orthodox Christians do not see a dichotomy between the body and the soul but rather consider them as a united whole, and they believe that what happens to one affects the other (this is known as the psychosomatic union between the body and the soul).[38][39] Saint Gregory Palamas argued that man's body is not an enemy but a partner and collaborator with the soul. Christ, by taking a human body at the Incarnation, has made the flesh an inexhaustible source of sanctification.[40] This same concept is also found in the much earlier homilies of Saint Macarius the Great.		Fasting can take up a significant portion of the calendar year. The purpose of fasting is not to suffer, but according to Sacred Tradition to guard against gluttony and impure thoughts, deeds and words.[41] Fasting must always be accompanied by increased prayer and almsgiving (donating to a local charity, or directly to the poor, depending on circumstances). To engage in fasting without them is considered useless or even spiritually harmful.[38] To repent of one's sins and to reach out in love to others is part and parcel of true fasting.		There are four fasting seasons, which include:		Wednesdays and Fridays are also fast days throughout the year (with the exception of fast-free periods). In some Orthodox monasteries, Mondays are also observed as fast days (Mondays are dedicated to the Angels, and monasticism is called the "angelic life").[39]		Other days occur which are always observed as fast days:		Fasting during these times includes abstention from:		When a feast day occurs on a fast day, the fast is often mitigated (lessened) to some degree (though meat and dairy are never consumed on any fast day). For example, the Feast of the Annunciation almost always occurs within the Great Lent in the Orthodox calendar: in this case fish (traditionally haddock fried in olive oil) is the main meal of the day.		There are two degrees of mitigation: allowance of wine and oil; and allowance of fish, wine and oil. The very young and very old, nursing mothers, the infirm, as well as those for whom fasting could endanger their health somehow, are exempt from the strictest fasting rules.[38]		On weekdays of the first week of Great Lent, fasting is particularly severe, and many observe it by abstaining from all food for some period of time. According to strict observance, on the first five days (Monday through Friday) there are only two meals eaten, one on Wednesday and the other on Friday, both after the Presanctified Liturgy. Those who are unable to follow the strict observance may eat on Tuesday and Thursday (but not, if possible, on Monday) in the evening after Vespers, when they may take bread and water, or perhaps tea or fruit juice, but not a cooked meal. The same strict abstention is observed during Holy Week, except that a vegan meal with wine and oil is allowed on Great Thursday.[38]		On Wednesday and Friday of the first week of Great Lent the meals which are taken consist of xerophagy (literally, "dry eating") i.e. boiled or raw vegetables, fruit, and nuts.[38] In a number of monasteries, and in the homes of more devout laypeople, xerophagy is observed on every weekday (Monday through Friday) of Great Lent, except when wine and oil are allowed.		Those desiring to receive Holy Communion keep a total fast from all food and drink from midnight the night before (see Eucharistic discipline). The sole exception is the Communion offered at the Easter Sunday midnight liturgy, when all are expressly invited and encouraged to receive the Eucharist, regardless of whether they have kept the prescribed fast.		During certain festal times the rules of fasting are done away with entirely, and everyone in the church is encouraged to feast with due moderation, even on Wednesday and Friday. Fast-free days are as follows:		In Methodism, fasting is considered one of the Works of Piety.[43] The Discipline of the Wesleyan Methodist Church required Methodists to fast on "the first Friday after New-Year's-day; after Lady-day; after Midsummer-day; and after Michaelmas-day."[44] Historically, Methodist clergy are required to fast on Wednesdays, in remembrance of the betrayal of Christ, and on Fridays, in remembrance of His crucifixion and death.[45] "The General Rules of the Methodist Church," written by the founder of Methodism, John Wesley, wrote that "It is expected of all who desire to continue in these societies that they should continue to evidence their desire of salvation, by attending upon all the ordinances of God, such are: the public worship of God; the ministry of the Word, either read or expounded; the Supper of the Lord; family and private prayer; searching the Scriptures; and fasting or abstinence."[45] The Directions Given to Band Societies (25 December 1744) mandated fasting on all Fridays of the year.[44] Wesley himself also fasted before receiving Holy Communion "for the purpose of focusing his attention on God," and asked other Methodist Christians to do the same.[45] In accordance with Scripture and the teachings of the Church Fathers, fasting in Methodism is done "from morning until evening".[45] The historic Methodist homilies regarding the Sermon on the Mount also stressed the importance of the Lenten fast.[46] The United Methodist Church therefore states that:		There is a strong biblical base for fasting, particularly during the 40 days of Lent leading to the celebration of Easter. Jesus, as part of his spiritual preparation, went into the wilderness and fasted 40 days and 40 nights, according to the Gospels.[47]		Rev. Jacqui King, the minister of Nu Faith Community United Methodist Church in Houston explained the philosophy of fasting during Lent as "I'm not skipping a meal because in place of that meal I'm actually dining with God".[48]		All Oriental Orthodox Churches practice fasting however the rules of each Church differ. All Churches require fasting before one receives Holy Communion. All Churches practice fasting on most Wednesday and Fridays throughout the year as well as observing many other days. Monks and nuns also observe additional fast days not required of the laity.		The Armenian Church (with the exception of the Armenian Patriarchate of Jerusalem) has followed the Gregorian Calendar since 1923, making it and the Finnish Orthodox Church the only Orthodox Churches to primarily celebrate Easter on the same date as Western Christianity. As a result, the Armenian Church's observation of Lent generally begins and ends before that of other Orthodox Churches.		With the exception of the fifty days following Easter in the Coptic Orthodox Church of Alexandria, fish is not allowed during Lent, or on Wednesdays, Fridays, and Paramon days. Other than that fish and shellfish are allowed during fasting days.		The discipline of fasting entails that, apart from Saturdays, Sundays, and holy feasts, one should keep a total fast from all food and drink from midnight the night before to a certain time in the day usually three o'clock in the afternoon (the hour Jesus died on the Cross). Also, it is preferred that one reduce one's daily intake of food (typically, by eating only one full meal a day).		The Eritrean Orthodox Tewahedo Church generally follows the fasting practices of the Coptic Church however in some cases it follows the Ethiopian Church.		The Ethiopian Orthodox Tewahedo Church has an especially rigorous fasting calendar.		Fasting in the Ethiopian Church implies abstention from food and drink. No animal products are consumed, including dairy, eggs and meat, and utensils that have touched such products must be washed before touching the strictly vegan foods that are consumed on fast days. During fast periods, Holy Liturgy (Mass) is held at noon (except on Saturdays and Sundays), and because no food can be consumed before communion, it is traditional for people to abstain from food until mass is over (around 2 to 3 in the afternoon). Every Wednesday and Friday are days of fasting because Wednesday is the day that the Lord was condemned and Friday is the day he was crucified (the Wednesdays and Fridays between Easter Sunday and Pentecost Sunday are an exception as well as when Christmas or Epiphany fall on a Wednesday or a Friday ). The fasts that are ordained in the canon of the Church of Ethiopia are:		In addition to these, there is the fast of repentance which a person keeps after committing sin, it being imposed as a penance by the priest for seven days, forty days or one year. There is also a fast which a bishop keeps at the time he is consecrated. Also there are fasts that are widely observed but which have not been included in the canon of the church and which are therefore considered strictly optional such as the "Tsige Tsom" or Spring Fast, also known as "Kweskwam Tsom" which marks the exile of the Holy Family in Egypt.		All persons above the age of 13 are expected to observe the church fasts. Most children over age 7 are expected to observe at least the Fast of the Assumption of the Holy Virgin. Dispensations are granted to those who are ill.		The total number of fasting days amounts to about 250 a year. While many observe the Coptic Church's allowance for fish during the longer fasts, it has increasingly become practice in the Ethiopian Church to abstain from fish during all fasts according to the canons of the Ethiopian Church.		The observation of Lent within the Syriac Orthodox Church was once very strict but now is comparatively lenient compared with how it is observed in other Orthodox Churches.		The Assyrian Church of the East practices fasting during Lent, the seven weeks prior to Easter, wherein the faithful abstain from eating eggs, meat and any dairy or animal products. This is preceded by Somikka night.		The Church of the East strictly observes the Nineveh Fast (Som Baoutha). This annual observance occurs exactly 3 weeks before the start of Lent. This tradition has been practised by all Christians of Syriac traditions since the 6th century. At that time, a plague afflicted the region of Nineveh, modern-day northern Iraq. The plague devastated the city and the villages surrounding it, and out of desperation the people ran to their bishop to find a solution. The bishop sought help through the Scriptures and came upon the story of Jonah in the Old Testament. Upon reading the story, the bishop ordered a three-day fast to ask God for forgiveness. At the end of the three days, the plague had miraculously stopped, so on the fourth day the people rejoiced.		In Protestantism, the continental reformers criticized fasting as a purely external observance that can never gain a person salvation. Martin Luther believed that a Christian may choose to fast individually as a spiritual exercise to discipline his own flesh, but that the time and manner of fasting should be left to the individual's discretion, thus he rejected the collective diet rules and prohibitions imposed by the canon law of the Catholic Church.[49] This position was upheld by Lutheran churches, in that collective fasting was not officially enforced, whereas individual voluntary fasting was encouraged.[50] John Calvin argued that instead of relying on designated fasting periods, the entire life of the religious should be "tempered with frugality and sobriety" in such a way as to produce "a sort of perpetual fasting". He believed that collective public fasting could be appropriate only in times of calamity and grief for the community.[51] Similarly, the Swiss Reformation of the "Third Reformer" Huldrych Zwingli began with an ostentatious public sausage-eating during Lent—though Zwingli himself did not partake of the sausage.		In general, fasting remains optional in most Protestant groups and is less popular than among other Christian denominations.[51] In more recent years, many churches affected by liturgical renewal movements have begun to encourage fasting as part of Lent and sometimes Advent, two penitential seasons of the liturgical year. Members of the Anabaptist movement generally fast in private. The practice is not regulated by ecclesiastic authority.[52] Some other Protestants consider fasting, usually accompanied by prayer, to be an important part of their personal spiritual experience, apart from any liturgical tradition.		As explained above, the Lutheran Churches encourage individual fasting.[53] Certain modern Lutheran communities also advocate fasting during designated times such as Lent.[54] It is also considered to be an appropriate physical preparation for partaking of the Eucharist, but fasting is not necessary for receiving the sacrament. Martin Luther wrote in his Small Catechism "Fasting and bodily preparation are certainly fine outward training, but a person who has faith in these words, 'given for you' and 'shed for you for the forgiveness of sin' is really worthy and well prepared.".[55]		Classical Pentecostalism does not have set days of abstinence and lent, but individuals in the movement may feel they are being directed by the Holy Spirit to undertake either short or extended fasts. Although Pentecostalism has not classified different types of fasting, certain writers within the movement have done so. Arthur Wallis writes about the "Normal Fast" in which pure water alone is consumed.[56] The "Black Fast" in which nothing, not even water, is consumed is also mentioned. Dr. Curtis Ward points out that undertaking a black fast beyond three days may lead to dehydration, may irreparably damage the kidneys, and result in possible death.[57] He further notes that nowhere in the New Testament is it recorded that anyone ever undertook a black fast beyond three days and that one should follow this biblical guideline. Dr. Herbert Shelton advises that one should drink water according to natural thirst.[58] In addition to the Normal Fast and the Black Fast, some undertake what is referred to as the Daniel Fast (or Partial Fast) in which only one type of food (e.g., fruit or fruit and non-starchy vegetables) is consumed.[56] In a Daniel Fast, meat is almost always avoided, in following the example of Daniel and his friends' refusal to eat the meat of Gentiles, which had been offered to idols and not slaughtered in a kosher manner. In some circles of Pentecostals, the term "fast" is simply used, and the decision to drink water is determined on an individual basis. In other circles profuse amounts of pure water is advised to be consumed during the fasting period to aid the cleansing of internal toxins. Most Pentecostal writers on fasting concur with Dr. Mark Mattson who says that sensible intermittent fasting with a sensible water intake can strengthen the organism and assist thwarting degenerative diseases.[59]		For charismatic Christians fasting is undertaken at what is described as the leading of God. Fasting is done in order to seek a closer intimacy with God, as well as an act of petition. Some take up a regular fast of one or two days each week as a spiritual observance. Members of holiness movements, such as those started by John Wesley and George Whitefield, often practice such regular fasts as part of their regimen.		For members of The Church of Jesus Christ of Latter-day Saints, fasting is total abstinence from food and drink accompanied by prayer. Members are encouraged to fast on the first Sunday of each month, designated as Fast Sunday. During Fast Sunday, members fast for two consecutive meals for a total of 24 hours. The money saved by not having to purchase and prepare meals is donated to the church as a fast offering, which is then used to help people in need.[60] Members are encouraged to donate more than just the minimal amount, and be as generous as possible. The late LDS President Gordon B. Hinckley asked: "Think ... of what would happen if the principles of fast day and the fast offering were observed throughout the world. The hungry would be fed, the naked clothed, the homeless sheltered. … A new measure of concern and unselfishness would grow in the hearts of people everywhere."[61] Fasting and the associated donations for use is assisting those in need, are an important principle as evidenced by Church leaders addresses on the subject during General Conferences of the Church, e.g. The blessing of a proper fast in 2004, Is Not This the Fast That I Have Chosen? in 2015		Sunday worship meetings on Fast Sunday include opportunities for church members to publicly bear testimony during the sacrament meeting portion, often referred to as fast and testimony meeting.[62]		Fasting is also encouraged for members any time they desire to grow closer to God and to exercise self-mastery of spirit over body. Members may also implement personal, family or group fasts any time they desire to solicit special blessings from God, including health or comfort for themselves or others.[62]		Fasting is a very integral part of the Hindu religion. Individuals observe different kinds of fasts based on personal beliefs and local customs. Some are listed below.		Methods of fasting also vary widely and cover a broad spectrum. If followed strictly, the person fasting does not partake any food or water from the previous day's sunset until 48 minutes after the following day's sunrise. Fasting can also mean limiting oneself to one meal during the day, abstaining from eating certain food types or eating only certain food types. In any case, the fasting person is not supposed to eat or even touch any animal products (i.e., meat, eggs) except dairy products. Amongst Hindus during fasting, starchy items such as Potatoes, Sago and Sweet potatoes are allowed. The other allowed food items include milk products, peanuts and fruits. It should be noted that peanuts and the starchy items mentioned above originate outside India.		In Shri Vidya, one is forbidden to fast because the Devi is within them, and starving would in return starve the god. The only exception in Srividya for fasting is on the anniversary of the day one's parents died.		Mahabharata:Anushasana Parva (Book 13)		Yudhishthira asks Bhishma, "what constitutes the highest penances?" Bheeshma states (in section 103) " ....there is no penance that is superior to abstention from food! In this connection is recited the ancient narrative of the discourse between Bhagiratha and the illustrious Brahman (the Grandsire of the Creation).[64]		Bhagiratha says, The vow of fast was known to Indra. He kept it a secret but USANAS first made it known to the universe. Bhagiratha says, "In my opinion, there is no penance higher than fast." Bhagiratha did many sacrifices and gave gifts and says "the present that flowed from me were as copious as the stream of the Ganga herself.(but ..) it is not through the merits of these acts that I have attained this region." Bhagiratha observed the vow of fasting and reached "the region of Brahman"		Bheeshma advises Yudhishthira, "Do thou practice this vow (of fasting) of very superior merit that is not known to all."		In section 109, of the same book, Yudhishthira asks Bheesma "what is the highest, most beneficial" and fruitful "of all kinds of fasts in the world". Bheeshma says "fasting on the 12th day of the lunar month" and worship Krishna, for the whole year. Krishna is worshipped in twelve forms as Kesava, Narayana, Madhava, Govinda, Vishnu, the slayer of Madhu, who covered the universe in three steps, the dwarf (who beguiled Mahabali), Sridhara, Hrishikesha, Padmanabha, Damodara, Pundhariksha. and Upendra. After fasting, one must feed a number of brahmans. Bheeshma says " the illustrious Vishnu, that ancient being, has himself said that there is no fast that possesses merit superior to what attach to fast of this kind." [65]		In section 106, of the same book, Yudhishthira says, "the disposition (of observing fasts) is seen in all orders of men including the very Mlechchhas..... What is the fruit that is earned in this world by the man that observes fasts?" Bheeshma replies that he had asked Angiras "the very same question that thou has asked me today." The illustrious Angiras says Brahmans and kshatriya should fast for three nights at a stretch is the maximum. A person who fasts on the eight and fourteenth day of the dark fortnight "becomes freed from maladies of all kinds and possessed of great energy."		Fasting for one meal every day during a lunar month gets various boons according to the month in which he fasts.[66] For example, fasting for one meal every day during Margashirsha, "acquires great wealth and corn".		In some specific periods of time (like Caturmasya, Ekadashi fasting… ) it is said that one who fasts on these days and properly doing spiritual practice on these days like associating with devotees -sangha), chanting holy names of Hari (Vishnu, Narayana, Rama, Krishna...) (kirtanam) and similar (shravanam, kirtanam vishno…) may be delivered from sins.[citation needed]		Muslims believe that fasting is more than abstaining from food and drink. Fasting also includes abstaining from any falsehood in speech and action, abstaining from any ignorant and indecent speech, and from arguing, fighting, and having lustful thoughts. Therefore, fasting strengthens control of impulses and helps develop good behavior. During the sacred month of Ramadan, believers strive to purify body and soul and increase their taqwa (good deeds and God-consciousness). This purification of body and soul harmonizes the inner and outer spheres of an individual. Muslims aim to improve their body by reducing food intake and maintaining a healthier lifestyle. Overindulgence in food is discouraged and eating only enough to silence the pain of hunger is encouraged. Muslims believe they should be active, tending to all their commitments and never falling short of any duty. On a moral level, believers strive to attain the most virtuous characteristics and apply them to their daily situations. They try to show compassion, generosity and mercy to others, exercise patience, and control their anger. In essence, Muslims are trying to improve what they believe to be good moral character and habits.[67]		Fasting is forbidden on these days:[68]		Although fasting at Ramadan is fard (obligatory), exceptions are made for persons in particular circumstances:		Fasting is very common among Jains and as a part of festivals.[citation needed]		Fasting for Jews means completely abstaining from food and drink, including water. Traditionally observant Jews fast six days of the year. With the exception of Yom Kippur, fasting is never permitted on Shabbat, for the commandment of keeping Shabbat is biblically ordained and overrides the later rabbinically instituted fast days. (The fast of the 10th of Teveth would also override the Sabbath, but the current calendar system prevents this from ever occurring.[69])		Yom Kippur is considered to be the most important day of the Jewish year and fasting as a means of repentance is expected of every Jewish man or woman above the age of bar mitzvah and bat mitzvah respectively. It is so important to fast on this day, that only those who would be put in mortal danger by fasting are exempt, such as the ill or frail (endangering a life is against a core principle of Judaism). Those that do eat on this day are encouraged to eat as little as possible at a time and to avoid a full meal. For some, fasting on Yom Kippur is considered more important than the prayers of this holy day. If one fasts, even if one is at home in bed, one is considered as having participated in the full religious service.		The second major day of fasting is Tisha B'Av, the day approximately 2500 years ago on which the Babylonians destroyed the first Holy Temple in Jerusalem, as well as on which the Romans destroyed the second Holy Temple in Jerusalem about 2000 years ago, and later after the Bar Kokhba revolt when the Jews were banished from Jerusalem, the day of Tisha B'Av was the one allowed exception. Tisha B'Av ends a three-week mourning period beginning with the fast of the 17th of Tammuz. This is also the day when observant Jews remember the many tragedies which have befallen the Jewish people, including the Holocaust. The atmosphere of this fast is serious and deeply sad (in contrast to Yom Kippur which is a day of atonement).		Tisha B'Av and Yom Kippur are the major fasts and are observed from sunset to the following day's dusk. The remaining four fasts are considered minor and fasting is only observed from sunrise to dusk. Both men and women are expected to observe them, but a rabbi may give a dispensation if the fast represents too much of a hardship to a sick or weak person, or pregnant or nursing woman.		The four other public but minor fast days are:		There are other minor fast days, but these are not universally observed, and they include:		It is an Ashkenazic tradition for a bride and groom to fast on their wedding day before the ceremony as the day represents a personal Yom Kippur. In some congregations, repentance prayers that are said on Yom Kippur service are included by the bride and groom in their private prayers before the wedding ceremony.		Aside from these official days of fasting, Jews may take upon themselves personal or communal fasts, often to seek repentance in the face of tragedy or some impending calamity. For example, a fast is sometimes observed if a sefer torah is dropped. The length of the fast varies, and some Jews will reduce the length of the fast through tzedakah, or charitable acts. Mondays and Thursdays are considered especially auspicious days for fasting. Traditionally, one also fasted upon awakening from an unexpected bad dream although this tradition is rarely kept nowadays. In the time of the Talmud, drought seems to have been a particularly frequent inspiration for fasts. In modern times as well the Israeli Chief Rabbinate has occasionally declared fasts in periods of drought. It is obligatory for a Jewish community to fast for 40 days within the year if someone in the community accidentally drops a Torah scroll or tefillin. This tradition has been widespread for many hundreds of years.[70]		Sikhism does not promote fasting except for medical reasons. The Sikh Gurus discourage the devotee from engaging in this ritual as it "brings no spiritual benefit to the person". The Sikh holy Scripture, Sri Guru Granth Sahib tell us: "Fasting, daily rituals, and austere self-discipline – those who keep the practice of these, are rewarded with less than a shell." (Guru Granth Sahib Ang 216).		Human mind requires wisdom, which can be achieved by contemplating on word's and evaluating it, torturing body is of no use: "He does not eat food; he tortures his body. Without the Guru's wisdom, he is not satisfied." (Guru Granth Sahib Ji, Ang 905)		If you keep fast, then do it a way so that you adopt the compassion, well being and ask for good will of everyone. "Let your mind be content, and be kind to all beings. In this way, your fast will be successful." (Guru Granth Sahib Ji, Ang 299)		Serve God who alone is your Savior instead indulge into ritual, he is only one who will save you every where: "I do not keep fasts, nor do I observe the month of Ramadaan. I serve only the One, who will protect me in the end. ||1||" (Guru Granth Sahib Ji, Ang 1136)		If you keep fast, to count everyday pledge yourself you will act honest, sincere, controls your desires, mediate. This is a way how you make yourself free of five thieves: "On the ninth day(naomi) of the month, make a vow to speak the Truth, and your sexual desire, anger and desire shall be eaten up. On the tenth day, regulate your ten doors; on the eleventh day, know that the Lord is One. On the twelfth day, the five thieves are subdued, and then, O Nanak, the mind is pleased and appeased. Observe such a fast as this, O Pandit, O religious scholar; of what use are all the other teachings? ||2||" (Guru Granth Sahib Ji, Ang 1245)		Goal of Human is to meet the Lord-groom, so Guru Sahib Ji says: "One who discards this grain, is practicing hypocrisy. She is neither a happy soul-bride, nor a widow. Those who claim in this world that they live on milk alone, secretly eat whole loads of food. ||3|| Without this grain, time does not pass in peace. Forsaking this grain, one does not meet the Lord of the World." (Guru Granth Sahib Ji, Ang 873)		"Fasting on Ekadashi, adoration of Thakurs (stones) one remains away from Hari engaged in the Maya and omens. Without the Guru's word in the company of Saints one does not get refuge no matter how good one looks." (Bhai Gurdas Ji, Vaar 7)		The bigu (辟谷 "avoiding grains") fasting practice originated as a Daoist technique for becoming a xian (仙 "transcendent; immortal"), and later became a Traditional Chinese medicine cure for the sanshi (三尸 "Three Corpses; the malevolent, life-shortening spirits that supposedly reside in the human body"). Chinese interpretations of avoiding gu "grains; cereals" have varied historically; meanings range from not eating particular foodstuffs such as food grain, Five Cereals (China), or staple food to not eating anything such as inedia, breatharianism, or aerophagia.		In Yoga principle, it is recommended that one maintains a spiritual fast on a particular day each week (Monday or Thursday). A fast should also be maintained on the full moon day of each month. It is essential on the spiritual fasting day to not only to abstain from meals, but also to spend the whole day with a positive, spiritual attitude. On the fasting day, intake of solid food during the day is avoided and only a light veggie meal around 5 o'clock is taken. Water can be taken any time as needed. If health does not permit fasting for a whole day, for example with Diabetes, careful planning is done to reduce or skip one meal.[71]		Since the mid 1970s alternative medicine has perpetuated ideas of "cleansing the body" through fasting.[11]		
A packed lunch (also called pack lunch, sack lunch or bag lunch in North America, or pack up[citation needed] in the United Kingdom, as well as the regional variations: bagging in Lancashire, Merseyside and Yorkshire, [1]) is a lunch prepared at home and carried to be eaten somewhere else, such as school, a workplace, or at an outing. The food is usually wrapped in plastic, aluminum foil, or paper and can be carried ("packed") in a lunch box, paper bag (a "sack"), or plastic bag. While packed lunches are usually taken from home by the people who are going to eat them, in Mumbai, India, tiffin boxes are most often picked up from the home and brought to workplaces later in the day by so-called dabbawallas. It is also possible to buy packed lunches from stores in several countries. Lunch boxes made out of metal, plastic or vinyl are now popular with today's youth. Lunch boxes provide a way to take heavier lunches in a sturdier box or bag. It is also environmentally friendly.		In the United States, an informal meeting at work, over lunch, where everyone brings a packed lunch, is a brown-bag lunch or colloquially a "brown bag". There are also white and other color bags for seasonal use.		One such brown bag lunch was used as a deliberate rebuff of the Chinese hosts, by the United States delegation, at peace negotiations in Kaesong during the Korean War. The Chinese hosts offered lunch and watermelon to the U.S. guests, which the U.S. delegates, who considered lunching with one's opposition to be fraternizing with the enemy, rejected in favor of their own packed lunches.[2]				
Breakfast is the first meal of a day, most often eaten in the early morning before undertaking the day's work.[1] The word literally refers to breaking the fasting period of the prior night.[2]		There is a strong tendency for one or more "typical", or "traditional", breakfast menus to exist in most places, but the composition of this varies widely from place to place, and has varied over time, so that globally a very wide range of preparations and ingredients are now associated with breakfast.						With breakfast commonly referred to as "the most important meal of the day", particularly for children,[3][4] some epidemiological research indicates that having a breakfast might lower risk of metabolic disorders and cardiovascular diseases.[5] While current professional opinions are largely in favor of eating breakfast,[3] some contest its "most important" status.[6] The influence of breakfast on managing body weight is unclear.[7]		The Old English word for dinner, disner, means to break a fast, and was the first meal eaten in the day until its meaning shifted in the mid-13th century.[8] It was not until the 15th century that “breakfast” came into use in written English to describe a morning meal,[2]:6 which literally means to break the fasting period of the prior night; in Old English the term was morgenmete meaning "morning meal."[9]		In Burma the traditional breakfast is htamin jaw, fried rice with boiled peas (pè byouk), and yei nway jan (green tea), especially among the poor.[10]		Glutinous rice or kao hnyin is steamed and wrapped in banana leaf often served with peas as kao hnyin baung with a sprinkle of crushed and salted toasted sesame.[10] Equally popular is the purple variety of rice known as nga cheik which is cooked the same way and called nga cheik paung. Si damin is sticky rice cooked with turmeric and onions in peanut oil which is served with crushed and salted toasted sesame and crisp fried onions. Assorted fritters such as baya jaw (urad dal) are often served as a complement.		Nan bya or naan (Indian-style flatbreads) again with pè byouk or simply buttered, is served with Indian tea or coffee. It goes well with hseiksoup (mutton soup).[10]		Fried chapati, blistered like nan bya but crispy, with pè byouk and crispy fried onions is a popular alternative.[11]		Htat ta ya, lit. "a hundred layers", is flaky multi-layered fried paratha served with either pè byouk or a sprinkle of sugar.[12]		Eeja gway (Chinese-style fried breadsticks or youtiao) with Indian tea or coffee is another favourite.[10]		Mohinga,[13] perhaps the most popular of all, now available as an "all-day breakfast" in many towns and cities, is rice vermicelli in fish broth kept on the boil with chickpea flour or crushed toasted rice, lemon grass, sliced banana stem, onions, garlic, ginger, pepper and fish paste and served with crispy fried onions, crushed dried chilli, coriander, fish sauce and lime. Add fritters such as split chickpea (pè jan jaw), urad dal (baya jaw) or gourd (bu jaw), boiled egg and fried fish cake (nga hpè jaw).		As Mainland China is made up of many distinct provinces each with their own unique cuisine, breakfast in China can vary significantly from province to province. In general, basic choices include sweet or salty pancakes, soup, deep fried bread sticks or doughnuts (youtiao), buns (mantou), porridge (congee), and fried or soup-based noodles.[14] These options are often accompanied by tea or sweetened soy bean milk. However, condiments for porridge and the soup base tend to vary between provinces and regions. The types of teas that are served and spices that are used can also differ significantly between the provinces.		Due to its near two centuries history as a British colony and proximity to China's Canton region, both English and traditional Cantonese style breakfasts are of somewhat equal popularity in Hong Kong, as well as the hybrid form of breakfast commonly offered in Cha chaan teng. Cha Chaan Teng breakfasts often include Hong Kong style milk tea, pan fried egg, bread, Cantonese noodles or Hong Kong style macaroni in soup.[15]		Traditional Cantonese breakfast may include dim sum, which include a variety of different ingredients and are prepared in numerous different forms from delicately wrapped baby shrimp steamed dumplings to sweet water chestnut cake. Each dish is designed to be sampled and diners can go through a large selection of dim sum quickly accompanied by a generous amount of good tea. Iron Buddha tea is the most common accompaniment, but other teas such as pu'er and oolong are also common.[16] Fried and rice-based noodles and cakes are also popular. In modern times, dim sum is commonly prepared and served in Yum Cha restaurants rather than home because of the skill and efforts involved in the preparation.		Breakfast in modern Japanese households comes in two major variations: Japanese style and Western style.[17] Japanese-style breakfasts are eaten widely in Japan, but often only on weekends and non-working days.[17] Younger Japanese couples may prefer Western-style breakfasts because they are generally less time consuming to prepare.[17]		The standard Japanese breakfast consists of steamed white rice, a bowl of miso soup, and Japanese styled pickles (like takuan or umeboshi).[17][18] A raw egg and nori are often served; the raw egg is beaten in a small bowl and poured on the hot rice[17] to make golden colored tamago kake gohan, whilst the nori (sheets of dried seaweed) is used to wrap rice.[17] Grilled fish and Japanese green tea are often served as well.[18]		Western-style breakfasts in Japanese households are similar to those in the United States. Japanese children often eat corn flakes and drink milk, hot chocolate or fruit juice. Japanese adults (especially younger ones) tend to have toast with butter or jam, eggs, and slices of vegetables. They often drink coffee or orange juice.[17]		Traditional Japanese inns (like ryokan) serve complete traditional breakfast.[17] Western-style hotels and restaurants in Japan generally offer a mix of the Western and Japanese styles.[17]		Traditionally, Korean breakfasts consist mainly of rice and soup dishes. These can include small amounts of fish or beef, and some form of broth, stew or porridge. Like all Korean meals, breakfast is usually served with banchan, or side dishes consisting of kimchi, Gyeran-jjim (steamed eggs) and tofu.[19]		In all, there are at least 25 types of Indian breakfasts, each consisting of a choice of over 100 different food items.[20] Each state in India has different specialties and items for breakfast. Thus there is no single standard Indian breakfast, with items changing with regions. However, one can broadly classify breakfast varieties in India into 2 types; North Indian and South Indian. The eastern and western parts of India also have individual breakfast items unique to their culture or state.		A typical south Indian breakfast consists of idli, vada or dosa[21] coupled with chutney and sambar. Many variations of these dishes exist such as Rava idli, thayir vadai (yogurt vada), sambar vada and masala dosa. Other popular south Indian breakfast items are pongal, bisibelebath (sambar rice), upma, and poori. The state of Kerala has some special breakfast items such as appam, parotta, puttu, idiyappam and palappam.[21]		A typical north Indian breakfast may either be a type of paratha or roti served with a vegetable curry, curd and pickles. There are several varieties of parathas available depending on the type of stuffing such as aloo (potato) paratha, Paneer (cottage cheese) Paratha, Mooli Paratha (Radish Paratha), etc.[22] Other popular breakfast items in the north are poori bhaji, poha and bhindi bhujia.		Among Bengals roti and curry are the general variants in breakfast. The menu may also include "Indian French toast" which is also known as "Bombay toast", chire bhaja (flaked rice fried in oil and salt is added to it according to taste),[23] and boiled eggs.		In Western India, a Gujarati household may serve dhoklas, khakhras or theplas for breakfast, the most popular of which is methi thepla.[24] In Mangalore the breakfast delicacy Oondees may be served.		South Indian Dosa served with Chutney and sambar.		A south Indian breakfast with idlis and a vada, served with chutney and sambar.		Aloo Paratha		Dhoklas being sold in a market in Gujarat		In Maharashtra typical breakfast (Nashta) consists of 'Kande Pohe','Upma, 'Ukkad', Thalipeeth,[25]'Spiced Puree'. Sometimes 'Chapati Bhaji' or 'Chapati roll with tea' becomes breakfast.		A traditional Filipino breakfast might include pandesal (small bread rolls), kesong puti (white cheese), champorado (chocolate rice porridge), sinangag (garlic fried rice), and meat—such as tapa, longganisa, tocino, karne norte (corned beef), or fish such as daing na bangus (salted and dried milkfish)—or itlog na pula (salted duck eggs). Coffee is also commonly served particularly kapeng barako, a variety of coffee produced in the mountains of Batangas noted for having a strong flavor.		Certain portmanteaus in Filipino have come into use to describe popular combinations of items in a Filipino breakfast. An example of such a combination order is kankamtuy: an order of kanin (rice), kamatis (tomatoes) and tuyo (dried fish). Another is tapsi: an order of tapa and sinangág. Other examples include variations using a silog suffix, usually some kind of meat served with sinangág and itlog (egg). The three most commonly seen silogs are tapsilog (having tapa as the meat portion), tocilog (having tocino as the meat portion), and longsilog (having longganisa as the meat portion). Other silogs include hotsilog (with a hot dog), bangsilog (with bangus (milkfish)), dangsilog (with danggit (rabbitfish)), spamsilog (with spam), adosilog (with adobo), chosilog (with chorizo), chiksilog (with chicken), cornsilog (with corned beef), and litsilog (with lechon/litson). Pankaplog is a slang term referring to a breakfast consisting of pandesal, kape (coffee), and itlog (egg).[26] An establishment that specializes in such meals is called a tapsihan or "tapsilugan".		A traditional Singaporean breakfast contains kaya toast (coconut milk jam with bread), Half-boiled eggs and Kopi. Locals usually dip the toast into the eggs mixed with soya sauce and pepper.[citation needed]		Breakfast in urban areas traditionally consists of café con leche that is sweetened and includes a pinch of salt. Toasted buttered Cuban bread, cut into lengths, is dunked in the coffee. In rural Cuba, farmers eat roasted pork, beans and white rice, café con leche and cuajada sweetened with caramel.[27]		In the Dominican Republic, breakfast varies depending on the region. In the interior of the island it is accustomed to have breakfast with a side of vegetables, the green plantain or cooking plantain being the most popular. It is served boiled or mashed known as Mangú. In the capital, breakfast is more light. It includes coffee with milk or hot chocolate, along with bread, butter and cheese. Normally accompanied by orange juice and other juices of fruits typical to the region. Milk punch (milk, egg, nutmeg and malt) boiled eggs with "harina de negrito" or some other type of cornstarch. Traditional breakfast bread is a water-based bread.[28]		A Jamaican breakfast includes ackee and saltfish, seasoned callaloo, boiled green bananas, and fried dumplings.[29]		In Panama, breakfast is a heavy meal, especially in the interior of the country where hard labor requires it. It always includes black coffee (tinto) or with milk (called pintado) with any of these sides: corn tortillas, traditional white cheese (or queso del pais), another type of tortillas or "torrejas" made of wheat flour known as "hojaldres." Another traditional breakfast side is "bollo" made out of either corn, white corn, or coconut that is wrapped in corn leaves and "preña'o" (meaning with child) that means it's filled with some type of meat.		As protein, a large serving of beef liver with onion, scrambled or fried eggs, beef stew or "tasajo" (a type of beef jerky), pork rinds and different kinds of sausages like chorizo or morcilla are the most popular. These are also accompanied by: slices of green plantain or cooking plantain, "patacones" (double fried plantain), carimañolas (yuca filled with meat), as well as different bread pastries both savory and sweet. These large breakfasts are normally reserved for special occasions while everyday breakfasts consist of more traditional food from the west like toast, ham, cheese, jam etc.		It is important to mention that the prolonged US presence has also influenced urban areas of Panama by introducing meals like cereal with milk as well as pancakes with syrup as traditional breakfast meals.[30]		In Costa Rica the most common breakfast is called "gallo pinto," which basically is made up of rice and beans (red or black) previously cooked. Sautéed in a pan with chile, onions, culantro (an herb typical to the region) and bean stock for color. It is accompanied with fried egg, cheese and fried plantain or cooking plantain. Black coffee as well as with milk are traditional drinks. As this is the most common breakfast of the country, in Limón Province they prepare gallo pinto with coconut milk instead.		Another popular breakfast food is the "chorreadas" which are basically savory sweet corn pancakes; they are usually accompanied with cheese or a type of sour cream called "natilla".[31]		In Nicaragua the typical breakfast consists of "gallo pinto" (national dish made out of red beans and rice), eggs, cheese, corn tortillas and sweet plantains. Meals are normally accompanied by different juices and coffee. On Sundays, nacatamales are the traditional breakfast. These consist of a mass of corn with rice, potatoes, pork or chicken and sliced onions wrapped in plantain leaves and is usually accompanied by cacao as a drink.[32]		In Guatemala, breakfast consists of one or two eggs either fried, scrambled or boiled accompanied by baked/fried beans with coffee. With this comes fresh cream, fresh cheese and fried plantains (or cooking plantain). It is common to add hot sauce made out of "chiltepes" (a type of pepper). They are prepared raw or boiled, then they are ground with some vinegar, chopped onions and chopped cilantro to make the hot sauce. A traditional egg dish prepared with both green and red sauces is called "huevos divorciados." In the eastern part of the country, specifically in Zacapa you can find "huevos a caballo" or (eggs on a horse) which is basically two fried eggs over roasted steak.		Other types of breakfast include milk cereals. The most common drinks are orange juice or other fruits, milk, atol (a milk pudding with chocolate) and corn starch. The coffee is normally served with sweet bread also called "pan de manteca" (or butter bread).		In Honduras it is typical to start the day with homemade bread, with coffee or a glass of milk. Then, a plate of food with beans, alongside eggs that can be scrambled or sunny side up, slivers of fried plantain (or cooking plantain), corn, tortillas, cheese and butter. Versions of egg preparation vary: "estrellados" which consists of just cracking the shell, in "torta" or omelette (beaten with some salt), scrambled or boiled. Another typical breakfast are the baleadas and tortillas with cheese; sometimes they are fried together with cheese in between. In the "garífuna" culture, coffee is accompanied by "mínimo" bread (banana bread) or coconut bread.		A typical Continental breakfast consists of coffee and milk (often mixed as Cappuccino or latte) or hot chocolate with a variety of sweet cakes such as brioche and pastries such as croissant, often with a sweet jam, cream, or chocolate filling.[33] It is often served with juice. The continental breakfast may also include sliced cold meats, such as salami or ham, and yogurt or cereal.[34] Some countries of Europe add a bit of fruit and cheese to the bread menu, occasionally even a boiled egg or a little salami.[citation needed]		In Albania the breakfast often consists of a scone, milk, tea, eggs, jam or cheese. Meat is not preferred during the breakfast and it is usually substituted for seafood such as canned sardines or tuna which is typically served with condiments such as mustard or mayonnaise. Wholegrain cereals and pastries are mostly consumed by children.		In Croatia the base is a continental breakfast with a variety of pastries with or without fillings (marmalade, chocolate, cheese, ham, nuts, poppy) and fermented milk products (yogurt, soured milk, soured cream). Cold cuts, such as prosciutto, ham, salami, kulen, bacon, and various cheeses, are also favored.[35] Fried eggs or omelet and Vienna sausage with mayonnaise, mustard or ajvar are very often consumed. In continental parts sir i vrhnje (cottage cheese with soured cream and some spices) is traditional. Coffee is much preferred over tea (mostly herbal tea).		A typical breakfast in Denmark consists of slices of rye bread ('rugbrød') with yellow cheese, a soft-boiled egg – or more rarely – ham, salami or liver spread ('leverpostej') or it may consist of breakfast cereals such as oatmeal, corn flakes, yogurt being popular options. White bread or bread rolls (rundstykker) with yellow cheese and different kinds of jams, usually made from berries or citrus fruits, and other toppings, accompanied by coffee, or tea. Weekends or festive occasions may call for Danish pastries (wienerbrød), chocolate, or a bitters, such as Gammel Dansk.[36]		Breakfast usually consists of coffee or tea with open sandwiches. The sandwich is often buttered (with margarine), with toppings such as hard cheese or cold cuts.[37] Finns usually do not have sweets on their breads such as jam, or chocolate. Sour milk products such as yogurt or viili are also common breakfast foods, usually served in a bowl with cereals such as corn flakes, muesli, and sometimes with sugar, fruit or jam. Oatmeal or mixed grain porridge may also be served, usually topped with butter.		In France, a typical domestic breakfast consists of a cup of coffee, often café au lait, or hot chocolate, sometimes accompanied by a glass of orange or grapefruit juice. The main food consists of sweet products such as tartines (slices of baguette or other breads spread with butter, jam or chocolate paste), sometimes dunked in the hot drink. Brioches and other pastries such as croissants, pains au chocolat (chocolatine) and pains aux raisins are also traditional, but more of a weekend special treat.[38] Other products such as breakfast cereals, fruit compote, fromage blanc, and yogurt are becoming increasingly common as part of the meal. A traditional French breakfast does not include any savory product, but breakfast buffets in hotels often include ham, cheese and eggs.		The typical German breakfast consists of bread or bread rolls, butter, jam, ham, cheeses, meat spreads, cold cuts, hard- or soft-boiled eggs and coffee or tea. Cereals have become popular, and regional variation is significant. Yogurt, granola and fruit (fresh or stewed) may appear, as well as eggs cooked to order (usually at smaller hotels or bed-and-breakfasts).[39] A second breakfast is traditional in parts of Germany, notably Bavaria where it is called Brotzeit (literally "bread time").		Home breakfasts in Greece include bread with butter, honey or marmalade with coffee or milk. Breakfast cereals are also eaten. Children also eat nutella type cream on bread. No breakfast at all is common.[40] Various kinds of savoury pastry (Tyropita, spanakopita, and bougatsa) are also eaten for breakfast, also by those eating out, usually accompanied with Greek coffee or Frappé coffee. Traditional Greek breakfast (hot milk, fresh bread, butter and honey, or yogurt) was also available in special "milk shops" (in Greek Galaktopoleia – Γαλακτοπωλεία γαλακτοπωλείο). Milk shops were phased out between 1970 and 1990 – there are very few left, one is in Athens[41] and some in small towns.		In Hungary people usually have a large breakfast. Hungarian breakfast generally is an open sandwich with fresh bread or a toast, butter, cheese or different cream cheeses, túró cheese or körözött (Liptauer cheese spread), cold cuts such as ham, liver pâté (called májkrém or kenőmájas), bacon, salami, beef tongue, mortadella, disznósajt (head cheese), different Hungarian sausages or kolbász.[42] Even eggs, (fried, scrambled or boiled), French toast called bundás kenyér and vegetables (like peppers, bell peppers, tomatoes, radish, scallion and cucumber) are part of the Hungarian breakfast. Sometimes breakfast is a cup of milk, tea or coffee with pastries, bread rolls or crescent-shaped bread (kifli), toast, pastries with different fillings (sweet and salty as well), butter, jam or honey and a bun or a strudel Hungarian cuisine,[43] or cereal like muesli and perhaps fruit.		The traditional breakfast in Italy simply consists of a caffè latte (hot coffee with milk) with bread or rolls, butter and jam – known as prima colazione or just colazione. Fette biscottate (a cookie-like hard bread often eaten with hazelnut chocolate spread or butter and jam) and biscotti (cookies) are commonly eaten. Hot coffee may be sometimes replaced by hot tea, depending on personal taste. Children often drink hot chocolate, plain milk, hot milk with barley coffee, or hot milk with very little coffee. Cereals, yogurt and fruit juices are also common. If breakfast is eaten in a bar (coffee shop), it is composed of cappuccino and cornetto (frothed hot milk with coffee, and a pastry).[44] It is not uncommon for Italians to have a quick breakfast snack during the morning.		Typical Latvian breakfast usually consists of open sandwiches with toppings made of vegetables, fish, eggs or cheese. Just like Finland it is often buttered with margarine. Curd mixed with vegetables and salt as well as other sour milk products are very popular as well. Very often light oat porridge is eaten too.[45] In general light, sour and salty tasting food is common for morning meal. Latvians usually drink coffee for breakfast.		On the island of Malta, breakfast integrates both British and continental elements. Usually the Maltese start their day with a bowl of cereal mixed with milk, sometimes with a cup of coffee or tea. Marmalade/jams or even chocolate spreads are also common on bread or toast. Today cereal bars are also becoming a common type of breakfast on the island. The traditional English breakfast of eggs, sausages and fried bacon was also popular among the Maltese, especially on Sundays, due to the strong British influence on the island but this has diminished almost completely, as locals have rediscovered a more Mediterranean and continental diet over the recent years. Hotels usually serve both a continental as well as a full English breakfast. Prayers are often said before breakfast in order to bless the meal.[46]		For breakfast, the Dutch typically eat sliced bread with butter or margarine and three choices of toppings: dairy products (numerous variations of cheese), a variety of cured and sliced meats, or sweet or semi-sweet products such as jam, syrup (from sugar beets or fruit), honey, Kokosbrood (a coconut product that is served thinly sliced like sliced cheese) or peanut butter. Another type of sweet toppings are the chocolate toppings; the Dutch have chocolate toppings in all variations: hagelslag (chocolate sprinkles), chocoladevlokken (chocolate flakes) (both typically Dutch), and chocoladepasta (chocolate spread).[47] Tea, dripolator coffee, milk, and juice are the most popular breakfast beverages. Breakfast may also include raisin bread and fried or boiled eggs. On special occasions, such as Easter, Christmas, Mother's Day etc., breakfast is usually the same, but with a wider range of choices (i.e. premium cheeses, special ham, hot buns, croissants etc.).		A 2012 opinion poll concluded that the Dutch believe that breakfast should be a more important meal than it is and that more time should be spent on it; almost three-quarters of those polled ate their sandwiches in less than fifteen minutes, and blame for an all-too quick breakfast was placed on "fast" breakfast products. A perfect "weekend breakfast" for the Dutch contained coffee or tea, fresh-baked bread rolls (and croissants), and a boiled egg. The poll also concluded that men are more interested than women in having breakfast with their partner.[48]		80% of Norwegians eat breakfast daily, mostly at home. The most common breakfast is open sandwiches, often whole wheat bread, with cheese, often Jarlsberg, Norvegia or brunost, cold cuts,[49] leverpostei, jam etc. Common drinks are water, various types of coffee such as French press, cafe latte or espresso, milk and juice. Another common breakfast is breakfast cereals like corn flakes eaten with milk, kulturmelk or yogurt. Whole-grain porridges with regular milk or butter are also eaten by some. More ample breakfasts may include fish, a diverse array of cheese, eggs, bacon, breads, and hot and cold cereals eaten in various combinations.[45] Pastries such as croissants or pain au chocolat have become increasingly common since the 1990s.		The traditional Polish breakfast is a large spread with a variety of sides eaten with bread or toast. Sides include various cold cuts, meat spreads, the Polish sausage kielbasa, tomatoes, Swiss cheese, and sliced pickles. Twaróg, a Polish cheese, is the breakfast classic and comes in many forms. Twaróg can be eaten plain, with salt, sugar, or honey, or it can be mixed with chives into a cream cheese-like spread. Eggs are served often as the main breakfast item, mostly soft-boiled or scrambled. For a quick winter breakfast, hot oatmeal, to which cocoa is sometimes added, is often served. Jam spreads are popular for a quick breakfast, including plum, raspberry, and black or red currant spreads. Breakfast drinks include coffee, milk, hot cocoa, or tea. Traditionally, the Poles avoid heavy-cooked foods for breakfast.[50] For the most part, one will not see fried meats or potatoes in a classic Polish breakfast. Emphasis is placed on a large variety of foods to satisfy everyone at the breakfast table.		The traditional Romanian breakfast is milk, tea or coffee alongside (toasted) bread with butter or margarine and on top of it, honey or fruit jams or preserves. Sometimes the buttered bread is served savory instead of sweet, in which case the Romanians add cured meats, salami or cheese. Another option is to spread on a slice of bread some liver pâté. In the recent years, Romanians have also started to serve cereal with dried fruits and milk instead of the traditional breakfast, though that is not yet very wide spread.[51] According to a 2014 study, 35% of Romanians eat cooked dishes such as omelet or fried eggs and 15% eat sandwiches. Most people drink coffee and 67% serve Turkish coffee (made in an ibrik), though more and more people are starting to use drip or filter coffee.[52] While crêpes served with fruit preserves, jams or cheese have traditionally been served as desserts, in the recent years more Romanians have started to have them as breakfast during weekends.[53]		Traditional Russian breakfast are concentrated on kashas, or porridges – the most important staple in Russian nutritional culture, with buckwheat and oat, as well as semolina, serving as the three most important bases of such dishes, usually cooked on water or milk, as well as consumed with or without milk. Breakfast foods also include pancakes or oladushki.[54] Oladushki are made from flour and rise on yeast. Blini, or crepes, are also popular for breakfast and are also made with flour, but without the yeast. Sirniki, is a cheese form of pancake. Sirniki are made of tvorog (quark cheese), which can be eaten separately with honey for breakfast. Also, a popular dish is buterbrod, open sandwiches with cold cuts and cheeses.[55]		In Serbia domestic breakfast may include: eggs in different forms (e.g. omelet with bacon, onion and feta cheese), canned fish or opened sandwiches with prosciutto or ham, feta cheese and salad (e.g. pickles). Different types of pies and pastry with various filings (e.g. Proja, Gibanica, Burek) are also served as the main dish.[35] Yogurt and coffee are preferred breakfast drinks. In the past it was a custom to drink Sljivovica before breakfast and after that Slatko with water.		In Bulgaria, Bosnia, Montenegro, Macedonia and parts of Croatia breakfast usually consists of various kinds of savory or sweet pastry, with cheese, meat or jam filling. The most typical breakfast consists of two slices of burek and a glass of yogurt.[56] Breakfast also often consists of open sandwiches. The sandwich is buttered (with margarine), with toppings such as prosciutto and yellow cheese.		In Central Spain there is a special breakfast 'known as chocolate con churros – hot chocolate with Spanish-style fritters, which are extruded sticks of doughnut-like dough with a star-shaped profile covered in sugar. The chocolate drink is made very thick and sweet. In Madrid, churros are somewhat smaller and shaped like a charity ribbon [clarification needed]. This meal is normally served in cafeterías but it is not a regular or usual breakfast at Madrid homes. The usual one is the same as in the rest of Spain: coffee with milk or Cola Cao, orange juice, biscuits or toasts, with butter and jam. In the North, East and West it is more common to have a cup of coffee (usually with milk) or Cola Cao and a toast with a choice of olive oil and salt, tomato and olive oil, butter, jam, pâté, jamón serrano (cured ham), and other options like sobrasada (a raw cured spiced sausage that is easy to spread),[57] and in Andalucia, pringá. Freshly squeezed orange juice is widely available in most places as an alternative for coffee. The breakfast is not often larger than these two items, because usually in late morning there is a break known as almuerzo when there is a snack. Sometimes, toast is replaced with galletas (a type of cookie made with flour, sugar, oil and vanilla flavour), magdalenas (a Spanish version of the French madeleine made with oil instead of butter) or buns.		Breakfast in Sweden is generally an open sandwich made of a large amount of different types of soft bread or crisp bread, cold cuts, smörgåskaviar, cheese, cottage cheese, cream cheese, eggs, scrambled or boiled, pâté (leverpastej) with pickled cucumber, tomatoes or cucumber, or a toast with marmalade or maybe honey, juices, coffee, hot chocolate or tea. Breakfast cereals or muesli with milk, yogurt or filmjölk, currants and fruits are popular or warm whole-grain porridge with milk and jam (for example lingonberry jam).[58] Bilberry-soup (blåbärssoppa) and rose hip soup are also possible breakfast alternatives.		Swiss breakfasts are often similar to those eaten in neighboring countries. A notable breakfast food of Swiss origin, now found throughout Europe, is muesli[59] (Birchermüesli in Swiss German), introduced in 1900 by Maximilian Bircher-Benner for patients in his hospital.		In Turkish cuisine, a typical breakfast consists of bread, cheese (beyaz peynir, kaşar etc.), butter, olives, eggs, tomatoes, cucumbers, jam, honey, and kaymak. Sucuk (spicy Turkish sausage), pastırma, börek, simit, poğaça and soups are eaten as a morning meal in Turkey. A common Turkish specialty for breakfast is called menemen, which is prepared with tomatoes, green peppers, onion, olive oil and eggs. Various soups (çorba) are also very common and traditional for Turkish breakfast; mainly chicken broth, lentil soup, and a national delicacy, tarhana soup (Turkish cereal food consisting of flour yogurt and vegetables fermented then dried; it is consumed as a soup by mixing it with stock or water) are most well known soups. Tripe soup, trotter soup, sheep's head soup are also traditionally very common all over Turkey for breakfast. The Turkish word for breakfast, kahvaltı, means "before coffee," (kahve, 'coffee'; altı, 'under').[60] but since after the First World War, during which the Ottoman Empire lost its coffee-producing territories, tea has displaced coffee as the everyday hot drink in Turkey. In Sirkeci district of Instanbul, Pide is a popular morning meal.[61]		In the United Kingdom, the classic breakfast has been the "full breakfast", which involves fried egg, scrambled egg or poached egg with bacon and sausages, usually with mushrooms, tomatoes, baked beans, fried bread, black pudding or white pudding and toast. A healthy and nutritious version consists of grilling the protein and using poached, rather than fried, eggs, and variations based on one egg, one protein and toast abound. The "full Scottish breakfast" tends to omit pork sausages and have beef sausages or haggis instead. At its most extensive it consists of eggs, square sausage, fried dumpling, potato scone, tomato, mushrooms, bacon beef links and fried bread. Originating in the British isles during the Victorian era, the full breakfast is among the most internationally recognised British dishes.[62]		Another traditional British breakfast consists of porridge, which has been consumed in Scotland as a staple food since the Middle Ages. The breakfast cereal Scott's Porage Oats was produced in Glasgow in 1880.[63] Before the arrival of American-style breakfast cereals in the early 20th century, dried bread soaked in hot milk or tea and porridge (boiled oats) was the common daily breakfast, while leftover vegetables (often, cabbage) and potatoes that hadn't been eaten the night before were often served re-fried; which became 'bubble-and-squeak';[64] in Ireland the dish is known as colcannon. Traditionally, breakfast would be served with a small amount of fruit, such as a slice of orange, believed to prevent the onset of scurvy.[65] Also traditional, but now less popular breakfasts included fish in the form of kippers (smoked herring) with poached egg and toast, and kedgeree (a Scoto-Indian smoked haddock, egg and rice dish originating in Colonial India). Most British breakfasts are consumed with tea, coffee or fruit juice.[64]		In the contemporary UK and Ireland, a weekday breakfast may involve a cereal dish, such as muesli, porridge or cereal, or toast spread with jam or marmalade. Tea and coffee remain equally popular accompaniments. Marmalade, originally a Portuguese confection, was a popular British spread to consume in the evening, before the Scots moved it to the breakfast table in the 18th century.[66]		In Old English, breakfast was known as morgenmete, meaning "morning meal".[2]		A continental breakfast in UK and Irish hotels normally consists of baked goods (fresh bread, toast, pastries such as croissants or pain au chocolat etc.) slices of cheese and cold meat, cereal, yogurts, fruit and drinks like coffee, tea or fruit juices.[67] Although this is the traditional breakfast in parts of continental Europe, elsewhere these breakfasts are common only in the hospitality sector (particularly in economy and limited service hotels with no restaurant, as they require little preparation).		Breakfast is often a quick meal, consisting of bread and dairy products, with tea and sometimes jam. Flat bread with olive oil and za'tar is also popular.[68]		Most Egyptians begin the day with a light breakfast. Ful medames (dish of cooked fava beans), one of Egypt's several national dishes, is typical. It is seasoned with salt and cumin, garnished with vegetable oil and optionally with tahini, chopped parsley, chopped tomato, garlic, onion, lemon juice, chili pepper and often served topped with a boiled egg. It is scooped up and eaten with the staple whole wheat pita bread called Eish Masri or Eish Baladi (Egyptian Arabic: عيش [ʕeːʃ]; Modern Standard Arabic: ʿayš) and usually accompanied by taʿamiya (Egyptian Arabic: طعمية‎‎) which is the local variant of falafel made with fava beans, fresh cut homemade French fries and various fresh or pickled vegetables (called torshi). Several kinds of cheeses are popular, including gebna bēḍa or Domyati cheese, gebna rūmi (Roman cheese) which is similar to Pecorino Romano or Manchego, and Istanbuli cheese (a brined white cheese with peppers added to the brine which makes it spicy). Fried eggs with pastirma is also common breakfast foods in Egypt.		For breakfast, many Moroccans eat bread, harsha (semolina griddle cakes), or msemen (oiled pancakes) with olive oil, tea, and different kinds of Moroccan crepes.[68]		Breakfast in Iran generally consists of different kinds of flatbread such as barbari, taftoon, sangak, lavash, white cheese, butter, jam and marmalade (morabba), honey, clotted cream (sar sheer), nuts and fresh/dried fruits, and black tea or coffee. Frequently breakfast can be as simple as butter and jam on bread with tea. Iranians prefer to drink their hot black tea with sugar. Traditional cooked dishes for breakfast include haleem (wheat and chicken/lamb/turkey porridge eaten with cinnamon) or kale pache (sheep's feet, stomach and other offal), adasi (green lentil soup), fried/boiled/soft-boiled eggs, omlet (eggs cooked in tomato sauce).		The Israeli breakfast is a mix of culinary influences from eastern Europe, agrarian Yishuv culture, North African cuisine and Levantine cuisine.[69] It usually consists of a range of cheeses along with sliced vegetables, scrambled eggs (or another kind of fried egg) [70][71] and bread, served with spreads like butter, jam or honey. The most commonly used vegetables are cucumbers, tomatoes, and red bell peppers; carrots, onions and radishes may also be included.[72] Cheeses include, at the very least, cottage cheese, quark or fromage blanc, and a local variety of gouda, and often Tzfatit and labneh too. Side dishes including pickled olives and herring may also be served. Typical Middle Eastern mezze such as Israeli salad, hummus, tehina and baba ghanoush, as well as Shakshouka and a variety of salads may be served.[72] The meal is most often accompanied by coffee, tea and orange juice.		Israeli hotels usually present this type of breakfast as a buffet.[73] Restaurants may prefer a pre-set "Israeli Breakfast" menu item.[74]		A typical Israeli meal could be either dairy- or meat-based, but not both. Only certain types of meat are considered kosher.		A traditional Mexican breakfast consists of eggs prepared in different ways (like Huevos rancheros), accompanied by beans with chile and tortillas. Also very traditional are chilaquiles (pieces of corn tortilla with a cheese and chile sauce) and entomatadas (a variation of the spicy enchiladas).[75]		Breakfast in the United States and Canada often consists of some combination of hot or cold cereals, eggs, breakfast meats, and breakfast quick breads such as pancakes, waffles, or biscuits. Variants of the full breakfast and continental breakfast are also common. Coffee is a common breakfast beverage among adults, but is not popular with children. Tea is also widely consumed in Canada during breakfast. Orange juice, tomato juice, and other fruit juices are consumed by people of all ages. In the United States, 65% of coffee is drunk during breakfast hours.[76] Milk is also widely consumed, drunk either plain or prepared with various flavorings, such as chocolate or strawberry. The average starting time for breakfasts has been found to be 8:12 a.m. This varies from 8:08 a.m. in the South to 8:17 a.m. in the West.[77]		In the Southeastern United States, grits, and biscuits and gravy are popularly eaten at breakfast.[78]		In many regions around the country, the bagel, a ring-shaped bread product, is a common breakfast item. A bagel is often topped with seeds, such as sesame or poppy, or other spiced toppings, and is often served sliced in half, toasted, and spread with butter or cream cheese or other toppings.		Across the United States, breakfast sandwiches are a common choice for the first meal of the day.[citation needed] The archetypal breakfast sandwich is composed of egg, cheese, and cooked breakfast meat such as bacon or sausage, on a roll, although regional varietals are common.[citation needed] In New Jersey, bacon is often swapped out of the breakfast sandwich and replaced with a processed meat called Pork Roll (also known as Taylor Ham).[citation needed] Other areas alter the breakfast sandwich medium with regional favorites: in New York, the roll takes second seat to the bagel, and in the South, the biscuit is the vehicle of choice. A further twist of the classic breakfast sandwich is the breakfast burrito, which originates from Southwestern and Tex-Mex cuisines.[citation needed]		Another popular breakfast item in the United States is the donut. This is often consumed exclusive of other breakfast foods, though it's typically accompanied by coffee.		Prior to the Second World War and the widespread adoption of household refrigerators, the traditional Australian breakfast consisted of grilled steaks and fried eggs, mainly because of the ready availability of beefsteak during that period. Although this is still eaten in the bush, very few urban Australians today would recall this breakfast format—but the steak-and-eggs breakfast has survived as the customary pre-landing breakfast of the United States Marine Corps, due to the Marines having copied it from Australian soldiers when the two countries campaigned together during the Pacific War.[79][80]		The majority of urban Australians eat commercially prepared cereal with pasteurised milk or yogurt and toast with preserves such as marmalade or vegemite[81] for breakfast.[82] Two of the most common cereals are cornflakes and a type of biscuit made from wheat, called Weet-bix. Fruit is also common at breakfast, either on the cereal or eaten separately. While not unusual, a cooked breakfast is more likely to be eaten on weekends or on special occasions either at home or at a café.[83] A cooked breakfast can include sausage, bacon, breakfast steaks, mushrooms, tomato, hash browns and pancakes, similar to both the British and American cooked breakfast. Breakfast habits differ more between age groups or families than between cities.[84]		Breakfast in New Zealand is very similar to the Australian breakfast. The range of processed breakfast cereals is vast and children are more likely to eat those that contain added sugar.[85] New Zealanders, particularly in winter, are likely to eat a hot oat cereal called "porridge". Porridge is typically served with milk, sugar, fruit or yogurt. Sliced bread which has been toasted and topped with preserves or spreads is a common alternative breakfast. Eating breakfast at a restaurant was unheard of until the 1990s; however cafes which serve breakfast until midday or all day are now common.[86] The Big Breakfast is the main item at cafes, which is similar to the British cooked breakfast, except that it seldom includes black pudding. Other common menu items are: eggs done how you like, eggs benedict, beans on toast, pancakes, cereal and yogurt, and smoothies. Breakfast nearly always includes coffee, tea or both, with children drinking milk either on their cereal, in a glass or hot milo. Recent concern has been raised about the cost of milk and some families being unable to afford it.[87]		New Zealand chains of the fast food franchise McDonald's offer a "kiwi big breakfast" which includes two sausages, hash brown, scrambled eggs, toasted muffin and choice of a small Filter coffee, Hot chocolate or tea.[88] Some NGOs and charities, such as the New Zealand Red Cross, provide breakfast to underclass primary school children.[89] Survey results released in 2013 claim that nearly half of all New Zealanders skip breakfast at least once a week with almost a third of those skipping breakfast up to three times a week.[90]		In ethnically Fijian villages, breakfast may be tea served with milk and sugar, and food made out of flour: tovoi or babakau (a type of fried dough), pancakes, bread or biscuits with butter.[91] Sometimes a starch, such as cassava, taro in coconut milk, or rice, is served instead. Leftover fish or meat from the previous night's meal may be served as well.[92] Tea made from lemon leaves (called draunimoli)[91] and fruits such as pineapple, banana, papaya, plantain, and watermelon are also occasionally served.[93] In urban households, tea and cereals are often consumed.[91] Breakfast foods eaten by Fiji Indians often include a vegetable curry with roti and sometimes differ from the above.[91] Fijians living in Rotuma sometimes eat nuqa fish in tarotaro (fermented coconut yogurt), with fresh tropical fruits.[93]		Breakfast in Africa varies greatly from region to region.[94]		Nigeria has over 250 different ethnic groups,[95] with a corresponding variety of cuisines. For the Hausa of northern Nigeria, a typical breakfast consists of kosai (cakes made from ground beans which are then fried) or funkaso (wheat flour soaked for a day then fried and served with sugar). Both of these cakes can be served with porridge and sugar known as koko. For the south western Yoruba people (Ilé Yorùbá) one of the most common breakfasts is Ògì— a porridge made from corn, usually served with evaporated milk. Ògì is eaten with Acarajé (akara) or Moi moi.[96] Both are made from ground bean paste; akara is fried in oil, and moi moi is wrapped in leaves or foil and then steamed. Ògì can also be steamed in leaves to harden it and eaten with akara or moi moi for breakfast. English tea or malta is served as a breakfast drink. Another popular option in southwest Nigeria is Gari, which is eaten like a cereal. Gari, known in Brazil as farofa, is made from the root of cassava. For breakfast, it is soaked in water and sweetened with sugar.[97]		Breakfast typically consists of café Touba,[a][98] spiced coffee with abundant sugar sometimes consumed with dried milk,[99] or kinkeliba tea.[100] Small beignets and fresh fruit, including mangoes and bananas, are often part of a simple breakfast, and are accompanied by baguette[101] with various spreads: Chocoleca, a Nutella equivalent made from peanuts; butter; or processed mild cheese.		Breakfast (quraac) is an important meal for Somalis, who often start the day with some style of tea (shaah). The main dish is typically a pancake-like bread (canjeero or canjeelo) similar to Ethiopian injera, but smaller and thinner. It might also be eaten with a stew (maraq) or soup.[102] Lahoh is a pancake-like bread originating in Somalia, Djibouti and Yemen.[103][104] It is often eaten along with honey and ghee, and washed down with a cup of tea. During lunch, lahoh is sometimes consumed with curry, soup or stew.		In Uganda, most tribes have different cuisines but the most popular breakfast dishes are Porridge and Katogo. Porridge is made by mixing maize flour or millet flour with water and bringing the mixture to a boil.[105] While Katogo is made from matoke (green bananas), peeled and cooked in the same pot with a sauce (Beef, peanuts, beans or greens), Katogo is served with tea or juice.[106] Both dishes are popular in all regions of Uganda.		A croissant, served mainly in France and Western Europe.		A breakfast consisting of a novelty-stamped waffle, cantaloupe melon slices, grapes, and butter. Photo taken in Currier House of Harvard College		A pancake from Japan, topped with butter and honey		idli and sambar - a south Indian breakfast		Inflight airline meal – breakfast during a Thai Airways flight		
Smörgåsbord (Swedish: [ˈsmœrɡɔsˌbuːɖ] ( listen)) is a type of Scandinavian meal, originating in Sweden, served buffet-style with multiple hot and cold dishes of various foods on a table.		Smörgåsbord became internationally known, spelled smorgasbord, at the 1939 New York World's Fair when it was offered at the Swedish Pavilion's "Three Crowns Restaurant."[1] It is typically a celebratory meal and guests can help themselves from a range of dishes laid out for their choice. In a restaurant, the term refers to a buffet-style table laid out with many small dishes from which, for a fixed amount of money, one is allowed to choose as many as one wishes.						In Northern Europe, the term varies between 'cold table' and 'buffet': In Norway it is called koldtbord or kaldtbord and in Denmark det kolde bord (literally the cold table); in Germany kaltes Buffet (literally cold buffet); in Iceland it is called hlaðborð (farmyard/courtyard table), in Estonia it is called külmlaud (cold table) or rootsi laud (Swedish table), in Finland voileipäpöytä (butter-bread/sandwich table) or ruotsalainen seisova pöytä (Swedish standing table/buffet). In Eastern Europe, each language has a term that literally means Swedish table: zviedru galds in Latvia (however aukstais galds - the cold table is also a popular name), švediškas stalas in Lithuania, шведский стол (shvedskiy stol) in Russia, szwedzki stół in Poland, шведський стіл (shvedskyi stil) in Ukraine, švédský stůl in the Czech Republic, švédsky stôl in Slovakia, svédasztal in Hungary, шведски сто / švedski sto in Serbia, švedski stol in Croatia, švedski stol / švedski sto in Bosnia-Herzegovina, шведска маса (shvedska masa) in Bulgaria, and "Bufet suedez" ("Swedish buffet") in Romania. Similarly, in Japan バイキング / ヴァイキング (baikingu / vaikingu, i.e. "Viking") is a popular name used.		The Swedish word smörgåsbord consists of the words smörgås (sandwich, usually open-faced) and bord (table). Smörgås in turn consists of the words smör (butter, cognate with English smear) and gås. Gås literally means goose, but later referred to the small pieces of butter that formed and floated to the surface of cream while it was churned.[2] These pieces reminded the old Swedish peasants of fat geese swimming to the surface.[citation needed] The small butter pieces were just the right size to be placed and flattened out on bread, so smörgås came to mean buttered bread. In Sweden, the term att breda smörgåsar (to spread butter on open-faced sandwiches) has been used since at least the 16th century.		In English and also in Scandinavian languages, the word smörgåsbord (or in English, more usually without diacritics as smorgasbord) refers loosely to any buffet with a variety of dishes — not necessarily with any connection to the Swedish Christmas traditions discussed in this article. In an extended sense, the word is used to refer to any situation which invites patrons to select whatever they wish among lots of pleasant things, such as the smorgasbord of university courses, books in a bookstore, etc.		A traditional Swedish smörgåsbord consists of both hot and cold dishes. Bread, butter, and cheese are always part of the smörgåsbord. It is customary to begin with the cold fish dishes which are generally various forms of herring, salmon, and eel. After eating the first portion, people usually continue with the second course (other cold dishes), and round off with hot dishes. Dessert may or may not be included in a smörgåsbord.		A special Swedish type of smörgåsbord is the julbord (literally "Christmas table"). The classic Swedish julbord is central to traditional Swedish cuisine, often including bread dipped in ham broth and continuing with a variety of fish (salmon, herring, whitefish and eel), baked ham, meatballs, pork ribs, head cheese, sausages, potato, Janssons frestelse, boiled potatoes, cheeses, beetroot salad, various forms of boiled cabbage, kale and rice pudding.		It is customary to eat particular foods together; herring is typically eaten with boiled potatoes and hard-boiled eggs and is frequently accompanied by strong spirits like snaps, brännvin or akvavit with or without spices. Other traditional are smoked eel, rollmops, herring salad, baked herring and smoked salmon.		Other dishes are pork sausages (fläskkorv), smoked pork and potato sausages (isterband), cabbage rolls (kåldolmar), baked beans, omelette with shrimps or mushrooms covered with béchamel sauce. Side dishes include beetroot salad in mayonnaise and warm stewed red, green or brown cabbage.		Lutfisk, lyed fish made of stockfish (dried ling or cod served with boiled potato and thick white sauce) and green peas that can be served with the warm dishes or as a separate fourth course. Lutfisk is often served as dinner the second day after the traditional Christmas Yule-table dinner. Julbord desserts include rice pudding (risgrynsgröt), sprinkled with cinnamon powder.photo Traditionally, an almond is hidden in the bowl of rice porridge and whoever finds it receives a small prize or is recognized for having good luck. Julbord is served from early December until just before Christmas at restaurants and until Epiphany in some homes. It is tradition for most Swedish and Norwegian workplaces to hold an annual Julbord between November and January.		In Denmark a typical tradition resembling the Swedish "julbord" is "Julefrokost" ("Christmas lunch"), which involves a wellstocked Danish smörgåsbord with cold as well as hot dishes, and plenty of beer and snaps. It is distinct from the Danish Christmas dinner which is served on December 24, and is served as a lunchtime meal, usually for family and friends on December 25 or 26. It is also a tradition for most Danish workplaces to hold an annual Julefrokost some time during the months of November to January as well.		The members of the Swedish merchant and upper class in sixteenth-century Sweden and Finland served schnapps table (brännvinsbord), a small buffet presented on a side table offering a variety of hors d'oeuvres served prior to a meal before sitting at the dinner table.[3] The most simple brännvinsbord was bread, butter, cheese, herring and several types of liqueurs, but smoked salmon, sausages and cold cuts were also served. The brännvinsbord was served as an appetizer for a gathering of people and eaten while standing before a dinner or supper, often two to five hours before dinner, sometimes with the men and women in separate rooms.[4] The smörgåsbord became popular in the mid-seventeenth century, when the food moved from the side table to the main table[4] and service began containing both warm and cold dishes. Smörgåsbord was also served as an appetizer in hotels and later at railway stations, before the dining cars time for the guests. Restaurants in Stockholm at the 1912 Olympic Games stopped serving smörgåsbord as an appetizer and started serving them instead as a main course.		The term is also used as a metaphor to indicate any diverse group, synonymous with a vast array of possible choices.[5] In the final of the Eurovision Song Contest 2013, host Petra Mede performed a song about all the Swedish clichés, referring to it as a "Swedish Smörgåsbord".[6]		
A carnivore /ˈkɑːrnɪvɔər/ meaning 'Meat Eater' (Latin, carne meaning 'meat' or 'flesh' and vorare meaning 'to devour') is an organism that derives its energy and nutrient requirements from a diet consisting mainly or exclusively of animal tissue, whether through predation or scavenging.[1][2] Animals that depend solely on animal flesh for their nutrient requirements are called obligate carnivores while those that also consume non-animal food are called facultative carnivores.[2] Omnivores also consume both animal and non-animal food, and apart from the more general definition, there is no clearly defined ratio of plant to animal material that would distinguish a facultative carnivore from an omnivore.[3] A carnivore that sits at the top of the food chain is termed an apex predator.		Plants that capture and digest insects (and, at times, other small animals) are called carnivorous plants. Similarly, fungi that capture microscopic animals are often called carnivorous fungi.						The word "carnivore" sometimes refers to the mammalian order Carnivora, but this is somewhat misleading. While many Carnivora meet the definition of being meat eaters, not all do, and even fewer are true obligate carnivores (see below). For example, most species of bears are actually omnivorous, except for the giant panda, which is almost exclusively herbivorous, and the exclusively meat-eating polar bear, which lives in the Arctic, where few plants grow. In addition, there are plenty of carnivorous species that are not members of Carnivora.		Outside the animal kingdom, there are several genera containing carnivorous plants and several phyla containing carnivorous fungi. The former are predominantly insectivores, while the latter prey mostly on microscopic invertebrates, such as nematodes, amoebae and springtails.		Carnivores are sometimes characterized by the type of prey that they consume. For example, animals that eat insects and similar invertebrates primarily or exclusively are called insectivores, while those that eat fish primarily or exclusively are called piscivores. The first tetrapods, or land-dwelling vertebrates, were piscivorous amphibians known as labyrinthodonts. They gave rise to insectivorous vertebrates and, later, to predators of other tetrapods.[4]		Carnivores may alternatively be classified according to the percentage of meat in their diet. The diet of a hypercarnivore consists of more than 70% meat, that of a mesocarnivore 50–70%, and that of a hypocarnivore less than 30%, with the balance consisting of non-animal foods such as fruits, other plant material, or fungi.		Obligate carnivores, or "true" carnivores, are those carnivores whose survival depends on nutrients which are found only in animal flesh. While obligate carnivores might be able to ingest small amounts of plant material, because of their evolution they lack the necessary physiology required to digest that plant matter. In fact, some obligate carnivorous mammals will only ever ingest vegetation for its specific use as an emetic to self-induce vomiting to rid itself of food that has upset its stomach.		For instance, felids including the domestic cat are obligate carnivores requiring a diet of primarily animal flesh and organs.[5] Specifically, cats have high protein requirements and their metabolisms appear unable to synthesize certain essential nutrients (including retinol, arginine, taurine, and arachidonic acid), and thus, in nature, they can rely only on animal flesh as their diet to supply these nutrients.[6][7]		Characteristics commonly associated with carnivores include organs for capturing and disarticulating prey (teeth and claws serve these functions in many vertebrates) and status as a predator. In truth, these assumptions may be misleading, as some carnivores do not hunt and are scavengers (though most hunting carnivores will scavenge when the opportunity exists). Thus they do not have the characteristics associated with hunting carnivores. Carnivores have comparatively short digestive systems, as they are not required to break down tough cellulose found in plants. Many animals that hunt other animals have evolved eyes that face forward, thus making depth perception possible. This is almost universal among mammalian predators. Other predators, like crocodiles, as well as most reptiles and amphibians, have sideways facing eyes and hunt by ambush rather than pursuit.		The first vertebrate carnivores were fish, and then amphibians that moved on to land. Early tetrapods were large amphibious piscivores. Some scientists assert that Dimetrodon "was the first terrestrial vertebrate to develop the curved, serrated teeth that enable a predator to eat prey much larger than itself."[8] While amphibians continued to feed on fish and later insects, reptiles began exploring two new food types, tetrapods (carnivory), and later, plants (herbivory). Carnivory was a natural transition from insectivory for medium and large tetrapods, requiring minimal adaptation (in contrast, a complex set of adaptations was necessary for feeding on highly fibrous plant materials).[4]		Carnivoramorphs are currently the dominant carnivorous mammals, and have been so since the Miocene. In the early to mid-Cenozoic, however, hyaenodonts, oxyaenid, entelodonts, ptolemaiidans, "arctocyonids" and "mesonychians" were dominant instead, representing a very high diversity of eutherian carnivores in the northern continents and Africa. In South America, sparassodonts were dominant instead, while Australia saw the presence of several marsupial predators, such as the dasyuromorphs and thylacoleonids.		In the Mesozoic, while theropod dinosaurs were the larger carnivores, several carnivorous mammal groups were already present. Most notable are the gobiconodontids, the triconodontid Jugulator, the deltatheroideans and Cimolestes. Many of these, such as Repenomamus, Jugulator and Cimolestes, were among the largest mammals in their faunal assemblages, capable of attacking dinosaurs.[9][10][11]		Most carnivorous mammals, from dogs to Deltatheridium, share several adaptations in common, such as carnassialiforme teeth, long canines and even similar tooth replacement patterns.[12] Most aberrant are thylacoleonids, which bear a diprodontan dentition completely unlike that of any mammal, and "eutriconodonts" like gobioconodontids and Jugulator, by virtue of their cusp anatomy, though they still worked in the same way as carnassials.[9]		Some theropod dinosaurs such as Tyrannosaurus rex that existed during the Mesozoic Era were probably obligate carnivores.		
Bento (弁当, bentō)[1] is a single-portion take-out or home-packed meal common in Japanese cuisine. A traditional bento holds rice, fish or meat, with pickled or cooked vegetables, usually in a box-shaped container. Containers range from disposable mass-produced to hand-crafted lacquerware. Bentos are readily available in many places throughout Japan, including convenience stores, bento shops (弁当屋, bentō-ya), railway stations, and department stores. However, Japanese homemakers often spend time and energy on a carefully prepared lunch box for their spouse, child, or themselves.		Bentos can be elaborately arranged in a style called "kyaraben" ("character bento"). Kyaraben are typically decorated to look like popular characters from Japanese animation (anime), comic books (manga), or video games. Another popular bento style is "oekakiben" or "picture bento". This is decorated to look like people, animals, buildings and monuments, or items such as flowers and plants. Contests are often held where bento arrangers compete for the most aesthetically pleasing arrangements.		There are similar forms of boxed lunches in Asian countries including the Philippines (baon), Korea (dosirak), Taiwan (biandang), and India (tiffin). Also, Hawaiian culture has adopted localized versions of bento featuring local tastes after over a century of Japanese influence in the islands.						In Japan, "Bento" is written as 弁当. The word of Bento originates from the Southern Song slang term 便当 (便當 (pinyin: biàndāng)), meaning "convenient" or "convenience." When imported to Japan, it was written with the ateji 便道 and 弁道[2][3].		In Japan, the word of Bento has been used since the 13th century and Bento, the container itself, has been in the 16th century[2].		In modern times, Bento is commonly used in The Western countries and East Asia. In mainland China and Taiwan, "Bento" is written as 便當 (pinyin: biàndāng).		The origin of bento can be traced back to the late Kamakura Period (1185 to 1333), when cooked and dried rice called hoshi-ii (糒 or 干し飯, literally "dried meal") was developed. Hoshi-ii can be eaten as is or boiled with water to make cooked rice, and is stored in a small bag. In the Azuchi-Momoyama Period (1568–1600), wooden lacquered boxes like today's were produced and bento would be eaten during a hanami or a tea party.		In the Edo period (1603–1867), bento culture spread and became more refined. Travelers and sightseers would carry a simple koshibentō (腰弁当, "waist bento"), consisting of several onigiri wrapped with bamboo leaves or in a woven bamboo box. One of the most popular styles of bento, called makuno-uchi bentō ("between-act bento"), was first made during this period.[4] People who came to see Noh and Kabuki ate specially prepared bentos between maku (acts). Numerous cookbooks were published detailing how to cook, how to pack, and what to prepare for occasions like Hanami and Hinamatsuri.		In the Meiji Period (1868–1912), the first ekibentō or ekiben (駅弁当 or 駅弁, "train station bento") was sold. There are several records that claim where ekiben was first sold, but it is believed that it was sold on 16 July 1885, at the Utsunomiya train station, and contained two onigiri and a serving of takuan wrapped in bamboo leaves. As early schools did not provide lunch, students and teachers carried bentos, as did many employees. "European" style bentos with sandwiches also went on sale during this period.		In the Taishō period (1912–1926), the aluminum bento box became a luxury item because of its ease of cleaning and its silver-like appearance. Also, a move to abolish the practice of bento in school became a social issue. Disparities in wealth spread during this period, following an export boom during World War I and subsequent crop failures in the Tohoku region. A bento too often reflected a student's wealth, and many wondered if this had an unfavorable influence on children both physically, from lack of adequate diet, and psychologically, from a clumsily made bento or the richness of food. After World War II, the practice of bringing bentos to school gradually declined and was replaced by uniform food provided for all students and teachers.		Bentos regained popularity in the 1980s, with the help of the microwave oven and the proliferation of convenience stores. In addition, the expensive wood and metal boxes have been replaced at most bento shops with inexpensive, disposable polystyrene boxes. However, even handmade bentos have made a comeback, and they are once again a common, although not universal, sight at Japanese schools. Bentos are still used by workers as a packed lunch, by families on day trips, for school picnics and sports days etc. The bento, made at home, is wrapped in a furoshiki cloth, which acts as both bag and table mat.		Airports also offer an analogous version of the ekiben: a bento filled with local cuisine, to be eaten while waiting for an airplane or during the flight.[5]		The bento made its way to Taiwan in the first half of the 20th century from Japan and remains very popular to the present day. The Japanese name was borrowed as bendong (Taiwanese: piān-tong) or Mandarin biàndāng (便當, "convenience pack").		In Japan, it is common for mothers to make bento for their children to take to school. Because making bento can take a while, some mothers will prepare the ingredients the night before and pack everything the following morning before their children go to school.[6]		It is often a social expectation of mothers to provide bento for their children, to create both a nutritionally balanced and aesthetically pleasing meal.[7] This expectation has bled into the gendered role of the mother and is emphasized by ideological state apparatus in Japan regarding childcare.[7] This construct is socially enforced by nursery school institutions.		An oekakiben containing rice balls decorated to resemble pandas		A set of stacking boxes for bento called "jūbako"		Bento served at a restaurant in Japan		Hanami bento in the Edo period		Shōkadō bentō		Two typical home made Bento (one open, one wrapped); note the furoshiki cloths		Taiwanese ekiben sold at Taitung Station		Hinomaru style rice (umeboshi in the center) in a Makunouchi bento		A bento consisting of salmon sashimi, chicken teriyaki and gyoza, served in a Japanese restaurant in Jakarta.		"Tōge no Kamameshi" bento		Okowa Bento, sticky glutinous rice mixed with all kinds of vegetables or meat and steamed served with teriyaki chicken and Japanese coleslaw		Bento boxes are now used in everyday life by by most people In Japan or/and China so thank them for this highly delicious food :)		
A commercium is a traditional academic feast known at universities in most Central and Northern European countries. In German it is called a Kommers or Commers. Today it is still organised by student fraternities in Germanic and Baltic countries and Poland.		At a commercium tables often are placed in the form of a U or a W, the participants drink beer and sing commercium songs. There are strict and traditional rules that govern this occasion but it may also integrate theatrical and musical aspects. A commercium is the more formal form of the tableround, called Kneipe in German.		The term is derived from French Commerce and had been used for any sort of noisy event.[1] A Commers gathering consists of speeches, toasts and songs, sometimes arranged pranks as well. The drink of preference is beer. The arrangements are governed by officials (Chargierte) elected by the members of the Studentenverbindung. The sort of event started to be more formalized after 1871. As well German associations like firefighters or Schützenvereine started to arrange commerciums in the 19th century and still do on special occasions.[2]		Some special customs include a salamander or Landesvater. The guests rise and having emptied their glasses hammer three times on the table with them. On the death of a student, his memory may be honored with a Trauercommers. The operetta The Student Prince made German students' drinking habits famous during the prohibition, and the rousing chorus of "Drink! Drink! Drink!" was especially popular with US theatergoers in 1924. In the last years of communist Eastern Germany, some students managed to arrange for new founded fraternities, e.g. Salana Jenensis in Jena and organized commerciums on the Rudelsburg.[3]		Invitation to a commercium in Eastern Germany 1988		Sorority students at a commercium in Riga, 2014		The head table of a fraternity commercium in Vienna in the early 1950s		Heidelberg Wingolf's hall for commerciums and roundtables		
1AX8		3952		16846		ENSG00000174697		ENSMUSG00000059201		P41159		P41160		NM_000230		NM_008493		NP_000221		NP_032519		Leptin (from Greek λεπτός leptos, "thin"), the "satiety hormone",[a] is a hormone made by adipose cells that helps to regulate energy balance by inhibiting hunger. Leptin is opposed by the actions of the hormone ghrelin, the "hunger hormone". Both hormones act on receptors in the arcuate nucleus of the hypothalamus to regulate appetite to achieve energy homeostasis.[4] In obesity, a decreased sensitivity to leptin occurs, resulting in an inability to detect satiety despite high energy stores.[5]		Although regulation of fat stores is deemed to be the primary function of leptin, it also plays a role in other physiological processes, as evidenced by its multiple sites of synthesis other than fat cells, and the multiple cell types beside hypothalamic cells that have leptin receptors. Many of these additional functions are yet to be defined.[6][7][8][9][10][11]						In 1949, a non-obese mouse colony being studied at the Jackson Laboratory produced a strain of obese offspring, suggesting that a mutation had occurred in a hormone regulating hunger and energy expenditure. Mice homozygous for the so-called ob mutation (ob/ob) ate voraciously and were massively obese.[12] In the 1960s, a second mutation causing obesity and a similar phenotype was identified by Douglas Coleman, also at the Jackson Laboratory, and was named diabetes (db), as both ob/ob and db/db were obese.[13][14][15] In 1990 Rudolph Leibel and Jeffrey M. Friedman reported mapping of the db gene.[16][17][18]		Consistent with Coleman’s and Leibel's hypothesis, several subsequent studies from Leibel's and Friedman’s labs and other groups confirmed that the ob gene encoded a novel hormone that circulated in blood and that could suppress food intake and body weight in ob and wild type mice, but not in db mice.[6][7][8][9]		In 1994, Friedman's laboratory reported the identification of the gene.[15] In 1995, Jose F. Caro's laboratory provided evidence that the mutations in the mouse ob gene did not occur in humans. Furthermore, since ob gene expression was increased, not decreased, in human obesity, it suggested resistance to leptin to be a possibility.[10] At the suggestion of Roger Guillemin, Friedman named this new hormone "leptin" from the Greek lepto meaning thin.[6][19] Leptin was the first fat cell-derived hormone (adipokine) to be discovered.[20]		Subsequent studies in 1995 confirmed that the db gene encodes the leptin receptor, and that it is expressed in the hypothalamus, a region of the brain known to regulate the sensation of hunger and body weight.[21][22][23][24]		Coleman and Friedman have been awarded numerous prizes acknowledging their roles in discovery of leptin, including the Gairdner Foundation International Award (2005),[25] the Shaw Prize (2009),[26] the Lasker Award,[27] the BBVA Foundation Frontiers of Knowledge Award[28] and the King Faisal International Prize,[29] Leibel has not received the same level of recognition from the discovery because he was omitted as a co-author of a scientific paper published by Friedman that reported the discovery of the gene. The various theories surrounding Friedman’s omission of Leibel and others as co-authors of this paper have been presented in a number of publications, including Ellen Ruppel Shell’s 2002 book The Hungry Gene.[30][31]		The discovery of leptin also is documented in a series of books including Fat: Fighting the Obesity Epidemic by Robert Pool,[32] The Hungry Gene by Ellen Ruppel Shell, and Rethinking Thin: The New Science of Weight Loss and the Myths and Realities of Dieting by Gina Kolata.[33][34] Fat: Fighting the Obesity Epidemic and Rethinking Thin: The New Science of Weight Loss and the Myths and Realities of Dieting review the work in the Friedman laboratory that led to the cloning of the ob gene, while The Hungry Gene draws attention to the contributions of Leibel.[citation needed]		The Ob(Lep) gene (Ob for obese, Lep for leptin) is located on chromosome 7 in humans.[35] Human leptin is a 16-kDa protein of 167 amino acids.		A human mutant leptin was first described in 1997,[36] and subsequently six additional mutations were described. All of those affected were from Eastern countries; and all had variants of leptin not detected by the standard immunoreactive technique, so leptin levels were low or undetectable. The most recently described eighth mutation reported in January 2015, in a child with Turkish parents, is unique in that it is detected by the standard immunoreactive technique, where leptin levels are elevated; but the leptin does not turn on the leptin receptor, hence the patient has functional leptin deficiency.[37] These eight mutations all cause extreme obesity in infancy, with hyperphagia.[37]		A nonsense mutation in the leptin gene that results in a stop codon and lack of leptin production was first observed in mice in 1950. In the mouse gene, arginine-105 is encoded by CGA and only requires one nucleotide change to create the stop codon TGA. The corresponding amino acid in humans is encoded by the sequence CGG and would require two nucleotides to be changed to produce a stop codon, which is much less likely to happen.[10]		A recessive frameshift mutation resulting in a reduction of leptin has been observed in two consanguineous children with juvenile obesity.		A Human Genome Equivalent (HuGE) review in 2004 looked at studies of the connection between genetic mutations affecting leptin regulation and obesity. They reviewed a common polymorphism in the leptin gene (A19G; frequency 0.46), three mutations in the leptin receptor gene (Q223R, K109R and K656N) and two mutations in the PPARG gene (P12A and C161T). They found no association between any of the polymorphisms and obesity.[38]		A 2006 study found a link between the common LEP-4548 G/A phenotype and morbid obesity in Taiwanese aborigines,[39][40] but a 2014 meta-analysis did not,[40] however, this polymorphism has been associated with weight gain in patients taking antipsychotics.[41][42][43]		The LEP-2548 G/A polymorphism has been linked with an increased risk of prostate cancer,[44] gestational diabetes,[45] and osteoporosis.[46]		Other rare polymorphisms have been found but their association with obesity are not consistent.[38]		A single case of a homozygous transversion mutation of the gene encoding for leptin was reported in January 2015.[37] It leads to functional leptin deficiency with high leptin levels in circulation. The transversion of (c.298G → T) changed aspartic acid to tyrosine at position 100 (p.D100Y). The mutant leptin could neither bind to nor activate the leptin receptor in vitro, nor in leptin-deficient mice in vivo. It was found in a two-year-old boy with extreme obesity with recurrent ear and pulmonary infections. Treatment with metreleptin led to "rapid change in eating behavior, a reduction in daily energy intake, and substantial weight loss".[37]		Leptin is produced primarily in the adipocytes of white adipose tissue. It also is produced by brown adipose tissue, placenta (syncytiotrophoblasts), ovaries, skeletal muscle, stomach (the lower part of the fundic glands), mammary epithelial cells, bone marrow,[47]gastric chief cells and P/D1 cells.[48]		Leptin circulates in blood in free form and bound to proteins.[49]		Leptin levels vary exponentially, not linearly, with fat mass.[50][51] Leptin levels in blood are higher between midnight and early morning, perhaps suppressing appetite during the night.[52] The diurnal rhythm of blood leptin levels may be modified by meal-timing.[53]		In humans, many instances are seen where leptin dissociates from the strict role of communicating nutritional status between body and brain and no longer correlates with body fat levels:		All known leptin mutations except one are associated with low to undetectable immunoreactive leptin blood levels. The exception is a mutant leptin reported in January 2015 which is not functional, but is detected with standard immunoreactive methods. It was found in a massively obese 2-1/2-year-old boy who had high levels of circulating leptin which had no effect on leptin receptors, so he was functionally leptin-deficient.[37]		It is important to recognize that the terms central, primary, and direct are not used interchangeably: Central vs peripheral refers to hypothalamic vs non-hypothalamic location of action of leptin; direct vs indirect refers to whether there is no intermediary, or there is an intermediary in the mode of action of leptin; and primary vs secondary is an arbitrary description of a particular function of leptin.[71]		Leptin acts on receptors in the lateral hypothalamus to inhibit hunger and the medial hypothalamus to stimulate satiety.[72]		Thus, a lesion in the lateral hypothalamus causes anorexia (due to a lack of hunger signals) and a lesion in the medial hypothalamus causes excessive hunger (due to a lack of satiety signals).[72] This appetite inhibition is long-term, in contrast to the rapid inhibition of hunger by cholecystokinin (CCK) and the slower suppression of hunger between meals mediated by PYY3-36. The absence of leptin (or its receptor) leads to uncontrolled hunger and resulting obesity. Fasting or following a very-low-calorie diet lowers leptin levels.[75][76][77][78] Leptin levels change more when food intake decreases than when it increases.[79] The dynamics of leptin due to an acute change in energy balance may be related to appetite and eventually, to food intake rather than fat stores.[80][81]		Leptin binds to neuropeptide Y (NPY) neurons in the arcuate nucleus in such a way as to decrease the activity of these neurons. Leptin signals to the hypothalamus which produces a feeling of satiety. Moreover, leptin signals may make it easier for people to resist the temptation of foods high in calories.[83]		Leptin receptor activation inhibits neuropeptide Y (NPY) and agouti-related peptide (AgRP), and activates α-melanocyte-stimulating hormone (α-MSH). The NPY neurons are a key element in the regulation of hunger; small doses of NPY injected into the brains of experimental animals stimulates feeding, while selective destruction of the NPY neurons in mice causes them to become anorexic. Conversely, α-MSH is an important mediator of satiety, and differences in the gene for the α-MSH receptor are linked to obesity in humans.		Leptin interacts with six types of receptors (Ob-Ra–Ob-Rf, or LepRa-LepRf), which in turn are encoded by a single gene, LEPR.[84] Ob-Rb is the only receptor isoform that can signal intracellularly via the Jak-Stat and MAPK signal transduction pathways,[85] and is present in hypothalamic nuclei.[86]		Generally, leptin is thought to enter the brain at the choroid plexus, where the intense expression of a form of leptin receptor molecule could act as a transport mechanism.[87]		Once leptin has bound to the Ob-Rb receptor, it activates the stat3, which is phosphorylated and travels to the nucleus to effect changes in gene expression, one of the main effects being the down-regulation of the expression of endocannabinoids, responsible for increasing hunger.[88] In response to leptin, receptor neurons have been shown to remodel themselves, changing the number and types of synapses that fire onto them.		Increased levels of melatonin causes a downregulation of leptin,[89] however, melatonin also appears to increase leptin levels in the presence of insulin, therefore causing a decrease in appetite during sleeping.[90] Partial sleep deprivation has also been associated with decreased leptin levels.[91]		Mice with type 1 diabetes treated with leptin or leptin plus insulin, compared to insulin alone had better metabolic profiles: blood sugar did not fluctuate so much; cholesterol levels decreased; less body fat formed.[92]		Non-hypothalamic targets of leptin are referred to as peripheral targets, in contrast to the hypothalamic target which is the central target. Leptin receptors are found on a wide range of cell types. There is a different relative importance of central and peripheral leptin interactions under different physiologic states, and variations between species.[47] In the periphery leptin is a modulator of energy expenditure, modulator between fetal and maternal metabolism, permissive factor in puberty, activator of immune cells, activator of beta islet cells, and a growth factor. Further, it interacts with other hormones and energy regulators: insulin, glucagon, insulin-like growth factor, growth hormone, glucocorticoids, cytokines, and metabolites.[47]		The role of leptin/leptin receptors in modulation of T cell activity in the immune system was shown in experimentation with mice. It modulates the immune response to atherosclerosis, of which obesity is a predisposing factor.[93]		Exogenous leptin can promote angiogenesis by increasing vascular endothelial growth factor levels.		Hyperleptinemia produced by infusion or adenoviral gene transfer decreases blood pressure in rats.[94][95]		Leptin microinjections into the nucleus of the solitary tract (NTS) have been shown to elicit sympathoexcitatory responses, and potentiate the cardiovascular responses to activation of the chemoreflex.[96]		In fetal lung, leptin is induced in the alveolar interstitial fibroblasts ("lipofibroblasts") by the action of PTHrP secreted by formative alveolar epithelium (endoderm) under moderate stretch. The leptin from the mesenchyme, in turn, acts back on the epithelium at the leptin receptor carried in the alveolar type II pneumocytes and induces surfactant expression, which is one of the main functions of these type II pneumocytes.[97]		In mice, and to a lesser extent in humans, leptin is required for male and female fertility. Ovulatory cycles in females are linked to energy balance (positive or negative depending on whether a female is losing or gaining weight) and energy flux (how much energy is consumed and expended) much more than energy status (fat levels). When energy balance is highly negative (meaning the woman is starving) or energy flux is very high (meaning the woman is exercising at extreme levels, but still consuming enough calories), the ovarian cycle stops and females stop menstruating. Only if a female has an extremely low body fat percentage does energy status affect menstruation. Leptin levels outside an ideal range may have a negative effect on egg quality and outcome during in vitro fertilization.[98] Leptin is involved in reproduction by stimulating gonadotropin-releasing hormone from the hypothalamus.[99]		The placenta produces leptin.[100] Leptin levels rise during pregnancy and fall after childbirth. Leptin is also expressed in fetal membranes and the uterine tissue. Uterine contractions are inhibited by leptin.[101] Leptin plays a role in hyperemesis gravidarum (severe morning sickness of pregnancy),[102] in polycystic ovary syndrome[103] and hypothalamic leptin is implicated in bone growth in mice.[104]		Immunoreactive leptin has been found in human breast milk; and leptin from mother's milk has been found in the blood of suckling infant animals.[105]		Leptin along with kisspeptin controls the onset of puberty.[106] High levels of leptin, as usually observed in obese females, can trigger neuroendocrine cascade resulting in early menarche.[107] This may eventually lead to shorter stature as oestrogen secretion starts during menarche and causes early closure of epiphyses.		Leptin's ability to regulate bone mass was first recognized in 2000.[108] Leptin can affect bone metabolism via direct signalling from the brain. Leptin decreases cancellous bone, but increases cortical bone. This "cortical-cancellous dichotomy" may represent a mechanism for enlarging bone size, and thus bone resistance, to cope with increased body weight.[109]		Bone metabolism can be regulated by central sympathetic outflow, since sympathetic pathways innervate bone tissue.[110] A number of brain-signalling molecules (neuropeptides and neurotransmitters) have been found in bone, including adrenaline, noradrenaline, serotonin, calcitonin gene-related peptide, vasoactive intestinal peptide and neuropeptide Y.[110][111] Leptin binds to its receptors in the hypothalamus, where it acts through the sympathetic nervous system to regulate bone metabolism.[112] Leptin may also act directly on bone metabolism via a balance between energy intake and the IGF-I pathway.[109][113] There is a potential for treatment of diseases of bone formation - such as impaired fracture healing - with leptin.[114]		Leptin receptors are expressed not only in the hypothalamus but also in other brain regions, particularly in the hippocampus. Thus some leptin receptors in the brain are classified as central (hypothalamic) and some as peripheral (non-hypothalamic).		Factors that acutely affect leptin levels are also factors that influence other markers of inflammation, e.g., testosterone, sleep, emotional stress, caloric restriction, and body fat levels. While it is well-established that leptin is involved in the regulation of the inflammatory response,[121][122][123] it has been further theorized that leptin's role as an inflammatory marker is to respond specifically to adipose-derived inflammatory cytokines.		In terms of both structure and function, leptin resembles IL-6 and is a member of the cytokine superfamily.[3][122][124] Circulating leptin seems to affect the HPA axis, suggesting a role for leptin in stress response.[125] Elevated leptin concentrations are associated with elevated white blood cell counts in both men and women.[126]		Similar to what is observed in chronic inflammation, chronically elevated leptin levels are associated with obesity, overeating, and inflammation-related diseases, including hypertension, metabolic syndrome, and cardiovascular disease. While leptin is associated with body fat mass, however, the size of individual fat cells, and the act of overeating, it is interesting that it is not affected by exercise (for comparison, IL-6 is released in response to muscular contractions). Thus, it is speculated that leptin responds specifically to adipose-derived inflammation.[127] Leptin is a pro-angiogenic, pro-inflammatory and mitogenic factor, the actions of which are reinforced through crosstalk with IL-1 family cytokines in cancer.[128]		Taken as such, increases in leptin levels (in response to caloric intake) function as an acute pro-inflammatory response mechanism to prevent excessive cellular stress induced by overeating. When high caloric intake overtaxes the ability of fat cells to grow larger or increase in number in step with caloric intake, the ensuing stress response leads to inflammation at the cellular level and ectopic fat storage, i.e., the unhealthy storage of body fat within internal organs, arteries, and/or muscle. The insulin increase in response to the caloric load provokes a dose-dependent rise in leptin, an effect potentiated by high cortisol levels.[129] (This insulin-leptin relationship is notably similar to insulin's effect on the increase of IL-6 gene expression and secretion from preadipocytes in a time- and dose-dependent manner.)[130] Furthermore, plasma leptin concentrations have been observed to gradually increase when acipimox is administered to prevent lipolysis, concurrent hypocaloric dieting and weight loss notwithstanding.[131] Such findings appear to demonstrate high caloric loads in excess of storage rate capacities of fat cells lead to stress responses that induce an increase in leptin, which then operates as an adipose-derived inflammation stopgap signaling for the cessation of food intake so as to prevent adipose-derived inflammation from reaching elevated levels. This response may then protect against the harmful process of ectopic fat storage, which perhaps explains the connection between chronically elevated leptin levels and ectopic fat storage in obese individuals.[61]		Although leptin reduces appetite as a circulating signal, obese individuals generally exhibit a higher circulating concentration of leptin than normal weight individuals due to their higher percentage body fat.[11] These people show resistance to leptin, similar to resistance of insulin in type 2 diabetes, with the elevated levels failing to control hunger and modulate their weight. A number of explanations have been proposed to explain this. An important contributor to leptin resistance is changes to leptin receptor signalling, particularly in the arcuate nucleus, however, deficiency of, or major changes to, the leptin receptor itself are not thought to be a major cause. Other explanations suggested include changes to the way leptin crosses the blood brain barrier (BBB) or alterations occurring during development.[132]		Studies on leptin cerebrospinal fluid (CSF) levels provide evidence for the reduction in leptin crossing the BBB and reaching obesity-relevant targets, such as the hypothalamus, in obese people.[133] In humans it has been observed that the ratio of leptin in the CSF compared to the blood is lower in obese people than in people of a normal weight.[134] The reason for this may be high levels of triglycerides affecting the transport of leptin across the BBB or due to the leptin transporter becoming saturated.[133] Although deficits in the transfer of leptin from the plasma to the CSF is seen in obese people, they are still found to have 30% more leptin in their CSF than lean individuals.[134] These higher CSF levels fail to prevent their obesity. Since the amount and quality of leptin receptors in the hypothalamus appears to be normal in the majority of obese humans (as judged from leptin-mRNA studies),[135] it is likely that the leptin resistance in these individuals is due to a post leptin-receptor deficit, similar to the post-insulin receptor defect seen in type 2 diabetes.[136]		When leptin binds with the leptin receptor, it activates a number of pathways. Leptin resistance may be caused by defects in one or more part of this process, particularly the JAK/STAT pathway. Mice with a mutation in the leptin receptor gene that prevents the activation of STAT3 are obese and exhibit hyperphagia. The PI3K pathway may also be involved in leptin resistance, as has been demonstrated in mice by artificial blocking of PI3K signalling. The PI3K pathway also is activated by the insulin receptor and is therefore an important area where leptin and insulin act together as part of energy homeostasis. The insulin-pI3K pathway can cause POMC neurons to become insensitive to leptin through hyperpolarization.[137]		The consumption of a high fructose diet from birth has been associated with a reduction in leptin levels and reduced expression of leptin receptor mRNA in rats. Long-term consumption of fructose in rats has been shown to increase levels of triglycerides and trigger leptin and insulin resistance,[138][139] however, another study found that leptin resistance only developed in the presence of both high fructose and high fat levels in the diet. A third study found that high fructose levels reversed leptin resistance in rats given a high fat diet. The contradictory results mean that it is uncertain whether leptin resistance is caused by high levels of carbohydrates or fats, or if an increase of both, is needed.[140]		Leptin is known to interact with amylin, a hormone involved in gastric emptying and creating a feeling of fullness. When both leptin and amylin were given to obese, leptin-resistant rats, sustained weight loss was seen. Due to its apparent ability to reverse leptin resistance, amylin has been suggested as possible therapy for obesity.[141]		It has been suggested that the main role of leptin is to act as a starvation signal when levels are low, to help maintain fat stores for survival during times of starvation, rather than a satiety signal to prevent overeating. Leptin levels signal when an animal has enough stored energy to spend it in pursuits besides acquiring food.[137][142] This would mean that leptin resistance in obese people is a normal part of mammalian physiology and possibly, could confer a survival advantage.[132] Leptin resistance (in combination with insulin resistance and weight gain) is seen in rats after they are given unlimited access to palatable, energy-dense foods.[143] This effect is reversed when the animals are put back on a low-energy diet.[144] This also may have an evolutionary advantage: allowing energy to be stored efficiently when food is plentiful would be advantageous in populations where food frequently may be scarce.[145]		Dieters who lose weight, particularly those with an overabundance of fat cells, experience a drop in levels of circulating leptin. This drop causes reversible decreases in thyroid activity, sympathetic tone, and energy expenditure in skeletal muscle, and increases in muscle efficiency and parasympathetic tone. The result is that a person who has lost weight below their natural body fat set-point has a lower basal metabolic rate than an individual at the same weight who is of that natural weight[citation needed]; these changes are leptin-mediated, homeostatic responses meant to reduce energy expenditure and promote weight regain as a result of fat cells being shrunken below normal size. Many of these changes are reversed by peripheral administration[clarification needed] of recombinant leptin to restore pre-diet levels.[146]		A decline in levels of circulating leptin also changes brain activity in areas involved in the regulatory, emotional, and cognitive control of appetite that are reversed by administration of leptin.[146]		Leptin was approved in the United States in 2014 for use in congenital leptin deficiency and generalized lipodystrophy.[147]		An analog of human leptin metreleptin (trade name Myalept) was first approved in Japan in 2013, and in the United States in February 2014. In the US it is indicated as a treatment for complications of leptin deficiency, and for the diabetes and hypertriglyceridemia associated with congenital or acquired generalized lipodystrophy.[148][149]		
An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSN are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2]		The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975.[3] ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.		When a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN), respectively.[citation needed] Conversely, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.[4]						The format of the ISSN is an eight digit code, divided by a hyphen into two four-digit numbers.[1] As an integer number, it can be represented by the first seven digits.[5] The last code digit, which may be 0-9 or an X, is a check digit. Formally, the general form of the ISSN code (also named "ISSN structure" or "ISSN syntax") can be expressed as follows:[6]		or by a PCRE regular expression:[7]		The ISSN of the journal Hearing Research, for example, is 0378-5955, where the final 5 is the check digit, that is C=5. To calculate the check digit, the following algorithm may be used:		To confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by its position in the number, counting from the right (if the check digit is X, then add 10 to the sum). The modulus 11 of the sum must be 0.		There is an online ISSN checker that can validate an ISSN, based on the above algorithm.[8][9]		ISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris. The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government. The International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System) otherwise known as the ISSN Register. At the end of 2016[update], the ISSN Register contained records for 1,943,572 items.[10]		ISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books. An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location. For this reason a new ISSN is assigned to a serial each time it undergoes a major title change.		Since the ISSN applies to an entire serial a new identifier, the Serial Item and Contribution Identifier (SICI), was built on top of it to allow references to specific volumes, articles, or other identifiable components (like the table of contents).		Separate ISSNs are needed for serials in different media (except reproduction microforms). Thus, the print and electronic media versions of a serial need separate ISSNs.[11] Also, a CD-ROM version and a web version of a serial require different ISSNs since two different media are involved. However, the same ISSN can be used for different file formats (e.g. PDF and HTML) of the same online serial.		This "media-oriented identification" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content, independent of media. This "content-oriented identification" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), as ISSN-independent initiative, consolidated in the 2000s.		Only later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an "ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media".[12]		The ISSN Register is not freely available for interrogation on the web, but is available by subscription. There are several routes to the identification and verification of ISSN codes for the public:		An ISSN can be encoded as a uniform resource name (URN) by prefixing it with "urn:ISSN:".[13] For example, Rail could be referred to as "urn:ISSN:0953-4563". URN namespaces are case-sensitive, and the ISSN namespace is all caps.[14] If the checksum digit is "X" then it is always encoded in uppercase in a URN.		The util URNs are content-oriented, but ISSN is media-oriented:		A unique URN for serials simplifies the search, recovery and delivery of data for various services including, in particular, search systems and knowledge databases.[12] ISSN-L (see Linking ISSN below) was created to fill this gap.		There are two most popular media types that adopted special labels (indicating below in italics), and one in fact ISSN-variant, with also an optional label. All are used in standard metadata context like JATS, and the labels also, frequently, as abbreviations.		p-ISSN is a standard label for "Print ISSN", the ISSN for the print media (paper) version of a serial. Usually it is the "default media", so the "default ISSN".		e-ISSN (or eISSN) is a standard label for "Electronic ISSN", the ISSN for the electronic media (online) version of a serial.		ISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007, the "linking ISSN (ISSN-L)" provides a mechanism for collocation or linking among the different media versions of the same continuing resource.		The ISSN-L is one ISSN number among the existing ISSNs, so, does not change the use or assignment of "ordinary" ISSNs;[16] it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L.		With ISSN-L is possible to designate one single ISSN for all those media versions of the title. The use of ISSN-L facilitates search, retrieval and delivery across all media versions for services like OpenURL, library catalogues, search engines or knowledge bases.[17]		
An assisted living residence or assisted living facility (ALF) is a housing facility for people with disabilities or for adults who cannot or chose not to live independently. The term is popular in the United States but is similar to a retirement home in the sense that facilities provide a group living environment and typically cater to an elderly population.		Assisted living exemplifies the shift from "care as service" to "care as business" in the broader health care arena predicted more than three decades ago.[1] A consumer-driven industry, assisted living offers a wide range of options, levels of care, and diversity of services (Lockhart, 2009) and is subject to state rather than federal regulatory oversight. Exactly what "assisted living" means depends on both the state and provider in question: variations in state regulatory definitions are significant and provider variables include everything from philosophy, geographic location and auspice, to organizational size and structure. Assisted living evolved from small "board and care" or "personal care" homes and offers a "social model" of care (compared to the medical model of a skilled nursing facility). The assisted living industry is a segment of the senior housing industry and assisted living services can be delivered in stand-alone facilities or as part of multi-level senior living community. The industry is fragmented and dominated by for-profit providers. In 2010, only six of the seventy largest providers were non-profit and none of the top twenty was non-profit (Martin, 2010). Information in this edit is from an article published in 2012 that reviewed the industry and reports results of a research study of assisted living facilities.[2]		In 2012 the U.S. Government estimated that there were 22,200 assisted living facilities in the U.S. (compared to 15,700 nursing homes) and that 713,300 people were residents of these facilities.[3] The number of assisted living facilities in the U.S. has increased dramatically since the early 2000s.		In the U.S. ALFs can be owned by for-profit companies (publicly traded companies or limited liability companies [LLCs]), non-profit organizations or governments.[4] These facilities typically provide supervision or assistance with activities of daily living (ADLs); coordination of services by outside health care providers; and monitoring of resident activities to help to ensure their health, safety, and well-being. Assistance often includes the administration or supervision of medication, or personal care services.		There has been controversy generated by reports of neglect, abuse and mistreatment of residents at assisted living facilities in the U.S.						In Canada, there are also some differences in how assisted living is understood from one province to the next. In most provinces, the phrase is understood as less independent than it is in the United States. People often require help with more than one of the activities of daily living or the more intensive ADLs like feeding or bathing. In the province of Alberta, "supportive living" is the distinct phrasing used for a type of care that is otherwise synonymous. The province's Supportive Living Accommodation Licensing Act is a comprehensive act with specific prescriptions governing care homes licensing, inspections and more.[5]		Within the United States assisted living spectrum, there is no nationally recognized definition of assisted living.[citation needed] Assisted living facilities are regulated and licensed at the US state level. More than two-thirds of the states use the licensure term "assisted living." Other licensure terms used for this philosophy of care include residential care home, assisted care living facilities, and personal care homes. Each state licensing agency has its own definition of the term it uses to describe assisted living. Because the term assisted living has not been defined in some states it is often a marketing term used by a variety of senior living communities, licensed or unlicensed. Assisted Living facilities in the United States had a national median monthly rate of $3,500.00 in 2014, a 1.45% increase over 2013 and a 4.29% increase over a five-year period from 2009-2014.[6][7]		As widely varied as the state licensing and definitions are, so are the types of physical layouts of buildings in which assisted living services are provided. Assisted living facilities can range in size from a small residential house for one resident up to very large facilities providing services to hundreds of residents. Assisted living falls somewhere between an independent living community and a skilled nursing facility in terms of the level of care provided. Continuing care retirement facilities combine independent living, assisted living, and nursing care in one facility.		People who live in newer assisted living facilities usually have their own private apartment. There is usually no special medical monitoring equipment that one would find in a nursing home, and their nursing staff may not be available at all hours. However, trained staff are usually on-site around the clock to provide other needed services. Household chores are performed: sheets are changed, laundry is done, and food is cooked and served as part of the base rent and included services. Depending on their disclosure of services, assisted living services may include medication management, bathing assistance, dressing, escorts to meals and activities, toileting, transferring, and insulin injections by an RN. Some homes even have a beauty parlor on site. Grocery service is often available too. Where provided, private apartments generally are self-contained; i.e., they have their own bedroom and bathroom, and may have a separate living area or small kitchen. Registered nurses and licensed practical nurses are available by phone or e-mail 24 hours a day, to ensure proper teaching and/or education of staff is available.		Alternatively, individual living spaces may resemble a dormitory or hotel room consisting of a private or semi-private sleeping area and a shared bathroom. There are usually common areas for socializing, as well as a central kitchen and dining room for preparing and eating meals.		An assisted living resident is defined as a resident who needs assistance with at least one of the activities of daily living.		A typical assisted living facility resident would usually be a senior citizen who does not need the level of care offered by a nursing home but prefers more companionship and needs some assistance in day-to-day living. Age groups will vary with every facility. There is currently a transformation occurring in long-term care. Assisted living communities are accepting higher and higher levels of care and nursing homes are becoming a place for those who are undergoing rehabilitation after a hospital stay or who need extensive assistance. Many assisted living communities now accept individuals who need assistance with all activities of daily living.		The "Overview of Assisted Living Report" from 2010 stated that 54 percent of assisted living residents are 85 years or older; 27 percent are 75–84 years old; 9 percent of residents are between 65 and 74 years; and 11 percent are younger than 65 years old. 74% of assisted living residents are female; 26 percent are male.[8]		The residence may assist in arranging the appropriate medical, health, and dental care services for each resident. The resident generally chooses his or her medical doctor and dental services.		Residents who have periods of temporary incapacity due to illness, injury, or recuperation from surgery often choose assisted living as a supportive option to help them recover quickly so then can return home. In the case of these short-term respite stays, assisted living residences act as the bridge between hospital and home.		Short-term respite stays in assisted living are also an option for families when the primary caregiver goes out of town or is otherwise unable to provide the needed care.[9]		More recently built facilities are designed with an emphasis on ease of use for disabled people. Bathrooms and kitchens are designed with wheelchairs and walkers in mind. Hallways and doors are extra-wide to accommodate wheelchairs. These facilities are by necessity fully compliant with the Americans with Disabilities Act of 1990 (ADA) or similar legislation elsewhere.		The socialization aspects of ALFs are very beneficial to the occupants. Normally the facility has many activities scheduled for the occupants, keeping in mind different disabilities and needs.		Many ALFs also serve the needs of people with some form of dementia including Alzheimer's disease and others with mental disabilities, as long as they do not present an imminent danger to themselves or others. These sections are often referred to as memory care. In the United States, legislation enacted by each state defines not only the level of care, but often what conditions are prohibited from being cared for in such a home.[10]		Many ALFs will work to accommodate a person who suffers from severe forms of Alzheimer's by having separate private units. These units are part of the main building but are secured so residents with Alzheimer's cannot leave and possibly do harm to themselves. The units usually house fewer people and more attention from the caregivers is provided.		The units, usually called locked units, focus on applying cognitive and mental activities to try to help keep the mind fresh. Since there is no cure for the disease, the goal is to work at prolonging or delaying the disease. If one is not engaged in activity, his or her memory will deteriorate more rapidly.		A 2011 investigation by the Miami Herald into assisted living facilities in Florida won the Pulitzer Prize and found that 1) "the safeguards once hailed as the most progressive in the nation have been ignored in a string of tragedies never before revealed to the public," 2) "that the Agency for Health Care Administration, which oversees the state’s 2,850 assisted-living facilities, has failed to monitor shoddy operators, investigate dangerous practices or shut down the worst offenders," and 3) "as the ranks of assisted-living facilities grew to make room for Florida’s booming elderly population, the state failed to protect the people it was meant to serve."[11] The investigation found dozens of incidents of gross mismanagement and criminal behavior at assisted living facilities across Florida, a state of 20 million people which is popular with American retirees. The newspaper requested the release of state documents related to the deaths of over 300 people in assisted living facilities between 2003 and 2011 but were denied these documents. Still, the newspaper's investigation found no less than 70 people who had died due to the "actions of their caregivers."[12] The deaths were found to have resulted from the mismanagement of assisted living facilities and by the practices of their staff and managers who drugged residents, deprived them of basic necessities such as food and water, abused residents verbally, psychologically and physically, and neglecting their needs.		On July 30, 2013 Frontline ran an hour long program[13] with help from ProPublica[14] detailing some tragedies that happened in assisted living.		At the time the documentary was broadcast and published, Frontline stated that "Today, nearly 750,000 people live in assisted living facilities across the country. National for-profit chains, concerned both about caring for their residents and pleasing their shareholders, have come to dominate the industry. Standards for care and training—and even definitions for the term 'assisted living'—vary from state to state. Assisted living facilities, unlike nursing homes, are not regulated by the federal government." An accompanying written brief cites deaths of residents, facilities that are understaffed, employees that are inadequately trained, and that an overall "push to fill facilities and maximize revenues has left staff overwhelmed and the care of residents endangered."		A related article by ProPublica (Thomson and Jones, July 29, 2013) states that a facility operated by Emeritus Senior Living "...had been found wanting in almost every important regard. And, in truth, those 'specially trained' staffers hadn’t actually been trained to care for people with Alzheimer’s and other forms of dementia, a violation of California law." It goes on to say, "The facility relied on a single nurse to track the health of its scores of residents, and the few licensed medical professionals who worked there tended not to last long," but also that "During some stretches, the facility went months without a full-time nurse on the payroll." ProPublica's article claimed the problem was not specific to one facility and that "State inspectors for years had cited Emeritus facilities across California." Emeritus replied to that claim, describing "any shortcomings as isolated," as well as that "any problems that arise are promptly addressed." The company cited their "growing popularity as evidence of consumer satisfaction."		
Happy hour is a marketing term for a period of time in which a venue (such as a restaurant, bar, bowling alley, stadium, or state or county fair) offers discounts on alcoholic drinks, such as beer, wine, and cocktails. Free Hors d'oeuvres, appetizers and discounted menu items are often served during Happy hour.						The words "happy" and "hour" have appeared together for centuries when describing pleasant times. In act I, scene 2 of William Shakespeare's King Henry V (said to have been written in about 1599), for example, King Henry says, "Therefore, my lords, omit no happy hour That may give furtherance to our expedition . . . ." The use of the phrase, "Happy Hour," to refer to a scheduled period of entertainment, however, is of much more recent vintage.		One possible origin of the term "Happy Hour," in the sense of a scheduled period of entertainment, is from the United States Navy. In early 1913, a group of "home makers" called the "Happy Hour Social" organized "semi-weekly smokers" on board USS Arkansas.[1] The name "Happy Hour Club," "Happy Hour Social Club," and similar variants, had been in use as the names of social clubs, primarily by women's social clubs, since at least the early 1880s. By June 1913, the crew of Arkansas had started referring to their regularly scheduled smokers as "Happy Hours."[2] The "Happy Hours" included a variety of entertainment, including boxing and wrestling matches, music, dancing and movies.[3] By the end of World War I, the practice of holding "Happy Hours" had spread throughout the entire Navy.[4]		The idea of drinking before dinner has its roots in the Prohibition era.[citation needed] When the 18th Amendment and the Volstead Act were passed banning alcohol consumption, people would host "cocktail hours", also known as "happy hours", at a speakeasy before eating at restaurants where alcohol could not be served. Cocktail lounges continued the trend of drinking before dinner.		The Random House Dictionary of American Slang dates "Happy hour," as a term for afternoon drinks in a bar, to a Saturday Evening Post article on military life in 1959. That article detailed the lives of government contractors and military personnel who worked at missile-tracking facilities in the Caribbean and the Atlantic. "Except for those who spend too much during “happy hour” at the bar – and there are few of these – the money mounts up fast."[3][5] Barry Popick's online etymology dictionary, The Big Apple, lists several pre-1959 citations to "Happy Hour" in print, mostly from places near Naval bases in California, from as early 1951.[6]		The Canadian province of Alberta created restrictions to happy hours that took effect in August 2008. All such promotions must end at 8 pm, and drink prices must conform to the Alberta Gaming and Liquor Commission's minimum price regulations at all times.[7]		In Ontario, while establishments may vary liquor prices as long as they stay above the minimum prices set by the Alcohol and Gaming Commission of Ontario, they are not permitted to advertise these prices "in a manner that may promote immoderate consumption." In particular, the phrase "happy hour" may not be used in such advertisement.[8]		Happy hour has been illegal in the Republic of Ireland since 2003 under the Intoxicating Liquor Act.[9]		The KHN, a hospitality sector lobby group, has agreed with its members to stop happy hours to discourage binge drinking by youth, but only if the government would vote to not raise the minimum drinking age.[10] In March 2013, the law to raise the drinking age to 18 was passed.[11]		In 2004 Glasgow banned happy hours to reduce binge drinking.[12] The national Mandatory Licensing Conditions introduced in 2010 required "all reasonable steps" to be taken to prevent irresponsible drinks promotions which effectively banned traditional happy hours.[13] Under the 2014 revision to these conditions, the licensee "must ensure" such promotions do not take place, although there is a subjective test, that takes account of the kind of establishment and its track record, for any promotions that offer unlimited or unspecified alcohol free or for a fixed or discounted fee.[13]		Massachusetts was one of the first U.S. states to implement a statewide ban on happy hours in 1984.[14] Other U.S. states also have similar restrictions, including Indiana and North Carolina. The reason for each ban varies, but most are for safety and health reasons.		In 1984, the U.S. military abolished happy hours at military base clubs.[15]		In 2011, the Utah State Legislature passed a ban on happy-hours, effective January 1, 2012.		In July 2011, Pennsylvania extended the period of time for happy hour from two hours to four hours.[16]		In June 2012, happy hour became legal in Kansas after a 26-year ban.[17]		In July 2015, a 25-year happy hour ban was ended in Illinois.[18]		By extension, certain file-hosting websites such as RapidShare and Megaupload use the term happy hour to designate periods during which users have complimentary access to certain premium features, such as increased bandwidth, elimination of queues, and bypassing of captcha verifications.		
Homemaking is a mainly American term for the management of a home, otherwise known as housework, housekeeping, or household management. It is the act of overseeing the organizational, day-to-day operations of a house or estate, and the managing of other domestic concerns. A person in charge of the homemaking, who isn't employed outside the home, is in the U.S. and Canada often called a homemaker, a gender-neutral term for a housewife or a househusband. The term "homemaker", however, may also refer to a social worker who manages a household during the incapacity of the housewife or househusband.[1]		Housework is not always a lifetime commitment; many, for economic or personal reasons, return to the workplace. In previous decades, there were many mandatory courses for the young to learn the skills of homemaking. In high school, courses included cooking, nutrition, home economics, family and consumer science (FACS), and food and cooking hygiene. This last one may underlie the tradition that a homemaker is portrayed wearing an apron. More recently, most of these courses have been abolished, and many youths in high school and college would be more likely to study child development and the management of children's behavior.						The method and function of housework are different in the industrial world and in other countries, with the balance of convenience, labor-saving devices and easier methods being in the industrial homemaker's favor. The reason for this is that mechanical invention has been applied extensively to different tasks of the home. Inventors have developed mechanical labor-saving devices not only for the shop and office, but also for the home. There are, on the market, thousands of household tools, devices and equipment for every domestic need. It only remains for the homemaker to choose between them.		Another reason for the great supply and demand for household labor savers in the industrial world is that the homemaker has to face the increasingly complex problem of scarce domestic help. With cheap labor, the need for the mechanical replacers of labor, or "mechanical servants," will not be keenly felt, however, the majority of homemakers perform their own household tasks. It is to this class of homemakers who are actively concerned in domestic work that the labor-saver and improved modern tool most appeal. The homemaker's time and effort are worth conserving by every means. Homemakers should, therefore, be eager to buy and use all the household tools that will save their strength and time and liberate them from household drudgery.		While some homemakers are "handy" with tools, the fact remains that most homemakers are unfamiliar with the different principles involved in mechanical tools and devices. The homemaker, however, is called to have knowledge of the principles of applied mechanics. Courses in school physics unfortunately leave a student with little practical knowledge that can be applied to domestic equipment. The gaining of knowledge concerning domestic tools may lead the homemaker to purchase good quality equipment, which may assist in saving time and labor.		Housekeeping by the homemaker is the care and control of property, ensuring its maintenance and proper use and appearance. A home is a place of residence.[2] In a private home a maid or housekeeper might be employed to do some of the housekeeping. Housework is work done by the act of housekeeping. Some housekeeping is housecleaning and some housekeeping is home chores. Home chores are housework that needs to be done at regular intervals,[3] Housekeeping includes the budget and control of expenditures, preparing meals and buying food, paying the heat bill, and cleaning the house.[4]		Most modern-day houses contain sanitary facilities and a means of preparing food. A kitchen is a room or part of a room used by the Homemaker for cooking, food preparation and food preservation. In the West, a modern kitchen is typically equipped with a stove, an oven, a sink with hot and cold running water, a refrigerator and kitchen cabinets. Many homemakers use a microwave oven, a dishwasher and other electric appliances. The main function of a kitchen is cooking or preparing food but it may also be used for dining and entertaining.		Cooking is the process of preparing food with or without heat, making and selecting, measuring and combining ingredients in an ordered procedure for producing safe and edible food. The process encompasses a vast range of methods, tools and combinations of ingredients to alter the flavor, appearance, texture, or digestibility of food. Factors affecting the final outcome include the variability of ingredients, ambient conditions, tools, and the skill of the individual doing the actual cooking.		The diversity of cooking worldwide is a reflection of the aesthetic, agricultural, economic, cultural, social and religious diversity throughout the nations, races, creeds and tribes across the globe. Applying heat to a food usually, though not always, chemically transforms it, thus changing its flavor, texture, consistency, appearance, and nutritional properties. Methods of cooking that involve the boiling of liquid in a receptacle have been practised at least since the 10th millennium BC, with the introduction of pottery.		Housecleaning by the homemaker is the systematic process of making a home neat and clean. This may be applied more broadly that just an individual home, or as a metaphor for a similar "clean up" process applied elsewhere such as a procedural reform. In the process of housecleaning general cleaning activities are completed, such as disposing of rubbish, storing of belongings in regular places, cleaning dirty surfaces, dusting and vacuuming. The details of this are various and complicated enough that many books have been published on the subject. How-to sites on the internet have many articles on housecleaning. Tools include the vacuum cleaner, broom and mop. Supplies such as cleaning solutions and sponges are sold in grocery stores and elsewhere. Professional cleaners can be hired for less frequent or specialist tasks such as cleaning blinds, rugs, and sofas. Professional services are also offered for the basic tasks. Safety is a consideration because some cleaning products are toxic and some cleaning tasks are physically demanding. Green cleaning refers to cleaning without causing pollution. The history of housecleaning has links to the advancement of technology.		Outdoor housecleaning chores include removing leaves from rain gutters, washing windows, sweeping doormats, cleaning the pool, putting away lawn furniture, and taking out the trash.[5]		Laundry refers to the act of washing clothing and linens, the place where that washing is done, and/or that which needs to be, is being, or has been laundered. Various chemicals may be used to increase the solvent power of water, such as the compounds in soaproot or yucca-root used by Native American tribes. Soap, a compound made from lye (from wood-ash) and fat, is an ancient and very common laundry aid. Modern washing machines typically use powdered or liquid laundry detergent in place of more traditional soap. Once clean, the clothes have been wrung out — twisted to remove most of the water. Then they were hung up on poles or clotheslines to air dry, or sometimes just spread out on clean grass.		Washing machines and dryers are now fixtures in homes around the world. In some parts of the world, including the USA, Canada, and Switzerland, apartment buildings and dormitories often have laundry rooms, where residents share washing machines and dryers. Usually the machines are set to run only when money is put in a coin slot. In other parts of the world, apartment buildings with laundry rooms are uncommon, and each apartment may have its own washing machine. Those without a machine at home or the use of a laundry room must either wash their clothes by hand or visit a commercial laundromat.		A clothes dryer is a household appliance that is used to remove moisture from a load of clothing and other textiles, generally shortly after they are cleaned in a washing machine. Most dryers consist of a rotating drum called a tumbler through which heated air is circulated to evaporate the moisture from the load. The tumbler is rotated relatively slowly in order to maintain space between the articles in the load. In most cases, the tumbler is belt-driven by an induction motor. Using these machines may cause clothes to shrink, become less soft (due to loss of short soft fibers/ lint) and fade. For these reasons, as well as environmental concerns, many people use open air methods such as a clothes line and clotheshorse.		Laundry starch is used in the laundering of clothes. Starch was widely used in Europe in the 16th and 17th centuries to stiffen the wide collars and ruffs of fine linen which surrounded the necks of the well-to-do. During the 19th century and early 20th century, it was stylish to stiffen the collars and sleeves of men'sshirts and the ruffles of girls' petticoats by applying starch to them as the clean clothes were being ironed. Aside from the smooth, crisp edges it gave to clothing, it served practical purposes as well. Dirt and sweat from a person's neck and wrists would stick to the starch rather than to the fibers of the clothing, and would easily wash away along with the starch. After each laundering, the starch would be reapplied. Today the product is sold in aerosol cans for home use.		Homemakers that follow predictive maintenance techniques determine the condition of in-service equipment in order to predict when maintenance should be performed. This approach offers cost savings over routine or time-based maintenance, because tasks are performed only when warranted. Homemakers that follow preventive maintenance methods ensure that household equipment and the house are in satisfactory operating condition by providing for inspection, detection, and correction of incipient failures either before they occur or before they develop into major defects.		Home maintenance involves the diagnosis and resolution of problems in a home, and is related to home maintenance to avoid such problems. Many types of maintenance are "Do it yourself" (DIY) projects. Maintenance is not necessarily the same as home improvement, although many improvements can result from repairs or maintenance. Often the costs of larger repairs will justify the alternative of investment in full-scale improvements. It may make just as much sense to upgrade a home system (with an improved one) as to repair it or incur ever-more-frequent and expensive maintenance for an inefficient, obsolete or dying system. For a DIY project, also useful is the established limits on time and money investments before a repair (or list of repairs) become overwhelming and discouraging, and less likely to ever be completed.		Homemakers that have a lawn responsibility adhere to seasonal lawn care practices, which vary to some extent depending on the climate zone and type of grass that is grown (whether cool season or warm season varieties). Various recognized method used by homemakers in lawn care are observed in any area. In spring or early summer, homemakers seed, sod, or sprig a yard when the ground is warmer. In Summer lawn mowers are used at high cutting for cool season grass, and lower cutting for warm season lawns. In autumn, lawns are mown by homemakers at a lower height and thatch build-up that occurs in warm season grasses are removed.[6] Homemakers do add sandy loam and apply fertilizer, containing some type of wetting agent. Cool season lawns are planted in the autumn with adequate rainfall. Lawn care in the winter is minimal, requiring only light feedings of organic material, such as green-waste compost, and minerals to encourage earthworms and beneficial microbes.		Household management by the homemaker is the act of overseeing the organizational, financial, and day-to-day operations of a house or estate. It differs from housekeeping, which consists of the physical maintenance and cleaning of a house.		Also common in the U.S are homemaking parties which involve a group of people doing household work instead of hanging out with their friends.		House organization or home organization includes interior design which is making the home aesthetically pleasing; and de-cluttering which is removing unnecessary things from the house.		Interior design is making the home aesthetically pleasing. Its activities include arranging furniture, having plants inside the house, and more.		Household de-cluttering involves putting things in their proper place after they have been used. "Cleaning up your mess" might involve removing glasses or eating utensils from the living room if you have eaten a meal there in front of the television. If several people have done that over a few days and not removed their glasses, dishes and utensils from the living room, the living room is considered to be "cluttered" with dishes. The dishes are out of place because they belong in the kitchen, washed and put away in the cupboards. That is the most common example of clutter in a modern American household.		There is another definition of clutter, which refers to having simply too many things and not enough room for all of it. Sometimes as happens in Asian households, the items are necessary, but the house is simply too small, and ingenious methods are needed to organize everything so that unsightly clutter does not result. However, removing unneeded or no longer necessary objects from a household or home is also an aspect of de-cluttering. Objects can be given away to friends or charitable organizations, sold as second-hand, recycled or thrown away.		Extreme forms of an inability to de-clutter is a behavioral aspect of compulsive hoarding. On the other end, a society that relies overly much on generating and then disposing of waste is referred to as throw-away society.		Household purchasing refers to homemaker's attempt to acquire goods or services to accomplish the goals of the household. Though there are several households that attempt to set standards in the purchasing process, processes can vary greatly between households. Typically the word “purchasing” is not used interchangeably with the word “procurement”, since procurement typically includes other concepts. Home makers decide the market goods that the household will buy, such as the groceries which have been bought at a grocer's.		Another important purchase handled by homemakers is the power source used for appliances. Home or other building heating may include boilers, furnaces, and water heaters. Compressed natural gas is used in rural homes without connections to piped-in public utility services, or with portable grills. However, due to being less economical than LPG, LPG (Propane) is the dominant source of rural gas for natural gas-powered ranges and/or ovens, natural gas-heated clothes dryers, heating/cooling and central heating. The amount of usages is determined by factors such as natural gas prices.		Homemakers may manage household workers or "domestic workers".		In sociology, household work strategy is the division of labour between members of a household, whether implicit or the result of explicit decision–making, with the alternatives weighed up in a simplified type of cost-benefit analysis.[7][8] It is a plan for the relative deployment of household members' time between the three domains of employment:		Household work strategies may vary over the life-cycle, as household members age, or with the economic environment; they may be imposed by one person or be decided collectively.[9]		Homemaking is described by economists as "household production". Household production has been defined as "the production of the goods and services by the members of a household, for their own consumption, using their own capital and their own unpaid labor. Goods and services produced by households for their own use include accommodation, meals, clean clothes, and child care. The process of household production involves the transformation of purchased intermediate commodities into final consumption commodities. Households use their own capital and their own labor." [10]		Goods and services created at the household level are generally consumed within the country within which they were produced, and hence contribute to "Domestic Consumption".[11]		The International Wages for Housework Campaign was a global, social movement co-founded in 1972 in Padua, Italy, by author and activist Selma James. The Campaign was formed to raise awareness of how housework and childcare are the base of all industrial work and to stake the claim that these unavoidable tasks should be compensated as paid, wage labor.[12] The demands for the Wages for Housework formally called for economic compensation for domestic work but also used these demands to more generally call attention to the affective labors of women, the reliance of capitalist economies on exploitative labor practices against women, and leisure inequality.[13]		Many home appliances have been invented that make housework faster or more effective compared to before the industrial revolution. These include:		Utilities can potentially eliminate work like gathering and chopping firewood, shovelling coal, fetching water from outdoors, and heating cold tap water.		Historian Ruth Schwartz Cowan claims that homemakers in the 1800s performed about 50–60 hours of work per week, and that this is the same as the 1990s. She says that labor-saving devices have been used to make the same amount of time do more work, such as by vacuuming a rug instead of sweeping it, or washing fabrics more frequently. Modern parents also more frequently transport their children after-school activities, and doctors no longer make house calls.[14]		
The brainstem (or brain stem) is the posterior part of the brain, adjoining and structurally continuous with the spinal cord. In the human brain the brainstem includes the midbrain, the pons, and the medulla oblongata. Sometimes the diencephalon, the caudal part of the forebrain, is included.[1]		The brainstem provides the main motor and sensory innervation to the face and neck via the cranial nerves. Of the twelve pairs of cranial nerves, ten pairs come from the brainstem. Though small, this is an extremely important part of the brain as the nerve connections of the motor and sensory systems from the main part of the brain to the rest of the body pass through the brainstem. This includes the corticospinal tract (motor), the posterior column-medial lemniscus pathway (fine touch, vibration sensation, and proprioception), and the spinothalamic tract (pain, temperature, itch, and crude touch).		The brainstem also plays an important role in the regulation of cardiac and respiratory function. It also regulates the central nervous system, and is pivotal in maintaining consciousness and regulating the sleep cycle. The brainstem has many basic functions including heart rate, breathing, sleeping, and eating.						The midbrain is divided into three parts. The first is the tectum, (Latin:roof), which forms the ceiling. The tectum comprises the paired structure of the superior and inferior colliculi and is the dorsal covering of the cerebral aqueduct. The inferior colliculus, is the principal midbrain nucleus of the auditory pathway and receives input from several peripheral brainstem nuclei, as well as inputs from the auditory cortex. Its inferior brachium (arm-like process) reaches to the medial geniculate nucleus of the diencephalon. Superior to the inferior colliculus, the superior colliculus marks the rostral midbrain. It is involved in the special sense of vision and sends its superior brachium to the lateral geniculate body of the diencephalon. The second part is the tegmentum which forms the floor of the midbrain, and is ventral to the cerebral aqueduct. Several nuclei, tracts, and the reticular formation are contained here. The third part, the ventral tegmentum is composed of paired cerebral peduncles. These transmit axons of upper motor neurons.		The midbrain consists of:		The pons lies between the medulla oblongata and the midbrain. It contains tracts that carry signals from the cerebrum to the medulla and to the cerebellum and also tracts that carry sensory signals to the thalamus. The pons is connected to the cerebellum by the cerebellar peduncles. The pons houses the respiratory pneumotaxic center and apneustic centers.		The medulla oblongata often just referred to as the medulla, is the lower half of the brainstem continuous with the spinal cord. Its upper part is continuous with the pons.[2] The medulla contains the cardiac, respiratory, vomiting and vasomotor centres dealing with heart rate, breathing and blood pressure.		In the medial part of the medulla is the anterior median fissure. Moving laterally on each side are the medullary pyramids. The pyramids contain the fibers of the corticospinal tract (also called the pyramidal tract), or the upper motor neuronal axons as they head inferiorly to synapse on lower motor neuronal cell bodies within the anterior grey column of the spinal cord.		The anterolateral sulcus is lateral to the pyramids. Emerging from the anterolateral sulci are the CN XII (hypoglossal nerve) rootlets. Lateral to these rootlets and the anterolateral sulci are the olives. The olives are swellings in the medulla containing underlying inferior nucleary nuclei (containing various nuclei and afferent fibers). Lateral (and dorsal) to the olives are the rootlets for CN IX (glossopharyngeal), CN X (vagus) and CN XI (accessory nerve). The pyramids end at the pontine medulla junction, noted most obviously by the large basal pons. From this junction, CN VI (abducens nerve), CN VII (facial nerve) and CN VIII (vestibulocochlear nerve) emerge. At the level of the midpons, CN V (the trigeminal nerve) emerges. Cranial nerve III (the oculomotor nerve) emerges ventrally from the midbrain, while the CN IV (the trochlear nerve) emerges out from the dorsal aspect of midbrain.		Between the two pyramids can be seen a decussation of fibres which marks the transition from the medulla to the spinal cord. The medulla is above the decussation and the spinal cord below.		The most medial part of the medulla is the posterior median sulcus. Moving laterally on each side is the fasciculus gracilis, and lateral to that is the fasciculus cuneatus. Superior to each of these, and directly inferior to the obex, are the gracile and cuneate tubercles, respectively. Underlying these are their respective nuclei. The obex marks the end of the 4th ventricle and the beginning of the central canal. The posterior intermediate sulci separates the fasciculi gracilis from the fasciculi cuneatus. Lateral to the fasciculi cuneatus is the lateral funiculus.		Superior to the obex is the floor of the 4th ventricle. In the floor of the 4th ventricle, various nuclei can be visualized by the small bumps that they make in the overlying tissue. In the midline and directly superior to the obex is the vagal trigone and superior to that it the hypoglossal trigone. Underlying each of these are motor nuclei for the respective cranial nerves. Superior to these trigones are fibers running laterally in both directions. These fibers are known collectively as the striae medullares. Continuing in a rostral direction, the large bumps are called the facial colliculi. Each facial colliculus, contrary to their names, do not contain the facial nerve nuclei. Instead, they have facial nerve axons traversing superficial to underlying abducens (CN VI) nuclei. Lateral to all these bumps previously discussed is an indented line, or sulcus that runs rostrally, and is known as the sulcus limitans. This separates the medial motor neurons from the lateral sensory neurons. Lateral to the sulcus limitans is the area of the vestibular system, which is involved in special sensation. Moving rostrally, the inferior, middle, and superior cerebellar peduncles are found connecting the midbrain to the cerebellum. Directly rostral to the superior cerebellar peduncle, there is the superior medullary velum and then the two trochlear nerves. This marks the end of the pons as the inferior colliculus is directly rostral and marks the caudal midbrain.		The adult human brainstem emerges from two of the three primary vesicles formed of the neural tube. The mesencephalon is the second of the three primary vesicles, and does not further differentiate into a secondary vesicle. This will become the midbrain. The third primary vesicle, the rhombencephalon (hindbrain) will further differentiate into two secondary vesicles, the metencephalon and the myelencephalon. The metencephalon will become the cerebellum and the pons. The more caudal myelencephalon will become the medulla.		The main supply of blood to the brainstem is provided by the basilar arteries and the vertebral arteries.[3]		Ten of the twelve pairs of cranial nerves either target or are sourced from the brainstem.[4] The nuclei of the oculomotor nerve (III) and trochlear nerve (IV) are located in the midbrain. The nuclei of the trigeminal nerve (V), abducens nerve (VI), facial nerve (VII) and vestibulocochlear nerve (VIII) are located in the pons. The nuclei of the glossopharyngeal nerve (IX), vagus nerve (X), accessory nerve (XI) and hypoglossal nerve (XII) are located in the medulla. The fibers of these cranial nerves exit the brainstem from these nuclei.[5]		There are three main functions of the brainstem:		Diseases of the brainstem can result in abnormalities in the function of cranial nerves that may lead to visual disturbances, pupil abnormalities, changes in sensation, muscle weakness, hearing problems, vertigo, swallowing and speech difficulty, voice change, and co-ordination problems. Localizing neurological lesions in the brainstem may be very precise, although it relies on a clear understanding on the functions of brainstem anatomical structures and how to test them.		Brainstem stroke syndrome can cause a range of impairments including locked-in syndrome.		Duret haemorrhages are areas of bleeding in the midbrain and upper pons due to a downward traumatic displacement of the brainstem.[7]		Cysts known as syrinxes can affect the brainstem, in a condition called syringobulbia. These fluid-filed cavities can be congenital, acquired or the result of a tumor.		Criteria for claiming brainstem death in the UK have developed in order to make the decision of when to stop ventilation of somebody who could not otherwise sustain life. These determining factors are that the patient is irreversibly unconscious and incapable of breathing unaided. All other possible causes must be ruled out that might otherwise indicate a temporary condition. The state of irreversible brain damage has to be unequivocal. There are brainstem reflexes that are checked for by two senior doctors so that imaging technology is unnecessary. The absence of the cough and gag reflexes, of the corneal reflex and the vestibulo-ocular reflex need to be established; the pupils of the eyes must be fixed and dilated; there must be an absence of motor response to stimulation and an absence of breathing marked by concentrations of carbon dioxide in the arterial blood. All of these tests must be repeated after a certain time before death can be declared.[8]		The midbrain, pons, and medulla oblongata are labelled on this coronal section of the human brain.		Brainstem. Anterior face.Deep dissection		Brainstem. Posterior face.Deep dissection		
A cookbook or cookery book[1] is a kitchen reference publication containing a collection of recipes, typically organized by type of dish.		Modern versions may also include colorful illustrations and advice on purchasing quality ingredients or making substitutions. Cookbooks can also cover a wide variety of topics, including cooking techniques for the home, recipes and commentary from famous chefs, institutional kitchen manuals, and cultural commentary.						The earliest cookbooks listed recipes of the finest cuisine of their day, either of the author's favorite dishes or to train professional cooks for banquets and upper-class, private homes.[citation needed]		Ancient Mesopotamian recipes have been found on three Akkadian tablets, dating to about 1700 BC.[2]		The earliest collection of recipes that has survived in Europe is De re coquinaria, written in Latin. An early version was first compiled sometime in the 1st century and has often been attributed to the Roman gourmet Marcus Gavius Apicius, though this has been cast in doubt by modern research. An Apicius came to designate a book of recipes. The current text appears to have been compiled in the late 4th or early 5th century; the first print edition is from 1483. It records a mix of ancient Greek and Roman cuisine, but with few details on preparation and cooking.[3]		An abbreviated epitome entitled Apici Excerpta a Vinidario, a "pocket Apicius" by Vinidarius, "an illustrious man",[4] was made in the Carolingian era.[5] In spite of its late date it represents the last manifestation of the cuisine of Antiquity.		The earliest cookbooks known in Arabic are those of al-Warraq (an early 10th-century compendium of recipes from the 9th and 10th centuries) and al-Baghdadi (13th century).[6]		Chinese recipe books are known from the Tang dynasty, but most were lost.[citation needed] One of the earliest surviving Chinese-language cookbooks is Hu Sihui's "Yinshan Zhengyao" (Important Principles of Food and Drink), believed to be from 1330. Hu Sihui, Buyantu Khan's dietitian and therapist, recorded a Chinese-inflected Central Asian cuisine as eaten by the Yuan court; his recipes were adapted from foods eaten all over the Mongol Empire.[7] Eumsik dimibang, written around 1670, is the oldest Korean cookbook and the first cookbook written by a woman in East Asia.		After a long interval, the first recipe books to be compiled in Europe since Late Antiquity started to appear in the late thirteenth century. About a hundred are known to have survived, some fragmentary, from the age before printing.[8] The earliest genuinely medieval recipes have been found in a Danish manuscript dating from around 1300, which in turn are copies of older texts that date back to the early 13th century or perhaps earlier.[9]		Low and High German manuscripts are among the most numerous. Among them is Daz buch von guter spise ("The Book of Good Food") written c. 1350 in Würzberg and Kuchenmeysterey ("Kitchen Mastery"), the first printed German cookbook from 1485.[10] Two French collections are probably the most famous: Le Viandier ("The Provisioner") was compiled in the late 14th century by Guillaume Tirel, master chef for two French kings; and Le Menagier de Paris ("The Householder of Paris"), a household book written by an anonymous middle class Parisian in the 1390s.[11]		From Southern Europe there is the 14th century Valencian manuscript Llibre de Sent Soví (1324), the Catalan Llibre de totes maneres de potatges de menjar ("The book of all recipes of dishes") and several Italian collections, notably the Venetian mid-14th century Libro per Cuoco,[12] with its 135 recipes alphabetically arranged. The printed De honesta voluptate et valetudine ("On honourable pleasure"), first published in 1475, is one of the first cookbooks based on Renaissance ideals, and, though it is as much a series of moral essays as a cookbook, has been described as "the anthology that closed the book on medieval Italian cooking".[13]		Recipes originating in England include the earliest recorded recipe for ravioli (1390s) and Forme of Cury, a late 14th-century manuscript written by chefs of Richard II of England.[14]		With the advent of the printing press in the 16th and 17th centuries, numerous books were written on how to manage households and prepare food. In Holland[15] and England[16] competition grew between the noble families as to who could prepare the most lavish banquet. By the 1660s, cookery had progressed to an art form and good cooks were in demand. Many of them published their own books detailing their recipes in competition with their rivals.[17] Many of these books have now been translated and are available online.[18]		By the 19th century, the Victorian preoccupation for domestic respectability brought about the emergence of cookery writing in its modern form. Although eclipsed in fame and regard by Isabella Beeton, the first modern cookery writer and compiler of recipes for the home was Eliza Acton. Her pioneering cookbook, Modern Cookery for Private Families published in 1845, was aimed at the domestic reader rather than the professional cook or chef. This was an immensely influential book, and it established the format for modern writing about cookery. The publication introduced the now-universal practice of listing the ingredients and suggested cooking times with each recipe. It included the first recipe for Brussels sprouts.[19] Contemporary chef Delia Smith is quoted as having called Acton "the best writer of recipes in the English language."[20] Modern Cookery long survived her, remaining in print until 1914 and available more recently in facsimile reprint. Acton's work was an important influence on Isabella Beeton,[21] who published Mrs Beeton's Book of Household Management in 24 monthly parts between 1857 and 1861. The book was a guide to running a Victorian household, with advice on fashion, child care, animal husbandry, poisons, the management of servants, science, religion, and industrialism.[22][23] Despite its title, most of the text consisted of recipes, such that another popular name for the volume is Mrs Beeton's Cookbook. Most of the recipes were illustrated with coloured engravings, and it was the first book to show recipes in a format that is still used today. Many of the recipes were plagiarised from earlier writers including Acton.		In 1896 the American cook Fannie Farmer (1857–1915) published The Boston Cooking School Cookbook which contained some 1,849 recipes.[24]		Cookbooks that serve as basic kitchen references (sometimes known as "kitchen bibles") began to appear in the early modern period. They provided not just recipes but overall instruction for both kitchen technique and household management. Such books were written primarily for housewives and occasionally domestic servants as opposed to professional cooks, and at times books such as The Joy of Cooking (USA), La bonne cuisine de Madame E. Saint-Ange (France), The Art of Cookery (UK, USA), Il cucchiaio d'argento (Italy), and A Gift to Young Housewives (Russia) have served as references of record for national cuisines. Cookbook also tell stories of the writers themselves and reflect upon the era in which they are written. They often reveal notions of social, political, environmental or economic contexts. For example, during the era of industrialization, convenience foods were brought into many households and were integrated and present in cookbooks written in this time. [25] Related to this class are instructional cookbooks, which combine recipes with in-depth, step-by-step recipes to teach beginning cooks basic concepts and techniques. In vernacular literature, people may collect traditional recipes in family cookbooks.		While western cookbooks usually group recipes for main courses by the main ingredient of the dishes, Japanese cookbooks usually group them by cooking techniques (e.g., fried foods, steamed foods, and grilled foods). Both styles of cookbook have additional recipe groupings such as soups or sweets.		International and ethnic cookbooks fall into two categories: the kitchen references of other cultures, translated into other languages; and books translating the recipes of another culture into the languages, techniques, and ingredients of a new audience. The latter style often doubles as a sort of culinary travelogue, giving background and context to a recipe that the first type of book would assume its audience is already familiar with. Popular Puerto Rican cookbook, Cocina Criollo, written by Carmen Aboy Valldejuli, includes recipes that are typically of traditional Puerto Rican cuisine such as mofongo and pasteles. Valldejuli’s cookbook was not only important to Puerto Ricans, but also very popular in the United States where her original cookbook has since been published in several editions, including English versions. [26]		Professional cookbooks are designed for the use of working chefs and culinary students and sometimes double as textbooks for culinary schools. Such books deal not only in recipes and techniques, but often service and kitchen workflow matters. Many such books deal in substantially larger quantities than home cookbooks, such as making sauces by the liter or preparing dishes for large numbers of people in a catering setting. While the most famous of such books today are books like Le guide culinaire by Escoffier or The Professional Chef by the Culinary Institute of America, such books go at least back to medieval times, represented then by works such as Taillevent's Viandier and Chiquart d'Amiço's Du fait de cuisine.		Single-subject books, usually dealing with a specific ingredient, technique, class of dishes or target group (e.g. for kids), are quite common as well. Jack Monroe for example features low budget recipes. Some imprints such as Chronicle Books have specialized in this sort of book, with books on dishes like curries, pizza, and simplified ethnic food. Popular subjects for narrow-subject books on technique include grilling/barbecue, baking, outdoor cooking, and even recipe cloning (Recipe cloning is copying commercial recipes where the original is a trade secret[27]).		Community cookbooks (also known as compiled, regional, charitable, and fund-raising cookbooks) are a unique genre of culinary literature. Community cookbooks focus on home cooking, often documenting regional, ethnic, family, and societal traditions, as well as local history.[28][29] Sondra Gotlieb, for example, wrote her cookbooks on Canadian food culture by visiting people and homes by region. She gathered recipes, observed the foodways, observed the people and their traditions of each region by being in their own homes. Gotlieb did this so that she could put together a comprehensive cookbook based on the communities and individuals that make up Canada. [30]Gooseberry Patch has been publishing community-style cookbooks since 1992 and built their brand on this community.		Cookbooks can also document the food of a specific chef (particularly in conjunction with a cooking show) or restaurant. Many of these books, particularly those written by or for a well-established cook with a long-running TV show or popular restaurant, become part of extended series of books that can be released over the course of many years. Popular chef-authors throughout history include people such as Delia Smith, Julia Child, James Beard, Nigella Lawson, Edouard de Pomiane, Jeff Smith, Emeril Lagasse, Claudia Roden, Madhur Jaffrey, Katsuyo Kobayashi, and possibly even Apicius, the semi-pseudonymous author of the Roman cookbook De re coquinaria, who shared a name with at least one other famous food figure of the ancient world.		Famous cookbooks from the past, in chronological order, include:		Several libraries have extensive collections of cookbooks.		The term cookbook is sometimes used metaphorically to refer to any book containing a straightforward set of already tried and tested "recipes" or instructions for a specific field or activity, presented in detail so that the users who are not necessarily expert in the field can produce workable results. Examples include a set of circuit designs in electronics, a book of magic spells, or The Anarchist Cookbook, a set of instructions on destruction and living outside the law. O'Reilly Media publishes a series of books about computer programming named the Cookbook series, and each of these books contain hundreds of ready to use, cut and paste examples to solve a specific problem in a single programming language.		
Humans in different cultures use a variety of tools to eat. This is a list of eating implements.						In some cultures, such as Ethiopian cuisine and Indian dining, breads or hands are used in place of utensils.		"Fun Dip" is a type of candy in the United States, where a solid candy "dipping stick" is used to convey flavored sugar to the eater's mouth. The dipper is first licked to provide moisture, and then dredged through a small pouch with the flavored sugar, so that the sugar sticks to the dipping stick.		Some single-serve ice cream is sold with a flat wooden spade, often erroneously called a "spoon", to lift the product to one's mouth.		Prepackaged tuna salad or cracker snacks may contain a flat plastic spade for similar purposes.		Over time, these utensils were combined in various ways in attempts to make eating more convenient or to reduce the total number of utensils required.		Some utensils are useful only for specific foods.		
An izakaya (居酒屋) (Japanese: [izakaja], ee-ZAH-ka-yah)[1] is a type of informal Japanese gastropub. They are casual places for after-work drinking. They have been compared to Irish pubs, tapas bars and early American saloons and taverns.[2]						The word izakaya entered the English language by 1987.[3] It is a compound word consisting of i (to stay) and sakaya (sake shop), indicating that izakaya originated from sake shops that allowed customers to sit on the premises to drink.[4] Izakaya are sometimes called akachōchin (red lantern) in daily conversation, as such paper lanterns are traditionally found in front of them.		Historian Penelope Francks points to the development of the izakaya in Japan, especially in Edo and along main routes, as one indicator of the growing popularity of sake as a consumer good by the late eighteenth century.[5] Before the Meiji period, people drank alcohol in sake shops standing. Some stores started using sake barrels as stools. Later, snacks were added.[6]		An izakaya in Tokyo made international news in 1962, when Robert F. Kennedy ate there during a meeting with Japanese labor leaders.[7]		Izakayas are often likened to taverns or pubs, but there are a number of differences.[8][9][10]		Depending on the izakaya, customers either sit on tatami mats and dine from low tables, as in the traditional Japanese style, or sit on chairs and drink/dine from tables. Many izakaya offer a choice of both as well as seating by the bar. Some izakaya restaurants are also tachi-nomi style, literally translated as "drinking while standing".[11]		Usually, customers are given an oshibori (wet towel) to clean their hands; the towels are cold in summer and hot in winter. Next, a tiny snack/an appetizer, called an otōshi in the Tokyo area or tsukidashi in the Osaka-Kobe area, will be served.[12] It is local custom and usually charged onto the bill in lieu of an entry fee.		The menu may be on the table, displayed on walls, or both. Picture menus are common in larger izakaya. Food and drink are ordered throughout the course of the session as desired. They are brought to the table, and the bill is added up at the end of the session. Unlike other Japanese styles of eating, food items are usually shared by everyone at the table, similar to Spanish tapas.		Common formats for izakaya (as well as much other) dining in Japan are known as nomi-hōdai ("all you can drink") and tabe-hōdai ("all you can eat"). For a set price per person, customers can continue ordering as much food and/or drink as they wish, usually with a time limit of two or three hours.		Izakaya dining can be intimidating to non-Japanese because of the wide variety of menu items and the slow pace. Food is normally ordered slowly over several courses rather than all at once. The kitchen will serve the food when it is ready rather than in the formal courses of Western restaurants. Typically a beer is ordered when one is sitting down before perusing the menu. Quickly prepared dishes such as hiyayakko or edamame are ordered first, followed with progressively more robust flavors such as yakitori or kara-age, finishing the meal with a rice or noodle dish to fill up.[13]		There are a wide variety of izakaya offering all sorts of dishes. Items typically available are:[14][15]		Some establishments offer a bottle keep service, where a patron can purchase an entire bottle of liquor (usually Shōchū or whisky) and store the unfinished portion for a future visit.[17]		Izakaya food is usually more substantial than tapas or mezze. Many items are designed to be shared.		Rice dishes such as ochazuke and noodle dishes such as yakisoba are sometimes eaten at the end to round off a drinking session. For the most part, the Japanese do not eat rice or noodles (shushoku – "staple food") at the same time as they drink alcohol, since sake, brewed from rice, traditionally takes the place of rice in a meal.[citation needed]		Izakaya were traditionally down-to-earth places where men drank sake and beer after work.[19] That trend is complemented by a growing population of independent women and students. Many izakaya today cater to a more diverse clientele by offering cocktails and wines as well as by improving the interior. Chain izakaya are often large and offer an extensive selection of food and drink, allowing it to host big, sometimes rowdy, parties. Watami, Shoya, Shirokiya, Tsubohachi, and Murasaki are some well known chains in Japan.[20]		Izakayas are often called akachōchin ("red lantern") after the red paper lanterns that are traditionally displayed outside.[21] Today, the term usually refers to small, non-chain izakaya.[citation needed] Some unrelated businesses that are not izakaya also sometimes display red lanterns.[22]		Cosplay izakaya became popular in the 2000s. The staff wears the costume and waits on customers. Sometimes, shows are run. Costumes include those for butlers and maids.[23][24]		Establishments specialising in oden are called oden-ya. They usually take the form of street stalls with seating and are popular in winter.		Robatayaki are places in which customers sit around an open hearth on which chefs grill seafood and vegetables. Fresh ingredients are displayed for customers to point at whenever they want to order.		Oden street stall in the property of Sensōji in Asakusa		Activity at a robatayaki. Seafood and vegetables to cook displayed		Yakitori-ya specialise in yakitori.[25] The chicken skewers are often grilled in front of customers.		Yakitori		Izakaya appear in Japanese novels with adaptations to TV drama and film. They have also inspired manga and gekiga. A modern novel Izakaya Chōji (居酒屋兆治)[26] is an example where the main character manages an izakaya; in the film adaptation, Ken Takakura played the part of Chōji.[27] A TV drama was produced in 1992 on Friday Drama Theater, Fuji Television.[28]		Images of izakaya in jidaigeki novels and films reflect the modern drinking and dining style of today sitting at tables. This was not often seen in countryside – aside from station towns along kaidō highways in the 17th to mid-19th century. Capacities at izakaya were restricted in major cities in the period that jidaigeki TV shows and films/movies set in Edo.[clarification needed]		
A tapa (Spanish pronunciation: [ˈtapa]), in Spanish cuisine, is an appetizer, or snack. It may be cold (such as mixed olives and cheese) or hot (such as chopitos, which are battered, fried baby squid). In select bars in Spain, tapas have evolved into an entire, sophisticated cuisine. In Spain, patrons of tapas can order many different tapas and combine them to make a full meal. In some Central American countries, such snacks are known as bocas. In parts of Mexico, similar dishes are called botanas.						The word "tapas" is derived from the Spanish/Portuguese verb tapar, "to cover", a cognate of the English top. Before the 19th century, European roads were in bad condition. Some were originally old Roman roads (viae romanae); some were trails dating from the Middle Ages. Travelling was slow and exhausting. Most people could not read or write, and Spain was no exception. Inns, called posadas, albergues, or bodegas, grew up along the roads, offering meals and rooms, plus fresh horses for travellers. Since few innkeepers could write and few travellers read, inns offered their guests a sample of the dishes available, on a "tapa" (the word for pot cover in Spanish). In fact, a "tapa" was (and still is) a small portion of any kind of Spanish cuisine.		According to The Joy of Cooking, the original tapas were thin slices of bread or meat which sherry drinkers in Andalusian taverns used to cover their glasses between sips. This was a practical measure meant to prevent fruit flies from hovering over the sweet sherry (see below for more explanations). The meat used to cover the sherry was normally ham or chorizo, which are both very salty and activate thirst. Because of this, bartenders and restaurant owners created a variety of snacks to serve with sherry, thus increasing their alcohol sales.[1] The tapas eventually became as important as the sherry.		Tapas have evolved through Spanish history by incorporating new ingredients and influences. Most of the Iberian Peninsula was invaded by the Romans, who introduced the olive[citation needed] and irrigation methods. The discovery of the New World brought the introduction of tomatoes, sweet and chili peppers, maize (corn), and potatoes, which were readily accepted and easily grown in Spain's microclimates.		There are many tapas competitions throughout Spain, but there is only one National Tapas competition,[2] which is celebrated every year in November. Since 2008, the City of Valladolid and the International School of Culinary Arts[3] have celebrated the International Tapas Competition for Culinary Schools.[4] Various schools from around the world come to Spain annually to compete for the best tapa concept.		Though the primary meaning of tapa is cover or lid, it has in Spain also become a term for this style of food. The origin of this new meaning is uncertain but there are several theories:		In Spain,[7] dinner is usually served between 9 and 11 p.m. (sometimes as late as midnight), leaving significant time between work and dinner. Therefore, Spaniards often go "bar hopping" (Spanish: Ir de tapas) and eat tapas in the time between finishing work and having dinner. Since lunch is usually served between 1 and 4 p.m., another common time for tapas is weekend days around noon as a means of socializing before proper lunch at home.		It is very common for a bar or a small local restaurant to have eight to 12 different kinds of tapas in warming trays with glass partitions covering the food. They are often very strongly flavored with garlic, chilies or paprika, cumin, salt, pepper, saffron and sometimes in plentiful amounts of olive oil. Often, one or more of the choices is seafood (mariscos), often including anchovies, sardines or mackerel in olive oil, squid or others in a tomato-based sauce, sometimes with the addition of red or green peppers or other seasonings. It is rare to see a tapas selection not include one or more types of olives, such as Manzanilla or Arbequina olives. One or more types of bread are usually available to eat with any of the sauce-based tapas.		In Andalusia and certain places in Madrid, Castilla-La Mancha, Castile and León, Asturias, and Extremadura, when one goes to a bar and orders a drink, often a tapa will be served with it free. As a drink, it is usual to ask for a caña (small beer), a chato (glass of wine) or a mosto (grape juice). In several cities, entire zones are dedicated to tapas bars, each one serving its own unique dish. In León, one can find the Barrio Húmedo, in Logroño Calle Laurel and in Burgos Calle de la Sombrerería and Calle de San Lorenzo.		Sometimes, especially in northern Spain, they are also called pinchos (pintxos in Basque) in Asturias, in Navarre, in La Rioja (Spain), the Basque Country, Cantabria and in some provinces, such as Salamanca, because many of them have a pincho or toothpick through them. The toothpick is used to keep whatever the snack is made of from falling off the slice of bread and to keep track of the number of tapas the customer has eaten. Differently priced tapas have different shapes or have toothpicks of different sizes. The price of a single tapa ranges from one to two euros. Another name for them is banderillas (diminutive of bandera "flag"), in part because some of them resemble the colorful spears used in bullfighting.		Tapas can be "upgraded" to bigger portions, equivalent to half a dish (media ración) or a whole one (ración). This is generally more economical when tapas are being ordered by more than one person. The portions are usually shared by diners, and a meal made up of raciones resembles a Chinese dim sum, Korean banchan or Middle Eastern mezze.		Tapas (pintxos) in San Sebastián		Pimientos de Padrón, deep-fried chili peppers, here served in a bar in Madrid		A tapa of calamares a la romana		Tapa of Russian salad		The term tapas narrowly refers to a type of Spanish cuisine, but it is also used more broadly to refer to any similar format dining. This is referred to more formally as small plates, but tapas is common. Such dishes are traditionally common in many parts of the world, and have become increasingly popular in the English-speaking world since about 2000, particularly under the influence of Spanish tapas.		Upmarket tapas restaurants and tapas bars are common in many cities of the United States, Mexico, Canada, Ireland and the United Kingdom. As with any cuisine exported from its original country, there can often be significant differences between the original Spanish dishes and the dishes as they are served abroad.[citation needed]		In Mexico, there are not many tapas bars. However, the "cantinas botaneras" come close to the Mexican version of a tapas bar, but they operate on a very different business model. The appetizers ("botanas") keep coming as long as the patron keeps ordering beer, liquor or mixed drinks. The more the patron drinks, the more he or she eats. These establishments, some over a hundred years old, such as La Opera, are particularly popular around the Centro Histórico in Mexico City, but there are similar cantinas farther out in other regions of the city (as in Coyoacán) and its metropolitan area, or even in other cities like Guadalajara, Jalisco and Xalapa, Veracruz.		Picada is a type of tapas eaten in Argentina, usually involving only cold dishes, such as olives, ham, salami and different types of cheese.		Tira-gostos (Portuguese pronunciation: [ˈt͡ʃiɾɐ ˈɡostʊs]) or petiscos ([peˈt͡ʃiskʊs]) are served in the bars of Brazil and typical as tapas-like side dishes to accompany beer or other alcoholic drinks. The better bars tend to have a greater variety, and rarer, more traditional, dishes (using, for example, lamb or goat meat, which are relatively uncommon in the diet of urbanites in southern Brazil).		People from the metropolitan area of Rio de Janeiro, which had the most Portuguese and the second-most Spanish immigration in Brazil, are among those who are most proud of their bar culture as a symbol of the city's nightlife, but bars that serve a variety of tapas-like side dishes are common in all state capitals and cities with more than 700,000 inhabitants.		Many tapas typical of Spanish cuisine that are rarer dishes in Portugal are more easily found in Brazil, due to the presence of the cultural heritage of the Spanish Brazilians as a result of immigration.		Cicchetti are small tapas-like dishes served in cicchetti bars in Venice, Italy. Venetians typically eat cicchetti for lunch or as late-afternoon snacks.		In Korea, drinking establishments often serve anju (안주) of various types, including meat, seafood, and vegetables. In Japan, izakaya are drinking establishments that serve accompaniments similar to tapas.		
Communal dining is the practice of dining with others.		Dining with others is centered on food and the people that come together in order to share a meal. Communal dining can take place in public establishments like restaurants or in private establishments (home). Communal dining is sharing a meal and conversation with a group of people. Usually people come together for a meal and conversation in a public dining establishment.[1][2]		Communal dining was an important part of ancient Rome's religious traditions. [3]		
Cicchetti (also sometimes spelled "cichetti" or called "cicheti" in Venetian language) are small snacks or side dishes, typically served in traditional "bàcari" (cicchetti bars or osterie) in Venice, Italy. Common cicchetti include tiny sandwiches, plates of olives or other vegetables, halved hard boiled eggs, small servings of a combination of one or more of seafood, meat and vegetable ingredients laid on top of a slice of bread or polenta,[1] and very small servings of typical full-course plates. Like Spanish tapas, one can also make a meal of cicchetti by ordering multiple plates. Normally not a part of home cooking, the cicchetti’s importance lies not just in the food itself, but also in how, when and where they are eaten: with fingers and toothpicks, usually standing up, hanging around the counter where they are displayed in numerous bars, osterie and bacari that offer them virtually all day long. Venice's many cicchetti bars are quite active during the day, as Venetians (and tourists) typically eat cicchetti in the late morning, for lunch, or as afternoon snacks. Cicchetti are usually accompanied by a small glass of local white wine, which the locals refer to as an "ombra" (shadow).[citation needed]		One of the most enjoyable aspects of Venetian social life is contained in the phrase: let’s go to drink a shadow, in Venetian language “Andémo béver un'ombra” is an invitation to go for a drink, and more exactly a small glass of wine (a shadow), which is typically drunk in one shot.		The expression is a prompt of the period in which the wine were transferred in the “Riva degli Schiavoni” and then sold in shaded stands, which were located at the base of the Bell Tower of San Marco; when the sun was rotating, the stands were moved, so they could continue to stay in the shade (ombra).[2]		Rest at a bar at any time of the day and will not be long before you see a client come in for a ciccheto. Sometimes it also happens to meet into a group of people, he is doing a tour of shadows, the Venetian version to go out for drinks.		Cicchetti is the plural form. A single piece of cicchetti is a cicchetto.						In the United States of America, cicchetti are popular in some Italian-American enclaves, where it is sometimes spelled "chiccetti."		
PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.		From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]						In addition to MEDLINE, PubMed provides access to:		Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]		Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]		As of 11 July 2017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed's records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950's to 16% in 2016.[8] Other significant proportion of records correspond to “Chemistry” (8.69%), “Therapy” (8.39%) and "Infection" (5%).				In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]		Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window.		PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms.		The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:		Likewise,		The new PubMed interface, launched in October 2009, encourages the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11]		For comprehensive, optimal searches in PubMed, it is necessary to have a thorough understanding of its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features, and are best carried out by PubMed search specialists or librarians,[12] who are able to select the right type of search and carefully adjust it for precision and recall.[13]		When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).		Publication type parameter enables many special features. A special feature of PubMed is its "Clinical Queries" section, where "Clinical Categories", "Systematic Reviews", and "Medical Genetics" subjects can be searched, with study-type 'filters' automatically applied to identify substantial, robust studies.[14] As these 'clinical girish' can generate small sets of robust studies with considerable precision, it has been suggested that this PubMed section can be used as a 'point-of-care' resource.[15]		Since July 2005, the MEDLINE article indexing process extracts important identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]		A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the 'Find related data' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The 'related articles' function has been judged to be so precise that some researchers suggest it can be used instead of a full search.[18]		A strong feature of PubMed is its ability to automatically link to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This important feature makes PubMed searches automatically more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.		The PubMed optional facility "My NCBI" (with free registration) provides tools for		and a wide range of other options.[19] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[20]		LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[21] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March 2010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[22] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.		In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[23]		PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[24] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[25]		askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[26]		A PMID (PubMed identifier or PubMed unique identifier)[27] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[28]		The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.		The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[29] As of October 2008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[30] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.		Lu[30] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:		As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[30] Without the need to store about 90 GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[44]		Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950's to 16% in 2016.[45]		
Chopsticks are shaped pairs of equal-length sticks that have been used as the traditional ancient kitchen and eating utensils in virtually all of East Asia for over 6000 years. First used by the Chinese, chopsticks later spread to other locations, including Japan, Korea, Cambodia, Laos, Malaysia, Myanmar, Nepal, Philippines,[2] Singapore, Taiwan, Thailand, Vietnam,[A] and more recently Hawaii, the West Coast of North America, [6][7] and cities with Chinese communities all around the globe. Chopsticks are smoothed and frequently tapered and are commonly made of bamboo, plastic, wood, or stainless steel. They are less commonly made from titanium, gold, silver, porcelain, jade, or ivory. Chopsticks are held in the dominant hand, between the thumb and fingers, and used to pick up pieces of food.						The English word "chopstick" may have derived from Chinese Pidgin English, in which "chop chop" meant "quickly".[8][9][10] According to the Oxford English Dictionary, the earliest published use of the word is in the 1699 book Voyages and Descriptions by William Dampier: "they are called by the English seamen Chopsticks".[11]		The Standard Chinese term for chopsticks is kuàizi (Chinese: 筷子). The first character (筷) is a semantic-phonetic compound with a phonetic part meaning "quick" (快), and a semantic part meaning "bamboo" (竹).[12]		In ancient written Chinese, the character for chopsticks was zhu (箸; Middle Chinese reconstruction: d̪jwo-). Although it may have been widely used in ancient spoken Chinese, its use was eventually replaced by the pronunciation for the character kuài (快), meaning "quick". The original character, though still used in writing, is rarely used in modern spoken Chinese. It, however, is preserved in Chinese dialects such as Hokkien and Teochew.		For written semantic differentiation between the "fast" (快) versus "chopsticks", a new character was created for "chopsticks" (筷) by adding the "bamboo" (竹) radical (⺮) to it.[13]		In Japanese, chopsticks are called hashi (箸). They are also known as otemoto (おてもと), a phrase commonly printed on the wrappers of disposable chopsticks. Te means hand and moto means the area under or around something. The preceding o is used for politeness.		In Korean, 저 (箸, jeo) is used in the compound jeotgarak (Hangul: 젓가락 ), which is composed of jeo "chopsticks" and garak "stick". Jeo cannot be used alone, but can be found in other compounds such as sujeo (Hangul: 수저 ), meaning "spoon and chopsticks".		In Vietnamese, chopsticks are called "đũa", which is written as 𥮊 with 竹 trúc (bamboo) as the semantic, and 杜 đỗ as the phonetic part. It is an archaic borrowing of the older Chinese term for chopsticks, 箸.		In Khmer, chopsticks are called "changkuah" (ចង្កឹះ).		in Filipino , Panipit or Pan-sipit or simply Sipit are the term for chopsticks (ᜉᜈᜒᜉᜒᜆ᜔) .		Chopsticks were invented in ancient China before the Shang dynasty (1766–1122 BCE) and most likely much earlier prior to establishment of the Xia dynasty sometime around 9000 years ago.[14] The earliest evidence were six chopsticks, made of bronze, 26 cm (10 inches) long and 1.1 to 1.3 cm (0.43 to 0.51 inches) wide, excavated from the Ruins of Yin near Anyang (Henan) and dated roughly to 1200 BCE; those were supposed to be used for cooking.[15][16][17] The earliest known extant textual reference to the use of chopsticks comes from the Han Feizi, a philosophical text written by Han Fei (c. 280–233 BCE) in the 3rd century BCE.[18]		The first chopsticks were probably used for cooking, stirring the fire, serving or seizing bits of food, and not as eating utensils. Chopsticks began to be used as eating utensils during the Han dynasty. Chopsticks were considered more lacquerware-friendly than other sharp eating utensils. It was not until the Ming dynasty that chopsticks came into normal use for both serving and eating. They then acquired the name kuaizi and the present shape.[19]		Chopsticks, spoon, and bowl of the Shang dynasty		A painting of a Japanese woman using chopsticks, by Utagawa Kuniyoshi		To use chopsticks, the lower chopstick is stationary, and rests at the base of the thumb, and between the ring finger and middle finger. The second chopstick is held like a pencil, using the tips of the thumb, index finger, and middle finger, and it is moved while eating, to pull food into the grasp of the chopsticks.[20] Chopsticks, when not in use, are placed either to the right or below one's plate in a Chinese table setting.[21]		Saibashi (菜箸; さいばし) are Japanese kitchen chopsticks used in Japanese cuisine. They are used in the preparation of Japanese food, and are not designed for eating. These chopsticks allow handling of hot food with one hand, and are used like regular chopsticks.[22] These chopsticks have a length of 30 cm (12 in) or more, and may be looped together with a string at the top. They are made from bamboo, but for deep frying, metal chopsticks with bamboo handles are preferred, as the tips of regular bamboo chopsticks discolor and get greasy after repeated use in hot oil. The bamboo handles protect against heat.		Chopsticks are usually used in noodle dishes such as Kuy Tiev and other soup dishes. Wooden or plastic chopsticks are the usual chopsticks used in Cambodian restaurants and other dining..[according to whom?]		Longer than other styles at about 27 centimetres (11 in), thicker, with squared or rounded sides and ending in either wide, blunt, flat tips or tapered pointed tips. Blunt tips are more common with plastic or melamine varieties whereas pointed tips are more common in wood and bamboo varieties. Chinese sticks may be composed of almost any material but the most common in modern-day restaurants is melamine plastic for its durability and ease of sanitation. The most common type of material in regular households is lacquered bamboo.		In Korea, chopsticks of medium-length with a small, flat rectangular shape are paired with a spoon made of the same, usually metal, material. The set is called sujeo. Also, Spoon and chopstick rest, which is the piece to rest sujeo without touching the table, is used in traditional eating. Many Korean metal chopsticks are ornately decorated at the grip.		In the past, materials for sujeo varied with social class: Sujeo used in the court were made with gold, silver, cloisonné and so on, while commoners usually used brass or wooden sujeo. Nowadays, sujeo is usually made with stainless steel, although bangjja is also popular in more traditional setting.		However, fork and spoon are they used aside for their hands to eat (referred to today as kamayan), chopsticks are mainly used in the Filipino Cuisine, especially in noodles dishes like Pancit, miki, Miswa and Sotanghon.[26][27]		Native cuisine uses a fork and spoon, adopted from the West. Ethnic Chinese immigrants introduced the use of chopsticks for foods that require them. Restaurants serving other Asian cuisines that utilize chopsticks use the style of chopstick, if any, appropriate for that cuisine.		Long sticks that taper to a blunt point; traditionally lacquered wood or bamboo. A đũa cả (𥮊奇) is a large pair of flat chopsticks that is used to serve rice from a pot.		Chopsticks are used in many parts of the world. While principles of etiquette are similar, finer points can differ from region to region.		In Cambodia, a fork and spoon are the typical utensils used in Cambodian dining and etiquette. Spoons are used to scoop up food or water and the fork is there to help guide the food onto the spoon. Chopsticks are normally used in noodle dishes such as the Kuy Tiev and soup dishes. When eating soup the chopsticks will typically be paired with the spoon, where the chopsticks will pick up the food and the spoon will be used to drink the broth. Forks are never to touch the mouth, it is thought as rude, thus it is not used to eat such dishes. [28] [29]		In Korea, chopsticks are paired with a spoon (the set is called sujeo).		In the Philippines spoon and forks are the typical utensils used , Chopsticks are typically used in noodle dishes (like Pancit for example)[39], the etiquette or manners of serving dishes with chopsticks are similarly to the Southern Chinese tradition.[40] serving dish with noodles or soup, chopsticks are paired with serving spoon, eating soup dish or noodles using bare hands or without serving spoons or chopsticks are inappropriate. [41] Do not begin to eat or drink until the oldest man at the table has been served and has begun. It is appropriate to thank the host at the end of the meal for the fine food.[42]		Historically, Thai people used bare hands to eat and occasionally used a spoon and fork for curries or soup,[43] an impact from the west. Many Thai noodle dishes, such as pad thai, are eaten with chopsticks.[44][45][46][43]		The most widespread use of disposable chopsticks is in Japan, where around a total of 24 billion pairs are used each year,[48][49][50] which is equivalent to almost 200 pairs per person yearly.[51] In China, an estimated 45 billion pairs of disposable chopsticks are produced yearly.[51] This adds up to 1.66 million cubic metres (59×10^6 cu ft) of timber[52] or 25 million fully grown trees every year.[51] In April 2006, the People's Republic of China imposed a five percent tax on disposable chopsticks to reduce waste of natural resources by overconsumption.[53][54] This measure had the most effect in Japan as many of its disposable chopsticks are imported from China,[51] which account for over 90% of the Japanese market.[50][55]		American manufacturers have begun exporting American-made chopsticks to China, using sweet gum and poplar wood as these materials do not need to be artificially lightened with chemicals or bleach, and are appealing to Asian consumers.[56]		The American-born Taiwanese singer Wang Leehom has publicly advocated use of reusable chopsticks made from sustainable materials.[57][58] In Japan, reusable chopsticks are known as maihashi or maibashi (マイ箸, meaning "my chopsticks").[59][60]		A 2003 study found that regular use of chopsticks by the elderly may slightly increase the risk of osteoarthritis in the hand, a condition in which cartilage is worn out, leading to pain and swelling in the hand joints.[61] There have also been concerns regarding the use of certain disposable chopsticks made from dark wood bleached white that may pose a health risk, causing coughing or leading to asthma.[62]		A 2006 Hong Kong Department of Health survey found that the proportion of people using distinctly separate serving chopsticks, spoons, or other utensils for serving food from a common dish has increased from 46% to 65% since the SARS outbreak in 2003.[63]		
Parenting or child rearing is the process of promoting and supporting the physical, emotional, social, and intellectual development of a child from infancy to adulthood. Parenting refers to the aspects of raising a child aside from the biological relationship.[1]		The most common caretaker in parenting is the biological parent(s) of the child in question, although others may be an older sibling, a grandparent, a legal guardian, aunt, uncle or other family member, or a family friend.[2] Governments and society may have a role in child-rearing as well. In many cases, orphaned or abandoned children receive parental care from non-parent blood relations. Others may be adopted, raised in foster care, or placed in an orphanage. Parenting skills vary, and a parent with good parenting skills may be referred to as a good parent.[3]		The English pediatrician and psychoanalyst Donald Winnicott described the concept of "good-enough" parenting in which a minimum of prerequisites for healthy child development are met. Winnicott wrote, "The good-enough mother...starts off with an almost complete adaptation to her infant's needs, and as time proceeds she adapts less and less completely, gradually, according to the infant's growing ability to deal with her failure."[4] Views on the characteristics that make one a good or "good-enough" parent vary from culture to culture. Additionally, research has supported that parental history both in terms of attachments of varying quality as well as parental psychopathology, particularly in the wake of adverse experiences, can strongly influence parental sensitivity and child outcomes.[5][6][7]						Social class, wealth, culture and income have a very strong impact on what methods of child rearing are used by parents.[8] Cultural values play a major role in how a parent raises their child. However, parenting is always evolving; as times change, cultural practices and social norms and traditions change[9]		In psychology, the parental investment theory suggests that basic differences between males and females in parental investment have great adaptive significance and lead to gender differences in mating propensities and preferences.[10]		A family's social class plays a large role in the opportunities and resources that will be made available to a child. Working-class children often grow up at a disadvantage with the schooling, communities, and parental attention made available to them compared to middle-class or upper-class upbringings[citation needed]. Also, lower working-class families do not get the kind of networking that the middle and upper classes do through helpful family members, friends, and community individuals and groups as well as various professionals or experts.[11]		A parenting style is the overall emotional climate in the home.[12] Developmental psychologist Diana Baumrind identified three main parenting styles in early child development: authoritative, authoritarian, and permissive.[13][14][15][16] These parenting styles were later expanded to four, including an uninvolved style. These four styles of parenting involve combinations of acceptance and responsiveness on the one hand and demand and control on the other.[17] Recent research has found that parenting style is significantly related to children's subsequent mental health and well-being. In particular, authoritative parenting is positively related to mental health and satisfaction with life, and authoritarian parenting is negatively related to these variables.[18]		There is no single or definitive model of parenting. With authoritarian and permissive (indulgent) parenting on opposite sides of the spectrum, most conventional and modern models of parenting fall somewhere in between. Parenting strategies as well as behaviors and ideals of what parents expect, whether communicated verbally and/or non-verbally, also play a significant role in a child's development.		A parenting practice is a specific behavior that a parent uses in raising a child.[12] For example, a common parent practice intended to promote academic success is reading books to the child. Storytelling is an important parenting practice for children in many Indigenous American communities.[22]		Parenting practices reflect the cultural understanding of children.[23] Parents in individualistic countries like Germany spend more time engaged in face-to-face interaction with babies and more time talking to the baby about the baby. Parents in more communal cultures, such as West African cultures, spend more time talking to the baby about other people, and more time with the baby facing outwards, so that the baby sees what the mother sees.[23] Children develop skills at different rates as a result of differences in these culturally driven parenting practices.[24] Children in individualistic cultures learn to act independently and to recognize themselves in a mirror test at a younger age than children whose cultures promote communal values. However, these independent children learn self-regulation and cooperation later than children in communal cultures. In practice, this means that a child in an independent culture will happily play by herself, but a child in a communal culture is more likely to follow his mother's instruction to pick up his toys.[24] Children that grow up in communities with a collaborative orientation to social interaction, such as some Indigenous American communities, are also able to self-regulate and become very self-confident, while remaining involved in the community.[25]		In Kenya, Africa, many male parents are not encouraged to be involved in their children's lives till they are about 12 years old.		Parenting skills are the guiding forces of a "good parent" to lead a child into a healthy adult, they influence on development, maintenance, and cessation of children’s negative and positive behaviors. Parenting takes a lot of skill and patience and is constant work and growth. The cognitive potential, social skills, and behavioral functioning a child acquires during the early years are fundamentally dependent on the quality of their interactions with their parents.		Canadian Council on Learning says that children benefit most (avoids poor developmental outcomes) when their parents:[26]		Parenting skills are often assumed to be self-evident or naturally present in parents. But those who come from a negative/vulnerable environment might tend to pass on what they suffered onto their families or for those who have inaccurate beliefs or poorer understanding of developmental milestones engage in only the way they know which may result in problematic parenting. Parenting practices are at particular risk during marital transitions like separation, divorce and remarriage;[27] if children fail to adequately adjust to these changes, they would be at risk of negative outcomes for example increased rule-breaking behavior, problems with peer relationships and increased emotional difficulties.[28] Virginia Satir emphasized these views by stating "Parenting...the most complicated job in the world."[29]		Research classifies competence and skills required in parenting as follows:[30]		Consistency is considered as the “backbone” of positive parenting skills and “overprotection” as the weakness.[32]		Parents around the world want what they believe is best for their children. However, parents in different cultures have different ideas of what is best.[33] For example, parents in a hunter–gatherer society or surviving through subsistence agriculture are likely to promote practical survival skills from a young age. Many such cultures begin teaching babies to use sharp tools, including knives, before their first birthdays.[34] This seen in communities where children have a considerate amount of autonomy at a younger age and are given the opportunity to become skilled in tasks that are sometimes classified as adult work by other cultures.[35] In some Indigenous American communities, child work provides children the opportunity to learn cultural values of collaborative participation and prosocial behavior through observation and participation alongside adults.[36] American parents strongly value intellectual ability, especially in a narrow "book learning" sense.[33] Italian parents value social and emotional abilities and having an even temperament.[33] Spanish parents want their children to be sociable.[33] Swedish parents value security and happiness.[33] Dutch parents value independence, long attention spans, and predictable schedules.[33] The Kipsigis people of Kenya value children who are not only smart, but who employ that intelligence in a responsible and helpful way, which they call ng/om.[33] Many Indigenous American communities value respect, participation in the community, and non-interference. The practice of non-interference is an important value in Cherokee culture. It requires that one respects the autonomy of others in the community by not interfering in their decision making by giving unsolicited advice.[37]		Differences in values cause parents to interpret different actions in different ways.[33] Asking questions is seen by many European American parents as a sign that the child is smart. Italian parents, who value social and emotional competence, believe that asking questions is a sign that the child has good interpersonal skills. Dutch parents, who value independence, view asking questions negatively, as a sign that the child is not independent.[33] Indigenous American parents often try to encourage curiosity in their children. Many use a permissive parenting style that enables the child to explore and learn through observation of the world around it.[25]		Differences in values can also cause parents to employ different tools to promote their values. Many European American parents expect specially purchased educational toys to improve their children's intelligence.[33] Some Spanish parents promote social skills by taking their children out for daily walks around the neighborhood.[33]		It is common for parents in many Indigenous American communities to use different tools in parenting such as storytelling—like myths—consejos, educational teasing, nonverbal communication, and observational learning to teach their children important values and life lessons.		Storytelling is a way for Indigenous American children to learn about their identity, community, and cultural history. Indigenous myths and folklore often personify animals and objects, reaffirming the belief that everything possess a soul and must be respected. These stories help preserve language and are used to reflect certain values or cultural histories.[38]		Consejos are a narrative form of advice giving that provides the recipient with maximum autonomy in the situation as a result of their indirect teaching style. Rather than directly informing the child what they should do, the parent instead might tell a story of a similar situation or scenario. The character in the story is used to help the child see what the implications of their decision may be, without directly making the decision for them. This teaches the child to be decisive and independent, while still providing some guidance.[39]		The playful form of teasing is a parenting method used in some Indigenous American communities to keep children out of danger and guide their behavior. This form of teasing utilizes stories, fabrications, or empty threats to guide children in making safe, intelligent decisions. It can teach children values by establishing expectations and encouraging the child to meet them via playful jokes and/or idle threats. For example, a parent may tell a child that there is a monster that jumps on children's backs if they walk alone at night. This explanation can help keep the child safe because instilling that alarm creates greater awareness and lessens the likelihood that they will wander alone into trouble.[40]		In Navajo families, a child’s development is partly focused on the importance of "respect" for all things as part of the child’s moral and human development. "Respect" in this sense is an emphasis of recognizing the significance of and understanding for one's relationship with other things and people in the world. Nonverbal communication is much of the way that children learn about such "respect" from parents and other family members.[41]		For example, in a Navajo parenting tool using nonverbal communication, children are initiated at an early age into the practice of an early morning run through any weather condition. This form of guidance fosters “respect” not only for the child's family members but also to the community as a whole. On this run, the community uses humor and laughter with each other, without directly including the child—who may not wish to get up early and run—to promote the child’s motivation to participate and become an active member of the community.[41] To modify children’s behavior in a nonverbal manner, parents also promote inclusion in the morning runs by placing their child in the snow and having them stay longer if they protest; this is done within a context of warmth, laughter, and community, to help incorporate the child into the practice.[41]		A tool parents use in Indigenous American cultures is to incorporate children into everyday life, including adult activities, to pass on the parents’ knowledge by allowing the child to learn through observation. This practice is known as LOPI, Learning by Observing and Pitching In, where children are integrated into all types of mature daily activities and encouraged to observe and contribute in the community. This inclusion as a parenting tool promotes both community participation and learning.[42]		In some Mayan communities, young girls are not permitted around the hearth, for an extended period of time since corn is sacred. Despite this being an exception to the more common Indigenous American practice of integrating children into all adult activities, including cooking, it is a strong example of observational learning. These Mayan girls can only see their mothers making tortillas in small bits at a time, they will then go and practice the movements their mother used on other objects, such as the example of kneading thin pieces of plastic like a tortilla. From this practice, when a girl comes of age, she is able to sit down and make tortillas without any explicit verbal instruction as a result of her observational learning.[43]		Judaism has an extensive tradition of parenting with an emphasis on education.[44][45][46][47][48][49][50][51][52][53]		Family planning is the decision regarding whether and when to become parents, including planning, preparing, and gathering resources. Parents should assess (among other matters) whether they have the required financial resources (the raising of a child costs around $16,198 yearly in the United States)[54] and should also assess whether their family situation is stable enough and whether they themselves are responsible and qualified enough to raise a child. Reproductive health and preconceptional care affect pregnancy, reproductive success and maternal and child physical and mental health.		During pregnancy the unborn child is affected by many decisions his or her parents make, particularly choices linked to their lifestyle. The health and diet decisions of the mother can have either a positive or negative impact on the child during prenatal parenting. In addition to physical management of the pregnancy, medical knowledge of your physician, hospital, and birthing options are important. Here are some key items of advice:		Many people[who?] believe that parenting begins with birth, but the mother begins raising and nurturing a child well before birth. Scientific evidence indicates that from the fifth month on, the unborn baby is able to hear sounds, become aware of motion, and possibly exhibit short-term memory. Several studies (e.g. Kissilevsky et al., 2003) show evidence that the unborn baby can become familiar with his or her parents' voices. Other research indicates that by the seventh month, external schedule cues influence the unborn baby's sleep habits. Based on this evidence, parenting actually begins well before birth.		How many children the mother carries also determines the amount of care needed during prenatal and post-natal periods.		Newborn parenting, is where the responsibilities of parenthood begins. A newborn's basic needs are food, sleep, comfort and cleaning which the parent provides. An infant's only form of communication is crying, and attentive parents will begin to recognize different types of crying which represent different needs such as hunger, discomfort, boredom, or loneliness. Newborns and young infants require feedings every few hours which is disruptive to adult sleep cycles. They respond enthusiastically to soft stroking, cuddling and caressing. Gentle rocking back and forth often calms a crying infant, as do massages and warm baths. Newborns may comfort themselves by sucking their thumb or a pacifier. The need to suckle is instinctive and allows newborns to feed. Breastfeeding is the recommended method of feeding by all major infant health organizations.[57] If breastfeeding is not possible or desired, bottle feeding is a common alternative. Other alternatives include feeding breastmilk or formula with a cup, spoon, feeding syringe, or nursing supplementer.		The forming of attachments is considered to be the foundation of the infant/child's capacity to form and conduct relationships throughout life. Attachment is not the same as love and/or affection although they often go together. Attachments develop immediately and a lack of attachment or a seriously disrupted capacity for attachment could potentially do serious damage to a child's health and well-being. Physically, one may not see symptoms or indications of a disorder but the child may be emotionally affected. Studies show that children with secure attachment have the ability to form successful relationships, express themselves on an interpersonal basis and have higher self-esteem[citation needed]. Conversely children who have caregivers who are neglectful or emotionally unavailable can exhibit behavioral problems such as post-traumatic stress disorder or oppositional-defiant disorder [58]		Oppositional-defiant disorder is a pattern of disobedient, hostile, and defiant behavior toward authority figures		Toddlers are much more active than infants and are challenged with learning how to do simple tasks by themselves. At this stage, parents are heavily involved in showing the child how to do things rather than just doing things for them, and the child will often mimic the parents. Toddlers need help to build their vocabulary, increase their communication skills, and manage their emotions. Toddlers will also begin to understand social etiquette such as being polite and taking turns.[citation needed]		Toddlers are very curious about the world around them and eager to explore it. They seek greater independence and responsibility and may become frustrated when things do not go the way they want or expect. Tantrums begin at this stage, which is sometimes referred to as the 'Terrible Twos'.[59] Tantrums are often caused by the child's frustration over the particular situation, sometimes simply not being able to communicate properly. Parents of toddlers are expected to help guide and teach the child, establish basic routines (such as washing hands before meals or brushing teeth before bed), and increase the child's responsibilities. It is also normal for toddlers to be frequently frustrated. It is an essential step to their development. They will learn through experience; trial and error. This means that they need to experience being frustrated when something does not work for them, in order to move on to the next stage. When the toddler is frustrated, they will often behave badly with actions like screaming, hitting or biting. Parents need to be careful when reacting to such behaviours, giving threats or punishments is not helpful and will only make the situation worse.[60] Research groups led by Daniel Schechter, Alytia Levendosky, and others have shown that parents with histories of maltreatment and violence exposure often have difficulty helping their toddlers and preschool-age children with these very same emotionally dysregulated behaviours, which can remind traumatized parents of their adverse experiences and associated mental states.[61][62][63]		Regarding gender differences in parenting, data from the US in 2014 states that, on an average day, among adults living in households with children under age 6, women spent 1.0 hour providing physical care (such as bathing or feeding a child) to household children. By contrast, men spent 23 minutes providing physical care.[64]		Younger children are becoming more independent and are beginning to build friendships. They are able to reason and can make their own decisions given hypothetical situations. Young children demand constant attention, but will learn how to deal with boredom and be able to play independently. They also enjoy helping and feeling useful and able. Parents may assist their child by encouraging social interactions and modelling proper social behaviors. A large part of learning in the early years comes from being involved in activities and household duties. Parents who observe their children in play or join with them in child-driven play have the opportunity to glimpse into their children’s world, learn to communicate more effectively with their children and are given another setting to offer gentle, nurturing guidance.[65] Parents are also teaching their children health, hygiene, and eating habits through instruction and by example.		Parents are expected to make decisions about their child's education. Parenting styles in this area diverge greatly at this stage with some parents becoming heavily involved in arranging organized activities and early learning programs. Other parents choose to let the child develop with few organized activities.		Children begin to learn responsibility, and consequences of their actions, with parental assistance. Some parents provide a small allowance that increases with age to help teach children the value of money and how to be responsible with it.		Parents who are consistent and fair with their discipline, who openly communicate and offer explanations to their children, and who do not neglect the needs of their children in some way often find they have fewer problems with their children as they mature.		During adolescence children are beginning to form their identity and are testing and developing the interpersonal and occupational roles that they will assume as adults. Therefore, it is important that parents treat them as young adults. Although adolescents look to peers and adults outside the family for guidance and models for how to behave, parents remain influential in their development. A teenager who thinks poorly of him or herself, is not confident, hangs around with gangs, lacks positive values, follows the crowd, is not doing well in studies, is losing interest in school, has few friends, lacks supervision at home or is not close to key adults like parents and is vulnerable to peer pressure. Parents often feel isolated and alone in parenting adolescents,[66] but they should still make efforts to be aware of their adolescents' activities, and to provide guidance, direction, and consultation. Adolescence can be a time of high risk for children, where new-found freedoms can result in decisions that drastically open up or close off life opportunities. Adolescents tend to increase the amount of time they spend with peers of the opposite gender; however, they still maintain the amount of time they spend with those of the same gender, and they do this by decreasing the amount of time they spend with their parents. Also, peer pressure is not the reason why peers have influence on adolescents; instead, it is often because they respect, admire and like their peers.[67] Parental issues at this stage of parenting include dealing with "rebellious" teenagers, who didn't know freedom while they were smaller. In order to prevent all these, it is important for the parents to build a trusting relationship with their children. This can be achieved by planning and taking part in fun activities together, keeping promises made to them, spending time with them, not reminding them about their past mistakes and listening to and talking to them. When a trusting relationship is built, adolescents are more likely to approach their parents for help when faced with negative peer pressure. Helping the child build a strong foundation will help them to resist negative peer pressure. It is important for the parents to build up the self-esteem of their child: Praise the child's strengths instead of focusing on their weaknesses (It will help to grow the child's sense of self-worth and self-confidence, so he/she does not feel the need to gain acceptance from his/her peers), acknowledge the child's efforts, do not simply focus on the final result (when they notice that the parent recognizes their efforts, they will keep trying), and lastly, disapprove the behavior, not the child, or they will turn to their peers for acceptance and comfort.		Parenting doesn't usually end when a child turns 18. Support can be needed in a child's life well beyond the adolescent years and continues into middle and later adulthood. Parenting can be a lifelong process.		Parents may provide financial support to their adult children, which can also include providing an inheritance after death. The life perspective and wisdom given by a parent can benefit their adult children in their own lives. Becoming a grandparent is another milestone and has many similarities with parenting.		Roles can be reversed in some ways when adult children become caregivers to their elderly parents.		Parents may receive assistance with caring for their children through child care programs.		Data from the British Household Panel Survey and the German Socio-Economic Panel suggests that having up to two children increases happiness in the years around the birth, and mostly so for those who have postponed childbearing. However, having a third child does not increase happiness.[68]		
Residential care refers to long-term care given to adults or children who stay in a residential setting rather than in their own home or family home.		There are various residential care options available, depending on the needs of the individual. People with disabilities, mental health problems, learning difficulties, Alzheimers, dementia or who are frail aged are often cared for at home by paid or voluntary caregivers, such as family and friends, with additional support from home care agencies. However, if home-based care is not available or not appropriate for the individual, residential care may be required.						Children may be removed from abusive or unfit homes by government action, or they may be placed in various types of out-of-home care by parents who are unable to care for them or their special needs. In most jurisdictions the child is removed from the home only as a last resort, for their own safety and well-being or the safety or others, since out-of-home care is regarded as very disruptive to the child. They are moved to a place called a foster home.[1]		A residential school is a school in which children generally stay 24 hours per day, 7 days per week (often called a boarding school). There is divided opinion about whether this type of schooling is beneficial for children. A case for residential special schooling has been advanced in the article: Residential special schooling: the inclusive option! in the Scottish Journal of Residential Child Care, Volume 3(2), 17-32, 2004 by Robin Jackson. Recent trends have favored placement of children in foster care rather than residential settings, partially for financial reasons, but a 1998 survey found that a majority of out-of-home children surveyed preferred residential or group homes over foster care.[2]		This type of out-of-home care is for orphans, or for children whose parents cannot or will not look after them. Orphaned, abandoned or high risk young people may live in small self-contained units established as home environments. Young people in this care are subject to government departmental evaluations that includes progressions within health, education, social presentations, family networks and others. These are referred to as life domains within the charter of Looking after Children (LAC).		Children may be placed or taken into care because they have a mental, developmental, or physical disability, often referred to as "special needs." A team of teachers, therapists, and carergivers look after the children, who may or may not go home to their parents at night or on weekends. Conditions and disabilities such as Autism, Down syndrome, epilepsy and cerebral palsy (to name a few) may require that children receive residential professional care. Specialized residential can be provided for children with conditions such as anorexia, bulimia, schizophrenia, addiction, or children who are practicing self-harm.		Children, including children with special needs, may be cared for in a licensed foster care home. Special training or special facilities may be required to foster a child who is medically fragile - for example, a child who has a serious medical condition or is dependent on medical technology such as oxygen support.[3]		Adults may be placed in Adult Residential Facilities because of a disability, often a mental disability such as Down syndrome or Autism, which makes them unable to care for their daily needs.[4]		Various forms of long-term residential care are available for elderly people. A person or couple who are able to take care of their daily needs may choose to live in a retirement apartment complex ("independent living") where they function autonomously. They may choose to fix their own meals or have meals provided, or some combination of both.		Many residential facilities are designed for elderly people who do not need 24-hour nursing care but are unable to live independently. Such facilities may be described as assisted living facilities, board and care homes, or rest homes. They typically provide a furnished or unfurnished room, together with all meals and housekeeping and laundry service. Depending on the needs of the resident they also provide assistance with daily activities such as personal hygiene, dressing, eating, and walking. They are not considered to be medical facilities, but they do have to meet state standards for care and safety.[5]		Nursing homes, also known as rest homes or skilled nursing facilities, are intended for people who need ongoing medical care as well as help with daily activities. Nursing home populations have been decreasing in the United States, despite the increase in the elderly population, because of the increasing availability of other options such as assisted living.[6]		Continuing care retirement communities provide several types of care - typically independent living, assisted living, and skilled nursing - in one location, with the resident being able to move from one level of to another as their needs dictate.[7] This is often referred to as 'Ageing in Place'.		Hospices provide a form of medical care for people with a terminal illness or condition, for example, cancer. It is generally used when a person is very close to death. Most hospices offer a choice of residential (nursing home) or in-home (supportive) care. A hospice emphasizes a palliative rather than curative approach; the patient is made comfortable, including pain relief as needed, and both patient and family are given emotional, spiritual, and practical support.[8]		People may be detained under the laws that state that they have to be sectioned in certain circumstances. In the United Kingdom, at least 2 doctors can sign a paper to get this to happen. Patients have to be a risk to themselves, property or other people to warrant being sectioned; this can include suicide attempts.		Some patients may volunteer to go to a psychiatric hospital because they recognize that they are ill.		Treatment can occur against the patient's wishes if this is needed and that can be with the use of drugs. The patients are generally detained until doctors believe that they are stable enough to leave.		People who are addicted to drugs or alcohol may be voluntarily or involuntarily admitted to a residential facility for treatment. Prescribed drugs are sometimes used to get people off illegal or addictive drugs, and to prevent the withdrawal symptoms of such drugs. The length of stay may be determined by the patient's needs or by external factors. In many cases the patient's insurance will cover such treatment in private facilities for only a limited period of time, and public rehabilitation facilities often have long waiting lists.[9]		Total care is when a resident or patient requires a caregiver in order to have all their survival needs met, including ambulation, respiration, bathing, dressing, feeding, and toileting.		The term "total care" is sometimes incorrectly used in nursing homes and other similar facilities to refer to a patient who simply needs diaper changes, but is able to provide other care on his/her own.		The term "self care" is used to refer to a resident or patient who resides in a caregiving facility, but is able to meet one's own needs, such as ambulation and toileting, and only requires a caregiver for occasional assistance.		
Elevenses ( /ᵻˈlɛvənzᵻz/) is a short break taken at around 11 a.m. to consume a drink or snack of some sort. The name and details vary between countries.						In Australia and New Zealand elevenses is known as 'morning tea', or smoko and can occur at any point between the start of the working day and lunchtime. Many workplaces organise morning teas for staff to welcome new employees, for special occasions such as a birthday, or simply as a regular event. Food will sometimes be provided by the business, but often employees will be expected to bring food to share.[1][2]		In many Spanish-speaking cultures, elevenses is observed under the name la once (in Spanish, once means 'eleven'). However, in Chile it has shifted to the afternoon.[3]		In the 2010-2011 National Food Consumption Survey, around 80% of the Chileans reported having once. This is due to once sometimes replacing the traditional dinner in Chile, which only 30% of the population reported having. In this process, once is losing its previous form and is now had, on average, around 7pm.[4]		An alternative widespread, but unfounded, popular etymology for the word in Chile is that priests (in other versions, workers or women) used the phrase tomar las once (Spanish: "drink the eleven") in reference to the eleven letters of the word Aguardiente, to conceal the fact that they were drinking during the day.[4]		In Colombia, it is common to have a snack named onces. It consists mainly of coffee or tea with crackers, usually taken around 5 o'clock in the afternoon.		In West Friesland country people had a similar meal called konkelstik (served at konkeltoid, the proper time for konkelen, a verb denoting "making a visit").[5][6]		Elevenses typically consists of tea or coffee, often with a biscuit.[7]		In Euskadi it's common to have a mid-morning snack consisting of high-protein food like eggs, bacon, or cured meats on bread, called hamarretako (literally "10 o'clock snack") or hamaiketako ("11 o'clock snack").		During the first decades of the 19th century, elevenses consisted of drinking whiskey.[8]		In Israel it's called ארוחת עשר (Hebrew for "10 o'clock meal"), mostly eaten at schools and kindergartens in the form of homemade sandwiches, often accompanied with a fruit or other snack, after the second hour of the schoolday and before the so-called "small break". It also occurs in major unionized workplaces, such as factories and customer services reception centres, where workers are handed tea.		Elevenses in Hungarian is called Tíz-órai which translates to "of the 10 o'clock", referring to "the meal of the 10 o'clock". This is a break between breakfast and lunch, when it is time for a light meal or snack. In schools the early lunch break is called a Tíz-órai break.		For elevenses, Winnie-the-Pooh preferred honey on bread with condensed milk. Paddington Bear often took elevenses at the antique shop on Portobello Road run by his friend Mr Gruber,[9] for which Paddington would buy buns and Mr Gruber would make cocoa (hot chocolate).		In the Middle-earth stories by J. R. R. Tolkien (The Lord of the Rings), it is a meal eaten by Hobbits between second breakfast and luncheon.[10]		Elevenses is the name of a brand of clothing sold by Anthropologie.[citation needed]		
The Leningrad première of Shostakovich's Symphony No. 7 took place on 9 August 1942 during the Second World War, while the city (now Saint Petersburg) was under siege by Nazi German forces. Dmitri Shostakovich (pictured) had intended for the piece to be premièred by the Leningrad Philharmonic Orchestra, but they had been evacuated because of the siege, along with the composer, and the world première was instead held in Kuybyshev. The Leningrad première was performed by the surviving musicians of the Leningrad Radio Orchestra, supplemented with military performers. Most of the musicians were starving, and three died during rehearsals. Supported by a Soviet military offensive intended to silence German forces, the performance was a success, prompting an hour-long ovation. The symphony was broadcast to the German lines by loudspeaker as a form of psychological warfare. The Leningrad première was considered by music critics to be one of the most important artistic performances of the war because of its psychological and political effects. Reunion concerts featuring surviving musicians were convened in 1964 and 1992 to commemorate the event. (Full article...)		August 9: International Day of the World's Indigenous Peoples; National Women's Day in South Africa		Hieronymus Bosch (d. 1516) · Elizabeth Schuyler Hamilton (b. 1757) · Gillian Anderson (b. 1968)		Marina City is a mixed-use residential/commercial building complex in Chicago, Illinois. It occupies almost an entire city block on State Street and sits on the north bank of the Chicago River in downtown Chicago, directly across from the Loop. The complex consists of two corncob-shaped, 587-foot (179 m), 65-story towers, as well as a saddle-shaped auditorium building and a mid-rise hotel building. Designed by Bertrand Goldberg, Marina City was the first building in the United States to be constructed with tower cranes.		Photograph: Diego Delso		Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:		This Wikipedia is written in English. Started in 2001 (2001), it currently contains 5,456,399 articles. Many other Wikipedias are available; some of the largest are listed below.		
Laxatives, purgatives, or aperients are substances that loosen stools[1] and increase bowel movements. They are used to treat and/or prevent constipation.		Laxatives vary as to how they work and the side effects they may have. Certain stimulant, lubricant and saline laxatives are used to evacuate the colon for rectal and bowel examinations, and may be supplemented by enemas under certain circumstances. Sufficiently high doses of laxatives may cause diarrhea.		Some laxatives combine more than one active ingredient. Laxatives may be oral or suppository in form.						Bulk-forming laxatives, also known as roughage, are substances, such as fiber in food and hydrophilic agents in over-the-counter drugs, that add bulk and water to stools so that they can pass more easily through the intestines (lower part of the digestive tract).[2]		Properties		Bulk-forming agents absorb water and should be taken with plenty of water.[3] Bulk-forming agents generally have the gentlest of effects among laxatives[1] and can be taken for long-term maintenance of regular bowel movements.		Foods that help with laxation include fiber-rich foods. Dietary fiber includes insoluble fiber and soluble fiber, such as:[4]		Emollient laxatives, also known as stool softeners, are anionic surfactants that enable additional water and fats to be incorporated in the stool, making it easier for them to move through the gastrointestinal tract.		Properties		Emollient agents should be taken with plenty of water. Emollient agents prevent constipation rather than treating long-term constipation.[3]		Lubricant laxatives are substances that coat the stool with slippery lipids and retard colonic absorption of water so that the stool slides through the colon more easily. Lubricant laxatives also increase the weight of stool and decrease intestinal transit time.[3]		Properties		Mineral oil is the only nonprescription lubricant. Mineral oil may decrease the absorption of fat-soluble vitamins and some minerals.[3]		Hyperosmotic laxatives are substances that cause the intestines to hold more water within and create an osmotic effect that stimulates a bowel movement.[3]		Properties		Lactulose works by the osmotic effect, which retains water in the colon, lowering the pH through bacterial fermentation to lactic, formic and acetic acid, and increasing colonic peristalsis. Lactulose is also indicated in portal-systemic encephalopathy. Glycerin suppositories work mostly by hyperosmotic action, but the sodium stearate in the preparation also causes local irritation to the colon.		Solutions of polyethylene glycol and electrolytes (sodium chloride, sodium bicarbonate, potassium chloride, and sometimes sodium sulfate) are used for whole bowel irrigation, a process designed to prepare the bowel for surgery or colonoscopy and to treat certain types of poisoning. Brand names for these solutions include GoLytely, GlycoLax, CoLyte, Miralax, Movicol, NuLytely, Suprep, and Fortrans. Solutions of sorbitol (SoftLax) have similar effects.		Saline laxatives are non-absorbable osmotic substances that attract and retain water in the intestinal lumen, increasing intraluminal pressure that mechanically stimulates evacuation of the bowel. Magnesium-containing agents also cause the release of cholecystokinin, which increases intestinal motility and fluid secretion.[3] Saline laxatives may alter a patient's fluid and electrolyte balance.		Properties		Saline laxatives should be taken with plenty of water.		Stimulant laxatives are substances that act on the intestinal mucosa or nerve plexus, altering water and electrolyte secretion.[9] They also stimulate peristaltic action and can be dangerous under certain circumstances.[10]		Properties		They are the most powerful among laxatives and should be used with care. Prolonged use of stimulant laxatives can create drug dependence by damaging the colon's haustral folds, making a user less able to move feces through the colon on their own. A study of patients with chronic constipation found that 28% of chronic stimulant laxative users lost haustral folds over the course of one year, while none of the control group did.[11]		Castor oil is a glyceride that is hydrolyzed by pancreatic lipase to ricinoleic acid, which produces laxative action by an unknown mechanism.		Properties		Long-term use of castor oil may result in loss of fluid, electrolytes, and nutrients.[3]		These are motility stimulants that work through activation of 5-HT4 receptors of the enteric nervous system in the gastrointestinal tract. However, some have been discontinued or restricted due to potentially harmful cardiovascular side-effects.		Tegaserod (brand name Zelnorm) was removed from the general U.S. and Canadian markets in 2007, due to reports of increased risks of heart attack or stroke. It is still available to physicians for patients in emergency situations that are life-threatening or require hospitalization.[12]		Prucalopride (brand name Resolor) is a current drug approved for use in the EU October 15, 2009[13] and in Canada (brand name Resotran) on December 7, 2011.[14] It has not been approved by the Food and Drug Administration for use in the United States, but it is in development by Shire PLC.[15]		Lubiprostone is used in the management of chronic idiopathic constipation and irritable bowel syndrome. It causes the intestines to produce a chloride-rich fluid secretion that softens the stool, increases motility, and promotes spontaneous bowel movements (SBM).		For adults, a randomized controlled trial found PEG (MiraLax or GlycoLax) 17 grams once per day to be superior to tegaserod at 6 mg twice per day.[18] A randomized controlled trial found greater improvement from two sachets (26 grams) of PEG versus two sachets (20 grams) of lactulose.[19] 17 grams per day of PEG has been effective and safe in a randomized controlled trial for six months.[20] Another randomized controlled trial found no difference between sorbitol and lactulose.[21]		For children, PEG was found to be more effective than lactulose.[22]		Some of the less significant adverse effects of laxative abuse include dehydration, hypotension, tachycardia, postural dizziness and syncope;[23] however, laxative abuse can lead to potentially fatal acid-base and electrolyte imbalances.[23] For example, severe hypokalaemia has been associated with distal renal tubular acidosis from laxative abuse.[23] Metabolic alkalosis is the most common acid-base imbalance observed.[23] Other significant adverse effects include rhabdomyolysis,[23] steatorrhoea,[23] inflammation and ulceration of colonic mucosa,[23] pancreatitis,[23][24] renal failure,[23][25][26] factitious diarrhea[23][27] and other problems.[23]		Although patients with eating disorders such as anorexia nervosa and bulimia nervosa frequently abuse laxatives in an attempt to lose weight, laxatives act to speed up the transit of feces through the large intestine, which occurs subsequent to the absorption of nutrients in the small intestine. Thus, studies of laxative abuse have found that effects on body weight reflect primarily temporary losses of body water rather than energy (calorie) loss.[23][28][29]		Physicians warn against the chronic use of stimulant laxatives due to concern that chronic use could cause the colonic tissues to get worn out over time and not be able to expel feces due to long-term overstimulation.[30] A common finding in patients having used stimulant laxatives is a brown pigment deposited in the intestinal tissue, known as melanosis coli.[citation needed]		Laxatives, then called physicks or purgatives, were used extensively in pre-modern medicine to treat a wide range of conditions for which they are now generally regarded as ineffective in modern evidence-based medicine.[31][citation needed] Likewise, laxatives (often termed colon cleanses) continue to be promoted by practitioners of alternative medicine for a range of conditions, including conditions that are not medically recognized, e.g. mucoid plaque.[32][citation needed]		
Eating utensil etiquette describes the correct etiquette with the use of eating utensils.						In many Asian cultures, it is impolite to point with chopsticks.		When used in conjunction with a knife to cut and consume food in Western social settings, two forms of fork etiquette are common. In the European style, the diner keeps the fork in his or her left hand, while in the American style the fork is shifted between the left and right hands. The American style is most common in the United States,[1] but the European style is considered proper in other countries.[2][3]		Originally, the traditional European method, once the fork was adopted as a utensil, was to transfer the fork to the right hand after cutting food, as it had been considered proper for all utensils to be used with the right hand only. This tradition was brought to America by British colonists and is still in use in the United States. Europe adopted the more rapid style of eating in relatively modern times. [4]		The European style is to hold the fork in the left hand and the knife in the right. Once a bite-sized piece of food has been cut, it is conducted straight to the mouth by the left hand. For other items, such as potatoes, vegetables or rice, the blade of the knife is used to assist or guide placement of the food on the back of the fork.[5] The tines remain pointing down.		The knife and fork are both held with the handle running along the palm and extending out to be held by thumb and forefinger. This style is sometimes called "hidden handle" because the palm conceals the handle.		In the American style, also called the zig-zag method or fork switching, the knife is initially held in the right hand and the fork in the left. Holding food to the plate with the fork tines-down, a single bite-sized piece is cut with the knife. The knife is then set down on the plate, the fork transferred from the left hand to the right hand, and the food is brought to the mouth for consumption. The fork is then transferred back to the left hand and the knife is picked up with the right.[1][6] In contrast to the European hidden handle grip, in the American style the fork is held much like a spoon or pen once it is transferred to the right hand to convey food to the mouth. Though called "American style", this style originated in Europe. [5]		Etiquette experts have noted that the American style of fork-handling is in flux, often being replaced by a hybrid of the traditional American and European styles. In this new style, the fork is not switched between hands between cutting and eating, but may be deployed "tines-up" as a scoop when convenient.[5]		The South East Asian style is similar to the European style, wherein the spoon is held in the right hand throughout consumption (except with certain dishes when a fork is more suitable). The difference is that a spoon is often used in the right hand and knives are rarely used. Rice and soups are a staple of the diet in South East Asian countries, so using a spoon would be practical in such dishes. The spoon is the main utensil in bringing food into the mouth, in tandem with using a fork. The spoon could also be used for manipulating food in the plate and as an alternative for a knife. Often dishes require slicing before serving or sliced into small portions before cooking to relinquish the use of a knife.		Tables are often set with two or more forks, meant to be used for different courses; for example, a salad fork, a meat fork, and a dessert fork. Some institutions wishing to give an impression of high formality set places with many different forks for meals of several courses, although many etiquette authorities regard this as vulgar and prefer that the appropriate cutlery be brought in with each course.[7]		It should not be necessary for the diner to distinguish between types of forks; forks are used in order from outside to inside, with the exception of oyster forks, which are placed on the right side, the tines nested in the bowl of a spoon.[citation needed]		Setting the knife and fork in a crossed position with the fork turned upward on the plate is used to indicate to the server or host that the diner has not yet finished with the meal, while placing them together, or crossed with the fork turned downwards with the handles at the 5 o'clock position is used to indicate that the diner has finished.[8]		
Competitive eating, or speed eating, is an activity in which participants compete against each other to consume large quantities of food in a short time period. Contests are typically eight to ten minutes long, although some competitions can last up to thirty minutes, with the person consuming the most food being declared the winner. Competitive eating is most popular in the United States, Canada, and Japan, where organized professional eating contests often offer prizes, including cash.						Traditionally, eating contests, often involving pies, were events at county fairs. The recent surge in the popularity of competitive eating is due in large part to the development of the Nathan's Hot Dog Eating Contest, an annual holiday tradition that has been held on July 4 virtually every year since the 1970s at Coney Island.[1] In 2001, Takeru Kobayashi transformed the competition and the world of competitive eating by downing 50 hot dogs – smashing the previous record (25.5). The event generates enormous media attention and has been aired on ESPN for the past eight years, contributing to the growth of the competitive eating phenomenon. Takeru Kobayashi won it consistently from 2001 through 2006. He was dethroned in 2007 by Joey Chestnut. In 2008, Chestnut and Kobayashi tied at 59 hot dogs in 10 minutes (the time span had previously been 12 minutes), and Chestnut won in an eat off in which he was the first of the two competitors to finish eating 5 hot dogs in overtime, earning Chestnut his second consecutive title. Chestnut still holds the world record of 69 hot dogs and buns in 10 minutes for the 2013 competition. Kobayashi holds six Guinness Records, for eating hot dogs, meatballs, Twinkies, hamburgers, and pizza. He competed in hot dog contests in 2011 and 2012 and claimed to have eaten 68 and 69. Kobayashi goes on to say that he completed this by drinking lots and lots of liquid for a few days before the event before not eating anything all day on the event. The current champion is Joey Chestnut, with a total of 72 hot dogs and buns eaten on July 4, 2017.[2]		All Pro Eating is the only Independent Competitive Eating organization in the world and also officially sanctions competitive eating contests.		All Pro Eating differs from the IFOCE with its adherence to "picnic style" competitive eating rules in addition to being the most recognized competitive eating organization that allows independent competitive eaters to participate (independent competitive eaters are not under any contractual obligation). Picnic style rules pay "respect to the food and maintains the integrity and dignity and public reputation of that food item."[3] Under these rules, the league forbids the dunking of any contest foods in water, a practice used by almost every IFOCE eater at IFOCE events, and one believed to speed the chewing and swallowing process. All Pro Eating Promotions is the only competitive eating organization that provides sanctioned independent competitive eating events that specifically follow picnic style rules.		Recognized All Pro Eating Competitive Eaters include Molly Schuyler, Eric "Silo" Dahl, Jamie "The Bear" McDonald and Stephanie "Xanadu" Torres.[4]		All Pro Eating has been significantly featured in American and International media including a strong connection with television projects produced by TV Tokyo.		In 2010, All Pro Eating was joined by new eaters including former IFOCE members "Jammin" Joe LaRue, Chris "The Mad Greek" Abatsas and newcomer "Munchin" Mike Longo.		The year started off with the USquare National Eating Championship in Madison Wisconsin. Ian The Invader Hickman won with 102 ounces of food court food in 8 minutes.		On June 5 in Rockville Centre, NY "Munchin" Mike Longo won the National Cheese Steak Eating Championship with 4.05 cheese steaks eating in 7 minutes.		"Munchin" Mike Longo also won the Little Jimmy's National Italian Ice Eating Championship again beating AICE eating star Ian The Invader Hickman and legendary eater King George Van Laar, eating 4 lbs 1.25 ounces of Italian ice in 7 minutes.		In Middlebury. Connecticut The Caribbean Food Eating Championship took place where Ben Clymer won the National Bun and Cheese National Championship eating 40 ounces in 4 min and 43 sec. King George Van Laar won the National Beef Pattie Eating Championship eating 60oz of beef patties in 7 minutes 32 sec.		5th Annual American Meatball Eating Championship took place in Midlothian, Illinois. Bob “KILLER” Kuhns of Pittsburgh, PA took 1st with 36.5 meatballs in 7 minutes to win $1,000.		The International Federation of Competitive Eating (IFOCE) hosts nearly 50 "Major League Eating" events[5] across North America every year. The IFOCE, which first established competitive eating in the 1990s, developing the Nathan's Famous contest and eating circuit, recently launched Major League Eating to serve as an umbrella for competitive eating while also providing a recognized brand for licensing of T-shirts and other products. It features videos of contests and eaters and offers official records. Among the top eaters who compete only in Major League Eating events are Joey Chestnut, Sonya Thomas, Eater X, Notorious B.O.B., Badlands Booker, Matt Stonie, Miki Sudo and Crazy Legs Conti. MLE developed and conducts nearly all of the major eating events, including the Acme Oyster-Eating Contest, the National Buffalo Wing Contest, The Hooters World Wing-Eating Championship, among many others.		IFOCE has produced a three-hour elimination tournament on ESPN called the Alka-Seltzer U.S. Open of Competitive Eating, plus additional hours of ESPN programming on eating for Johnsonville Brats and Krystal hamburgers. The IFOCE also produced a series of 30-minute television shows, "Eats of Strength," for high-definition network InHD. Spike TV also ran a series of one-hour Major League Eating events, featuring the top eaters of the IFOCE, and the IFOCE produced a one-hour docudrama on the Acme World Oyster Eating Contest in New Orleans.		Other eating contests sponsored by restaurants can involve a challenge to eat large or extraordinarily spicy food items, including giant steaks, hamburgers and curries in a set amount of time. Those who finish the item are often rewarded by not having to pay for the item, or with a T-shirt and the addition of their name and/or photo on a wall of challenge victors. Ward's House of Prime located in Milwaukee, Wis has a prime rib meat challenge. The current record is 360 ounces by Molly Schuyler in June of 2017. Various challenges of this type are featured in the Travel Channel show Man v. Food.		The Great American Eat Off - a show that pins two average eaters against each other to see who can eat the fastest, while raising awareness for charity and then brings in a professional competitive eater to beat the winning time, raises the stakes for competitive eaters by incorporating various challenges and obstacles that would interfere with their speed. Obstacles may include eating with two spoons, eating with no hands, or Interval Eating where the competitive eat is permitted to eat for a limited time and then must rest for a specific time (i.e., eat 20 seconds, rest 40 seconds, eat 20 seconds, rest 40 seconds etc.) until they have completed the designated food. Interval Eating was created by Gail Kasper.		The type of food used in contests varies greatly, with each contest typically only using one type of food (e.g. a hot dog eating contest). Foods used in professional eating contests include hamburgers, hot dogs,[6] pies, pancakes, chicken wings, asparagus, pizza, ribs, whole turkeys, among many other types of food. There is also a vegan hot dog eating competition held in Austin, Texas.[7]		Competitive eating contests often adhere to an 8, 10, 12 or 15 minute time limit. Most contests are presided over by a master of ceremonies, whose job is to announce the competitors prior to the contest and keep the audience engaged throughout the contest with enthusiastic play-by-play commentary and amusing anecdotes. A countdown from 10 usually takes place at the end of the contest, with all eating coming to an end with the expiration of time.		Many professional contests also employ a series of judges, whose role is to enforce the contest rules and warn eaters about infractions. Judges will also be called upon to count or weigh each competitor's food and certify the results of the contest prior to the winner being announced.		Many eaters will attempt to put as much food in their mouths as possible during the final seconds of a contest, a practice known by professionals as "chipmunking."[8] If chipmunking is allowed in a contest, eaters are given a reasonable amount of time (typically less than two minutes) to swallow the food or risk a deduction from their final totals.		In many contests, except those adhering to "picnic style rules" mentioned previously, eaters are allowed to dunk foods in water or other liquids in order to soften the food and make it easier to chew and swallow. Dunking typically takes place with foods involving a bun or other doughy parts. Professional contests often enforce a limit on the number of times competitors are allowed to dunk food.		Competitors are expected to maintain a relatively clean eating surface throughout the contest. Excess debris after the contest may result in a deduction from the eater's final totals.		If, at any point during or immediately after the contest, a competitor regurgitates any food, he or she will be disqualified. Vomiting, also known as a "reversal", or, as ESPN and the Nathan's Hot Dog Eating Contest call it, a "reversal of fortune", includes obvious signs of vomiting as well as any small amounts of food that may fall from the mouth deemed by judges to have come from the stomach. Small amounts of food already in the mouth prior to swallowing are excluded from this rule.		Many professional competitive eaters undergo rigorous personal training in order to increase their stomach capacity and eating speed with various foods. Stomach elasticity is usually considered the key to eating success, and competitors commonly train by drinking large amounts of water over a short time to stretch out the stomach. Others combine the consumption of water with large quantities of low calorie foods such as vegetables or salads. Some eaters chew large amounts of gum in order to build jaw strength.[9]		For a marquee event like the Nathan's Hot Dog Eating Contest, some eaters, like former contest champion Joey Chestnut, will begin training several months before the event with personal time trials using the contest food.[10] Retired competitive eater Ed "Cookie" Jarvis trained by consuming entire heads of boiled cabbage followed by drinking up to two gallons of water every day for two weeks before a contest.[11] Due to the risks involved with training alone or without emergency medical supervision, the IFOCE actively discourages training of any sort.[12]		The chief criticism of competitive eating is the message the gluttonous sport sends as obesity levels rise among Americans[14] and the example it sets for youth.[15] However, many competitive eating champions (e.g. Sonya Thomas, Joey Chestnut, etc.) are notably not overweight, and are able to consume far more food than average individuals twice their size.		Others, like actor Ryan Reynolds in an editorial on The Huffington Post, contend that competitive eating is yet another example of Western gluttony at a time when so many others around the world are starving.[16] In the same article, retired competitive eater Don "Moses" Lerman foreshadows the dangers of competitive eating when he admits "I'll stretch my stomach until it causes internal bleeding."		The argument that competitive eating can cause weight gain,[17] which may lead to obesity and elevated cholesterol and blood pressure, is common. The potential damage that competitive eating can cause to the human digestive system was the subject of a 2007 study by the University of Pennsylvania School of Medicine. The study observed professional eater Tim Janus, who ate 36 hot dogs in 10 minutes before doctors intervened. It was concluded that through training, Janus' stomach failed to have normal muscle contractions called peristalsis, a function which transfers food from the stomach down the digestive tract.[18]		Other medical professionals contend that binge eating can cause stomach perforations in those with ulcers and gulping large quantities of water during training can lead to water intoxication, a condition which dilutes electrolytes in the blood.[19] Gastroparesis, also known as stomach paralysis, is also a concern among those who routinely stretch their stomachs beyond capacity. The condition may lead to the stomach's inability to contract and lose its ability to empty itself. Side effects of gastroparesis include chronic indigestion, nausea and vomiting.[20]		In October 2012, a 32-year-old man died while competitively eating live roaches and worms in a contest to win a ball python. An autopsy revealed he choked to death.[21] On July 4, 2014, a 47-year-old competitive eater similarly choked to death during a hot dog eating contest.[22] At a Sacred Heart University event on April 2, 2017, a 20-year-old female student died as a result of a pancake-eating contest.[23]		
In a restaurant, there is a menu of food and beverage offerings. A menu may be à la carte – which guests use to choose from a list of options – or table d'hôte, in which case a pre-established sequence of courses is served.						Menus, as a list of prepared foods, have been discovered dating back to the Song Dynasty in China.[1] In the larger populated cities of the time, merchants found a way to cater to busy customers who had little time or energy to prepare food during the evening. The variation in Chinese cuisine from different regions led caterers to create a list or menu for their patrons.		The word "menu", like much of the terminology of cuisine, is French in origin. It ultimately derives from Latin "minutus", something made small; in French it came to be applied to a detailed list or résumé of any kind. The original menus that offered consumers choices were prepared on a small chalkboard, in French a carte; so foods chosen from a bill of fare are described as "à la carte", "according to the board."		The menu first appeared in China during the second half of the eighteenth century, or The Romantic Age. Prior to this time eating establishments or table d'hôte served dishes that were chosen by the chef or proprietors. Customers ate what the house was serving that day, as in contemporary banquets or buffets and meals were served from a common table. The establishment of restaurants and restaurant menus allowed customers to choose from a list of unseen dishes, which were produced to order according to the customer's selection. A table d'hôte establishment charged its customers a fixed price; the menu allowed customers to spend as much or as little money as they chose.[2]		As early as the mid-20th century, some restaurants have relied on “menu specialists” to design and print their menus. Prior to the emergence of digital printing, these niche printing companies printed full-color menus on offset presses. The economics of full-color offset made it impractical to print short press runs. The solution was to print a “menu shell” with everything but the prices. The prices would later be printed on a less costly black-only press. In a typical order, the printer might produce 600 menu shells, then finish and laminate 150 menus with prices. When the restaurant needed to reorder, the printer would add prices and laminate some of the remaining shells.		With the advent of digital presses, it became practical in the 1990s to print full-color menus affordably in short press runs, sometimes as few as 25 menus. Because of limits on sheet size, larger laminated menus were impractical for single-location independent re to produce press runs of as few as 300 menus, but some restaurants may want to place far fewer menus into service. Some menu printers continue to use shells. The disadvantage for the restaurant is that it is unable to update anything but prices without creating a new shell.		During the economic crisis in the 1970s, many restaurants found that they were having to incur costs from having to reprint the menu as inflation caused prices to increase. Economists noted this transaction cost, and it has become part of economic theory, under the term "menu costs." As a general economic phenomenon, "menu costs" can be experienced by a range of businesses beyond restaurants; for example, during a period of inflation, any company that prints catalogues or product price lists will have to reprint these items with new price figures.		To avoid having to reprint the menus throughout the year as prices changed, some restaurants began to display their menus on chalkboards, with the menu items and prices written in chalk. This way, the restaurant could easily modify the prices without going to the expense of reprinting the paper menus. A similar tactic continued to be used in the 2000s with certain items that are sensitive to changing supply, fuel costs, and so on: the use of the term "market price" or "Please ask server" instead of stating the price. This allows restaurants to modify the price of lobster, fresh fish and other foods subject to rapid changes in cost.		The latest trend in menus is the advent of handheld tablets that hold the menu and the guests can browse through that and look at the photographs of the dishes.		The main categories within a typical menu in the US are "appetizers," "side orders and a la carte," "entrées," "desserts" and "beverages." Sides and a la carte may include such items as soups, salads and dips. There may be special age-restricted sections for "seniors" or for children, presenting smaller portions at lower prices. Any of these sections may be pulled out as a separate menu, such as desserts and/or beverages, or a wine list. Children's menus may also be presented as placemats with games and puzzles to help keep children entertained.		Menus can provide other useful information to diners. Some menus describe the chef's or proprietor's food philosophy, the chef's resume, or the mission statement of the restaurant. Menus often present a restaurant's policies about ID checks for alcohol, lost items, or gratuities for larger parties. In the United States, county health departments frequently require restaurants to include health warnings about raw or undercooked meat, poultry, eggs and seafood.		As a form of advertising, the prose found on printed menus is famous for the degree of its puffery. Menus frequently emphasize the processes used to prepare foods, call attention to exotic ingredients, and add French or other foreign language expressions to make the dishes appear sophisticated and exotic. Higher-end menus often add adjectives to dishes such as "glazed," "sautéed," "poached," and so on. "Menu language, with its hyphens, quotation marks, and random outbursts of foreign words, serves less to describe food than to manage your expectations"; restaurants are often "plopping in foreign words (80 percent of them French) like "spring mushroom civet," "plin of rabbit," "orange-jaggery gastrique." [3]		Brian McGrory quips that, when going to a high-end restaurant, he sometimes feels that he needs "an unabridged dictionary, a Biology 101 textbook, and a pile of Fun With Phonics just to figure out the meaning of gianduja ice cream, hazelnut financiers, yellow watermelon, and bulgur crackers[--] just some of the inscrutable listings from the dessert menu..."[4] Terry Pratchett satirizes this in his novel Hogfather, after a fancy restaurant has its stock of expensive foods replaced with mud and old boots. The resulting menu features such items as Panier de la Pate de Chaussures (Mud mousse in a basket of shoe pastry), Cafe de Terre, and Spaghetti Carbonara (boiled boot laces).		Part of the function of menu prose is to impress customers with the notion that the dishes served at the restaurant require such skill, equipment, and exotic ingredients that the diners could not prepare similar foods at home.[3] In some cases, ordinary foods are made to sound more exciting by replacing everyday terms with their French equivalent. For example, instead of stating that a pork chop has a dollop of applesauce, a high-end restaurant menu might state "Tenderloin of pork avec compôte de pommes." Although "avec compôte de pommes" translates directly as "with applesauce," it sounds more exotic—and more worthy of an inflated price tag. Menus may use the culinary terms concassé to describe coarsely chopped vegetables, coulis to describe a puree of vegetables or fruit, or au jus, to describe meat served with its own natural gravy of pan drippings.		Menus vary in length and detail depending on the type of restaurant. The simplest hand-held menus are printed on a single sheet of paper, though menus with multiple pages or "views" are common. In some cafeteria-style restaurants and chain restaurants, a single-page menu may double as a disposable placemat. To protect a menu from spills and wear, it may be protected by heat-sealed vinyl page protectors, laminating or menu covers. Restaurants weigh their positioning in the marketplace (e.g. fine dining, fast food, informal) in deciding which style of menu to use.		While some restaurants may use a single menu as the sole way of communicating information about menu items to customers, in other cases, the meal menu is supplemented with ancillary menus, such as:		Some restaurants use only text in their menus. In other cases, restaurants include illustrations and photos, either of the dishes or of an element of the culture which is associated with the restaurant. An example of the latter is in cases where a Lebanese kebab restaurant decorates its menu with photos of Lebanese mountains and beaches. Particularly with the ancillary menu types, the menu may be provided in alternative formats, because these menus (other than wine lists) tend to be much shorter than food menus. For example, an appetizer menu or a dessert menu may be displayed on a folded paper table tent, a hard plastic table stand, a flipchart style wooden "table stand," or even, in the case of a pizza restaurant with a limited wine selection, a wine list glued to an empty bottle.		Take-out restaurants often leave paper menus in the lobbies and doorsteps of nearby homes as advertisement. The first to do so may have been New York City's Empire Szechuan chain, founded in 1976.[5] The chain and other restaurants' aggressive menu distribution in the Upper West Side of Manhattan caused the "Menu Wars" of the 1990s, including invasions of Empire Szechuan by the "Menu Vigilantes", the revoking of its cafe license, several lawsuits, and physical attacks on menu distributors.[6][5][7][8]		Some restaurants – typically fast-food restaurants and cafeteria-style establishments – provide their menu in a large poster or display board format up high on the wall or above the service counter. This way, all of the patrons can see all of the choices, and the restaurant does not have to provide printed menus. This large format menu may also be set up outside (see the next section). The simplest large format menu boards have the menu printed or painted on a large flat board. More expensive large format menu boards include boards that have a metal housing, a translucent surface, and a backlight (which facilitates the reading of the menu in low light), and boards that have removable numbers for the prices. This enables the restaurant to change prices without having to have the board reprinted or repainted.		Some restaurants such as cafes and small eateries use a large chalkboard to display the entire menu. The advantage of using a chalkboard is that the menu items and prices can be changed; the downside is that the chalk may be hard to read in lower light or glare, and the restaurant has to have a staff member who has attractive, clear handwriting.		A high-tech successor to the chalkboard menu is the 'write-on wipe-off" illuminated sign, using LED technology. The text appears in a vibrant color against a black background.		Some restaurants provide a copy of their menu outside the restaurant. Fast-food restaurants that have a drive-through or walk-up window will often put the entire menu on a board, lit-up sign, or poster outside, so that patrons can select their meal choices. High-end restaurants may also provide a copy of their menu outside the restaurant, with the pages of the menu placed in a lit-up glass display case; this way, prospective patrons can see if the menu choice is to their liking. As well, some mid-level and high-end restaurants may provide a partial indication of their menu listings–the "specials"–on a chalkboard displayed outside the restaurant. The chalkboard will typically provide a list of seasonal items or dishes that are the specialty of the chef which are only available for a few days.		With the invention of LCD and Plasma displays, some menus have moved from a static printed model, to one which can change dynamically. By using a flat LCD screen and a computer server, menus can be digitally displayed allowing moving images, animated effects and the ability to edit details and prices.		For fast food restaurants, a benefit is the ability to update prices and menu items as frequently as needed, across an entire chain. Digital menu boards also allow restaurant owners to control the day parting of their menus, converting from a breakfast menu in the late morning. Some platforms support the ability allow local operators to control their own pricing while the design aesthetic is controlled by the corporate entity. Various software tools and hardware developments have been created for the specific purpose of managing a digital menu board system. Digital menu screens can also alternate between displaying the full menu and showing video commercials to promote specific dishes or menu items.		Websites featuring online restaurant menus have been on the Internet for nearly a decade. In recent years, however, more and more restaurants outside of large metropolitan areas have been able to feature their menus online as a result of this trend.		Several restaurant-owned and startup online food ordering websites already included menus on their websites, yet due to the limitations of which restaurants could handle online orders, many restaurants were left invisible to the Internet aside from an address listing. Multiple companies came up with the idea of posting menus online simultaneously, and it is difficult to ascertain who was first. Menus and online food ordering have been available online since at least 1997. Since 1997, hundreds of online restaurant menu web sites have appeared on the Internet. Some sites are city-specific, some list by region, state or province.		Another phenomenon is the so-called "secret menu" where some fast food restaurants are known for having unofficial and unadvertised selections that customers learn by word of mouth.[9] Fast food restaurants will often prepare variations on items already available, but to have them all on the menu would create clutter. Chipotle Mexican Grill is well known for having a simple five item menu, but some might not know they offer quesadillas and single tacos, despite neither being on the menu board.		In-N-Out Burger has a very simple menu of burgers, fries, sodas, and shakes, but has a wide variety of "secret" styles of preparations, the most famous being "Animal Style" burgers and fries.[10] This can also occur in high-end restaurants, which may be willing to prepare certain items which are not listed on the menu (e.g., dishes that have long been favorites of regular clientele). Sometimes restaurants may name foods often ordered by regular clientele after them, for either convenience or prestige.		
A savoury is the final course of a traditional British formal meal, following the sweet pudding or dessert course. The savoury is designed to "clear the palate" before the Port is served. It generally consists of salty and plain elements.		Typical savouries are:		
Coffee culture describes a social atmosphere or series of associated social behaviors that depends heavily upon coffee, particularly as a social lubricant. The term also refers to the diffusion and adoption of coffee as a widely consumed stimulant by a culture. In the late 20th century, particularly in the Western world and urbanized centers around the globe, espresso has been an increasingly dominant form.		The formation of culture around coffee and coffeehouses dates back to 14th century Turkey. Coffeehouses in Western Europe and the Eastern Mediterranean were traditionally social hubs, as well as artistic and intellectual centers. For example, Les Deux Magots in Paris, now a popular tourist attraction, was once associated with the intellectuals Jean-Paul Sartre and Simone de Beauvoir. In the late 17th and 18th centuries, coffeehouses in London became popular meeting places for artists, writers and socialites, and were also the center for much political and commercial activity. Elements of today's coffeehouses (slower paced gourmet service, tastefully decorated environments, or social outlets such as open mic nights) have their origins in early coffeehouses, and continue to form part of the concept of coffee culture.		In the United States in particular, the term is frequently used to designate the ubiquitous presence of hundreds of espresso stands and coffee shops in the Seattle metropolitan area and the spread of franchises of businesses such as Starbucks and their clones across the United States. Other aspects of coffee culture include the presence of free wireless Internet access for customers, many of whom do business in these locations for hours on a regular basis. The style of coffee culture varies by country, with an example being the strength of existing cafe style coffee culture in Australia used to explain the poor performance of Starbucks there.[1]		In many urban centers in the world, it is not unusual to see several espresso shops and stands within walking distance of each other or on opposite corners of the same intersection, typically with customers overflowing into parking lots. Thus, the term coffee culture is also used frequently in popular and business media to describe the deep impact of the market penetration of coffee-serving establishments.						Coffee culture frequently shows up in comics, television, and movies in a variety of ways. TV shows such as NCIS show characters constantly with espresso in hand or people distributing take-out cups to other characters. The comic strips Adam and Pearls Before Swine frequently center the strip around visiting or working at coffee shops.		Daily Mail writer Philip Nolan stated that the spread of the coffee culture in Ireland is largely accredited to American television shows Friends and Frasier, saying, "We saw it reflected in the lifestyles of our TV favorites the Friends gang in Central Perk drinking coffee instead of alcohol; Frasier and Niles having latte and biscotti in the [Café] Nervosa; every cop on TV being called out on a 911 just as he ambled back to his car with Dunkin' Donuts and a cup of strong, black coffee."[2]		A "coffeehouse or "café" is an establishment which primarily serves prepared coffee or other hot drinks. Historically cafés have been an important social gathering point in Europe. They were—and continue to be—venues where people gather to talk, write, read, entertain one another, or pass the time. During the 16th-century coffeehouses were banned in Mecca because they attracted political gatherings.		In addition to coffee, many cafés also serve tea, sandwiches, pastries, and other light refreshments. Some provide other services, such as wired or wireless internet access (thus the name, "internet café" — which has carried over to stores that provide internet service without any coffee) for their customers.		Many social aspects of coffee can be seen in the modern-day lifestyle. By absolute volume, the United States is the largest market for coffee, followed by Germany and Japan. Canada, Australia, Sweden and New Zealand are the other large coffee consuming countries. Tim Hortons is Canada's largest coffee chain, making millions of cups of coffee a day.[3] The Nordic countries consume the most coffee per capita, with Finland typically occupying the top spot with a per-capita consumption with 12 kg per year, followed by Norway, Iceland and Denmark.[4][5] Consumption has also vastly increased in recent years in the traditionally tea-drinking United Kingdom, but as of 2005 it was still below 5 kg per year. Turkish coffee is popular in Turkey, the Eastern Mediterranean, and southeastern Europe. Coffeehouse culture has a high penetration in much of the former Ottoman Empire, where Turkish coffee remains the dominant style of preparation.		Coffee has also been important in Austrian and in French culture since the late 19th and early 20th centuries. Vienna's coffeehouses are prominent in Viennese culture and known internationally, while Paris was important in the development of "café society" in the first half of the 20th century.		In some countries, notably in Northern Europe, coffee parties are a popular form of entertainment. Besides coffee, the host or hostess at the coffee party also serves cake and pastries, sometimes homemade. In Germany, Netherlands, Austria and the Nordic countries, strong black coffee is also regularly drunk along with or immediately after main meals such as lunch and dinner, and several times at work or school. In the café culture of these countries, especially Germany and Sweden, free refills of black coffee are often provided at restaurants and cafés, especially if customers have also bought a sweet treat or pastry with the coffee.		Coffee plays a large role in much of history and literature because of the large effects the coffee industry has had on cultures where it is produced or consumed. Coffee is often mentioned as one of the main economic goods used in imperial control of trade, and with colonized trade patterns in "goods" such as slaves, coffee, and sugar, which defined Brazilian trade, for example, for centuries. Coffee in culture or trade is a central theme and prominently referenced in much poetry, fiction, and regional history.		A coffee break (or "fika", as it's commonly referred to in Sweden) is a routine social gathering for a snack and short downtime practiced by employees in business and industry. The coffee break allegedly originated in the late 19th century in Stoughton, Wisconsin, with the wives of Norwegian immigrants. The city celebrates this every year with the Stoughton Coffee Break Festival.[6] In 1951, Time noted that "[s]ince the war, the coffee break has been written into union contracts".[7] The term subsequently became popular through a Pan-American Coffee Bureau ad campaign of 1952 which urged consumers, "Give yourself a Coffee-Break — and Get What Coffee Gives to You."[8] John B. Watson, a behavioral psychologist who worked with Maxwell House later in his career, helped to popularize coffee breaks within the American culture.[9]		Coffee breaks usually last from 10 to 20 minutes and frequently occur at the end of the first third of the work shift. In some companies and some civil service, the coffee break may be observed formally at a set hour. In some places, a "cart" with hot and cold beverages and cakes, breads and pastries arrives at the same time morning and afternoon, an employer may contract with an outside caterer for daily service, or coffee breaks may take place away from the actual work-area in a designated cafeteria or tea room.		
Nyotaimori (女体盛り, "serve (foods) on the female body"), often referred to as "body sushi", is the Japanese practice of serving sashimi or sushi from the naked body of a woman.[1] Nantaimori (男体盛り) refers to the same practice using a male model.[citation needed]		The Japanese practice of nyotaimori – serving sushi on a naked body – is said to have its origins in the samurai period in Japan.[2] In the words of chef Mike Keenan, "The naked sushi idea began during the samurai period in Japan. It was a subculture to the geishas. It would take place in a geisha house as a celebration after a victorious battle."[3]		Nyotaimori originated in Ishikawa Prefecture[4][5][6] and continues to be practiced there.[7]						"Before becoming a living sushi platter, the person (usually a woman) is trained to lie down for hours without moving. She or he must also be able to withstand the prolonged exposure to the cold food. Before service, the individual is supposed to have taken a bath using a special fragrance-free soap and then finished off with a splash of cold water to chill the body down somewhat for the sushi. In some parts of the world, in order to comply with sanitation laws, there must be a layer of plastic or other material between the sushi and the body of the woman or man."		In traditional nyotaimori, the model is generally expected to lie still at all times and not talk with guests. The sushi is placed on sanitized leaves on the model's body to prevent skin-to-fish contact and on sufficiently flat areas of the body off which the sushi will not roll. Nyotaimori is considered an art form.[2][8]		Usually champagne and sake are served in naked sushi restaurants. Guests must be respectful and observe the strictest decorum. Talking with the models is highly discouraged. Inappropriate gestures or comments are not tolerated and diners can only pick up sushi with chopsticks, although rules in some restaurants are less strict. For example, in some restaurants guests can nibble nori rolls off nipples if they choose.[2][9][10]		The practice has been described as decadent,[11] humiliating,[12] cruel,[12] and objectifying.[12] Guardian columnist Julie Bindel notes that the woman being used to serve the food, on at least one occasion in London, looked "as if in a morgue, awaiting a postmortem".[1] It has received popularity in Japanese organized crime.[12]		Worldwide reception varies as several countries have banned the practice.[11] In 2005, China has outlawed nyotaimori due to public health reasons and imposed moral censorship issues.[13]		The birthday party of South African entrepreneur Kenny Kunene on 21 October 2010, which hosted ANCYL president Julius Malema and featured nyotaimori,[14] was criticised by COSATU secretary general Zwelinzima Vavi, leading to a political row.[15][16] The ANCWL condemned nyotaimori at Kunene's party as an attack on the bodily integrity and dignity of women in South Africa.[17]		
A food festival is a festival, usually held annually, that uses food, often produce, as its central theme. These festivals have always been a means of uniting communities through celebrations of harvests and giving thanks for a plentiful growing season. They can be traced back thousands of years to celebrating the arrival of harvest time, the autumnal equinox, and the honoring of earth gods. Some food festivals are also harvest festivals. Note this list includes festivals focused on beverages, such as wine festivals and beer festivals, as well.						See List of food festivals in Canada.		There are several Florida food festivals and New Jersey food festivals. Other festivals include the Gilroy Garlic Festival in Gilroy, California; Brentwood Cornfest in Brentwood, California; Mushroom Festivals in various locales; the Castroville Artichoke Festival, in Castroville, California; the Stockton Asparagus Festival, in Stockton, California; the ¡Latin Food Festival! in San Diego, California; the Lexington Barbecue Festival in North Carolina; the Posen Potato Festival, in Posen, Michigan; the Norwalk Oyster Festival, in Norwalk, Connecticut, Vaisakhi Festival at Yuba City, and the Howell Melon Festival in Howell, Michigan, known for electing the Howell Melon Queen.		Vegetarian food festivals include Vegfest in Salt Lake City, Utah, Seattle, Washington and San Francisco, California.		
Anju (안주; 按酒) is a general term for a Korean food consumed with alcohol. It consists of a variety of foods, including both main dishes and side dishes. Consuming food with alcohol is a widespread practice in Korea, especially when the alcoholic beverage soju is involved.[1][2]		Certain types of foods consumed primarily as Anju include golbaengi muchim, nogari with peanuts, and jokbal.						Until the Chosun Dynasty, alcohol was mainly served in jumaks (a type of inn or tavern), where soups with rice, along with traditional alcohol such as makgeolli, were served to guests. Since the introduction of beer and Western foods into Korea, mainly from Japan in the nineteenth century, bars and pubs have enjoyed a newfound popularity, and many types of Western foods have been consumed as anju. nowadays, anju is good market source of grocers.[3]		Some foods are considered to be best complemented by certain types of alcohol. For example, samgyeopsal, grilled pork belly, is considered to go best with soju, while fried chicken or Korean seasoned chicken goes well with beer. Pa-jun and makkeoli (or dongdongju) is a popular combination for rainy days.		There are a number of different types of bars in South Korea, and each category sells different kinds of food and alcoholic beverages.		Nogari (young Alaska pollock) with peanuts		Nakji bokkeum with somyeon (fried octopus in gochujang with fine noodles)		Dubu kimchi		
A dining car (American English) or a restaurant car (British English), also a diner, is a railroad passenger car that serves meals in the manner of a full-service, sit-down restaurant.		It is distinct from other railroad food service cars that do not duplicate the full-service restaurant experience, such as cars in which one purchases food from a walk-up counter to be consumed either within the car or elsewhere in the train. Grill cars, in which customers sit on stools at a counter and purchase and consume food cooked on a grill behind the counter are generally considered to be an "intermediate" type of dining car.						Before dining cars in passenger trains were common in the United States, a rail passenger's option for meal service in transit was to patronize one of the roadhouses often located near the railroad's "water stops". Fare typically consisted of rancid meat, cold beans, and old coffee. Such poor conditions discouraged many from making the journey.		Most railroads began offering meal service on trains even before the First Transcontinental Railroad. By the mid-1880s, dedicated dining cars were a normal part of long-distance trains from Chicago to points west, save those of the Santa Fe Railway, which relied on America's first interstate network of restaurants to feed passengers en route. The "Harvey Houses", located strategically along the line, served top-quality meals to railroad patrons during water stops and other planned layovers and were favored over in-transit facilities for all trains operating west of Kansas City.		As competition among railroads intensified, dining car service was taken to new levels. When the Santa Fe unveiled its new Pleasure Dome lounge cars in 1951, the railroad introduced the travelling public to the Turquoise Room, promoted as "The only private dining room in the world on rails." The room accommodated 12 guests, and could be reserved anytime for private dinner or cocktail parties, or other special functions. The room was often used by celebrities and dignitaries traveling on the Super Chief.		Edwin Kachel was a steward for more than twenty-five years in the Dining-Car Department of the Great Northern Railway. He said that "on a dining car, three elements can be considered -- the equipment, the employee, then passenger." In other words, "the whole is constituted by two-thirds of human parts."[1] As cross-country train travel became more commonplace, passengers began to expect high-quality food to be served at the meals on board. The level of meal service on trains in the 1920s and 1930s rivaled that of high-end restaurants and clubs.[2]		Elegance is one of the main words used to describe the concept of dining on a train. Use of fresh ingredients was encouraged whenever possible. Some of the dishes prepared by chefs were: Braised Duck Cumberland, Hungarian Beef Goulash with Potato Dumplings, Lobster Americaine, Mountain Trout Au Bleu, Curry of Lamb Madras, Scalloped Brussels Sprouts, Pecan and Orange Sticks and Pennepicure Pie to name a few items.[2]		The Christmas menu for the Chicago, Milwaukee & St. Paul Railway in 1882 listed the following items: Hunter's Soup, Salmon with Hollandaise Sauce, Boned Pheasant in Aspic Jelly, Chicken Salad, Salmis Prairie Chicken, Oyster Patties, Rice Croquette, Roast Beef, English Ribs of Beef, Turkey with Cranberry Sauce, Stuffed Suckling Pig with Applesauce, Antelope Steak with Currant Jelly, potatoes, green peas, tomatoes, sweet potatoes, Mince Pie, Plum Pudding, Cake, Ice Cream, Fruits and coffee.[2]		In one of the most common dining car configurations, one end of the car contains a galley (with an aisle next to it, so passengers can pass through the car to the rest of the train) while the other end has table or booth seating on either side of a center aisle.		Trains with high demand for dining car services sometimes feature "double-unit dining cars" consisting of two adjacent cars functioning to some extent as a single entity, generally with one car containing a galley plus table or booth seating and the other car containing table or booth seating only.		In the dining cars of Amtrak's modern bilevel Superliner trains, booth seating on either side of a center aisle occupies almost the entire upper level, while the galley is below; food is sent to the upper level on a dumbwaiter.		Dining cars enhance the familiar restaurant experience with the unique visual entertainment of the ever-changing view. While dining cars are less common today than in the past (having been supplemented, or in some cases replaced altogether by other types of food-service cars) they still play a significant role in passenger railroading, especially on medium- and long-distance trains.		Today, a number of tourist-oriented railroads offer dinner excursions to capitalize on the public's fascination with the dining car experience.		The U76/U70 tram line between the German cities of Düsseldorf and Krefeld offers a Bistrowagen (dining car in German), where passengers can order drinks and snacks. This practise comes from the early 20th Century, when interurban trams conveyed a dining car. Despite the introduction of modern tram units, 4 trams still have a Bistrowagen and operate every weekday.		The dining car of the Via Rail Canadian prepared for meal service		Wagons-Lits dining car in Austria in 2003		The pantry aboard former Santa Fe dining car #1474, the Cochiti. Over a million meals were served in the car, which remained in service through the late 1960s.		An 1880s print advertisement extolling the virtues of meal service aboard the Chicago and Alton Railroad		Indian Railways dining car kitchen		
The following outline is provided as an overview of and topical guide to cuisines:		Cuisine – specific set of cooking traditions and practices, often associated with a specific culture. It is frequently named after the region or place where its underlining culture is present. A cuisine is primarily influenced by the ingredients that are available locally or through trade. Religious food laws can also exercise a strong influence on culinary practices.								Meals   (outline) – cuisine is generally served in the form of a meal. A meal is an eating occasion that takes place at a certain time and includes specific, prepared food, or the food eaten on that occasion.[2][3] The names used for specific meals in English vary greatly, depending on the speaker's culture, the time of day, or the size of the meal. Meals are composed of one or more courses,[4] which in turn are composed of one or more dishes.		Meal structure varies by culture. Here are some examples:		List of historical cuisines		
In restaurant terminology a table d'hôte (French pronunciation: ​[tablə.dot]; lit. "table of the host") menu is a menu where multi-course meals with only a few choices are charged at a fixed total price. Such a menu may be called prix fixe ("fixed price"). The terms set meal and set menu are also used. The cutlery on the table may also already be set for all of the courses.		Table d'hôte contrasts with à la carte, where customers may order any of the separately priced menu items available.						Table d'hôte is a French loan phrase that literally means "the host's table". The term is used to denote a table set aside for residents of a guesthouse (fr), who presumably sit at the same table as their host.		The meaning shifted to include any meal featuring a set menu at a fixed price. In the original sense, its use in English is documented as early as 1617, while the later extended use, now more common, dates from the early nineteenth century. This meaning is not used in France.[1]		Many restaurants in the United States convert their menus to prix fixe only for special occasions. Generally, this practice is limited to holidays where entire families dine together, such as Easter and Thanksgiving, or on couple-centric holidays like Valentine's Day and Sweetest Day.[2]		In France, table d'hôte refers to the shared dining (sometimes breakfast and lunch) offered in a vacation named chambre d'hôte (similar to "bed and breakfast"). Every guest of a chambre d'hôte can join this meal, cooked by the hosting family. It is not a restaurant, there is only one service, the price is fixed and usually included in the vacation. Everyone sits around a large table and makes small-talk about the house, the country, and so on.		What is closer in French to the meaning of table d'hôte in English is a menu ("lunch special" or "fixed menu"). It usually includes several dishes to pick in a fixed list: an entrée (introductory course), a main course (a choice between up to four dishes), a cheese, a dessert, bread, and sometimes beverage (wine) and coffee all for a set price fixed for the year between €15 to €55. The menu du jour, a cheaper version with less choice, an entrée and a main course, the plat du jour ("dish of the day") changed every day, is usually between €9 to €15.[3]		In Belgium, restaurants in the medium to high price range tend to serve menus where the customer can compose a menu from a list of entrees, main courses and desserts. These dishes can be ordered separately and all have a different pricing depending on the ingredients used. However combined in a three-, five-, or seven-course menu they will be served at a fixed pricing that is usually €10–15 cheaper than when ordered separately. Also in many cases if a menu is chosen it will be accompanied by amuses (little side dishes between the courses). Wine and other beverages are almost always excluded.		In Sweden almost all restaurants - from the simplest diner to the finest luxury restaurant - serve Dagens rätt ("the daily dish") during lunch hours (on weekdays) at a much lower price than the same dish would cost at other times. Most commonly there is a choice of two or three dishes: a meat/fish/poultry dish, a vegetarian alternative, and a pasta. Salad buffet, bread and butter and beverage are included, and sometimes also a simple starter, like a soup.		In India, the thali (meaning "plate") is very common in restaurants. The main course consisting of rice or roti (flat bread) and assorted side dishes and vegetables is arranged on a large plate. This may be followed by dessert. There may be more than one kind of thali – vegetarian, tandoori, deluxe – the name signifying the prix-fixe items[clarification needed] as well as the price.		In Spain, there is the menú or menú del día, which usually includes a starter, a main dish, bread, drink and choice of coffee or dessert. It may range in price from €8 to €30, with €10 being the average price.[citation needed]		In Romania, the most typical fix-price menu is called daily menu (meniul zilei), taken in the daytime, on weekdays only.		In Russia, the most typical fix-price menu is called business lunch (бизнес ланч), taken in the daytime, on weekdays only.		In Japan, a similar practice is referred to as teishoku (定食). This has a fixed menu and often comes with side dishes such as pickled vegetables and miso soup.[4] Typical prices can range from ¥800 to ¥1,500.[5]		In Italy, this is the typical practice in small rural restaurants called osterie (singular osteria, from oste meaning "host" as in the French hôte mentioned above). Osterie vary widely in what they offer, but most serve simple foods and wine sourced locally, and prepared according to the local practices. Other Italian restaurants offer a selection of antipasti at a fixed price; often enough to fare una tavola completa ("fill the table"). Diners enjoy an informal meal as they serve themselves various small portions family style.		
Culinary artist, in which culinary means "related to cooking", is the art of the preparation, cooking and presentation of food, usually in the form of meals. People working in this field – especially in establishments such as restaurants – are commonly called "chefs" or "cooks", although, at its most general, the terms "culinary artist" and "culinarian" are also used. Table manners ("the table arts") are sometimes referred to as a culinary art.		Expert Culinarians are required to have knowledge of food science, nutrition and diet and are responsible for preparing meals that are as pleasing to the eye as well as to the palate. After restaurants, their primary places of work include delicatessens and relatively large institutions such as hotels and hospitals.						The Culinary Arts, in the Western world, as a craft and later as a field of study, began to evolve at the end of the Renaissance period. Prior to this, chefs worked in castles, cooking for kings and queens, as well as their families, guests, and other workers of the castle. As Monarchical rule became phased out as a modality, the chefs took their craft to inns and hotels. From here, the craft evolved into a field of study.		A great deal of the study of Culinary Arts in Europe was organized by Jean Anthelme Brillat-Savarin, a man famous for his quote "Tell me what you eat, and I will tell you what you are," which has since been mistranslated and oversimplified into "You are what you eat." Other people helped to parse out the different parts of food science and gastronomy. Over time, increasingly deeper and more detailed studies into foods and the Culinary Arts has led to a greater wealth of knowledge.		In Asia, a similar path led to a separate study of the Culinary Arts, which later essentially merged with the Western counterpart. In the modern international marketplace, there is no longer a distinct divide between Western and Eastern foods. Culinary Arts students today, generally speaking, are introduced to the different cuisines of many different cultures from around the world.		Today, there are thousands of Culinary Arts schools around the world. Additionally, most universities, as well as many smaller tertiary schools like community colleges, offer some type of Culinary Arts Degree, which is technically a Bachelor of Arts Degree.		Modern Culinary Arts students study many different aspects of food. Specific areas of study include butchery, chemistry and thermodynamics, visual presentation, food safety, human nutrition and physiology, international history, the manufacture of food items (such as the milling of wheat into flour or the refining of cane plants into crystalline sucrose), and many others.		Training in culinary arts is possible in most countries around the world. Usually at tertiary level (university etc.). With institutions government funded, privately funded or commercial.		
Beverages most commonly eligible for free refills are coffee and fountain soft drinks.[1]						Although no one can trace the exact roots of the "free refill", there are a few historical references. According to the book The World of Caffeine: The Science and Culture of the World's Most Popular Drug, around the 1890s if you were to ask a European visitor what in his opinion is the most noteworthy feature of American cafes, he is most likely to say, "they refill your cup without charge, without asking!"[2] This book also contains another historical reference; it refers to the American roots of free refill. It states, "perhaps, the endless refill is symbolic of America's special affection for coffee and its culture of largesse and informality as well".[2]		In 1988 Taco Bell launched their "value initiative" which included drive-through windows, reduced prices, and free refills. As a major soda company owned Taco Bell, this was strategically done to increase revenue and build the soda company's brand awareness.[3] Free refills are now offered in most American restaurants.		Free refills are seen as a good way to attract customers to an establishment, especially one whose beverages are not their primary source of income.[1] Due to the extremely low cost of fountain soft drinks, often offering a profit margin of 80-82%, establishments tend to offer free refills as a sales gimmick.[4] Coffee produces a similar high profit margin, allowing establishments to include coffee in free refill offers.[1] Most of these establishments have fast customer turnover, thus customers rarely consume enough beverage to make the offering unprofitable. Some establishments, who make their primary income with beverage sales, only offer free refills to members of rewards programs.[5]		Bars in the United States often do not charge designated drivers for soft drinks, including refills.		In certain areas of the United States, such as Massachusetts and New York, politicians have proposed banning free refills as a move against obesity.[6]		When New York City banned sugary drinks over 16 ounces in 2012, critics faulted free refills as one of the ban's biggest weaknesses.[7]		In June 2012 Cambridge, Massachusetts, mayor Henrietta Davis unsuccessfully proposed banning soft drink refills.[8][9]		The French government is another critic of free refills, due to health concerns about obesity.[10] France created a tax on sugary drinks in 2011.[10] In September 2014, Serge Hercberg, head of France's National Nutrition and Health Programme, stated that free refills of sugary drinks should be banned.[10] In January 2017, a law was passed banning the unlimited sale of sodas and other sugary drinks at all public eateries.[11]		
Merienda is a light meal[1] in Southern Europe, particularly Spain, Portugal (lanche), Italy (merenda), Slovenia and Croatia (marenda), as well as Hispanic America and the Philippines. Usually taken in the afternoon or for brunch, it fills in the meal gap between the noontime meal and the evening meal, being the equivalent of afternoon tea; or between breakfast and lunch. It is a simple meal that often consists of a piece of fruit, cookies, yogurt, and other snacks paired with juice, hot chocolate, coffee, spirits, or other beverages.		It is typical for Argentines, Paraguayans and Uruguayans to have merienda or "tea" around 5pm, between the midday meal and supper. It generally consists of an infusion (tea, mate, coffee, mate cocido, etc.) and a baked snack (scones, bread, toasts, cake, facturas, etc.), usually accompanied with dulce de leche, honey, butter or jam. North Americans may consider this light meal a kind of "second breakfast."		In the Philippines, merienda (Filipino: meryenda or minandál) is a generic term encompassing two light meals: the first is a morning snack that may correspond to either brunch, elevenses, or second breakfast; the second one is the equivalent of afternoon tea.[2] Merienda taken in the early evening around sunset just before or in place of dinner is meanwhile distinctly referred to as merienda cena.[3] Broadly, merienda is any sort of dish or snack in a portion smaller than the traditional "full meal" consisting of rice and a complementary viand (unless the merienda is taken as brunch or merienda cena), coupled with either a cool or hot drink (usually coffee). Common fare may be sweet or savoury, ranging from breads, pandesal, pastries, or even noodles (pancit). Sometimes Filipino meriendas can include wide ranging desserts such as halo-halo, "champorado"and even food such as balút (egg with an advanced embryo in it).[4]		In coastal parts of Croatia, Slovenia, and in the Greek island of Corfu,[5] it is called marenda, a meal eaten between breakfast and lunch.[6] Usually it is a light snack, like sandwiches or toast, eaten during a work break.		French speaking countries will refer to a "Goûter" for this 4 PM light meal.		
Supper is the main evening meal or can be used to describe a light snack later in the evening.						The term is derived from the French souper, which is used for this meal in Canadian French, Swiss French, and sometimes in Belgian French. It is related to soup. It is also related to the Scandinavian word for soup, soppa' and the German word for soup, Suppe. The Oxford English Dictionary, however, suggests that the root, sup, remains obscure in origin.[1]		The distinction between dinner and supper was common in United States farming communities into the twentieth century. In most parts of the United States and Canada today, "supper" and "dinner" are considered synonyms (although supper is a more antiquated term). In Saskatchewan, and much of Atlantic Canada, "supper" means the main meal of the day, usually served in the late afternoon, while "dinner" is served around noon. "Dinner" is used in some areas, such as Newfoundland and Labrador, to describe the noon meal as well as special meals, such as "Thanksgiving dinner", "flipper dinner" or "Christmas dinner," the evening meal being "supper." The word "supper" is also regionally reserved for harvest meals put on by churches and other community organizations: "fowl suppers" or "fall suppers" (featuring turkey) are common in Canada; "pancake suppers" given by church groups are common in the United States; and "bean suppers" (featuring baked beans) are common in New England and especially the state of Maine.[3] In addition, the term "supper" is most frequently used in Atlantic Canada.[citation needed]		"Supper" may refer to, on largely class-based distinctions, either a late-evening snack (working and middle class usage) or else to make a distinction between "supper" as an informal family meal (which would be eaten in the kitchen or family dining room) as opposed to "dinner", a generally grander affair (either or both in terms of the meal and the courses within the meal itself), which would be eaten in the best dining room, could well have guests from outside the household, and for which there might be a dress code.[4] It is common for social interest and hobby clubs that meet in the evening after normal dinner hours to announce that "a light supper" will be served after the main business of the meeting. Supper can also refer to the largest meal of the day.[citation needed]		In England and much of Canada, whereas “dinner,” when used for the evening meal, is fairly formal, “supper” is used to describe a less formal, simpler family meal. In some areas of the United Kingdom, "supper" is used to describe an evening meal when dinner has been eaten around noon. In some northern British and some Australian homes, as in New Zealand and Ireland, "tea" is used for the evening meal. In parts of the United Kingdom, supper is a term for a snack eaten after the evening meal and before bed, usually consisting of a warm, milky drink and British biscuits or cereal, but can include sandwiches.[citation needed]		In Scotland and some parts of northern England, in traditional fast food take-away fish and chip shops, it is common to refer to an item served with chips as a "supper", regardless of when it is served. For example, fish served with chips would be a "fish supper", or a sausage served with chips would be a "sausage supper". The term is in such common usage that it is necessary to qualify items ordered without chips as a "single", for example a "a fish supper and a single fish". It should also be noted that a "single" often comprises two items. A single sausage consists of two deep fried battered sausages.		In New Zealand it is similar, generally referring to cake and tea or coffee served later in the evening, particularly when people have visitors.[citation needed]		In Chinese food, the xiaoye (宵夜) is a late night supper or small meal, often translated into English as "supper". Many Chinese restaurants serve a special menu for xiaoye, including simpler items such as congee or soup noodles.[citation needed] Xiaoye may also be eaten at a night market, where small meals or substantial snacks known as xiaochi are commonly served.[citation needed] The xiaoye is a fourth meal commonly consumed by a person whose lifestyle or schedule requires staying awake late into the night, many hours after the normal evening meal; for example, a shift worker or an attendee at a night concert.[citation needed]		
A tea party is a formal, ritualized gathering for the small meal called afternoon tea.[1][2]		Formal tea parties are often characterized by the use of prestige utensils, such as porcelain, bone china or silver. The table is made to look its prettiest, with cloth napkins and matching cups and plates. In addition to tea, larger parties may provide punch, or in cold weather, hot chocolate. The tea is accompanied by a variety of foods that are easy to manage while in a sitting room: thin sandwiches, such as cucumber or tomato, cake slices, buns or rolls, cookies, biscuits and scones are all common.						The afternoon tea party was a feature of great houses in the Victorian and Edwardian ages in the United Kingdom and the Gilded Age in the United States, as well as in all continental Europe (France, Germany, and above all in the Russian Empire). The formal tea party still survives as a special event, as in the debutante teas of some affluent American communities.		In the older version, servants stayed outside the room until needed. Writing in 1922, Emily Post asserted that servants were never to enter the room unless rung for, to bring in fresh water and dishes or to remove used dishes.[3] This was partly due to the rigidity of social convention at the time, but it also reflected the intimate nature of the afternoon tea. Proving the truth of 18th-century author Henry Fielding's quip that "love and scandal are the best sweeteners of tea", the custom of banning servants from the drawing room during tea shows the hostess's desire to encourage free conversation among her guests. Most of the formalities of that age have disappeared, particularly since World War II, when economic changes made household servants a rarity, but afternoon tea can still provide a good opportunity for intimate conversation and a refreshing light meal.		A less formal large afternoon party for tea was known during the 18th and 19th centuries as a "kettle drum". A widespread but possibly false folk etymology suggests that the name "kettle drum" may have originated in the informal tea gatherings hosted by British camp officers' wives during East India Company rule or the British occupation of India, during which kettle drums are claimed to have served as tea tables in the camps.[4][5] Alternatively, "kettle drum" may have been an amalgam of "drum" — 18th-century slang for a vivacious party — and "kettle" for the tea served.[6] At kettle drums, guests traditionally came for short periods and left at will, mingled and conversed with little formality, and partook of tea, chocolate, lemonade, cakes, and sandwiches. Guests were expected to dress for ordinary daytime visiting, but not more formally.		Tea parties are also created by young children where the guests consist of stuffed animals, dolls, friends (both real and imaginary) and family members.[7]		In the chapter "A Mad Tea-Party" in Alice's Adventures in Wonderland, Alice becomes a guest at a tea party along with the March Hare, the Hatter, and a sleeping Dormouse who remains asleep for most of the chapter. The other characters give Alice many riddles and stories, including the famous 'Why is a raven like a writing desk?'. The Hatter reveals that they have tea all day because time has punished him by eternally standing still at 6 pm (tea time). Alice becomes insulted and tired of being bombarded with riddles and she leaves, claiming that it was the stupidest tea party that she had ever been to.[8]		Yum cha is the Chinese equivalent of a tea party, though it is usually held in a restaurant.[9]		
Global Cosmopolitans refers to "a talented population of highly educated multilingual people that have lived, worked and studied for extensive periods in different cultures. While their international identities have diverse starting points and experiences, their views of the world and themselves are profoundly affected by both the realities of living in different cultures and their manner of coping with the challenges that emerge.".[1]		The term was developed by Linda Brimm, Professor of Organizational Behavior at INSEAD and further explored in her book Global Cosmopolitans: The Creative Edge of Difference.				
Mukbang (from Korean: 먹방; meokbang; lit. "eating broadcasting") is an online audiovisual broadcast in which a host eats large quantities of food while interacting with their audience. Usually done through an internet webcast (such streaming platforms include Afreeca), mukbang became popular in South Korea in the 2010s.[1][2][3] Foods ranging from pizza to noodles are consumed in front of a camera for an internet audience (who pay or not, depending on which platform one is watching).		In each broadcast, a host will often interact with their viewers through online chatrooms. With the rising popularity of these eating shows, hosts have found lucrative ways of benefiting from the online show. Many hosts generate revenue through mukbang, by accepting donations or partnering with advertising networks.[3]						The word mukbang comes from the Korean words for "eating" (먹는; meokneun) and "broadcast" (방송; bangsong).[4][5]		Other genres of mukbang include "cook-bang" (cooking and eating) shows. The idea of socializing with an audience remains the same, however; the host would then eat what was cooked and describe to the audience what was consumed.[citation needed]		South Korean video game players have sometimes broadcast mukbang as breaks during their overall streams. The popularity of this practice among local users led the video game streaming service Twitch.tv to begin trialling a dedicated "Social eating" category in July 2016; a representative of the service stated that this category is not necessarily specific to mukbang, but would leave the concept open to interpretation by streamers within its guidelines.[6]		The mukbang Internet culture began on AfreecaTV in 2009.[7]		There are several explanations given by various scholars. Jeff Yang, an Asian-American cultural critic and senior vice president of the global research firm Kantar Futures, said that mukbang had its origins in “the loneliness of unmarried or uncoupled [South] Koreans, in addition to the inherently social aspect of eating in [South] Korea” during the interview with Quartz.[8]		Kim-Hae Jin, Ph.D candidate from Choson University, argued that one can vicariously satisfy the desire for the food. The hosts, who call themselves BJs (Broadcast Jockeys), interact with the people who are watching the broadcast through chatting. BJs sometimes claim to be the audience's “avatar” and will exactly follow what people ask them to do.[9]		Adema contends in her article: "food television incorporates the vicarious pleasures of watching someone else cook and eat; the emulsion of entertainment and cooking; the jumbling of traditional gender roles; and ambivalence toward cultural standards of body, consumption, and health. … simultaneously perpetuates the stress of social expectations, and sprinkles sexual innuendos in a venue traditionally associated with maternal security."[10]		The popularity of mukbang has inspired different variations and adaptions of the “Eating Broadcasting” concept. This trend has continued to gain viewers, create stars, and profit, catching the interest of mainstream media both domestically and internationally. In South Korea, there was a drama called Let's Eat (Hangul: 식샤를 합시다; RR: Siksyareul Habsida) that focused on people who were brought together due to their love of food. In the drama, the characters explore various restaurants and after each episode, the featured foods became a hot topic among young adult viewers. Viewers sought out these restaurants.[11]		Broadcasting stations are looking to capitalize on this interest in other ways as well. Happy Together, a popular entertainment show in South Korea, has a segment where their celebrity guests will cook and then share their favorite dishes with the rest of the cast.[12] JTBC, a South Korean general cable TV network is also looking to jump on the bandwagon with a new variety show in the works. They are planning on a food-centric variety show called Girls Who Eat Well and are looking to cast girl group members from popular South Korean K-pop girl groups.[13] Popular South Korean variety series Infinite Challenge has also showcased the phenomenon.[14]		Mainstream media is not the only platform to showcase mukbang. For example, celebrities have done mukbang broadcasts as a CF to promote a food brand.[15]		Mukbang has also gained international interest as well. The popular YouTube series, Youtubers React, showed various YouTube stars reacting to the South Korean trend and ended with their own mini mukbang show.		Park Seo-yeon is known to have been the highest earning Broadcast Jockey to date. She earned an estimated $9,300 a month from her fans' and viewers' donations in 2014.[16] Her broadcast videos can be found on AfreecaTV and YouTube. A CNN segment featuring her drew more attention towards the South Korean phenomenon of mukbang.		BJ Fitness Fairy was a former physique builder who became interested in the phenomenon of sitting in front of a camera and eating, broadcasting to many people online. She streams on AfreecaTV and spends several hours eating and communicating with her fans and viewers, earning about $4000 a week. BJ Fitness Fairy spends several hours exercising to keep up her physique.[17]		BJ Hyo-Jjang's real name is Kim Hyo Jin, and she is a broadcaster that is watched by over 100 viewers. Before she began this phenomenon of a mukbang, she was a translator. As she started recording herself eat, she decided to become a full-time mukbang star. She plans to continue broadcasting as long as she has captive viewers.[18]		BJ Patoo is a 14-year-old broadcaster who makes an estimated $1,500 a night.[19]		Internet personality Trisha Paytas has been both noted for her popular mukbang videos that she has uploaded to YouTube. Most notably, Paytas' "KFC/Fried Chicken" mukbang video has amassed to over a million views on YouTube.[20][21]		Yang Soobin (Hangul: 양수빈; born October 17, 1994) is a South Korean influencer known best for her food broadcast, Mukbang. She has 2.4M fans focusing on her Facebook page. Her collaboration with KFC Thailand resulted in 4.9M views (and counting).[22] She’s been receiving much popularity especially in South East Asian countries. Her Mukbang post weekly reach is 12M on average. She has also starred in a couple of TV series.[23]		
Zakuski (plural from Russian: закуски [zɐˈkuskʲɪ]; singular zakuska from закуска) is a Slavic term for hot and cold hors d'oeuvres, entrées and snacks,[1][2] either as a course as it is or "intended to follow each shot of vodka or another alcoholic drink."[3] The word literally means something to bite after.[4]		The introduction of zakuski into Russian cuisine is usually attributed to Peter the Great who brought many cultural elements from Northern Europe to Russia. Thereby this custom was probably adopted from Swedish and Finnish brännvinsbord which was also the ancestor of modern smörgåsbord.[2] A table with zakuski was kept in the houses of the Russian gentry for feeding casual visitors who travelled long distances and whose arrival time was often unpredictable.[1] At banquets and parties, zakuski were often served in a separate room adjacent to the dining room or on a separate table in the dining room. The tradition eventually spread to other layers of society and remained in the Soviet times, but due to lack of space, they were served on the dinner table. Zakuski became thus the first course of a festive dinner.[2]		Nowadays, these appetizers are commonly served at banquets, dinners, parties and receptions in countries which were formerly part of the Russian Empire including some post-Soviet states and Poland (Polish: zakąski).[1][5] A broad selection of zakuski constitutes a standard first course at any feast table. Usually, zakuski are already laid on the table when guests are called to the dining room.[1]		Typical zakuski consist of cold cuts, cured fishes, mixed salads, kholodets (meat jelly), pirogs or pirozhki, various pickled vegetables such as tomatoes, beets, cucumbers, sauerkraut, pickled mushrooms, deviled eggs, hard cheeses, caviar, canapés, open sandwiches, and breads.[2][6]						Pickled cucumbers, popular form of zakąska in Poland[5]		Pirozhki, pickled tomato, mixed salads		Kholodets		Canapés with sprats		Olivier salad		Brined or pickled herring		Sauerkraut salad		Dressed herring		Breads with salo		Julienne (ru)		Squash caviar (ru)		Rasstegai (ru)		Kolbasa (sausages)		
Small plates is a manner of dining that became popular in Western food service after 2000. Small plates may either refer to small dishes resembling appetizers which are ordered à la carte and often shared (such as tapas), or to the small courses served as part of a more formal meal.		Some types of small plates which have influenced the modern Western concept are:		
Table setting (laying a table) or place setting refers to the way to set a table with tableware—such as eating utensils and for serving and eating. The arrangement for a single diner is called a place setting. The practice of dictating the precise arrangement of tableware has varied across cultures and historical periods.						Informal settings generally have fewer utensils and dishes but use a layout based on more formal settings. Utensils are arranged in the order and according to the manner in which the diner will use them. In the West, forks, plate, butter knife, and napkin generally are placed to the left of the dinner plate, and knives, spoons, stemware and tumblers, cups, and saucers to the right. (By contrast, formal settings in Armenia place the fork to the right of the dinner plate and informal settings in Turkey place the fork to the right of the dinner plate if not accompanied by a knife) Sauceboats and serving dishes, when used, either are placed on the table or, more formally, may be kept on a side table.		At an informal setting, fewer utensils are used and serving dishes are placed on the table. Sometimes the cup and saucer are placed on the right side of the spoon, about 30cm or 12 inches from the edge of the table. Often, in less formal settings, the napkin should be in the wine glass. However, such objects as napkin rings are very rare in the United Kingdom, Spain, Mexico, or Italy.		Utensils are placed inward about 20cm or 8 inches from the edge of the table, with all placed either upon the same invisible baseline or upon the same invisible median line. Utensils in the outermost position are to be used first (for example, a soup spoon or a salad fork, later the dinner fork and the dinner knife). The blades of the knives are turned toward the plate. Glasses are placed an inch (2.5 cm) or so above the knives, also in the order of use: white wine, red wine, dessert wine, and water tumbler.		The most formal dinner is served from the kitchen. When the meal is served, in addition to the central plate (a service plate or dinner plate at supper; at luncheon, a service plate or luncheon plate) at each place there are a bread roll (generally on a bread plate, sometimes in the napkin), napkin, and flatware (knives and spoons to the right of the central plate, and forks to the left). Coffee is served in Butler Service style in demitasses, and a spoon placed on the saucer to the right of each handle. Serving dishes and utensils are not placed on the table for a formal dinner.[1] The only exception in the West to these general rules is the protocol followed at the Spanish royal court, which was also adopted by the Austrian court, in which all cutlery was placed to the right of the central plate for each diner.		At a less formal dinner, not served from the kitchen, the dessert fork and spoon can be set above the plate, fork pointing right, spoon pointing left.[2]		
In biology, detritus (/dᵻˈtraɪtəs/) is dead particulate organic material (as opposed to dissolved organic material). It typically includes the bodies or fragments of dead organisms as well as fecal material. Detritus is typically colonized by communities of microorganisms which act to decompose (or remineralize) the material. In terrestrial ecosystems, it is encountered as leaf litter and other organic matter intermixed with soil, which is referred to as humus. Detritus of aquatic ecosystems is organic material suspended in water and piling up on seabed floors, which is referred to as marine snow.		Dead plants or animals, material derived from animal tissues (such as skin cast off during moulting etc) gradually lose their form, due to both physical processes and the action of decomposers, including grazers, bacteria and fungi. Decomposition, the process through which organic matter is decomposed, takes place in many stages. Materials like proteins, lipids and sugars with low molecular weight are rapidly consumed and absorbed by microorganisms and organisms that feed on dead matter. Other compounds, such as complex carbohydrates are broken down more slowly. The various microorganisms involved in the decomposition break down the organic materials in order to gain the resources they require for their own survival and proliferation. Accordingly, at the same time that the materials of plants and animals are being broken down, the materials (biomass) making up the bodies of the microorganisms are built up by a process of assimilation. When microorganisms die, fine organic particles are produced, and if these are eaten by small animals which feed on microorganisms, they will collect inside the intestine, and change shape into large pellets of dung. As a result of this process, most of the materials from dead organisms disappears from view and is not obviously present in any recognisable form, but is in fact present in the form of a combination of fine organic particles and the organisms using them as nutrients. This combination is detritus.		In ecosystems on land, detritus is deposited on the surface of the ground, taking forms such as the humic soil beneath a layer of fallen leaves. In aquatic ecosystems, most detritus is suspended in water, and gradually settles. In particular, many different types of material are collected together by currents, and much material settles in slowly flowing areas.		Much detritus is used as a source of nutrition for animals. In particular, many bottom feeding animals (benthos) living in mud flats feed in this way. In particular, since excreta are materials which other animals do not need, whatever energy value they might have, they are often unbalanced as a source of nutrients, and are not suitable as a source of nutrition on their own. However, there are many microorganisms which multiply in natural environments. These microorganisms do not simply absorb nutrients from these particles, but also shape their own bodies so that they can take the resources they lack from the area around them, and this allows them to make use of excreta as a source of nutrients. In practical terms, the most important constituents of detritus are complex carbohydrates, which are persistent (difficult to break down), and the microorganisms which multiply using these absorb carbon from the detritus, and materials such as nitrogen and phosphorus from the water in their environment to synthesise the components of their own cells.		A characteristic type of food chain called the detritus cycle takes place involving detritus feeders (detritivores), detritus and the microorganisms that multiply on it. For example, mud flats are inhabited by many univalves which are detritus feeders, such as moon shells[disambiguation needed]. When these detritus feeders take in detritus with microorganisms multiplying on it, they mainly break down and absorb the microorganisms, which are rich in proteins, and excrete the detritus, which is mostly complex carbohydrates, having hardly broken it down at all. At first this dung is a poor source of nutrition, and so univalves pay no attention to it, but after several days, microorganisms begin to multiply on it again, its nutritional balance improves, and so they eat it again. Through this process of eating the detritus many times over and harvesting the microorganisms from it, the detritus thins out, becomes fractured and becomes easier for the microorganisms to use, and so the complex carbohydrates are also steadily broken down and disappear over time.		What is left behind by the detritivores is then further broken down and recycled by decomposers, such as bacteria and fungi.		This detritus cycle plays a large part in the so-called purification process, whereby organic materials carried in by rivers is broken down and disappears, and an extremely important part in the breeding and growth of marine resources. In ecosystems on land, far more essential material is broken down as dead material passing through the detritus chain than is broken down by being eaten by animals in a living state. In both land and aquatic ecosystems, the role played by detritus is too large to ignore.		In contrast to land ecosystems, dead materials and excreta in aquatic ecosystems do not settle immediately, and the finer the particles involved are, the longer they tend to take. In freshwater bodies organic material from plants can form a silt known as mulm or humus on the bottom. This material, some called undissolved organic carbon breaks down into dissolved organic carbon and can bond to heavy metal ions via chelation. It can also break down into colored dissolved organic matter such as tannin, a specific form of tannic acid. In saltwater bodies, organic material breaks down and forms a marine snow that slowly settles down to the ocean bottom.		Detritus occurs in a variety of terrestrial habitats including forest, chaparral and grassland. In forests the detritus is typically dominated by leaf, twig, and bacteria litter as measured by biomass dominance. This plant litter provides important cover for seedling protection as well as cover for a variety of arthropods, reptiles[1] and amphibians. Some insect larvae feed on the detritus.[2] Fungi and bacteria continue the decomposition process[3] after grazers have consumed larger elements of the organic materials, and animal trampling has assisted in mechanically breaking down organic matter. At the later stages of decomposition, mesophilic micro-organisms decompose residual detritus, generating heat from exothermic processes; such heat generation is associated with the well known phenomenon of the elevated temperature of composting.		There is an extremely large number of detritus feeders in water. After all, a large quantity of material is carried in by water currents. Even if an organism stays in a fixed position, as long as it has a system for filtering water, it will be able to obtain enough food to get by. Many rooted organisms survive in this way, using developed gills or tentacles to filter the water to take in food, a process known as filter feeding.		Another more widely used method of feeding, which also incorporates filter feeding, is a system where an organism secretes mucus to catch the detritus in lumps, and then carries these to its mouth using an area of cilia. This is called mucus feeding.		Many organisms, including sea slugs and serpent's starfish, scoop up the detritus which has settled on the water bed. Bivalves which live inside the water bed do not simply suck in water through their tubes, but also extend them to fish for detritus on the surface of the bed.		In contrast, from the point of view of organisms using photosynthesis such as plants and plankton, detritus reduces the transparency of the water and gets in the way of this process. Given that these organisms also require a supply of nutrient salts—in other words fertilizer—for photosynthesis, their relationship with detritus is a complex one.		In land ecosystems, the waste products of plants and animals collect mainly on the ground (or on the surfaces of trees), and as decomposition proceeds, plants are supplied with fertiliser in the form of inorganic salts. In water ecosystems, relatively little waste collects on the water bed, and so the progress of decomposition in water takes a more important role. Investigating the level of inorganic salts in sea ecosystems shows that unless there is an especially large supply, the quantity increases from winter to spring—but is normally extremely low in summer. As such, the quantity of seaweed present reaches a peak in early summer and then decreases. The thinking is that organisms like plants grow quickly in warm periods and thus the quantity of inorganic salts is not enough to keep up with the demand. In other words, during winter, plant-like organisms are inactive and collect fertiliser, but if the temperature rises to some extent they will use this up in a very short period.		It is not entirely true that their productivity falls during the warmest periods. Organisms such as dinoflagellate have mobility, the ability to take in solid food, and the ability to photosynthesise. This type of micro-organism can take in substances such as detritus to grow, without waiting for it to be broken down into fertiliser.		In recent years, the word detritus has also come to be used in relation to aquariums (the word "aquarium" is a general term for any installation for keeping aquatic animals).		When animals such as fish are kept in an aquarium, substances such as excreta, mucus and dead skin cast off during moulting are produced by the animals and, naturally, generate detritus, and are continually broken down by micro-organisms.		Modern sealife aquariums often use the Berlin Method, which employs a piece of equipment called a protein skimmer, which produces air bubbles which the detritus adheres to, and forces it outside the tank before it decomposes, and also a highly porous type of natural rock called live rock where many bentos and bacteria live (hermatype which has been dead for some time is often used), which causes the detritus-feeding bentos and micro-organisms to undergo a detritus cycle. The Monaco system, where an anaerobic layer is created in the tank, to denitrify the organic compounds in the tank, and also the other nitrogen compounds, so that the decomposition process continues until the stage where water, carbon dioxide and nitrogen are produced, has also been implemented.		Initially, the filtration systems in water tanks often worked as the name suggests, using a physical filter to remove foreign substances in the water. Following this, the standard method for maintaining the water quality was to convert ammonium or nitrates in excreta, which have a high degree of neurotoxicity, but the combination of detritus feeders, detritus and micro-organisms has now brought aquarium technology to a still higher level.		
A dish in gastronomy is a specific food preparation, a "distinct article or variety of food,"[1] with cooking finished, and ready to eat, or be served.		A dish may be served on tableware, or may be eaten out of hand; but breads are generally not called dishes.		Instructions for preparing a dish are called recipes. Some dishes, for example vanilla ice cream with fudge sauce, rarely have their own recipes (and are not found in most cookbooks), as they are made by simply combining two ready to eat preparations of foods.						Many dishes have specific names (e.g. sauerbraten), while others are simply described ("broiled ribsteak"). Many are named for particular places, sometimes because of a specific association with that place like Boston baked beans or bistecca alla fiorentina. Sometimes not: poached eggs Florentine ends up meaning essentially "with spinach".[2] Some are named for particular individuals, perhaps to honor them including Brillat-Savarin cheese named for the 18th-century French gourmet and political figure Jean Anthelme Brillat-Savarin,[3] or perhaps because the dish was first prepared for them such as Chaliapin steak made by the order of the Russian opera singer Feodor Chaliapin in 1934 in Japan,[4][5] or perhaps they named it for themselves because they invented the dish, or perhaps because the dish was invented in their kitchens. Because of the many stories that have been told about the names of different dishes, it is often hard to know exactly where the names came from.		
Major depressive disorder (MDD), also known simply as depression, is a mental disorder characterized by at least two weeks of low mood that is present across most situations.[1] It is often accompanied by low self-esteem, loss of interest in normally enjoyable activities, low energy, and pain without a clear cause.[1] People may also occasionally have false beliefs or see or hear things that others cannot.[1] Some people have periods of depression separated by years in which they are normal while others nearly always have symptoms present.[3] Major depressive disorder can negatively affect a person's personal, work, or school life, as well as sleeping, eating habits, and general health.[1][3] Between 2–7% of adults with major depression die by suicide,[2] and up to 60% of people who die by suicide had depression or another mood disorder.[6]		The cause is believed to be a combination of genetic, environmental, and psychological factors.[1] Risk factors include a family history of the condition, major life changes, certain medications, chronic health problems, and substance abuse.[1][3] About 40% of the risk appears to be related to genetics.[3] The diagnosis of major depressive disorder is based on the person's reported experiences and a mental status examination.[7] There is no laboratory test for major depression.[3] Testing, however, may be done to rule out physical conditions that can cause similar symptoms.[7] Major depression should be differentiated from sadness which is a normal part of life and is less severe.[3] The United States Preventive Services Task Force (USPSTF) recommends screening for depression among those over the age 12,[8][9] while a prior Cochrane review found that the routine use of screening questionnaires have little effect on detection or treatment.[10]		Typically, people are treated with counseling and antidepressant medication.[1] Medication appears to be effective, but the effect may only be significant in the most severely depressed.[11][12] It is unclear whether medications affect the risk of suicide.[13] Types of counseling used include cognitive behavioral therapy (CBT) and interpersonal therapy.[1][14] If other measures are not effective electroconvulsive therapy (ECT) may be tried.[1] Hospitalization may be necessary in cases with a risk of harm to self and may occasionally occur against a person's wishes.[15]		Major depressive disorder affected approximately 216 million people (3% of the world's population) in 2015.[5] The percentage of people who are affected at one point in their life varies from 7% in Japan to 21% in France.[4] Lifetime rates are higher in the developed world (15%) compared to the developing world (11%).[4] It causes the second most years lived with disability after low back pain.[16] The most common time of onset is in a person in their 20s and 30s. Females are affected about twice as often as males.[3][4] The American Psychiatric Association added "major depressive disorder" to the Diagnostic and Statistical Manual of Mental Disorders (DSM-III) in 1980.[17] It was a split of the previous depressive neurosis in the DSM-II which also encompassed the conditions now known as dysthymia and adjustment disorder with depressed mood.[17] Those currently or previously affected may be stigmatized.[18]		Major depression significantly affects a person's family and personal relationships, work or school life, sleeping and eating habits, and general health.[19] Its impact on functioning and well-being has been compared to that of other chronic medical conditions such as diabetes.[20]		A person having a major depressive episode usually exhibits a very low mood, which pervades all aspects of life, and an inability to experience pleasure in activities that were formerly enjoyed. Depressed people may be preoccupied with, or ruminate over, thoughts and feelings of worthlessness, inappropriate guilt or regret, helplessness, hopelessness, and self-hatred.[21] In severe cases, depressed people may have symptoms of psychosis. These symptoms include delusions or, less commonly, hallucinations, usually unpleasant.[22] Other symptoms of depression include poor concentration and memory (especially in those with melancholic or psychotic features),[23] withdrawal from social situations and activities, reduced sex drive, irritability,[24] and thoughts of death or suicide. Insomnia is common among the depressed. In the typical pattern, a person wakes very early and cannot get back to sleep.[25] Hypersomnia, or oversleeping, can also happen.[25] Some antidepressants may also cause insomnia due to their stimulating effect.[26]		A depressed person may report multiple physical symptoms such as fatigue, headaches, or digestive problems; physical complaints are the most common presenting problem in developing countries, according to the World Health Organization's criteria for depression.[27] Appetite often decreases, with resulting weight loss, although increased appetite and weight gain occasionally occur.[21] Family and friends may notice that the person's behavior is either agitated or lethargic.[25] Older depressed people may have cognitive symptoms of recent onset, such as forgetfulness,[23] and a more noticeable slowing of movements.[28] Depression often coexists with physical disorders common among the elderly, such as stroke, other cardiovascular diseases, Parkinson's disease, and chronic obstructive pulmonary disease.[29]		Depressed children may often display an irritable mood rather than a depressed mood,[21] and show varying symptoms depending on age and situation.[30] Most lose interest in school and show a decline in academic performance. They may be described as clingy, demanding, dependent, or insecure.[25] Diagnosis may be delayed or missed when symptoms are interpreted as normal moodiness.[21]		Major depression frequently co-occurs with other psychiatric problems. The 1990–92 National Comorbidity Survey (US) reports that half of those with major depression also have lifetime anxiety and its associated disorders such as generalized anxiety disorder.[31] Anxiety symptoms can have a major impact on the course of a depressive illness, with delayed recovery, increased risk of relapse, greater disability and increased suicide attempts.[32] There are increased rates of alcohol and drug abuse and particularly dependence,[33] and around a third of individuals diagnosed with ADHD develop comorbid depression.[34] Post-traumatic stress disorder and depression often co-occur.[19] Depression may also coexist with attention deficit hyperactivity disorder (ADHD), complicating the diagnosis and treatment of both.[35] Depression is also frequently comorbid with alcohol abuse and personality disorders.[36]		Depression and pain often co-occur. One or more pain symptoms are present in 65% of depressed patients, and anywhere from 5 to 85% of patients with pain will be suffering from depression, depending on the setting; there is a lower prevalence in general practice, and higher in specialty clinics. The diagnosis of depression is often delayed or missed, and the outcome worsens. The outcome can also worsen if the depression is noticed but completely misunderstood.[37]		Depression is also associated with a 1.5- to 2-fold increased risk of cardiovascular disease, independent of other known risk factors, and is itself linked directly or indirectly to risk factors such as smoking and obesity. People with major depression are less likely to follow medical recommendations for treating and preventing cardiovascular disorders, which further increases their risk of medical complications.[38] In addition, cardiologists may not recognize underlying depression that complicates a cardiovascular problem under their care.[39]		The cause of major depressive disorder is unknown. The biopsychosocial model proposes that biological, psychological, and social factors all play a role in causing depression.[3][40] The diathesis–stress model specifies that depression results when a preexisting vulnerability, or diathesis, is activated by stressful life events. The preexisting vulnerability can be either genetic,[41][42] implying an interaction between nature and nurture, or schematic, resulting from views of the world learned in childhood.[43]		Childhood abuse, either physical, sexual or psychological are all risk factors for depression, among other psychiatric issues that co-occur such as anxiety and drug abuse. Childhood trauma also correlates with severity of depression, lack of response to treatment and length of illness. However, some are more susceptible to developing mental illness such as depression after trauma, and various genes have been suggested to control susceptibility.[44]		The 5-HTTLPR, or serotonin transporter promoter gene's short allele has been associated with increased risk of depression. However, since the 1990s results have been inconsistent, with three recent reviews finding an effect and two finding none.[45][46][47][48][49] Other genes that have been linked to a GxE interaction include CRHR1, FKBP5 and BDNF, the first two of which are related to the stress reaction of the HPA axis, and the latter of which is involved in neurogenesis.		Depression may also come secondary to a chronic or terminal medical conditioned such as HIV/AIDS, or asthma and may be labeled "secondary depression".[50][51] It is unknown if the underlying diseases induce depression through effect on quality of life, of through shared etiologies (such as degeneration of the basal ganglia in parkinson's disease or immune dysregulation in asthma).[52] Depression may also be iatrogenic (the result of healthcare), such as drug induced depression. Therapies associated with depression include interferon therapy, beta-blockers, Isotretinoin, contraceptives,[53] cardiac agents, anticonvulsants, antimigraine drugs, antipsychotics, and hormonal agents agents such as gonadotropin-releasing hormone agonist.[54] Drug abuse in early age is also associated with increased risk of developing depression later in life.[55] Depression that occurs as a result of pregnancy is called postpartum depression, and is though to be the result of hormonal changes associated with pregnancy.[56] Seasonal affective disorder, a type of depression associated with seasonal changes in sunlight, is though to be the result of decreased sunlight.[57]		The pathophysiology of depression is not yet understood, but the current theories center around monoaminergic systems, the circadian rhythm, immunological dysfunction, HPA axis dysfunction and structural or functional abnormalities of emotional circuits.		The monoamine theory, derived from the efficacy of monoaminergic drugs in treating depression, was the dominant theory until recently. The theory postulates that insufficient activity of monoamine neurotransmitters is the primary cause of depression. Evidence for the monoamine theory comes from multiple areas. Firstly, acute depletion of tryptophan, a necessary precursor of serotonin, a monoamine, can cause depression in those in remission or relatives of depressed patients; this suggests that decreased serotonergic neurotransmission is important in depression.[58] Secondly, the correlation between depression risk and polymorphisms in the 5-HTTLPR gene, which codes for serotonin receptors, suggests a link. Third, decreased size of the locus coeruleus, decreased activity of tyrosine hydroxylase, increased density of alpha-2 adrenergic receptor, and evidence from rat models suggest decreased adrenergic neurotransmission in depression.[59] Furthermore, decreased levels of homovanillic acid, altered response to dextroamphetamine, responses of depressive symptoms to dopamine receptor agonists, decreased dopamine receptor D1 binding in the striatum,[60] and polymorphism of dopamine receptor genes implicate dopamine in depression.[61][62] Lastly, increased activity of monoamine oxidase, which degrades monoamines, has been associated with depression.[63] However, this theory is inconsistent with the fact that serotonin depletion does not cause depression in healthy persons, the fact that antidepressants instantly increase levels of monoamines but take weeks to work, and the existence of atypical antidepressants which can be effective despite not targeting this pathway.[64] One proposed explanation for the therapeutic lag, and further support for the deficiency of monoamines, is a desensitization of self-inhibition in raphe nuclei by the increased serotonin mediated by antidepressants.[65] However, disinhibition of the dorsal raphe has been proposed to occur as a result of decreased serotonergic activity in tryptophan depletion, resulting in a depressed state mediated by increased serotonin. Further countering the monoamine hypothesis is the fact that rats with lesions of the dorsal raphe are not more depressive that controls, the finding of increased jugular 5-HIAA in depressed patients that normalized with SSRI treatment, and the preference for carbohydrates in depressed patients.[66] Already limited, the monoamine hypothesis has been further oversimplified when presented to the general public.[67]		Immune system abnormalities have been observed, including increased levels of cytokines involved in generating sickness behavior (which shares overlap with depression).[68][69][70] The effectiveness of nonsteroidal anti-inflammatory drugs (NSAIDs) and cytokine inhibitors in treating depression,[71] and normalization of cytokine levels after successful treatment further suggest immune system abnormalities in depression.[72]		HPA axis abnormalities have been suggested in depression given the association of CRHR1 with depression and the increased frequency of dexamethasone test non-suppression in depressed patients. However, this abnormality is not adequate as a diagnosis tool, because its sensitivity is only 44%.[73][74] These stress-related abnormalities have been hypothesized to be the cause of hippocampal volume reductions seen in depressed patients.[75] Furthermore, a meta-analysis yielded decreased dexamethasone suppression, and increased response to psychological stressors.[76] Further abnormal results have been obscured with the cortisol awakening response, with increased response being associated with depression.[77]		Theories unifying neuroimaging findings have been proposed. The first model proposed is the "Limbic Cortical Model", which involves hyperactivity of the ventral paralimbic regions and hypoactivity of frontal regulatory regions in emotional processing.[78] Another model, the "Corito-Striatal model", suggests that abnormalities of the prefrontal cortex in regulating striatal and subcortical structures results in depression.[79] Another model proposes hyperactivity of salience structures in identifying negative stimuli, and hypoactivity of cortical regulatory structures resulting in a negative emotional bias and depression, consistent with emotional bias studies.[80]		A diagnostic assessment may be conducted by a suitably trained general practitioner, or by a psychiatrist or psychologist,[19] who records the person's current circumstances, biographical history, current symptoms, and family history. The broad clinical aim is to formulate the relevant biological, psychological, and social factors that may be impacting on the individual's mood. The assessor may also discuss the person's current ways of regulating mood (healthy or otherwise) such as alcohol and drug use. The assessment also includes a mental state examination, which is an assessment of the person's current mood and thought content, in particular the presence of themes of hopelessness or pessimism, self-harm or suicide, and an absence of positive thoughts or plans.[19] Specialist mental health services are rare in rural areas, and thus diagnosis and management is left largely to primary-care clinicians.[81] This issue is even more marked in developing countries.[82] The mental health examination may include the use of a rating scale such as the Hamilton Rating Scale for Depression[83] or the Beck Depression Inventory[84] or the Suicide Behaviors Questionnaire-Revised.[85] The score on a rating scale alone is insufficient to diagnose depression to the satisfaction of the DSM or ICD, but it provides an indication of the severity of symptoms for a time period, so a person who scores above a given cut-off point can be more thoroughly evaluated for a depressive disorder diagnosis.[86] Several rating scales are used for this purpose.[86]		Primary-care physicians and other non-psychiatrist physicians have more difficulty with underrecognition and undertreatment of depression compared to psychiatric physicians, in part because of the physical symptoms that often accompany depression, in addition to the many potential patient, provider, and system barriers that the authors describe. A review found that non-psychiatrist physicians miss about two-thirds of cases, though this has improved somewhat in more recent studies.[87]		Before diagnosing a major depressive disorder, in general a doctor performs a medical examination and selected investigations to rule out other causes of symptoms. These include blood tests measuring TSH and thyroxine to exclude hypothyroidism; basic electrolytes and serum calcium to rule out a metabolic disturbance; and a full blood count including ESR to rule out a systemic infection or chronic disease.[88] Adverse affective reactions to medications or alcohol misuse are often ruled out, as well. Testosterone levels may be evaluated to diagnose hypogonadism, a cause of depression in men.[89] Vitamin D levels might be evaluated, as low levels of vitamin D have been associated with greater risk for depression.[90]		Subjective cognitive complaints appear in older depressed people, but they can also be indicative of the onset of a dementing disorder, such as Alzheimer's disease.[91][92] Cognitive testing and brain imaging can help distinguish depression from dementia.[93] A CT scan can exclude brain pathology in those with psychotic, rapid-onset or otherwise unusual symptoms.[94] In general, investigations are not repeated for a subsequent episode unless there is a medical indication.		No biological tests confirm major depression.[95] Biomarkers of depression have been sought to provide an objective method of diagnosis. There are several potential biomarkers, including Brain-Derived Neurotrophic Factor and various functional MRI techniques. One study developed a decision tree model of interpreting a series of fMRI scans taken during various activities. In their subjects, the authors of that study were able to achieve a sensitivity of 80% and a specificity of 87%, corresponding to a negative predictive value of 98% and a positive predictive value of 32% (positive and negative likelihood ratios were 6.15, 0.23, respectively). However, much more research is needed before these tests could be used clinically.[96]		The most widely used criteria for diagnosing depressive conditions are found in the American Psychiatric Association's revised fourth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR), and the World Health Organization's International Statistical Classification of Diseases and Related Health Problems (ICD-10), which uses the name depressive episode for a single episode and recurrent depressive disorder for repeated episodes.[97] The latter system is typically used in European countries, while the former is used in the US and many other non-European nations,[98] and the authors of both have worked towards conforming one with the other.[99]		Both DSM-IV-TR and ICD-10 mark out typical (main) depressive symptoms.[100] ICD-10 defines three typical depressive symptoms (depressed mood, anhedonia, and reduced energy), two of which should be present to determine depressive disorder diagnosis.[101][102] According to DSM-IV-TR, there are two main depressive symptoms—depressed mood and anhedonia. At least one of these must be present to make a diagnosis of major depressive episode.[103]		Major depressive disorder is classified as a mood disorder in DSM-IV-TR.[104] The diagnosis hinges on the presence of single or recurrent major depressive episodes.[21] Further qualifiers are used to classify both the episode itself and the course of the disorder. The category Depressive Disorder Not Otherwise Specified is diagnosed if the depressive episode's manifestation does not meet the criteria for a major depressive episode. The ICD-10 system does not use the term major depressive disorder but lists very similar criteria for the diagnosis of a depressive episode (mild, moderate or severe); the term recurrent may be added if there have been multiple episodes without mania.[97]		A major depressive episode is characterized by the presence of a severely depressed mood that persists for at least two weeks.[21] Episodes may be isolated or recurrent and are categorized as mild (few symptoms in excess of minimum criteria), moderate, or severe (marked impact on social or occupational functioning). An episode with psychotic features—commonly referred to as psychotic depression—is automatically rated as severe. If the patient has had an episode of mania or markedly elevated mood, a diagnosis of bipolar disorder is made instead.[105] Depression without mania is sometimes referred to as unipolar because the mood remains at one emotional state or "pole".[106]		DSM-IV-TR excludes cases where the symptoms are a result of bereavement, although it is possible for normal bereavement to evolve into a depressive episode if the mood persists and the characteristic features of a major depressive episode develop.[107] The criteria have been criticized because they do not take into account any other aspects of the personal and social context in which depression can occur.[108] In addition, some studies have found little empirical support for the DSM-IV cut-off criteria, indicating they are a diagnostic convention imposed on a continuum of depressive symptoms of varying severity and duration:[109] Excluded are a range of related diagnoses, including dysthymia, which involves a chronic but milder mood disturbance;[110] recurrent brief depression, consisting of briefer depressive episodes;[111][112] minor depressive disorder, whereby only some symptoms of major depression are present;[113] and adjustment disorder with depressed mood, which denotes low mood resulting from a psychological response to an identifiable event or stressor.[114]		The DSM-IV-TR recognizes five further subtypes of MDD, called specifiers, in addition to noting the length, severity and presence of psychotic features:		In 2016, the United States Preventive Services Task Force (USPSTF) recommended screening in the adult populations with evidence that it increases the detection of people with depression and with proper treatment improves outcomes.[8] They recommend screening in those between the age of 12 to 18 as well.[9]		A Cochrane review from 2005 found screening programs do not significantly improve detection rates, treatment, or outcome.[10]		To confer major depressive disorder as the most likely diagnosis, other potential diagnoses must be considered, including dysthymia, adjustment disorder with depressed mood, or bipolar disorder. Dysthymia is a chronic, milder mood disturbance in which a person reports a low mood almost daily over a span of at least two years. The symptoms are not as severe as those for major depression, although people with dysthymia are vulnerable to secondary episodes of major depression (sometimes referred to as double depression).[110] Adjustment disorder with depressed mood is a mood disturbance appearing as a psychological response to an identifiable event or stressor, in which the resulting emotional or behavioral symptoms are significant but do not meet the criteria for a major depressive episode.[114] Bipolar disorder, also known as manic–depressive disorder, is a condition in which depressive phases alternate with periods of mania or hypomania. Although depression is currently categorized as a separate disorder, there is ongoing debate because individuals diagnosed with major depression often experience some hypomanic symptoms, indicating a mood disorder continuum.[120] Further differential diagnoses involve chronic fatigue syndrome.[121]		Other disorders need to be ruled out before diagnosing major depressive disorder. They include depressions due to physical illness, medications, and substance abuse. Depression due to physical illness is diagnosed as a Mood disorder due to a general medical condition. This condition is determined based on history, laboratory findings, or physical examination. When the depression is caused by a medication, drug of abuse, or exposure to a toxin, it is then diagnosed as a specific mood disorder (previously called Substance-induced mood disorder in the DSM-IV-TR).[3]		Preventative efforts may result in decreases in rates of the condition of between 22 and 38%.[122] Eating large amounts of fish may also reduce the risk.[123]		Behavioral interventions, such as interpersonal therapy and cognitive-behavioral therapy, are effective at preventing new onset depression.[122][124][125] Because such interventions appear to be most effective when delivered to individuals or small groups, it has been suggested that they may be able to reach their large target audience most efficiently through the Internet.[126]		However, an earlier meta-analysis found preventive programs with a competence-enhancing component to be superior to behavior-oriented programs overall, and found behavioral programs to be particularly unhelpful for older people, for whom social support programs were uniquely beneficial. In addition, the programs that best prevented depression comprised more than eight sessions, each lasting between 60 and 90 minutes, were provided by a combination of lay and professional workers, had a high-quality research design, reported attrition rates, and had a well-defined intervention.[127]		The Netherlands mental health care system provides preventive interventions, such as the "Coping with Depression" course (CWD) for people with sub-threshold depression. The course is claimed to be the most successful of psychoeducational interventions for the treatment and prevention of depression (both for its adaptability to various populations and its results), with a risk reduction of 38% in major depression and an efficacy as a treatment comparing favorably to other psychotherapies.[124][128]		The three most common treatments for depression are psychotherapy, medication, and electroconvulsive therapy. Psychotherapy is the treatment of choice (over medication) for people under 18. The UK National Institute for Health and Care Excellence (NICE) 2004 guidelines indicate that antidepressants should not be used for the initial treatment of mild depression, because the risk-benefit ratio is poor. The guidelines recommend that antidepressants treatment in combination with psychosocial interventions should be considered for:		The guidelines further note that antidepressant treatment should be continued for at least six months to reduce the risk of relapse, and that SSRIs are better tolerated than tricyclic antidepressants.[129]		American Psychiatric Association treatment guidelines recommend that initial treatment should be individually tailored based on factors including severity of symptoms, co-existing disorders, prior treatment experience, and patient preference. Options may include pharmacotherapy, psychotherapy, exercise, electroconvulsive therapy (ECT), transcranial magnetic stimulation (TMS) or light therapy. Antidepressant medication is recommended as an initial treatment choice in people with mild, moderate, or severe major depression, and should be given to all patients with severe depression unless ECT is planned.[130]		Treatment options are much more limited in developing countries, where access to mental health staff, medication, and psychotherapy is often difficult. Development of mental health services is minimal in many countries; depression is viewed as a phenomenon of the developed world despite evidence to the contrary, and not as an inherently life-threatening condition.[131] A 2014 Cochrane review found insufficient evidence to determine the effectiveness of psychological versus medical therapy in children.[132]		Physical exercise is recommended for management of mild depression,[133] and has a moderate effect on symptoms.[134] Exercise has also been found to be effective for (unipolar) major depression.[135] It is equivalent to the use of medications or psychological therapies in most people.[134] In older people it does appear to decrease depression.[136] Exercise may be recommended to people who are willing, motivated, and physically healthy enough to participate in an exercise program as treatment.[135]		There is a small amount of evidence that skipping a night's sleep may improve depressive symptoms, with the effects usually showing up within a day. This effect is usually temporary. Besides sleepiness, this method can cause a side effect of mania or hypomania.[137]		In observational studies smoking cessation has benefits in depression as large as or larger than those of medications.[138]		Besides exercise, sleep and diet may play a role in depression, and interventions in these areas may be an effective add on to conventional methods.[139]		Psychotherapy can be delivered, to individuals, groups, or families by mental health professionals. A 2015 review found that cognitive behavioral therapy appears to be similar to antidepressant medication in terms of effect.[140] A 2012 review found psychotherapy to be better than no treatment but not other treatments.[141] With more complex and chronic forms of depression, a combination of medication and psychotherapy may be used.[142][143] A 2014 Cochrane review found that work-directed interventions combined with clinical interventions helped to reduce sick days taken by people with depression.[144]		Psychotherapy has been shown to be effective in older people.[145][146] Successful psychotherapy appears to reduce the recurrence of depression even after it has been terminated or replaced by occasional booster sessions.		Cognitive behavioral therapy (CBT) currently has the most research evidence for the treatment of depression in children and adolescents, and CBT and interpersonal psychotherapy (IPT) are preferred therapies for adolescent depression.[147] In people under 18, according to the National Institute for Health and Clinical Excellence, medication should be offered only in conjunction with a psychological therapy, such as CBT, interpersonal therapy, or family therapy.[148] Cognitive behavioral therapy has also been shown to reduce the number of sick days taken by people with depression, when used in conjunction with primary care.[144]		The most-studied form of psychotherapy for depression is CBT, which teaches clients to challenge self-defeating, but enduring ways of thinking (cognitions) and change counter-productive behaviors. Research beginning in the mid-1990s suggested that CBT could perform as well as or better than antidepressants in patients with moderate to severe depression.[149][150] CBT may be effective in depressed adolescents,[151] although its effects on severe episodes are not definitively known.[152] Several variables predict success for cognitive behavioral therapy in adolescents: higher levels of rational thoughts, less hopelessness, fewer negative thoughts, and fewer cognitive distortions.[153] CBT is particularly beneficial in preventing relapse.[154][155]		Cognitive behavioral therapy and occupational programs (including modification of work activities and assistance) have been shown to be effective in reducing sick days taken by workers with depression.[156]		Several variants of cognitive behavior therapy have been used in those with depression, the most notable being rational emotive behavior therapy,[157] and mindfulness-based cognitive therapy.[158] Mindfulness based stress reduction programs may reduce depression symptoms.[159][160] Mindfulness programs also appear to be a promising intervention in youth.[161]		Psychoanalysis is a school of thought, founded by Sigmund Freud, which emphasizes the resolution of unconscious mental conflicts.[162] Psychoanalytic techniques are used by some practitioners to treat clients presenting with major depression.[163] A more widely practiced therapy, called psychodynamic psychotherapy, is in the tradition of psychoanalysis but less intensive, meeting once or twice a week. It also tends to focus more on the person's immediate problems, and has an additional social and interpersonal focus.[164] In a meta-analysis of three controlled trials of Short Psychodynamic Supportive Psychotherapy, this modification was found to be as effective as medication for mild to moderate depression.[165]		Conflicting results have arisen from studies that look at the effectiveness of antidepressants in people with acute, mild to moderate depression. Stronger evidence supports the usefulness of antidepressants in the treatment of depression that is chronic (dysthymia) or severe.		While small benefits were found, researchers Irving Kirsch and Thomas Moore state they may be due to issues with the trials rather than a true effect of the medication.[166] In a later publication, Kirsch concluded that the overall effect of new-generation antidepressant medication is below recommended criteria for clinical significance.[12] Similar results were obtained in a meta analysis by Fornier.[11]		A review commissioned by the National Institute for Health and Care Excellence concluded that there is strong evidence that SSRIs have greater efficacy than placebo on achieving a 50% reduction in depression scores in moderate and severe major depression, and that there is some evidence for a similar effect in mild depression.[167] Similarly, a Cochrane systematic review of clinical trials of the generic tricyclic antidepressant amitriptyline concluded that there is strong evidence that its efficacy is superior to placebo.[168]		In 2014 the U.S. FDA published a systematic review of all antidepressant maintenance trials submitted to the agency between 1985 and 2012. The authors concluded that maintenance treatment reduced the risk of relapse by 52% compared to placebo, and that this effect was primarily due to recurrent depression in the placebo group rather than a drug withdrawal effect.[11]		To find the most effective antidepressant medication with minimal side-effects, the dosages can be adjusted, and if necessary, combinations of different classes of antidepressants can be tried. Response rates to the first antidepressant administered range from 50–75%, and it can take at least six to eight weeks from the start of medication to remission.[169] Antidepressant medication treatment is usually continued for 16 to 20 weeks after remission, to minimize the chance of recurrence,[169] and even up to one year of continuation is recommended.[170] People with chronic depression may need to take medication indefinitely to avoid relapse.[19]		Selective serotonin reuptake inhibitors (SSRIs) are the primary medications prescribed, owing to their relatively mild side-effects, and because they are less toxic in overdose than other antidepressants.[171] People who do not respond to one SSRI can be switched to another antidepressant, and this results in improvement in almost 50% of cases.[172] Another option is to switch to the atypical antidepressant bupropion.[173] Venlafaxine, an antidepressant with a different mechanism of action, may be modestly more effective than SSRIs.[174] However, venlafaxine is not recommended in the UK as a first-line treatment because of evidence suggesting its risks may outweigh benefits,[175] and it is specifically discouraged in children and adolescents.[176][177]		For child and adolescent depression, fluoxetine is recommended if medication are used.[178] Fluoxetine; however, appears to have only slight benefit in children,[178][179] while other antidepressants have not been shown to be effective.[180] There is also insufficient evidence to determine effectiveness in those with depression complicated by dementia.[181] Any antidepressant can cause low serum sodium levels (also called hyponatremia);[182] nevertheless, it has been reported more often with SSRIs.[171] It is not uncommon for SSRIs to cause or worsen insomnia; the sedating antidepressant mirtazapine can be used in such cases.[183][184]		Irreversible monoamine oxidase inhibitors, an older class of antidepressants, have been plagued by potentially life-threatening dietary and drug interactions. They are still used only rarely, although newer and better-tolerated agents of this class have been developed.[185] The safety profile is different with reversible monoamine oxidase inhibitors such as moclobemide where the risk of serious dietary interactions is negligible and dietary restrictions are less strict.[186]		For children, adolescents, and probably young adults between 18 and 24 years old, there is a higher risk of both suicidal ideations and suicidal behavior in those treated with SSRIs.[187][188] For adults, it is unclear whether SSRIs affect the risk of suicidality. One review found no connection;[189] another an increased risk;[190] and a third no risk in those 25–65 years old and a decrease risk in those more than 65.[191] A black box warning was introduced in the United States in 2007 on SSRI and other antidepressant medications due to increased risk of suicide in patients younger than 24 years old.[192] Similar precautionary notice revisions were implemented by the Japanese Ministry of Health.[193]		There is some evidence that omega-3 fatty acids fish oil supplements containing high levels of eicosapentaenoic acid (EPA) to docosahexaenoic acid (DHA) are effective in the treatment of, but not the prevention of major depression.[194] However, a Cochrane review determined there was insufficient high quality evidence to suggest Omega-3 fatty acids were effective in depression.[195] There is limited evidence that vitamin D supplementation is of value in alleviating the symptoms of depression in individuals who are vitamin D deficient.[196] There is some preliminary evidence that COX-2 inhibitors have a beneficial effect on major depression.[197] Lithium appears effective at lowering the risk of suicide in those with bipolar disorder and unipolar depression to nearly the same levels as the general population.[198] There is a narrow range of effective and safe dosages of lithium thus close monitoring may be needed.[199] Low-dose thyroid hormone may be added to existing antidepressants to treat persistent depression symptoms in people who have tried multiple courses of medication.[200] Limited evidence suggests stimulants such as amphetamine and modafinil may be effective in the short term, or as add on therapy.[201][202]		Electroconvulsive therapy (ECT) is a standard psychiatric treatment in which seizures are electrically induced in patients to provide relief from psychiatric illnesses.[203]:1880 ECT is used with informed consent[204] as a last line of intervention for major depressive disorder.[205]		A round of ECT is effective for about 50% of people with treatment-resistant major depressive disorder, whether it is unipolar or bipolar.[206] Follow-up treatment is still poorly studied, but about half of people who respond relapse within twelve months.[207]		Aside from effects in the brain, the general physical risks of ECT are similar to those of brief general anesthesia.[208]:259 Immediately following treatment, the most common adverse effects are confusion and memory loss.[205][209] ECT is considered one of the least harmful treatment options available for severely depressed pregnant women.[210]		A usual course of ECT involves multiple administrations, typically given two or three times per week until the patient is no longer suffering symptoms. ECT is administered under anesthetic with a muscle relaxant.[211] Electroconvulsive therapy can differ in its application in three ways: electrode placement, frequency of treatments, and the electrical waveform of the stimulus. These three forms of application have significant differences in both adverse side effects and symptom remission. After treatment, drug therapy is usually continued, and some patients receive maintenance ECT.[205]		ECT appears to work in the short term via an anticonvulsant effect mostly in the frontal lobes, and longer term via neurotrophic effects primarily in the medial temporal lobe.[212]		Transcranial magnetic stimulation (TMS) or deep transcranial magnetic stimulation is a noninvasive method used to stimulate small regions of the brain.[213] TMS was approved by the FDA for treatment-resistant major depressive disorder in 2008[214] and as of 2014 evidence supports that it is probably effective.[215] The American Psychiatric Association[216] the Canadian Network for Mood and Anxiety Disorders,[217] and the Royal Australia and New Zealand College of Psychiatrists have endorsed rTMS for trMDD.[218]		Bright light therapy reduces depression symptom severity, with benefit was found for both seasonal affective disorder and for nonseasonal depression, and an effect similar to those for conventional antidepressants. For non-seasonal depression, adding light therapy to the standard antidepressant treatment was not effective.[219] For non-seasonal depression where light was used mostly in combination with antidepressants or wake therapy a moderate effect was found, with response better than control treatment in high-quality studies, in studies that applied morning light treatment, and with people who respond to total or partial sleep deprivation.[220] Both analyses noted poor quality, short duration, and small size of most of the reviewed studies. There is insufficient evidence for Reiki[221] and dance movement therapy in depression.[222]		Major depressive episodes often resolve over time whether or not they are treated. Outpatients on a waiting list show a 10–15% reduction in symptoms within a few months, with approximately 20% no longer meeting the full criteria for a depressive disorder.[223] The median duration of an episode has been estimated to be 23 weeks, with the highest rate of recovery in the first three months.[224]		Studies have shown that 80% of those suffering from their first major depressive episode will suffer from at least 1 more during their life,[225] with a lifetime average of 4 episodes.[226] Other general population studies indicate that around half those who have an episode recover (whether treated or not) and remain well, while the other half will have at least one more, and around 15% of those experience chronic recurrence.[227] Studies recruiting from selective inpatient sources suggest lower recovery and higher chronicity, while studies of mostly outpatients show that nearly all recover, with a median episode duration of 11 months. Around 90% of those with severe or psychotic depression, most of whom also meet criteria for other mental disorders, experience recurrence.[228][229]		Recurrence is more likely if symptoms have not fully resolved with treatment. Current guidelines recommend continuing antidepressants for four to six months after remission to prevent relapse. Evidence from many randomized controlled trials indicates continuing antidepressant medications after recovery can reduce the chance of relapse by 70% (41% on placebo vs. 18% on antidepressant). The preventive effect probably lasts for at least the first 36 months of use.[230]		Those people experiencing repeated episodes of depression require ongoing treatment in order to prevent more severe, long-term depression. In some cases, people must take medications for long periods of time or for the rest of their lives.[231]		Cases when outcome is poor are associated with inappropriate treatment, severe initial symptoms that may include psychosis, early age of onset, more previous episodes, incomplete recovery after 1 year, pre-existing severe mental or medical disorder, and family dysfunction as well.[232]		Depressed individuals have a shorter life expectancy than those without depression, in part because depressed patients are at risk of dying by suicide.[233] However, they also have a higher rate of dying from other causes,[234] being more susceptible to medical conditions such as heart disease.[235] Up to 60% of people who die by suicide have a mood disorder such as major depression, and the risk is especially high if a person has a marked sense of hopelessness or has both depression and borderline personality disorder.[236] The lifetime risk of suicide associated with a diagnosis of major depression in the US is estimated at 3.4%, which averages two highly disparate figures of almost 7% for men and 1% for women[237] (although suicide attempts are more frequent in women).[238] The estimate is substantially lower than a previously accepted figure of 15%, which had been derived from older studies of hospitalized patients.[239]		Depression is often associated with unemployment and poverty.[240] Major depression is currently the leading cause of disease burden in North America and other high-income countries, and the fourth-leading cause worldwide. In the year 2030, it is predicted to be the second-leading cause of disease burden worldwide after HIV, according to the World Health Organization.[241] Delay or failure in seeking treatment after relapse, and the failure of health professionals to provide treatment, are two barriers to reducing disability.[242]		Major depressive disorder affects approximately 216 million people in 2015 (3% of the global population).[5] The percentage of people who are affected at one point in their life varies from 7% in Japan to 21% in France.[4] In most countries the number of people who have depression during their lives falls within an 8–18% range.[4] In North America, the probability of having a major depressive episode within a year-long period is 3–5% for males and 8–10% for females.[244][245] Major depression to be about twice as common in women as in men, although it is unclear why this is so, and whether factors unaccounted for are contributing to this.[246] The relative increase in occurrence is related to pubertal development rather than chronological age, reaches adult ratios between the ages of 15 and 18, and appears associated with psychosocial more than hormonal factors.[246] Depression is a major cause of disability worldwide.[247]		People are most likely to develop their first depressive episode between the ages of 30 and 40, and there is a second, smaller peak of incidence between ages 50 and 60.[248] The risk of major depression is increased with neurological conditions such as stroke, Parkinson's disease, or multiple sclerosis, and during the first year after childbirth.[249] It is also more common after cardiovascular illnesses, and is related more to a poor outcome than to a better one.[235][250] Studies conflict on the prevalence of depression in the elderly, but most data suggest there is a reduction in this age group.[251] Depressive disorders are more common to observe in urban than in rural population and the prevalence is in groups with stronger socioeconomic factors i.e. homelessness.[252]		The Ancient Greek physician Hippocrates described a syndrome of melancholia as a distinct disease with particular mental and physical symptoms; he characterized all "fears and despondencies, if they last a long time" as being symptomatic of the ailment.[253] It was a similar but far broader concept than today's depression; prominence was given to a clustering of the symptoms of sadness, dejection, and despondency, and often fear, anger, delusions and obsessions were included.[254]		The term depression itself was derived from the Latin verb deprimere, "to press down".[255] From the 14th century, "to depress" meant to subjugate or to bring down in spirits. It was used in 1665 in English author Richard Baker's Chronicle to refer to someone having "a great depression of spirit", and by English author Samuel Johnson in a similar sense in 1753.[256] The term also came into use in physiology and economics. An early usage referring to a psychiatric symptom was by French psychiatrist Louis Delasiauve in 1856, and by the 1860s it was appearing in medical dictionaries to refer to a physiological and metaphorical lowering of emotional function.[257] Since Aristotle, melancholia had been associated with men of learning and intellectual brilliance, a hazard of contemplation and creativity. The newer concept abandoned these associations and through the 19th century, became more associated with women.[254]		Although melancholia remained the dominant diagnostic term, depression gained increasing currency in medical treatises and was a synonym by the end of the century; German psychiatrist Emil Kraepelin may have been the first to use it as the overarching term, referring to different kinds of melancholia as depressive states.[258]		Sigmund Freud likened the state of melancholia to mourning in his 1917 paper Mourning and Melancholia. He theorized that objective loss, such as the loss of a valued relationship through death or a romantic break-up, results in subjective loss as well; the depressed individual has identified with the object of affection through an unconscious, narcissistic process called the libidinal cathexis of the ego. Such loss results in severe melancholic symptoms more profound than mourning; not only is the outside world viewed negatively but the ego itself is compromised.[259] The patient's decline of self-perception is revealed in his belief of his own blame, inferiority, and unworthiness.[260] He also emphasized early life experiences as a predisposing factor.[254] Adolf Meyer put forward a mixed social and biological framework emphasizing reactions in the context of an individual's life, and argued that the term depression should be used instead of melancholia.[261] The first version of the DSM (DSM-I, 1952) contained depressive reaction and the DSM-II (1968) depressive neurosis, defined as an excessive reaction to internal conflict or an identifiable event, and also included a depressive type of manic-depressive psychosis within Major affective disorders.[262]		In the mid-20th century, researchers theorized that depression was caused by a chemical imbalance in neurotransmitters in the brain, a theory based on observations made in the 1950s of the effects of reserpine and isoniazid in altering monoamine neurotransmitter levels and affecting depressive symptoms.[263] The chemical imbalance theory has never been proven.[264]		The term "unipolar" (along with the related term "bipolar") was coined by the neurologist and psychiatrist Karl Kleist, and subsequently used by his disciples Edda Neele and Karl Leonhard.[265]		The term Major depressive disorder was introduced by a group of US clinicians in the mid-1970s as part of proposals for diagnostic criteria based on patterns of symptoms (called the "Research Diagnostic Criteria", building on earlier Feighner Criteria),[266] and was incorporated into the DSM-III in 1980.[267] To maintain consistency the ICD-10 used the same criteria, with only minor alterations, but using the DSM diagnostic threshold to mark a mild depressive episode, adding higher threshold categories for moderate and severe episodes.[100][267] The ancient idea of melancholia still survives in the notion of a melancholic subtype.		The new definitions of depression were widely accepted, albeit with some conflicting findings and views. There have been some continued empirically based arguments for a return to the diagnosis of melancholia.[268][269] There has been some criticism of the expansion of coverage of the diagnosis, related to the development and promotion of antidepressants and the biological model since the late 1950s.[270]		The term "depression" is used in a number of different ways. It is often used to mean this syndrome but may refer to other mood disorders or simply to a low mood. People's conceptualizations of depression vary widely, both within and among cultures. "Because of the lack of scientific certainty," one commentator has observed, "the debate over depression turns on questions of language. What we call it—'disease,' 'disorder,' 'state of mind'—affects how we view, diagnose, and treat it."[272] There are cultural differences in the extent to which serious depression is considered an illness requiring personal professional treatment, or is an indicator of something else, such as the need to address social or moral problems, the result of biological imbalances, or a reflection of individual differences in the understanding of distress that may reinforce feelings of powerlessness, and emotional struggle.[273][274]		The diagnosis is less common in some countries, such as China. It has been argued that the Chinese traditionally deny or somatize emotional depression (although since the early 1980s, the Chinese denial of depression may have modified).[275] Alternatively, it may be that Western cultures reframe and elevate some expressions of human distress to disorder status. Australian professor Gordon Parker and others have argued that the Western concept of depression "medicalizes" sadness or misery.[276][277] Similarly, Hungarian-American psychiatrist Thomas Szasz and others argue that depression is a metaphorical illness that is inappropriately regarded as an actual disease.[278] There has also been concern that the DSM, as well as the field of descriptive psychiatry that employs it, tends to reify abstract phenomena such as depression, which may in fact be social constructs.[279] American archetypal psychologist James Hillman writes that depression can be healthy for the soul, insofar as "it brings refuge, limitation, focus, gravity, weight, and humble powerlessness."[280] Hillman argues that therapeutic attempts to eliminate depression echo the Christian theme of resurrection, but have the unfortunate effect of demonizing a soulful state of being.		Historical figures were often reluctant to discuss or seek treatment for depression due to social stigma about the condition, or due to ignorance of diagnosis or treatments. Nevertheless, analysis or interpretation of letters, journals, artwork, writings, or statements of family and friends of some historical personalities has led to the presumption that they may have had some form of depression. People who may have had depression include English author Mary Shelley,[281] American-British writer Henry James,[282] and American president Abraham Lincoln.[283] Some well-known contemporary people with possible depression include Canadian songwriter Leonard Cohen[284] and American playwright and novelist Tennessee Williams.[285] Some pioneering psychologists, such as Americans William James[286][287] and John B. Watson,[288] dealt with their own depression.		There has been a continuing discussion of whether neurological disorders and mood disorders may be linked to creativity, a discussion that goes back to Aristotelian times.[289][290] British literature gives many examples of reflections on depression.[291] English philosopher John Stuart Mill experienced a several-months-long period of what he called "a dull state of nerves", when one is "unsusceptible to enjoyment or pleasurable excitement; one of those moods when what is pleasure at other times, becomes insipid or indifferent". He quoted English poet Samuel Taylor Coleridge's "Dejection" as a perfect description of his case: "A grief without a pang, void, dark and drear, / A drowsy, stifled, unimpassioned grief, / Which finds no natural outlet or relief / In word, or sigh, or tear."[292][293] English writer Samuel Johnson used the term "the black dog" in the 1780s to describe his own depression,[294] and it was subsequently popularized by depression sufferer former British Prime Minister Sir Winston Churchill.[294]		Social stigma of major depression is widespread, and contact with mental health services reduces this only slightly. Public opinions on treatment differ markedly to those of health professionals; alternative treatments are held to be more helpful than pharmacological ones, which are viewed poorly.[295] In the UK, the Royal College of Psychiatrists and the Royal College of General Practitioners conducted a joint Five-year Defeat Depression campaign to educate and reduce stigma from 1992 to 1996;[296] a MORI study conducted afterwards showed a small positive change in public attitudes to depression and treatment.[297]		Trials are looking at the effects of botulinum toxins on depression. The idea is that the drug is used to make the person look less frowning and that this stops the negative facial feedback from the face.[298] In 2015 it turned out, however, that the partly positive effects that had been observed until then could have been placebo effects.[299]		MDD has been studied by taking MRI scans of patients with depression have revealed a number of differences in brain structure compared to those who are not depressed. Meta-analyses of neuroimaging studies in major depression reported that, compared to controls, depressed patients had increased volume of the lateral ventricles and adrenal gland and smaller volumes of the basal ganglia, thalamus, hippocampus, and frontal lobe (including the orbitofrontal cortex and gyrus rectus).[300][301] Hyperintensities have been associated with patients with a late age of onset, and have led to the development of the theory of vascular depression.[302]		Depression is especially common among those over 65 years of age and increases in frequency with age beyond this age.[303] In addition the risk of depression increases in relation to the age and frailty of the individual.[303] Depression is one the most important factors which negatively impact quality of life in adults as well as the elderly.[303] Both symptoms and treatment among the elderly differ from those of the rest of the adult populations.[303]		As with many other diseases it is common among the elderly not to present classical depressive symptoms.[303] Diagnosis and treatment is further complicated in that the elderly are often simultaneously treated with a number of other drugs, and often have other concurrent diseases.[303] Treatment differs in that studies of SSRI-drugs have shown lesser and often inadequate effect among the elderly, while other drugs with more clear effects have adverse effects which can be especially difficult to handle among the elderly.[303] Duloxetine is an SNRI-drug with documented effect on recurring depression among the elderly, but has adverse effects in form of dizziness, dryness of the mouth, diarrhea, and constipation.[303]		Problem solving therapy was as of 2015 the only psychological therapy with proven effect, and can be likened to a simpler form of cognitive behavioral therapy.[303] However, elderly with depression are seldom offered any psychological treatment, and the evidence surrounding which other treatments are effective is incomplete.[303] Electroconvulsive therapy (ECT or electric-shock therapy) has been used as treatment of the elderly, and register-studies suggest it is effective although less so among the elderly than among the rest of the adult population.[303]		The risks involved with treatment of depression among the elderly as opposed to benefits is not entirely clear.[303] Awaiting more evidence on how depression-treatment among the elderly is best designed it is important to follow up treatment results, and to reconsider changing treatments if it does not help.[303]		Models of depression in animals for the purpose of study include iatrogenic depression models (such as drug induced), forced swim tests, tail suspension test, and learned helplessness models. Criteria frequently used to assess depression in animals include expression of despair, neurovegetative changes, and anhedonia, as many other depressive criteria are untestable in animals such as guilt and suicidality.[304]								
Walking (also known as ambulation) is one of the main gaits of locomotion among legged animals. Walking is typically slower than running and other gaits. Walking is defined by an 'inverted pendulum' gait in which the body vaults over the stiff limb or limbs with each step. This applies regardless of the number of limbs—even arthropods, with six, eight or more limbs, walk.						The word walk is descended from the Old English wealcan "to roll". In humans and other bipeds, walking is generally distinguished from running in that only one foot at a time leaves contact with the ground and there is a period of double-support. In contrast, running begins when both feet are off the ground with each step. This distinction has the status of a formal requirement in competitive walking events. For quadrupedal species, there are numerous gaits which may be termed walking or running, and distinctions based upon the presence or absence of a suspended phase or the number of feet in contact any time do not yield mechanically correct classification.[1] The most effective method to distinguish walking from running is to measure the height of a person's centre of mass using motion capture or a force plate at midstance. During walking, the centre of mass reaches a maximum height at midstance, while during running, it is then at a minimum. This distinction, however, only holds true for locomotion over level or approximately level ground. For walking up grades above 9%, this distinction no longer holds for some individuals. Definitions based on the percentage of the stride during which a foot is in contact with the ground (averaged across all feet) of greater than 50% contact corresponds well with identification of 'inverted pendulum' mechanics and are indicative of walking for animals with any number of limbs, although this definition is incomplete.[1] Running humans and animals may have contact periods greater than 50% of a gait cycle when rounding corners, running uphill or carrying loads.		Speed is another factor that distinguishes walking from running. Although walking speeds can vary greatly depending on many factors such as height, weight, age, terrain, surface, load, culture, effort, and fitness, the average human walking speed is about 5.0 kilometres per hour (km/h), or about 3.1 miles per hour (mph). Specific studies have found pedestrian walking speeds ranging from 4.51 kilometres per hour (2.80 mph) to 4.75 kilometres per hour (2.95 mph) for older individuals and from 5.32 kilometres per hour (3.31 mph) to 5.43 kilometres per hour (3.37 mph) for younger individuals;[2][3] a brisk walking speed can be around 6.5 kilometres per hour (4.0 mph).[4] Champion racewalkers can average more than 14 kilometres per hour (8.7 mph) over a distance of 20 kilometres (12 mi).		An average human child achieves independent walking ability at around 11 months old.[5]		Regular, brisk exercise of any kind can improve confidence, stamina, energy, weight control and life expectancy and reduce stress.[citation needed] It can also reduce the risk of coronary heart disease, strokes, diabetes, high blood pressure, bowel cancer and osteoporosis.[citation needed] Scientific studies have also shown that walking, besides its physical benefits, is also beneficial for the mind, improving memory skills, learning ability, concentration and abstract reasoning,[citation needed] as well as ameliorating spirits.[clarification needed] Sustained walking sessions for a minimum period of thirty to sixty minutes a day, five days a week, with the correct walking posture,[6] reduce health risks and have various overall health benefits, such as reducing the chances of cancer, type 2 diabetes, heart disease, anxiety disorder and depression.[7] Life expectancy is also increased even for individuals suffering from obesity or high blood pressure. Walking also improves bone health, especially strengthening the hip bone, and lowering the harmful low-density lipoprotein (LDL) cholesterol, and raising the useful high-density lipoprotein (HDL) cholesterol.[8] Studies have found that walking may also help prevent dementia and Alzheimer's.[9]		The Centers for Disease Control and Prevention's fact sheet on the "Relationship of Walking to Mortality Among U.S. Adults with Diabetes" states that those with diabetes who walked for 2 or more hours a week lowered their mortality rate from all causes by 39 per cent. "Walking lengthened the life of people with diabetes regardless of age, sex, race, body mass index, length of time since diagnosis, and presence of complications or functional limitations."[10] It has been suggested that there is a relationship between the speed of walking and health, and that the best results are obtained with a speed of more than 2.5 mph (4 km/h).[11]		Governments now recognize the benefits of walking for mental and physical health and are actively encouraging it. This growing emphasis on walking has arisen because people walk less nowadays than previously. In the UK, a Department of Transport report[12] found that between 1995/97 and 2005 the average number of walk trips per person fell by 16%, from 292 to 245 per year. Many professionals in local authorities and the NHS are employed to halt this decline by ensuring that the built environment allows people to walk and that there are walking opportunities available to them. Professionals working to encourage walking come mainly from six sectors: health, transport, environment, schools, sport and recreation, and urban design.		One programme to encourage walking is "The Walking the Way to Health Initiative", organized by the British walkers association The Ramblers, which is the largest volunteer led walking scheme in the United Kingdom. Volunteers are trained to lead free Health Walks from community venues such as libraries and doctors' surgeries. The scheme has trained over 35,000 volunteers and have over 500 schemes operating across the UK, with thousands of people walking every week.[13] A new organization called "Walk England" launched a web site in June 2008 to provide these professionals with evidence, advice and examples of success stories of how to encourage communities to walk more. The site has a social networking aspect to allow professionals and the public to ask questions, post news and events and communicate with others in their area about walking, as well as a "walk now" option to find out what walks are available in each region. Similar organizations exist in other countries and recently a "Walking Summit" was held in the United States. This "assembl[ed] thought-leaders and influencers from business, urban planning and real estate, [along with] physicians and public health officials," and others, to discuss how to make American cities and communities places where "people can and want to walk".[14]		It is theorized that "walking" among tetrapods originated underwater with air-breathing fish that could "walk" underwater, giving rise to the plethora of land-dwelling life that walk on four or two limbs.[15] While terrestrial tetrapods are theorised to have a single origin, arthropods and their relatives are thought to have independently evolved walking several times, specifically in insects, myriapods, chelicerates, tardigrades, onychophorans, and crustaceans.[16]		Judging from footprints discovered on a former shore in Kenya, it is thought possible that ancestors of modern humans were walking in ways very similar to the present activity as many as 1.5 million years ago.[17][18]		Human walking is accomplished with a strategy called the double pendulum. During forward motion, the leg that leaves the ground swings forward from the hip. This sweep is the first pendulum. Then the leg strikes the ground with the heel and rolls through to the toe in a motion described as an inverted pendulum. The motion of the two legs is coordinated so that one foot or the other is always in contact with the ground. The process of walking recovers approximately sixty per cent of the energy used due to pendulum dynamics and ground reaction force.[25][26]		Walking differs from a running gait in a number of ways. The most obvious is that during walking one leg always stays on the ground while the other is swinging. In running there is typically a ballistic phase where the runner is airborne with both feet in the air (for bipedals).		Another difference concerns the movement of the centre of mass of the body. In walking the body "vaults" over the leg on the ground, raising the centre of mass to its highest point as the leg passes the vertical, and dropping it to the lowest as the legs are spread apart. Essentially kinetic energy of forward motion is constantly being traded for a rise in potential energy. This is reversed in running where the centre of mass is at its lowest as the leg is vertical. This is because the impact of landing from the ballistic phase is absorbed by bending the leg and consequently storing energy in muscles and tendons. In running there is a conversion between kinetic, potential, and elastic energy.		There is an absolute limit on an individual's speed of walking (without special techniques such as those employed in speed walking) due to the upwards acceleration of the centre of mass during a stride – if it's greater than the acceleration due to gravity the person will become airborne as they vault over the leg on the ground. Typically however, animals switch to a run at a lower speed than this due to energy efficiencies.		Many people enjoy walking as a recreation in the mainly urban modern world, and it is one of the best forms of exercise.[27] For some, walking is a way to enjoy nature and the outdoors; and for others the physical, sporting and endurance aspect is more important.		There are a variety of different kinds of walking, including bushwalking, racewalking, beach walking, hillwalking, volksmarching, Nordic walking, trekking and hiking. Some people prefer to walk indoors on a treadmill, or in a gym, and fitness walkers and others may use a pedometer to count their steps. Hiking is the usual word used in Canada, the United States and South Africa for long vigorous walks; similar walks are called tramps in New Zealand, or hill walking or just walking in Australia, the UK and the Irish Republic. Australians also bushwalk. In English-speaking parts of North America the term walking is used for short walks, especially in towns and cities. Snow shoeing is walking in snow; a slightly different gait is required compared with regular walking.		In terms of tourism the possibilities range from guided walking tours in cities, to organized trekking holidays in the Himalayas. In the UK the term walking tour also refers to a multi-day walk or hike undertaken by a group or individual. Well-organized systems of trails exist in many other European counties, as well as Canada, United States, New Zealand, and Nepal. Systems of lengthy waymarked walking trails now stretch across Europe from Norway to Turkey, Portugal to Cyprus.[28] Many also walk the traditional pilgrim routes, of which the most famous is El Camino de Santiago, The Way of St. James.		Numerous walking festivals and other walking events take place each year in many countries. The world's largest multi-day walking event is the International Four Days Marches Nijmegen in the Netherlands. The "Vierdaagse" (Dutch for "Four day Event") is an annual walk that has taken place since 1909; it has been based at Nijmegen since 1916. Depending on age group and category, walkers have to walk 30, 40 or 50 kilometers each day for four days.[citation needed] Originally a military event with a few civilians, it now is a mainly civilian event. Numbers have risen in recent years, with over 40,000 now taking part, including about 5,000 military personnel.[citation needed] Due to crowds on the route, since 2004 the organizers have limited the number of participants. In the U.S., there is the annual Labor Day walk on Mackinac Bridge, Michigan, which draws over 60,000 participants; it is the largest single-day walking event;[citation needed] while the Chesapeake Bay Bridge Walk in Maryland draws over 50,000 participants each year.[citation needed] There are also various walks organised as charity events, with walkers sponsored for a specific cause. These walks range in length from two miles (3 km) or five km to 50 miles (80 km). The MS Challenge Walk is an 80 km or 50 mile walk which raises money to fight multiple sclerosis, while walkers in the Oxfam Trailwalker cover 100 km or 60 miles.		In Britain, The Ramblers, a registered charity, is the largest organisation that looks after the interests of walkers, with some 139,000 members.[citation needed] Its "Get Walking Keep Walking" project provides free route guides, led walks, as well as information for people new to walking.[29] The Long Distance Walkers Association in the UK is for the more energetic walker, and organizes lengthy challenge hikes of 20 or even 50 miles (30 to 80 km) or more in a day. The LDWA's annual "Hundred" event, entailing walking 100 miles or 160 km in 48 hours, takes place each British Spring Bank Holiday weekend.[30]		There has been a recent focus among urban planners in some communities to create pedestrian-friendly areas and roads, allowing commuting, shopping and recreation to be done on foot. The concept of walkability has arisen as a measure of the degree to which an area is friendly to walking. Some communities are at least partially car-free, making them particularly supportive of walking and other modes of transportation. In the United States, the active living network is an example of a concerted effort to develop communities more friendly to walking and other physical activities.		An example of such efforts to make urban development more pedestrian friendly is the pedestrian village. This is a compact, pedestrian-oriented neighborhood or town, with a mixed-use village center, that follows the tenets of New Pedestrianism.[31][32] Shared-use lanes for pedestrians and those using bicycles, Segways, wheelchairs, and other small rolling conveyances that do not use internal combustion engines. Generally, these lanes are in front of the houses and businesses, and streets for motor vehicles are always at the rear. Some pedestrian villages might be nearly car-free with cars either hidden below the buildings or on the periphery of the village. Venice, Italy is essentially a pedestrian village with canals. The canal district in Venice, California, on the other hand, combines the front lane/rear street approach with canals and walkways, or just walkways.[31][33][34]		Walking is also considered to be a clear example of a sustainable mode of transport, especially suited for urban use and/or relatively shorter distances. Non-motorised transport modes such as walking, but also cycling, small-wheeled transport (skates, skateboards, push scooters and hand carts) or wheelchair travel are often key elements of successfully encouraging clean urban transport.[35] A large variety of case studies and good practices (from European cities and some worldwide examples) that promote and stimulate walking as a means of transportation in cities can be found at Eltis, Europe's portal for local transport.[36]		The development of specific rights of way with appropriate infrastructure can promote increased participation and enjoyment of walking. Examples of types of investment include pedestrian malls, and foreshoreways such as oceanways and also river walks.		The first purpose-built pedestrian street in Europe is the Lijnbaan in Rotterdam, opened in 1953. The first pedestrianised shopping centre in the United Kingdom was in Stevenage in 1959. A large number of European towns and cities have made part of their centres car-free since the early 1960s. These are often accompanied by car parks on the edge of the pedestrianised zone, and, in the larger cases, park and ride schemes. Central Copenhagen is one of the largest and oldest: It was converted from car traffic into pedestrian zone in 1962.		The first successful attempts at walking robots tended to have six legs. The number of legs was reduced as microprocessor technology advanced, and there are now a number of robots that can walk on two legs. One, for example, is ASIMO. Although robots have taken great strides in advancement, they still don't walk nearly as well as human beings as they often need to keep their knees bent permanently in order to improve stability.		In 2009, Japanese roboticist Tomotaka Takahashi developed a robot that can jump three inches off the ground. The robot, named Ropid, is capable of getting up, walking, running, and jumping.[37]		The walk is a four-beat gait that averages about 4 miles per hour (6.4 km/h). When walking, a horse's legs follow this sequence: left hind leg, left front leg, right hind leg, right front leg, in a regular 1-2-3-4 beat. At the walk, the horse will always have one foot raised and the other three feet on the ground, save for a brief moment when weight is being transferred from one foot to another. A horse moves its head and neck in a slight up and down motion that helps maintain balance.[38]		Ideally, the advancing rear hoof oversteps the spot where the previously advancing front hoof touched the ground. The more the rear hoof oversteps, the smoother and more comfortable the walk becomes. Individual horses and different breeds vary in the smoothness of their walk. However, a rider will almost always feel some degree of gentle side-to-side motion in the horse's hips as each hind leg reaches forward.		The fastest "walks" with a four-beat footfall pattern are actually the lateral forms of ambling gaits such as the running walk, singlefoot, and similar rapid but smooth intermediate speed gaits. If a horse begins to speed up and lose a regular four-beat cadence to its gait, the horse is no longer walking, but is beginning to either trot or pace.		Elephants can move both forwards and backwards, but cannot trot, jump, or gallop. They use only two gaits when moving on land, the walk and a faster gait similar to running.[39] In walking, the legs act as pendulums, with the hips and shoulders rising and falling while the foot is planted on the ground. With no "aerial phase", the fast gait does not meet all the criteria of running, although the elephant uses its legs much like other running animals, with the hips and shoulders falling and then rising while the feet are on the ground.[40] Fast-moving elephants appear to 'run' with their front legs, but 'walk' with their hind legs and can reach a top speed of 18 km/h (11 mph).[41] At this speed, most other quadrupeds are well into a gallop, even accounting for leg length.		Walking fish, sometimes called ambulatory fish, is a general term that refers to fish that are able to travel over land for extended periods of time. The term may also be used for some other cases of nonstandard fish locomotion, e.g., when describing fish "walking" along the sea floor, as the handfish or frogfish.		
An eating disorder is a mental disorder defined by abnormal eating habits that negatively affect a person's physical or mental health. They include binge eating disorder where people eat a large amount in a short period of time, anorexia nervosa where people eat very little and thus have a low body weight, bulimia nervosa where people eat a lot and then try to rid themselves of the food, pica where people eat non-food items, rumination disorder where people regurgitate food, avoidant/restrictive food intake disorder where people have a lack of interest in food, and a group of other specified feeding or eating disorders. Anxiety disorders, depression, and substance abuse are common among people with eating disorders.[2] These disorders do not include obesity.[1]		The cause of eating disorders is not clear.[3] Both biological and environmental factors appear to play a role.[2][3] Cultural idealization of thinness is believed to contribute.[3] Eating disorders affect about 12 percent of dancers.[4] Those who have experienced sexual abuse are also more likely to develop eating disorders.[5] Some disorders such as pica and rumination disorder occur more often in people with intellectual disabilities. Only one eating disorder can be diagnosed at a given time.[1]		Treatment can be effective for many eating disorders. This typically involves counselling, a proper diet, a normal amount of exercise, and the reduction of efforts to eliminate food. Hospitalization is occasionally needed. Medications may be used to help with some of the associated symptoms.[2] At five years about 70% of people with anorexia and 50% of people with bulimia recover. Recovery from binge eating disorder is less clear and estimated at 20% to 60%. Both anorexia and bulimia increase the risk of death.[6]		In the developed world binge eating disorder affects about 1.6% of women and 0.8% of men in a given year. Anorexia affects about 0.4% and bulimia affects about 1.3% of young women in a given year.[1] During the entire life up to 4% of women have anorexia, 2% have bulimia, and 2% have binge eating disorder.[6] Anorexia and bulimia occur nearly ten times more often in females than males.[1] Typically they begin in late childhood or early adulthood.[2] Rates of other eating disorders are not clear.[1] Rates of eating disorders appear to be lower in less developed countries.[7]		Bulimia nervosa is a disorder characterized by binge eating and purging, as well as excessive evaluation of one's self-worth in terms of body weight or shape.[8] Purging can include self-induced vomiting, over-exercising, and the use of diuretics, enemas, and laxatives. Anorexia nervosa is characterized by extreme food restriction and excessive weight loss, accompanied by the fear of being fat.[9] The extreme weight loss often causes women and girls who have begun menstruating to stop having menstrual periods, a condition known as amenorrhea. Although amenorrhea was once a required criterion for the disorder, it is no longer required to meet criteria for anorexia nervosa due to its exclusive nature for sufferers who are male, post-menopause, or who do not menstruate for other reasons.[10] The DSM-5 specifies two subtypes of anorexia nervosa—the restricting type and the binge/purge type. Those who suffer from the restricting type of anorexia nervosa restrict food intake and do not engage in binge eating, whereas those suffering from the binge/purge type lose control over their eating at least occasionally and may compensate for these binge episodes.[11] The most notable difference between anorexia nervosa binge/purge type and bulimia nervosa is the body weight of the person. Those diagnosed with anorexia nervosa binge/purge type are underweight, while those with bulimia nervosa may have a body weight that falls within the range from normal to obese.[12][13]		These eating disorders are specified as mental disorders in standard medical manuals, such as in the ICD-10,[14] the DSM-5, or both.		Symptoms and complications vary according to the nature and severity of the eating disorder:[27]		Some physical symptoms of eating disorders are weakness, fatigue, sensitivity to cold, reduced beard growth in men, reduction in waking erections, reduced libido, weight loss and failure of growth.[35] Unexplained hoarseness may be a symptom of an underlying eating disorder, as the result of acid reflux, or entry of acidic gastric material into the laryngoesophageal tract. Patients who induce vomiting, such as those with anorexia nervosa, binge eating-purging type or those with purging-type bulimia nervosa are at risk for acid reflux.[medical citation needed] Polycystic ovary syndrome (PCOS) is the most common endocrine disorder to affect women. Though often associated with obesity it can occur in normal weight individuals. PCOS has been associated with binge eating and bulimic behavior.[36][37][38][39][40][41] Other possible manifestations are dry lips,[42] burning tongue,[42] parotid gland swelling,[42] and temporomandibular disorders.[42]		Pro-ana refers to the promotion of behaviors related to the eating disorder anorexia nervosa. Several websites promote eating disorders, and can provide a means for individuals to communicate in order to maintain eating disorders. Members of these websites typically feel that their eating disorder is the only aspect of a chaotic life that they can control.[43] These websites are often interactive and have discussion boards where individuals can share strategies, ideas, and experiences, such as diet and exercise plans that achieve extremely low weights.[44] A study comparing the personal web-blogs that were pro-eating disorder with those focused on recovery found that the pro-eating disorder blogs contained language reflecting lower cognitive processing, used a more closed-minded writing style, contained less emotional expression and fewer social references, and focused more on eating-related contents than did the recovery blogs.[45]		The psychopathology of eating disorders centers around body image disturbance, such as concerns with weight and shape; self-worth being too dependent on weight and shape; fear of gaining weight even when underweight; denial of how severe the symptoms are and a distortion in the way the body is experienced.[35]		The cause of eating disorder is not clear.		Many people with eating disorders suffer also from body dysmorphic disorder, altering the way a person sees themself.[46][47] Studies have found that a high proportion of individuals diagnosed with body dysmorphic disorder also had some type of eating disorder, with 15% of individuals having either anorexia nervosa or bulimia nervosa.[46] This link between body dysmorphic disorder and anorexia stems from the fact that both BDD and anorexia nervosa are characterized by a preoccupation with physical appearance and a distortion of body image.[47] There are also many other possibilities such as environmental, social and interpersonal issues that could promote and sustain these illnesses.}[48] Also, the media are oftentimes blamed for the rise in the incidence of eating disorders due to the fact that media images of idealized slim physical shape of people such as models and celebrities motivate or even force people to attempt to achieve slimness themselves. The media are accused of distorting reality, in the sense that people portrayed in the media are either naturally thin and thus unrepresentative of normality or unnaturally thin by forcing their bodies to look like the ideal image by putting excessive pressure on themselves to look a certain way. While past findings have described the causes of eating disorders as primarily psychological, environmental, and sociocultural, new studies have uncovered evidence that there is a prevalent genetic/heritable aspect of the causes of eating disorders.[49]		Numerous studies show a possible genetic predisposition toward eating disorders as a result of Mendelian inheritance.[50][50][51] Twin studies have found a slight instances of genetic variance when considering the different criterion of both anorexia nervosa and bulimia nervosa as endophenotypes contributing to the disorders as a whole.[48] A genetic link has been found on chromosome 1 in multiple family members of an individual with anorexia nervosa.[49] An individual who is a first degree relative of someone who has or currently has an eating disorder is seven to twelve times more likely have an eating disorder themselves.[52] Twin studies also show that at least a portion of the vulnerability to develop eating disorders can be inherited, and there is evidence to show that there is a genetic locus that shows susceptibility for developing anorexia nervosa.[52] About 60% of eating disorder cases are attributable to biological and genetic components. Other cases are due to external reasons or developmental problems.[53] There are also other neurobiological factors at play tied to emotional reactivity and impulsivity that could lead to binging and purging behaviors.[54]		Epigenetics: Epigenetic mechanisms are means by which environmental effects alter gene expression via methods such as DNA methylation; these are independent of and do not alter the underlying DNA sequence. They are heritable, but also may occur throughout the lifespan, and are potentially reversible. Dysregulation of dopaminergic neurotransmission due to epigenetic mechanisms has been implicated in various eating disorders.[55][56] One study has found that "epigenetic mechanisms may contribute to the known alterations of ANP homeostasis in women with eating disorders."[55][57] Other candidate genes for epigenetic studies in eating disorders include leptin, pro-opiomelanocortin (POMC) and brain-derived neurotrophic factor (BDNF).[58]		Eating disorders are classified as Axis I[59] disorders in the Diagnostic and Statistical Manual of Mental Health Disorders (DSM-IV) published by the American Psychiatric Association. There are various other psychological issues that may factor into eating disorders, some fulfill the criteria for a separate Axis I diagnosis or a personality disorder which is coded Axis II and thus are considered comorbid to the diagnosed eating disorder. Axis II disorders are subtyped into 3 "clusters": A, B and C. The causality between personality disorders and eating disorders has yet to be fully established.[60] Some people have a previous disorder which may increase their vulnerability to developing an eating disorder.[61][62][63] Some develop them afterwards.[64] The severity and type of eating disorder symptoms have been shown to affect comorbidity.[65] The DSM-IV should not be used by laypersons to diagnose themselves, even when used by professionals there has been considerable controversy over the diagnostic criteria used for various diagnoses, including eating disorders. There has been controversy over various editions of the DSM including the latest edition, DSM-V, due in May 2013.[66][67][68][69][70]		Attentional bias may have an effect on eating disorders. Many studies have been performed to test this theory.		There are various childhood personality traits associated with the development of eating disorders.[85] During adolescence these traits may become intensified due to a variety of physiological and cultural influences such as the hormonal changes associated with puberty, stress related to the approaching demands of maturity and socio-cultural influences and perceived expectations, especially in areas that concern body image. Eating disorders have been associated with a fragile sense of self and with disordered mentalization.[86] Many personality traits have a genetic component and are highly heritable. Maladaptive levels of certain traits may be acquired as a result of anoxic or traumatic brain injury, neurodegenerative diseases such as Parkinson's disease, neurotoxicity such as lead exposure, bacterial infection such as Lyme disease or viral infection such as Toxoplasma gondii as well as hormonal influences. While studies are still continuing via the use of various imaging techniques such as fMRI; these traits have been shown to originate in various regions of the brain[87] such as the amygdala[88][89] and the prefrontal cortex.[90] Disorders in the prefrontal cortex and the executive functioning system have been shown to affect eating behavior.[91][92]		People with gastrointestinal disorders may be more risk of developing disordered eating practices than the general population, principally restrictive eating disturbances.[93] An association of anorexia nervosa with celiac disease has been found.[94] The role that gastrointestinal symptoms play in the development of eating disorders seems rather complex. Some authors report that unresolved symptoms prior to gastrointestinal disease diagnosis may create a food aversion in these persons, causing alterations to their eating patterns. Other authors report that greater symptoms throughout their diagnosis led to greater risk. It has been documented that some people with celiac disease, irritable bowel syndrome or inflammatory bowel disease who are not conscious about the importance of strictly following their diet, choose to consume their trigger foods to promote weight loss. On the other hand, individuals with good dietary management may develop anxiety, food aversion and eating disorders because of concerns around cross contamination of their foods.[93] Some authors suggest that medical professionals should evaluate the presence of an unrecognized celiac disease in all people with eating disorder, especially if they present any gastrointestinal symptom (such as decreased appetite, abdominal pain, bloating, distension, vomiting, diarrhea or constipation), weight loss, or growth failure; and also routinely ask celiac patients about weight or body shape concerns, dieting or vomiting for weight control, to evaluate the possible presence of eating disorders,[94] specially in women.[95]		Child abuse which encompasses physical, psychological and sexual abuse, as well as neglect has been shown to approximately triple the risk of an eating disorder.[96] Sexual abuse appears to about double the risk of bulimia; however, the association is less clear for anorexia.[96]		Social isolation has been shown to have a deleterious effect on an individual's physical and emotional well-being. Those that are socially isolated have a higher mortality rate in general as compared to individuals that have established social relationships. This effect on mortality is markedly increased in those with pre-existing medical or psychiatric conditions, and has been especially noted in cases of coronary heart disease. "The magnitude of risk associated with social isolation is comparable with that of cigarette smoking and other major biomedical and psychosocial risk factors." (Brummett et al.)		Social isolation can be inherently stressful, depressing and anxiety-provoking. In an attempt to ameliorate these distressful feelings an individual may engage in emotional eating in which food serves as a source of comfort. The loneliness of social isolation and the inherent stressors thus associated have been implicated as triggering factors in binge eating as well.[97][98][99][100]		Waller, Kennerley and Ohanian (2007) argued that both bingeing–vomiting and restriction are emotion suppression strategies, but they are just utilized at different times. For example, restriction is used to pre-empt any emotion activation, while bingeing–vomiting is used after an emotion has been activated.[101]		Parental influence has been shown to be an intrinsic component in the development of eating behaviors of children. This influence is manifested and shaped by a variety of diverse factors such as familial genetic predisposition, dietary choices as dictated by cultural or ethnic preferences, the parents' own body shape and eating patterns, the degree of involvement and expectations of their children's eating behavior as well as the interpersonal relationship of parent and child. This is in addition to the general psychosocial climate of the home and the presence or absence of a nurturing stable environment. It has been shown that maladaptive parental behavior has an important role in the development of eating disorders. As to the more subtle aspects of parental influence, it has been shown that eating patterns are established in early childhood and that children should be allowed to decide when their appetite is satisfied as early as the age of two. A direct link has been shown between obesity and parental pressure to eat more.		Coercive tactics in regard to diet have not been proven to be efficacious in controlling a child's eating behavior. Affection and attention have been shown to affect the degree of a child's finickiness and their acceptance of a more varied diet.[102][103][104][105][106][107]		Hilde Bruch, a pioneer in the field of studying eating disorders, asserts that anorexia nervosa often occurs in girls who are high achievers, obedient, and always trying to please their parents. Their parents have a tendency to be over-controlling and fail to encourage the expression of emotions, inhibiting daughters from accepting their own feelings and desires. Adolescent females in these overbearing families lack the ability to be independent from their families, yet realize the need to, often resulting in rebellion. Controlling their food intake may make them feel better, as it provides them with a sense of control.[108]		In various studies such as one conducted by The McKnight Investigators, peer pressure was shown to be a significant contributor to body image concerns and attitudes toward eating among subjects in their teens and early twenties.		Eleanor Mackey and co-author, Annette M. La Greca of the University of Miami, studied 236 teen girls from public high schools in southeast Florida. "Teen girls' concerns about their own weight, about how they appear to others and their perceptions that their peers want them to be thin are significantly related to weight-control behavior", says psychologist Eleanor Mackey of the Children's National Medical Center in Washington and lead author of the study. "Those are really important."		According to one study, 40% of 9- and 10-year-old girls are already trying to lose weight.[109] Such dieting is reported to be influenced by peer behavior, with many of those individuals on a diet reporting that their friends also were dieting. The number of friends dieting and the number of friends who pressured them to diet also played a significant role in their own choices.[110][111][112][113]		Elite athletes have a significantly higher rate in eating disorders. Female athletes in sports such as gymnastics, ballet, diving, etc. are found to be at the highest risk among all athletes. Women are more likely than men to acquire an eating disorder between the ages of 13–30. 0–15% of those with bulimia and anorexia are men.[114]		There is a cultural emphasis on thinness which is especially pervasive in western society. There is an unrealistic stereotype of what constitutes beauty and the ideal body type as portrayed by the media, fashion and entertainment industries. "The cultural pressure on men and women to be 'perfect' is an important predisposing factor for the development of eating disorders".[115][116] Further, when women of all races base their evaluation of their self upon what is considered the culturally ideal body, the incidence of eating disorders increases.[117] Eating disorders are becoming more prevalent in non-Western countries where thinness is not seen as the ideal, showing that social and cultural pressures are not the only causes of eating disorders.[118] For example, observations of anorexia in all of the non-Western regions of the world point to the disorder not being "culture-bound" as once thought.[119] However, studies on rates of bulimia suggest that it might be culturally bound. In non-Western countries, bulimia is less prevalent than anorexia, but these non-Western countries where it is observed can be said to have probably or definitely been influenced or exposed to Western culture and ideology.[120]		Socioeconomic status (SES) has been viewed as a risk factor for eating disorders, presuming that possessing more resources allows for an individual to actively choose to diet and reduce body weight.[121] Some studies have also shown a relationship between increasing body dissatisfaction with increasing SES.[122] However, once high socioeconomic status has been achieved, this relationship weakens and, in some cases, no longer exists.[119]		The media plays a major role in the way in which people view themselves. Countless magazine ads and commercials depict thin celebrities like Lindsay Lohan, Nicole Richie, Victoria Beckham and Mary Kate Olsen, who appear to gain nothing but attention from their looks. Society has taught people that being accepted by others is necessary at all costs.[123] Unfortunately this has led to the belief that in order to fit in one must look a certain way. Televised beauty competitions such as the Miss America Competition contribute to the idea of what it means to be beautiful because competitors are evaluated on the basis of their opinion.[124]		In addition to socioeconomic status being considered a cultural risk factor so is the world of sports. Athletes and eating disorders tend to go hand in hand, especially the sports where weight is a competitive factor. Gymnastics, horse back riding, wrestling, body building, and dancing are just a few that fall into this category of weight dependent sports. Eating disorders among individuals that participate in competitive activities, especially women, often lead to having physical and biological changes related to their weight that often mimic prepubescent stages. Oftentimes as women's bodies change they lose their competitive edge which leads them to taking extreme measures to maintain their younger body shape. Men often struggle with binge eating followed by excessive exercise while focusing on building muscle rather than losing fat, but this goal of gaining muscle is just as much an eating disorder as obsessing over thinness. The following statistics taken from Susan Nolen-Hoeksema's book, (ab)normal psychology, shows the estimated percentage of athletes that struggle with eating disorders based on the category of sport.		Although most of these athletes develop eating disorders to keep their competitive edge, others use exercise as a way to maintain their weight and figure. This is just as serious as regulating food intake for competition. Even though there is mixed evidence showing at what point athletes are challenged with eating disorders, studies show that regardless of competition level all athletes are at higher risk for developing eating disorders that non-athletes, especially those that participate in sports where thinness is a factor.[125]		Pressure from society is also seen within the homosexual community. Homosexual men are at greater risk of eating disorder symptoms than heterosexual men.[126] Within the gay culture, muscularity gives the advantages of both social and sexual desirability and also power.[127] These pressures and ideas that another homosexual male may desire a mate who is thinner or muscular can possibly lead to eating disorders. The higher eating disorder symptom score reported, the more concern about how others perceive them and the more frequent and excessive exercise sessions occur.[127] High levels of body dissatisfaction are also linked to external motivation to working out and old age; however, having a thin and muscular body occurs within younger homosexual males than older.[126][127]		It is important to realize some of the limitations and challenges of many studies that try to examine the roles of culture, ethnicity, and SES. For starters, most of the cross-cultural studies use definitions from the DSM-IV-TR, which has been criticized as reflecting a Western cultural bias. Thus, assessments and questionnaires may not be constructed to detect some of the cultural differences associated with different disorders. Also, when looking at individuals in areas potentially influenced by Western culture, few studies have attempted to measure how much an individual has adopted the mainstream culture or retained the traditional cultural values of the area. Lastly, the majority of the cross-cultural studies on eating disorders and body image disturbances occurred in Western nations and not in the countries or regions being examined.[128]		While there are many influences to how an individual processes their body image, the media does play a major role. Along with the media, parental influence, peer influence, and self-efficacy beliefs also play a large role in an individual's view of themselves. The way the media presents images can have a lasting effect on an individual's perception of their body image. Eating disorders are a worldwide issue and while women are more likely to be affected by an eating disorder it still affects both genders (Schwitzer 2012). The media influences eating disorders whether shown in a positive or negative light, it then has a responsibility to use caution when promoting images that projects an ideal that many turn to eating disorders to attain.[129]		To try to address unhealthy body image in the fashion world, in 2015, France passed a law requiring models to be declared healthy by a doctor to participate in fashion shows. It also requires re-touched images to be marked as such in magazines.[130]		There is a relationship between “thin ideal” social media content and body dissatisfaction and eating disorders among young adult women, especially in the Western hemisphere.[131] New research points to an “internalization” of distorted images online, as well as negative comparisons among young adult women.[132] Most studies have been based in the U.S, the U.K, and Australia, these are places where the thin ideal is strong among women, as well as the strive for the “perfect” body.[132]		In addition to mere media exposure, there is an online “pro-eating disorder” community. Through personal blogs and Twitter, this community promotes eating disorders as a “lifestyle”, and continuously posts pictures of emaciated bodies, and tips on how to stay thin. The hashtag “#proana” (pro-anorexia), is a product of this community,[133] as well as images promoting weight loss, tagged with the term “thinspiration”. According to social comparison theory, young women have a tendency to compare their appearance to others, which can result in a negative view of their own bodies and altering of eating behaviors, that in turn can develop disordered eating behaviors.[134]		When body parts are isolated and displayed in the media as objects to be looked at, it is called objectification, and women are affected most by this phenomenon. Objectification increases self-objectification, where women judge their own body parts as a mean of praise and pleasure for others. There is a significant link between self-objectification, body dissatisfaction, and disordered eating, as the beauty ideal is altered through social media.[131]		The initial diagnosis should be made by a competent medical professional. "The medical history is the most powerful tool for diagnosing eating disorders"(American Family Physician).[177] There are many medical disorders that mimic eating disorders and comorbid psychiatric disorders. All organic causes should be ruled out prior to a diagnosis of an eating disorder or any other psychiatric disorder. In the past 30 years eating disorders have become increasingly conspicuous and it is uncertain whether the changes in presentation reflect a true increase.[citation needed] Anorexia nervosa and bulimia nervosa are the most clearly defined subgroups of a wider range of eating disorders. Many patients present with subthreshold expressions of the two main diagnoses: others with different patterns and symptoms.[178]		The diagnostic workup typically includes complete medical and psychosocial history and follows a rational and formulaic approach to the diagnosis. Neuroimaging using fMRI, MRI, PET and SPECT scans have been used to detect cases in which a lesion, tumor or other organic condition has been either the sole causative or contributory factor in an eating disorder. "Right frontal intracerebral lesions with their close relationship to the limbic system could be causative for eating disorders, we therefore recommend performing a cranial MRI in all patients with suspected eating disorders" (Trummer M et al. 2002), "intracranial pathology should also be considered however certain is the diagnosis of early-onset anorexia nervosa. Second, neuroimaging plays an important part in diagnosing early-onset anorexia nervosa, both from a clinical and a research prospective".(O'Brien et al. 2001).[158][179]		After ruling out organic causes and the initial diagnosis of an eating disorder being made by a medical professional, a trained mental health professional aids in the assessment and treatment of the underlying psychological components of the eating disorder and any comorbid psychological conditions. The clinician conducts a clinical interview and may employ various psychometric tests. Some are general in nature while others were devised specifically for use in the assessment of eating disorders. Some of the general tests that may be used are the Hamilton Depression Rating Scale[186] and the Beck Depression Inventory.[187][188] longitudinal research showed that there is an increase in chance that a young adult female would develop bulimia due to their current psychological pressure and as the person ages and matures, their emotional problems change or are resolved and then the symptoms decline.[189]		There are multiple medical conditions which may be misdiagnosed as a primary psychiatric disorder, complicating or delaying treatment. These may have a synergistic effect on conditions which mimic an eating disorder or on a properly diagnosed eating disorder.		Psychological disorders which may be confused with an eating disorder, or be co-morbid with one:		Prevention aims to promote a healthy development before the occurrence of eating disorders. It also intends early identification of an eating disorder before it is too late to treat. Children as young as ages 5–7 are aware of the cultural messages regarding body image and dieting. Prevention comes in bringing these issues to the light. The following topics can be discussed with young children (as well as teens and young adults).		Internet and modern technologies provide new opportunities for prevention. On-line programs have the potential to increase the use of prevention programs. The development and practice of prevention programs via on-line sources make it possible to reach a wide range of people at minimal cost.[215] Such an approach can also make prevention programs to be sustainable.		Treatment varies according to type and severity of eating disorder, and usually more than one treatment option is utilized.[216] There is no well-established treatment for eating disorders, meaning that current views about treatment are based mainly on clinical experience. Family doctors play an important role in early treatment of people with eating disorders by encouraging those who are also reluctant to see a psychiatrist.[217] Treatment can take place in a variety of different settings such as community programs, hospitals, day programs, and groups.[218] The American Psychiatric Association (APA) recommends a team approach to treatment of eating disorders. The members of the team are usually a psychiatrist, therapist, and registered dietitian, but other clinicians may be included.[219]		That said, some treatment methods are:		There are few studies on the cost-effectiveness of the various treatments.[251] Treatment can be expensive;[252][253] due to limitations in health care coverage, people hospitalized with anorexia nervosa may be discharged while still underweight, resulting in relapse and rehospitalization.[254]		For children with anorexia, the only well-established treatment is the family treatment-behavior.[255] For other eating disorders in children, however, there is no well-established treatments, though family treatment-behavior has been used in treating bulimia.[255]		Outcome estimates are complicated by non-uniform criteria used by various studies, but for anorexia nervosa, bulimia nervosa, and binge eating disorder, there seems to be general agreement that full recovery rates are in the 50% to 85% range, with larger proportions of people experiencing at least partial remission.[249][256][257][258] The outcomes of eating disorders (ED) vary among the cases. For many, it can be a lifelong struggle or it can be overcome within months. In the United States, twenty million women and ten million men have an eating disorder at least once in their lifetime.[259] The mortality rate for those with anorexia nervosa is 5.4 per 1000 individuals per year. Roughly 1.3 deaths were due to suicide. A person who is or had been in an inpatient setting had a rate of 4.6 deaths per 1000. Of individuals with bulimia nervosa about 2 persons per 1000 persons die per year and among those with EDNOS about 3.3 per 1000 people die per year.[260]		Anorexia Nervosa symptoms include the increasing chance of getting osteoporosis. This disease causes the bones of an individual to become brittle, weak, and low in density. Thinning of the hair as well as dry hair and skin is also very common. The muscles of the heart will also start to change if no treatment is inflicted on the patient. This causes the heart to have an abnormally slow heart rate along with low blood pressure. Heart failure becomes a major consideration when this begins to occur. Muscles throughout the body begin to lose their strength. This will cause the individual to begin feeling faint, drowsy, and weak. Along with these symptoms, the body will begin to grow a layer of hair called lanugo. The human body does this in response to the lack of heat and insulation due to the low percentage of body fat.[259]		Bulimia nervosa symptoms include heart problems like an irregular heartbeat that can lead to heart failure and death may occur. This occurs because of the electrolyte imbalance that is a result of the constant binge and purge process. The probability of a gastric rupture increases. A gastric rupture is when there is a sudden rupture of the stomach lining that can be fatal.The acids that are contained in the vomit can cause a rupture in the esophagus as well as tooth decay. As a result, to laxative abuse, irregular bowel movements may occur along with constipation. Sores along the lining of the stomach called peptic ulcers begin to appear and the chance of developing pancreatitis increases.[259]		Binge eating symptoms include high blood pressure, which can cause heart disease if it is not treated. Many patients recognize an increase in the levels of cholesterol. The chance of being diagnosed with gallbladder disease increases, which affects an individual’s digestive tract.[259]		Eating disorders result in about 7,000 deaths a year as of 2010, making them the mental illnesses with the highest mortality rate.[264]		One study in the United States found a higher rate in college students who are transgender.[265]						
A garnish is an item or substance used as a decoration or embellishment accompanying a prepared food dish or drink.[1] In many cases, it may give added or contrasting flavor. Some garnishes are selected mainly to augment the visual impact of the plate, while others are selected specifically for the flavor they may impart.[2] This is in contrast to a condiment, a prepared sauce added to another food item primarily for its flavor. A food item which is served with garnish may be described as being garni, the French term for 'garnished.'		Many garnishes are not intended to be eaten, though for some it is fine to do so. Parsley is an example of a traditional garnish; this pungent green herb has small distinctly shaped leaves, firm stems, and is easy to trim into a garnish.						A garnish makes food or drink items more visually appealing.[3][4] They may, for example, enhance their color,[3] such as when paprika is sprinkled on a salmon salad. They may provide a color contrast, for example when chives are sprinkled on potatoes. They may make a cocktail more visually appealing, such as when a cocktail umbrella is added to an exotic drink, or when a Mai Tai is topped with any number of tropical fruit pieces. Sushi may be garnished with baran, a type of plastic grass or leaf. Sometimes a garnish and a condiment will be used together to finish the presentation of a dish; for example, an entrée could be topped with a sauce, as the condiment, along with a sprig of parsley as a garnish.		A garnish may be so readily identified with a specific dish that the dish may appear incomplete without the garnish. Examples include a banana split sundae with cherries on top or buffalo wings served with celery stick garnish and blue cheese dressing.		Garnishes for foods and entrees include:		Garnishes for desserts and sweets include:		Garnishes for beverages include:		Classic French garnishes include[24]		For soups:		For relevés and entrées:		In Korean cuisine, decorative garnishes are referred to as komyǒng.[29][30]		Jidan, a Korean egg garnish for soups		Tools often used for creating food garnishes include skewers, knives, graters, toothpicks, and parchment cones.[40]		Fried onions are used as a garnish		A chocolate cake garnished with violets		A slice of butter cake garnished with sliced almonds		Egg Biryani garnished with cilantro		A cappuccino garnished with cocoa powder		Ice cream garnished with pistachio pieces and rolled wafers		A crabcake with a cream sauce and a garnish of microgreens		Cheese tray garnished with red pepper rings and chicory		A Bloody Mary with several garnishes		A wedding cake topped with a wedding cake topper		
Cocaine- and amphetamine-regulated transcript, also known as CART, is a protein that in humans is encoded by the CARTPT gene.[1][2] CART appears to have roles in reward, feeding, and stress,[3] and it has the functional properties of an endogenous psychostimulant.[4]						CART is a neuropeptide that produces similar behaviour in animals to cocaine and amphetamine, but conversely blocks the effects of cocaine when they are co-administered. The peptide is found in several areas, among them the ventral tegmental area (VTA) of the brain. When CART is injected into rat VTA, increased locomotor activity is seen, which is one of the signs of "central stimulation" caused by substances such as cocaine and amphetamine. The rats also tended to return to the place where they had been injected. This is called conditioned place preference and is seen after injection of cocaine.		CART peptides, in particular, CART (55–102), seem to have an important function in the regulation of energy homeostasis, and interact with several hypothalamic appetite circuits. CART expression is regulated by several peripheral peptide hormones involved in appetite regulation, including leptin,[5] cholecystokinin and ghrelin,[6] with CART and cholecystokinin having synergistic effects on appetite regulation.[7]		CART is released in response to repeated dopamine release in the nucleus accumbens, and may regulate the activity of neurons in this area.[8] CART production is upregulated by CREB,[9] a protein thought to be involved with the development of drug addiction, and CART may be an important therapeutic target in the treatment of stimulant abuse.[10][11][12]		CART is an anorectic peptide and is widely expressed in both the central and peripheral nervous systems, particularly concentrated in the hypothalamus.[13] CART is outside of the nervous system also expressed in pituitary endocrine cells, adrenomedullary cells, islet somatostatin cells, and in rat antral gastrin cells.[14] Other structures and pathways associated with CART expression include the mesolimbic pathway (linking the ventral tegmental area to the nucleus accumbens) and amygdala.		CART is also found in a subset of retinal ganglion cells (RGCS), the primary afferent neuron in the retina. Specifically, it has been demonstrated to label ON/OFF Direction Selective Ganglion Cells (ooDSGCs), a subpopulation of RGC that stratify in both the ON and OFF sublamina of the Inner Plexiform Layer (IPL) of the retina. It is also found in a subset of amacrine cells in the Inner Nuclear Layer.[15] No role as of yet has been proposed for the peculiar location of this protein in these cells types.		Studies of CART (54–102) action in rat lateral ventricle and amygdala suggest that CART play a role in anxiety-like behavior, induced by ethanol withdrawal in rats.[16] Studies on CART knock-out mice indicates that CART modulates the locomotor, conditioned place preference and cocaine self-administration effect of psychostimulants. This suggests a positive neuromodulatory action of CART on psychostimulants effect on rat.[17] CART is altered in the ventral tegmental area of cocaine overdose victims, and a mutation in the CART gene associates with alcoholism.[18] CART peptides are inhibitors of food intake (anorexigenic) and closely associated with leptin and neuropeptide Y, two important food intake regulators. CART hypoactivity in the hypothalamus of depressed animals is associated with hyperphagia and weight gain.[19][20] CART peptides are also involved in fear and startle behavior.[21] CART is thought to play a key role in the opioid mesolimbic dopamine circuit that modulates natural reward processes.[22] CART also appears to play an important role in higher brain functions like cognition.[23]		CART was found by examining changes in the brain following cocaine or amphetamine administration. CART mRNA increased with cocaine administration. One of the goals was to find an endogenous anorexigenic substance. CART inhibited rat food intake by as much as 30 percent. When naturally occurring CART peptides were blocked by means of injecting antibodies to CART, feeding increased. This led to suggestions CART may play a role - though not being the only peptide - in satiety. In the end of the 1980s, researchers started to synthesize cocaine-like and CART-like-acting substances in order to find medications that could affect eating disorders as well as cocaine abuse. These cocaine-like substances are called phenyltropanes.[24]		The putative receptor target for CART has not yet been identified as of 2011,[25] however in vitro studies strongly suggest that CART binds to a specific G protein-coupled receptor coupled to Gi/Go, resulting in increased ERK release inside the cell.[25][26][27][28]		Several fragments of CART have been tested to try and uncover the pharmacophore,[29][30] but the natural splicing products CART 55–102 and CART 62–102 are still of highest activity, with the reduced activity of smaller fragments thought to indicate that a compact structure retaining all three of CART's disulphide bonds is preferred.[31]		
A sense is a physiological capacity of organisms that provides data for perception. The senses and their operation, classification, and theory are overlapping topics studied by a variety of fields, most notably neuroscience, cognitive psychology (or cognitive science), and philosophy of perception. The nervous system has a specific sensory nervous system, and a sense organ, dedicated to each sense.		Humans have a multitude of senses. Sight (vision), hearing (audition), taste (gustation), smell (olfaction), and touch (somatosensation) are the five traditionally recognized senses. The ability to detect other stimuli beyond those governed by these most broadly recognized senses also exists, and these sensory modalities include temperature (thermoception), kinesthetic sense (proprioception), pain (nociception), balance (equilibrioception), vibration (mechanoreception), and various internal stimuli (e.g. the different chemoreceptors for detecting salt and carbon dioxide concentrations in the blood, or sense of hunger and sense of thirst). However, what constitutes a sense is a matter of some debate, leading to difficulties in defining what exactly a distinct sense is, and where the borders between responses to related stimuli lie.		Other animals also have receptors to sense the world around them, with degrees of capability varying greatly between species. Humans have a comparatively weak sense of smell and a stronger sense of sight relative to many other mammals while some animals may lack one or more of the traditional five senses. Some animals may also intake and interpret sensory stimuli in very different ways. Some species of animals are able to sense the world in a way that humans cannot, with some species able to sense electrical and magnetic fields, and detect water pressure and currents.						A broadly acceptable definition of a sense would be "A system that consists of a group of sensory cell types that responds to a specific physical phenomenon, and that corresponds to a particular group of regions within the brain where the signals are received and interpreted." There is no firm agreement as to the number of senses because of differing definitions of what constitutes a sense.		The senses are frequently divided into exteroceptive and interoceptive:		Non-human animals may possess senses that are absent in humans, such as electroreception and detection of polarized light.		In Buddhist philosophy, Ayatana or "sense-base" includes the mind as a sense organ, in addition to the traditional five. This addition to the commonly acknowledged senses may arise from the psychological orientation involved in Buddhist thought and practice. The mind considered by itself is seen as the principal gateway to a different spectrum of phenomena that differ from the physical sense data. This way of viewing the human sense system indicates the importance of internal sources of sensation and perception that complements our experience of the external world.		Sight or vision (adjectival form: visual/optical) is the capability of the eye(s) to focus and detect images of visible light on photoreceptors in the retina of each eye that generates electrical nerve impulses for varying colors, hues, and brightness. There are two types of photoreceptors: rods and cones. Rods are very sensitive to light, but do not distinguish colors. Cones distinguish colors, but are less sensitive to dim light. There is some disagreement as to whether this constitutes one, two or three senses. Neuroanatomists generally regard it as two senses, given that different receptors are responsible for the perception of color and brightness. Some argue[citation needed] that stereopsis, the perception of depth using both eyes, also constitutes a sense, but it is generally regarded as a cognitive (that is, post-sensory) function of the visual cortex of the brain where patterns and objects in images are recognized and interpreted based on previously learned information. This is called visual memory.		The inability to see is called blindness. Blindness may result from damage to the eyeball, especially to the retina, damage to the optic nerve that connects each eye to the brain, and/or from stroke (infarcts in the brain). Temporary or permanent blindness can be caused by poisons or medications.		People who are blind from degradation or damage to the visual cortex, but still have functional eyes, are actually capable of some level of vision and reaction to visual stimuli but not a conscious perception; this is known as blindsight. People with blindsight are usually not aware that they are reacting to visual sources, and instead just unconsciously adapt their behaviour to the stimulus.		On February 14, 2013 researchers developed a neural implant that gives rats the ability to sense infrared light which for the first time provides living creatures with new abilities, instead of simply replacing or augmenting existing abilities.[3]		Hearing or audition (adjectival form: auditory) is the sense of sound perception. Hearing is all about vibration. Mechanoreceptors turn motion into electrical nerve pulses, which are located in the inner ear. Since sound is vibration, propagating through a medium such as air, the detection of these vibrations, that is the sense of the hearing, is a mechanical sense because these vibrations are mechanically conducted from the eardrum through a series of tiny bones to hair-like fibers in the inner ear, which detect mechanical motion of the fibers within a range of about 20 to 20,000 hertz,[4] with substantial variation between individuals. Hearing at high frequencies declines with an increase in age. Inability to hear is called deafness or hearing impairment. Sound can also be detected as vibrations conducted through the body by tactition. Lower frequencies that can be heard are detected this way. Some deaf people are able to determine direction and location of vibrations picked up through the feet.[5]		Taste or gustation (adjectival form: gustatory) is one of the traditional five senses. It refers to the capability to detect the taste of substances such as food, certain minerals, and poisons, etc. The sense of taste is often confused with the "sense" of flavor, which is a combination of taste and smell perception. Flavor depends on odor, texture, and temperature as well as on taste. Humans receive tastes through sensory organs called taste buds, or gustatory calyculi, concentrated on the upper surface of the tongue. There are five basic tastes: sweet, bitter, sour, salty and umami. Other tastes such as calcium[6][7] and free fatty acids[8] may also be basic tastes but have yet to receive widespread acceptance. The inability to taste is called ageusia.		Smell or olfaction (adjectival form: olfactory) is the other "chemical" sense. Unlike taste, there are hundreds of olfactory receptors (388 according to one source[9]), each binding to a particular molecular feature. Odor molecules possess a variety of features and, thus, excite specific receptors more or less strongly. This combination of excitatory signals from different receptors makes up what we perceive as the molecule's smell. In the brain, olfaction is processed by the olfactory system. Olfactory receptor neurons in the nose differ from most other neurons in that they die and regenerate on a regular basis. The inability to smell is called anosmia. Some neurons in the nose are specialized to detect pheromones.[10]		Touch or somatosensation (adjectival form: somatic), also called tactition (adjectival form: tactile) or mechanoreception, is a perception resulting from activation of neural receptors, generally in the skin including hair follicles, but also in the tongue, throat, and mucosa. A variety of pressure receptors respond to variations in pressure (firm, brushing, sustained, etc.). The touch sense of itching caused by insect bites or allergies involves special itch-specific neurons in the skin and spinal cord.[11] The loss or impairment of the ability to feel anything touched is called tactile anesthesia. Paresthesia is a sensation of tingling, pricking, or numbness of the skin that may result from nerve damage and may be permanent or temporary.		Balance, equilibrioception, or vestibular sense is the sense that allows an organism to sense body movement, direction, and acceleration, and to attain and maintain postural equilibrium and balance. The organ of equilibrioception is the vestibular labyrinthine system found in both of the inner ears. In technical terms, this organ is responsible for two senses of angular momentum acceleration and linear acceleration (which also senses gravity), but they are known together as equilibrioception.		The vestibular nerve conducts information from sensory receptors in three ampulla that sense motion of fluid in three semicircular canals caused by three-dimensional rotation of the head. The vestibular nerve also conducts information from the utricle and the saccule, which contain hair-like sensory receptors that bend under the weight of otoliths (which are small crystals of calcium carbonate) that provide the inertia needed to detect head rotation, linear acceleration, and the direction of gravitational force.		Thermoception is the sense of heat and the absence of heat (cold) by the skin and internal skin passages, or, rather, the heat flux (the rate of heat flow) in these areas. There are specialized receptors for cold (declining temperature) and for heat (increasing temperature). The cold receptors play an important part in the animal's sense of smell, telling wind direction. The heat receptors are sensitive to infrared radiation and can occur in specialized organs, for instance in pit vipers. The thermoceptors in the skin are quite different from the homeostatic thermoceptors in the brain (hypothalamus), which provide feedback on internal body temperature.		Proprioception, the kinesthetic sense, provides the parietal cortex of the brain with information on the movement and relative positions of the parts of the body. Neurologists test this sense by telling patients to close their eyes and touch their own nose with the tip of a finger. Assuming proper proprioceptive function, at no time will the person lose awareness of where the hand actually is, even though it is not being detected by any of the other senses. Proprioception and touch are related in subtle ways, and their impairment results in surprising and deep deficits in perception and action.[12]		Nociception (physiological pain) signals nerve-damage or damage to tissue. The three types of pain receptors are cutaneous (skin), somatic (joints and bones), and visceral (body organs). It was previously believed that pain was simply the overloading of pressure receptors, but research in the first half of the 20th century indicated that pain is a distinct phenomenon that intertwines with all of the other senses, including touch. Pain was once considered an entirely subjective experience, but recent studies show that pain is registered in the anterior cingulate gyrus of the brain.[13] The main function of pain is to attract our attention to dangers and motivate us to avoid them. For example, humans avoid touching a sharp needle, or hot object, or extending an arm beyond a safe limit because it is dangerous, and thus hurts. Without pain, people could do many dangerous things without being aware of the dangers.		An internal sense also known as interoception[14] is "any sense that is normally stimulated from within the body".[15] These involve numerous sensory receptors in internal organs, such as stretch receptors that are neurologically linked to the brain. Interoception is thought to be atypical in clinical conditions such as alexithymia.[16] Some examples of specific receptors are:		Chronoception refers to how the passage of time is perceived and experienced. Although the sense of time is not associated with a specific sensory system, the work of psychologists and neuroscientists indicates that human brains do have a system governing the perception of time,[19][20] composed of a highly distributed system involving the cerebral cortex, cerebellum and basal ganglia. One particular component, the suprachiasmatic nucleus, is responsible for the circadian (or daily) rhythm, while other cell clusters appear to be capable of shorter-range (ultradian) timekeeping.		One or more dopaminergic pathways in the central nervous system appear to have a strong modulatory influence on mental chronometry, particularly interval timing.[21]		The sense of agency refers to the subjective feeling of having chosen a particular action. Some conditions, such as schizophrenia, can lead to a loss of this sense, causing a person to feel like a machine or even leading to delusions of being controlled from some outside source. The opposite extreme occurs too, with some people experiencing everything in their environment as if they had decided that it would happen.[22]		Even in non-pathological cases, there is a measurable difference between making a decision and the feeling of agency. Through methods such as the Libet experiment, a gap of half a second or more can be detected from the time when there are detectable neurological signs of a decision having been made to the time when the subject actually becomes conscious of the decision.		There are also experiments in which an illusion of agency is induced in psychologically normal subjects. In Wegner and Wheatley 1999, subjects were given instructions to move a mouse around a scene and point to an image about once every thirty seconds. However, a second person — acting as a test subject but actually a confederate — had their hand on the mouse at the same time, and controlled some of the movement. Experimenters were able to arrange for subjects to perceive certain "forced stops" as if they were their own choice.[23][24]		Recognition memory is sometimes divided into two functions by neuroscientists: familiarity and recollection.[25] A strong sense of familiarity can occur without any recollection, for example in cases of deja vu. The temporal lobe, in particular the perirhinal cortex, responds differently to stimuli which feel novel than to things which feel familiar. Firing rates in the perirhinal cortex are connected with the sense of familiarity in humans and other mammals. In tests, stimulating this area at 10–15 Hz caused animals to treat even novel images as familiar, and stimulation at 30–40 Hz caused novel images to be partially treated as familiar.[26] Specifically, stimulation at 30–40 Hz led to animals looking at a familiar image for longer periods, as they would for an unfamiliar one; but it did not lead to the same exploration behavior normally associated with novelty. Recent studies on lesions in the area concluded that rats with a damaged perirhinal cortex were still more interested in exploring when novel objects were present, but seemed unable to tell novel objects from familiar ones — they examined both equally. Thus, other brain regions are involved with noticing unfamiliarity, but the perirhinal cortex is needed to associate the feeling with a specific source.[27]		Other living organisms have receptors to sense the world around them, including many of the senses listed above for humans. However, the mechanisms and capabilities vary widely.		Most non-human mammals have a much keener sense of smell than humans, although the mechanism is similar. An example of smell in non-mammals is that of sharks, which combine their keen sense of smell with timing to determine the direction of a smell. They follow the nostril that first detected the smell.[28] Insects have olfactory receptors on their antennae.		Many animals (salamanders, reptiles, mammals) have a vomeronasal organ[29] that is connected with the mouth cavity. In mammals it is mainly used to detect pheromones of marked territory, trails, and sexual state. Reptiles like snakes and monitor lizards make extensive use of it as a smelling organ by transferring scent molecules to the vomeronasal organ with the tips of the forked tongue. In reptiles the vomeronasal organ is commonly referred to as Jacobsons organ. In mammals, it is often associated with a special behavior called flehmen characterized by uplifting of the lips. The organ is vestigial in humans, because associated neurons have not been found that give any sensory input in humans.[30]		Flies and butterflies have taste organs on their feet, allowing them to taste anything they land on. Catfish have taste organs across their entire bodies, and can taste anything they touch, including chemicals in the water.[31]		Cats have the ability to see in low light, which is due to muscles surrounding their irides–which contract and expand their pupils–as well as to the tapetum lucidum, a reflective membrane that optimizes the image. Pit vipers, pythons and some boas have organs that allow them to detect infrared light, such that these snakes are able to sense the body heat of their prey. The common vampire bat may also have an infrared sensor on its nose.[32] It has been found that birds and some other animals are tetrachromats and have the ability to see in the ultraviolet down to 300 nanometers. Bees and dragonflies[33] are also able to see in the ultraviolet. Mantis shrimps can perceive both polarized light and multispectral images and have twelve distinct kinds of color receptors, unlike humans which have three kinds and most mammals which have two kinds.[34]		Many invertebrates have a statocyst, which is a sensor for acceleration and orientation that works very differently from the mammalian's semi-circular canals.		Some plants (such as mustard) have genes that are necessary for the plant to sense the direction of gravity. If these genes are disabled by a mutation, a plant cannot grow upright.[35]		In addition, some animals have senses that humans do not, including the following:		Certain animals, including bats and cetaceans, have the ability to determine orientation to other objects through interpretation of reflected sound (like sonar). They most often use this to navigate through poor lighting conditions or to identify and track prey. There is currently an uncertainty whether this is simply an extremely developed post-sensory interpretation of auditory perceptions or it actually constitutes a separate sense. Resolution of the issue will require brain scans of animals while they actually perform echolocation, a task that has proven difficult in practice.		Blind people report they are able to navigate and in some cases identify an object by interpreting reflected sounds (especially their own footsteps), a phenomenon known as human echolocation.		Electroreception (or electroception) is the ability to detect electric fields. Several species of fish, sharks, and rays have the capacity to sense changes in electric fields in their immediate vicinity. For cartilaginous fish this occurs through a specialized organ called the Ampullae of Lorenzini. Some fish passively sense changing nearby electric fields; some generate their own weak electric fields, and sense the pattern of field potentials over their body surface; and some use these electric field generating and sensing capacities for social communication. The mechanisms by which electroceptive fish construct a spatial representation from very small differences in field potentials involve comparisons of spike latencies from different parts of the fish's body.		The only orders of mammals that are known to demonstrate electroception are the dolphin and monotreme orders. Among these mammals, the platypus[36] has the most acute sense of electroception.		A dolphin can detect electric fields in water using electroreceptors in vibrissal crypts arrayed in pairs on its snout and which evolved from whisker motion sensors.[37] These electroreceptors can detect electric fields as weak as 4.6 microvolts per centimeter, such as those generated by contracting muscles and pumping gills of potential prey. This permits the dolphin to locate prey from the seafloor where sediment limits visibility and echolocation.		Body modification enthusiasts have experimented with magnetic implants to attempt to replicate this sense.[38] However, in general humans (and it is presumed other mammals) can detect electric fields only indirectly by detecting the effect they have on hairs. An electrically charged balloon, for instance, will exert a force on human arm hairs, which can be felt through tactition and identified as coming from a static charge (and not from wind or the like). This is not electroreception, as it is a post-sensory cognitive action.		Magnetoception (or magnetoreception) is the ability to detect the direction one is facing based on the Earth's magnetic field. Directional awareness is most commonly observed in birds, which rely on their magnetic sense to navigate during migration.[39][39][40][41][42] It has also been observed in insects such as bees. Cattle make use of magnetoception to align themselves in a north-south direction.[43] Magnetotactic bacteria build miniature magnets inside themselves and use them to determine their orientation relative to the Earth's magnetic field.[44][45]		Hygroreception is the ability to detect changes in the moisture content of the environment.[46][47]		The ability to sense infrared thermal radiation evolved independently in various families of snakes. Essentially, it allows these reptiles to "see" radiant heat at wavelengths between 5 and 30 μm to a degree of accuracy such that a blind rattlesnake can target vulnerable body parts of the prey at which it strikes.[48] It was previously thought that the organs evolved primarily as prey detectors, but it is now believed that it may also be used in thermoregulatory decision making.[49] The facial pit underwent parallel evolution in pitvipers and some boas and pythons, having evolved once in pitvipers and multiple times in boas and pythons.[50] The electrophysiology of the structure is similar between the two lineages, but they differ in gross structural anatomy. Most superficially, pitvipers possess one large pit organ on either side of the head, between the eye and the nostril (Loreal pit), while boas and pythons have three or more comparatively smaller pits lining the upper and sometimes the lower lip, in or between the scales. Those of the pitvipers are the more advanced, having a suspended sensory membrane as opposed to a simple pit structure. Within the family Viperidae, the pit organ is seen only in the subfamily Crotalinae: the pitvipers. The organ is used extensively to detect and target endothermic prey such as rodents and birds, and it was previously assumed that the organ evolved specifically for that purpose. However, recent evidence shows that the pit organ may also be used for thermoregulation. According to Krochmal et al., pitvipers can use their pits for thermoregulatory decision making while true vipers (vipers who do not contain heat-sensing pits) cannot.		In spite of its detection of IR light, the pits' IR detection mechanism is not similar to photoreceptors – while photoreceptors detect light via photochemical reactions, the protein in the pits of snakes is in fact a temperature sensitive ion channel. It senses infrared signals through a mechanism involving warming of the pit organ, rather than chemical reaction to light.[51] This is consistent with the thin pit membrane, which allows incoming IR radiation to quickly and precisely warm a given ion channel and trigger a nerve impulse, as well as vascularize the pit membrane in order to rapidly cool the ion channel back to its original "resting" or "inactive" temperature.[51]		By using a variety of sense receptors, plants sense light, gravity, temperature, humidity, chemical substances, chemical gradients, reorientation, magnetic fields, infections, tissue damage and mechanical pressure. The absence of a nervous system notwithstanding, plants interpret and respond to these stimuli by a variety of hormonal and cell-to-cell communication pathways that result in movement, morphological changes and physiological state alterations at the organism level, that is, result in plant behavior. Such physiological and cognitive functions are generally not believed to give rise to mental phenomena or qualia, however, as these are typically considered the product of nervous system activity. The emergence of mental phenomena from the activity of systems functionally or computationally analogous to that of nervous systems is, however, a hypothetical possibility explored by some schools of thought in the philosophy of mind field, such as functionalism and computationalism.		In the time of William Shakespeare, there were commonly reckoned to be five wits or five senses.[52] At that time, the words "sense" and "wit" were synonyms,[52] so the senses were known as the five outward wits.[53][54] This traditional concept of five senses is common today.		The traditional five senses are enumerated as the "five material faculties" (pañcannaṃ indriyānaṃ avakanti) in Hindu literature. They appear in allegorical representation as early as in the Katha Upanishad (roughly 6th century BC), as five horses drawing the "chariot" of the body, guided by the mind as "chariot driver".		Depictions of the five traditional senses as allegory became a popular subject for seventeenth-century artists, especially among Dutch and Flemish Baroque painters. A typical example is Gérard de Lairesse's Allegory of the Five Senses (1668), in which each of the figures in the main group alludes to a sense: Sight is the reclining boy with a convex mirror, hearing is the cupid-like boy with a triangle, smell is represented by the girl with flowers, taste is represented by the woman with the fruit, and touch is represented by the woman holding the bird.		
In social science foodways are the cultural, social, and economic practices relating to the production and consumption of food. Foodways often refers to the intersection of food in culture, traditions, and history.[1][2]						The Merriam-Webster Dictionary defines Foodways as "the eating habits and culinary practices of a people, region, or historical period".[3]		As the field of Foodways develops, scholars offer their own definitions: "Contemporary scholarship defines foodways as the study of what we eat, as well as how and why and under what circumstances we eat it. As folklorist Jay Anderson argued in a pioneering 1971 essay, foodways encompasses “the whole interrelated system of food conceptualization and evaluation, procurement, preservation, preparation, consumption and nutrition shared by all the members of a particular society. This broad definition embraces both historical and regional differences while simultaneously pointing to the importance of food events (barbecues, Brunswick stew suppers, oyster roasts), food processes (ham curing ,snap-bean canning, friend apple pie making), and even the aesthetic realms that touch upon the world of food (country songs about food, quilts raffled at community fish fries, literary references to eating)." - John T. Edge [4]		Anthropologists, folklorists, sociologists, historians, and food scholars often use the term foodways to describe the study of why we eat what we eat and what it means. The term, therefore, looks at food consumption on a deeper than concrete level and includes, yet goes, beyond sustenance, recipes, and/or taste. Thus, according to Harris, Lyon and McLaughlin: “…everything about eating including what we consume, how we acquire it, who prepares it and who’s at the table – is a form of communication rich with meaning. Our attitudes, practices and rituals around food are a window onto our most basic beliefs about the world and ourselves.” [5]		Topics like social inclusion and exclusion, power, and sense making are explored under the umbrella term foodways. Furthermore, the ways in which food shapes and is shaped by social organization are essential to examination of foodways. Since consumption of food is socially constructed, cultural study is also incorporated in the term.[citation needed]		Anthropologist Mary Douglas, explains: “A very modest life of subsistence contrasts with our own use of goods, in for example, the use of food. How would we be able to say all of the things we want to say, even just to the members of our families, about different kinds of events and occasions and possibilities if we did not make any difference between breakfast and lunch and dinner and if we made no difference between Sunday and weekends, and never had a different kind of meal when friends came in, and if Christmas Day had also to be celebrated with the same kind of food?” [6]		While in fields like anthropology, the production, procurement, preparation, presentation, and consumption of foods have always been regarded as central in the study of cultures[7] the use of the term foodways in popular culture is used as an oriented way of looking at food practices. In this sense, the term is a consumer culture expression that encompasses, in popularly understandable and debatable formats, contemporaneous social practices related to foods as well as nutritional and culinary aspects of foods.		The term foodways can be employed when referencing the "ways of food" of a region or location. For example:		Immigrant foodways are also featured prominently in America. For example, The University of Massachusetts Dartmouth, located in Southeastern Massachusetts - which has a very large immigrant population from Cape Verde - describes Cape Verdean Foodways by publishing recipes from these southern Atlantic islands.[13]		In contrast to the anthropological treatments of food, the term foodways aims at a highly cross-disciplinary approach to food and nutrition. For example, the refereed journal Food and Foodways, published by Taylor & Francis, is "devoted to publishing original scholarly articles on the history and culture of human nourishment. By reflecting on the role food plays in human relations, this unique journal explores the powerful but often subtle ways in which food has shaped, and shapes, our lives socially, economically, politically, mentally, nutritionally, and morally. Because food is a pervasive social phenomenon, it cannot be approached by any one discipline".[14]		In consumer culture research, contemporary and postmodern foodways are topics of interest. In an article in the journal Consumption Markets & Culture, from Taylor & Francis publications, Douglas Brownlie, Paul Hewer, and Suzanne Horne explore culinary consumptionscapes through a study of contemporary cookbooks, with chic recipes often turning intensely into a kind of "gastroporn", creating a "simulacrum of desire" as well as a "simulacrum of satisfaction."[15]		Historical studies of foodways help scientists, anthropologists, and scholars gain insight into past cultures. Springer Publishing has released the book, Pre-Columbian Foodways, by John Staller and Michael Carrasco, which studies and examines "the symbolic complexity of food and its preparation, as well as the social importance of feasting in contemporary and historical societies."[16] Books like this help further the study of foodways and increase public awareness.		
Tea (in reference to food, rather than the drink) has long been used as an umbrella term for several different meals. Isabella Beeton, whose books on Home economics were widely read in the 19th century, describes afternoon teas of various kinds, and provides menus for the old-fashioned tea, the at-home tea, the family tea and the high tea.[1] Teatime is the time at which the tea meal is usually eaten, which is late afternoon to early evening.[2] Tea as a meal is associated with Britain, Ireland, and some Commonwealth countries.						Afternoon tea is a light meal typically eaten between 3.30 pm and 5 pm. Observance of the custom originated amongst the wealthy social classes in England in the 1840s.[3] Anna Maria Russell, Duchess of Bedford, is widely credited as transforming afternoon tea in England into a late-afternoon meal whilst visiting Belvoir Castle. By the end of the nineteenth century, afternoon tea developed to its current form and was observed by both the upper and middle classes. It had become ubiquitous, even in the isolated village in the fictionalised memoir Lark Rise to Candleford, where a cottager lays out what she calls a "visitor's tea" for their landlady: "the table was laid… there were the best tea things with a fat pink rose on the side of each cup; hearts of lettuce, thin bread and butter, and the crisp little cakes that had been baked in readiness that morning."[4]		For the more privileged, afternoon tea was accompanied by delicate savouries (customarily cucumber sandwiches or egg and cress sandwiches), bread and butter, possibly scones (with clotted cream and jam, as for cream tea), and usually cakes and pastries (such as Battenberg cake or Victoria sponge). The sandwiches usually have the crusts removed, and are cut into small segments, either as triangles or fingers (also known as tea sandwiches). Biscuits are not usually served.		Nowadays, a formal afternoon tea is more of a special occasion, taken as a treat in a hotel.[5] The food is often served on a tiered stand; there may be no sandwiches, but bread or scones with butter or margarine and optional jam or other spread, or toast, muffins or crumpets.[6][7][8] Afternoon tea as a treat may be supplemented with a glass of Champagne or a similar alcoholic drink.		A less formal establishment is known as a tearoom, similar to a coffee shop. These used to be common in the UK, but these establishments have declined in popularity since World War II. A.B.C. tea shops and Lyons Corner Houses were successful chains of such establishments, and played a role in opening up possibilities for Victorian women. A list of significant tea houses in Britain gives more examples.		A tea party is a social gathering around this meal – not to be confused with the Boston Tea Party, a mid-December 1773 incident at the beginning of the American Revolution, or the 21st century political movement named after it.		This snack is associated with the West Country, i.e. Cornwall and Devon. It usually consists of scones, clotted cream, strawberry jam, plus of course, tea to drink. Some venues will provide butter instead of clotted cream.		Tea (also known as high tea or meat tea) is one name for the evening meal. It is associated with the working class and is typically eaten between 5 pm and 7 pm. In the North of England, North and South Wales, the English Midlands, Scotland and in rural and working class areas of Northern Ireland and the Republic of Ireland, people traditionally call their midday meal dinner and their evening meal tea (served around 6 pm), whereas the upper social classes would call the midday meal lunch or luncheon and the evening meal (served after 7 pm) dinner (if formal) or supper (if informal).[9] This differentiation in usage is one of the classic social markers of British English (see U and non-U English).		High tea typically consists of a hot dish, followed by cakes and bread, butter and jam. Occasionally there would be cold cuts of meat, such as ham salad. The term was first used around 1825, and "high" is used in the sense of well-advanced (like high noon, for example) to signify that it was taken later in the day[10] than afternoon tea.[11][12]		A stereotypical expression "You'll have had your tea" is used to parody people from Edinburgh as being rather shortcoming with hospitality.[13] A BBC Radio 4 comedy series of this name was made by Graeme Garden and Barry Cryer.		Not a meal as such, but a chance to "down tools" (or get away from the computer) and relax from work for 10-15 minutes. This may occur mid-morning (see elevenses) or mid-afternoon. It may equally involve coffee, and almost inevitably, biscuits. Once upon a time, the drinks were served by the workplace's tea lady, a position that is now almost defunct. The British habit of dunking biscuits in tea has been exported around the globe.[14]		In Australia any short break for tea in the afternoon is referred to as "afternoon tea". As a result, the term "high tea" is used to describe the more formal affair that the English would call "afternoon tea".[15] Most Australians are unaware of the origin of the term "high tea". In Australia, the evening meal is still often called tea whereas the midday meal is now commonly called lunch. In rural areas, dinner is still used quite often for the midday meal, tea is around 6pm, and supper is either a very late meal at night, or food served at night at a social function, such as the town's annual Christmas dance and supper.		
Biologically, an adult is a human or other organism that has reached sexual maturity. In human context, the term adult additionally has meanings associated with social and legal concepts. In contrast to a "minor", a legal adult is a person who has attained the age of majority and is therefore regarded as independent, self-sufficient, and responsible.		Human adulthood encompasses psychological adult development. Definitions of adulthood are often inconsistent and contradictory; a person may be biologically an adult, and have adult behavior but still be treated as a child if they are under the legal age of majority. Conversely, one may legally be an adult but possess none of the maturity and responsibility that may define an adult character.		In different cultures there are events that relate passing from being a child to becoming an adult or coming of age. This often encompasses the passing a series of tests to demonstrate that a person is prepared for adulthood, or reaching a specified age, sometimes in conjunction with demonstrating preparation. Most modern societies determine legal adulthood based on reaching a legally specified age without requiring a demonstration of physical maturity or preparation for adulthood.						Historically and cross-culturally, adulthood has been determined primarily by the start of puberty (the appearance of secondary sex characteristics such as menstruation in women, ejaculation in men, and pubic hair in both sexes). In the past, a person usually moved from the status of child directly to the status of adult, often with this shift being marked by some type of coming-of-age test or ceremony.[1]		After the social construct of adolescence was created, adulthood split into two forms: biological adulthood and social adulthood. Thus, there are now two primary forms of adults: biological adults (people who have attained reproductive ability, are fertile, or who evidence secondary sex characteristics) and social adults (people who are recognized by their culture or law as being adults). Depending on the context, adult can indicate either definition.		Although few or no established dictionaries provide a definition for the two word term biological adult, the first definition of adult in multiple dictionaries includes "the stage of the life cycle of an animal after reproductive capacity has been attained".[2][3] Thus, the base definition of the word adult is the period beginning at puberty. Although this is the primary definition of the base word adult, the two word term biological adult stresses or clarifies that the original definition, based on the beginning of puberty, is being used.		Puberty generally begins around 10 or 11 years of age for girls and 11 or 12 years of age for boys, though this will vary from person to person.[4][5] Because the term adult is most often used without the adjective social or biological, and since the term is frequently used to refer to social adults, some writers have taken the meaning of the two word phrase biological adult to begin at the end of physical maturation rather than the onset of puberty.		Legally, adulthood means that one can engage in a contract. The same or a different minimum age may be applicable to, for example, parents losing parenting rights and duties regarding the person concerned, parents losing financial responsibility, marriage, voting, having a job, serving in the military, buying/possessing firearms, driving, traveling abroad, involvement with alcoholic beverages, smoking, sexual activity, gambling, being a prostitute or a client of a prostitute, being a model or actor in pornography, running for President etc. Admission of a young person to a place may be restricted because of danger for that person, concern that the place may lead the person to immoral behavior or because of the risk that the young person causes damage (for example, at an exhibition of fragile items).		One can distinguish the legality of acts of a young person, or of enabling a young person to carry out that act, by selling, renting out, showing, permitting entrance, allowing participation, etc. There may be distinction between commercially and socially enabling. Sometimes there is the requirement of supervision by a legal guardian, or just by an adult. Sometimes there is no requirement, but rather a recommendation.		Using the example of pornography, one can distinguish between:		With regard to films with violence, etc.:		The legal definition of entering adulthood usually varies between ages 16–21, depending on the region in question. Some cultures in Africa[clarification needed] define adulthood at age 13.		In most of the world, including most of the United States, parts of the United Kingdom (England, Northern Ireland, Wales), India and China, the legal adult age is 18 for most purposes, with some notable exceptions:		According to Jewish tradition, adulthood is reached at age 13 (the minimal age of the Bar Mitzvah or Bat Mitzvah) for Jewish boys and girls; they are expected to demonstrate preparation for adulthood by learning the Torah and other Jewish practices. The Christian Bible and Jewish scripture contain no age requirement for adulthood or marrying, which includes engaging in sexual activity. The 1983 Code of Canon Law states, "A man before he has completed his sixteenth year of age, and likewise a woman before she has completed her fourteenth year of age, cannot enter a valid marriage".[6] According to The Disappearance of Childhood by Neil Postman, the Christian Church of the Middle Ages considered the age of accountability, when a person could be tried and even executed as an adult, to be age 7.		
Food energy is chemical energy that animals (including humans) derive from food through the process of cellular respiration. Cellular respiration may either involve the chemical reaction of food molecules with molecular oxygen[1] (aerobic respiration) or the process of reorganizing the food molecules without additional oxygen (anaerobic respiration).		Humans and other animals need a minimum intake of food energy to sustain their metabolism and to drive their muscles. Foods are composed chiefly of carbohydrates, fats, proteins, water, vitamins, and minerals. Carbohydrates, fats, proteins, and water represent virtually all the weight of food, with vitamins and minerals making up only a small percentage of the weight. (Carbohydrates, fats, and proteins comprise ninety percent of the dry weight of foods.[2]) Organisms derive food energy from carbohydrates, fats and proteins as well as from organic acids, polyols, and ethanol present in the diet.[3] Some diet components that provide little or no food energy, such as water, minerals, vitamins, cholesterol, and fiber, may still be necessary to health and survival for other reasons. Water, minerals, vitamins, and cholesterol are not broken down (they are used by the body in the form in which they are absorbed) and so cannot be used for energy. Fiber cannot be completely digested by most animals, including humans. However, ruminants can extract food energy from the respiration of cellulose because of bacteria in their rumens.		Using the International System of Units, researchers measure energy in joules (J) or in its multiples; the kilojoule (kJ) is most often used for food-related quantities. An older metric system unit of energy, still widely used in food-related contexts, is the calorie; more precisely, the "food calorie", "large calorie" or kilocalorie (kcal or Cal), equal to 4.184 kilojoules. (Contrast the "small calorie" (cal), equal to 1/1000 of a food calorie, that is often used in chemistry and in physics.) Within the European Union, both the kilocalorie ("kcal") and kilojoule ("kJ") appear on nutrition labels. In many countries, only one of the units is displayed; in the US and Canada labels spell out the unit as "calorie" or as "Calorie".		Fats and ethanol have the greatest amount of food energy per gram, 37 and 29 kJ/g (8.8 and 6.9 kcal/g), respectively. Proteins and most carbohydrates have about 17 kJ/g (4 kcal/g).[4] The differing energy density of foods (fat, alcohols, carbohydrates and proteins) lies mainly in their varying proportions of carbon, hydrogen, and oxygen atoms: For food of elemental composition CcHhOoNn, the heat of combustion underlying the food energy is 100 kcal/g (c + 0.3 h – 0.5 o)/(12 c + h + 16 o +14 n) to a good approximation (±3%).[1] Carbohydrates that are not easily absorbed, such as fiber, or lactose in lactose-intolerant individuals, contribute less food energy. Polyols (including sugar alcohols) and organic acids contribute 10 kJ/g (2.4 kcal/g) and 13 kJ/g (3.1 kcal/g) respectively.[5] The amount of water, fat, and fiber in foods determines those foods' energy density.		Theoretically, one could measure food energy in different ways, using (say) the Gibbs free energy of combustion, or the amount of ATP generated by metabolizing the food. However, the convention is to use the heat of the oxidation reaction producing liquid water. Conventional food energy is based on heats of combustion in a bomb calorimeter and corrections that take into consideration the efficiency of digestion and absorption and the production of urea and other substances in the urine. The American chemist Wilbur Atwater worked these corrections out in the late 19th century[6] (see Atwater system for more detail). Based on the work of Atwater, it became common practice to calculate energy content of foods using 4 kcal/g for carbohydrates and proteins and 9 kcal/g for lipids.[6] The system was later improved by Annabel Merrill and Bernice Watt of the USDA, who derived a system whereby specific calorie conversion factors for different foods were proposed.[7]						Many governments require food manufacturers to label the energy content of their products, to help consumers control their energy intake.[8] In the European Union, manufacturers of packaged food must label the nutritional energy of their products in both kilocalories and kilojoules, when required. In the United States, the equivalent mandatory labels display only "Calories",[9] often as a substitute for the name of the quantity being measured, food energy; an additional kilojoules figure is optional and is rarely used. In Australia and New Zealand, the food energy must be stated in kilojoules (and optionally in kilocalories as well), and other nutritional energy information is similarly conveyed in kilojoules.[10][11] The energy available from the respiration of food is usually given on labels for 100 g, for a typical serving size (according to the manufacturer), and/or for the entire pack contents.[citation needed]		The amount of food energy associated with a particular food could be measured by completely burning the dried food in a bomb calorimeter, a method known as direct calorimetry.[12] However, the values given on food labels are not determined in this way. The reason for this is that direct calorimetry also burns the dietary fiber, and so does not allow for fecal losses; thus direct calorimetry would give systematic overestimates of the amount of fuel that actually enters the blood through digestion. What are used instead are standardized chemical tests or an analysis of the recipe using reference tables for common ingredients[13] to estimate the product's digestible constituents (protein, carbohydrate, fat, etc.). These results are then converted into an equivalent energy value based on the following standardized table of energy densities.[5][14] However "energy density" is a misleading term for it once again assumes that energy is IN the particular food, whereas it simply means that "high density" food needs more oxygen during respiration, leading to greater transfer of energy.[1][15]		Note that the following standardized table of energy densities[14] is an approximation and the value in kJ/g does not convert exactly to kcal/g using a conversion factor.		The use of such a simple system has been criticized for not taking into consideration other factors pertaining to the influence of different foods on obesity.[6]		All the other nutrients in food are noncaloric and are thus not counted.		Increased mental activity has been linked with moderately increased brain energy consumption.[17] Older people and those with sedentary lifestyles require less energy; children and physically active people require more.		Recommendations in the United States are 2,600 and 2,000 kcal (10,900 and 8,400 kJ) for men and women (respectively) between 31 and 35, at a physical activity level equivalent to walking about 2 to 5 km (1 1⁄2 to 3 mi) per day at 5 to 6 km/h (3 to 4 mph) in addition to the light physical activity associated with typical day-to-day life,[18] with French guidance suggesting roughly the same levels.[19]		Recognizing that people of different age and gender groups have varying daily activity levels, Australia's National Health and Medical Research Council recommends no single daily energy intake, but instead prescribes an appropriate recommendation for each age and gender group.[20] Notwithstanding, nutrition labels on Australian food products typically recommend the average daily energy intake of 2,100 kcal (8,800 kJ).		According to the Food and Agriculture Organization of the United Nations, the average minimum energy requirement per person per day is about 7,500 kJ (1,800 kcal).[21]		The human body uses the energy released by respiration for a wide range of purposes: about 20% of the energy is used for brain metabolism, and much of the rest is used for the basal metabolic requirements of other organs and tissues. In cold environments, metabolism may increase simply to produce heat to maintain body temperature. Among the diverse uses for energy, one is the production of mechanical energy by skeletal muscle to maintain posture and produce motion.		The conversion efficiency of energy from respiration into mechanical (physical) power depends on the type of food and on the type of physical energy usage (e.g., which muscles are used, whether the muscle is used aerobically or anaerobically). In general, the efficiency of muscles is rather low: only 18 to 26% of the energy available from respiration is converted into mechanical energy.[22] This low efficiency is the result of about 40% efficiency of generating ATP from the respiration of food, losses in converting energy from ATP into mechanical work inside the muscle, and mechanical losses inside the body. The latter two losses are dependent on the type of exercise and the type of muscle fibers being used (fast-twitch or slow-twitch). For an overall efficiency of 20%, one watt of mechanical power is equivalent to 4.3 kcal (18 kJ) per hour. For example, a manufacturer of rowing equipment shows calories released from 'burning' food as four times the actual mechanical work, plus 300 kcal (1,300 kJ) per hour,[23] which amounts to about 20% efficiency at 250 watts of mechanical output. It can take up to 20 hours of little physical output (e.g., walking) to "burn off" 4,000 kcal (17,000 kJ)[24] more than a body would otherwise consume. For reference, each kilogram of body fat is roughly equivalent to 32,300 kilojoules of food energy (i.e., 3,500 kilocalories per pound).[25]		In addition, the quality of calories matters because the energy absorption rate of different foods with equal amounts of calories may vary.[citation needed] Some nutrients have regulatory roles affected by cell signaling, in addition to providing energy for the body.[26] For example, leucine plays an important role in the regulation of protein metabolism and suppresses an individual's appetite.[27]		Swings in body temperature – either hotter or cooler – increase the metabolic rate, thus burning more energy. Prolonged exposure to extremely warm or very cold environments increases the basal metabolic rate (BMR). People who live in these types of settings often have BMRs 5–20% higher than those in other climates.[citation needed] Physical activity also significantly increases body temperature, which in turn uses more energy from respiration.[citation needed]						
A meal replacement is a drink, bar, soup, etc. intended as a substitute for a solid food meal, usually with controlled quantities of calories and nutrients. Some drinks are in the form of a health shake. Medically prescribed meal replacement drinks include the required vitamins and minerals.[1] Bodybuilders sometimes use meal replacements, not formulated for weight loss, to save food preparation time when they are eating 5 to 6 meals a day.[2]		In the European Union, weight-reduction meal replacements intended either to supplement ("Meal replacement for weight control") or to replace totally ("Total diet replacement for weight control") normal meals are regulated as to their energy content, the nutrients they must provide, and information and advice on packaging by COMMISSION DIRECTIVE 96/8/EC of 26 February 1996 on foods intended for use in energy-restricted diets for weight reduction.[3] For example, a meal replacement must provide between 200 and 400 food calories of energy, of which not more than 30% from fat, and not less than specified amounts for various vitamins and minerals. Labeling information is prescribed, and packaging must provide information such as a statement that the product should not be used for more than three weeks without medical advice. This protects users of meal replacements without other food from inadvertent malnutrition.		In the United States, the term "meal replacement" is not defined in federal Food and Drug Administration regulations, but generally refers to a calorie-controlled, prepackaged product in the form of a bar or beverage (ready to drink or powder), that replaces a regular meal. Meal-replacement products usually provide 200 to 250 calories per serving, are fortified with more than 20 vitamins and minerals at "good" or "excellent source" levels and often bear nutrient content claims, such as percent fat free and reduced sugar. Meal replacement products can be regulated as conventional or functional foods.[4]		Meal replacements have been a regular feature of science fiction, especially the space travel genre, at least since the film Santa Claus Conquers the Martians (1964) and TV's Lost in Space (1965).		
The Encyclopædia Britannica (Latin for "British Encyclopaedia"), published by Encyclopædia Britannica, Inc., is a general knowledge English-language encyclopaedia. It is written by about 100 full-time editors and more than 4,000 contributors, who have included 110 Nobel Prize winners and five American presidents. The 2010 version of the 15th edition, which spans 32 volumes[1] and 32,640 pages, was the last printed edition; digital content and distribution has continued since then.		The Britannica is the oldest English-language encyclopaedia still in production. It was first published between 1768 and 1771 in the Scottish capital of Edinburgh, as three volumes. The encyclopaedia grew in size: the second edition was 10 volumes, and by its fourth edition (1801–1810) it had expanded to 20 volumes. Its rising stature as a scholarly work helped recruit eminent contributors, and the 9th (1875–1889) and 11th editions (1911) are landmark encyclopaedias for scholarship and literary style. Beginning with the 11th edition and following its acquisition by an American firm, the Britannica shortened and simplified articles to broaden its appeal to the North American market. In 1933, the Britannica became the first encyclopaedia to adopt "continuous revision", in which the encyclopaedia is continually reprinted, with every article updated on a schedule. In March 2012, Encyclopædia Britannica, Inc. announced it would no longer publish printed editions, and would focus instead on Encyclopædia Britannica Online.		The 15th edition has a three-part structure: a 12-volume Micropædia of short articles (generally fewer than 750 words), a 17-volume Macropædia of long articles (two to 310 pages), and a single Propædia volume to give a hierarchical outline of knowledge. The Micropædia is meant for quick fact-checking and as a guide to the Macropædia; readers are advised to study the Propædia outline to understand a subject's context and to find more detailed articles. Over 70 years, the size of the Britannica has remained steady, with about 40 million words on half a million topics. Though published in the United States since 1901, the Britannica has for the most part maintained British English spelling.						Since 1985, the Britannica has had four parts: the Micropædia, the Macropædia, the Propædia, and a two-volume index. The Britannica's articles are found in the Micro- and Macropædia, which encompass 12 and 17 volumes, respectively, each volume having roughly one thousand pages. The 2007 Macropædia has 699 in-depth articles, ranging in length from 2 to 310 pages and having references and named contributors. In contrast, the 2007 Micropædia has roughly 65,000 articles, the vast majority (about 97%) of which contain fewer than 750 words, no references, and no named contributors.[2] The Micropædia articles are intended for quick fact-checking and to help in finding more thorough information in the Macropædia. The Macropædia articles are meant both as authoritative, well-written articles on their subjects and as storehouses of information not covered elsewhere.[3] The longest article (310 pages) is on the United States, and resulted from the merger of the articles on the individual states. The 2013 edition of Britannica contained approximately forty thousand articles.[4]		Information can be found in the Britannica by following the cross-references in the Micropædia and Macropædia; however, these are sparse, averaging one cross-reference per page.[5] Hence, readers are recommended to consult instead the alphabetical index or the Propædia, which organises the Britannica's contents by topic.[6]		The core of the Propædia is its "Outline of Knowledge", which aims to provide a logical framework for all human knowledge.[7] Accordingly, the Outline is consulted by the Britannica's editors to decide which articles should be included in the Micro- and Macropædia.[7] The Outline is also intended to be a study guide, to put subjects in their proper perspective, and to suggest a series of Britannica articles for the student wishing to learn a topic in depth.[7] However, libraries have found that it is scarcely used, and reviewers have recommended that it be dropped from the encyclopaedia.[8] The Propædia also has color transparencies of human anatomy and several appendices listing the staff members, advisors, and contributors to all three parts of the Britannica.		Taken together, the Micropædia and Macropædia comprise roughly 40 million words and 24,000 images.[6] The two-volume index has 2,350 pages, listing the 228,274 topics covered in the Britannica, together with 474,675 subentries under those topics.[5] The Britannica generally prefers British spelling over American;[5] for example, it uses colour (not color), centre (not center), and encyclopaedia (not encyclopedia). However, there are exceptions to this rule, such as defense rather than defence.[9] Common alternative spellings are provided with cross-references such as "Color: see Colour."		Since 1936, the articles of the Britannica have been revised on a regular schedule, with at least 10% of them considered for revision each year.[5][10] According to one Britannica website, 46% of its articles were revised over the past three years;[11] however, according to another Britannica web-site, only 35% of the articles were revised.[12]		The alphabetisation of articles in the Micropædia and Macropædia follows strict rules.[13] Diacritical marks and non-English letters are ignored, while numerical entries such as "1812, War of" are alphabetised as if the number had been written out ("Eighteen-twelve, War of"). Articles with identical names are ordered first by persons, then by places, then by things. Rulers with identical names are organised first alphabetically by country and then by chronology; thus, Charles III of France precedes Charles I of England, listed in Britannica as the ruler of Great Britain and Ireland. (That is, they are alphabetised as if their titles were "Charles, France, 3" and "Charles, Great Britain and Ireland, 1".) Similarly, places that share names are organised alphabetically by country, then by ever-smaller political divisions.		In March 2012, the company announced that the 2010 edition would be the last printed version. This was announced as a move by the company to adapt to the times and focus on its future using digital distribution.[14] The peak year for the printed encyclopaedia was 1990 when 120,000 sets were sold, but it dropped to 40,000 in 1996.[15] 12,000 sets of the 2010 edition were printed, of which 8,000 had been sold as of 2012[update].[16] By late April 2012, the remaining copies of the 2010 edition had sold out at Britannica's online store. As of 2016, a replica of Britannica's 1768 first edition is sold on the online store.[17]		Britannica Junior was first published in 1934 as 12 volumes. It was expanded to 15 volumes in 1947, and renamed Britannica Junior Encyclopædia in 1963.[18] It was taken off the market after the 1984 printing.		A British Children's Britannica edited by John Armitage was issued in London in 1960.[19] Its contents were determined largely by the eleven-plus standardised tests given in Britain.[20] Britannica introduced the Children's Britannica to the U.S. market in 1988, aimed at ages 7 to 14.		In 1961 a 16 volume Young Children's Encyclopaedia was issued for children just learning to read.[20]		My First Britannica is aimed at children ages six to twelve, and the Britannica Discovery Library is for children aged three to six (issued 1974 to 1991).[21]		There have been and are several abridged Britannica encyclopaedias. The single-volume Britannica Concise Encyclopædia has 28,000 short articles condensing the larger 32-volume Britannica;[22] there are authorized translations in languages such as Chinese[23] and Vietnamese.[24][25] Compton's by Britannica, first published in 2007, incorporating the former Compton's Encyclopedia, is aimed at 10- to 17-year-olds and consists of 26 volumes and 11,000 pages.[26]		Since 1938, Encyclopædia Britannica, Inc. has published annually a Book of the Year covering the past year's events. A given edition of the Book of the Year is named in terms of the year of its publication, though the edition actually covers the events of the previous year. Articles dating back to the 1994 edition are included online.[27] The company also publishes several specialised reference works, such as Shakespeare: The Essential Guide to the Life and Works of the Bard (Wiley, 2006).		The Britannica Ultimate Reference Suite 2012 DVD contains over 100,000 articles.[28] This includes regular Britannica articles, as well as others drawn from the Britannica Student Encyclopædia, and the Britannica Elementary Encyclopædia. The package includes a range of supplementary content including maps, videos, sound clips, animations and web links. It also offers study tools and dictionary and thesaurus entries from Merriam-Webster.		Britannica Online is a website with more than 120,000 articles and is updated regularly.[29] It has daily features, updates and links to news reports from The New York Times and the BBC. As of 2009[update], roughly 60% of Encyclopædia Britannica's revenue came from online operations, of which around 15% came from subscriptions to the consumer version of the websites.[30] As of 2006[update], subscriptions were available on a yearly, monthly or weekly basis.[31] Special subscription plans are offered to schools, colleges and libraries; such institutional subscribers constitute an important part of Britannica's business. Beginning in early 2007, the Britannica made articles freely available if they are hyperlinked from an external site. Non-subscribers are served pop-ups and advertising.[32]		On 20 February 2007, Encyclopædia Britannica, Inc. announced that it was working with mobile phone search company AskMeNow to launch a mobile encyclopaedia.[33] Users will be able to send a question via text message, and AskMeNow will search Britannica's 28,000-article concise encyclopaedia to return an answer to the query. Daily topical features sent directly to users' mobile phones are also planned. On 3 June 2008, an initiative to facilitate collaboration between online expert and amateur scholarly contributors for Britannica's online content (in the spirit of a wiki), with editorial oversight from Britannica staff, was announced.[34][35] Approved contributions would be credited,[36] though contributing automatically grants Encyclopædia Britannica, Inc. perpetual, irrevocable license to those contributions.[37]		On 22 January 2009, Britannica's president, Jorge Cauz, announced that the company would be accepting edits and additions to the online Britannica website from the public. The published edition of the encyclopaedia will not be affected by the changes.[38] Individuals wishing to edit the Britannica website will have to register under their real name and address prior to editing or submitting their content.[39] All edits submitted will be reviewed and checked and will have to be approved by the encyclopaedia's professional staff.[39] Contributions from non-academic users will sit in a separate section from the expert-generated Britannica content,[40] as will content submitted by non-Britannica scholars.[41] Articles written by users, if vetted and approved, will also only be available in a special section of the website, separate from the professional articles.[38][41] Official Britannica material would carry a "Britannica Checked" stamp, to distinguish it from the user-generated content.[42]		On 14 September 2010, Encyclopædia Britannica, Inc. announced a partnership with mobile phone development company Concentric Sky to launch a series of iPhone products aimed at the K-12 market.[43] On 20 July 2011, Encyclopædia Britannica, Inc. announced that Concentric Sky had ported the Britannica Kids product line to Intel's Intel Atom-based Netbooks[44][45] and on 26 October 2011 that it had launched its encyclopedia as an iPad app.[46] In 2010, Brittannica released Brittanica ImageQuest, a database of images.[47]		In March 2012, it was announced that the company would cease printing the encyclopaedia set, and that it would focus more on its online version.		The 2007 print version of the Britannica has 4,411 contributors, many eminent in their fields, such as Nobel laureate economist Milton Friedman, astronomer Carl Sagan, and surgeon Michael DeBakey.[48] Roughly a quarter of the contributors are deceased, some as long ago as 1947 (Alfred North Whitehead), while another quarter are retired or emeritus. Most (approximately 98%) contribute to only a single article; however, 64 contributed to three articles, 23 contributed to four articles, 10 contributed to five articles, and 8 contributed to more than five articles. An exceptionally prolific contributor is Christine Sutton of the University of Oxford, who contributed 24 articles on particle physics.		While Britannica's authors have included writers such as Albert Einstein, Marie Curie, and Leon Trotsky, as well as notable independent encyclopaedists such as Isaac Asimov, some have been criticised for lack of expertise:[49]		With a temerity almost appalling, [the Britannica contributor, Mr. Philips] ranges over nearly the whole field of European history, political, social, ecclesiastical... The grievance is that [this work] lacks authority. This, too—this reliance on editorial energy instead of on ripe special learning—may, alas, be also counted an "Americanizing": for certainly nothing has so cheapened the scholarship of our American encyclopaedias.		As of 2007[update] in the fifteenth edition of Britannica, Dale Hoiberg, a sinologist, was listed as Britannica's Senior Vice President and editor-in-chief.[50] Among his predecessors as editors-in-chief were Hugh Chisholm (1902–1924), James Louis Garvin (1926–1932), Franklin Henry Hooper (1932–1938),[51] Walter Yust (1938–1960), Harry Ashmore (1960–1963), Warren E. Preece (1964–1968, 1969–1975), Sir William Haley (1968–1969), Philip W. Goetz (1979–1991),[3] and Robert McHenry (1992–1997).[52] As of 2007[update] Anita Wolff was listed as the Deputy Editor and Theodore Pappas as Executive Editor.[50] Prior Executive Editors include John V. Dodge (1950–1964) and Philip W. Goetz.		Paul T. Armstrong remains the longest working employee of Encyclopædia Britannica. He began his career there in 1934, eventually earning the positions of Treasurer, Vice President, and Chief Financial Officer in his 58 years with the company, before retiring in 1992.[53]		The 2007 editorial staff of the Britannica included five Senior Editors and nine Associate Editors, supervised by Dale Hoiberg and four others. The editorial staff helped to write the articles of the Micropædia and some sections of the Macropædia.[54] The preparation and publication of the Encyclopædia Britannica required trained staff. According to the final page of the 2007 Propædia, the staff were organised into ten departments:[55]		Some of these departments were organised hierarchically. For example, the copy editors were divided into 4 copy editors, 2 senior copy editors, 4 supervisors, plus a coordinator and a director. Similarly, the Editorial department was headed by Dale Hoiberg and assisted by four others; they oversaw the work of five senior editors, nine associate editors, and one executive assistant.		The Britannica has an Editorial Board of Advisors, which includes 12 distinguished scholars:[56][57] non-fiction author Nicholas Carr, religion scholar Wendy Doniger, political economist Benjamin M. Friedman, Council on Foreign Relations President Emeritus Leslie H. Gelb, computer scientist David Gelernter, Physics Nobel laureate Murray Gell-Mann, Carnegie Corporation of New York President Vartan Gregorian, philosopher Thomas Nagel, cognitive scientist Donald Norman, musicologist Don Michael Randel, Stewart Sutherland, Baron Sutherland of Houndwood, President of the Royal Society of Edinburgh, and cultural anthropologist Michael Wesch.		The Propædia and its Outline of Knowledge were produced by dozens of editorial advisors under the direction of Mortimer J. Adler.[58] Roughly half of these advisors have since died, including some of the Outline's chief architects — Rene Dubos (d. 1982), Loren Eiseley (d. 1977), Harold D. Lasswell (d. 1978), Mark Van Doren (d. 1972), Peter Ritchie Calder (d. 1982) and Mortimer J. Adler (d. 2001). The Propædia also lists just under 4,000 advisors who were consulted for the unsigned Micropædia articles.[59]		In January 1996, the Britannica was purchased from the Benton Foundation by billionaire Swiss financier Jacqui Safra,[60] who serves as its current Chair of the Board. In 1997, Don Yannias, a long-time associate and investment advisor of Safra, became CEO of Encyclopædia Britannica, Inc.[61] In 1999, a new company, Britannica.com Inc., was created to develop digital versions of the Britannica; Yannias assumed the role of CEO in the new company, while his former position at the parent company remained vacant for two years. Yannias' tenure at Britannica.com Inc. was marked by missteps, considerable lay-offs, and financial losses.[62] In 2001, Yannias was replaced by Ilan Yeshua, who reunited the leadership of the two companies.[63] Yannias later returned to investment management, but remains on the Britannica′s Board of Directors.		In 2003, former management consultant Jorge Aguilar-Cauz was appointed President of Encyclopædia Britannica, Inc. Cauz is the senior executive and reports directly to the Britannica's Board of Directors. Cauz has been pursuing alliances with other companies and extending the Britannica brand to new educational and reference products, continuing the strategy pioneered by former CEO Elkan Harrison Powell in the mid-1930s.[64]		Under Safra's ownership, the company has experienced financial difficulties, and has responded by reducing the price of its products and implementing drastic cost cuts. According to a 2003 report in the New York Post, the Britannica management has eliminated employee 401(k) accounts and encouraged the use of free images. These changes have had negative impacts, as freelance contributors have waited up to six months for checks and the Britannica staff have gone years without pay rises.[65]		As the Britannica is a general encyclopaedia, it does not seek to compete with specialised encyclopaedias such as the Encyclopaedia of Mathematics or the Dictionary of the Middle Ages, which can devote much more space to their chosen topics. In its first years, the Britannica's main competitor was the general encyclopaedia of Ephraim Chambers and, soon thereafter, Rees's Cyclopædia and Coleridge's Encyclopædia Metropolitana. In the 20th century, successful competitors included Collier's Encyclopedia, the Encyclopedia Americana, and the World Book Encyclopedia. Nevertheless, from the 9th edition onwards, the Britannica was widely considered to have the greatest authority of any general English language encyclopaedia,[66] especially because of its broad coverage and eminent authors.[3][5] The print version of the Britannica was significantly more expensive than its competitors.[3][5]		Since the early 1990s, the Britannica has faced new challenges from digital information sources. The Internet, facilitated by the development of search engines, has grown into a common source of information for many people, and provides easy access to reliable original sources and expert opinions, thanks in part to initiatives such as Google Books, MIT's release of its educational materials and the open PubMed Central library of the National Library of Medicine.[67][68] In general, the Internet tends to provide more current coverage than print media, due to the ease with which material on the Internet can be updated.[69] In rapidly changing fields such as science, technology, politics, culture and modern history, the Britannica has struggled to stay up-to-date, a problem first analysed systematically by its former editor Walter Yust.[70] Eventually, the Britannica turned to focus more on its online edition.		The Encyclopædia Britannica has been compared with other print encyclopaedias, both qualitatively and quantitatively.[2][3][5] A well-known comparison is that of Kenneth Kister, who gave a qualitative and quantitative comparison of the Britannica with two comparable encyclopaedias, Collier's Encyclopedia and the Encyclopedia Americana.[3] For the quantitative analysis, ten articles were selected at random—circumcision, Charles Drew, Galileo, Philip Glass, heart disease, IQ, panda bear, sexual harassment, Shroud of Turin and Uzbekistan—and letter grades of A–D or F were awarded in four categories: coverage, accuracy, clarity, and recency. In all four categories and for all three encyclopaedias, the four average grades fell between B− and B+, chiefly because none of the encyclopaedias had an article on sexual harassment in 1994. In the accuracy category, the Britannica received one "D" and seven "A"s, Encyclopedia Americana received eight "A"s, and Collier's received one "D" and seven "A"s; thus, Britannica received an average score of 92% for accuracy to Americana's 95% and Collier's 92%. In the timeliness category, Britannica averaged an 86% to Americana's 90% and Collier's 85%.		The most notable competitor of the Britannica among CD/DVD-ROM digital encyclopaedias was Encarta,[71] now discontinued, a modern, multimedia encyclopaedia that incorporated three print encyclopaedias: Funk & Wagnalls, Collier's and the New Merit Scholar's Encyclopedia. Encarta was the top-selling multimedia encyclopaedia, based on total US retail sales from January 2000 to February 2006.[72] Both occupied the same price range, with the 2007 Encyclopædia Britannica Ultimate CD or DVD costing US$40–50[73][74] and the Microsoft Encarta Premium 2007 DVD costing US$45.[75] The Britannica contains 100,000 articles and Merriam-Webster's Dictionary and Thesaurus (US only), and offers Primary and Secondary School editions.[74] Encarta contained 66,000 articles, a user-friendly Visual Browser, interactive maps, math, language and homework tools, a US and UK dictionary, and a youth edition.[75] Like Encarta, the Britannica has been criticised for being biased towards United States audiences; the United Kingdom-related articles are updated less often, maps of the United States are more detailed than those of other countries, and it lacks a UK dictionary.[71] Like the Britannica, Encarta was available online by subscription, although some content could be accessed free.[76]		The dominant internet encyclopaedia and main alternative to Britannica is the multilingual, Web-based, free-content encyclopaedia, Wikipedia.[77][78] The key differences between the two lie in accessibility; the model of participation they bring to an encyclopedic project; their respective style sheets and editorial policies; relative ages; the number of subjects treated; the number of languages in which articles are written and made available; and their underlying economic models: unlike Britannica, Wikipedia is a not-for-profit and is not connected with traditional profit- and contract-based publishing distribution networks.		The 699 printed Macropædia articles are generally written by identified contributors, and the roughly 65,000 printed Micropædia articles are the work of the editorial staff and identified outside consultants. Thus, a Britannica article either has known authorship or a set of possible authors (the editorial staff). With the exception of the editorial staff, most of the Britannica's contributors are experts in their field—some are Nobel laureates.[48] By contrast, the articles of Wikipedia are written by people of unknown degrees of expertise: most do not claim any particular expertise, and of those who do, many are anonymous and have no verifiable credentials.[79] It is for this lack of institutional vetting, or certification, that former Britannica editor-in-chief Robert McHenry notes his belief that Wikipedia cannot hope to rival the Britannica in accuracy.[80]		In 2005, the journal Nature chose articles from both websites in a wide range of science topics and sent them to what it called "relevant" field experts for peer review. The experts then compared the competing articles—one from each site on a given topic—side by side, but were not told which article came from which site. Nature got back 42 usable reviews.		In the end, the journal found just eight serious errors, such as general misunderstandings of vital concepts: four from each site. It also discovered many factual errors, omissions or misleading statements: 162 in Wikipedia and 123 in Britannica, an average of 3.86 mistakes per article for Wikipedia and 2.92 for Britannica.[79][81] Although Britannica was revealed as the more accurate encyclopedia, with lesser errors, Encyclopædia Britannica, Inc. in its detailed 20-page rebuttal called Nature's study flawed and misleading[82] and called for a "prompt" retraction. It noted that two of the articles in the study were taken from a Britannica yearbook and not the encyclopaedia, and another two were from Compton's Encyclopedia (called the Britannica Student Encyclopedia on the company's website). The rebuttal went on to mention that some of the articles presented to reviewers were combinations of several articles, and that other articles were merely excerpts but were penalised for factual omissions. The company also noted that several of what Nature called errors were minor spelling variations, and that others were matters of interpretation. Nature defended its story and declined to retract, stating that, as it was comparing Wikipedia with the web version of Britannica, it used whatever relevant material was available on Britannica's website.[83]		Interviewed in February 2009, the managing director of Britannica UK said:		Wikipedia is a fun site to use and has a lot of interesting entries on there, but their approach wouldn't work for Encyclopædia Britannica. My job is to create more awareness of our very different approaches to publishing in the public mind. They're a chisel, we're a drill, and you need to have the correct tool for the job.[30]		Since the 3rd edition, the Britannica has enjoyed a popular and critical reputation for general excellence.[2][3][5] The 3rd and the 9th editions were pirated for sale in the United States,[84] beginning with Dobson's Encyclopaedia.[85] On the release of the 14th edition, Time magazine dubbed the Britannica the "Patriarch of the Library".[86] In a related advertisement, naturalist William Beebe was quoted as saying that the Britannica was "beyond comparison because there is no competitor."[87] References to the Britannica can be found throughout English literature, most notably in one of Sir Arthur Conan Doyle's favourite Sherlock Holmes stories, "The Red-Headed League". The tale was highlighted by the Lord Mayor of London, Gilbert Inglefield, at the bicentennial of the Britannica.[88]		The Britannica has a reputation for summarising knowledge.[66] To further their education, some people have devoted themselves to reading the entire Britannica, taking anywhere from three to 22 years to do so.[84] When Fat'h Ali became the Shah of Persia in 1797, he was given a set of the Britannica's 3rd edition, which he read completely; after this feat, he extended his royal title to include "Most Formidable Lord and Master of the Encyclopædia Britannica".[88] Writer George Bernard Shaw claimed to have read the complete 9th edition—except for the science articles[84]—and Richard Evelyn Byrd took the Britannica as reading material for his five-month stay at the South Pole in 1934, while Philip Beaver read it during a sailing expedition. More recently, A.J. Jacobs, an editor at Esquire magazine, read the entire 2002 version of the 15th edition, describing his experiences in the well-received 2004 book, The Know-It-All: One Man's Humble Quest to Become the Smartest Person in the World. Only two people are known to have read two independent editions: the author C. S. Forester[84] and Amos Urban Shirk, an American businessman, who read the 11th and 14th editions, devoting roughly three hours per night for four and a half years to read the 11th.[89] Several editors-in-chief of the Britannica are likely to have read their editions completely, such as William Smellie (1st edition), William Robertson Smith (9th edition), and Walter Yust (14th edition).		The CD/DVD-ROM version of the Britannica, Encyclopædia Britannica Ultimate Reference Suite, received the 2004 Distinguished Achievement Award from the Association of Educational Publishers.[90] On 15 July 2009, Encyclopædia Britannica was awarded a spot as one of "Top Ten Superbrands in the UK" by a panel of more than 2,000 independent reviewers, as reported by the BBC.[91]		Topics are chosen in part by reference to the Propædia "Outline of Knowledge".[7] The bulk of the Britannica is devoted to geography (26% of the Macropædia), biography (14%), biology and medicine (11%), literature (7%), physics and astronomy (6%), religion (5%), art (4%), Western philosophy (4%), and law (3%).[3] A complementary study of the Micropædia found that geography accounted for 25% of articles, science 18%, social sciences 17%, biography 17%, and all other humanities 25%.[5] Writing in 1992, one reviewer judged that the "range, depth, and catholicity of coverage [of the Britannica] are unsurpassed by any other general Encyclopaedia."[92]		The Britannica does not cover topics in equivalent detail; for example, the whole of Buddhism and most other religions is covered in a single Macropædia article, whereas 14 articles are devoted to Christianity, comprising nearly half of all religion articles.[93] However, the Britannica has been lauded as the least biased of general Encyclopaedias marketed to Western readers[3] and praised for its biographies of important women of all eras.[5]		It can be stated without fear of contradiction that the 15th edition of the Britannica accords non-Western cultural, social, and scientific developments more notice than any general English-language encyclopedia currently on the market.		On rare occasions, the Britannica was criticised for its editorial choices. Given its roughly constant size, the encyclopaedia has needed to reduce or eliminate some topics to accommodate others, resulting in controversial decisions. The initial 15th edition (1974–1985) was faulted for having reduced or eliminated coverage of children's literature, military decorations, and the French poet Joachim du Bellay; editorial mistakes were also alleged, such as inconsistent sorting of Japanese biographies.[94] Its elimination of the index was condemned, as was the apparently arbitrary division of articles into the Micropædia and Macropædia.[3][95] Summing up, one critic called the initial 15th edition a "qualified failure...[that] cares more for juggling its format than for preserving."[94] More recently, reviewers from the American Library Association were surprised to find that most educational articles had been eliminated from the 1992 Macropædia, along with the article on psychology.[8]		Some very few Britannica-appointed contributors are mistaken. A notorious instance from the Britannica's early years is the rejection of Newtonian gravity by George Gleig, the chief editor of the 3rd edition (1788–1797), who wrote that gravity was caused by the classical element of fire.[84] The Britannica has also staunchly defended a scientific approach to cultural topics, as it did with William Robertson Smith's articles on religion in the 9th edition, particularly his article stating that the Bible was not historically accurate (1875).[84]		The Britannica has received criticism, especially as editions become outdated. It is expensive to produce a completely new edition of the Britannica,[96] and its editors delay for as long as fiscally sensible (usually about 25 years).[10] For example, despite continuous revision, the 14th edition became outdated after 35 years (1929–1964). When American physicist Harvey Einbinder detailed its failings in his 1964 book, The Myth of the Britannica,[97] the encyclopaedia was provoked to produce the 15th edition, which required 10 years of work.[3] It is still difficult to keep the Britannica current; one recent critic writes, "it is not difficult to find articles that are out-of-date or in need of revision", noting that the longer Macropædia articles are more likely to be outdated than the shorter Micropædia articles.[3] Information in the Micropædia is sometimes inconsistent with the corresponding Macropædia article(s), mainly because of the failure to update one or the other.[2][5] The bibliographies of the Macropædia articles have been criticised for being more out-of-date than the articles themselves.[2][3][5]		In 2010 an inaccurate entry about the Irish civil war was discussed in the Irish press following a decision of the Department of Education and Science to pay for online access.[98][99]		Speaking of the 3rd edition (1788–1797), Britannica's chief editor George Gleig wrote that "perfection seems to be incompatible with the nature of works constructed on such a plan, and embracing such a variety of subjects."[100] In March 2006, the Britannica wrote, "we in no way mean to imply that Britannica is error-free; we have never made such a claim."[82] The sentiment is expressed by its original editor, William Smellie:		With regard to errors in general, whether falling under the denomination of mental, typographical or accidental, we are conscious of being able to point out a greater number than any critic whatever. Men who are acquainted with the innumerable difficulties of attending the execution of a work of such an extensive nature will make proper allowances. To these we appeal, and shall rest satisfied with the judgment they pronounce.		However, Jorge Cauz (president of Encyclopædia Britannica Inc.) asserted in 2012 that "Britannica […] will always be factually correct."[1]		Past owners have included, in chronological order, the Edinburgh, Scotland printers Colin Macfarquhar and Andrew Bell, Scottish bookseller Archibald Constable, Scottish publisher A & C Black, Horace Everett Hooper, Sears Roebuck and William Benton.		The present owner of Encyclopædia Britannica Inc. is Jacqui Safra, a Swiss billionaire and actor. Recent advances in information technology and the rise of electronic encyclopaedias such as Encyclopædia Britannica Ultimate Reference Suite, Encarta and Wikipedia have reduced the demand for print encyclopaedias.[101] To remain competitive, Encyclopædia Britannica, Inc. has stressed the reputation of the Britannica, reduced its price and production costs, and developed electronic versions on CD-ROM, DVD, and the World Wide Web. Since the early 1930s, the company has promoted spin-off reference works.[10]		The Britannica has been issued in 15 editions, with multi-volume supplements to the 3rd and 4th editions (see the Table below). The 5th and 6th editions were reprints of the 4th, the 10th edition was only a supplement to the 9th, just as the 12th and 13th editions were supplements to the 11th. The 15th underwent massive re-organisation in 1985, but the updated, current version is still known as the 15th. The 14th and 15th editions were edited every year throughout their runs, so that later printings of each were entirely different from early ones.		Throughout history, the Britannica has had two aims: to be an excellent reference book, and to provide educational material.[102] In 1974, the 15th edition adopted a third goal: to systematise all human knowledge.[7] The history of the Britannica can be divided into five eras, punctuated by changes in management, or re-organisation of the dictionary.		In the first era (1st–6th editions, 1768–1826), the Britannica was managed and published by its founders, Colin Macfarquhar and Andrew Bell, by Archibald Constable, and by others. The Britannica was first published between 1768 and 1771 in Edinburgh as the Encyclopædia Britannica, or, A Dictionary of Arts and Sciences, compiled upon a New Plan. In part, it was conceived in reaction to the French Encyclopédie of Denis Diderot and Jean le Rond d'Alembert (published 1751–72), which had been inspired by Chambers's Cyclopaedia (first edition 1728).		The Britannica of this period was primarily a Scottish enterprise, and it is one of the most enduring legacies of the Scottish Enlightenment.[103] In this era, the Britannica moved from being a three-volume set (1st edition) compiled by one young editor—William Smellie[104]—to a 20-volume set written by numerous authorities. Several other encyclopaedias competed throughout this period, among them editions of Abraham Rees's Cyclopædia and Coleridge's Encyclopædia Metropolitana[105] and David Brewster's Edinburgh Encyclopædia.		During the second era (7th–9th editions, 1827–1901), the Britannica was managed by the Edinburgh publishing firm A & C Black. Although some contributors were again recruited through friendships of the chief editors, notably Macvey Napier, others were attracted by the Britannica's reputation. The contributors often came from other countries and included the world's most respected authorities in their fields. A general index of all articles was included for the first time in the 7th edition, a practice maintained until 1974.		Production of the 9th edition was overseen by Thomas Spencer Baynes, the first English-born editor-in-chief. Dubbed the "Scholar's Edition", the 9th edition is the most scholarly of all Britannicas.[3][84] After 1880, Baynes was assisted by William Robertson Smith.[106] No biographies of living persons were included.[107] James Clerk Maxwell and Thomas Huxley were special advisors on science.[108] However, by the close of the 19th century, the 9th edition was outdated, and the Britannica faced financial difficulties.		In the third era (10th–14th editions, 1901–73), the Britannica was managed by American businessmen who introduced direct marketing and door-to-door sales. The American owners gradually simplified articles, making them less scholarly for a mass market. The 10th edition was a nine-volume supplement to the 9th, but the 11th edition was a completely new work, and is still praised for excellence; its owner, Horace Hooper, lavished enormous effort on its perfection.[84]		When Hooper fell into financial difficulties, the Britannica was managed by Sears Roebuck for 18 years (1920–23, 1928–43). In 1932, the vice-president of Sears, Elkan Harrison Powell, assumed presidency of the Britannica; in 1936, he began the policy of continuous revision. This was a departure from earlier practice, in which the articles were not changed until a new edition was produced, at roughly 25-year intervals, some articles unchanged from earlier editions.[10] Powell developed new educational products that built upon the Britannica's reputation.		In 1943, Sears donated the Encyclopædia Britannica to the University of Chicago. William Benton, then a vice president of the University, provided the working capital for its operation. The stock was divided between Benton and the University, with the University holding an option on the stock.[109] Benton became Chairman of the Board and managed the Britannica until his death in 1973.[110] Benton set up the Benton Foundation, which managed the Britannica until 1996. In 1968, near the end of this era, the Britannica celebrated its bicentennial.		In the fourth era (1974–94), the Britannica introduced its 15th edition, which was re-organised into three parts: the Micropædia, the Macropædia, and the Propædia. Under Mortimer J. Adler (member of the Board of Editors of Encyclopædia Britannica since its inception in 1949, and its chair from 1974; director of editorial planning for the 15th edition of Britannica from 1965),[111] the Britannica sought not only to be a good reference work and educational tool, but to systematise all human knowledge. The absence of a separate index and the grouping of articles into parallel encyclopaedias (the Micro- and Macropædia) provoked a "firestorm of criticism" of the initial 15th edition.[3][95] In response, the 15th edition was completely re-organised and indexed for a re-release in 1985. This second version of the 15th edition continued to be published and revised until the 2010 print version. The official title of the 15th edition is the New Encyclopædia Britannica, although it has also been promoted as Britannica 3.[3]		On 9 March 1976 the U.S. Federal Trade Commission entered an opinion and order enjoining Encyclopædia Britannica, Inc. from using: a) deceptive advertising practices in recruiting sales agents and obtaining sales leads, and b) deceptive sales practices in the door-to-door presentations of its sales agents.[112]		In the fifth era (1994–present), digital versions have been developed and released on optical media and online. In 1996, the Britannica was bought by Jacqui Safra at well below its estimated value, owing to the company's financial difficulties. Encyclopædia Britannica, Inc. split in 1999. One part retained the company name and developed the print version, and the other, Britannica.com Inc., developed digital versions. Since 2001, the two companies have shared a CEO, Ilan Yeshua, who has continued Powell's strategy of introducing new products with the Britannica name. In March 2012, Britannica's president, Jorge Cauz, announced that it would not produce any new print editions of the encyclopaedia, with the 2010 15th edition being the last. The company will focus only on the online edition and other educational tools.[1][113]		Britannica's final print edition was in 2010, a 32-volume set.[1] Britannica Global Edition was also printed in 2010. It contained 30 volumes and 18,251 pages, with 8,500 photographs, maps, flags, and illustrations in smaller "compact" volumes. It contained over 40,000 articles written by scholars from across the world, including Nobel Prize winners. Unlike the 15th edition, it did not contain Macro- and Micropædia sections, but ran A through Z as all editions up to the 14th had. The following is Britannica's description of the work:[4]		The editors of Encyclopædia Britannica, the world standard in reference since 1768, present the Britannica Global Edition. Developed specifically to provide comprehensive and global coverage of the world around us, this unique product contains thousands of timely, relevant, and essential articles drawn from the Encyclopædia Britannica itself, as well as from the Britannica Concise Encyclopedia, the Britannica Encyclopedia of World Religions, and Compton's by Britannica. Written by international experts and scholars, the articles in this collection reflect the standards that have been the hallmark of the leading English-language encyclopedia for over 240 years.		The Britannica was dedicated to the reigning British monarch from 1788 to 1901 and then, upon its sale to an American partnership, to the British monarch and the President of the United States.[3] Thus, the 11th edition is "dedicated by Permission to His Majesty George the Fifth, King of Great Britain and Ireland and of the British Dominions beyond the Seas, Emperor of India, and to William Howard Taft, President of the United States of America."[114] The order of the dedications has changed with the relative power of the United States and Britain, and with relative sales; the 1954 version of the 14th edition is "Dedicated by Permission to the Heads of the Two English-Speaking Peoples, Dwight David Eisenhower, President of the United States of America, and Her Majesty, Queen Elizabeth the Second."[70] Consistent with this tradition, the 2007 version of the current 15th edition was "dedicated by permission to the current President of the United States of America, George W. Bush, and Her Majesty, Queen Elizabeth II,"[115] while the 2010 version of the current 15th edition is "dedicated by permission to Barack Obama, President of the United States of America, and Her Majesty Queen Elizabeth II."[116]		1Supplement to the fourth, fifth, and sixth editions of the Encyclopædia Britannica. With preliminary dissertations on the history of the sciences.		2 The 7th to 14th editions included a separate index volume.		3 The 9th edition featured articles by notables of the day, such as James Maxwell on electricity and magnetism, and William Thomson (who became Lord Kelvin) on heat.		4 The 10th edition included a maps volume and a cumulative index volume for the 9th and 10th edition volumes: the new volumes, constituting, in combination with the existing volumes of the 9th ed., the 10th ed. ... and also supplying a new, distinctive, and independent library of reference dealing with recent events and developments		5 Vols. 30–32 ... the New volumes constituting, in combination with the twenty-nine volumes of the eleventh edition, the twelfth edition		6 This supplement replaced the previous supplement: The three new supplementary volumes constituting, with the volumes of the latest standard edition, the thirteenth edition.		7 At this point Encyclopædia Britannica began almost annual revisions. New revisions of the 14th edition appeared every year between 1929 and 1973 with the exceptions of 1931, 1934 and 1935.[122]		8 Annual revisions were published every year between 1974 and 2007 with the exceptions of 1996, 1999, 2000, 2004 and 2006.[122] The 15th edition (introduced as "Britannica 3") was published in three parts: a 10-volume Micropædia (which contained short articles and served as an index), a 19-volume Macropædia, plus the Propædia (see text).		9 In 1985, the system was modified by adding a separate two-volume index; the Macropædia articles were further consolidated into fewer, larger ones (for example, the previously separate articles about the 50 U.S. states were all included into the "United States of America" article), with some medium-length articles moved to the Micropædia. The Micropædia had 12 vols. and the Macropædia 17.		The first CD-ROM edition was issued in 1994. At that time also an online version was offered for paid subscription. In 1999 this was offered free, and no revised print versions appeared. The experiment was ended in 2001 and a new printed set was issued in 2001.		
A toast is a ritual in which a drink is taken as an expression of honor or goodwill. The term may be applied to the person or thing so honored, the drink taken, or the verbal expression accompanying the drink. Thus, a person could be "the toast of the evening," for whom someone "proposes a toast" to congratulate and for whom a third person "toasts" in agreement. The ritual forms the basis of the literary and performance genre, of which Mark Twain's "To the Babies" is a well-known example.[1]		The toast as described here is rooted in Western culture, but certain cultures outside that sphere have their own traditions in which consuming a drink is connected with ideas of celebration and honor. While the physical and verbal ritual of the toast may be elaborate and formal, merely raising one's glass towards someone or something and then drinking is essentially a toast as well, the message being one of goodwill towards the person or thing indicated.						According to various apocryphal stories, the custom of touching glasses evolved from concerns about poisoning. By one account, clinking glasses together would cause each drink to spill over into the others' (though there is no real evidence for such an origin).[2] According to other stories, the word toast became associated with the custom in the 17th century, based on a custom of flavoring drinks with spiced toast. The word originally referred to the lady in whose honor the drink was proposed, her name being seen as figuratively flavoring the drink.[3][4] The International Handbook on Alcohol and Culture says toasting "is probably a secular vestige of ancient sacrificial libations in which a sacred liquid was offered to the gods: blood or wine in exchange for a wish, a prayer summarized in the words 'long life!' or 'to your health!'"[5]		Toasts are generally offered at times of celebration or commemoration, including certain holidays, such as New Year's Eve. Other occasions include retirement celebrations, housewarming parties, births, etc.[6] The protocol for toasting at weddings is comparatively elaborate and fixed. At a wedding reception, the father of the bride, in his role as host, regularly offers the first toast, thanking the guests for attending, offering tasteful remembrances of the bride's childhood, and wishing the newlyweds a happy life together. The best man usually proposes a toast in the form of best wishes and congratulations to the newlyweds. A best man's toast takes the form of a short speech (3–5 minutes) that combines a mixture of humor and sincerity.[7] The humor often comes in the shape of the best man telling jokes at the groom's expense whilst the sincerity incorporates the praise and complimentary comments that a best man should make about the bride and groom, amongst others. The actual "toast" is then delivered at the end of the speech and is a short phrase wishing the newlyweds a happy, healthy, loving life together. The maid of honor may follow suit, appropriately tailoring her comments to the bride. The groom may offer the final toast, thanking the bride's parents for hosting the wedding, the wedding party for their participation, and finally dedicating the toast to the bridesmaids.[8]		Typical traditional wedding toasts include the following:[9]		(to the couple) Here's to your coffins May they be made of hundred-year-old oaks Which we shall plant tomorrow. May you both live as long as you want, and never want as long as you live May the best of your yesterdays be the worst of your tomorrows.		(to the bride) May I see you grey And combing your grandchildren's hair.		Toasts are also offered on patriotic occasions, as in the case of Stephen Decatur's famous "Our country! In our intercourse with foreign nations may we always be in the right; but our country, right or wrong." Equally traditional are satiric verses:		Here's to dear old Boston, The home of the bean and the cod, Where Lowells speak only to Cabots, And Cabots speak only to God.[10]		Toasts may be solemn, sentimental, humorous, even bawdy[11] or insulting.[12] The practice of announcing one's intention to make a toast and signalling for quiet by rapping on the wineglass, while common, is nonetheless[8] regarded by some authorities as rude. Except in very small and informal gatherings, a toast is offered standing. At a gathering, none should offer a toast to the guest of honor until the host has had the opportunity to do so. In English-speaking countries, guests may signal their approval of the toast by saying "hear hear."[13] The person honored should neither stand nor drink,[14] but after the toast should rise to thank the one who has offered the toast, perhaps but not necessarily offering a toast in turn. As toasts may occur in long series, experienced attendees often make sure to leave enough wine in the glass to allow participation in numerous toasts.[15]		Putting one's glass down before the toast is complete, or simply holding one's glass without drinking is widely regarded as impolite, suggesting that one does not share the benevolent sentiments expressed in the toast, nor the unity and fellowship implicit in toasting itself.[16] Even the non-drinker is counseled not to refuse to allow wine to be poured for a toast.[17] Inverting the glass is especially discouraged.[18]		Toasting traditionally involves alcoholic beverages.[19] Champagne (or at least some variety of sparkling wine) is regarded as especially festive and is widely associated with New Year's Eve and other celebrations.[20] Many people nowadays substitute sparkling fruit juice (often packaged in champagne-style bottles[21]), and many authorities consider it perfectly acceptable to participate in a toast while drinking water.[18] Toasting with an empty glass is also viewed as acceptable behavior for the non-drinker.[22]		Teetotalers may view the drinking of toasts to be abominable and incompatible with their stand, as witnessed by this narrative from The Teetotaler (1840):		At the anniversary of Cheshunt College, Sir Culling Eardley Smith was in the chair. This gentleman, after dinner, said "he had subscribed to the Teetotal Pledge, which of course was incompatible with the drinking of toasts;" when the Rev. J. Blackburn, (minister of Claremont Chapel, Pentonville,) said "he was not a teetotaler,--he was not in bondage,[23]--and on that subject he had very recently been preaching." What could the Rev. Gentleman mean by this, but that he had recently been preaching against Teetotalism? Let the Rev. Gentleman look at drinking customs and their enormous evils, and ask himself if he has done his duty; or whether he expects to be pronounced "a good an faithful servant," if he continues even from the pulpit to encourage the great damning evil of this nation. Mr. Donaldson said that he was happy to add, that one of the most popular ministers of the day, the Rev. J. Sherman, gave Mr. B. a pretty severe and well-merited reply, by saying, "His brother Blackburn had said, he (Mr. B.) was not in bondage; he must be allowed to say, that he rejoiced that he (Mr. S.) had been enabled to break through the old and stupid custom of washing down sentiments by draughts of intoxicating liquors. He had thus become a free man.[24]		Mr. Donaldson concluded with some very severe animadversions upon the infamous conduct of Mr. Blackburn.[25]		It is a superstition in the United States Navy that a toast is never to be made with water, since the person so honored will be doomed to a watery grave.[26] During a United States Air Force Dining In, all toasts are traditionally made with wine except for the final toast of the night made in honor of POWs/MIAs;[27] because these honorees did not have the luxury of wine while in captivity, the toast is made with water. Some versions of the protocol prescribe a toast in water for all deceased comrades.[13]		It is or was the custom on the (British) Royal Navy to drink toasts sitting, because in old-type wooden warships below decks there was not enough headroom to stand upright.		Prosit/Prost		Prosit is a Latin word from which the German short form "prost" is derived. It is a toast, that is an acclamation made before drinking an alcoholic beverage when drinkers chink glasses. The expression dates back to the beginning of the 18th century when it was used among university students and eventually made its way into every day language. In a ceremonious context and in connection with a short speech, the English word toast may also be used.		Origin of the Word		The word is, as mentioned above, of Latin origin and it comes from the verb "prodesse" (= "to benefit sth/sb", "to be beneficial"). Consequently, "prosit" is the conjugated form (3rd person Singular, Present Subjunctive, Active) and therefore an optative: "To you/ to your health". Like the colloquial "prost", "prosit" was originally used by university students.[28]		Usage		In German, synonyms like " Wohl bekomm's!" , "Zum Wohl!" and many versions from other languages may also be used instead of "prosit". The acclamation itself is also referred to as a "prosit". The verb form is "zuprosten", where the prefix "zu" means that the speech act is targeted at one or several people.		In the Swabian dialect, the word has the further meaning of a belch, called a "Prositle". The acclamation is followed by the chinking of glasses, often linked to other rules like making eye contact. This ritual is commonly attributed to a medieval custom, whereby one could avoid being poisoned by one's drinking companions, as a few drops of each beverage got mixed when chinking glasses. There is every likelihood that this did not work. It was much more effective for one table to share one or more drinking vessels, a procedure which was common for a long time.		In Danish, Swedish and Norwegian, prosit is a blessing used in response to a sneeze, in the same way the English expression "bless you" is used.		In Germany, toasting, not necessarily by words but usually just by touching each other's drinking vessels, is usually a very closely observed part of drinking culture. In private company, no one should drink a sip of alcohol before having toasted all the other people at the table. In doing this, it is very important to look directly into the other drinker's eyes. Not practising this is considered rude and often, humorously, believed to attract all kinds of bad luck (e.g. "seven years of bad luck" and the like).		In the British Royal Navy, the officers' noon mess typically began with the loyal toast, followed by a toast distinctive for the day of the week:		The sequence was also prescribed in at least one publication for the United States Navy.[30]		A toast might be spontaneous and free-form, a carefully planned original speech, or a recitation of traditional sentiments such as this Irish example:[31]		May the road rise to meet you. May the wind be always at your back. May the sun shine warm upon your face. And rains fall soft upon your fields. And until we meet again, May God hold you in the hollow of His hand.		An informal variation of the last 2 lines is:		"And may ye be in Heaven a half-hour afore the devil knows ye're dead!"		In many cultures, toasting is common and to not do so may be a breach of etiquette. The general theme of the common brief toast is "good luck" or "good health". At formal meals in certain countries of the Commonwealth of Nations, the first toast to be proposed is traditionally the Loyal Toast ("The Queen"). This may be adapted in other countries to give a loyal toast to the appropriate Head of State.[15]		Other examples include:		
Sakana (肴) or shukō (酒肴) is a Japanese term referring to food eaten as an accompaniment to alcohol as originated from the word saka (sake) and na (food). Sakana may also be referred to as otsumami (snack),[a] and especially dried fish and salted fish roe, were popular choice for these dishes, over the years the term sakana also came to mean "fish".						In Japan, when alcohol is consumed, it is customary that the drinks are always accompanied with some sort of foodstuff. The term sakana traditionally refer to food served to accompany sake. These are usually quite salty and served in relatively small portions. However, since the 19th century, the market share for Japanese beer has been expanded in Japan, which in 1959 overtook sake as the nation's most popular alcoholic beverage in taxable shipping volume,[3] and at the same time various foods designed to accompany beer have become popular. These dishes, served in restaurant-pubs known as izakaya, are usually more substantial than tapas although they are not considered a meal as such as they do not contain the all-important Japanese rice. Traditionally, the Japanese regarded sake, which is made from rice, as a substitute for white rice served in a standard Japanese meal, and as a result many Japanese do not eat rice and drink alcohol simultaneously.[original research?]		Listed below are some common sakana.		
Starvation is a severe deficiency in caloric energy intake needed to maintain an organism's life. It is the most extreme form of malnutrition. In humans, prolonged starvation can cause permanent organ damage[1] and eventually, death. The term inanition refers to the symptoms and effects of starvation. Starvation may also be used as a means of torture or execution.		According to the World Health Organization, hunger is the single gravest threat to the world's public health.[2] The WHO also states that malnutrition is by far the biggest contributor to child mortality, present in half of all cases.[2] Undernutrition is a contributory factor in the death of 3.1 million children under five every year.[3] Figures on actual starvation are difficult to come by, but according to the Food and Agriculture Organization, the less severe condition of undernourishment currently affects about 842 million people, or about one in eight (12.5%) people in the world population.[4]		The bloated stomach, as seen in the adjacent picture, represents a form of malnutrition called kwashiorkor which is caused by insufficient protein despite a sufficient caloric intake.[5] Children are more vulnerable to kwashiorkor, advanced symptoms of which include weight loss and muscle wasting.[5]						Causes of hunger are related to poverty. There are inter-related issues causing hunger, which are related to economics and other factors that cause poverty. They include land rights, and ownership, diversion of land use to non productive use, increasing emphasis on export oriented agriculture, inefficient agricultural practices, war, famine, drought, over fishing, poor crop yield, etc.		The basic cause of starvation is an imbalance between energy intake and energy expenditure. In other words, the body expends more energy than it takes in. This imbalance can arise from one or more medical conditions or circumstantial situations, which can include:		Medical reasons		Circumstantial causes		The main causes of starvation are as follows:		Early symptoms include impulsivity, irritability, hyperactivity, and other symptoms. Atrophy (wasting away) of the stomach weakens the perception of hunger, since the perception is controlled by the percentage of the stomach that is empty. Individuals experiencing starvation lose substantial fat (adipose tissue) and muscle mass as the body breaks down these tissues for energy.[6] Catabolysis is the process of a body breaking down its own muscles and other tissues in order to keep vital systems such as the nervous system and heart muscle (myocardium) functioning. The energy deficiency inherent in starvation causes fatigue and renders the victim more apathetic over time. As the starving person becomes too weak to move or even eat, their interaction with the surrounding world diminishes. In females, menstruation ceases when the body fat percentage is too low to support a fetus.		Victims of starvation are often too weak to sense thirst, and therefore become dehydrated. All movements become painful due to muscle atrophy and dry, cracked skin that is caused by severe dehydration. With a weakened body, diseases are commonplace. Fungi, for example, often grow under the esophagus, making swallowing painful. Vitamin deficiency is also a common result of starvation, often leading to anemia, beriberi, pellagra, and scurvy. These diseases collectively can also cause diarrhea, skin rashes, edema, and heart failure. Individuals are often irritable and lethargic as a result.		There is insufficient scientific data on exactly how long people can live without food.[7] Although the length of time varies with an individual's percentage of body fat and general health, one medical study estimates that in adults complete starvation leads to death within 8 to 12 weeks.[8] There are isolated cases of individuals living up to 25 weeks without food.[9] Starvation begins when an individual has lost about 30% of their normal body weight.[10] Once the loss reaches 40% death is almost inevitable.[10]		Under normal metabolic conditions, the human body relies on free blood glucose as its primary energy source. The level of blood sugar is tightly regulated; as blood glucose is consumed, the pancreas releases glucagon, a hormone that stimulates the liver to convert stored glycogen into glucose. The glycogen stores are ordinarily replenished after every meal, but if the store is depleted before it can be replenished, the body enters hypoglycemia, and begins the starvation response.[citation needed]		After the exhaustion of the glycogen reserve, and for the next 2–3 days, fatty acids become the principal metabolic fuel. At first, the brain continues to use glucose, because, if a non-brain tissue is using fatty acids as its metabolic fuel, the use of glucose in the same tissue is switched off. Thus, when fatty acids are being broken down for energy, all of the remaining glucose is made available for use by the brain. Basically the body will use up stored fat cells first, then move on to muscles.		After 2 or 3 days of fasting, the liver begins to synthesize ketone bodies from precursors obtained from fatty acid breakdown. The brain uses these ketone bodies as fuel, thus cutting its requirement for glucose. After fasting for 3 days, the brain gets 30% of its energy from ketone bodies. After 4 days, this goes up to 75%.[11] Thus, the production of ketone bodies cuts the brain's glucose requirement from 80 g per day to about 30 g per day. Of the remaining 30 g requirement, 20 g per day can be produced by the liver from glycerol (itself a product of fat breakdown). But this still leaves a deficit of about 10 g of glucose per day that must be supplied from some other source. This other source will be the body's own proteins.		After several days of fasting, all cells in the body begin to break down protein. This releases alanine and lactate produced from pyruvate into the bloodstream, which can be converted into glucose by the liver. Since much of human muscle mass is protein, this phenomenon is responsible for the wasting away of muscle mass seen in starvation. However, the body is able to selectively decide which cells will break down protein and which will not. About 2–3 g of protein has to be broken down to synthesize 1 g of glucose; about 20–30 g of protein is broken down each day to make 10 g of glucose to keep the brain alive. However, this number may decrease the longer the fasting period is continued in order to conserve protein.		Starvation ensues when the fat reserves are completely exhausted and protein is the only fuel source available to the body. Thus, after periods of starvation, the loss of body protein affects the function of important organs, and death results, even if there are still fat reserves left unused. (In a leaner person, the fat reserves are depleted earlier, the protein depletion occurs sooner, and therefore death occurs sooner.) The ultimate cause of death is, in general, cardiac arrhythmia or cardiac arrest brought on by tissue degradation and electrolyte imbalances.		For the individual, prevention consists of ensuring they eat plenty of food, varied enough to provide a nutritionally complete diet.		Starvation can be caused by factors, other than illness, outside of the control of the individual. The Rome Declaration on World Food Security outlines several policies aimed at increasing food security[12] and, consequently, preventing starvation. These include:		Supporting farmers in areas of food insecurity through such measures as free or subsidized fertilizers and seeds increases food harvest and reduces food prices.[13]		Starving patients can be treated, but this must be done cautiously to avoid refeeding syndrome.[14] Rest and warmth must be provided and maintained. Small sips of water mixed with glucose should be given in regular intervals. Fruit juices can also be given. Later, food can be given gradually in small quantities. The quantity of food can be increased over time. Proteins may be administered intravenously to raise the level of serum proteins.[15]		Many organizations have been highly effective at reducing starvation in different regions. Aid agencies give direct assistance to individuals, while political organizations pressure political leaders to enact more macro-scale policies that will reduce famine and provide aid.		According to estimates by the Food and Agriculture Organization there were 925 million under- or malnourished people in the world in 2010.[16] This was a decrease from an estimate of roughly 1 billion malnourished people in 2009.[17] In 2007, 923 million people were reported as being undernourished, an increase of 80 million since 1990-92.[18] It has also been recorded that the world already produces enough food to support the world's population.		As the definitions of starving and malnourished people are different, the number of starving people is different from that of malnourished. Generally, far fewer people are starving, than are malnourished.		The proportion of malnourished and of starving people in the world has been more or less continually decreasing for at least several centuries.[19] This is due to an increasing supply of food and to overall gains in economic efficiency. In 40 years, the proportion of malnourished people in the developing world has been more than halved. The proportion of starving people has decreased even faster.		Historically, starvation has been used as a death sentence. From the beginning of civilization to the Middle Ages, people were immured, or walled in, and would die for want of food.		In ancient Greco-Roman societies, starvation was sometimes used to dispose of guilty upper class citizens, especially erring female members of patrician families. For instance, in the year 31, Livilla, the niece and daughter-in-law of Tiberius, was discreetly starved to death by her mother for her adulterous relationship with Sejanus and for her complicity in the murder of her own husband, Drusus the Younger.		Another daughter-in-law of Tiberius, named Agrippina the Elder (a granddaughter of Augustus and the mother of Caligula), also died of starvation, in 33 AD. (However, it is not clear whether her starvation was self-inflicted.)		A son and daughter of Agrippina were also executed by starvation for political reasons; Drusus Caesar, her second son, was put in prison in 33 AD, and starved to death by orders of Tiberius (he managed to stay alive for nine days by chewing the stuffing of his bed); Agrippina's youngest daughter, Julia Livilla, was exiled on an island in 41 by her uncle, Emperor Claudius, and subsequently her death by starvation was arranged by the empress Messalina.		It is also possible that Vestal Virgins were starved when found guilty of breaking their vows of celibacy.		Ugolino della Gherardesca, his sons and other members of his family were immured in the Muda, a tower of Pisa, and starved to death in the thirteenth century. Dante, his contemporary, wrote about Gherardesca in his masterpiece The Divine Comedy.		In Sweden in 1317, King Birger of Sweden imprisoned his two brothers for a coup they had staged several years earlier (Nyköping Banquet). According to legend they died of starvation a few weeks later, since their brother had thrown the prison key in the castle moat.		In Cornwall in the UK in 1671, John Trehenban from St Columb Major was condemned to be starved to death in a cage at Castle An Dinas for the murder of two girls. The Makah, a Native American tribe inhabiting the Pacific Northwest near the modern border of Canada and the United States, practiced death by starvation as a punishment for slaves.[22]		Many of the prisoners died in the Nazi concentration camps through deliberate maltreatment, disease, starvation, and overwork, or were executed as unfit for labor. Many occupants of ghettos in eastern Europe also starved to death, most notoriously in the Warsaw ghetto. Prisoners were transported in inhumane conditions by rail freight cars, in which many died before reaching their destination. The prisoners were confined to the cattle cars for days or even weeks, with little or no food or water. Many died of dehydration in the intense heat of summer or froze to death in winter. Nazi concentration camps in Europe from 1933 to 1945 deliberately underfed prisoners, who were at the same time forced to perform heavy labour. The diet was restricted to watery vegetable soup and a little bread, with little or no dietary fats, proteins or other essential nutrients. Such treatment led to loss of body tissues, and prisoners became skeletal, the so-called Muselmann who were murdered by gas or bullet when examined by camp doctors.		Starvation was also used as a punishment where victims were locked into a small cell until dead, a process which could take many days. Saint Maximilian Kolbe, a martyred Polish friar, underwent a sentence of starvation in Auschwitz concentration camp in 1941. Ten prisoners had been condemned to death by starvation in the wake of a successful escape from the camp. Kolbe volunteered to take the place of a man with a wife and children. After two weeks of starvation, Kolbe and three other inmates remained alive; they were then executed with injections of phenol.		
Cooking or cookery is the art, technology and craft of preparing food for consumption with or without the use of heat. Cooking techniques and ingredients vary widely across the world, from grilling food over an open fire to using electric stoves, to baking in various types of ovens, reflecting unique environmental, economic, and cultural traditions and trends. The ways or types of cooking also depend on the skill and type of training an individual cook has. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments. Cooking can also occur through chemical reactions without the presence of heat, such as in ceviche, a traditional South American dish where fish is cooked with the acids in lemon or lime juice.		Preparing food with heat or fire is an activity unique to humans. It may have started around 2 million years ago, though archaeological evidence for it reaches no more than 1 million years ago.		The expansion of agriculture, commerce, trade and transportation between civilizations in different regions offered cooks many new ingredients. New inventions and technologies, such as the invention of pottery for holding and boiling water, expanded cooking techniques. Some modern cooks apply advanced scientific techniques to food preparation to further enhance the flavor of the dish served.[1]						Phylogenetic analysis suggests that human ancestors may have invented cooking as far back as 1.8 million to 2.3 million years ago.[2] Re-analysis of burnt bone fragments and plant ashes from the Wonderwerk Cave, South Africa, has provided evidence supporting human control of fire there by 1 million years ago.[3] There is evidence that Homo erectus was cooking their food as early as 500,000 years ago.[4] Evidence for the controlled use of fire by Homo erectus beginning some 400,000 years ago has wide scholarly support.[5][6] Archeological evidence, from 300,000 years ago,[7] in the form of ancient hearths, earth ovens, burnt animal bones, and flint, are found across Europe and the Middle East. Anthropologists think that widespread cooking fires began about 250,000 years ago, when hearths started appearing.[8] More recently, the earliest hearths have been reported to be at least 790,000 years old.[9]		In the seventeenth and eighteenth centuries, food was a classic marker of identity in Europe. In the nineteenth-century "Age of Nationalism" cuisine became a defining symbol of national identity.		Communication between the Old World and the New World in the Colombian exchange influenced the history of cooking. The movement of foods across the Atlantic, from the New World, such as potatoes, tomatoes, corn (maize), yams, beans, bell pepper, chili pepper, vanilla, pumpkin, cassava, avocado, peanut, pecan, cashew, pineapple, blueberry, sunflower, chocolate, gourds, and squash, had a profound effect on Old World cooking. The movement of foods across the Atlantic, from the Old World, such as cattle, sheep, pigs, wheat, oats, barley, rice, apples, pears, peas, chickpeas, green beans, mustard, and carrots, similarly changed New World cooking.[10]		The Industrial Revolution brought mass-production, mass-marketing and standardization of food. Factories processed, preserved, canned, and packaged a wide variety of foods, and processed cereals quickly became a defining feature of the American breakfast.[11] In the 1920s, freezing methods, cafeterias and fast-food establishments emerged.		Along with changes in food, starting early in the 20th century, governments have issued nutrition guidelines, leading to the food pyramid[12] (introduced in Sweden in 1974). The 1916 "Food For Young Children" became the first USDA guide to give specific dietary guidelines. Updated in the 1920s, these guides gave shopping suggestions for different-sized families along with a Depression Era revision which included four cost levels. In 1943, the USDA created the "Basic Seven" chart to make sure that people got the recommended nutrients. It included the first-ever Recommended Daily Allowances from the National Academy of Sciences. In 1956, the "Essentials of an Adequate Diet" brought recommendations which cut the number of groups that American school children would learn about down to four. In 1979, a guide called "Food" addressed the link between too much of certain foods and chronic diseases, but added "fats, oils, and sweets" to the four basic food groups.		Most ingredients in cooking are derived from living organisms. Vegetables, fruits, grains and nuts as well as herbs and spices come from plants, while meat, eggs, and dairy products come from animals. Mushrooms and the yeast used in baking are kinds of fungi. Cooks also use water and minerals such as salt. Cooks can also use wine or spirits.		Naturally occurring ingredients contain various amounts of molecules called proteins, carbohydrates and fats. They also contain water and minerals. Cooking involves a manipulation of the chemical properties of these molecules.		Carbohydrates include the common sugar, sucrose (table sugar), a disaccharide, and such simple sugars as glucose (made by enzymatic splitting of sucrose) and fructose (from fruit), and starches from sources such as cereal flour, rice, arrowroot and potato.		The interaction of heat and carbohydrate is complex. Long-chain sugars such as starch tend to break down into simpler sugars when cooked, while simple sugars can form syrups. If sugars are heated so that all water of crystallisation is driven off, then caramelization starts, with the sugar undergoing thermal decomposition with the formation of carbon, and other breakdown products producing caramel. Similarly, the heating of sugars and proteins elicits the Maillard reaction, a basic flavor-enhancing technique.		An emulsion of starch with fat or water can, when gently heated, provide thickening to the dish being cooked. In European cooking, a mixture of butter and flour called a roux is used to thicken liquids to make stews or sauces. In Asian cooking, a similar effect is obtained from a mixture of rice or corn starch and water. These techniques rely on the properties of starches to create simpler mucilaginous saccharides during cooking, which causes the familiar thickening of sauces. This thickening will break down, however, under additional heat.		Types of fat include vegetable oils, animal products such as butter and lard, as well as fats from grains, including corn and flax oils. Fats are used in a number of ways in cooking and baking. To prepare stir fries, grilled cheese or pancakes, the pan or griddle is often coated with fat or oil. Fats are also used as an ingredient in baked goods such as cookies, cakes and pies. Fats can reach temperatures higher than the boiling point of water, and are often used to conduct high heat to other ingredients, such as in frying, deep frying or sautéing. Fats are used to add flavor to food (e.g., butter or bacon fat), prevent food from sticking to pans and create a desirable texture.		Edible animal material, including muscle, offal, milk, eggs and egg whites, contains substantial amounts of protein. Almost all vegetable matter (in particular legumes and seeds) also includes proteins, although generally in smaller amounts. Mushrooms have high protein content. Any of these may be sources of essential amino acids. When proteins are heated they become denatured (unfolded) and change texture. In many cases, this causes the structure of the material to become softer or more friable – meat becomes cooked and is more friable and less flexible. In some cases, proteins can form more rigid structures, such as the coagulation of albumen in egg whites. The formation of a relatively rigid but flexible matrix from egg white provides an important component in baking cakes, and also underpins many desserts based on meringue.		Cooking often involves water, frequently present in other liquids, which is both added in order to immerse the substances being cooked (typically water, stock or wine), and released from the foods themselves. A favorite method of adding flavor to dishes is to save the liquid for use in other recipes. Liquids are so important to cooking that the name of the cooking method used is often based on how the liquid is combined with the food, as in steaming, simmering, boiling, braising, and blanching. Heating liquid in an open container results in rapidly increased evaporation, which concentrates the remaining flavor and ingredients – this is a critical component of both stewing and sauce making.		Vitamins and minerals are required for normal metabolism but which the body cannot manufacture itself and which must therefore come from external sources. Vitamins come from several sources including fresh fruit and vegetables (Vitamin C), carrots, liver (Vitamin A), cereal bran, bread, liver (B vitamins), fish liver oil (Vitamin D) and fresh green vegetables (Vitamin K). Many minerals are also essential in small quantities including iron, calcium, magnesium, sodium chloride and sulfur; and in very small quantities copper, zinc and selenium. The micronutrients, minerals, and vitamins[13] in fruit and vegetables may be destroyed or eluted by cooking. Vitamin C is especially prone to oxidation during cooking and may be completely destroyed by protracted cooking.[14][not in citation given] The bioavailability of some vitamins such as thiamin, vitamin B6, niacin, folate, and carotenoids are increased with cooking by being freed from the food microstructure.[15] Blanching or steaming vegetables is a way of minimizing vitamin and mineral loss in cooking.		There are very many methods of cooking, most of which have been known since antiquity. These include baking, roasting, frying, grilling, barbecuing, smoking, boiling, steaming and braising. A more recent innovation is microwaving. Various methods use differing levels of heat and moisture and vary in cooking time. The method chosen greatly affects the end result because some foods are more appropriate to some methods than others. Some major hot cooking techniques include:		Cooking can prevent many foodborne illnesses that would otherwise occur if the food is eaten raw. When heat is used in the preparation of food, it can kill or inactivate harmful organisms, such as bacteria and viruses, as well as various parasites such as tapeworms and Toxoplasma gondii. Food poisoning and other illness from uncooked or poorly prepared food may be caused by bacteria such as pathogenic strains of Escherichia coli, Salmonella typhimurium and Campylobacter, viruses such as noroviruses, and protozoa such as Entamoeba histolytica. Bacteria, viruses and parasites may be introduced through salad, meat that is uncooked or done rare, and unboiled water.[16]		The sterilizing effect of cooking depends on temperature, cooking time, and technique used. Some food spoilage bacteria such as Clostridium botulinum or Bacillus cereus can form spores that survive boiling, which then germinate and regrow after the food has cooled. This makes it unsafe to reheat cooked food more than once.[17]		Cooking increases the digestibility of many foods which are inedible or poisonous when raw. For example, raw cereal grains are hard to digest, while kidney beans are toxic when raw or improperly cooked due to the presence of phytohaemagglutinin, which is inactivated by cooking for at least ten minutes at 100 °C (212 °F).[18]		Food safety depends on the safe preparation, handling, and storage of food. Food spoilage bacteria proliferate in the "Danger zone" temperature range from 40 to 140 °F (4 to 60 °C), food therefore should not be stored in this temperature range. Washing of hands and surfaces, especially when handling different meats, and keeping raw food separate from cooked food to avoid cross-contamination, are good practices in food preparation.[19] Foods prepared on plastic cutting boards may be less likely to harbor bacteria than wooden ones.[20][21] Washing and disinfecting cutting boards, especially after use with raw meat, poultry, or seafood, reduces the risk of contamination.[21]		Proponents of raw foodism argue that cooking food increases the risk of some of the detrimental effects on food or health. They point out that during cooking of vegetables and fruit containing vitamin C, the vitamin elutes into the cooking water and becomes degraded through oxidation. Peeling vegetables can also substantially reduce the vitamin C content, especially in the case of potatoes where most vitamin C is in the skin.[22] However, research has shown that in the specific case of carotenoids a greater proportion is absorbed from cooked vegetables than from raw vegetables.[14]		German research in 2003 showed significant benefits in reducing breast cancer risk when large amounts of raw vegetable matter are included in the diet. The authors attribute some of this effect to heat-labile phytonutrients.[23] Sulforaphane, a glucosinolate breakdown product, which may be found in vegetables such as broccoli, has been shown to be protective against prostate cancer, however, much of it is destroyed when the vegetable is boiled.[24][25]		The USDA has studied retention data for 16 vitamins, 8 minerals, and alcohol for approximately 290 foods for various cooking methods.[26]		In a human epidemiological analysis by Richard Doll and Richard Peto in 1981, diet was estimated to cause a large percentage of cancers.[27] Studies suggest that around 32% of cancer deaths may be avoidable by changes to the diet.[28] Some of these cancers may be caused by carcinogens in food generated during the cooking process, although it is often difficult to identify the specific components in diet that serve to increase cancer risk. Many foods, such as beef steak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.[29]		Several studies published since 1990 indicate that cooking meat at high temperature creates heterocyclic amines (HCAs), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done.[30] While avoiding meat or eating meat raw may be the only ways to avoid HCAs in meat fully, the National Cancer Institute states that cooking meat below 212 °F (100 °C) creates "negligible amounts" of HCAs. Also, microwaving meat before cooking may reduce HCAs by 90% by reducing the time needed for the meat to be cooked at high heat.[30] Nitrosamines are found in some food, and may be produced by some cooking processes from proteins or from nitrites used as food preservatives; cured meat such as bacon has been found to be carcinogenic, with links to colon cancer. Ascorbate, which is added to cured meat, however, reduces nitrosamine formation.[29][31]		Research has shown that grilling, barbecuing and smoking meat and fish increases levels of carcinogenic polycyclic aromatic hydrocarbons (PAH). In Europe, grilled meat and smoked fish generally only contribute a small proportion of dietary PAH intake since they are a minor component of diet – most intake comes from cereals, oils and fats.[32] However, in the US, grilled/barbecued meat is the second highest contributor of the mean daily intake of a known PAH carcinogen benzo[a]pyrene at 21% after ‘bread, cereal and grain’ at 29%.[32]		Baking, grilling or broiling food, especially starchy foods, until a toasted crust is formed generates significant concentrations of acrylamide, a known carcinogen from animal studies; its potential to cause cancer in humans at normal exposures is uncertain.[33] Public health authorities recommend reducing the risk by avoiding overly browning starchy foods or meats when frying, baking, toasting or roasting them.[33]		Cooking dairy products may reduce a protective effect against colon cancer. Researchers at the University of Toronto suggest that ingesting uncooked or unpasteurized dairy products (see also Raw milk) may reduce the risk of colorectal cancer.[34] Mice and rats fed uncooked sucrose, casein, and beef tallow had one-third to one-fifth the incidence of microadenomas as the mice and rats fed the same ingredients cooked.[35][36] This claim, however, is contentious. According to the Food and Drug Administration of the United States, health benefits claimed by raw milk advocates do not exist. "The small quantities of antibodies in milk are not absorbed in the human intestinal tract," says Barbara Ingham, PhD, associate professor and extension food scientist at the University of Wisconsin-Madison. "There is no scientific evidence that raw milk contains an anti-arthritis factor or that it enhances resistance to other diseases."[37]		Heating sugars with proteins or fats can produce advanced glycation end products ("glycotoxins").[38] These have been linked to ageing and health conditions such as diabetes and obesity.		Deep fried food in restaurants may contain high level of trans fat, which is known to increase levels of low-density lipoprotein that in turn may increase risk of heart diseases and other conditions. However, many fast food chains have now switched to trans-fat-free alternatives for deep-frying.[39]		The application of scientific knowledge to cooking and gastronomy has become known as molecular gastronomy. This is a subdiscipline of food science. Important contributions have been made by scientists, chefs and authors such as Herve This (chemist), Nicholas Kurti (physicist), Peter Barham (physicist), Harold McGee (author), Shirley Corriher (biochemist, author), Heston Blumenthal (chef), Ferran Adria (chef), Robert Wolke (chemist, author) and Pierre Gagnaire (chef).[citation needed]		Chemical processes central to cooking include the Maillard reaction – a form of non-enzymatic browning involving an amino acid, a reducing sugar and heat.[40]		Home cooking has traditionally been a process carried out informally in a home or around a communal fire, and can be enjoyed by all members of the family, although in many cultures women bear primary responsibility.[41] Cooking is also often carried out outside of personal quarters, for example at restaurants, or schools. Bakeries were one of the earliest forms of cooking outside the home, and bakeries in the past often offered the cooking of pots of food provided by their customers as an additional service. In the present day, factory food preparation has become common, with many "ready-to-eat" foods being prepared and cooked in factories and home cooks using a mixture of scratch made, and factory made foods together to make a meal. The nutritional value of including more commercially prepared foods has been found to be inferior to home-made foods.[42] Home-cooked meals tend to be healthier with fewer calories, and less saturated fat, cholesterol and sodium on a per calorie basis while providing more fiber, calcium, and iron.[43] The ingredients are also directly sourced, so there is control over authenticity, taste, and nutritional value. The superior nutritional quality of home-cooking could therefore play a role in preventing chronic disease.[44] Cohort studies following the elderly over 10 years show that adults who cook their own meals have significantly lower mortality, even when controlling for confounding variables.[45]		"Home-cooking" may be associated with comfort food,[46] and some commercially produced foods are presented through advertising or packaging as having been "home-cooked", regardless of their actual origin.[citation needed]		Commercial cooking methods have evolved to a point where many of the ingredients and techniques used at home are being used in commercial cooking to great success and acceptance by patrons.[citation needed]		
A full course dinner is a dinner consisting of multiple dishes, or courses. In its simplest form, it can consist of three or four courses, such as appetizers, fish course, entrée, main course and dessert.						In formal dining, a full course dinner can consist of 5, 6, 8, 10, 12, or 16 courses, and, in its extreme form, has been known to have 21 courses. In these more formalized dining events, the courses are carefully planned to complement each other gastronomically. The courses are smaller and spread out over a long evening, up to three, four or five hours. They follow conventions of menu planning that have been established over many years. Most courses (excluding some light courses such as sorbets) in the most formal full course dinners are usually paired with a different wine, beer, liqueur, or other spirit.		In service à la russe, courses are brought to the table in sequence. Only empty plates are set in front of each guest. Courses are served on platters, and guests make selections from a variety of dishes and fill their own plate. Food presentation is skillfully focused on the platters. A filled plate is never placed in front of a guest because that would imply limited portions. Guests are expected to choose whatever they like and to eat as much as they want.		In service à la française, food is served "family-style", with all courses on the table at the same time. Guests serve themselves so that all dishes are not served at their optimum temperatures. Alternatively, buffet style is a variation of the French service where all food is available at the correct temperature in a serving space other than the dining table. Guests commute to the buffet to be served or sometimes serve themselves and then carry their plates back to the table.		In an American formal dining course, each course is served sequentially. Guests are served plates already filled with food in individual portions. Often, guests have an opportunity to choose between vegetarian or meat entrées. There is no opportunity to request something different or to ask for more than a single serving. However, portions are usually large.[citation needed] Since there are no platters, food presentation is focused on individual portions, skillfully decorated to look like art in which each plate is a masterpiece.		Table settings can be elaborate. More formal settings sometimes include all silverware and glassware that will be needed for the entire meal, and lay out the silverware so that the outermost tools are used for the dishes appearing earliest on the menu. In this scheme, when diners are served the first course, they can depend on finding the correct implement at the outermost edge of the arrangement.		An alternative scheme arranges the place setting so that only the implements needed for the first one or two courses appear in the table setting. As the dinner progresses and new courses arrive, used implements are removed with the dishes, and new silverware is placed next to the plates. This scheme is commonly used when dinners are offered à la carte, so that the most appropriate implement is selected for a given course. For example, some diners may order clear, thin soups and others may order thick, creamy soups. As each of these soups has its own unique spoon[citation needed], it would be considered improper and impractical to lay out a spoon that may not be needed.		The first class passengers aboard the ill-fated ocean liner R.M.S. Titanic were served the following eleven course meal in the first class dining saloon on the night of April 14th, 1912:[1]		First Course — Hors d'oeuvre		Second Course — Soups		Third Course — Fish		Fourth Course — Entrées		Fifth Course — Removes		Sixth Course — Punch or Sorbet		Seventh Course — Roast		Eighth Course — Salad		Ninth Course — Cold Dish		Tenth Course — Sweets		Eleventh Course — Dessert		After Dinner		or		or		
Safety is the state of being "safe" (from French sauf), the condition of being protected from harm or other non-desirable outcomes. Safety can also refer to the control of recognized hazards in order to achieve an acceptable level of risk.						There are two slightly different meanings of safety. For example, home safety may indicate a building's ability to protect against external harm events (such as weather, home invasion, etc.), or may indicate that its internal installations (such as appliances, stairs, etc.) are safe (not dangerous or harmful) for its inhabitants.		Discussions of safety often include mention of related terms. Security is such a term. With time the definitions between these two have often become interchanged, equated, and frequently appear juxtaposed in the same sentence. Readers unfortunately are left to conclude whether they comprise a redundancy. This confuses the uniqueness that should be reserved for each by itself. When seen as unique, as we intend here, each term will assume its rightful place in influencing and being influenced by the other.		Safety is the condition of a “steady state” of an organization or place doing what it is supposed to do. “What it is supposed to do” is defined in terms of public codes and standards, associated architectural and engineering designs, corporate vision and mission statements, and operational plans and personnel policies. For any organization, place, or function, large or small, safety is a normative concept. It complies with situation-specific definitions of what is expected and acceptable.[1]		Using this definition, protection from a home’s external threats and protection from its internal structural and equipment failures (see Meanings, above) are not two types of safety but rather two aspects of a home’s steady state.		In the world of everyday affairs, not all goes as planned. Some entity’s steady state is challenged. This is where security science, which is of more recent date, enters. Drawing from the definition of safety, then:		Security is the process or means, physical or human, of delaying, preventing, and otherwise protecting against external or internal, defects, dangers, loss, criminals, and other individuals or actions that threaten, hinder or destroy an organization’s “steady state,” and deprive it of its intended purpose for being.		Using this generic definition of safety it is possible to specify the elements of a security program.[1]		Safety can be limited in relation to some guarantee or a standard of insurance to the quality and unharmful function of an object or organization. It is used in order to ensure that the object or organization will do only what it is meant to do.		It is important to realize that safety is relative. Eliminating all risk, if even possible, would be extremely difficult and very expensive. A safe situation is one where risks of injury or property damage are low and manageable.		It is important to distinguish between products that meet standards, that are safe, and those that merely feel safe. The highway safety community uses these terms:		Normative safety is achieved when a product or design meets applicable standards and practices for design and construction or manufacture, regardless of the product's actual safety history.		Substantive or objective safety occurs when the real-world safety history is favorable, whether or not standards are met.		Perceived or subjective safety refers to the users' level of comfort and perception of risk, without consideration of standards or safety history. For example, traffic signals are perceived as safe, yet under some circumstances, they can increase traffic crashes at an intersection. Traffic roundabouts have a generally favorable safety record[2] yet often make drivers nervous.		Low perceived safety can have costs. For example, after the 9/11/2001 attacks, many people chose to drive rather than fly, despite the fact that, even counting terrorist attacks, flying is safer than driving. Perceived risk discourages people from walking and bicycling for transportation, enjoyment or exercise, even though the health benefits outweigh the risk of injury.[3]		Also called social safety or public safety, security addresses the risk of harm due to intentional criminal acts such as assault, burglary or vandalism.		Because of the moral issues involved, security is of higher importance to many people than substantive safety. For example, a death due to murder is considered worse than a death in a car crash, even though in many countries, traffic deaths are more common than homicides.		Safety is generally interpreted as implying a real and significant impact on risk of death, injury or damage to property. In response to perceived risks many interventions may be proposed with engineering responses and regulation being two of the most common.		Probably the most common individual response to perceived safety issues is insurance, which compensates for or provides restitution in the case of damage or loss.		System safety and reliability engineering is an engineering discipline. Continuous changes in technology, environmental regulation and public safety concerns make the analysis of complex safety-critical systems more and more demanding.		A common fallacy, for example among electrical engineers regarding structure power systems, is that safety issues can be readily deduced. In fact, safety issues have been discovered one by one, over more than a century in the case mentioned, in the work of many thousands of practitioners, and cannot be deduced by a single individual over a few decades. A knowledge of the literature, the standards and custom in a field is a critical part of safety engineering. A combination of theory and track record of practices is involved, and track record indicates some of the areas of theory that are relevant. (In the USA, persons with a state license in Professional Engineering in Electrical Engineering are expected to be competent in this regard, the foregoing notwithstanding, but most electrical engineers have no need of the license for their work.)		Safety is often seen as one of a group of related disciplines: quality, reliability, availability, maintainability and safety. (Availability is sometimes not mentioned, on the principle that it is a simple function of reliability and maintainability.) These issues tend to determine the value of any work, and deficits in any of these areas are considered to result in a cost, beyond the cost of addressing the area in the first place; good management is then expected to minimize total cost.		Safety measures are activities and precautions taken to improve safety, i.e. reduce risk related to human health. Common safety measures include:		A number of standards organizations exist that promulgate safety standards. These may be voluntary organizations or government agencies. These agencies first define the safety standards, which they publish in the form of codes. They are also Accreditation Bodies and entitle independent third parties such as testing and certification agencies to inspect and ensure compliance to the standards they defined. For instance, the American Society of Mechanical Engineers (ASME) formulated a certain number of safety standards in its Boiler and Pressure Vessel Code (BPVC) and accredited TÜV Rheinland to provide certification services to guarantee product compliance to the defined safety regulations.[4]		A major American standards organization is the American National Standards Institute (ANSI). Usually, members of a particular industry will voluntarily form a committee to study safety issues and propose standards. Those standards are then recommended to ANSI, which reviews and adopts them. Many government regulations require that products sold or used must comply with a particular ANSI standard.		Many government agencies set safety standards for matters under their jurisdiction, such as:		Product safety testing, for the United States, is largely controlled by the Consumer Product Safety Commission. In addition, workplace related products come under the jurisdiction of the Occupational Safety and Health Administration (OSHA), which certifies independent testing companies as Nationally Recognized Testing Laboratories (NRTL), see.[5]		The European Commission provides the legal framework, but the different Member States may authorize test laboratories to carry out safety testing.		Many countries have national organizations that have accreditation to test and/or submit test reports for safety certification. These are typically referred to as a Notified or Competent Body.		
A communal meal is a meal eaten by a group of people that serves a social and/or ceremonial purpose.		Some examples of communal meals are the Passover Seder, the Thanksgiving meal, cocktail parties, and company picnics.		
A herbivore is an animal anatomically and physiologically adapted to eating plant material, for example foliage, for the main component of its diet. As a result of their plant diet, herbivorous animals typically have mouthparts adapted to rasping or grinding. Horses and other herbivores have wide flat teeth that are adapted to grinding grass, tree bark, and other tough plant material.		A large percentage of herbivores have mutualistic gut flora that help them digest plant matter, which is more difficult to digest than animal prey.[1] This gut flora is made up of cellulose-digesting protozoans or bacteria living in the herbivores' intestines.[2]						Herbivore is the anglicized form of a modern Latin coinage, herbivora, cited in Charles Lyell's 1830 Principles of Geology.[3] Richard Owen employed the anglicized term in an 1854 work on fossil teeth and skeletons.[3] Herbivora is derived from the Latin herba meaning a small plant or herb,[4] and vora, from vorare, to eat or devour.[5]		Herbivory is a form of consumption in which an organism principally eats autotrophs[6] such as plants, algae and photosynthesizing bacteria. More generally, organisms that feed on autotrophs in general are known as primary consumers. Herbivory usually refers to animals eating plants; fungi, bacteria and protists that feed on living plants are usually termed plant pathogens (plant diseases), and microbes that feed on dead plants are saprotrophs. Flowering plants that obtain nutrition from other living plants are usually termed parasitic plants. There is, however, no single exclusive and definitive ecological classification of consumption patterns; each textbook has its own variations on the theme.[7][8][9]		Our understanding of herbivory in geological time comes from three sources: fossilized plants, which may preserve evidence of defence (such as spines), or herbivory-related damage; the observation of plant debris in fossilised animal faeces; and the construction of herbivore mouthparts.[10]		Although herbivory was long thought to be a Mesozoic phenomenon, fossils have shown that within less than 20 million years after the first land plants evolved, plants were being consumed by arthropods.[11] Insects fed on the spores of early Devonian plants, and the Rhynie chert also provides evidence that organisms fed on plants using a "pierce and suck" technique.[10]		During the next 75 million years[citation needed], plants evolved a range of more complex organs, such as roots and seeds. There is no evidence of any organism being fed upon until the middle-late Mississippian, 330.9 million years ago. There was a gap of 50 to 100 million years between the time each organ evolved and the time organisms evolved to feed upon them; this may be due to the low levels of oxygen during this period, which may have suppressed evolution.[11] Further than their arthropod status, the identity of these early herbivores is uncertain.[11] Hole feeding and skeletonisation are recorded in the early Permian, with surface fluid feeding evolving by the end of that period.[10]		Herbivory among four-limbed terrestrial vertebrates, the tetrapods developed in the Late Carboniferous (307 - 299 million years ago).[12] Early tetrapods were large amphibious piscivores. While amphibians continued to feed on fish and insects, some reptiles began exploring two new food types, tetrapods (carnivory) and plants (herbivory). The entire dinosaur order ornithischia was composed with herbivores dinosaurs.[12] Carnivory was a natural transition from insectivory for medium and large tetrapods, requiring minimal adaptation. In contrast, a complex set of adaptations was necessary for feeding on highly fibrous plant materials.[12]		Arthropods evolved herbivory in four phases, changing their approach to it in response to changing plant communities.[13] Tetrapod herbivores made their first appearance in the fossil record of their jaws near the Permio-Carboniferous boundary, approximately 300 million years ago. The earliest evidence of their herbivory has been attributed to dental occlusion, the process in which teeth from the upper jaw come in contact with teeth in the lower jaw is present. The evolution of dental occlusion led to a drastic increase in plant food processing and provides evidence about feeding strategies based on tooth wear patterns. Examination of phylogenetic frameworks of tooth and jaw morphologes has revealed that dental occlusion developed independently in several lineages tetrapod herbivores. This suggests that evolution and spread occurred simultaneously within various lineages.[14]		Herbivores form an important link in the food chain; because they consume plants in order to digest the carbohydrates photosynthetically produced by a plant. Carnivores in turn consume herbivores for the same reason, while omnivores can obtain their nutrients from either plants or animals. Due to a herbivore's ability to survive solely on tough and fibrous plant matter, they are termed the primary consumers in the food cycle (chain). Herbivory, carnivory, and omnivory can be regarded as special cases of Consumer-Resource Systems.[15]		Two herbivore feeding strategies are grazing (e.g. cows) and browsing (e.g. moose). Although the exact definition of the feeding strategy may depend on the writer, most authors[who?] agree that to define a grazer at least 90% of the forage has to be grass, and for a browser at least 90% tree leaves and/or twigs. An intermediate feeding strategy is called "mixed-feeding".[16] In their daily need to take up energy from forage, herbivores of different body mass may be selective in choosing their food.[17] "Selective" means that herbivores may choose their forage source depending on, e.g., season or food availability, but also that they may choose high quality (and consequently highly nutritious) forage before lower quality. The latter especially is determined by the body mass of the herbivore, with small herbivores selecting for high quality forage, and with increasing body mass animals are less selective.[17] Several theories attempt to explain and quantify the relationship between animals and their food, such as Kleiber's law, Holling's disk equation and the marginal value theorem (see below).		Kleiber's law describes the relationship between an animal's size and its feeding strategy, saying that larger animals need to eat less food per unit weight than smaller animals.[18] Kleiber’s law states that the metabolic rate (q0) of an animal is the mass of the animal (M) raised to the 3/4 power: q0=M3/4 Therefore, the mass of the animal increases at a faster rate than the metabolic rate.[19]		Herbivores employ numerous types of feeding strategies. Many herbivores do not fall into one specific feeding strategy, but employ several strategies and eat a variety of plant parts.		Optimal Foraging Theory is a model for predicting animal behavior while looking for food or other resource, such as shelter or water. This model assesses both individual movement, such as animal behavior while looking for food, and distribution within a habitat, such as dynamics at the population and community level. For example, the model would be used to look at the browsing behavior of a deer while looking for food, as well as that deer's specific location and movement within the forested habitat and its interaction with other deer while in that habitat.[citation needed]		This model has been criticized as circular and untestable. Critics have pointed out that its proponents use examples that fit the theory, but do not use the model when it does not fit the reality.[20][21] Other critics point out that animals do not have the ability to assess and maximize their potential gains, therefore the optimal foraging theory is irrelevant and derived to explain trends that do not exist in nature.[22][23]		Holling's disk equation models the efficiency at which predators consume prey. The model predicts that as the number of prey increases, the amount of time predators spend handling prey also increases and therefore the efficiency of the predator decreases.[24][page needed] In 1959, S. Holling proposed an equation to model the rate of return for an optimal diet: Rate (R ) = Energy gained in foraging (Ef)/(time searching (Ts) + time handling (Th)) R = E f / ( T s + T h ) {\displaystyle R=Ef/(Ts+Th)} Where s = cost of search per unit time f = rate of encounter with items, h = handling time, e = energy gained per encounter In effect, this would indicate that a herbivore in a dense forest would spend more time handling (eating) the vegetation because there was so much vegetation around than a herbivore in a sparse forest, who could easily browse through the forest vegetation. According to the Holling's disk equation, a herbivore in the sparse forest would be more efficient at eating than the herbivore in the dense forest		The marginal value theorem describes the balance between eating all the food in a patch for immediate energy, or moving to a new patch and leaving the plants in the first patch to regenerate for future use. The theory predicts that absent complicating factors, an animal should leave a resource patch when the rate of payoff (amount of food) falls below the average rate of payoff for the entire area.[25] According to this theory, locus should move to a new patch of food when the patch they are currently feeding on requires more energy to obtain food than an average patch. Within this theory, two subsequent parameters emerge, the Giving Up Density (GUD) and the Giving Up Time (GUT). The Giving Up Density (GUD) quantifies the amount of food that remains in a patch when a forager moves to a new patch.[26] The Giving Up Time (GUT) is used when an animal continuously assesses the patch quality.[27]		The myriad defenses displayed by plants means that their herbivores need a variety of skills to overcome these defenses and obtain food. These allow herbivores to increase their feeding and use of a host plant. Herbivores have three primary strategies for dealing with plant defenses: choice, herbivore modification, and plant modification.		Feeding choice involves which plants a herbivore chooses to consume. It has been suggested that many herbivores feed on a variety of plants to balance their nutrient uptake and to avoid consuming too much of any one type of defensive chemical. This involves a tradeoff however, between foraging on many plant species to avoid toxins or specializing on one type of plant that can be detoxified.[28]		Herbivore modification is when various adaptations to body or digestive systems of the herbivore allow them to overcome plant defenses. This might include detoxifying secondary metabolites,[29] sequestering toxins unaltered,[30] or avoiding toxins, such as through the production of large amounts of saliva to reduce effectiveness of defenses. Herbivores may also utilize symbionts to evade plant defences. For example, some aphids use bacteria in their gut to provide essential amino acids lacking in their sap diet.[31]		Plant modification occurs when herbivores manipulate their plant prey to increase feeding. For example, some caterpillars roll leaves to reduce the effectiveness of plant defenses activated by sunlight.[32]		A plant defense is a trait that increases plant fitness when faced with herbivory. This is measured relative to another plant that lacks the defensive trait. Plant defenses increase survival and/or reproduction (fitness) of plants under pressure of predation from herbivores.[citation needed]		Defense can be divided into two main categories, tolerance and resistance. Tolerance is the ability of a plant to withstand damage without a reduction in fitness. This can occur by diverting herbivory to non-essential plant parts or by rapid regrowth and recovery from herbivory. Resistance refers to the ability of a plant to reduce the amount of damage it receives from a herbivore. This can occur via avoidance in space or time,[33] physical defenses, or chemical defenses. Defenses can either be constitutive, always present in the plant, or induced, produced or translocated by the plant following damage or stress.[34]		Physical, or mechanical, defenses are barriers or structures designed to deter herbivores or reduce intake rates, lowering overall herbivory. Thorns such as those found on roses or acacia trees are one example, as are the spines on a cactus. Smaller hairs known as trichomes may cover leaves or stems and are especially effective against invertebrate herbivores.[35] In addition, some plants have waxes or resins that alter their texture, making them difficult to eat. Also the incorporation of silica into cell walls is analogous to that of the role of lignin in that it is a compression-resistant structural component of cell walls; so that plants with their cell walls impregnated with silica are thereby afforded a measure of protection against herbivory.[36]		Chemical defenses are secondary metabolites produced by the plant that deter herbivory. There are a wide variety of these in nature and a single plant can have hundreds of different chemical defenses. Chemical defenses can be divided into two main groups, carbon-based defenses and nitrogen-based defenses.[citation needed]		Plants have also changed features that enhance the probability of attracting natural enemies to herbivores. Some emit semiochemicals, odors that attract natural enemies, while others provide food and housing to maintain the natural enemies’ presence, e.g. ants that reduce herbivory.[38] A given plant species often has many types of defensive mechanisms, mechanical or chemical, constitutive or induced, which allow it to escape from herbivores.[citation needed]		According to the theory of predator–prey interactions, the relationship between herbivores and plants is cyclic.[39] When prey (plants) are numerous their predators (herbivores) increase in numbers, reducing the prey population, which in turn causes predator number to decline.[40] The prey population eventually recovers, starting a new cycle. This suggests that the population of the herbivore fluctuates around the carrying capacity of the food source, in this case the plant.		Several factors play into these fluctuating populations and help stabilize predator–prey dynamics. For example, spatial heterogeneity is maintained, which means there will always be pockets of plants not found by herbivores. This stabilizing dynamic plays an especially important role for specialist herbivores that feed on one species of plant and prevents these specialists from wiping out their food source.[41] Prey defenses also help stabilize predator–prey dynamics, and for more information on these relationships see the section on Plant Defenses. Eating a second prey type helps herbivores’ populations stabilize.[42] Alternating between two or more plant types provides population stability for the herbivore, while the populations of the plants oscillate.[43] This plays an important role for generalist herbivores that eat a variety of plants. Keystone herbivores keep vegetation populations in check and allow for a greater diversity of both herbivores and plants.[42] When an invasive herbivore or plant enters the system, the balance is thrown off and the diversity can collapse to a monotaxon system.[42]		The back and forth relationship of plant defense and herbivore offense can be seen as a sort of "adaptation dance" in which one partner makes a move and the other counters it.[29] This reciprocal change drives coevolution between many plants and herbivores, resulting in what has been referred to as a "coevolutionary arms race".[44] The escape and radiation mechanisms for coevolution, presents the idea that adaptations in herbivores and their host plants, has been the driving force behind speciation.[45][46]		While much of the interaction of herbivory and plant defense is negative, with one individual reducing the fitness of the other, some is actually beneficial. This beneficial herbivory takes the form of mutualisms in which both partners benefit in some way from the interaction. Seed dispersal by herbivores and pollination are two forms of mutualistic herbivory in which the herbivore receives a food resource and the plant is aided in reproduction.[47]		Herbivorous fish and marine animals are an indispensable part of the coral reef ecosystem. Since algae and seaweeds grow much faster than corals they can occupy spaces where corals could have settled. They can outgrow and thus outcompete corals on bare surfaces. In the absence of plant-eating fish, seaweeds deprive corals of sunlight. They can also physically damage corals with scrapes.[48]		The impact of herbivory can be seen in areas ranging from economics to ecological, and both. For example, environmental degradation from white-tailed deer (Odocoileus virginianus) in the US alone has the potential to both change vegetative communities through over-browsing and cost forest restoration projects upwards of $750 million annually. Agricultural crop damage by the same species totals approximately $100 million every year. Insect crop damages also contribute largely to annual crop losses in the U.S.[49] Herbivores affect economics through the revenue generated by hunting and ecotourism. For example, the hunting of herbivorous game species such as white-tailed deer, cottontail rabbits, antelope, and elk in the U.S. contributes greatly to the billion-dollar annually hunting industry.[citation needed] Ecotourism is a major source of revenue, particularly in Africa, where many large mammalian herbivores such as elephants, zebras, and giraffes help to bring in the equivalent of millions of US dollars to various nations annually.[citation needed]		
A myth is any traditional story consisting of events that are ostensibly historical, explaining the origins of a cultural practice or natural phenomenon.[3] The word "myth" is derived from the Greek word mythos (μῦθος), which simply means "story". Mythology can refer either to the study of myths, or to a body or collection of myths.[4] Myth can mean 'sacred story', 'traditional narrative' or 'tale of the gods'. A myth can also be a story to explain why something exists.[5]		Human cultures' mythologies usually include a cosmogonical or creation myth, concerning the origins of the world, or how the world came to exist. The active beings in myths are generally gods and goddesses, heroes and heroines, or animals and plants. Most myths are set in a timeless past before recorded time or beginning of the critical history. A myth can be a story involving symbols that are capable of multiple meanings.[5]		A myth is a sacred narrative because it holds religious or spiritual significance for those who tell it. Myths also contribute to and express a culture's systems of thought and values, such as the myth of gremlins invented by aircraft technicians during World War II to avoid apportioning blame. Myths are often therefore stories that are currently understood as being exaggerated or fictitious.[6]						According to Albert A. Anderson, a professor of philosophy, the term mythos appears in the works of Homer and other poets of Homer's era.[7] In these works, the term had several meanings: conversation, narrative, speech, story, tale, and word. Like the related term logos, mythos expresses whatever can be delivered in the form of words.[7] Anderson contrasts the two terms with ergon, a Greek term for action, deed, and work.[7]		The term mythos lacks an explicit distinction between true or false narratives.[7]		In the context of the Theatre of ancient Greece, the term mythos referred to the myth, the narrative, the plot, and the story of a theatrical play.[8] According to David Wiles, the Greek term mythos in this era covered an entire spectrum of different meanings, from undeniable falsehoods to stories with religious and symbolic significance.[8]		According to philosopher Aristotle (384-322 BC), the spirit of a theatrical play was its mythos.[8] The term mythos was also used for the source material of Greek tragedy. The tragedians of the era could draw inspiration from Greek mythology, a body of "traditional storylines" which concerned gods and heroes.[8] David Wiles observes that modern conceptions about Greek tragedy can be misleading. It is commonly thought that the ancient audience members were already familiar with the mythos behind a play, and could predict the outcome of the play. However, the Greek dramatists were not expected to faithfully reproduce traditional myths when adapting them for the stage. They were instead recreating the myths and producing new versions.[8] Storytellers like Euripides (c. 480-406 BC) relied on suspense to excite their audiences. In one of his works, Merope attempts to kill her son's murderer with an axe, unaware that the man in question is actually her son. According to an ancient description of audience reactions to this work, the audience members were genuinely unsure of whether she would commit filicide or she will be stopped in time. They rose to their feet in terror and caused an uproar.[8]		David Wiles points that the traditional mythos of Ancient Greece, was primarily a part of its oral tradition. The Greeks of this era were a literate culture, but produced no sacred texts. There were no definitive or authoritative versions of myths recorded in texts and preserved forever in an unchanging form.[9] Instead multiple variants of myths were in circulation. These variants were adapted into songs, dances, poetry, and visual art. Performers of myths could freely reshape their source material for a new work, adapting it to the needs of a new audience or in response to a new situation.[9]		Children in Ancient Greece were familiar with traditional myths from an early age. Based on the writings of philosopher Plato (c. 428-347 BC), mothers and nursemaids narrated myths and stories to the children in their charge.[9] These women were tasked with rearing children. Apparently they had to find ways to stimulate the children's language skills and imaginations. They lacked access to children's literature or television, so the solution was to turn to storytelling. David Wiles describes them as a repository of mythological lore.[9]		Bruce Lincoln has called attention to the apparent meaning of the terms mythos and logos in the works of Hesiod. In Theogony, Hesiod attributes to the Muses the ability to both proclaim truths and narrate plausible falsehoods (falsehoods which seem like real things).[10] The verb used for narrating the falsehoods in the text is legein, which is etymologically associated with logos. There are two variants in the manuscript tradition for the verb used to proclaim truths. One variant uses gerusasthai, the other mythesasthai. The latter is a form of the verb mytheomai (to speak, to tell), which is etymologically associated with mythos.[10] In the Works and Days, Hesiod describes his dispute with his brother Perses. He also announces to his readers his intention to tell true things to his brother. The verb he uses for telling the truth is mythesaimen, another form of mytheomai.[10]		Lincoln draws the conclusion that Hesiod associated the "speech of mythos" (as Lincoln calls it) with telling the truth. While he associated the "speech of logos" with telling lies, and hiding one's true thoughts (dissimulation).[10] This conclusion is strengthened by the use of the plural term logoi (the plural form of logos) elsewhere in Hesiod's works. Three times the term is associated with the term "seductive" and three times with the term "falsehoods".[10] In his genealogy of the gods, Hesiod lists logoi among the children of Eris, the goddess personifying strife. Eris' children are ominous figures, which personify various physical and verbal forms of conflict.[10]		The term is common in the academic fields of mythology, mythography.[11] or folkloristics. Use of the term by scholars has no implication for the truth or falsity of the myth. While popular usage interchangeably employs the terms legend, fiction, fairy tale, folklore, fable and urban legend, each has a distinct meaning in academia.		A myth can be a collectively held belief that has no basis in fact. This usage, which is often pejorative,[12] arose from labeling the religious myths and beliefs of other cultures as incorrect, but it has spread to cover non-religious beliefs as well.[13] Because of this popular and subjective word usage, many people take offense when the narratives they believe to be true are called myths.		To the source culture a myth by definition is "true", in that it embodies beliefs, concepts and ways of questioning to make sense of the world.[5]		
In the cuisine of the Southern United States, a meat and three restaurant is one where the customer picks one meat from a daily selection of three to six choices (such as fried chicken, country ham, beef, country-fried steak, meatloaf, or pork chop[1][2]) and three side dishes from a list that may include up to a dozen other options (usually vegetables, potatoes, corn, green or lima beans,[3] but also other selections such as gelatin, creamed corn, macaroni and cheese, and spaghetti).[2][4]		A meat-and-three meal is often served with cornbread and sweet tea.[3][4] Meat and three is popular throughout the United States, but its roots can be traced to Tennessee and its capital of Nashville.[2][3][4][5] The phrase has been described as implying "glorious vittles served with utmost informality."[4] It is also associated with soul food.[3]		Similar concepts include the Hawaiian plate lunch, which features a variety of entrée choices but typically has standardized side items,[6][7] and the southern Louisiana plate lunch, which features menu options that change daily.[8] It is somewhat similar to a blue-plate special but with a more fixed menu.[9] The Boston Market chain of restaurants offers a similar style of food selection.[10]		
Second breakfast (or zweites Frühstück, drugie śniadanie, tízórai) is a meal eaten after breakfast, but before lunch. It is a traditional meal in Bavaria, Poland, and Hungary. In Bavaria and Poland, special dishes are made exclusively to be eaten during second breakfast. In Vienna and most other parts of Austria the second breakfast is referred to as Jause.[1] It is typical to eat four to five meals a day in these locations.						The second breakfast is typically a lighter meal or snack eaten around 10:30 in the morning (its Hungarian name, tízórai, actually means "[snack] at 10"). It consists of coffee and pastries or some sausages. The typical sausage is a white sausage, Weißwurst, which is considered the specialty of Munich. The sausage is prepared during the early morning to serve during the second breakfast. It is served with pretzels, sweet mustard, and wheat beer. The meal is roughly similar in concept to the British elevenses. In Poland second breakfast usually consists of some snacks like sandwiches, or pastries, but may consist of light dessert-type dishes like chocolate pudding or kisiel.		First and second breakfast is also a common custom in farm areas of North America. Farmers who need to rise early to tend to animals or perform other chores may eat a small "first breakfast", such as toast and coffee, just after rising, followed by a heartier second breakfast after the first round of chores is done.		In J. R. R. Tolkien's novel The Hobbit, the protagonist, Bilbo Baggins, eats a second breakfast, and in the preface to its sequel, The Lord of the Rings, Tolkien mentions that hobbits prefer to eat seven meals a day. In Peter Jackson's film adaptation of The Fellowship of the Ring, as Aragorn is leading the hobbits on a march, Pippin – hoping for a meal break – is horrified when Merry tells him that the man probably doesn't know about second breakfast. Pippin goes on to ask if he knows about the other meals commonly eaten by hobbits, including elevenses, luncheon, afternoon tea, dinner, and supper. Aragorn throws an apple to each of the pair to tide them over until their next "regular" meal break.		In Thomas Mann's books The Magic Mountain or Buddenbrooks, frequent and detailed references are made to second breakfasts. Beer is served along with "cold cuts on toast". Sometimes food from the first breakfast appears again such as oatmeal and fruit. In fact, Mann refers in Magic Mountain to "third breakfasts" as well.		
Hygiene is a set of practices performed for the preservation of health. According to the World Health Organization (WHO), "Hygiene refers to conditions and practices that help to maintain health and prevent the spread of diseases."[2]		Whereas in popular culture and parlance it can often mean mere 'cleanliness', hygiene goes much beyond that to include all circumstances and practices, lifestyle issues, premises and commodities that engender a safe and healthy environment, especially in modern medicine. Some regular hygienic practices may be considered good habits by a society, while the neglect of hygiene can be considered disgusting, disrespectful or even threatening.		First attested in English in 1677s, the word hygiene comes from the French hygiène, the latinisation of the Greek ὑυγιεινή (τέχνη) hugieinē technē, meaning "(art) of health", from ὑυγιεινός hugieinos, "good for the health, healthy",[3] in turn from ὑυγιής (hugiēs), "healthful, sound, salutary, wholesome".[4] In ancient Greek religion, Hygeia (Ὑυγίεια) was the personification of health, cleanliness and hygiene.[5]		Hygiene is a concept related to cleanliness, health and medicine, as well as to personal and professional care practices related to most aspects of living. In medicine and in home (domestic) and everyday life settings, hygiene practices are employed as preventative measures to reduce the incidence and spreading of disease. In the manufacture of food, pharmaceutical, cosmetic and other products, good hygiene is a key part of quality assurance i.e. ensuring that the product complies with microbial specifications appropriate to its use. The terms cleanliness (or cleaning) and hygiene are often used interchangeably, which can cause confusion. In general, hygiene mostly means practices that prevent spread of disease-causing organisms. Since cleaning processes (e.g., hand washing) remove infectious microbes as well as dirt and soil, they are often the means to achieve hygiene. Other uses of the term appear in phrases including: body hygiene, personal hygiene, sleep hygiene, mental hygiene, dental hygiene, and occupational hygiene, used in connection with public health. Hygiene is also the name of a branch of science that deals with the promotion and preservation of health, also called hygienic. Hygiene practices vary widely, and what is considered acceptable in one culture might not be acceptable in another.		Medical hygiene pertains to the hygiene practices related to the administration of medicine, and medical care, that prevents or minimizes disease and the spreading of disease.		Medical hygiene practices include:		Most of these practices were developed in the 19th century and were well established by the mid-20th century. Some procedures (such as disposal of medical waste) were refined in response to late-20th century disease outbreaks, notably AIDS and Ebola.		Home hygiene pertains to the hygiene practices that prevent or minimize disease and the spreading of disease in home (domestic) and in everyday life settings such as social settings, public transport, the work place, public places etc.		Hygiene in home and everyday life settings plays an important part in preventing spread of infectious diseases.[6] It includes procedures used in a variety of domestic situations such as hand hygiene, respiratory hygiene, food and water hygiene, general home hygiene (hygiene of environmental sites and surfaces), care of domestic animals, and home healthcare (the care of those who are at greater risk of infection).		At present, these components of hygiene tend to be regarded as separate issues, although all are based on the same underlying microbiological principles. Preventing the spread of infectious diseases means breaking the chain of infection transmission. The simple principle is that, if the chain of infection is broken, infection cannot spread. In response to the need for effective codes of hygiene in home and everyday life settings the International Scientific Forum on Home Hygiene has developed a risk-based approach based on Hazard Analysis Critical Control Point (HACCP), which has come to be known as "targeted hygiene". Targeted hygiene is based on identifying the routes of spread of pathogens in the home, and applying hygiene procedures at critical points at appropriate times to break the chain of infection.		The main sources of infection in the home[7] are people (who are carriers or are infected), foods (particularly raw foods) and water, and domestic animals (in the U.S. more than 50% of homes have one or more pets[8]). Additionally, sites that accumulate stagnant water—such as sinks, toilets, waste pipes, cleaning tools, face cloths—readily support microbial growth, and can become secondary reservoirs of infection, though species are mostly those that threaten "at risk" groups. Germs (potentially infectious bacteria, viruses etc.) are constantly shed from these sources via mucous membranes, faeces, vomit, skin scales, etc. Thus, when circumstances combine, people become exposed, either directly or via food or water, and can develop an infection.		The main "highways" for spread of germs[7] in the home are the hands, hand and food contact surfaces, and cleaning cloths and utensils. Germs can also spread via clothing and household linens, such as towels. Utilities such as toilets and wash basins, for example, were invented for dealing safely with human waste, but still have risks associated with them, which may become critical at certain times, e.g., when someone has sickness or diarrhea. Safe disposal of human waste is a fundamental need; poor sanitation is a primary cause of diarrhea disease in low income communities. Respiratory viruses and fungal spores are also spread via the air.		Good home hygiene means targeting hygiene procedures at critical points, at appropriate times, to break the chain of infection i.e. to eliminate germs before they can spread further.[7] Because the "infectious dose" for some pathogens can be very small (10-100 viable units, or even less for some viruses), and infection can result from direct transfer from surfaces via hands or food to the mouth, nasal mucosa or the eye, 'hygienic cleaning' procedures should be sufficient to eliminate pathogens from critical surfaces. Hygienic cleaning can be done by:		Hand hygiene is defined as hand washing or washing hands and nails with soap and water or using a waterless hand sanitizer. Hand hygiene is central to preventing spread of infectious diseases in home and everyday life settings.[9]		In situations where hand washing with soap is not an option (e.g. when in a public place with no access to wash facilities), a waterless hand sanitizer such as an alcohol hand gel can be used. They can also be used in addition to hand washing, to minimize risks when caring for "at risk" groups. To be effective, alcohol hand gels should contain not less than 60%v/v alcohol.		The World Health Organization recommends hand washing with ash if soap is not available in emergencies,[10] schools without access to soap[11] and other difficult situations like post-emergencies where use of (clean) sand is recommended too.[12] Use of ash is common in rural areas of developing countries and has in experiments been shown at least as effective as soap for removing bacteria.[13]		Correct respiratory and hand hygiene when coughing and sneezing reduces the spread of germs particularly during the cold and flu season.[6]		Food hygiene is concerned with the hygiene practices that prevent food poisoning. The five key principles of food hygiene, according to WHO, are:[14]		Routine cleaning of (hand, food, & drinking water) sites and surfaces (such as toilet seats and flush handles, door and tap handles, work surfaces, bath and basin surfaces) in the kitchen, bathroom and toilet reduces the risk of spread of germs.[15] The infection risk from flush toilets is not high, provided they are properly maintained, although some splashing and aerosol formation can occur during flushing, particularly where someone in the family has diarrhea. Germs can survive in the scum or scale left behind on baths and wash basins after washing and bathing.		Water left stagnant in the pipes of showers can be contaminated with germs that become airborne when the shower is turned on. If a shower has not been used for some time, it should be left to run at a hot temperature for a few minutes before use.		Thorough cleaning is important in preventing the spread of fungal infections.[16] Molds can live on wall and floor tiles and on shower curtains. Mold can be responsible for infections, cause allergic responses, deteriorate/damage surfaces and cause unpleasant odors. Primary sites of fungal growth are inanimate surfaces, including carpets and soft furnishings.[17] Air-borne fungi are usually associated with damp conditions, poor ventilation or closed air systems.		Laundry hygiene pertains to the practices that prevent or minimize disease and the spreading of disease via soiled clothing and household linens such as towels.[18] Items most likely to be contaminated with pathogens are those that come into direct contact with the body, e.g., underwear, personal towels, facecloths, nappies. Cloths or other fabric items used during food preparation, or for cleaning the toilet or cleaning up material such as faeces or vomit are a particular risk.[19]		Microbiological and epidemiological data indicates that clothing and household linens etc. are a risk factor for infection transmission in home and everyday life settings as well as institutional settings, although the lack of quantitative data directly linking contaminated clothing to infection in the domestic setting makes it difficult to assess the extent of the risk.[18][19][20] Although microbiological data indicates that risks from clothing and household linens are somewhat less than those associated with hands, hand contact and food contact surfaces, and cleaning cloths, nevertheless these risks needs to be appropriately managed through effective laundering practices. In the home, this routine should be carried out as part of a multibarrier approach to hygiene which also includes hand, food, respiratory and other hygiene practices.[18][19][20]		Infectious diseases risks from contaminated clothing etc. can increase significantly under certain conditions. e.g. in healthcare situations in hospitals, care homes and the domestic setting where someone has diarrhoea, vomiting, or a skin or wound infection. It also increases in circumstances where someone has reduced immunity to infection.		Hygiene measures, including laundry hygiene, are an important part of reducing spread of antibiotic resistant strains.[21][22] In the community, otherwise healthy people can become persistent skin carriers of MRSA, or faecal carriers of enterobacteria strains which can carry multi-antibiotic resistance factors (e.g. NDM-1 or ESBL-producing strains). The risks are not apparent until, for example, they are admitted to hospital, when they can become "self infected" with their own resistant organisms following a surgical procedure. As persistent nasal, skin or bowel carriage in the healthy population spreads "silently" across the world, the risks from resistant strains in both hospitals and the community increases.[22] In particular the data indicates that clothing and household linens are a risk factor for spread of S. aureus (including MRSA and PVL-producing MRSA strains), and that effectiveness of laundry processes may be an important factor in defining the rate of community spread of these strains.[18][23] Experience in the USA suggests that these strains are transmissible within families, but also in community settings such as prisons, schools and sport teams. Skin-to-skin contact (including unabraded skin) and indirect contact with contaminated objects such as towels, sheets and sports equipment seem to represent the mode of transmission.[18]		During laundering, temperature, together with the action of water and detergent work together to reduce microbial contamination levels on fabrics. During the wash cycle soil and microbes are detached from fabrics and suspended into the wash water. These are then "washed away" during the rinse and spin cycles. In addition to physical removal, micro-organisms can be killed by thermal inactivation which increases as the temperature is increased. Chemical inactivation of microbes by the surfactants and activated oxygen-based bleach used in detergents also contributes to the hygiene effectiveness of laundering. Adding hypochlorite bleach in the washing process also achieves inactivation of microbes. A number of other factors can also contribute including drying and ironing.		Laundry detergents contain a mix of ingredients including surfactants, builders, optical brighteners, etc. Cleaning action arises primarily from the action of the surfactants and other ingredients, which are designed to maximise release and suspension of dirt and microbes into the wash liquid, together with enzymes and/or an activated oxygen-based bleach which digest and remove stains. Although activated oxygen bleach is included in many powder detergents to digest and remove stains, it also produces some chemical inactivation of bacteria, fungi and viruses. As a rule of thumb, powders and tablets normally contain an activated oxygen bleach, but liquids, and all products (liquid or powder) used for "coloureds" do not. Surfactants also exert some chemical inactivation action against certain species although the extent of their action is not known.		In 2013 the International Scientific Forum on Home Hygiene (IFH) reviewed some 30 studies of the hygiene effectiveness of laundering at various temperatures ranging from room temperature to 70 °C, under varying conditions.[24] A key finding was the lack of standardisation and control within studies, and the variability in test conditions between studies such as wash cycle time, number of rinses etc. The consequent variability in the data (i.e. the reduction in contamination on fabrics) obtained, in turn makes it extremely difficult to propose guidelines for laundering with any confidence, based on currently available data. As a result, there is significant variability in the recommendations for hygienic laundering of clothing etc. given by different agencies.[25][26][27][28][29][30]		Of concern is recent data suggesting that, in reality, modern domestic washing machines do not reach the temperature specified on the machine controls.[31][32]		Medical hygiene pertains to the hygiene practices that prevents or minimizes disease and the spreading of disease in relation to administering medical care to those who are infected or who are more "at risk" of infection in the home. Across the world, governments are increasingly under pressure to fund the level of healthcare that people expect. Care of increasing numbers of patients in the community, including at home is one answer, but can be fatally undermined by inadequate infection control in the home. Increasingly, all of these "at-risk" groups are cared for at home by a carer who may be a household member who thus requires a good knowledge of hygiene. People with reduced immunity to infection, who are looked after at home, make up an increasing proportion of the population (currently up to 20%).[6] The largest proportion are the elderly who have co-morbidities, which reduce their immunity to infection. It also includes the very young, patients discharged from hospital, taking immuno-suppressive drugs or using invasive systems, etc. For patients discharged from hospital, or being treated at home special "medical hygiene" (see above) procedures may need to be performed for them e.g. catheter or dressing replacement, which puts them at higher risk of infection.		Antiseptics may be applied to cuts, wounds abrasions of the skin to prevent the entry of harmful bacteria that can cause sepsis. Day-to-day hygiene practices, other than special medical hygiene procedures[33] are no different for those at increased risk of infection than for other family members. The difference is that, if hygiene practices are not correctly carried out, the risk of infection is much greater.		Chemical disinfectants are products that kill germs (harmful bacteria, viruses and fungi). If the product is a disinfectant, the label on the product should say "disinfectant" and/or "kills" germs or bacteria etc. Some commercial products, e.g. bleaches, even though they are technically disinfectants, say that they "kill germs", but are not actually labelled as "disinfectants". Not all disinfectants kill all types of germs. All disinfectants kill bacteria (called bactericidal). Some also kill fungi (fungicidal), bacterial spores (sporicidal) and/or viruses (virucidal).		An antibacterial product is a product that acts against bacteria in some unspecified way. Some products labelled "antibacterial" kill bacteria while others may contain a concentration of active ingredient that only prevent them multiplying. It is, therefore, important to check whether the product label states that it "kills" bacteria." An antibacterial is not necessarily anti-fungal or anti-viral unless this is stated on the label.		The term sanitizer has been used to define substances that both clean and disinfect. More recently this term has been applied to alcohol-based products that disinfect the hands (alcohol hand sanitizers). Alcohol hand sanitizers however are not considered to be effective on soiled hands.		The term biocide is a broad term for a substance that kills, inactivates or otherwise controls living organisms. It includes antiseptics and disinfectants, which combat micro-organisms, and also includes pesticides.		In developing countries, universal access to water and sanitation has been seen as the essential step in reducing the preventable infectious diseases burden, but it is now clear that this is best achieved by programs that integrate hygiene promotion with improvements in water quality and availability, and sanitation. This approach has been integrated into the Sustainable Development Goal Number 6 whose second target states: "By 2030, achieve access to adequate and equitable sanitation and hygiene for all and end open defecation, paying special attention to the needs of women and girls and those in vulnerable situations".[34] Due to their close linkages, water, sanitation, hygiene are together abbreviated and funded under the term WASH in development cooperation.		About 2 million people die every year due to diarrheal diseases, most of them are children less than 5 years of age.[35] The most affected are the populations in developing countries, living in extreme conditions of poverty, normally peri-urban dwellers or rural inhabitants. Providing access to sufficient quantities of safe water, the provision of facilities for a sanitary disposal of excreta, and introducing sound hygiene behaviors are of capital importance to reduce the burden of disease caused by these risk factors.		Research shows that, if widely practiced, hand washing with soap could reduce diarrhea by almost fifty percent[36][37][38] and respiratory infections by nearly twenty-five percent[39][40] Hand washing with soap also reduces the incidence of skin diseases,[41][42] eye infections like trachoma and intestinal worms, especially ascariasis and trichuriasis.[43]		Other hygiene practices, such as safe disposal of waste, surface hygiene, and care of domestic animals, are also important in low income communities to break the chain of infection transmission.[44]		Cleaning of toilets and hand wash facilities is important to prevent odors and make them socially acceptable. Social acceptance is an important part of encouraging people to use toilets and wash their hands, in situations where open defecation is still seen as a possible alternative, e.g. in rural areas of some developing countries.		Household water treatment and safe storage ensure drinking water is safe for consumption. These interventions are part of the approach of self-supply of water for households.[45] Drinking water quality remains a significant problem, not only in developing countries[46] but also in developed countries;[47] even in the European region it is estimated that 120 million people do not have access to safe drinking water. Point-of-use water quality interventions can reduce diarrheal disease in communities where water quality is poor, or in emergency situations where there is a breakdown in water supply.[46][47][48][49] Since water can become contaminated during storage at home (e.g. by contact with contaminated hands or using dirty storage vessels), safe storage of water in the home is also important.		Methods for treatment of drinking water,[49][15] include:		Personal hygiene involves those practices performed by an individual to care for one's bodily health and well being, through cleanliness. Motivations for personal hygiene practice include reduction of personal illness, healing from personal illness, optimal health and sense of well being, social acceptance and prevention of spread of illness to others. What is considered proper personal hygiene can be cultural-specific and may change over time. In some cultures removal of body hair is considered proper hygiene. Other practices that are generally considered proper hygiene include bathing regularly, washing hands regularly and especially before handling food, washing scalp hair, keeping hair short or removing hair, wearing clean clothing, brushing one's teeth, cutting finger nails, besides other practices. Some practices are gender-specific, such as by a woman during her menstrual cycle. People tend to develop a routine for attending to their personal hygiene needs. Other personal hygienic practices would include covering one's mouth when coughing, disposal of soiled tissues appropriately, making sure toilets are clean, and making sure food handling areas are clean, besides other practices. Some cultures do not kiss or shake hands to reduce transmission of bacteria by contact.		Personal grooming extends personal hygiene as it pertains to the maintenance of a good personal and public appearance, which need not necessarily be hygienic. It may involve, for example, using deodorants or perfume, shaving, or combing, besides other practices.		Excessive body hygiene is one example of obsessive compulsive disorder.		The hygiene hypothesis was first formulated in 1989 by Strachan who observed that there was an inverse relationship between family size and development of atopic allergic disorders – the more children in a family, the less likely they were to develop these allergies.[51] From this, he hypothesised that lack of exposure to "infections" in early childhood transmitted by contact with older siblings could be a cause of the rapid rise in atopic disorders over the last thirty to forty years. Strachan further proposed that the reason why this exposure no longer occurs is, not only because of the trend towards smaller families, but also "improved household amenities and higher standards of personal cleanliness".		Although there is substantial evidence that some microbial exposures in early childhood can in some way protect against allergies, there is no evidence that humans need exposure to harmful microbes (infection) or that it is necessary to suffer a clinical infection.[52][53][54][55] Nor is there evidence that hygiene measures such as hand washing, food hygiene etc. are linked to increased susceptibility to atopic disease.[43][44] If this is the case, there is no conflict between the goals of preventing infection and minimising allergies. A consensus is now developing among experts that the answer lies in more fundamental changes in lifestyle etc. that have led to decreased exposure to certain microbial or other species, such as helminths, that are important for development of immuno-regulatory mechanisms.[56] There is still much uncertainty as to which lifestyle factors are involved.		Although media coverage of the hygiene hypothesis has declined, a strong ‘collective mindset’ has become established that dirt is ‘healthy’ and hygiene somehow ‘unnatural’. This has caused concern among health professionals that everyday life hygiene behaviours, which are the foundation of public health, are being undermined. In response to the need for effective hygiene in home and everyday life settings, the International Scientific Forum on Home Hygiene has developed a "risk-based" or targeted approach to home hygiene that seeks to ensure that hygiene measures are focussed on the places, and at the times most critical for infection transmission.[7] Whilst targeted hygiene was originally developed as an effective approach to hygiene practice, it also seeks, as far as possible, to sustain "normal" levels of exposure to the microbial flora of our environment to the extent that is important to build a balanced immune system.		Excessive body hygiene of the ear canals can result in infection or irritation. The ear canals require less body hygiene care than other parts of the body, because they are sensitive, and the body system adequately cares for these parts. Most of the time the ear canals are self-cleaning; that is, there is a slow and orderly migration of the skin lining the ear canal from the eardrum to the outer opening of the ear. Old earwax is constantly being transported from the deeper areas of the ear canal out to the opening where it usually dries, flakes, and falls out.[57] Attempts to clean the ear canals through the removal of earwax can actually reduce ear canal cleanliness by pushing debris and foreign material into the ear that the natural movement of ear wax out of the ear would have removed. Excessive application of soaps, creams, and ointments can also adversely affect certain of the natural processes of the skin. For examples, soaps and ointments can deplete the skin of natural protective oils and fat-soluble content such as cholecalciferol (vitamin D3), and external substances can be absorbed, to disturb natural hormonal balances.[citation needed]		Culinary hygiene pertains to the practices related to food management and cooking to prevent food contamination, prevent food poisoning and minimize the transmission of disease to other foods, humans or animals. Culinary hygiene practices specify safe ways to handle, store, prepare, serve and eat food.		Culinary practices include:		Personal service hygiene pertains to the practices related to the care and use of instruments used in the administration of personal care services to people:		Personal hygiene practices include:		Sleep hygiene is the recommended behavioral and environmental practice that is intended to promote better quality sleep.[58] This recommendation was developed in the late 1970s as a method to help people with mild to moderate insomnia, but, as of 2014[update], the evidence for effectiveness of individual recommendations is "limited and inconclusive".[58] Clinicians assess the sleep hygiene of people who present with insomnia and other conditions, such as depression, and offer recommendations based on the assessment. Sleep hygiene recommendations include establishing a regular sleep schedule, using naps with care, not exercising physically or mentally too close to bedtime, limiting worry, limiting exposure to light in the hours before sleep, getting out of bed if sleep does not come, not using bed for anything but sleep and avoiding alcohol as well as nicotine, caffeine, and other stimulants in the hours before bedtime, and having a peaceful, comfortable and dark sleep environment.		The earliest written account of Elaborate codes of hygiene can be found in several Hindu texts, such as the Manusmriti and the Vishnu Purana.[59] Bathing is one of the five Nitya karmas (daily duties) in Hinduism, and not performing it leads to sin, according to some scriptures.		Regular bathing was a hallmark of Roman civilization.[60] Elaborate baths were constructed in urban areas to serve the public, who typically demanded the infrastructure to maintain personal cleanliness. The complexes usually consisted of large, swimming pool-like baths, smaller cold and hot pools, saunas, and spa-like facilities where individuals could be depilated, oiled, and massaged. Water was constantly changed by an aqueduct-fed flow. Bathing outside of urban centers involved smaller, less elaborate bathing facilities, or simply the use of clean bodies of water. Roman cities also had large sewers, such as Rome's Cloaca Maxima, into which public and private latrines drained. Romans didn't have demand-flush toilets but did have some toilets with a continuous flow of water under them.		Until the late 19th Century, only the elite in Western cities typically possessed indoor facilities for relieving bodily functions. The poorer majority used communal facilities built above cesspools in backyards and courtyards. This changed after Dr. John Snow discovered that cholera was transmitted by the fecal contamination of water. Though it took decades for his findings to gain wide acceptance, governments and sanitary reformers were eventually convinced of the health benefits of using sewers to keep human waste from contaminating water. This encouraged the widespread adoption of both the flush toilet and the moral imperative that bathrooms should be indoors and as private as possible.[61]		Christianity has always placed a strong emphasis on hygiene,[62] Despite the denunciation of the mixed bathing style of Roman pools by early Christian clergy, as well as the pagan custom of women naked bathing in front of men, this did not stop the Church from urging its followers to go to public baths for bathing,[63] which contributed to hygiene and good health according to the Church Father, Clement of Alexandria. The Church also built public bathing facilities that were separate for both sexes near monasteries and pilgrimage sites; also, the popes situated baths within church basilicas and monasteries since the early Middle Ages.[64] Pope Gregory the Great urged his followers on value of bathing as a bodily need.[65]		Contrary to popular belief[66] and although the Early Christian leaders, such as Boniface I,[67] condemned bathing as unspiritual,[68] bathing and sanitation were not lost in Europe with the collapse of the Roman Empire.[69][70] Soapmaking first became an established trade during the so-called "Dark Ages". The Romans used scented oils (mostly from Egypt), among other alternatives.		Northern Europeans were not in the habit of bathing: in the ninth century Notker the Stammerer, a Frankish monk of St Gall, related a disapproving anecdote that attributed ill results of personal hygiene to an Italian fashion:		There was a certain deacon who followed the habits of the Italians in that he was perpetually trying to resist nature. He used to take baths, he had his head very closely shaved, he polished his skin, he cleaned his nail, he had his hair cut as short as if it were turned on a lathe, and he wore linen underclothes and a snow-white shirt.		Secular medieval texts constantly refer to the washing of hands before and after meals, but Sone de Nansay, hero of a 13th-century romance, discovers to his chagrin that the Norwegians do not wash up after eating.[71] In the 11th and 12th centuries, bathing was essential to the Western European upper class: the Cluniac monasteries to which they resorted or retired were always provided with bathhouses, and even the monks were required to take full immersion baths twice a year, at the two Christian festivals of renewal, though exhorted not to uncover themselves from under their bathing sheets.[72] In 14th century Tuscany, the newlywed couple's bath together was such a firm convention one such couple, in a large coopered tub, is illustrated in fresco in the town hall of San Gimignano.[73]		Bathing had fallen out of fashion in Northern Europe long before the Renaissance, when the communal public baths of German cities were in their turn a wonder to Italian visitors. Bathing was replaced by the heavy use of sweat-bathing and perfume, as it was thought in Europe that water could carry disease into the body through the skin. Bathing encouraged an erotic atmosphere that was played upon by the writers of romances intended for the upper class;[74] in the tale of Melusine the bath was a crucial element of the plot. "Bathing and grooming were regarded with suspicion by moralists, however, because they unveiled the attractiveness of the body. Bathing was said to be a prelude to sin, and in the penitential of Burchard of Worms we find a full catalogue of the sins that ensued when men and women bathed together."[75] Medieval church authorities believed that public bathing created an environment open to immorality and disease; the 26 public baths of Paris in the late 13th century were strictly overseen by the civil authorities.[75] At a later date Roman Catholic Church officials even banned public bathing in an unsuccessful effort to halt syphilis epidemics from sweeping Europe.[76]		Modern sanitation was not widely adopted until the 19th and 20th centuries. According to medieval historian Lynn Thorndike, people in Medieval Europe probably bathed more than people did in the 19th century.[77] Some time after Louis Pasteur's experiments proved the germ theory of disease and Joseph Lister and others put them into practice in sanitation, hygienic practices came to be regarded as synonymous with health, as they are in modern times.		Since the 7th century, Islam has always placed a strong emphasis on hygiene. Other than the need to be ritually clean in time for the daily prayer (Arabic: Salat) through Wuzu and Ghusl, there are a large number of other hygiene-related rules governing the lives of Muslims. Other issues include the Islamic dietary laws. In general, the Qur'an advises Muslims to uphold high standards of physical hygiene and to be ritually clean whenever possible.		
In politics, humanitarian aid, and social science, hunger is a condition in which a person, for a sustained period, is unable to eat sufficient food to meet basic nutritional needs.		Throughout history, portions of the world's population have often experienced sustained periods of hunger. In many cases, this resulted from food supply disruptions caused by war, plagues, or adverse weather. For the first few decades after World War II, technological progress and enhanced political cooperation suggested it might be possible to substantially reduce the number of people suffering from hunger. While progress was uneven, by 2000 the threat of extreme hunger subsided for many of the world's people. According to the WFP some statistics are that, "Some 795 million people in the world do not have enough food to lead a healthy active life. That's about one in nine people on earth. The vast majority of the world's hungry people live in developing countries, where 12.9 percent of the population is undernourished."[1]		Until 2006, the average international price of food had been largely stable for several decades. In the closing months of 2006, however, prices began to rise rapidly. By 2008, rice had tripled in price in some regions, and this severely affected developing countries. Food prices fell in early 2009, but rose to another record high in 2011, and have since decreased slightly. The 2008 worldwide financial crisis further increased the number of people suffering from hunger, including dramatic increases even in advanced economies such as Great Britain, the Eurozone and the United States.		The Millennium Development Goals included a commitment to a further 50% reduction in the proportion of the world's population who suffer from extreme hunger by 2015. As of 2012, this target appeared difficult to achieve, due in part to persistent inflation in food prices. However, in late 2012 the UN's Food and Agriculture Organization (FAO) stated it is still possible to hit the target with sufficient effort. In 2013, the FAO estimated that 842 million people are undernourished (12% of the global population). Malnutrition is a cause of death for more than 3.1 million children under 5 every year. UNICEF estimates 300 million children go to bed hungry each night; and that 8000 children under the age of 5 are estimated to die of malnutrition every day.[2]						The physical sensation of hunger is related to contractions of the stomach muscles. These contractions—sometimes called hunger pangs once they become severe—are believed to be triggered by high concentrations of the ghrelin hormone. The hormones Peptide YY and Leptin can have an opposite effect on the appetite, causing the sensation of being full. Ghrelin can be released if blood sugar levels get low—a condition that can result from long periods without eating. Stomach contractions from hunger can be especially severe and painful in children and young adults.		Hunger pangs can be made worse by irregular meals. People who can't afford to eat more than once a day sometimes refuse one-off additional meals, because if they don't eat at around the same time on the next days, they may suffer extra severe hunger pangs.[3] Older people may feel less violent stomach contractions when they get hungry, but still suffer the secondary effects resulting from low food intake: these include weakness, irritability and decreased concentration. Prolonged lack of adequate nutrition also causes increased susceptibility to disease and reduced ability for the body to self heal.[4][5]		In both developing and advanced countries, parents sometimes go without food so they can feed their children. Women, however, seem more likely to make this sacrifice than men. World Bank studies consistently find that about 60% of those who are hungry are female. The apparent explanation for this imbalance is that, compared to men, women more often forgo meals in order to feed their children.		Older sources sometimes claim this phenomenon is unique to developing countries, due to greater sexual inequality. More recent findings suggested that mothers often miss meals in advanced economies too. For example, a 2012 study undertaken by Netmums in the UK found that one in five mothers sometimes misses out on food to save their children from hunger.[6][7][8]		In several periods and regions, gender has also been an important factor determining whether or not victims of hunger would make suitable examples for generating enthusiasm for hunger relief efforts. James Vernon, in his Hunger: A Modern History, wrote that in Britain before the 20th century, it was generally only women and children suffering from hunger who could arouse compassion. Men who failed to provide for themselves and their families were often regarded with contempt.		This changed after World War I, where thousands of men who had proved their manliness in combat found themselves unable to secure employment. Similarly, female gender could be advantageous for those wishing to advocate for hunger relief, with Vernon writing that being a woman helped Emily Hobhouse draw the plight of hungry people to wider attention during the Second Boer War.[9]		The annual FAO, WFP and IFAD The State of Food Insecurity in the World reports provide a statistical overview on hunger, and are usually considered the main reference in this regard (e.g., for the Millennium Development Goals). However, it is important to note that they have several caveats. First, undernourishment is defined solely in terms of dietary energy availability (i.e., disregarding micro-nutrients such as vitamins or minerals). Second, it uses the energy requirements for minimum activity levels as a benchmark, whereas many hungry people most likely face hard manual labour. Third, the numbers do not reflect short-term undernourishment (e.g., from food price shocks), unless they change long-term food consumption.		In October 2012, the FAO published a report saying that their earlier 2009 estimate that one Billion people were suffering from chronic hunger was over stated, due to flawed methodology resulting from the pressure they were under to quickly estimate the effects of the financial crisis on hunger. They also said the number of people currently suffering from chronic hunger is close to 842 million.		According to the United States Department of Agriculture in 2015, 50 million Americans experienced food insecurity in 2009, including 17 million children. This represents nearly one in four American children.[10][11][12]		The Global Hunger Index (GHI) is a multidimensional statistical tool used to describe the state of countries’ hunger situation. The GHI measures progress and failures in the global fight against hunger.[13] The GHI is updated once a year. The data from the 2015 report shows that Hunger levels have dropped 27% since 2000. Fifty two countries remain at serious or alarming levels. In addition to the latest statistics on Hunger and Food Security, the GHI also features different special topics each year. The 2015 report include an article on conflict and food security.[14]		Ghana has already taken major steps to eradicating hunger and completing the UN 2020 goals. Steps such as lowering taxes on farmers, and funding resources and tools required to enhance production. These steps caused Ghana's GDP per capita to increase from $400 in 2001 to $1,300 in 2007. However, multiple problems also prevent Ghana from reaching its full potential and eradicate hunger completely, such as having a stable supply of electricity, as well as balancing the amount of food production in the more industrial southern region of Ghana and the more agricultural northern region, the latter of which accounts for 63% of the overall number of people in Ghana living below the poverty line. The total population is about 27 million, meaning that 63% would approximately be 16,317,000 out of 26,900,000. 45% of the poverty-stricken population is living on less than $1.25, and Ghana is known as a "lower-middle income" country, meaning that its per-capita income is between $400 and $4000.		Although the United States Department of Agriculture reported in 2012 that an estimated 85.5 percent of households in the country are food secure, millions of people in America struggle with the threat of hunger or experience hunger on a daily basis.[15] The USDA defines food security as the economic condition of a household wherein which there is reliable access to a sufficient amount of food so all household members can lead a healthy productive life.[16] Hunger is most commonly related to poverty since a lack of food helps perpetuate the cycle of poverty. Most obviously, when individuals live in poverty they lack the financial resources to purchase food or paid for unexpected events, such as a medical emergency. When such emergencies arise, families are forced to cut back on food spending so they can meet the financial demands of the unexpected emergency.[17] There is not one single cause of hunger but rather a complex interconnected web of various factors. Some of the most vulnerable populations to hunger are the elderly, children, people from a low socioeconomic status, and minority groups; however, hunger's impact is not limited to these individuals.		The largest nonprofit food relief organization in the United States, Feeding America, feeds 46.5 million citizens a year to address the nation's food insecurity issue.[16] This equates to one in seven Americans requiring their aid in a given year. An organization that focuses on providing food for the elderly population is Meals on Wheels, which is a nonprofit that delivers meals to seniors' homes. The government also works towards providing relief through programs such as the Supplemental Nutrition Assistance Program (SNAP) which was formerly known to the public as Food Stamps. Another well known government program is the National School Lunch Program (NSLP) which provides free or reduced lunches to students who qualify for the program.		The number of Americans suffering from hunger rose after the 2008 financial crisis, with children and working adults now making up a large proportion of those affected. In 2012, Gleaners Indiana Food bank reported that there were now 50 million Americans struggling with food insecurity (about 1 in 6 of the population), and that the number of folks seeking help from food banks had increased by 46% since 2005.[18] According to a 2012 study by UCLA Center for Health Policy Research, even married couples who both work but have low incomes sometimes require the aid of food banks.[19][20]		Throughout history, the need to aid those suffering from hunger has been commonly, though not universally,[21] recognized.		The philosopher Simone Weil wrote that feeding the hungry when you have resources to do so is the most obvious of all human obligations. She says that as far back as Ancient Egypt, many believed that people had to show they had helped the hungry in order to justify themselves in the afterlife. Weil writes that Social progress is commonly held to be first of all, "...a transition to a state of human society in which people will not suffer from hunger." [22] Social historian Karl Polanyi wrote that before markets became the world's dominant form of economic organization in the 19th century, most human societies would either starve all together or not at all, because communities would invariably share their food.[23]		Hunger as an academic and social topic came to prominence during the Great Depression. As many individuals struggled for food, the same agricultural industries were suddenly producing large surpluses as means of increased production to counter the drop in demand from the European markets. This increased output was meant to ease the growing debt levels, however domestic demand could not keep up with prices. Instead, what is often called "the paradox of want amid plenty," agricultural surpluses and large demand simply did not fit together, causing the Hoover administration to buy large amounts of product, such as grain, to stabilize prices. Initially refusing to further compromise the distressed price levels, political pressure from starving families across the country forced Congress to reconsider. With large deposits of grain already wasting away in government possession, the only political move left was to begin a process of donations to the hungry from the Farm Board, a federal oversight created in 1929 to promote the sale and stabilization of agricultural products. Instead of hunger being a reason for the allocation of large grain surpluses, waste became the eventual driving force.[24]		From the first age of globalization, which began in the 19th century, it became more common for people to consider problems like hunger in global terms. However, as early globalization largely coincided with the high peak of influence for classical liberalism, there was relatively little call for politicians to address world hunger.[25][26]		In the late nineteenth and early twentieth century, the view that politicians ought not to intervene against hunger was increasingly challenged by campaigning journalists, with some academics and politicians also calling for or organizing intervention against world hunger, such as U.S. President Woodrow Wilson.[9] [27] [28][29]		After World War II, a new international politico-economic order came into being, which was later described as Embedded liberalism.		For at least the first decade after the war, the United States, by far the period's most dominant national actor, was strongly supportive of efforts to tackle world hunger and to promote international development. It heavily funded the United Nation's development programmes, and later the efforts of other multilateral organizations like the International Monetary Fund (IMF) and the World Bank (WB).[27][29][30]		The newly established United Nations became a leading player in co-ordinating the global fight against hunger. The UN has three agencies that work to promote food security and agricultural development: the Food and Agriculture Organization (FAO), the World Food Programme (WFP) and the International Fund for Agricultural Development (IFAD). FAO is the world's agricultural knowledge agency, providing policy and technical assistance to developing countries to promote food security, nutrition and sustainable agricultural production, particularly in rural areas.		WFP's key mission is to deliver food into the hands of the hungry poor. The agency steps in during emergencies and uses food to aid recovery after emergencies. Its longer term approaches to hunger helps the transition from recovery to development. IFAD, with its knowledge of rural poverty and exclusive focus on poor rural people, designs and implements programmes to help those people access the assets, services and opportunities they need to overcome poverty.[27][29][30]		Following successful post WWII reconstruction of Germany and Japan, the IMF and WB began to turn their attention to the developing world. A great many civil society actors were also active in trying to combat hunger, especially after the late 1970s when global media began to bring the plight of starving people in places like Ethiopia to wider attention. Most significant of all, especially in the late 1960s and 70s, the Green revolution helped improved agricultural technology propagate throughout the world.[27][29][30]		The United States began to change its approach to the problem of world hunger from about the mid 1950s. Influential members of the administration became less enthusiastic about methods they saw as promoting an over reliance on the state, as they feared that might assist the spread of communism.[citation needed]		Despite this view, during the 1960s postwar era hunger within the United States was overshadowed by hunger in Europe and Asia. John F. Kennedy did in fact use Executive Order to double the amount of commodities available from the surplus commodity program as well as initiated the pilot Food Stamp Program which later became permanent in 1964.[31]		By the 1980s, the previous consensus in favour of moderate government intervention had been displaced across the western world. The IMF and World Bank in particular began to promote market-based solutions. In cases where countries became dependent on the IMF, they sometimes forced national governments to prioritize debt repayments and sharply cut public services. This sometimes had a negative effect on efforts to combat hunger.[32][33][34]		Organizations such as Food First raised the issue of food sovereignty and claimed that every country on earth (with the possible minor exceptions of some city-states) has sufficient agricultural capacity to feed its own people, but that the "free trade" economic order, which from the late 1970s to about 2008 had been associated with such institutions as the IMF and World Bank, had prevented this from happening.[citation needed]		The World Bank itself claimed it was part of the solution to hunger, asserting that the best way for countries to break the cycle of poverty and hunger was to build export-led economies that provide the financial means to buy foodstuffs on the world market. However, in the early 21st century the World Bank and IMF became less dogmatic about promoting free market reforms. They increasingly returned to the view that government intervention does have a role to play, and that it can be advisable for governments to support food security with policies favourable to domestic agriculture, even for countries that do not have a Comparative advantage in that area. As of 2012, the World Bank remains active in helping governments to intervene against hunger.[6][27][29][30][35]		Until at least the 1980s—and, to an extent, the 1990s—the dominant academic view concerning world hunger was that it was a problem of demand exceeding supply. Proposed solutions often focused on boosting food production, and sometimes on birth control. There were exceptions to this, even as early as the 1940s, Lord Boyd-Orr, the first head of the UN's FAO, had perceived hunger as largely a problem of distribution, and drew up comprehensive plans to correct this. Few agreed with him at the time, however, and he resigned after failing to secure support for his plans from the US and Great Britain. In 1998, Amartya Sen won a Nobel Prize in part for demonstrating that hunger in modern times is not typically the product of a lack of food. Rather, hunger usually arises from food distribution problems, or from governmental policies in the developed and developing world. It has since been broadly accepted that world hunger results from issues with the distribution as well as the production of food.[32][33][34] Sen's 1981 essay Poverty and Famines: An Essay on Entitlement and Deprivation played a prominent part in forging the new consensus.[29][36]		In 2007 and 2008, rapidly increasing food prices caused a global food crisis, increasing the numbers suffering from hunger by over a hundred million. Food riots erupted in several dozen countries; in at least two cases, Haiti and Madagascar, this led to the toppling of governments. A second global food crisis unfolded due to the spike in food prices of late 2010 and early 2011. Fewer food riots occurred, due in part to greater availability of food stock piles for relief. However, several analysts argue the food crisis was one of the causes of the Arab Spring.[30][37][38]		The World Hunger Education Service works to educate on the subject matter of world malnutrition, and this group has helped people learn about our world hunger problems. There are also several smaller organizations that work to educate people on poverty in their homes and neighborhoods around the globe.		In the early 21st century, there was relatively little awareness of hunger from leaders of advanced nations such as those that form the G8.[37] Prior to 2009, efforts to fight hunger were mainly undertaken by governments of the worst affected countries, by civil society actors, and by multilateral and regional organizations. In 2009, Pope Benedict published his third encyclical, Caritas in Veritate, which emphasised the importance of fighting against hunger. The encyclical was intentionally published immediately before the July 2009 G8 Summit to maximise its influence on that event. At the Summit, which took place at L'Aquila in central Italy, the L'Aquila Food Security Initiative was launched, with a total of US$22 billion was committed to combat hunger.[39][40]		Food prices did fall sharply in 2009 and early 2010, though analysts credit this much more to farmers increasing production in response to the 2008 spike in prices, than to the fruits of enhanced government action. However, since the 2009 G8 summit, the fight against hunger has remained a high-profile issue among the leaders of the worlds major nations, and was a prominent part of the agenda for the 2012 G-20 summit.[37] [41] [42]		In April 2012, the Food Assistance Convention was signed, the world's first legally binding international agreement on food aid. The May 2012 Copenhagen Consensus recommended that efforts to combat hunger and malnutrition should be the first priority for politicians and private sector philanthropists looking to maximize the effectiveness of aid spending. They put this ahead of other priorities, like the fight against malaria and AIDS.[43] Also in May 2012, U.S. President Barack Obama launched a "new alliance for food security and nutrition"—a broad partnership between private sector, governmental and civil society actors—that aimed to "...achieve sustained and inclusive agricultural growth and raise 50 million people out of poverty over the next 10 years."[32][41][44][45] The UK's prime minister David Cameron held a hunger summit on 12 August, the last day of the 2012 Summer Olympics.[41]		The fight against hunger has also been joined by an increased number of regular people. While folk throughout the world had long contributed to efforts to alleviate hunger in the developing world, there has recently been a rapid increase in the numbers involved in tackling domestic hunger even within the economically advanced nations of the Global North.		This had happened much earlier in North America than it did in Europe. In the US, the Reagan administration scaled back welfare the early 1980s, leading to a vast increase of charity sector efforts to help Americans unable to buy enough to eat. According to a 1992 survey of 1000 randomly selected US voters, 77% of Americans had contributed to efforts to feed the hungry, either by volunteering for various hunger relief agencies such as food banks and soup kitchens, or by donating cash or food.[46] Europe, with its more generous welfare system, had little awareness of domestic hunger until the food price inflation that began in late 2006, and especially as austerity-imposed welfare cuts began to take effect in 2010. Various surveys reported that upwards of 10% of Europe's population had begun to suffer from food insecurity. Especially since 2011, there has been a substantial increase in grass roots efforts to help the hungry by means of food banks, within both the UK and continental Europe.[3][47][48][49][50]		By July 2012, the 2012 US drought had already caused a rapid increase in the price of grain and soy, with a knock on effect on the price of meat. As well as affecting hungry people in the US, this caused prices to rise on the global markets; the US is the world's biggest exporter of food. This led to much talk of a possible third 21st century global food crisis. The Financial Times reported that the BRICS may not be as badly affected as they were in the earlier crises of 2008 and 2011. However, smaller developing countries that must import a substantial portion of their food could be hard hit. The UN and G20 has begun contingency planning so as to be ready to intervene if a third global crisis breaks out.[6][38][51][52] By August 2013 however, concerns had been allayed, with above average grain harvests expected from major exporters, including Brazil, Ukraine and the U.S.[53] 2014 also saw a good worldwide harvest, leading to speculation that grain prices could soon begin to fall.[54]		In an April 2013 summit held in Dublin concerning Hunger, Nutrition, Climate Justice, and the post 2015 MDG framwework for global justice, Ireland's President Higgins said that only 10% of deaths from hunger are due to armed conflict and natural disasters, with ongoing hunger being both the "greatest ethical failure of the current global system" and the "greatest ethical challenge facing the global community."[55] $4.15 billion of new commitments were made to tackle hunger at a June 2013 Hunger Summit held in London, hosted by the governments of Britain and Brazil, together with The Children's Investment Fund Foundation.[56][57]		The EndingHunger campaign is an online communication campaign aimed at raising awareness of the hunger problem. It has many worked through viral videos depicting celebrities voicing their anger about the large number of hungry people in the world.		The main global policy to reduce hunger and poverty are the recently approved Sustainable Development Goals. In particular Goal 2: Zero Hunger sets globally agreed targets to end hunger, achieve food security and improved nutrition and promote sustainable agriculture.[58]		While SDG 2 aims for an end to hunger in 2030, a number of organizations have formed initiatives with the more ambitious goal to achieve this outcome in only 10 years, by 2025:		Goal # 1 of Millennium Development Goals in 2000 states the following plan:		A food bank or foodbank is a non-profit, charitable organization that distributes food to those who have difficulty purchasing enough food to avoid hunger.		A soup kitchen, meal center, or food kitchen is a place where food is offered to the hungry for free or at a below market price. Frequently located in lower-income neighborhoods, they are often staffed by volunteer organizations, such as church or community groups. Soup kitchens sometimes obtain food from a food bank for free or at a low price, because they are considered a charity, which makes it easier for them to feed the many people who require their services.		A basic income (also called unconditional basic income, basic income guarantee, universal basic income or universal demogrant[67]) is a form of social security[68] in which all citizens or residents of a country regularly receive an unconditional sum of money, either from a government or some other public institution, in addition to any income received from elsewhere.		
Thali (Hindi/Nepali: थाली, Tamil: தட்டு; meaning "plate") is the Indian name for the platter, which could also refer to an Indian-style meal, made up of a selection of various dishes, served on a platter. It simply means a round platter used to serve food. The 'thali' style meal serving is popular in India, Nepal, Bangladesh and Singapore.						As noted by INTACH, the earliest evidence of use of continuity in cooking and food habits of India can be established by the existence of tandoor (cooking oven), thali, lotas and chakla-belan for making chapatis found in excavations at Indus Valley Civilization site of Kalibangan (3500 BCE - 2500 BCE).[1]		Thaali refer to a metal plate that thali meal may be served on.[2] According to Indian food serving customs, a proper meal should be a perfect balance of all these 6 flavors. The idea behind a Thali is to offer all the 6 different flavors of sweet, salt, bitter, sour, astringent and spicy on one single plate (although the latter two flavors are actually forms of chemesthesis). Restaurants typically offer a choice of vegetarian or meat-based thalis. Vegetarian thalis are very typical and commonplace in Tamil Nadu canteens (and South India in general), and is a popular lunch choice.		Dishes served in a Thali vary from region to region in South Asia and are usually served in small bowls, called katori. These 'katoris' are placed along the borders of the round tray - the actual thali; even a steel tray with multiple compartments is used. Typical dishes include rice, dal, vegetables, roti, papad, curd (yoghurt), small amounts of chutney or pickle, and a sweet dish to top it off.[3] Rice or Roti is the usual main dish which occupies the central portion of the Thali, while the side dishes like vegetable curries and other aforementioned delicacies are lined circularly along the round Thali. Depending on the restaurant or the region, the thali consists of delicacies native to that region. In general, a thali begins with different types of breads such as puris or chapatis (rotis) and different vegetarian specialities (curries). However, in South India, rice is the only staple served with thalis. Thalis are sometimes referred to by the regional characteristic of the dishes they contain. For example, one may encounter Nepalese thali, Rajasthani thali, Gujarati thali and Maharashtrian thali. In many parts of India and Nepal, the bread and the rice portions are not served together in the thali. Typically, the bread is offered first with rice being served afterwards, often in a separate bowl or dish.		Unlimited thalis are those that come with limitless refills.[4][5] Kunal Vijaykar considers an unlimited thali as quintessentially Indian, not just for variety or limitlessness, but because it is true to Indian tradition.[6]Ajanta, a restaurant in Dubai, serves an unlimited thali but charges a penalty for leaving food on the plate uneaten.[7]		
2LNE, 2LNF, 2LNG, 2LYW, 3F6K, 2OYV, 2OYW, 4PO7		4922		67405		ENSG00000133636		ENSMUSG00000019890		P30990 Q6FH20		Q9D3P9		NM_006183		NM_024435		NP_006174 NP_006174.1		NP_077755		Neurotensin is a 13 amino acid neuropeptide that is implicated in the regulation of luteinizing hormone and prolactin release and has significant interaction with the dopaminergic system. Neurotensin was first isolated from extracts of bovine hypothalamus based on its ability to cause a visible vasodilation in the exposed cutaneous regions of anesthetized rats.[3]		Neurotensin is distributed throughout the central nervous system, with highest levels in the hypothalamus, amygdala and nucleus accumbens. It induces a variety of effects, including analgesia, hypothermia and increased locomotor activity. It is also involved in regulation of dopamine pathways. In the periphery, neurotensin is found in enteroendocrine cells of the small intestine, where it leads to secretion and smooth muscle contraction.[4]						Neurotensin shares significant sequence similarity in its 6 C-terminal amino acids with several other neuropeptides, including neuromedin N (which is derived from the same precursor). This C-terminal region is responsible for the full biological activity, the N-terminal portion having a modulatory role. The neurotensin/neuromedin N precursor can also be processed to produce large 125–138 amino acid peptides with the neurotensin or neuromedin N sequence at their C terminus. These large peptides appear to be less potent than their smaller counterparts, but are also less sensitive to degradation and may represent endogenous, long-lasting activators in a number of pathophysiological situations.		The sequence of bovine neurotensin was determined to be pyroGlu-Leu-Tyr-Glu-Asn-Lys-Pro-Arg-Arg-Pro-Tyr-Ile-Leu-OH.[5] Neurotensin is synthesized as part of a 169 or 170 amino acid precursor protein that also contains the related neuropeptide neuromedin N.[6][7] The peptide coding domains are located in tandem near the carboxyl terminal end of the precursor and are bounded and separated by paired basic amino acid (lysine-arginine) processing sites.		Neurotensin is a potent mitogen for colorectal cancer.[8]		Neurotensin has been implicated in the modulation of dopamine signaling, and produces a spectrum of pharmacological effects resembling those of antipsychotic drugs, leading to the suggestion that neurotensin may be an endogenous neuroleptic. Neurotensin-deficient mice display defects in responses to several antipsychotic drugs consistent with the idea that neurotensin signaling is a key component underlying at least some antipsychotic drug actions.[9] These mice exhibit modest defects in prepulse inhibition (PPI) of the startle reflex, a model that has been widely used to investigate antipsychotic drug action in animals. Antipsychotic drug administration augments PPI under certain conditions. Comparisons between normal and neurotensin-deficient mice revealed striking differences in the ability of different antipsychotic drugs to augment PPI. While the atypical antipsychotic drug clozapine augmented PPI normally in neurotensin-deficient mice, the conventional antipsychotic haloperidol and the newer atypical antipsychotic quetiapine were ineffective in these mice, in contrast to normal mice where these drugs significantly augmented PPI. These results suggest that certain antipsychotic drugs require neurotensin for at least some of their effects. Neurotensin-deficient mice also display defects in striatal activation following haloperidol, but not clozapine administration in comparison to normal wild type mice, indicating that striatal neurotensin is required for the full spectrum of neuronal responses to a subset of antipsychotic drugs.[10]		Neurotensin is an endogenous neuropeptide involved in thermoregulation that can induce hypothermia and neuroprotection in experimental models of cerebral ischemia.[11]		
Anorexia nervosa, often referred to simply as anorexia,[10] is an eating disorder characterized by low weight, fear of gaining weight, and a strong desire to be thin, resulting in food restriction.[1] Many people with anorexia see themselves as overweight even though they are in fact underweight.[1][2] If asked they usually deny they have a problem with low weight.[3] Often they weigh themselves frequently, eat only small amounts, and only eat certain foods.[1] Some will exercise excessively, force themselves to vomit, or use laxatives to produce weight loss.[1] Complications may include osteoporosis, infertility and heart damage, among others.[1] Women will often stop having menstrual periods.[3]		The cause is not known.[2] There appear to be some genetic components with identical twins more often affected than non-identical twins.[2] Cultural factors also appear to play a role with societies that value thinness having higher rates of disease.[3] Additionally, it occurs more commonly among those involved in activities that value thinness such as high-level athletics, modelling, and dancing.[3][4] Anorexia often begins following a major life-change or stress-inducing event.[3] The diagnosis requires a significantly low weight.[3] The severity of disease is based on body mass index (BMI) in adults with mild disease having a BMI of greater than 17, moderate a BMI of 16 to 17, severe a BMI of 15 to 16, and extreme a BMI less than 15.[3] In children a BMI for age percentile of less than the 5th percentile is often used.[3]		Treatment of anorexia involves restoring a healthy weight, treating the underlying psychological problems, and addressing behaviors that promote the problem.[1] While medications do not help with weight gain, they may be used to help with associated anxiety or depression.[1] A number of types of therapy may be useful including an approach where parents assume responsibility for feeding their child, known as Maudsley family therapy and cognitive behavioral therapy.[1][11] Sometimes people require admission to hospital to restore weight.[6] Evidence for benefit from nasogastric tube feeding, however, is unclear.[12] Some people will just have a single episode and recover while others may have many episodes over years.[6] Many complications improve or resolve with regaining of weight.[6]		Globally, anorexia is estimated to affect 2.9 million people as of 2015.[8] It is estimated to occur in 0.9% to 4.3% of women and 0.2% to 0.3% of men in Western countries at some point in their life.[13] About 0.4% of young women are affected in a given year and it is estimated to occur ten times less commonly in men.[3][13] Rates in most of the developing world are unclear.[3] Often it begins during the teen years or young adulthood.[1] While anorexia became more commonly diagnosed during the 20th century it is unclear if this was due to an increase in its frequency or simply better diagnosis.[2] In 2013 it directly resulted in about 600 deaths globally, up from 400 deaths in 1990.[14] Eating disorders also increase a person's risk of death from a wide range of other causes, including suicide.[1][13] About 5% of people with anorexia die from complications over a ten-year period, a nearly 6 times increased risk.[3][7] The term anorexia nervosa was first used in 1873 by William Gull to describe this condition.[15]		Anorexia nervosa is an eating disorder characterized by attempts to lose weight, to the point of starvation. A person with anorexia nervosa may exhibit a number of signs and symptoms, the type and severity of which may vary and may be present but not readily apparent.[16]		Anorexia nervosa, and the associated malnutrition that results from self-imposed starvation, can cause complications in every major organ system in the body.[17] Hypokalaemia, a drop in the level of potassium in the blood, is a sign of anorexia nervosa.[18][19] A significant drop in potassium can cause abnormal heart rhythms, constipation, fatigue, muscle damage and paralysis.[20]		Symptoms may include:		Anorexia has classically been associated with disturbances to interoception with people hyper-focused on their bodies. People with anorexia concentrate on distorted perceptions of their body exterior in fear of weight gain, and they also report altered physical states within their bodies, such as indistinct feelings of fullness or are not able to distinguish emotional states from bodily sensations in general (also called alexithymia).[26] Further, people with anorexia experience abnormally intense cardiorespiratory sensations, particularly of the breath, and this is most prevalent before they consume a meal. Abnormal senses of interoceptive awareness such as these have been observed so frequently in anorexia that they have become key characteristics of the illness.[26]		Other psychological issues may factor into anorexia nervosa; some fulfill the criteria for a separate Axis I diagnosis or a personality disorder which is coded Axis II and thus are considered comorbid to the diagnosed eating disorder. Some people have a previous disorder which may increase their vulnerability to developing an eating disorder and some develop them afterwards.[medical citation needed] The presence of Axis I or Axis II psychiatric comorbidity has been shown to affect the severity and type of anorexia nervosa symptoms in both adolescents and adults.[medical citation needed]		Obsessive-compulsive disorder (OCD) and obsessive-compulsive personality disorder (OCPD) are highly comorbid with AN, particularly the restrictive subtype.[27] Obsessive-compulsive personality disorder is linked with more severe symptomatology and worse prognosis.[28] The causality between personality disorders and eating disorders has yet to be fully established.[medical citation needed] Other comorbid conditions include depression,[29] alcoholism,[30] borderline and other personality disorders,[31][32] anxiety disorders,[33] attention deficit hyperactivity disorder,[34] and body dysmorphic disorder (BDD).[35] Depression and anxiety are the most common comorbidities,[36] and depression is associated with a worse outcome.[36]		Autism spectrum disorders occur more commonly among people with eating disorders than in the general population.[37] Zucker et al. (2007) proposed that conditions on the autism spectrum make up the cognitive endophenotype underlying anorexia nervosa and appealed for increased interdisciplinary collaboration.[38]		There is evidence for biological, psychological, developmental, and sociocultural risk factors, but the exact cause of eating disorders is unknown.[39]		Studies have hypothesized the continuance of disordered eating patterns may be epiphenomena of starvation. The results of the Minnesota Starvation Experiment showed normal controls exhibit many of the behavioral patterns of anorexia nervosa (AN) when subjected to starvation. This may be due to the numerous changes in the neuroendocrine system, which results in a self-perpetuating cycle.[48][49][50]		Another hypothesis is that anorexia nervosa is more likely to occur in populations in which obesity is more prevalent, and results from a sexually selected evolutionary drive to appear youthful in populations in which size becomes the primary indicator of age.[51]		Anorexia nervosa is more likely to occur in a person's pubertal years. Some explanatory hypotheses for the rising prevalence of eating disorders in adolescence are "increase of adipose tissue in girls, hormonal changes of puberty, societal expectations of increased independence and autonomy that are particularly difficult for anorexic adolescents to meet; [and] increased influence of the peer group and its values." [52]		Early theories of the cause of anorexia linked it to childhood sexual abuse or dysfunctional families;[53][54] evidence is conflicting, and well-designed research is needed.[39] The fear of food is known as sitiophobia,[55] cibophobia,[56] or sitophobia and is part of the differential diagnosis.[57][58] Other psychological causes of anorexia include low self-esteem, feeling like there is lack of control, depression, anxiety, and loneliness.[59]		Anorexia nervosa has been increasingly diagnosed since 1950;[60] the increase has been linked to vulnerability and internalization of body ideals.[52] People in professions where there is a particular social pressure to be thin (such as models and dancers) were more likely to develop anorexia,[medical citation needed] and those with anorexia have much higher contact with cultural sources that promote weight loss.[medical citation needed] This trend can also be observed for people who partake in certain sports, such as jockeys and wrestlers.[61] There is a higher incidence and prevalence of anorexia nervosa in sports with an emphasis on aesthetics, where low body fat is advantageous, and sports in which one has to make weight for competition.[62] Family dynamics can play big part in the cause of anorexia.[63] When there is a constant pressure from people to be thin, teasing, bullying can cause low self-esteem and other psychological symptoms.[59]		Constant exposure to media that presents body ideals may constitute a risk factor for body dissatisfaction and anorexia nervosa. The cultural ideal for body shape for men versus women continues to favor slender women and athletic, V-shaped muscular men. A 2002 review found that, of the magazines most popular among people aged 18 to 24 years, those read by men, unlike those read by women, were more likely to feature ads and articles on shape than on diet.[unreliable medical source?][64] Body dissatisfaction and internalization of body ideals are risk factors for anorexia nervosa that threaten the health of both male and female populations.[medical citation needed]		Websites that stress the importance of attainment of body ideals extol and promote anorexia nervosa through the use of religious metaphors, lifestyle descriptions, "thinspiration" or "fitspiration" (inspirational photo galleries and quotes that aim to serve as motivators for attainment of body ideals).[65] Pro-anorexia websites reinforce internalization of body ideals and the importance of their attainment.[65]		The media give men and women a false view of what people truly look like. In magazines, movies and even on billboards most of the actors/models are photoshopped in multiple ways. People then strive to look like these "perfect" role models when in reality they aren't any where near perfection themselves.[66]		A diagnostic assessment includes the person's current circumstances, biographical history, current symptoms, and family history. The assessment also includes a mental state examination, which is an assessment of the person's current mood and thought content, focusing on views on weight and patterns of eating.		Anorexia nervosa is classified under the Feeding and Eating Disorders in the latest revision of the Diagnostic and Statistical Manual of Mental Disorders (DSM 5).		Relative to the previous version of the DSM (DSM-IV-TR), the 2013 revision (DSM5) reflects changes in the criteria for anorexia nervosa, most notably that of the amenorrhea criterion being removed.[6][71] Amenorrhea was removed for several reasons: it does not apply to males, it is not applicable for females before or after the age of menstruation or taking birth control pills, and some women who meet the other criteria for AN still report some menstrual activity.[6]		There are two subtypes of AN:[17][72]		Body mass index (BMI) is used by the DSM-5 as an indicator of the level of severity of anorexia nervosa. The DSM-5 states these as follows:[73]		Medical tests to check for signs of physical deterioration in anorexia nervosa may be performed by a general physician or psychiatrist, including:		A variety of medical and psychological conditions have been misdiagnosed as anorexia nervosa; in some cases the correct diagnosis was not made for more than ten years.		The distinction between the diagnoses of anorexia nervosa, bulimia nervosa and eating disorder not otherwise specified (EDNOS) is often difficult to make as there is considerable overlap between people diagnosed with these conditions. Seemingly minor changes in a people's overall behavior or attitude can change a diagnosis from anorexia: binge-eating type to bulimia nervosa. A main factor differentiating binge-purge anorexia from bulimia is the gap in physical weight. Someone with bulimia nervosa is ordinarily at a healthy weight, or slightly overweight. Someone with binge-purge anorexia is commonly underweight.[87] People with the binge-purging subtype of AN may be significantly underweight and typically do not binge-eat large amounts of food, yet they purge the small amount of food they eat.[87] In contrast, those with bulimia nervosa tend to be at normal weight or overweight and binge large amounts of food.[87] It is not unusual for a person with an eating disorder to "move through" various diagnoses as their behavior and beliefs change over time.[38]		There is no conclusive evidence that any particular treatment for anorexia nervosa works better than others; however, there is enough evidence to suggest that early intervention and treatment are more effective.[88] Treatment for anorexia nervosa tries to address three main areas.		Although restoring the person's weight is the primary task at hand, optimal treatment also includes and monitors behavioral change in the individual as well.[90] There is some evidence that hospitalisation might adversely affect long term outcome.[91]		Psychotherapy for individuals with AN is challenging as they may value being thin and may seek to maintain control and resist change.[92] Some studies demonstrate that family based therapy in adolescents with AN is superior to individual therapy.[93]		Treatment of people with AN is difficult because they are afraid of gaining weight. Initially developing a desire to change may be important.[94]		Diet is the most essential factor to work on in people with anorexia nervosa, and must be tailored to each person's needs. Food variety is important when establishing meal plans as well as foods that are higher in energy density.[95] People must consume adequate calories, starting slowly, and increasing at a measured pace.[23] Evidence of a role for zinc supplementation during refeeding is unclear.[12]		Family-based treatment (FBT) has been shown to be more successful than individual therapy for adolescents with AN.[7][96] Various forms of family-based treatment have been proven to work in the treatment of adolescent AN including conjoint family therapy (CFT), in which the parents and child are seen together by the same therapist, and separated family therapy (SFT) in which the parents and child attend therapy separately with different therapists.[7] Proponents of Family therapy for adolescents with AN assert that it is important to include parents in the adolescent's treatment.[7]		A four- to five-year follow up study of the Maudsley family therapy, an evidence-based manualized model, showed full recovery at rates up to 90%.[97] Although this model is recommended by the NIMH,[98] critics claim that it has the potential to create power struggles in an intimate relationship and may disrupt equal partnerships.[medical citation needed]		Cognitive behavioral therapy (CBT) is useful in adolescents and adults with anorexia nervosa;[99] acceptance and commitment therapy is a type of CBT, which has shown promise in the treatment of AN.[100] Cognitive remediation therapy (CRT) is used in treating anorexia nervosa.[101]		Pharmaceuticals have limited benefit for anorexia itself.[102]		AN has a high mortality[103] and patients admitted in a severely ill state to medical units are at particularly high risk. Diagnosis can be challenging, risk assessment may not be performed accurately, consent and the need for compulsion may not be assessed appropriately, refeeding syndrome may be missed or poorly treated and the behavioural and family problems in AN may be missed or poorly managed.[104] The MARSIPAN guidelines recommend that medical and psychiatric experts work together in managing severely ill people with AN.[105]		The rate of refeeding can be difficult to establish, because the fear of refeeding syndrome (RFS) can lead to underfeeding. It is thought that RFS, with falling phosphate and potassium levels, is more likely to occur when BMI is very low, and when medical comorbidities such as infection or cardiac failure, are present. In those circumstances, it is recommended to start refeeding slowly but to build up rapidly as long as RFS does not occur. Recommendations on energy requirements vary, from 5–10 kCal/Kg/day in the most medically compromised patients, who appear to have the highest risk of RFS to 1900 Kcal/day[106][107]		AN has the highest mortality rate of any psychological disorder.[7] The mortality rate is 11 to 12 times greater than in the general population, and the suicide risk is 56 times higher.[18] Half of women with AN achieve a full recovery, while an additional 20–30% may partially recover.[7][18] Not all people with anorexia recover completely: about 20% develop anorexia nervosa as a chronic disorder.[88] If anorexia nervosa is not treated, serious complications such as heart conditions[16] and kidney failure can arise and eventually lead to death.[108] The average number of years from onset to remission of AN is seven for women and three for men. After ten to fifteen years, 70% of people no longer meet the diagnostic criteria, but many still continue to have eating-related problems.[109]		Alexithymia influences treatment outcome.[102] Recovery is also viewed on a spectrum rather than black and white. According to the Morgan-Russell criteria, individuals can have a good, intermediate, or poor outcome. Even when a person is classified as having a "good" outcome, weight only has to be within 15% of average, and normal menstruation must be present in females. The good outcome also excludes psychological health. Recovery for people with anorexia nervosa is undeniably positive, but recovery does not mean a return to normal.[medical citation needed]		Anorexia nervosa can have serious implications if its duration and severity are significant and if onset occurs before the completion of growth, pubertal maturation, or the attainment of peak bone mass.[medical citation needed] Complications specific to adolescents and children with anorexia nervosa can include the following: Growth retardation may occur, as height gain may slow and can stop completely with severe weight loss or chronic malnutrition. In such cases, provided that growth potential is preserved, height increase can resume and reach full potential after normal intake is resumed.[medical citation needed] Height potential is normally preserved if the duration and severity of illness are not significant or if the illness is accompanied by delayed bone age (especially prior to a bone age of approximately 15 years), as hypogonadism may partially counteract the effects of undernutrition on height by allowing for a longer duration of growth compared to controls.[medical citation needed] Appropriate early treatment can preserve height potential, and may even help to increase it in some post-anorexic subjects, due to factors such as long-term reduced estrogen-producing adipose tissue levels compared to premorbid levels.[medical citation needed] In some cases, especially where onset is before puberty, complications such as stunted growth and pubertal delay are usually reversible.[110]		Anorexia nervosa causes alterations in the female reproductive system; significant weight loss, as well as psychological stress and intense exercise, typically results in a cessation of menstruation in women who are past puberty. In patients with anorexia nervosa, there is a reduction of the secretion of gonadotropin releasing hormone in the central nervous system, preventing ovulation.[111] Anorexia nervosa can also result in pubertal delay or arrest. Both height gain and pubertal development are dependent on the release of growth hormone and gonadotrophins (LH and FSH) from the pituitary gland. Suppression of gonadotrophins in people with anorexia nervosa has been documented.[112] Typically, growth hormone (GH) levels are high, but levels of IGF-1, the downstream hormone that should be released in response to GH are low; this indicates a state of “resistance” to GH due to chronic starvation.[113] IGF-1 is necessary for bone formation, and decreased levels in anorexia nervosa contribute to a loss of bone density and potentially contribute to osteopenia or osteoporosis.[113] Anorexia nervosa can also result in reduction of peak bone mass. Buildup of bone is greatest during adolescence, and if onset of anorexia nervosa occurs during this time and stalls puberty, low bone mass may be permanent.[114] Hepatic steatosis, or fatty infiltration of the liver, can also occur, and is an indicator of malnutrition in children.[115] Neurological disorders that may occur as complications include seizures and tremors. Wernicke encephalopathy, which results from vitamin B1 deficiency, has been reported in patients who are extremely malnourished; symptoms include confusion, problems with the muscles responsible for eye movements and abnormalities in walking gait.		The most common gastrointestinal complications of anorexia nervosa are delayed stomach emptying and constipation, but also include elevated liver function tests, diarrhea, acute pancreatitis, heartburn, difficulty swallowing, and, rarely, superior mesenteric artery syndrome.[116] Delayed stomach emptying, or gastroparesis, often develops following food restriction and weight loss; the most common symptom is bloating with gas and abdominal distension, and often occurs after eating. Other symptoms of gastroparesis include early satiety, fullness, nausea, and vomiting. The symptoms may inhibit efforts at eating and recovery, but can be managed by limiting high-fiber foods, using liquid nutritional supplements, or using metoclopramide to increase emptying of food from the stomach.[116] Gastroparesis generally resolves when weight is regained.		Anorexia nervosa increases the risk of sudden cardiac death, though the precise cause is unknown. Cardiac complications include structural and functional changes to the heart.[117] Some of these cardiovascular changes are mild and are reversible with treatment, while others may be life-threatening. Cardiac complications can include arrhythmias, abnormally slow heart beat, low blood pressure, decreased size of the heart muscle, reduced heart volume, mitral valve prolapse, myocardial fibrosis, and pericardial effusion.[117]		Abnormalities in conduction and repolarization of the heart that can result from anorexia nervosa include QT prolongation, increased QT dispersion, conduction delays, and junctional escape rhythms.[117] Electrolyte abnormalities, particularly hypokalemia and hypomagnesemia, can cause anomalies in the electrical activity of the heart, and result in life-threatening arrhythmias. Hypokalemia most commonly results in anorexic patients when restricting is accompanied by purging (induced vomiting or laxative use). Hypotension (low blood pressure) is common, and symptoms include fatigue and weakness. Orthostatic hypotension, a marked decrease in blood pressure when standing from a supine position, may also occur. Symptoms include lightheadedness upon standing, weakness, and cognitive impairment, and may result in fainting or near-fainting.[117] Orthostasis in anorexia nervosa indicates worsening cardiac function and may indicate a need for hospitalization.[117] Hypotension and orthostasis generally resolve upon recovery to a normal weight. The weight loss in anorexia nervosa also causes atrophy of cardiac muscle. This leads to decreased ability to pump blood, a reduction in the ability to sustain exercise, a diminished ability to increase blood pressure in response to exercise, and a subjective feeling of fatigue.[118] Some individuals may also have a decrease in cardiac contractility. Cardiac complications can be life-threatening, but the heart muscle generally improves with weight gain, and the heart normalizes in size normalizes over weeks to months, with recovery.[118] Atrophy of the heart muscle is a marker of the severity of the disease, and while it is reversible with treatment and refeeding, it is possible that it may cause permanent, microscopic changes to the heart muscle that increase the risk of sudden cardiac death.[117] Individuals with anorexia nervosa may experience chest pain or palpitations; these can be a result of mitral valve prolapse. Mitral valve prolapse occurs because the size of the heart muscle decreases while the tissue of the mitral valve remains the same size. Studies have shown rates of mitral valve prolapse of around 20 percent in those with anorexia nervosa, while the rate in the general population is estimated at 2–4 percent.[119] It has been suggested that there is an association between mitral valve prolapse and sudden cardiac death, but it has not been proven to be causative, either in patients with anorexia nervosa or in the general population.[117]		Relapse occurs in approximately a third of people in hospital, and is greatest in the first six to eighteen months after release from an institution.[120]		Anorexia is estimated to occur in 0.9% to 4.3% of women and 0.2% to 0.3% of men in Western countries at some point in their life.[13] About 0.4% of young females are affected in a given year and it is estimated to occur three to ten times less commonly in males.[3][13][120] Rates in most of the developing world are unclear.[3] Often it begins during the teen years or young adulthood.[1]		The lifetime rate of atypical anorexia nervosa, a form of ED-NOS in which not all of the diagnostic criteria for AN are met, is much higher, at 5–12%.[121]		While anorexia become more commonly diagnosed during the 20th century it is unclear if this was due to an increase in its frequency or simply better diagnosis.[2] Most studies show that since at least 1970 the incidence of AN in adult women is fairly constant, while there is some indication that the incidence may have been increasing for girls aged between 14 and 20.[122]		Eating disorders are less reported in preindustrial, non-westernized countries than in Western countries. In Africa, not including South Africa, the only data presenting information about eating disorders occurs in case reports and isolated studies, not studies investigating prevalence. Data shows in research that in westernized civilizations, ethnic minorities have very similar rates of eating disorders, contrary to the belief that eating disorders predominantly occur in Caucasian people.[medical citation needed]		Due to different standards of beauty for men and women, men are often not diagnosed as anorexic. Generally men who alter their bodies do so to be lean and muscular rather than thin. In addition, men who might otherwise be diagnosed with anorexia may not meet the DSM IV criteria for BMI since they have muscle weight, but have very little fat.[123] Men and women athletes are often overlooked as anorexic.[123] Research emphasizes the importance to take athletes' diet, weight and symptoms into account when diagnosing anorexia, instead of just looking at weight and BMI. For athletes, ritualized activities such as weigh-ins place emphasis on weight, which may promote the development of eating disorders among them.[citation needed] While women use diet pills, which is an indicator of unhealthy behavior and an eating disorder, men use steroids, which contextualizes the beauty ideals for genders. This also shows men having a preoccupation with their body, which is an indicator of an eating disorder.[39] In a Canadian study, 4% of boys in grade nine used anabolic steroids.[39] Anorexic men are sometimes referred to as manorexic.[124]		The term anorexia nervosa was coined in 1873 by Sir William Gull, one of Queen Victoria's personal physicians.[125] The history of anorexia nervosa begins with descriptions of religious fasting dating from the Hellenistic era[126] and continuing into the medieval period. The medieval practice of self-starvation by women, including some young women, in the name of religious piety and purity also concerns anorexia nervosa; it is sometimes referred to as anorexia mirabilis.[127][128]		The earliest medical descriptions of anorexic illnesses are generally credited to English physician Richard Morton in 1689.[126] Case descriptions fitting anorexic illnesses continued throughout the 17th, 18th and 19th centuries.[129]		In the late 19th century anorexia nervosa became widely accepted by the medical profession as a recognized condition. In 1873, Sir William Gull, one of Queen Victoria's personal physicians, published a seminal paper which coined the term anorexia nervosa and provided a number of detailed case descriptions and treatments.[129] In the same year, French physician Ernest-Charles Lasègue similarly published details of a number of cases in a paper entitled De l'Anorexie hystérique.[130]		Awareness of the condition was largely limited to the medical profession until the latter part of the 20th century, when German-American psychoanalyst Hilde Bruch published The Golden Cage: the Enigma of Anorexia Nervosa in 1978. Despite major advances in neuroscience,[131] Bruch's theories tend to dominate popular thinking. A further important event was the death of the popular singer and drummer Karen Carpenter in 1983, which prompted widespread ongoing media coverage of eating disorders.[132]		The term is of Greek origin: an- (ἀν-, prefix denoting negation) and orexis (ὄρεξις, "appetite"), translating literally to a nervous loss of appetite.[133]								
Parenteral nutrition (PN) is the feeding of a person intravenously, bypassing the usual process of eating and digestion. The person receives nutritional formulae that contain nutrients such as glucose, salts, amino acids, lipids and added vitamins and dietary minerals. It is called total parenteral nutrition (TPN) or total nutrient admixture (TNA) when no significant nutrition is obtained by other routes, and partial parenteral nutrition (PPN) when nutrition is also partially enteric. It may be called peripheral parenteral nutrition (PPN) when administered through vein access in a limb rather than through a central vein as central venous nutrition (CVN).						Total parenteral nutrition (TPN) is provided when the gastrointestinal tract is nonfunctional because of an interruption in its continuity (it is blocked, or has a leak - a fistula) or because its absorptive capacity is impaired.[1] It has been used for comatose patients, although enteral feeding is usually preferable, and less prone to complications. Parenteral nutrition is used to prevent malnutrition in patients who are unable to obtain adequate nutrients by oral or enteral routes.[2]		TPN may be the only feasible option for providing nutrition to patients who do not have a functioning gastrointestinal tract or who have disorders requiring complete bowel rest, including bowel obstruction,[3] short bowel syndrome,[3] Gastroschisis,[3] prolonged diarrhea regardless of its cause,[3] high-output fistula,[3] very severe Crohn's disease[3] or ulcerative colitis,[3] and certain pediatric GI disorders including congenital GI anomalies and necrotizing enterocolitis.[4]		The benefit of TPN to cancer patients is largely debated, and studies to date have generally showed minimal long term benefit. On the other hand, there is no evidence to support the idea that intravenous nutrition 'feeds the cancer, not the patient'.[citation needed]		Short-term PN may be used if a person's digestive system has shut down (for instance by peritonitis), and they are at a low enough weight to cause concerns about nutrition during an extended hospital stay. Long-term PN is occasionally used to treat people suffering the extended consequences of an accident, surgery, or digestive disorder. PN has extended the life of children born with nonexistent or severely deformed organs.		Approximately 40,000 people use TPN at home in the United States, and because TPN requires anywhere from 10–16 hours to be administered, daily life can be affected.[5] Although daily lifestyle can be changed, most patients agree that these changes are better than staying at the hospital.[6] Many different types of pumps exist to limit the time the patient is “hooked-up”. Usually a backpack pump is used, allowing for mobility. The time required to be connected to the IV is dependent on the situation of each patient; some require once a day, or five days a week.[5]		It is important for patients to avoid as much TPN related change as possible in their lifestyles. This allows for the best possible mental health situation; constantly being held down can lead to resentment and depression. Physical activity is also highly encouraged, but patients must avoid contact sports (equipment damage) and swimming (infection). Many teens find it difficult to live with TPN due to issues regarding body image and not being able to participate in activities and events.[5]		TPN fully bypasses the GI tract and normal methods of nutrient absorption. Possible complications, which may be significant, are listed below.		TPN requires a chronic IV access for the solution to run through, and the most common complication is infection of this catheter. Infection is a common cause of death in these patients, with a mortality rate of approximately 15% per infection, and death usually results from septic shock.[7]		Chronic IV access leaves a foreign body in the vascular system, and blood clots on this IV line are common.[8] Death can result from pulmonary embolism wherein a clot that starts on the IV line but breaks off goes into the lungs.[9]		Patients on TPN who have such clots occluding their catheter may receive a thrombolytic flush to dissolve the clots and prevent further complications.		Fatty liver is usually a more long term complication of TPN, though over a long enough course it is fairly common. The pathogenesis is due to using linoleic acid (an omega-6 fatty acid component of soybean oil) as a major source of calories.[10][11] TPN-associated liver disease strikes up to 50% of patients within 5–7 years, correlated with a mortality rate of 2–50%. Onset of this liver disease is the major complication that leads TPN patients to requiring an intestinal transplant.[12]		Because patients are being fed intravenously, the subject does not physically eat, resulting in intense hunger pangs. The brain uses signals from the mouth (taste and smell), the stomach/G.I. Tract (fullness) and blood (nutrient levels) to determine conscious feelings of hunger.[13] In cases of TPN, the taste, smell and physical fullness requirements are not met, and so the patient experiences hunger, despite the fact that the body is being fully nourished.		Patients who eat food despite the inability can experience a wide range of complications.[14]		Total parenteral nutrition increases the risk of acute cholecystitis[15] due to complete disuse of gastrointestinal tract, which may result in bile stasis in the gallbladder. Other potential hepatobiliary dysfunctions include steatosis,[16] steatohepatitis, cholestasis, and cholelithiasis.[17] Six percent of patients on TPN longer than 3 weeks and 100% of patients on TPN longer than 13 weeks develop biliary sludge. The formation of sludge is the result of stasis due to lack of enteric stimulation and is not due to changes in bile composition. Gallbladder sludge disappears after 4 weeks of normal oral diet. Administration of exogenous cholecystokinin (CCK) or stimulation of endogenous CCK by periodic pulse of large amounts of amino acids have been shown to help prevent sludge formation. These therapies are not routinely recommended.[18] Such complications are suggested to be the main reason for mortality in people requiring long-term total parenteral nutrition, such as in short bowel syndrome.[19] In newborn infants with short bowel syndrome with less than 10% of expected intestinal length, thereby being dependent upon total parenteral nutrition, 5 year survival is approximately 20%.[20]		Infants who are sustained on TPN without food by mouth for prolonged periods are at risk for developing gut atrophy.[21]		Other complications are either related to catheter insertion, or metabolic, including refeeding syndrome. Catheter complications include pneumothorax, accidental arterial puncture, and catheter-related sepsis. The complication rate at the time of insertion should be less than 5%.[22] Catheter-related infections may be minimised by appropriate choice of catheter and insertion technique.[23] Metabolic complications include the refeeding syndrome characterised by hypokalemia, hypophosphatemia and hypomagnesemia. Hyperglycemia is common at the start of therapy, but can be treated with insulin added to the TPN solution. Hypoglycaemia is likely to occur with abrupt cessation of TPN. Liver dysfunction can be limited to a reversible cholestatic jaundice and to fatty infiltration (demonstrated by elevated transaminases). Severe hepatic dysfunction is a rare complication.[24] Overall, patients receiving TPN have a higher rate of infectious complications. This can be related to hyperglycemia.[25]		Pregnancy can cause major complications when trying to properly dose the nutrient mixture. Because all of the baby’s nourishment comes from the mother’s blood stream, the doctor must properly calculate the dosage of nutrients to meet both recipient’s needs and have them in usable forms. Incorrect dosage can lead to many adverse, hard-to-guess effects, such as death, and varying degrees of deformation or other developmental problems.[26]		It is recommended that parenteral nutrition administration begin after a period of natural nutrition so doctors can properly calculate the nutritional needs of the fetus. Otherwise, it should only be administered by a team of highly skilled doctors who can accurately assess the fetus’ needs.[26]		Solutions for total parenteral nutrition may be customized to individual patient requirements, or standardized solutions may be used. The use of standardized parenteral nutrition solutions is cost effective and may provide better control of serum electrolytes.[27] Ideally each patient is assessed individually before commencing on parenteral nutrition, and a team consisting of specialised doctors, nurses, clinical pharmacists and Registered Dietitians evaluate the patient's individual data and decide what PN formula to use and at what infusion rate.		For energy only, intravenous sugar solutions with dextrose or glucose are generally used. This is not considered to be parenteral nutrition as it does not prevent malnutrition when used on its own. Standardized solutions may also differ between developers. Following are some examples of what compositions they may have. The solution for normal patients may be given both centrally and peripherally.		Prepared solutions generally consist of water and electrolytes; glucose, amino acids, and lipids; essential vitamins, minerals and trace elements are added or given separately. Previously lipid emulsions were given separately but it is becoming more common for a "three-in-one" solution of glucose, proteins, and lipids to be administered.[28][29]		Individual nutrient components may be added to more precisely adjust the body contents of it. That individual nutrient may, if possible, be infused individually, or it may be injected into a bag of nutrient solution or intravenous fluids (volume expander solution) that is given to the patient.		Administration of individual components may be more hazardous than administration of pre-mixed solutions such as those used in total parenteral nutrition, because the latter are generally already balanced in regard to e.g. osmolarity and ability to infuse peripherally. Incorrect IV administration of concentrated potassium can be lethal, but this is not a danger if the potassium is mixed in TPN solution and diluted.[30]		Vitamins may be added to a bulk premixed nutrient immediately before administration, since the additional vitamins can promote spoilage of stored product[citation needed]. Vitamins can be added in two doses, one fat-soluble, the other water-soluble. There are also single-dose preparations with both fat- and water-soluble vitamins such as Cernevit.[31][32]		Minerals and trace elements for parenteral nutrition are available in prepared mixtures, such as Addaven.[33]		Only a limited number of emulsifiers are commonly regarded as safe to use for parenteral administration, of which the most important is lecithin.[medical citation needed] Lecithin can be biodegraded and metabolized, since it is an integral part of biological membranes, making it virtually non-toxic. Other emulsifiers can only be excreted via the kidneys,[citation needed] creating a toxic load. The emulsifier of choice for most fat emulsions used for parenteral nutrition is a highly purified egg lecithin,[34] due to its low toxicity and complete integration with cell membranes.[35] Use of egg-derived emulsifiers is not recommended for people with an egg allergy due to the risk of reaction. In situations where there is no suitable emulsifying agent for a person at risk of developing essential fatty acid deficiency, cooking oils may be spread upon large portions of available skin for supplementation by transdermal absorption.		Another type of fat emulsion Omegaven is being used experimentally within the US primarily in the pediatric population. It is made of fish oil instead of the egg based formulas more widely in use. Research has shown use of Omegaven may reverse and prevent liver disease and cholestasis.[36]		Developed in the 1960s by Dr Stanley J. Dudrick, who as a surgical resident in the University of Pennsylvania, working in the basic science laboratory of Dr Jonathan Rhoads, was the first to successfully nourish initially Beagle puppies and subsequently newborn babies with catastrophic gastrointestinal malignancies.[37] Dr Dudrick collaborated with Dr Willmore and Dr Vars to complete the work necessary to make this nutritional technique safe and successful.[38]		
Brunch is a combination of breakfast and lunch eaten usually during the late morning to early afternoon, generally served from 11am up to 3pm.[1][2] The word is a portmanteau of breakfast and lunch.[3] Brunch originated in England in the late 19th century and became popular in the United States in the 1930s.[4]						The 1896 supplement to the Oxford English Dictionary cites Punch magazine which wrote that the term was coined in Britain in 1895 to describe a Sunday meal for "Saturday-night carousers" in the writer Guy Beringer's article "Brunch: A Plea"[5] in Hunter's Weekly'[6]		Instead of England's early Sunday dinner, a postchurch ordeal of heavy meats and savory pies, why not a new meal, served around noon, that starts with tea or coffee, marmalade and other breakfast fixtures before moving along to the heavier fare? By eliminating the need to get up early on Sunday, brunch would make life brighter for Saturday-night carousers. It would promote human happiness in other ways as well. "Brunch is cheerful, sociable and inciting." Beringer wrote. "It is talk-compelling. It puts you in a good temper, it makes you satisfied with yourself and your fellow beings, it sweeps away the worries and cobwebs of the week."		It is sometimes credited to reporter Frank Ward O'Malley who wrote for the New York newspaper The Sun from 1906 until 1919,[8] allegedly based on the typical mid-day eating habits of a newspaper reporter.[9][10]		Some colleges and hotels serve brunch. Such brunches are often serve-yourself buffets, but menu-ordered meals may be available instead of, or with, the buffet. The meal usually involves standard breakfast foods such as eggs, sausages, bacon, ham, fruits, pastries, pancakes, waffles, scones, and the like.		The United States and United Kingdom militaries often serves weekend brunch in the dining facilities. They offer both breakfast and lunch options and are open from about 09:00-13:00 (though times vary).		The dim sum brunch is popular in Chinese restaurants worldwide.[11] It consists of a variety of stuffed buns, dumplings, and other savory or sweet food items that have been steamed, deep-fried, or baked. Customers pick small portions from passing carts, as the kitchen continuously produces and sends out more freshly prepared dishes. Dim sum is usually eaten at a mid-morning, midday, and/or mid-afternoon teatime.		Brunch is prepared by restaurants and hotels for special occasions, such as weddings, Valentine's Day, Mother's Day, or Easter Sunday.		The Office québécois de la langue française accepts "brunch" as a valid word but also provides a synonym déjeuner-buffet. Note that, however, in Quebec, déjeuner alone (even without the qualifying adjective petit) means "breakfast".[12] In Quebec, the word—when Francized—is pronounced [bʁɔ̃ʃ].[13]		Chinese word “早午饭” is defined as brunch, “早饭” means breakfast and “午饭” means lunch in Chinese. The combination of “早饭” and “午饭” is “早午饭”, as known as brunch.		'Friday Brunch' is considered[14] something of an institution in Dubai. Many large hotels and restaurants offer an all inclusive drinks and food buffet during early afternoons, and large groups of expatriates and tourists make this the highlight of their weekend, with parties going on well into the night.		In many regions of Canada, in particular in Southern Ontario, brunch is popular on Sundays when families will often host relatives or friends in their dining room. The typical brunch can last a few hours and go late into the afternoon. Montreal-style bagels may be served alongside egg dishes, waffles or crepes, smoked meat or fish, fruit, salads, cheese, and dessert. Often, champagne or wine will be served and following the meal tea or coffee is usually consumed.[citation needed]		Many restaurants offer brunch service as well, and the Leslieville neighbourhood of Toronto is sometimes called the brunch capital of Toronto[15] as many renowned establishments serve brunch in that neighbourhood.		In Canada, brunch is served in private homes using homemade foods and in restaurants. In both cases, brunch typically consists of the same dishes as would be standard in an American brunch, namely coffee,[16] tea, fruit juices,[17] breakfast foods including pancakes,[18] waffles,[19] and french toast;[20] meats such as ham,[21] bacon[22] and sausages;[23] egg dishes such as scrambled eggs,[24] omelettes[25] and Eggs Benedict;[26] bread products such as toast,[27] bagels[28] or croissants;[29] pastries[30] or cakes such as cinnamon rolls or coffee cake;[31] and fresh, cut fruit pieces [32] or fruit salad. Brunches may also include foods not typically associated with breakfast, such as roasted meats,[33] quiche,[34] soup,[35] smoked salmon[36] and salads[37] such as Cobb salad.		When served in a private home or a restaurant, a brunch may be served buffet style,[38] in which trays of foods and beverages are available and guests can serve themselves and select the items they want, often in an "all-you-can-eat" fashion.[39] Restaurant brunches may also be served from a menu, in which guests select specific items which are served to them by waitstaff. Restaurant brunch meals range from relatively inexpensive brunches available at diners and family restaurants to expensive brunches served at high-end restaurants and bistros.		In South Africa, brunch is a favourite activity of many families. It is globally-distinctive in that only pancakes and fruit are consumed.[citation needed]		
Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, inhibition of nearly all voluntary muscles, and reduced interactions with surroundings.[1] It is distinguished from wakefulness by a decreased ability to react to stimuli, but is more easily reversed than the state of being comatose. Sleep occurs in repeating periods, in which the body alternates between two distinct modes known as non-REM and REM sleep. Although REM stands for "rapid eye movement", this mode of sleep has many other aspects, including virtual paralysis of the body. A well-known feature of sleep is the dream, an experience typically recounted in narrative form, which resembles waking life while in progress, but which usually can later be distinguished as fantasy.		During sleep, most of the body's systems are in an anabolic state, helping to restore the immune, nervous, skeletal, and muscular systems; these are vital processes that maintain mood, memory, and cognitive performance, and play a large role in the function of the endocrine and immune systems.[2] The internal circadian clock promotes sleep daily at night. The diverse purposes and mechanisms of sleep are the subject of substantial ongoing research.[3] The advent of artificial light has substantially altered sleep timing in industrialized countries.[4]		Humans may suffer from various sleep disorders, including dyssomnias, such as insomnia, hypersomnia, narcolepsy, and sleep apnea; parasomnias, such as sleepwalking and REM behavior disorder; bruxism; and circadian rhythm sleep disorders.						The most pronounced physiological changes in sleep occur in the brain.[5] Especially during non-REM sleep, the brain uses significantly less energy during sleep than it does in waking. In areas with reduced activity, the brain restores its supply of adenosine triphosphate (ATP), the molecule used for short-term storage and transport of energy.[6] (Since in quiet waking the brain is responsible for 20% of the body's energy use, this reduction has an independently noticeable impact on overall energy consumption.)[7]		Sleep increases the sensory threshold. In other words, sleeping persons perceive fewer stimuli. However, they can generally still respond to loud noises and other salient sensory events.[7][5]		During slow-wave sleep, humans secrete bursts of growth hormone. All sleep, even during the day, is associated with secretion of prolactin.[8]		Key physiological measurements indicators of sleep include EEG of brain waves, electrooculography (EOG) of eye movements, electromyography (EMG) of skeletal muscle activity. Simultaneous collection of these measurements is called polysomnography, and can be performed in a specialized sleep laboratory.[9][10] Sleep researchers also use simplified electrocardiography (EKG) for cardiac activity and actigraphy for motor movements.[10]		Sleep is divided into two broad types: rapid eye movement (REM sleep) and non-rapid eye movement (non-REM or NREM sleep). REM and non-REM sleep are so different that physiologists identify them as distinct behavioral states. Non-REM sleep occurs first and after a transitional period is called slow wave sleep or deep sleep. During this phase, body temperature and heart rate fall, and the brain uses less energy.[5] REM sleep (also known as paradoxical sleep), a smaller portion of total sleep time and the main occasion for dreams (or nightmares), is associated with desynchronized and fast brain waves, eye movements, loss of muscle tone,[1] and suspension of homeostasis.[11]		The sleep cycle of alternate NREM and REM sleep takes an average of 90 minutes, occurring 4–6 times in a good night's sleep.[10][12] The American Academy of Sleep Medicine (AASM) divides NREM into three stages: N1, N2, and N3, the last of which is also called delta sleep or slow-wave sleep.[13] The whole period normally proceeds in the order: N1 → N2 → N3 → N2 → REM. REM sleep occurs as a person returns to stage 2 or 1 from a deep sleep.[1] There is a greater amount of deep sleep (stage N3) earlier in the night, while the proportion of REM sleep increases in the two cycles just before natural awakening.[10]		Awakening can mean the end of sleep, or simply a moment to survey the environment and readjust body position before falling back asleep. Sleepers typically awaken soon after the end of a REM phase or sometimes in the middle of REM. Internal circadian indicators, along with successful reduction of homeostatic sleep need, typically bring about awakening and the end of the sleep episode.[14] Awakening involves heightened electrical activation in the brain, beginning with the thalamus and spreading throughout the cortex.[14]		During a night's sleep, a small portion is usually spent in a waking state. As measured by electroencephalography, young females are awake for 0–1% of the larger sleeping period; young males are awake for 0–2%. In adults, wakefulness increases, especially in later cycles. One study found 3% awake time in the first ninety-minute sleep cycle, 8% in the second, 10% in the third, 12% in the fourth, and 13–14% in the fifth. Most of this awake time occurred shortly after REM sleep.[14]		Today, many humans wake up with an alarm clock.[15] (Some people, however, can reliably wake themselves up at a specific time with no need for an alarm.)[14] Many sleep quite differently on workdays versus days off, a pattern which can lead to chronic circadian desynchronization.[16][15] Many people regularly look at television and other screens before going to bed, a factor which may exacerbate this mass circadian disruption.[17] Scientific studies on sleep have shown that sleep stage at awakening is an important factor in amplifying sleep inertia.[18]		Sleep timing is controlled by the circadian clock (Process C), sleep-wake homeostasis (Process S), and to some extent by individual will.		Sleep timing depends greatly on hormonal signals from the circadian clock, or Process C, a complex neurochemical system which uses signals from an organism's environment to recreate an internal day–night rhythm. Process C counteracts the homeostatic drive for sleep during the day (in diurnal animals) and to augment it at night.[19][16] The suprachiasmatic nucleus (SCN), a brain area directly above the optic chiasm, is presently considered the most important nexus for this process; however, secondary clock systems have been found throughout the body.		An organism whose circadian clock exhibits a regular rhythm corresponding to outside signals is said to be entrained; the rhythm so established persists even if the outside signals suddenly disappear. If an entrained human is isolated in a bunker with constant light or darkness, he or she will continue to experience rhythmic increases and decreases of body temperature and melatonin, on a period which slightly exceeds 24 hours. Scientists refer to such conditions as free-running of the circadian rhythm. Under natural conditions, light signals regularly adjust this period downward, so that it corresponds better with the exact 24 hours of an Earth day.[15][20][21]		The clock exerts constant influence on the body, effecting sinusoidal oscillation of body temperature between roughly 36.2 °C and 37.2 °C.[21][22] The suprachiasmatic nucleus itself shows conspicuous oscillation activity, which intensifies during subjective day (i.e., the part of the rhythm corresponding with daytime, whether accurately or not) and drops to almost nothing during subjective night.[23] The circadian pacemaker in the suprachiasmatic nucleus has a direct neural connection to the pineal gland, which releases the hormone melatonin at night.[23] Cortisol levels typically rise throughout the night, peak in the awakening hours, and diminish during the day.[8][24] Circadian prolactin secretion begins in the late afternoon, especially in women, and is subsequently augmented by sleep-induced secretion, to peak in the middle of the night. Circadian rhythm exerts some influence on the nighttime secretion of growth hormone.[8]		The circadian rhythm influences the ideal timing of a restorative sleep episode.[15][25] Sleepiness increases during the night. REM sleep occurs more during body temperature minimum within the circadian cycle, whereas slow-wave sleep can occur more independently of circadian time.[21]		The internal circadian clock is profoundly influenced by changes in light, since these are its main clues about what time it is. Exposure to even small amounts of light during the night can suppress melatonin secretion, increase body temperature, and increase cognitive ability. Short pulses of light, at the right moment in the circadian cycle, can significantly 'reset' the internal clock.[22] Blue light, in particular, exerts the strongest effect,[16] leading to concerns that electronic media use before bed may interfere with sleep.		Modern humans often find themselves desynchronized from their internal circadian clock, due to the requirements of work (especially night shifts), long-distance travel, and the influence of universal indoor lighting.[21] Even if they have sleep debt, or feel sleepy, people can have difficulty staying asleep at the peak of their circadian cycle. Conversely they can have difficulty waking up in the trough of the cycle.[14] A healthy young adult entrained to the sun will (during most of the year) fall asleep a few hours after sunset, experience body temperature minimum at 6AM, and wake up a few hours after sunrise.[21]		Generally speaking, the longer an organism is awake, the more it feels a need to sleep ("sleep debt"). This driver of sleep is referred to as Process S. The balance between sleeping and waking is regulated by a process called homeostasis. Induced or perceived lack of sleep is commonly called sleep deprivation.		Process S is driven by the depletion of glycogen and accumulation of adenosine in the forebrain that disinhibits the Ventrolateral preoptic nucleus, allowing for inhibition of the ascending reticular activating system.[26]		Sleep deprivation tends to cause slower brain waves in the frontal cortex, shortened attention span, higher anxiety, impaired memory, and a grouchy mood. Conversely, a well-rested organism tends to have improved memory and mood.[27] Neurophysiological and functional imaging studies have demonstrated that frontal regions of the brain are particularly responsive to homeostatic sleep pressure.[28]		Scientists do not agree on how much sleep debt it is possible to accumulate; whether it is accumulated against an individual's average sleep or some other benchmark; nor on whether the prevalence of sleep debt among adults has changed appreciably in the industrialized world in recent decades. Sleep debt does show some evidence of being cumulative. Subjectively, however, humans seem to reach maximum sleepiness after 30 hours of waking.[21] It is likely that children are sleeping less than previously in Western societies.[29]		One neurochemical indicator of sleep debt is adenosine, a neurotransmitter that inhibits many of the bodily processes associated with wakefulness. Adenosine levels increase in the cortex and basal forebrain during prolonged wakefulness and decrease during the sleep-recovery period, potentially acting as a homeostatic regulator of sleep.[30][31] Coffee and caffeine temporarily block the effect of adenosine, prolong sleep latency, and reduce total sleep time and quality.[32]		Humans are also influenced by aspects of social time: the hours when other people are awake, the hours when work is required, the time on the clock, etc. Time zones, standard times used to unify the timing for people in the same area, correspond only approximately to the natural rising and setting of the sun. The approximate nature of the timezone can be shown with China, a country which used to span five time zones and now uses only one (UTC +8).[15]		In polyphasic sleep, an organism sleeps at multiple times during a 24-hour cycle. Monophasic sleep occurs all at once. Under experimental conditions, humans tend to alternate more frequently between sleep and wakefulness (i.e., exhibit more polyphasic sleep) if they have nothing better to do.[21] Given a 14-hour period of darkness in experimental conditions, humans tended towards bimodal sleep, with two sleep periods concentrated at the beginning and at the end of the dark time. Bimodal sleep in humans was more common before the industrial revolution.[24]		Different characteristic sleep patterns, such as the familiarly so-called "early bird" and "night owl", are called chronotypes. Genetics and sex have some influence on chronotype, but so do habits. Chronotype is also liable to change over the course of a person's lifetime. Seven-year-olds are better disposed to wake up early in the morning than are fifteen-year-olds.[16][15] Chronotypes far outside the normal range are called circadian rhythm sleep disorders.[33]		The siesta habit has recently been associated with a 37% lower coronary mortality, possibly due to reduced cardiovascular stress mediated by daytime sleep.[34] Short naps at mid-day and mild evening exercise were found to be effective for improved sleep, cognitive tasks, and mental health in elderly people.[35]		Many people have a temporary drop in alertness in the early afternoon, commonly known as the "post-lunch dip." While a large meal can make a person feel sleepy, the post-lunch dip is mostly an effect of the circadian clock. People naturally feel most sleepy at two times of the day about 12 hours apart—for example, at 2:00 a.m. and 2:00 p.m. At those two times, the body clock "kicks in." At about 2 p.m. (14:00), it overrides the homeostatic buildup of sleep debt, allowing several more hours of wakefulness. At about 2 a.m. (02:00), with the daily sleep debt paid off, it "kicks in" again to ensure a few more hours of sleep.		It is hypothesized that a considerable amount of sleep-related behavior, such as when and how long a person needs to sleep, is regulated by genetics. Researchers have discovered some evidence that seems to support this assumption.[36] Monozygotic (identical) but not dizygotic (fraternal) twins tend to have similar sleep habits. Neurotransmitters, molecules whose production can be traced to specific genes, are one genetic influence on sleep which can be analyzed. And the circadian clock has its own set of genes.[37] Genes which may influence sleep include ABCC9,[38] DEC2,[39][40] and variants near PAX 8 and VRK2.[41]		The quality of sleep may be evaluated from an objective and a subjective point of view. Objective sleep quality refers to how difficult it is for a person to fall asleep and remain in a sleeping state, and how many times they wake up during a single night. Poor sleep quality disrupts the cycle of transition between the different stages of sleep.[42] Subjective sleep quality in turn refers to a sense of being rested and regenerated after awaking from sleep. A study by A. Harvey et al. (2002) found that insomniacs were more demanding in their evaluations of sleep quality than individuals who had no sleep problems.[43]		Homeostatic sleep propensity (the need for sleep as a function of the amount of time elapsed since the last adequate sleep episode) must be balanced against the circadian element for satisfactory sleep.[44][45] Along with corresponding messages from the circadian clock, this tells the body it needs to sleep.[46] A person who regularly awakens at an early hour will generally not be able to sleep much later than his or her normal waking time, even if moderately sleep-deprived[citation needed]. The timing is correct when the following two circadian markers occur after the middle of the sleep episode and before awakening:[47] maximum concentration of the hormone melatonin, and minimum core body temperature.		Human sleep needs vary by age and amongst individuals, and sleep is considered to be adequate when there is no daytime sleepiness or dysfunction. Moreover, self-reported sleep duration is only moderately correlated with actual sleep time as measured by actigraphy,[49] and those affected with sleep state misperception may typically report having slept only four hours despite having slept a full eight hours.[50]		Researchers have found that sleeping 6–7 hours each night correlates with longevity and cardiac health in humans, though many underlying factors may be involved in the causality behind this relationship.[51][52][53][54][41][55][56]		Sleep difficulties are furthermore associated with psychiatric disorders such as depression, alcoholism, and bipolar disorder.[57] Up to 90% of adults with depression are found to have sleep difficulties. Dysregulation found on EEG includes disturbances in sleep continuity, decreased delta sleep and altered REM patterns with regard to latency, distribution across the night and density of eye movements.[58]		By the time infants reach the age of two, their brain size has reached 90 percent of an adult-sized brain;[59] a majority of this brain growth has occurred during the period of life with the highest rate of sleep. The hours that children spend asleep influence their ability to perform on cognitive tasks.[60][61] Children who sleep through the night and have few night waking episodes have higher cognitive attainments and easier temperaments than other children.[61][62][63]		Sleep also influences language development. To test this, researchers taught infants a faux language and observed their recollection of the rules for that language.[64] Infants who slept within four hours of learning the language could remember the language rules better, while infants who stayed awake longer did not recall those rules as well. There is also a relationship between infants' vocabulary and sleeping: infants who sleep longer at night at 12 months have better vocabularies at 26 months.[63]		Children need many hours of sleep per day in order to develop and function properly: up to 18 hours for newborn babies, with a declining rate as a child ages.[46] Early in 2015, after a two-year study,[65] the National Sleep Foundation in the US announced newly revised recommendations as shown in the table below.		The human organism physically restores itself during sleep, healing itself and removing waste which builds up during periods of activity. This restoration takes place mostly during slow-wave sleep, during which body temperature, heart rate, and brain oxygen consumption decrease. The brain, especially, requires sleep for restoration, whereas in the rest of the body these processes can take place during quiescent waking. In both cases, the reduced rate of metabolism enables countervailing restorative processes.[68]		While awake, metabolism generates reactive oxygen species, which are damaging to cells. In sleep, metabolic rates decrease and reactive oxygen species generation is reduced allowing restorative processes to take over. The sleeping brain has been shown to remove metabolic waste products at a faster rate than during an awake state.[69][70] It is further theorized that sleep helps facilitate the synthesis of molecules that help repair and protect the brain from these harmful elements generated during waking.[71] Anabolic hormones such as growth hormones are secreted preferentially during sleep. Sleep has also been theorized to effectively combat the accumulation of free radicals in the brain, by increasing the efficiency of endogenous antioxidant mechanisms.[72] The concentration of the sugar compound glycogen in the brain increases during sleep, and is depleted through metabolism during wakefulness.[68]		Wound healing has been shown to be affected by sleep.[73]		It has been shown that sleep deprivation affects the immune system.[74] It is now possible to state that "sleep loss impairs immune function and immune challenge alters sleep," and it has been suggested that sleep increases white blood cell counts.[75] A 2014 study found that depriving mice of sleep increased cancer growth and dampened the immune system's ability to control cancers.[76]		The effect of sleep duration on somatic growth is not completely known. One study recorded growth, height, and weight, as correlated to parent-reported time in bed in 305 children over a period of nine years (age 1–10). It was found that "the variation of sleep duration among children does not seem to have an effect on growth."[77] It is well established that slow-wave sleep affects growth hormone levels in adult men.[8] During eight hours' sleep, Van Cauter, Leproult, and Plat found that the men with a high percentage of SWS (average 24%) also had high growth hormone secretion, while subjects with a low percentage of SWS (average 9%) had low growth hormone secretion.[78]		Scientists demonstrated in various ways that sleep enhances memory.[79][80] Apparently procedural memory benefits from late, REM-rich sleep, whereas declarative memory benefits from early, slow wave-rich sleep.[81][82]		Sleep researchers Saper[83] and Stickgold[84] point out that an essential part of memory and learning consists of nerve cell dendrites' sending of information to the cell body to be organized into new neuronal connections. This process demands that no external information is presented to these dendrites, and it is suggested that this may be why it is during sleep that memories and knowledge are solidified and organized. Giulio Tononi and Chiara Cirelli have focused on the process of forgetting by pruning weaker connections during sleep, to select important memories and enable future memory encoding. They further suggest that synaptic potentiation drives Process S (homeostatic need for sleep).[85][86]		Recent studies examining gene expression and evolutionary increases in brain size offer complimentary support for the role of sleep in the mammalian memory consolidation theory. Evolutionary advances in the size of the mammalian amygdala, (a brain structure active during sleep and involved in memory processing), are also associated with increases in NREM sleep durations.[87] Likewise, nighttime gene expression differs from daytime expression and specifically targets genes thought to be involved in memory consolidation and brain plasticity.[88]		During sleep, especially REM sleep, people tend to have dreams: elusive first-person experiences, which, despite their frequently bizarre qualities, seem realistic while in progress. Dreams can seamlessly incorporate elements within a person's mind that would not normally go together. They can include apparent sensations of all types, especially vision and movement.[89]		People have proposed many hypotheses about the functions of dreaming. Sigmund Freud postulated that dreams are the symbolic expression of frustrated desires that have been relegated to the unconscious mind, and he used dream interpretation in the form of psychoanalysis in attempting to uncover these desires.[90]		Counterintuitively, penile erections during sleep are not more frequent during sexual dreams than during other dreams.[91] The parasympathetic nervous system experiences increased activity during REM sleep which may cause erection of the penis or clitoris. In males, 80% to 95% of REM sleep is normally accompanied by partial to full penile erection, while only about 12% of men's dreams contain sexual content.[92]		John Allan Hobson and Robert McCarley propose that dreams are caused by the random firing of neurons in the cerebral cortex during the REM period. Neatly, this theory helps explain the irrationality of the mind during REM periods, as, according to this theory, the forebrain then creates a story in an attempt to reconcile and make sense of the nonsensical sensory information presented to it. This would explain the odd nature of many dreams.[93]		Using antidepressants,[clarification needed] acetaminophen, ibuprofen, or alcoholic beverages is thought to potentially suppress dreams, whereas melatonin may have the ability to encourage them.[94]		Insomnia is a general term for difficulty falling asleep and staying asleep. Insomnia is the most common sleep problem, with many adults reporting occasional insomnia, and 10–15% reporting a chronic condition.[95] Insomnia can have many different causes, including psychological stress, a poor sleep environment, an inconsistent sleep schedule, or excessive mental or physical stimulation in the hours before bedtime. Insomnia is often treated through behavioral changes like keeping a regular sleep schedule, avoiding stimulating or stressful activities before bedtime, and cutting down on stimulants such as caffeine. The sleep environment may be improved by installing heavy drapes to shut out all sunlight, and keeping computers, televisions and work materials out of the sleeping area.		A 2010 review of published scientific research suggested that exercise generally improves sleep for most people, and helps sleep disorders such as insomnia. The optimum time to exercise may be 4 to 8 hours before bedtime, though exercise at any time of day is beneficial, with the exception of heavy exercise taken shortly before bedtime, which may disturb sleep. However, there is insufficient evidence to draw detailed conclusions about the relationship between exercise and sleep.[96] Sleeping medications such as Ambien and Lunesta are an increasingly popular treatment for insomnia. Although these nonbenzodiazepine medications are generally believed to be better and safer than earlier generations of sedatives, they have still generated some controversy and discussion regarding side-effects. White noise appears to be a promising treatment for insomnia.[97]		Obstructive sleep apnea is a condition in which major pauses in breathing occur during sleep, disrupting the normal progression of sleep and often causing other more severe health problems. Apneas occur when the muscles around the patient's airway relax during sleep, causing the airway to collapse and block the intake of oxygen.[98] Obstructive sleep apnea is more common than central sleep apnea.[99] As oxygen levels in the blood drop, the patient then comes out of deep sleep in order to resume breathing. When several of these episodes occur per hour, sleep apnea rises to a level of seriousness that may require treatment.		Diagnosing sleep apnea usually requires a professional sleep study performed in a sleep clinic, because the episodes of wakefulness caused by the disorder are extremely brief and patients usually do not remember experiencing them. Instead, many patients simply feel tired after getting several hours of sleep and have no idea why. Major risk factors for sleep apnea include chronic fatigue, old age, obesity and snoring.		Sleep disorders include narcolepsy, periodic limb movement disorder (PLMD), restless leg syndrome (RLS), upper airway resistance syndrome (UARS), and the circadian rhythm sleep disorders. Fatal familial insomnia, or FFI, an extremely rare genetic disease with no known treatment or cure, is characterized by increasing insomnia as one of its symptoms; ultimately sufferers of the disease stop sleeping entirely, before dying of the disease.[100]		Somnambulism, known as sleep walking, is also a common sleeping disorder, especially among children. In somnambulism the individual gets up from his/her sleep and wanders around while still sleeping.[101]		Older people may be more easily awakened by disturbances in the environment[102] and may to some degree lose the ability to consolidate sleep.		Drugs which induce sleep, known as hypnotics, include Benzodiazepines, although these interfere with REM[103]; Nonbenzodiazepine hypnotics such as eszopiclone (Lunesta), zaleplon (Sonata), and zolpidem (Ambien); Antihistamines, such as diphenhydramine (Benadryl) and doxylamine; Alcohol (ethanol), despite its rebound effect later in the night and interference with REM;[103][104] barbiturates, which have the same problem; melatonin, a component of the circadian clock, and released naturally at night by the pineal gland;[105] and cannabis, which may also interfere with REM.[106]		Stimulants, which inhibit sleep, include caffeine, an adenosine antagonist; amphetamine, MDMA, empathogen-entactogens, and related drugs; cocaine, which can alter the circadian rhythm,[107][108] and methylphenidate, which acts similarly; and other analeptic drugs like Modafinil and Armodafinil with poorly understood mechanisms.		Dietary and nutritional choices affect sleep duration and quality. Research is being conducted in an attempt to discover what kinds of nutritional choices result in better sleep quality. A study in the Western Journal of Nursing Research in 2011[109] compared how sleep quality was affected by four different diets: a high-protein diet, a high-fat diet, a high-carbohydrate diet, and a control diet. Results indicated that the diets high in protein resulted in fewer wakeful episodes during night-time sleep. The high carbohydrate diet was linked to much shorter periods of quiescent or restful sleep. These results suggest that ingested nutrients do play a role in determining sleep quality. Another investigation published in Nutrition Research in 2012[110] examined the effects of various combinations of dietary choices in regard to sleep. Although it is difficult to determine one perfect diet for sleep enhancement, this study indicated that a variety of micro and macro nutrients are needed to maintain levels of healthful and restful sleep. A varied diet containing fresh fruits and vegetables, low-fat proteins, and whole grains can be the best nutritional option for individuals seeking to improve the quality of their sleep.		Research suggests that sleep patterns vary significantly across cultures.[111][112] The most striking differences are between societies that have plentiful sources of artificial light and ones that do not.[111] The primary difference appears to be that pre-light cultures have more broken-up sleep patterns.[111] For example, people without artificial light might go to sleep far sooner after the sun sets, but then wake up several times throughout the night, punctuating their sleep with periods of wakefulness, perhaps lasting several hours.[111]		The boundaries between sleeping and waking are blurred in these societies.[111] Some observers believe that nighttime sleep in these societies is most often split into two main periods, the first characterized primarily by deep sleep and the second by REM sleep.[111]		Some societies display a fragmented sleep pattern in which people sleep at all times of the day and night for shorter periods. In many nomadic or hunter-gatherer societies, people will sleep on and off throughout the day or night depending on what is happening.[111] Plentiful artificial light has been available in the industrialized West since at least the mid-19th century, and sleep patterns have changed significantly everywhere that lighting has been introduced.[111] In general, people sleep in a more concentrated burst through the night, going to sleep much later, although this is not always the case.[111]		Historian A. Roger Ekirch thinks that the traditional pattern of "segmented sleep," as it is called, began to disappear among the urban upper class in Europe in the late 17th century and the change spread over the next 200 years; by the 1920s "the idea of a first and second sleep had receded entirely from our social consciousness."[113][114] Ekirch attributes the change to increases in "street lighting, domestic lighting and a surge in coffee houses," which slowly made nighttime a legitimate time for activity, decreasing the time available for rest.[114] Today in most societies people sleep during the night, but in very hot climates they may sleep during the day.[115] During Ramadan, many Muslims sleep during the day rather than at night.[116]		In some societies, people sleep with at least one other person (sometimes many) or with animals. In other cultures, people rarely sleep with anyone except for an intimate partner. In almost all societies, sleeping partners are strongly regulated by social standards. For example, a person might only sleep with the immediate family, the extended family, a spouse or romantic partner, children, children of a certain age, children of specific gender, peers of a certain gender, friends, peers of equal social rank, or with no one at all. Sleep may be an actively social time, depending on the sleep groupings, with no constraints on noise or activity.[111]		People sleep in a variety of locations. Some sleep directly on the ground; others on a skin or blanket; others sleep on platforms or beds. Some sleep with blankets, some with pillows, some with simple headrests, some with no head support. These choices are shaped by a variety of factors, such as climate, protection from predators, housing type, technology, personal preference, and the incidence of pests.[111]		Writing about the thematical representations of sleep in art, physician and sleep researcher Meir Kryger noted: "[Artists] have intense fascination with mythology, dreams, religious themes, the parallel between sleep and death, reward, abandonment of conscious control, healing, a depiction of innocence and serenity, and the erotic."[117]		Zwei schlafende Mädchen auf der Ofenbank, Albert Anker, 1895		Lullaby, William-Adolphe Bouguereau, 1875		Flaming June by Frederic Lord Leighton, ~1895		The Sentry by Carel Fabritius, 1654		Sleep and his Half-brother Death by John William Waterhouse, 1874		Noon – Rest from Work (after Millet) by Vincent van Gogh, 1890		Sleeping Jaguar, by Paul Klimsch		Chrapek (Snorer), Wrocław's dwarfs		The Sleep of Reason Produces Monsters by Goya, 1799		
Unlicensed assistive personnel (UAP) is a class of paraprofessionals who assist individuals with physical disabilities, mental impairments, and other health care needs with their activities of daily living (ADLs) and provide bedside care—including basic nursing procedures—all under the supervision of a registered nurse, licensed practical nurse or other health care professional. UAPs must demonstrate their abilities and competencies before gaining any expanded responsibilities within the clinical setting.		They provide care for patients in hospitals, residents of nursing facilities, clients in private homes, and others in need of their services due to effects of old age or disability. UAPs, by definition, do not hold a license or other mandatory professional requirements for practice, though many hold various certifications. They are collectively categorized under the group "personal care workers in health services" in the International Standard Classification of Occupations, 2008 revision.						Nursing assistant, nursing auxiliary, auxiliary nurse, patient care technician, home health aide/assistant, geriatric aide/assistant, psychiatric aide, nurse aide, or nurse tech are all common titles that are considered to be UAPs in many countries.		In the United States, certified nursing assistants (CNAs) typically work in a nursing home or hospital and perform everyday living tasks for the elderly, chronically sick, or rehabilitation patients who cannot care for themselves. There are some differences in scope of care across UAPs based on title and description. CNAs must become certified based on respective states' requirements. Not all states' requirements are the same. Below is a list of general requirements:[1]		Once the above requirements are completed, the person will then be certified for that respective state. Moving to a different state, however, would require recertification in the new state, unless both states use the NNAAP standard.[2] In this case, the new state would accept previous NNAAP test scores and allow registration. These certification exams are distributed by the state. Classes to study for these exams are provided by the American Red Cross as well as other providers. The courses offered by the American Red Cross encompass all facets that are addressed in the state exams from communication to health terms to sensitivity.[3]		Similar titles in the United Kingdom and elsewhere include healthcare assistant, healthcare support worker, or clinical support worker. These providers usually work in hospitals or community settings under the guidance of a qualified healthcare professional.[4]		A home health aide (HHA) provides in-home care for patients who need assistance with daily living beyond what family or friends are capable of providing. Patients include those who have a physical or mental disability, are recovering from an injury or surgery, have a chronic illness, or are advanced in age. Training requirements to become an HHA are generally minimal and vary depending on the state. HHA training is available from various outlets such as:[5]		Personal support worker (PSW) is the title for a similar type of health worker in the Province of Ontario in Canada. Some of the responsibilities and duties of a personal support worker include, but are not limited to:[6]		Personal support work is unique among health care professions in that the scope of the PSW's duties does not extend beyond what the client could do him/herself, if the client were physically and cognitively able. No other profession's scope is similarly described.[8]		Surgical technologists are considered UAPs in the US, where they are also sometimes called "scrub tech". However this title can mean different things in different countries. For example, in Mozambique, surgical technologists are medical professionals trained and registered to perform advanced clinical procedures including emergency surgery.[9]		Birth assistants, such as doulas, childbirth educators and other persons providing emotional support and general care and advice to women and families during pregnancy and childbirth, are also typically considered UAPs. They are distinguished from midwives, physicians, nurses, and other professionals who are trained and licensed to provide basic and emergency pregnancy and childbirth-related health care services and manage complications.		Unlicensed assistive personnel are important members of the health care team who often hold a high level of experience and ability. While they do not require extensive health care training to practice their profession, a high level of manual dexterity and good interpersonal communication skills are usually necessary. They often undergo some formal education, apprenticeship or on-the-job training in areas such as body mechanics, nutrition, anatomy and physiology, cognitive impairments and mental health issues, infection control, personal care skills, and records-keeping.[4][10][11][12] Most community colleges offer CNA training in one semester. However, there are other sources that offer accelerated programs. Many nursing homes will actually pay for their employees to take CNA training on the premise that once completed the student will then work for them.		In the context of aging populations and health care reform, UAPs are in growing demand in many countries.		However, without formal health professional qualifications, UAPs are often unable to perform some tasks due to issues of liability and legality. Attempts to regulate, control, and verify education have been made in some places. This allows an employer to verify experience and knowledge as well as to assist in preventing individuals who have been "struck off" (had registration/certification invalidated) from continuing to work in healthcare roles. For example, in the UK, the credibility of the Healthcare Assistant and other social care workers is intended to be strengthened by their compulsory registration from 2009 with the General Social Care Council in England or its Scottish or Welsh equivalents.		In the United States, families and employers can verify a UAPs certification in accordance with state and local laws by checking with a Family Care Safety Registry (FCSR). A Family Care Safety Registry was established by law to promote family and community safety. The registry helps to protect children, seniors, and the disabled by providing background information on that individual. Families and employers can call the registry's toll-free line by phone, fax or E-mail to request background information on registered child care, elder care, and personal care workers or to request licensure status information on licensed child care and elder care providers at no-cost to the requestor.		In Ontario, Canada in May 2011, the Ministry of Health and Long-Term Care (MOHLTC) announced the creation of a Registry of Personal Support Workers to acknowledge the care they provide daily to some of Ontario's most vulnerable populations, including seniors and people with chronic illnesses and disabilities.[13] On June 1, 2012, the Ontario PSW Registry was officially launched and now has over 23,000 registered PSWs and counting.[14]		In the United Kingdom, the Care Certificate was introduced in April 2015, following the Cavendish Review of April 2013 into standards of care among health care assistants and support workers in the NHS and social care settings.[15] It was produced to address inconsistencies in training and competencies in the workforce so that all staff have the same introductory skills, knowledge and behaviours to provide safe, high quality and compassionate care of the highest standards.[15] The Care Certificate was jointly developed by Skills for Health,[16] Health Education England[17] and Skills for Care.[18][19]		Typically, the turnover rate among unlicensed assistive personnel within an organization is very high, which can be detrimental to quality of care of patients and also create negative stress and dissatisfaction among the personnel.[20] Studies exploring the reasons for turnover show that turnover is not simply a matter of higher pay, but can have many factors such as the degree of respect experienced by the unlicensed personnel, scheduling flexibility, commitment and calling to the profession and so forth.[21]				
A banquet (/ˈbæŋk.wɪt/; French: [bɑ̃.kɛ]) is a large meal or feast,[1] complete with main courses and desserts, always served with ad libitum alcoholic beverages[citation needed], such as wine or beer. A banquet usually serves a purpose such as a charitable gathering, a ceremony, or a celebration, and is often preceded or followed by speeches in honor of someone.		In the majority of banquets, the gathering is seated at round tables with around 8-10 people per table.						Overall, there is an archaeological debate of when feasting began. Archaeologist Brian Hayden argues that feasts were an important event because the surplus of food that resulted in feasts turned into social and political ties and a competition in order to display one's own wealth. During these feasts, luxury foods were offered to their guest. What these luxury goods were are still up to debate. However, Hayden argues that animal meat and rice are some of these luxury goods because they were domesticated despite their difficulty in doing so.[2] The term banquet, however, termed from a different time period.		The idea of banqueting is ancient (see Sellisternium, Belshazzar's Feast, and Mead halls). In the 16th century, a banquet was very different from our modern perception and stems from the medieval 'ceremony of the void'. After dinner, the guests would stand and drink sweet wine and spices while the table was cleared, or ‘voided’. (Later in the 17th century ‘void’ would be replaced with the French ‘dessert’.) During the 16th century, guests would no longer stand in the great chamber whilst the table was cleared and the room prepared for entertainment, but would retire to the parlour or banqueting room.		As the idea of banqueting developed, it could take place at any time during the day and have much more in common with the later practice of taking tea. Banqueting rooms varied greatly from house to house, but were generally on an intimate scale, either in a garden room or inside such as the small banqueting turrets in Longleat House.		Today, banquets serve many purposes from training sessions, to formal business dinners. Business banquets are a popular way to strengthen bonds between businessmen and their partners. It is common that a banquet is organized at the end of an academic conference. A luau is one variety of banquet originally used in Hawaii. The Nei Mongol provincial government in China levies a tax on banquets.		
The main course is the featured or primary dish in a meal consisting of several courses. It usually follows the entrée ("entry") course. In the United States and parts of Canada, it may be called "entrée".		The main dish is usually the heaviest, heartiest, and most complex or substantial dish on a menu. The main ingredient is usually meat, fish or another protein source. It is most often preceded by an appetizer, soup or salad, and followed by a dessert. For those reasons the main course is sometimes referred to as the "meat course".		In formal dining, a well-planned main course can function as a sort of gastronomic apex or climax. In such a scheme, the preceding courses are designed to prepare for and lead up to the main course in such a way that the main course is anticipated and, when the scheme is successful, increased in its ability to satisfy and delight the diner. The courses following the main course then calm the palate and the stomach, acting as a sort of dénouement or anticlimax.		
This list of glassware[1] includes drinking vessels (drinkware) and tableware used to set a table for eating a meal, general glass items such as vases, and glasses used in the catering industry. It does not include laboratory glassware.						Drinkware, beverageware (colloquially referred to as cups) is a general term for a vessel intended to contain beverages or liquid foods for drinking or consumption.[2]		The word cup comes from Middle English cuppe, from Old English, from Late Latin cuppa, drinking vessel, perhaps variant of Latin cupa, tub, cask.[2] The first known use of the word cup is before the 12th century.[4]		Tumblers are flat-bottomed drinking glasses.		
Dosirak (도시락) in South Korea or kwakpap (곽밥) in North Korea refers to a packed meal. It usually consists of bap (cooked rice) and several banchan (side dishes).[1][2][3] The lunch boxes, also called dosirak or dosirak-tong (dosirak case), are typically plastic or thermo-steel containers with or without compartments or tiers.[4] Dosirak is often home-made, but is also sold in train stations and convenience stores.[5][6]						Home-made dosirak is often packed in tiered lunch boxes that can separate bap (cooked rice) and banchan (side dishes).[7] The guk (soup) tier, if included, is usually kept warm by insulation.[8] Plastic or thermo-steel containers are most common, but combinations of wood and lacquer, ceramics and bamboo, as well as other materials, are also used.[9]		Yennal-dosirak (옛날 도시락; "old-time dosirak") consists of bap (rice), stir-fried kimchi, egg-washed and pan-fried sausages, fried eggs, and shredded gim (seaweed), typically packed in a rectangular lunchbox made of tinplate or German silver. It is shaken with the lid on, thereby mixing the ingredients, prior to eating.[4][8]		Gimbap-dosirak (김밥 도시락; "packed gimbap"), made with sliced gimbap (seaweed rolls), is often packed for picnics.[10]		Home-made dosirak		Yennal-dosirak (old-time dosirak)		Gimbap-dosirak		Dosirak sold in convenience stores		Simple dosirak in a plastic container		Thermal dosirak case		
Mangal (or Manghal/Mangla) is a given name and a surname. Notable people with the name include:		
Technology ("science of craft", from Greek τέχνη, techne, "art, skill, cunning of hand"; and -λογία, -logia[2]) is the collection of techniques, skills, methods and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation. Technology can be the knowledge of techniques, processes, and the like, or it can be embedded in machines which can be operated without detailed knowledge of their workings.		The simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times, including the printing press, the telephone, and the Internet, have lessened physical barriers to communication and allowed humans to interact freely on a global scale. The steady progress of military technology has brought weapons of ever-increasing destructive power, from clubs to nuclear weapons.		Technology has many effects. It has helped develop more advanced economies (including today's global economy) and has allowed the rise of a leisure class. Many technological processes produce unwanted by-products known as pollution and deplete natural resources to the detriment of Earth's environment. Innovations have always influenced the values of a society and raised new questions of the ethics of technology. Examples include the rise of the notion of efficiency in terms of human productivity, and the challenges of bioethics.		Philosophical debates have arisen over the use of technology, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticise the pervasiveness of technology, arguing that it harms the environment and alienates people; proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition.						The use of the term "technology" has changed significantly over the last 200 years. Before the 20th century, the term was uncommon in English, and usually referred to the description or study of the useful arts.[3] The term was often connected to technical education, as in the Massachusetts Institute of Technology (chartered in 1861).[4]		The term "technology" rose to prominence in the 20th century in connection with the Second Industrial Revolution. The term's meanings changed in the early 20th century when American social scientists, beginning with Thorstein Veblen, translated ideas from the German concept of Technik into "technology." In German and other European languages, a distinction exists between technik and technologie that is absent in English, which usually translates both terms as "technology." By the 1930s, "technology" referred not only to the study of the industrial arts but to the industrial arts themselves.[5]		In 1937, the American sociologist Read Bain wrote that "technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them."[6] Bain's definition remains common among scholars today, especially social scientists. Scientists and engineers usually prefer to define technology as applied science, rather than as the things that people make and use.[7] More recently, scholars have borrowed from European philosophers of "technique" to extend the meaning of technology to various forms of instrumental reason, as in Foucault's work on technologies of the self (techniques de soi).		Dictionaries and scholars have offered a variety of definitions. The Merriam-Webster Learner's Dictionary offers a definition of the term: "the use of science in industry, engineering, etc., to invent useful things or to solve problems" and "a machine, piece of equipment, method, etc., that is created by technology."[8] Ursula Franklin, in her 1989 "Real World of Technology" lecture, gave another definition of the concept; it is "practice, the way we do things around here."[9] The term is often used to imply a specific field of technology, or to refer to high technology or just consumer electronics, rather than technology as a whole.[10] Bernard Stiegler, in Technics and Time, 1, defines technology in two ways: as "the pursuit of life by means other than life," and as "organized inorganic matter."[11]		Technology can be most broadly defined as the entities, both material and immaterial, created by the application of mental and physical effort in order to achieve some value. In this usage, technology refers to tools and machines that may be used to solve real-world problems. It is a far-reaching term that may include simple tools, such as a crowbar or wooden spoon, or more complex machines, such as a space station or particle accelerator. Tools and machines need not be material; virtual technology, such as computer software and business methods, fall under this definition of technology.[12] W. Brian Arthur defines technology in a similarly broad way as "a means to fulfill a human purpose."[13]		The word "technology" can also be used to refer to a collection of techniques. In this context, it is the current state of humanity's knowledge of how to combine resources to produce desired products, to solve problems, fulfill needs, or satisfy wants; it includes technical methods, skills, processes, techniques, tools and raw materials. When combined with another term, such as "medical technology" or "space technology," it refers to the state of the respective field's knowledge and tools. "State-of-the-art technology" refers to the high technology available to humanity in any field.		Technology can be viewed as an activity that forms or changes culture.[14] Additionally, technology is the application of math, science, and the arts for the benefit of life as it is known. A modern example is the rise of communication technology, which has lessened barriers to human interaction and as a result has helped spawn new subcultures; the rise of cyberculture has at its basis the development of the Internet and the computer.[15] Not all technology enhances culture in a creative way; technology can also help facilitate political oppression and war via tools such as guns. As a cultural activity, technology predates both science and engineering, each of which formalize some aspects of technological endeavor.		The distinction between science, engineering, and technology is not always clear. Science is systematic knowledge of the physical or material world gained through observation and experimentation.[16] Technologies are not usually exclusively products of science, because they have to satisfy requirements such as utility, usability, and safety.		Engineering is the goal-oriented process of designing and making tools and systems to exploit natural phenomena for practical human means, often (but not always) using results and techniques from science. The development of technology may draw upon many fields of knowledge, including scientific, engineering, mathematical, linguistic, and historical knowledge, to achieve some practical result.		Technology is often a consequence of science and engineering, although technology as a human activity precedes the two fields. For example, science might study the flow of electrons in electrical conductors by using already-existing tools and knowledge. This new-found knowledge may then be used by engineers to create new tools and machines such as semiconductors, computers, and other forms of advanced technology. In this sense, scientists and engineers may both be considered technologists; the three fields are often considered as one for the purposes of research and reference.[17]		The exact relations between science and technology in particular have been debated by scientists, historians, and policymakers in the late 20th century, in part because the debate can inform the funding of basic and applied science. In the immediate wake of World War II, for example, it was widely considered in the United States that technology was simply "applied science" and that to fund basic science was to reap technological results in due time. An articulation of this philosophy could be found explicitly in Vannevar Bush's treatise on postwar science policy, Science – The Endless Frontier: "New products, new industries, and more jobs require continuous additions to knowledge of the laws of nature ... This essential new knowledge can be obtained only through basic scientific research."[18] In the late-1960s, however, this view came under direct attack, leading towards initiatives to fund science for specific tasks (initiatives resisted by the scientific community). The issue remains contentious, though most analysts resist the model that technology simply is a result of scientific research.[19][20]		The use of tools by early humans was partly a process of discovery and of evolution. Early humans evolved from a species of foraging hominids which were already bipedal,[21] with a brain mass approximately one third of modern humans.[22] Tool use remained relatively unchanged for most of early human history. Approximately 50,000 years ago, the use of tools and complex set of behaviors emerged, believed by many archaeologists to be connected to the emergence of fully modern language.[23]		Hominids started using primitive stone tools millions of years ago. The earliest stone tools were little more than a fractured rock, but approximately 75,000 years ago,[24] pressure flaking provided a way to make much finer work.		The discovery and utilization of fire, a simple energy source with many profound uses, was a turning point in the technological evolution of humankind.[25] The exact date of its discovery is not known; evidence of burnt animal bones at the Cradle of Humankind suggests that the domestication of fire occurred before 1 Ma;[26] scholarly consensus indicates that Homo erectus had controlled fire by between 500 and 400 ka.[27][28] Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.[29]		Other technological advances made during the Paleolithic era were clothing and shelter; the adoption of both technologies cannot be dated exactly, but they were a key to humanity's progress. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 ka, humans were constructing temporary wood huts.[30][31] Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate out of Africa by 200 ka and into other continents such as Eurasia.[32]		Human's technological ascent began in earnest in what is known as the Neolithic Period ("New Stone Age"). The invention of polished stone axes was a major advance that allowed forest clearance on a large scale to create farms. This use of polished stone axes increased greatly in the Neolithic, but were originally used in the preceding Mesolithic in some areas such as Ireland.[33] Agriculture fed larger populations, and the transition to sedentism allowed simultaneously raising more children, as infants no longer needed to be carried, as nomadic ones must. Additionally, children could contribute labor to the raising of crops more readily than they could to the hunter-gatherer economy.[34][35]		With this increase in population and availability of labor came an increase in labor specialization.[36] What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war amongst adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.[37]		Continuing improvements led to the furnace and bellows and provided the ability to smelt and forge native metals (naturally occurring in relatively pure form).[38] Gold, copper, silver, and lead, were such early metals. The advantages of copper tools over stone, bone, and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 ka).[39] Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4000 BCE). The first uses of iron alloys such as steel dates to around 1800 BCE.[40][41]		Meanwhile, humans were learning to harness other forms of energy. The earliest known use of wind power is the sailboat; the earliest record of a ship under sail is that of a Nile boat that dates back to the 8th millennium BCE.[42] From prehistoric times, Egyptians probably used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and "catch" basins. Similarly, the early peoples of Mesopotamia, the Sumerians, learned to use the Tigris and Euphrates Rivers for much the same purposes. However, more extensive use of wind and water (and even human) power required another invention.		According to archaeologists, the wheel was invented around 4000 BCE probably independently and nearly simultaneously in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture) and Central Europe.[43] Estimates on when this may have occurred range from 5500 to 3000 BCE with most experts putting it closer to 4000 BCE.[44] The oldest artifacts with drawings that depict wheeled carts date from about 3500 BCE;[45] however, the wheel may have been in use for millennia before these drawings were made. There is also evidence from the same period for the use of the potter's wheel. More recently, the oldest-known wooden wheel in the world was found in the Ljubljana marshes of Slovenia.[46]		The invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources.		Innovations continued through the Middle Ages with innovations such as silk, the horse collar and horseshoes in the first few hundred years after the fall of the Roman Empire. Medieval technology saw the use of simple machines (such as the lever, the screw, and the pulley) being combined to form more complicated tools, such as the wheelbarrow, windmills and clocks. The Renaissance brought forth many of these innovations, including the printing press (which facilitated the greater communication of knowledge), and technology became increasingly associated with science, beginning a cycle of mutual advancement. The advancements in technology in this era allowed a more steady supply of food, followed by the wider availability of consumer goods.		Starting in the United Kingdom in the 18th century, the Industrial Revolution was a period of great technological discovery, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, driven by the discovery of steam power. Technology took another step in a second industrial revolution with the harnessing of electricity to create such innovations as the electric motor, light bulb, and countless others. Scientific advancement and the discovery of new concepts later allowed for powered flight and advancements in medicine, chemistry, physics, and engineering. The rise in technology has led to skyscrapers and broad urban areas whose inhabitants rely on motors to transport them and their food supply. Communication was also greatly improved with the invention of the telegraph, telephone, radio and television. The late 19th and early 20th centuries saw a revolution in transportation with the invention of the airplane and automobile.		The 20th century brought a host of innovations. In physics, the discovery of nuclear fission has led to both nuclear weapons and nuclear power. Computers were also invented and later miniaturized utilizing transistors and integrated circuits. Information technology subsequently led to the creation of the Internet, which ushered in the current Information Age. Humans have also been able to explore space with satellites (later used for telecommunication) and in manned missions going all the way to the moon. In medicine, this era brought innovations such as open-heart surgery and later stem cell therapy along with new medications and treatments.		Complex manufacturing and construction techniques and organizations are needed to make and maintain these new technologies, and entire industries have arisen to support and develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have been created to support them, including engineering, medicine, and computer science, and other fields have been made more complex, such as construction, transportation and architecture.		Generally, technicism is the belief in the utility of technology for improving human societies.[47] Taken to an extreme, technicism "reflects a fundamental attitude which seeks to control reality, to resolve all problems with the use of scientific-technological methods and tools."[48] In other words, human beings will someday be able to master all problems and possibly even control the future using technology. Some, such as Stephen V. Monsma,[49] connect these ideas to the abdication of religion as a higher moral authority.		Optimistic assumptions are made by proponents of ideologies such as transhumanism and singularitarianism, which view technological development as generally having beneficial effects for the society and the human condition. In these ideologies, technological development is morally good.		Transhumanists generally believe that the point of technology is to overcome barriers, and that what we commonly refer to as the human condition is just another barrier to be surpassed.		Singularitarians believe in some sort of "accelerating change"; that the rate of technological progress accelerates as we obtain more technology, and that this will culminate in a "Singularity" after artificial general intelligence is invented in which progress is nearly infinite; hence the term. Estimates for the date of this Singularity vary,[50] but prominent futurist Ray Kurzweil estimates the Singularity will occur in 2045.		Kurzweil is also known for his history of the universe in six epochs: (1) the physical/chemical epoch, (2) the life epoch, (3) the human/brain epoch, (4) the technology epoch, (5) the artificial intelligence epoch, and (6) the universal colonization epoch. Going from one epoch to the next is a Singularity in its own right, and a period of speeding up precedes it. Each epoch takes a shorter time, which means the whole history of the universe is one giant Singularity event.[51]		Some critics see these ideologies as examples of scientism and techno-utopianism and fear the notion of human enhancement and technological singularity which they support. Some have described Karl Marx as a techno-optimist.[52]		On the somewhat skeptical side are certain philosophers like Herbert Marcuse and John Zerzan, who believe that technological societies are inherently flawed. They suggest that the inevitable result of such a society is to become evermore technological at the cost of freedom and psychological health.		Many, such as the Luddites and prominent philosopher Martin Heidegger, hold serious, although not entirely, deterministic reservations about technology (see "The Question Concerning Technology"[53]). According to Heidegger scholars Hubert Dreyfus and Charles Spinosa, "Heidegger does not oppose technology. He hopes to reveal the essence of technology in a way that 'in no way confines us to a stultified compulsion to push on blindly with technology or, what comes to the same thing, to rebel helplessly against it.' Indeed, he promises that 'when we once open ourselves expressly to the essence of technology, we find ourselves unexpectedly taken into a freeing claim.'[54] What this entails is a more complex relationship to technology than either techno-optimists or techno-pessimists tend to allow."[55]		Some of the most poignant criticisms of technology are found in what are now considered to be dystopian literary classics such as Aldous Huxley's Brave New World, Anthony Burgess's A Clockwork Orange, and George Orwell's Nineteen Eighty-Four. In Goethe's Faust, Faust selling his soul to the devil in return for power over the physical world is also often interpreted as a metaphor for the adoption of industrial technology. More recently, modern works of science fiction such as those by Philip K. Dick and William Gibson and films such as Blade Runner and Ghost in the Shell project highly ambivalent or cautionary attitudes toward technology's impact on human society and identity.		The late cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called "technopolies," societies that are dominated by the ideology of technological and scientific progress to the exclusion or harm of other cultural practices, values and world-views.[56]		Darin Barney has written about technology's impact on practices of citizenship and democratic culture, suggesting that technology can be construed as (1) an object of political debate, (2) a means or medium of discussion, and (3) a setting for democratic deliberation and citizenship. As a setting for democratic culture, Barney suggests that technology tends to make ethical questions, including the question of what a good life consists in, nearly impossible, because they already give an answer to the question: a good life is one that includes the use of more and more technology.[57]		Nikolas Kompridis has also written about the dangers of new technology, such as genetic engineering, nanotechnology, synthetic biology, and robotics. He warns that these technologies introduce unprecedented new challenges to human beings, including the possibility of the permanent alteration of our biological nature. These concerns are shared by other philosophers, scientists and public intellectuals who have written about similar issues (e.g. Francis Fukuyama, Jürgen Habermas, William Joy, and Michael Sandel).[58]		Another prominent critic of technology is Hubert Dreyfus, who has published books such as On the Internet and What Computers Still Can't Do.		A more infamous anti-technological treatise is Industrial Society and Its Future, written by the Unabomber Ted Kaczynski and printed in several major newspapers (and later books) as part of an effort to end his bombing campaign of the techno-industrial infrastructure.		The notion of appropriate technology was developed in the 20th century by thinkers such as E. F. Schumacher and Jacques Ellul to describe situations where it was not desirable to use very new technologies or those that required access to some centralized infrastructure or parts or skills imported from elsewhere. The ecovillage movement emerged in part due to this concern.		This section mainly focuses on American concerns even if it can reasonably be generalized to other Western countries.		The inadequate quantity and quality of American jobs is one of the most fundamental economic challenges we face. [...] What's the linkage between technology and this fundamental problem?		In his article, Jared Bernstein, a Senior Fellow at the Center on Budget and Policy Priorities,[59] questions the widespread idea that automation, and more broadly, technological advances, have mainly contributed to this growing labor market problem. His thesis appears to be a third way between optimism and skepticism. Essentially, he stands for a neutral approach of the linkage between technology and American issues concerning unemployment and declining wages.		He uses two main arguments to defend his point. First, because of recent technological advances, an increasing number of workers are losing their jobs. Yet, scientific evidence fails to clearly demonstrate that technology has displaced so many workers that it has created more problems than it has solved. Indeed, automation threatens repetitive jobs but higher-end jobs are still necessary because they complement technology and manual jobs that "requires flexibility judgment and common sense"[60] remain hard to replace with machines. Second, studies have not shown clear links between recent technology advances and the wage trends of the last decades.		Therefore, according to Bernstein, instead of focusing on technology and its hypothetical influences on current American increasing unemployment and declining wages, one needs to worry more about "bad policy that fails to offset the imbalances in demand, trade, income and opportunity."[60]		Thomas P. Hughes stated that because technology has been considered as a key way to solve problems, we need to be aware of its complex and varied characters to use it more efficiently.[61] What is the difference between a wheel or a compass and cooking machines such as an oven or a gas stove? Can we consider all of them, only a part of them, or none of them as technologies?		Technology is often considered too narrowly; according to Hughes, "Technology is a creative process involving human ingenuity".[62] This definition's emphasis on creativity avoids unbounded definitions that may mistakenly include cooking “technologies," but it also highlights the prominent role of humans and therefore their responsibilities for the use of complex technological systems.		Yet, because technology is everywhere and has dramatically changed landscapes and societies, Hughes argues that engineers, scientists, and managers have often believed that they can use technology to shape the world as they want. They have often supposed that technology is easily controllable and this assumption has to be thoroughly questioned.[61] For instance, Evgeny Morozov particularly challenges two concepts: “Internet-centrism” and “solutionism."[63] Internet-centrism refers to the idea that our society is convinced that the Internet is one of the most stable and coherent forces. Solutionism is the ideology that every social issue can be solved thanks to technology and especially thanks to the internet. In fact, technology intrinsically contains uncertainties and limitations. According to Alexis Madrigal's review of Morozov's theory, to ignore it will lead to “unexpected consequences that could eventually cause more damage than the problems they seek to address."[64] Benjamin R. Cohen and Gwen Ottinger also discussed the multivalent effects of technology.[65]		Therefore, recognition of the limitations of technology, and more broadly, scientific knowledge, is needed – especially in cases dealing with environmental justice and health issues. Ottinger continues this reasoning and argues that the ongoing recognition of the limitations of scientific knowledge goes hand in hand with scientists and engineers’ new comprehension of their role. Such an approach of technology and science "[require] technical professionals to conceive of their roles in the process differently. [They have to consider themselves as] collaborators in research and problem solving rather than simply providers of information and technical solutions."[66]		Technology is properly defined as any application of science to accomplish a function. The science can be leading edge or well established and the function can have high visibility or be significantly more mundane, but it is all technology, and its exploitation is the foundation of all competitive advantage.		Technology-based planning is what was used to build the US industrial giants before WWII (e.g., Dow, DuPont, GM) and it is what was used to transform the US into a superpower. It was not economic-based planning.		The use of basic technology is also a feature of other animal species apart from humans. These include primates such as chimpanzees,[67] some dolphin communities,[68] and crows.[69][70] Considering a more generic perspective of technology as ethology of active environmental conditioning and control, we can also refer to animal examples such as beavers and their dams, or bees and their honeycombs.		The ability to make and use tools was once considered a defining characteristic of the genus Homo.[71] However, the discovery of tool construction among chimpanzees and related primates has discarded the notion of the use of technology as unique to humans. For example, researchers have observed wild chimpanzees utilising tools for foraging: some of the tools used include leaf sponges, termite fishing probes, pestles and levers.[72] West African chimpanzees also use stone hammers and anvils for cracking nuts,[73] as do capuchin monkeys of Boa Vista, Brazil.[74]		Theories of technology often attempt to predict the future of technology based on the high technology and science of the time. As with all predictions of the future, however, technology's is uncertain.		Futurist Ray Kurzweil predicts that the future of technology will be mainly consist of an overlapping "GNR Revolution" of Genetics, Nanotechnology, and Robotics, with robotics being the most important of the three.[75]		
Lunch, the abbreviation for luncheon, is a meal typically eaten at midday.[1] The origin of the words lunch and luncheon relate to a small snack originally eaten at any time of the day or night. During the 20th century the meaning gradually narrowed to a small or mid-sized meal eaten at midday. Lunch is commonly the second meal of the day, after breakfast. The meal varies in size depending on the culture, and significant variations exist in different areas of the world.						The abbreviation lunch is taken from the more formal Northern English word luncheon, which is derived from the Anglo-Saxon word nuncheon or nunchin meaning 'noon drink'.[2] The term has been in common use since 1823.[3][a] The Oxford English Dictionary (OED) reports usage of the words beginning in 1580 to describe a meal that was eaten between more substantial meals. It may also mean a piece of cheese or bread.[3]		In medieval Germany, there are references to similariar, a sir lunchentach according to the OED, a noon draught – of ale, with bread – an extra meal between midday dinner and supper, especially during the long hours of hard labour during haying or early harvesting.		Meals have become ingrained in each society as being natural and logical. What one society eats may seem extraordinary to another. The same is true of what was eaten long ago in history as food tastes, menu items and meal periods have changed greatly over time. The word supper means bread and soup[4] (from the German word sop- soup or stew over bread[5]). Dinner comes from the French word disner which originates from the Latin word disjejeunare which meant to break fast and was a meal eaten in the morning, not the end of the day.[4]		In general, during the Middle Ages the main meal for almost everyone took place late in the morning, after several hours of work, when there was no need for artificial lighting. During the 17th and 18th centuries, this meal, called dinner, was gradually pushed back into the evening, creating a greater time gap between breakfast and dinner. A meal called lunch came to fill the gap.[6] A formal evening meal, artificially lit by candles, sometimes with entertainment, was a supper party as late as the Regency era.		Up until the early 19th century, luncheon was generally reserved for the ladies, who would often have lunch with one another when their husbands were out. As late as 1945, Emily Post wrote in the magazine Etiquette that luncheon is "generally given by and for women, but it is not unusual, especially in summer places or in town on Saturday or Sunday, to include an equal number of men" – hence the mildly disparaging phrase, "the ladies who lunch". Lunch was a ladies' light meal; when the Prince of Wales stopped to eat a dainty luncheon with lady friends, he was laughed at for this effeminacy.[6]		Beginning in the 1840s, afternoon tea supplemented this luncheon at four o'clock.[6] Mrs Beeton's Book of Household Management (1861) – a guide to all aspects of running a household in Victorian Britain, edited by Isabella Beeton – had much less to explain about luncheon than about dinners or ball suppers:		The remains of cold joints, nicely garnished, a few sweets, or a little hashed meat, poultry or game, are the usual articles placed on the table for luncheon, with bread and cheese, biscuits, butter, etc. If a substantial meal is desired, rump-steaks or mutton chops may be served, as also veal cutlets, kidneys... In families where there is a nursery, the mistress of the house often partakes of the meal with the children, and makes it her luncheon. In the summer, a few dishes of fresh fruit should be added to the luncheon, or, instead of this, a compote of fruit or fruit tart, or pudding.[7]		With the onset of industrialization in the 19th century, male workers began to work long shifts at the factory, severely disrupting the age-old eating habits of rural life. Initially, workers were sent home for a brief dinner provided by their wives, but as the workplace was removed farther from the home, working men took to providing themselves with something portable to eat during a break in the middle of the day.		The lunch meal slowly became institutionalized in England when workers with long and fixed hour jobs at the factory were eventually given an hour off of work to eat lunch and thus gain strength for the afternoon shift. Stalls and later chop houses near the factories began to provide mass-produced food for the working class, and the meal soon became an established part of the daily routine, remaining so to this day.[8]		In many countries and regions lunch is the dinner or main meal.[9] Prescribed lunchtimes allow workers to return to their homes to eat with their families. Consequently, where lunch is the customary main meal of the day, businesses close during lunchtime. Lunch also becomes dinner on special days, such as holidays or special events, including, for example, Christmas dinner and harvest dinners such as Thanksgiving; on these special days, dinner is usually served in early afternoon. Among Christians, the main meal on Sunday, whether at a restaurant or at home, is called "Sunday dinner", and is served after morning church services.[citation needed]		A traditional Bengali lunch is a seven-course meal. Bengali cuisine is a culinary style originating in Bengal, a region in the eastern part of the Indian subcontinent, which is now divided between Bangladesh and West Bengal. The first course is shukto, which is a mix of vegetables cooked with few spices and topped with a coconut sauce. The second course consists of rice, dal, and a vegetable curry. The third course consists of rice and fish curry. The fourth course is that of rice and meat curry (generally chevon, mutton, chicken or lamb). The fifth course contains sweet preparations like rasgulla, pantua, rajbhog, sandesh, etc. The sixth course consists of payesh or mishti doi (sweet yogurt). The seventh course is that of paan, which acts as a mouth freshener.		In China today, lunch is not nearly as complicated as it was before industrialization. Rice, noodles and other mixed hot foods are often eaten, either at a restaurant or brought in a container. Western cuisine is not uncommon. It is called 午餐 or 午饭 in most areas.		Lunch foods at a Japanese restaurant in Hong Kong		Japanese cuisine: An udon lunch set		A typical South Indian lunch served in a plate		Typical Indian "thali" lunch		Vietnamese phở, commonly served as lunch		Lunch in Denmark, referred to as frokost,[10] is a light meal. Often it includes rye bread with different toppings such as liver pâté, herring, and cheese.[11][12][13] Smørrebrød is a Danish lunch delicacy that is often used for business meetings or special events.		In Finland, lunch is a full hot meal,[b] served as one course, sometimes with small salads and desserts. Dishes are diverse, ranging from meat or fish courses to soups that are heavy enough to constitute a meal.[15]		In France, the midday meal is taken between noon and 2:00 pm.[16]		In Germany lunch is the main meal of the day.[c] It is traditionally a substantial hot meal, sometimes with additional courses like soup and dessert. It is usually a savory dish, consisting of protein (e.g., meat), starchy foods (e.g., potatoes) and vegetables or salad. Casseroles and stews are popular as well. There are a few sweet dishes like Germknödel or rice pudding that can serve as a main course, too. Lunch is called Mittagessen – literally, "midday's food".		In the Netherlands, Belgium and Norway, it is common to eat sandwiches for lunch: slices of bread that people usually carry to work or school and eat in the canteen. The slices of bread are usually filled with sweet or savory foodstuffs such as chocolate sprinkles (vlokken), apple syrup, peanut butter, slices of meat, cheese or kroket. The meal typically includes coffee, milk or juice, and sometimes yogurt, some fruit or soup. It is eaten around noon, during a lunch break.		In Portugal, lunch (almoço in Portuguese) consists of a full hot meal, similar to dinner, normally with soup, a meat or fish course, and dessert. It is served between noon and 2:00 pm. It is the main meal of the day throughout the country with the exceptions of the Metropolitan areas of Lisbon and Porto, where lighter meals or snacks are not uncommon. The Portuguese word lanches derives from the English word "lunch", but it refers to a lighter meal or snack taken during the afternoon (around 5 pm) due to the fact that, traditionally, Portuguese dinner is served at a later hour than in English-speaking countries.		In Spain, the midday meal, "lunch" takes place between 1:00 pm and 3:00 pm and is effectively dinner, (the main meal of the day); in contrast, supper does not usually begin until between 8:30 pm and 10:00 pm. Being the main meal of the day everywhere, it usually consists of a three-course meal: the first course usually consists of an appetizer; the main course of a more elaborate dish, usually meat- or fish-based; the dessert of something sweet, often accompanied by a coffee or small amounts of spirits. Most places of work have a complete restaurant with a lunch break of a least an hour. Spanish schools have a complete restaurant as well, and students have a one-hour break. Three courses are common practice at home, workplace, and schools. Most small shops close for between two and four hours – usually between 1:30 pm to 4:30 pm – to allow to go home for a full lunch.		In Sweden, lunch is usually a full hot meal, much as in Finland.[b]		In the United Kingdom, lunch is often a small meal, designed to stave off hunger until returning home from work and eating dinner. It is usually eaten early in the afternoon.[18] Lunch is often purveyed and consumed in pubs.[19] Pub lunch dishes include fish and chips, ploughman's lunch and others.[18]		A typical Swedish school lunch		A Swedish outdoor picnic		A cheeseburger at a pub in London		In Hungary, lunch is traditionally the main meal of the day,[20] following a leves (soup).		In Poland the main meal of the day (called obiad) is traditionally eaten between 1:00 pm and 5:00 pm,[d] and consists of a soup and a main dish. Most Poles equate the English word "lunch" with "obiad" because it is the second of the three main meals of the day; śniadanie (breakfast), obiad (lunch/dinner) and kolacja (dinner/supper). There is another meal eaten by some called drugie śniadanie, which means "second breakfast". Drugie śniadanie is eaten around 10:00 am and is a light snack, usually consisting of sandwiches, salad or a thin soup.		In Russia, the midday meal is taken in the afternoon. Usually, lunch is the biggest meal[e] and consists of a first course, usually a soup, and a second course which would be meat and a garnish. Tea is standard.		In Bosnia and Herzegovina, lunch is the main meal of the day. It is traditionally a substantial hot meal, sometimes with additional courses like soup and dessert. It is usually a savory dish, consisting of protein (such as meat), starchy foods (such as potatoes), and a vegetable or salad. It is usually eaten around 2:00 pm.		In Romania, lunch (prânz in Romanian) is the main hot meal of the day.[23] It is usually eaten at 12:00, but never later than 3:00 pm. Lunch normally consists of two dishes: usually, the first course is a light soup and the second course, the main course, often consists of meat accompanied by potato, rice or pasta (garnitură)[citation needed] Traditionally, people used to bake and eat desserts, but nowadays it is less common. On Sundays, the lunch is more consistent and is usually accompanied by an appetizer or salad.		In the Middle East and in most Arab countries, lunch is eaten between 1:00 pm and 4:00 pm and is the main meal of the day. It usually consists of meat, rice, vegetables and sauces and is sometimes but not always followed by dessert. Lunch is also eaten as a light meal at times in the Middle East, such as when children arrive at home from school while the parents are still out working.[24] Water is commonly served, which may be iced, and other beverages such as soft drinks or yogurt drinks are also consumed.[24]		In the United States and Canada, lunch is usually a moderately sized meal generally eaten around noon. During the work week, North Americans generally eat a quick lunch that often includes some type of sandwich, soup, or leftovers from the previous night's dinner (e.g., rice or pasta). Children often bring packed lunches to school, which might consist of a sandwich such as bologna (or other cold cut) and cheese, tuna, chicken, or peanut butter and jelly, or, in Canada, savoury pie, as well as some fruit, chips, dessert and a drink such as juice, milk, or water. Adults may leave work to go out for a quick lunch, which might include some type of hot or cold sandwich such as a hamburger or "sub" sandwich. Salads and soups are also common, as well as a soup and sandwich, tacos, burritos, sushi, bento boxes, and pizza. Some individuals may pack leftovers for lunch. Lunch may be consumed at various types of restaurants, such as formal, fast casual and fast food restaurants. Canadians and Americans generally do not go home for lunch, and lunch rarely lasts more than an hour except for business lunches, which may last longer. In the United States the three-martini lunch – so called because the meal extends to the amount of time it takes to drink three martinis – has been making a comeback since 2010. [25] Businesses can deduct 80% of the cost of these lunches.[26] Children generally are given a break in the middle of the school day to eat lunch. Public schools often have a cafeteria where children can buy lunch or eat a packed lunch. Boarding schools and private schools, including universities, often have a cafeteria where lunch is served.		In Mexico, lunch (Comida) is usually the main meal of the day and normally takes place between 2:00 pm and 4:00 pm. It usually includes three or four courses: the first is an entrée of rice, noodles or pasta, but also may include a soup or salad. The second consists of a main dish, called a guisado, served with one or two side dishes such as refried beans, cooked vegetables, rice or salad. The main dish is accompanied by tortillas or a bread called bolillo. The third course is a combination of a traditional dessert or sweet, café de olla, and a digestif. During the meal, it is usual to drink aguas frescas, although soft drinks have gained ground in recent years.		In Australia, a light meal eaten in the period between 10:30 am and noon is considered brunch; an actual lunch will be eaten between 12 and 2PM.[citation needed] While usually consisting of fruit or a cereal product, a typical Australian brunch may include other foods as well such as burgers, sandwiches, other light food items, and hot dishes[citation needed]. Sometimes a meal during the late afternoon is referred to as "afternoon tea"[citation needed], a meal in which food portions are usually significantly smaller than at lunch, sometimes consisting of nothing more than coffee or other beverages[citation needed].		In Argentina, lunch is usually the main meal of the day, and normally takes place between noon and 2:00 p.m. People usually eat a wide variety of foods[27][f] like chicken, beef, pasta, salads and a drink such as water, soda or wine and some dessert. Although at work, people usually take a fast meal which can consist on some kind of sandwich brought from home or bought in a fast food place.		In Brazil, lunch is the main meal of the day,[g] taking place between 11:30 a.m. and 2:00 p.m. Brazilians basically eat rice with beans, salad and meat, but the kind of food may vary from region to region. In the Northern areas (Amazon basin and other large rivers), most people eat fish, but there is also beef, rice, beans, and farofa. Fried chicken is also widely consumed. During the weekend, Brazilians usually eat churrasco (barbecue) and Feijoada. In Bahia, it is common to eat fish with dende oil with acarajé. In São Paulo, Italian and Japanese foods, mixed with barbecue and other native dishes, are popular. Brazilians also like to eat tapioca and cheese bread (known as pão de queijo), both made with cassava powder, and can substitute wheat bread for people. There is a large variety of Brazilian fruits, such as cupuaçu, cashew fruit and nut, fruta do conde, amora, maracujá, papaya, jabuticaba, cajá and others, and many dishes made with them, such as açaí. guaraná soft drink, coffee, caipirinha, beer and coconut water and a candy called brigadeiro.		Since lunch typically falls in the early-middle of the working day, it can either be eaten on a break from work, or as part of the workday. The difference between those who work through lunch and those who take it off could be a matter of cultural, social class, bargaining power, or the nature of the work. Also, to simplify matters, some cultures refer to meal breaks at work as "lunch" no matter when they occur – even in the middle of the night. This is especially true for jobs that have employees that rotate shifts.		Farmworkers taking a lunch break at Nieuw-Scheemda, Oldambt, Groningen, Netherlands, c. 1955		U.S. President Barack Obama and Canadian Prime Minister Stephen Harper with aides during a working luncheon in the Canadian Parliament in Ottawa, Ontario, in 2009		
Peptide hormones and protein hormones are hormones whose molecules are peptides or proteins, respectively. The latter have longer amino acid chain lengths than the former. These hormones have an effect on the endocrine system of animals, including humans.[1] Most hormones can be classified as either amino acid–based hormones (amine, peptide, or protein) or steroid hormones. The former are water-soluble and act on the surface of target cells via second messengers; the latter, being lipid-soluble, move through the plasma membranes of target cells (both cytoplasmic and nuclear) to act within their nuclei		Like all peptides and proteins, peptide hormones and protein hormones are synthesized in cells from amino acids according to mRNA transcripts, which are synthesized from DNA templates inside the cell nucleus. Preprohormones, peptide hormone precursors, are then processed in several stages, typically in the endoplasmic reticulum, including removal of the N-terminal signal sequence and sometimes glycosylation, resulting in prohormones. The prohormones are then packaged into membrane-bound secretory vesicles, which can be secreted from the cell by exocytosis in response to specific stimuli (e.g. --an increase in Ca2+ and cAMP concentration in cytoplasm).[2]		These prohormones often contain superfluous amino acid residues that were needed to direct folding of the hormone molecule into its active configuration but have no function once the hormone folds. Specific endopeptidases in the cell cleave the prohormone just before it is released into the bloodstream, generating the mature hormone form of the molecule. Mature peptide hormones then travel through the blood to all of the cells of the body, where they interact with specific receptors on the surfaces of their target cells.		Some neurotransmitters are secreted and released in a similar fashion to peptide hormones, and some 'neuropeptides' may be used as neurotransmitters in the nervous system in addition to acting as hormones when released into the blood.		When a peptide hormone binds to a receptor on the surface of the cell, a second messenger appears in the cytoplasm, which triggers signal transduction leading to the cellular responses.[3]		Some peptide/protein hormones (angiotensin II, basic fibroblast growth factor-2, parathyroid hormone-related protein) also interact with intracellular receptors located in the cytoplasm or nucleus by an intracrine mechanism.[4]		Several important peptide hormones are secreted from the pituitary gland. The anterior pituitary secretes three hormones: prolactin, which acts on the mammary gland; adrenocorticotropic hormone (ACTH), which acts on the adrenal cortex to regulate the secretion of glucocorticoids; and growth hormone, which acts on bone, muscle, and the liver. The posterior pituitary gland secretes antidiuretic hormone, also called vasopressin, and oxytocin. Peptide hormones are produced by many different organs and tissues, however, including the heart (atrial-natriuretic peptide (ANP) or atrial natriuretic factor (ANF)) and pancreas (glucagon, insulin and somatostatin), the gastrointestinal tract (cholecystokinin, gastrin), and adipose tissue stores (leptin).[5][6]				
A potluck is a gathering where each guest contributes a different and unique dish of food, often homemade, to be shared. Synonyms include: potluck dinner, spread, Jacob's join,[1][2] Jacob's supper, faith supper, covered dish supper, dish party, bring and share, shared lunch, pitch-in, bring-a-plate, dish-to-pass, fuddle, and carry-in.						The word pot-luck appears in the 16th century English work of Thomas Nashe, and used to mean "food provided for an unexpected or uninvited guest, the luck of the pot."[this quote needs a citation] The sense "communal meal, where guests bring their own food," originated in the 1930s during the Depression [3], and is an eggcorn from potlatch and by extension of traditional sense of "luck of the pot".		Potluck dinners are events where the attendees bring a dish to a meal. Potluck dinners are often organized by religious or community groups, since they simplify the meal planning and distribute the costs among the participants. Smaller, more informal get-togethers with distributed food preparation may also be called potlucks. The only traditional rule is that each dish be large enough to be shared among a good portion (but not necessarily all) of the anticipated guests. In some cases each participant agrees ahead of time to bring a single course, and the result is a multi-course meal. Guests may bring in any form of food, ranging from the main course to desserts. In the United States, potlucks are associated with crockpot dishes, casseroles (often called hot dishes in the upper Midwest), dessert bars, and jello salads. Traditionally, potlucks were a simple combination of dishes brought together by event attendees without a general theme. However, recent times have seen the growth of themed dinners for parties or special occasions.		
Driving is the controlled operation and movement of a motorized vehicle with wheels, such as a car, motorcycle, truck, or bus by either a human or computer controller.						The origin of the term driver, as recorded from the 15th century, refers to the occupation of driving working animals, especially pack horses or draft horses. The verb ' to drive ' in origin means "to force to move, to impel by physical force". It is first recorded of electric railway drivers in 1889 and of a motor-car driver in 1896. Early alternatives were motorneer,[1] motor-man, motor-driver or motorist. French favors "conducteur" (the English equivalent, "conductor", being used —from the 1830s— not of the driver but of the person in charge of passengers and collecting fares), while German influenced areas adopted Fahrer (used of coach-drivers in the 18th century, but shortened about 1900 from the compound Kraftwagenfahrer), and the verbs führen, lenken, steuern —all with a meaning "steer, guide, navigate"— translating to conduire.		The world's first long distance road trip by automobile[2] took place in Germany in August 1888 when Bertha Benz, the wife of Karl Benz, the inventor of the first patented motor car (the Benz Patent-Motorwagen), travelled from Mannheim to Pforzheim[3] (a distance of 106 km or 66 miles)[4] and back in the third experimental Benz motor car, which had a maximum speed of 10 mph or 16 km/h, with her two teenage sons Richard and Eugen but without the consent and knowledge of her husband. Her official reason was that she wanted to visit her mother, but unofficially she intended to generate publicity for her husband's invention, which had only been taken on short test drives before. The automobile took off greatly afterwards and the Benz's family business eventually evolved into the present day Mercedes-Benz company.[5]		In 1899, F. O. Stanley and his wife, Flora, drove their Stanley Steamer automobile, sometimes called a locomobile, to the summit of Mount Washington in New Hampshire in the United States to generate publicity for their automobile.[6] The 7.6-mile (12.2 km) journey took over two hours (not counting time to add more water); the descent was accomplished by putting the engine in low gear and doing lots of braking.[6]		In certain countries, especially the Scandinavian countries, driving and its equivalent word in the respective language has become a colloquialism for action in a general sense. For instance, the phrase "Let's drive" means roughly the same thing as the more common "Let's go" but more often pertaining to engaging in a task rather than departing or travelling somewhere.		Driving in traffic is more than just knowing how to operate the mechanisms which control the vehicle; it requires knowing how to apply the rules of the road (which ensures safe and efficient sharing with other users). An effective driver also has an intuitive understanding of the basics of vehicle handling and can drive responsibly. [7]		Although direct operation of a bicycle and a mounted animal are commonly referred to as riding, such operators are legally considered drivers and are required to obey the rules of the road. Driving over a long distance is referred to as a road trip.		A driver must have physical skills to be able to control direction, acceleration, and deceleration. For motor vehicles, the detailed tasks include: [8]		Avoiding or successfully handling an emergency driving situation can involve the following skills:[9]		Distractions can compromise a driver's mental skills. One study on the subject of mobile phones and driving safety concluded that, after controlling for driving difficulty and time on task, drivers talking on a phone exhibited greater impairment than drivers who were suffering from alcohol intoxication.[10]		Another survey indicated that music could adversely affect a driver's concentration.[11]		Seizure disorders and Alzheimer's disease are among the leading medical causes of mental impairment among drivers in the United States and Europe.[12] Whether or not physicians should be allowed, or even required, to report such conditions to state authorities, remains highly controversial.[12]		Driveability of a vehicle means the smooth delivery of power, as demanded by the driver. Typical causes of driveability degradation are rough idling, misfiring, surging, hesitation, or insufficient power.[13]		A driver is subject to the laws of the jurisdiction in which he or she is driving. The rules of the road, driver licensing and vehicle registration schemes vary considerably between jurisdictions, as do laws imposing criminal responsibility for negligent driving, vehicle safety inspections and compulsory insurance. Most countries also have differing laws against driving while under the influence of alcohol or other drugs. Aggressive driving and road rage have become problems for drivers in some areas.		Some countries require a vision screening test for individuals to acquire or renew a driver's license. A 2010 systematic review[14] found insufficient evidence to assess the effects of vision screening tests on subsequent motor vehicle crash reduction. The review concluded that there is a need to develop valid and reliable tools of vision screening that can predict driving performance.		Motorists are almost universally required to take lessons with an approved instructor and to pass a driving test before being granted a license. Almost all countries allow all adults with good vision and health to apply to take a driving test and, if successful, to drive on public roads. Saudi Arabia, however, bans women from driving vehicles (whether pedal or motor powered) on public roads.[citation needed] Saudi women have periodically staged driving protests against these restrictions.		In many countries, even after passing one's driving test, new motorists are initially subject to special restrictions. For example, in Australia, novice drivers are required to carry "P" ("provisional") plates, in new Zealand its called restricted (r) and are subject to alcohol limits, and other restrictions, for their first two years of driving. Many U.S. states now issue graduated drivers' licenses to novice minors. Typically, newly licensed minors may not drive or operate a motorized vehicle at night or with a passenger other than family members. The duration of the restriction varies from six months to until the driver is 18 years old. This is due to the mental aptitude of a young or unexperienced driver not being fully developed.[15]		
Clothing (also known as clothes and attire) is fiber and textile material worn on the body. The wearing of clothing is mostly restricted to human beings and is a feature of nearly all human societies. The amount and type of clothing worn depends on body type, social, and geographic considerations. Some clothing can be gender-specific.		Physically, clothing serves many purposes: it can serve as protection from the elements, and can enhance safety during hazardous activities such as hiking and cooking. It protects the wearer from rough surfaces, rash-causing plants, insect bites, splinters, thorns and prickles by providing a barrier between the skin and the environment. Clothes can insulate against cold or hot conditions. Further, they can provide a hygienic barrier, keeping infectious and toxic materials away from the body. Clothing also provides protection from ultraviolet radiation. Wearing clothes is also a social norm, as being deprived of clothing in front of others may be embarrassing, or not wearing clothes in public to the extent that genitals, breasts or buttocks are visible could be seen as indecent exposure.						There is no easy way to determine when clothing was first developed, but some information has been inferred by studying lice. The body louse specifically lives in clothing, and diverged from head lice about 170,000 years ago, suggesting that clothing existed at that time.[1][2][3] Another theory is that modern humans are the only survivors of several species of primates who may have worn clothes[4] and that clothing may have been used as long ago as 650 millennia. Other louse-based estimates put the introduction of clothing at around 42,000–72,000 B.P.[5]		The most obvious function of clothing is to improve the comfort of the wearer, by protecting the wearer from the elements. In hot climates, clothing provides protection from sunburn or wind damage, while in cold climates its thermal insulation properties are generally more important. Shelter usually reduces the functional need for clothing. For example, coats, hats, gloves, and other superficial layers are normally removed when entering a warm home, particularly if one is residing or sleeping there. Similarly, clothing has seasonal and regional aspects, so that thinner materials and fewer layers of clothing are generally worn in warmer seasons and regions than in colder ones.		Clothing performs a range of social and cultural functions, such as individual, occupational and gender differentiation, and social status.[6] In many societies, norms about clothing reflect standards of modesty, religion, gender, and social status. Clothing may also function as a form of adornment and an expression of personal taste or style.		Clothing can and has in history been made from a very wide variety of materials. Materials have ranged from leather and furs, to woven materials, to elaborate and exotic natural and synthetic fabrics. Not all body coverings are regarded as clothing. Articles carried rather than worn (such as purses), worn on a single part of the body and easily removed (scarves), worn purely for adornment (jewelry), or those that serve a function other than protection (eyeglasses), are normally considered accessories rather than clothing, as are footwear and hats.		Clothing protects against many things that might injure the uncovered human body. Clothes protect people from the elements, including rain, snow, wind, and other weather, as well as from the sun. However, clothing that is too sheer, thin, small, tight, etc., offers less protection. Clothes also reduce risk during activities such as work or sport. Some clothing protects from specific environmental hazards, such as insects, noxious chemicals, weather, weapons, and contact with abrasive substances. Conversely, clothing may protect the environment from the clothing wearer, as with doctors wearing medical scrubs.		Humans have shown extreme invention in devising clothing solutions to environmental hazards. Examples include: space suits, air conditioned clothing, armor, diving suits, swimsuits, bee-keeper gear, motorcycle leathers, high-visibility clothing, and other pieces of protective clothing. Meanwhile, the distinction between clothing and protective equipment is not always clear-cut—since clothes designed to be fashionable often have protective value and clothes designed for function often consider fashion in their design. Wearing clothes also has social implications. They cover parts of the body that social norms require to be covered, act as a form of adornment, and serve other social purposes.		Although dissertations on clothing and its function appear from the 19th century as colonising countries dealt with new environments,[7] concerted scientific research into psycho-social, physiological and other functions of clothing (e.g. protective, cartage) occurred in the first half of the 20th century, with publications such as J. C. Flügel's Psychology of Clothes in 1930,[6] and Newburgh's seminal Physiology of Heat Regulation and The Science of Clothing in 1949.[8] By 1968, the field of environmental physiology had advanced and expanded significantly, but the science of clothing in relation to environmental physiology had changed little.[9] While considerable research has since occurred and the knowledge-base has grown significantly, the main concepts remain unchanged, and indeed Newburgh's book is still cited by contemporary authors, including those attempting to develop thermoregulatory models of clothing development.[10]		In most cultures, gender differentiation of clothing is considered appropriate. The differences are in styles, colors and fabrics.		In Western societies, skirts, dresses and high-heeled shoes are usually seen as women's clothing, while neckties are usually seen as men's clothing. Trousers were once seen as exclusively male clothing, but can nowadays be worn by both genders. Male clothes are often more practical (that is, they can function well under a wide variety of situations), but a wider range of clothing styles are available for females. Males are typically allowed to bare their chests in a greater variety of public places. It is generally more or less acceptable for a woman to wear clothing perceived as masculine, while the opposite is seen as unusual.		In some cultures, sumptuary laws regulate what men and women are required to wear. Islam requires women to wear more modest forms of attire, usually hijab. What qualifies as "modest" varies in different Muslim societies. However, women are usually required to cover more of their bodies than men are. Articles of clothing Muslim women wear for modesty range from the head-scarf to the burqa.		Men may sometimes choose to wear men's skirts such as togas or kilts, especially on ceremonial occasions. Such garments were (in previous times) often worn as normal daily clothing by men.		Clothing designed to be worn by either sex is called unisex clothing. Unisex clothes, such as T-shirts, tends to be cut straighter to fit a wider variety of bodies. The majority of unisex clothing styles have started out as menswear, but some articles, like the fedora, were originally worn by women.		In some societies, clothing may be used to indicate rank or status. In ancient Rome, for example, only senators could wear garments dyed with Tyrian purple. In traditional Hawaiian society, only high-ranking chiefs could wear feather cloaks and palaoa, or carved whale teeth. In China, before establishment of the republic, only the emperor could wear yellow. History provides many examples of elaborate sumptuary laws that regulated what people could wear. In societies without such laws, which includes most modern societies, social status is instead signaled by the purchase of rare or luxury items that are limited by cost to those with wealth or status. In addition, peer pressure influences clothing choice.		Religious clothing might be considered a special case of occupational clothing. Sometimes it is worn only during the performance of religious ceremonies. However, it may also be worn everyday as a marker for special religious status.		For example, Jains and Muslim men wear unstitched cloth pieces when performing religious ceremonies. The unstitched cloth signifies unified and complete devotion to the task at hand, with no digression.[citation needed] Sikhs wear a turban as it is a part of their religion.		The cleanliness of religious dresses in Eastern religions like Hinduism, Sikhism, Buddhism, Islam and Jainism is of paramount importance, since it indicates purity.		Clothing figures prominently in the Bible where it appears in numerous contexts, the more prominent ones being: the story of Adam and Eve who made coverings for themselves out of fig leaves, Joseph's cloak, Judah and Tamar, Mordecai and Esther. Furthermore, the priests officiating in the Temple had very specific garments, the lack of which made one liable to death.		The Quran says about husbands and wives, regarding clothing: "...They are clothing/covering (Libaas) for you; and you for them" (chapter 2:187).		Jewish ritual also requires rending of one's upper garment as a sign of mourning. This practice is found in the Bible when Jacob hears of the apparent death of his son Joseph.[11]		According to archaeologists and anthropologists, the earliest clothing likely consisted of fur, leather, leaves, or grass that were draped, wrapped, or tied around the body. Knowledge of such clothing remains inferential, since clothing materials deteriorate quickly compared to stone, bone, shell and metal artifacts. Archeologists have identified very early sewing needles of bone and ivory from about 30,000 BC, found near Kostenki, Russia in 1988.[12] Dyed flax fibers that could have been used in clothing have been found in a prehistoric cave in the Republic of Georgia that date back to 36,000 BP.[13][14]		Scientists are still debating when people started wearing clothes. Ralf Kittler, Manfred Kayser and Mark Stoneking, anthropologists at the Max Planck Institute for Evolutionary Anthropology, have conducted a genetic analysis of human body lice that suggests clothing originated quite recently, around 170,000 years ago. Body lice is an indicator of clothes-wearing, since most humans have sparse body hair, and lice thus require human clothing to survive. Their research suggests the invention of clothing may have coincided with the northward migration of modern Homo sapiens away from the warm climate of Africa, thought to have begun between 50,000 and 100,000 years ago. However, a second group of researchers using similar genetic methods estimate that clothing originated around 540,000 years ago [15] For now, the date of the origin of clothing remains unresolved.		Some human cultures, such as the various people of the Arctic Circle, traditionally make their clothing entirely of prepared and decorated furs and skins. Other cultures supplemented or replaced leather and skins with cloth: woven, knitted, or twined from various animal and vegetable fibers including wool, linen, cotton, silk, hemp, and ramie.		Although modern consumers may take the production of clothing for granted, making fabric by hand is a tedious and labor-intensive process involving fiber making, spinning, and weaving. The textile industry was the first to be mechanized – with the powered loom – during the Industrial Revolution.		Different cultures have evolved various ways of creating clothes out of cloth. One approach simply involves draping the cloth. Many people wore, and still wear, garments consisting of rectangles of cloth wrapped to fit – for example, the dhoti for men and the sari for women in the Indian subcontinent, the Scottish kilt or the Javanese sarong. The clothes may simply be tied up, as is the case of the first two garments; or pins or belts hold the garments in place, as in the case of the latter two. The precious cloth remains uncut, and people of various sizes or the same person at different sizes can wear the garment.		Another approach involves measuring, cutting, and sewing the cloth by hand or with a sewing machine. Clothing can be cut from a sewing pattern and adjusted by a tailor to a persons measurements. An adjustable sewing mannequin or dress form is used to create form fitting clothing. Fabrics are expensive and efforts are made to use every bit of the cloth rectangle in constructing the clothing. The tailor may cut triangular pieces from one corner of the cloth, and then add them elsewhere as gussets. Traditional European patterns for men's shirts and women's chemises take this approach. These remnant can also be rused to make patchwork hats, vests, and skirts.		Modern European fashion treats cloth much less conservatively, typically cutting in such a way as to leave various odd-shaped cloth remnants. Industrial sewing operations sell these as waste; home sewers may turn them into quilts.		In the thousands of years that humans have spent constructing clothing, they have created an astonishing array of styles, many of which have been reconstructed from surviving garments, photos, paintings, mosaics, etc., as well as from written descriptions. Costume history serves as a source of inspiration to current fashion designers, as well as a topic of professional interest to costumers constructing for plays, films, television, and historical reenactment.		The Western dress code has changed over the past 500+ years. The mechanization of the textile industry made many varieties of cloth widely available at affordable prices. Styles have changed, and the availability of synthetic fabrics has changed the definition of "stylish". In the latter half of the 20th century, blue jeans became very popular, and are now worn to events that normally demand formal attire. Activewear has also become a large and growing market.		The licensing of designer names was pioneered by designers like Pierre Cardin in the 1960s and has been a common practice within the fashion industry from about the 1970s. Among the more popular include Marc Jacobs and Gucci, named for Marc Jacobs and Guccio Gucci respectively.		By the early years of the 21st century, western clothing styles had, to some extent, become international styles. This process began hundreds of years earlier, during the periods of European colonialism. The process of cultural dissemination has perpetuated over the centuries as Western media corporations have penetrated markets throughout the world, spreading Western culture and styles. Fast fashion clothing has also become a global phenomenon. These garments are less expensive, mass-produced Western clothing. Donated used clothing from Western countries are also delivered to people in poor countries by charity organizations.		People may wear ethnic or national dress on special occasions or in certain roles or occupations. For example, most Korean men and women have adopted Western-style dress for daily wear, but still wear traditional hanboks on special occasions, like weddings and cultural holidays. Items of Western dress may also appear worn or accessorized in distinctive, non-Western ways. A Tongan man may combine a used T-shirt with a Tongan wrapped skirt, or tupenu.		Most sports and physical activities are practiced wearing special clothing, for practical, comfort or safety reasons. Common sportswear garments include shorts, T-shirts, tennis shirts, leotards, tracksuits, and trainers. Specialized garments include wet suits (for swimming, diving or surfing), salopettes (for skiing) and leotards (for gymnastics). Also, spandex materials are often used as base layers to soak up sweat. Spandex is also preferable for active sports that require form fitting garments, such as volleyball, wrestling, track & field, dance, gymnastics and swimming.		There exists a diverse range of styles[weasel words] in fashion, varying by geography, exposure to modern media, economic conditions, and ranging from expensive haute couture to traditional garb, to thrift store grunge.		The world of clothing is always changing, as new cultural influences meet technological innovations. Researchers in scientific labs have been developing prototypes for fabrics that can serve functional purposes well beyond their traditional roles, for example, clothes that can automatically adjust their temperature, repel bullets, project images, and generate electricity. Some practical advances already available to consumers are bullet-resistant garments made with kevlar and stain-resistant fabrics that are coated with chemical mixtures that reduce the absorption of liquids. New blends of Spandex cotton blends allow for form fitting and stretching of closer fitting mass produced patterns. New mesh materials allow for better breathe-ability in shoes. New insulation fibers and batting make lighter rainments that keep you warm when wet and recent advances in coatings for fabrics or down also repel water.		Though mechanization transformed most aspects of human industry by the mid-20th century, garment workers have continued to labor under challenging conditions that demand repetitive manual labor. Mass-produced clothing is often made in what are considered by some to be sweatshops, typified by long work hours, lack of benefits, and lack of worker representation. While most examples of such conditions are found in developing countries, clothes made in industrialized nations may also be manufactured similarly.[citation needed]		Coalitions of NGOs, designers (including Katharine Hamnett, American Apparel, Veja, Quiksilver, eVocal, and Edun) and campaign groups like the Clean Clothes Campaign (CCC) and the Institute for Global Labour and Human Rights as well as textile and clothing trade unions have sought to improve these conditions as much as possible by sponsoring awareness-raising events, which draw the attention of both the media and the general public to the workers.		Outsourcing production to low wage countries like Bangladesh, China, India and Sri Lanka became possible when the Multi Fibre Agreement (MFA) was abolished. The MFA, which placed quotas on textiles imports, was deemed a protectionist measure.[citation needed] Although many countries recognize treaties like the International Labour Organization, which attempt to set standards for worker safety and rights, many countries have made exceptions to certain parts of the treaties or failed to thoroughly enforce them. India for example has not ratified sections 87 and 92 of the treaty.[citation needed]		Despite the strong reactions that "sweatshops" evoked among critics of globalization, the production of textiles has functioned as a consistent industry for developing nations providing work and wages, whether construed as exploitative or not, to many thousands of people.		The use of animal fur in clothing dates to prehistoric times. It is currently associated in developed countries with expensive, designer clothing, although fur is still used by indigenous people in arctic zones and higher elevations for its warmth and protection. Once uncontroversial, it has recently been the focus of campaigns on the grounds that campaigners consider it cruel and unnecessary. PETA, along with other animal rights and animal liberation groups have called attention to fur farming and other practices they consider cruel.		Clothing suffers assault both from within and without. The human body sheds skin cells and body oils, and exudes sweat, urine, and feces. From the outside, sun damage, moisture, abrasion and dirt assault garments. Fleas and lice can hide in seams. Worn clothing, if not cleaned and refurbished, itches, looks scruffy, and loses functionality (as when buttons fall off, seams come undone, fabrics thin or tear, and zippers fail).		In some cases, people wear an item of clothing until it falls apart. Cleaning leather presents difficulties, and bark cloth (tapa) cannot be washed without dissolving it. Owners may patch tears and rips, and brush off surface dirt, but old leather and bark clothing always look old.		But most clothing consists of cloth, and most cloth can be laundered and mended (patching, darning, but compare felt).		Humans have developed many specialized methods for laundering, ranging from early methods of pounding clothes against rocks in running streams, to the latest in electronic washing machines and dry cleaning (dissolving dirt in solvents other than water). Hot water washing (boiling), chemical cleaning and ironing are all traditional methods of sterilizing fabrics for hygiene purposes.		Many kinds of clothing are designed to be ironed before they are worn to remove wrinkles. Most modern formal and semi-formal clothing is in this category (for example, dress shirts and suits). Ironed clothes are believed to look clean, fresh, and neat. Much contemporary casual clothing is made of knit materials that do not readily wrinkle, and do not require ironing. Some clothing is permanent press, having been treated with a coating (such as polytetrafluoroethylene) that suppresses wrinkles and creates a smooth appearance without ironing.		Once clothes have been laundered and possibly ironed, they are usually hung on clothes hangers or folded, to keep them fresh until they are worn. Clothes are folded to allow them to be stored compactly, to prevent creasing, to preserve creases or to present them in a more pleasing manner, for instance when they are put on sale in stores.		A resin used for making non-wrinkle shirts releases formaldehyde, which could cause contact dermatitis for some people; no disclosure requirements exist, and in 2008 the U.S. Government Accountability Office tested formaldehyde in clothing and found that generally the highest levels were in non-wrinkle shirts and pants.[16] In 1999, a study of the effect of washing on the formaldehyde levels found that after 6 months after washing, 7 of 27 shirts had levels in excess of 75 ppm, which is a safe limit for direct skin exposure.[17]		In past times, mending was an art. A meticulous tailor or seamstress could mend rips with thread raveled from hems and seam edges so skillfully that the tear was practically invisible. When the raw material – cloth – was worth more than labor, it made sense to expend labor in saving it. Today clothing is considered a consumable item. Mass-manufactured clothing is less expensive than the labor required to repair it. Many people buy a new piece of clothing rather than spend time mending. The thrifty still replace zippers and buttons and sew up ripped hems.		Used, unwearable clothing can be repurposed for quilts, rags, rugs, bandages, and many other household uses. It can also be recycled into paper. In Western societies, used clothing is often thrown out or donated to charity (such as through a clothing bin). It is also sold to consignment shops, dress agencies, flea markets, and in online auctions. Used clothing is also often collected on an industrial scale to be sorted and shipped for re-use in poorer countries.		There are many concerns about the life cycle of synthetics, which come primarily from petrochemicals.[weasel words] Unlike natural fibers, their source is not renewable and they are not biodegradable.[18]		
Dining in is a formal military ceremony for members of a company or other unit, which includes a dinner, drinking, and other events to foster camaraderie and esprit de corps.		The United States Army, the United States Navy, the United States Coast Guard, and the United States Air Force refer to this event as a dining in or dining-in. The United States Marine Corps refers to it as mess night. Other names include regimental dinner, guest night, formal mess dinner, and band night.[1]		The dining in is a formal event for all unit members, male and female; though some specialized mess nights can be officer- or enlisted-only. The unit chaplain is usually also invited, if an invocation is needed. A unit's dining-in consists of only the members of the unit, with the possible exception of the guest(s) of honor. An optional formal dinner, known as the dining-out may include spouses and other guests. The dining-out follows the same basic rules of the dining-in, but is often tailored to minimize some of the military traditions and be more interesting to civilian guests.		Except for the annual celebration of the Marine Corps Birthday, no social function associated with the smaller of America's naval services is more enjoyed, admired and imitated than the mess night.						The practice of dining in is thought to have formally begun in 16th-century England, in monasteries and universities; though some records indicate that militaries have held formal dinners as far back as the Roman Legions. The Vikings held formal ceremonies to honor and celebrate battles and heroes.[3] During the 18th century, the British Army incorporated the practice of formal dining into their regimental mess system. Customs and rules of the mess were soon institutionalized rules, known as the "Queen's Regulations". The mess night or "Dining in" became a tradition in all British regiments. The Americans, taking many of their traditions from the British military, held mess nights in the 18th and 19th century, but the tradition waned after the Civil War.		Dining in took a temporary halt in the Navy and Marine Corps when Navy Secretary Josephus Daniels imposed prohibition of alcoholic drink, but soon the tradition was restored. During World War II, the custom was revived in the U.S. military, initially in the US Army Air Forces 8th Air Force, which was based in Britain. AAFOfficers were invited to participate in British military hosted Mess Nights and then were obligated to reciprocate.		A Formal function is one at which all mess members may be required to attend and Service Personnel are on official duty.[4]		The entitlements for official functions are as follows:		12 Formal Functions per annum. Generally these are 2 x seasonal balls and 10 x other functions (e.g. Mess Dinners) as agreed by mess committees.[4]		6 Formal Functions per annum. Generally these are 2 x seasonal balls and 4 x other functions (e.g. Mess Dinners) as agreed by mess committees.[4]		The port and madeira decanters are to be placed at pre-determined places and in front of the President and Vice President. The President and Vice President will remove the stoppers simultaneously and pass the decanters in a clockwise direction/to the left. Wine stewards are to follow the decanters round the table with a jug of water, filling the glass of any diner who declines port or madeira. The glasses of the President and Vice President will be filled last, after which they are to re-stop the decanters. All stewards may then be required to leave the dining room before the President calls upon the Vice President to propose the Loyal Toast. It should be remembered that whilst the Army and RAF stand for the Loyal Toast, the RN remain seated. Also, whilst passing the port, the RN would ensure the decanter does not leave the table whilst in the RAF; it is passed from hand to hand without touching the table. In the Army, it will depend on the traditions of the Regiment.[4]		By the late 18th century, the British Army's "mess night" developed formal rules, as a result of troops being stationed in remote areas. Officers elected mess committees to conduct their meals. Towards the latter part of the 19th century, attending officers were expected to adhere to the rigid etiquette of Victorian-era society.		In modern times it is sometimes known as a "regimental dinner".		Royal Navy officers have the privilege of remaining seated when toasting the Sovereign. Some sources state that this privilege was granted by William IV. A popular story states that Charles II was on board his namesake ship Royal Charles and had bumped his head on the low overhead of the wardroom when he stood up to reply a toast that had been drunk to him. He stated that henceforth, naval officers would never again rise to toast a British sovereign. In 1964, Queen Elizabeth II extended the privilege to the Royal Marines in honour of their 300th anniversary.[5]		The custom of "dining in" to welcome new officers and "dining out" to farewell retiring officers originated with the British, although the term came about much later. It has been discontinued in some countries such as the United States but is still practised by the Royal Navy.[6]		The traditional toasts after dinner for ships at sea are shown below. On certain days, an alternative toast is available but the first one is most usual.[4]		Sunday ‘Absent friends’ or ‘Absent friends and those at sea’		Monday ‘Our ship at sea’ or ‘Our native land’		Tuesday ‘Our Sailors’		Wednesday ‘Ourselves’ (as no one is likely to concern themselves with our welfare)’ or ‘Ourselves – Our swords’		Thursday ‘A bloody war or a sickly season’		Friday ‘A willing foe and sea room’ ‘Fox hunting and old port’		Saturday ‘Our Families’		For Trafalgar Night Mess Dinners, the routine varies slightly from a normal dinner night. When the main course is about to be served, the Baron of Beef is first paraded around the table behind a drummer. Similarly, before commencing the service of the sweet, the Ships of the Line are also paraded around the table in a similar fashion to the Beef. The toasts used at dinner on Trafalgar Night are:[4]		The Royal Air Force inherited many mess traditions from the British Army as a former corps. These customs were notably passed on the US Army Air Forces during World War II as British and American crew served alongside one another in close quarters.[7]		Portions of the event tend to become quite humorous in nature, while others remain somber. Etiquette requires a diner to know what is appropriate at any given time.		The dining in follows established protocols. After a brief cocktail period of 30 to 45 minutes, the presiding officer, known as the "President of the Mess", announces, "Please be seated." The group will then retire to the dining area to be seated.		After tasting the meat (usually beef), the President will declare it "tasty and fit for human consumption", after which the meal will be served to the diners. After the dessert is finished, the President will invite the chief steward to bring forth wine and/or punch to be served, and toasting will begin. After the toasts have concluded, the floor will be opened to the levying of fines. The president and the guest of honor will have the opportunity to speak if they so desire. After this, the mess is often then returned to an open cocktail hour, and then the evening concludes with final honors.		Formal toasts are the heart of the formal dining in. A junior officer, known as "Mr Vice", proposes a toast to the guests, at which the guests remain seated. After this, various parties will offer toasts to the Commander in Chief, to the heads of state of a visiting or host nations, to their branch of service, to the units, and to the fallen members of the military.		Notice that the United States does not have a king, queen, or Prime Minister, and that the commander in chief and President are the same person.		The final and most solemn toast is always to fallen comrades. Often this tribute is marked with a table setting dedicated to those military members killed, captured, or missing in action.[8]		Some unusual forms of toasting are common to the U.S. and Canadian traditions. In the Toronto Scottish Regiment, for example, a loyal toast to the regiment's colonel-in-chief is performed standing with one foot on the chair and one foot on the dining table, facing a portrait of the C-in-C and drinking after the piper has played.[9] Others, particularly Scottish regiments,[10] perform toasts in the same way with one foot on a chair and one on the table.[11] This is a frequent form of toasting in the United States Marine Corps as well.[11] In the Scottish form, the glass is raised, lowered, brought out, and brought in, as the words of the toast, usually including some form of "Up", "Down", "To you", "To me", are recited, and finally drunk to the cry "Drink it up!" or similar.[10][11]		Violations of the formal etiquette of the dining in are "punished", generally with fines. The following are examples of what could be considered "Violations of the Mess":		At some mess nights, violators of the mess are obliged to publicly drink from a grog bowl in front of the mess attendees. The grog is sometimes contained in a toilet bowl, consisting of various alcoholic beverages mixed together. As a more disgusting effect, the grog may also contain floating solids, such as meatballs, raw oysters, or Tootsie Rolls. The tradition of drinking grog originated with the British Navy. Grog consisted of the regulation rum ration diluted with water to discourage binge drinking. In modern times, grog comes in two varieties: alcoholic and non-alcoholic, the latter of which may contain anything that will make it less appealing to the taste, including hot sauce. For additional effect, the drinker may be required to drink from a boot.		In addition to visiting the grog bowl and paying fines, violators may be sentenced to sing songs, tell jokes, do pushups, or perform menial tasks to entertain the mess. In most cases, when a violator has been identified, he or she is given the opportunity to provide a rebuttal or defense for the violation, which rarely results in the violator being excused for the offense, and usually only results in more punishment.		Traditionally, all fines collected throughout the night are split amongst the stewards that served the attendees as a token of appreciation for their efforts. The fines can also be used to pay for the drinks consumed, while some units have used the Mess Night as a fund raiser (often to pay for a ball).		Members of the mess may also be singled out for some good-natured ribbing and teasing. In some units, members go out of their way to be picked on, often wearing obvious uniform violations, such as crowns, tiaras, eye-patches, bowties and cummerbunds of the wrong color, and other items that have no place on any military uniform (although it is common for US Artillerymen to wear red socks, suspenders and even bowties, in a nod to tradition at the expense of uniform regulations). Some will attempt to leave sabotaging evidence on or around others they wish to see fined, so care must be taken to not be the butt of a joke.		Navy and Marine traditions also include that no diner may leave the hall to use the restroom without permission until Mr. Vice suggests that the company "shed a tear for Lord Admiral Nelson", a reference to the fact that his body was preserved in a barrel of brandy after his death at Trafalgar.		Most Messes attempt to furnish the night with military music and marches, with live bands if possible, or recorded music. Depending on how formal the ceremony is, the diners may be required to march to their seats.		In recent times, Marines have established a variant of the mess in a "field environment",[14] substituting in mess dress for utilities and combat equipment (to include camouflage facepaint), canteen cups, and tentage, while still retaining the formal nature of the ceremony.		
In biology, an organism (from Greek: οργανισμός, organismos) is any individual life form, of an animal, plant, fungus, or single-celled microorganism such as a protist, bacterium, and archaeon.[1] All types of organisms are capable of reproduction, growth and development, maintenance, and some degree of response to stimuli. An organism consists of one or more cells; when it has one cell it is known as a unicellular organism; and when it has more than one it is known as a multicellular organism. Humans are multicellular organisms composed of many trillions of cells grouped into specialized tissues and organs.		An organism may be either a prokaryote or a eukaryote. Prokaryotes are represented by two separate domains—bacteria and archaea. Eukaryotic organisms are characterized by the presence of a membrane-bound cell nucleus and contain additional membrane-bound compartments called organelles (such as mitochondria in animals and plants and plastids in plants and algae, all generally considered to be derived from endosymbiotic bacteria).[2] Fungi, animals and plants are examples of kingdoms of organisms within the eukaryotes.		Estimates on the number of Earth's current species range from 10 million to 14 million,[3] of which only about 1.2 million have been documented.[4] More than 99% of all species, amounting to over five billion species,[5] that ever lived are estimated to be extinct.[6][7] In 2016, a set of 355 genes from the last universal common ancestor (LUCA) of all living organisms living was identified.[8][9]						The term "organism" (from Greek ὀργανισμός, organismos, from ὄργανον, organon, i.e. "instrument, implement, tool, organ of sense or apprehension"[10][11]) first appeared in the English language in 1703 and took on its current definition by 1834 (Oxford English Dictionary). It is directly related to the term "organization". There is a long tradition of defining organisms as self-organizing beings, going back at least to Immanuel Kant's 1790 Critique of Judgment.[12]		An organism may be defined as an assembly of molecules functioning as a more or less stable whole that exhibits the properties of life. Dictionary definitions can be broad, using phrases such as "any living structure, such as a plant, animal, fungus or bacterium, capable of growth and reproduction".[13] Many definitions exclude viruses and possible man-made non-organic life forms, as viruses are dependent on the biochemical machinery of a host cell for reproduction.[14] A superorganism is an organism consisting of many individuals working together as a single functional or social unit.[15]		There has been controversy about the best way to define the organism[16][17][18][19][20][21][22][23][24] and indeed about whether or not such a definition is necessary.[25][26] Several contributions[27] are responses to the suggestion that the category of "organism" may well not be adequate in biology.[28][page needed]		Viruses are not typically considered to be organisms because they are incapable of autonomous reproduction, growth or metabolism. This controversy is problematic because some cellular organisms are also incapable of independent survival (but are capable of independent metabolism and procreation) and live as obligatory intracellular parasites. Although viruses have a few enzymes and molecules characteristic of living organisms, they have no metabolism of their own; they cannot synthesize and organize the organic compounds from which they are formed. Naturally, this rules out autonomous reproduction: they can only be passively replicated by the machinery of the host cell. In this sense, they are similar to inanimate matter. While viruses sustain no independent metabolism, and thus are usually not classified as organisms, they do have their own genes, and they do evolve by mechanisms similar to the evolutionary mechanisms of organisms.		The most common argument in support of viruses as living organisms is their ability to undergo evolution and replicate through self-assembly. Some scientists argue that viruses neither evolve, nor self- reproduce. In fact, viruses are evolved by their host cells, meaning that there was co-evolution of viruses and host cells. If host cells did not exist, viral evolution would be impossible. This is not true for cells. If viruses did not exist, the direction of cellular evolution could be different, but cells would nevertheless be able to evolve. As for the reproduction, viruses totally rely on hosts' machinery to replicate.[29] The discovery of viral megagenomes with genes coding for energy metabolism and protein synthesis fueled the debate about whether viruses belong in the tree of life. The presence of these genes suggested that viruses were once able to metabolize. However, it was found later that the genes coding for energy and protein metabolism have a cellular origin. Most likely, these genes were acquired through horizontal gene transfer from viral hosts.[29]		Organisms are complex chemical systems, organized in ways that promote reproduction and some measure of sustainability or survival. The same laws that govern non-living chemistry govern the chemical processes of life. It is generally the phenomena of entire organisms that determine their fitness to an environment and therefore the survivability of their DNA-based genes.		Organisms clearly owe their origin, metabolism, and many other internal functions to chemical phenomena, especially the chemistry of large organic molecules. Organisms are complex systems of chemical compounds that, through interaction and environment, play a wide variety of roles.		Organisms are semi-closed chemical systems. Although they are individual units of life (as the definition requires), they are not closed to the environment around them. To operate they constantly take in and release energy. Autotrophs produce usable energy (in the form of organic compounds) using light from the sun or inorganic compounds while heterotrophs take in organic compounds from the environment.		The primary chemical element in these compounds is carbon. The chemical properties of this element such as its great affinity for bonding with other small atoms, including other carbon atoms, and its small size making it capable of forming multiple bonds, make it ideal as the basis of organic life. It is able to form small three-atom compounds (such as carbon dioxide), as well as large chains of many thousands of atoms that can store data (nucleic acids), hold cells together, and transmit information (protein).		Compounds that make up organisms may be divided into macromolecules and other, smaller molecules. The four groups of macromolecule are nucleic acids, proteins, carbohydrates and lipids. Nucleic acids (specifically deoxyribonucleic acid, or DNA) store genetic data as a sequence of nucleotides. The particular sequence of the four different types of nucleotides (adenine, cytosine, guanine, and thymine) dictate many characteristics that constitute the organism. The sequence is divided up into codons, each of which is a particular sequence of three nucleotides and corresponds to a particular amino acid. Thus a sequence of DNA codes for a particular protein that, due to the chemical properties of the amino acids it is made from, folds in a particular manner and so performs a particular function.		These protein functions have been recognized:		A bilayer of phospholipids makes up the membrane of cells that constitutes a barrier, containing everything within the cell and preventing compounds from freely passing into, and out of, the cell. Due to the selective permeability of the phospholipid membrane only specific compounds can pass through it. In some multicellular organisms they serve as a storage of energy and mediate communication between cells. Carbohydrates are more easily broken down than lipids and yield more energy to compare to lipids and proteins. In fact, carbohydrates are the number one source of energy for all living organisms.		All organisms consist of structural units called cells; some contain a single cell (unicellular) and others contain many units (multicellular). Multicellular organisms are able to specialize cells to perform specific functions. A group of such cells is a tissue, and in animals these occur as four basic types, namely epithelium, nervous tissue, muscle tissue, and connective tissue. Several types of tissue work together in the form of an organ to produce a particular function (such as the pumping of the blood by the heart, or as a barrier to the environment as the skin). This pattern continues to a higher level with several organs functioning as an organ system such as the reproductive system, and digestive system. Many multicellular organisms consist of several organ systems, which coordinate to allow for life.		The cell theory, first developed in 1839 by Schleiden and Schwann, states that all organisms are composed of one or more cells; all cells come from preexisting cells; and cells contain the hereditary information necessary for regulating cell functions and for transmitting information to the next generation of cells.		There are two types of cells, eukaryotic and prokaryotic. Prokaryotic cells are usually singletons, while eukaryotic cells are usually found in multicellular organisms. Prokaryotic cells lack a nuclear membrane so DNA is unbound within the cell; eukaryotic cells have nuclear membranes.		All cells, whether prokaryotic or eukaryotic, have a membrane, which envelops the cell, separates its interior from its environment, regulates what moves in and out, and maintains the electric potential of the cell. Inside the membrane, a salty cytoplasm takes up most of the cell volume. All cells possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells.		All cells share several similar characteristics of:[30]		The last universal common ancestor (LUCA) is the most recent organism from which all organisms now living on Earth descend.[31] Thus it is the most recent common ancestor of all current life on Earth. The LUCA is estimated to have lived some 3.5 to 3.8 billion years ago (sometime in the Paleoarchean era).[32][33] The earliest evidence for life on Earth is graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland[34] and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia.[35][36] Although more than 99 percent of all species that ever lived on the planet are estimated to be extinct,[6][7] there are currently 10–14 million species of life on Earth.[3]		Information about the early development of life includes input from many different fields, including geology and planetary science. These sciences provide information about the history of the Earth and the changes produced by life. However, a great deal of information about the early Earth has been destroyed by geological processes over the course of time.		All organisms are descended from a common ancestor or ancestral gene pool. Evidence for common descent may be found in traits shared between all living organisms. In Darwin's day, the evidence of shared traits was based solely on visible observation of morphologic similarities, such as the fact that all birds have wings, even those that do not fly.		There is strong evidence from genetics that all organisms have a common ancestor. For example, every living cell makes use of nucleic acids as its genetic material, and uses the same twenty amino acids as the building blocks for proteins. All organisms use the same genetic code (with some extremely rare and minor deviations) to translate nucleic acid sequences into proteins. The universality of these traits strongly suggests common ancestry, because the selection of many of these traits seems arbitrary. Horizontal gene transfer makes it more difficult to study the last universal ancestor.[37] However, the universal use of the same genetic code, same nucleotides, and same amino acids makes the existence of such an ancestor overwhelmingly likely.[38]		Sexual reproduction is widespread among current eukaryotes, and was likely present in the last common ancestor.[39] This is suggested by the finding of a core set of genes for meiosis in the descendants of lineages that diverged early form the eukaryotic evolutionary tree.[40] and Malik et al.[41] It is further supported by evidence that eukaryotes previously regarded as "ancient asexuals", such as Amoeba, were likely sexual in the past, and that most present day asexual amoeboid lineages likely arose recently and independently.[42]		In prokaryotes, natural bacterial transformation involves the transfer of DNA from one bacterium to another and integration of the donor DNA into the recipient chromosome by recombination. Natural bacterial transformation is considered to be a primitive sexual process and occurs in both bacteria and archaea, although it has been studied mainly in bacteria. Transformation is clearly a bacterial adaptation and not an accidental occurrence, because it depends on numerous gene products that specifically interact with each other to enter a state of natural competence to perform this complex process.[43] Transformation is a common mode of DNA transfer among prokaryotes.[44]		The ancestry of living organisms has traditionally been reconstructed from morphology, but is increasingly supplemented with phylogenetics—the reconstruction of phylogenies by the comparison of genetic (DNA) sequence.		Sequence comparisons suggest recent horizontal transfer of many genes among diverse species including across the boundaries of phylogenetic "domains". Thus determining the phylogenetic history of a species can not be done conclusively by determining evolutionary trees for single genes.[45]		Biologist Gogarten suggests "the original metaphor of a tree no longer fits the data from recent genome research", therefore "biologists (should) use the metaphor of a mosaic to describe the different histories combined in individual genomes and use (the) metaphor of a net to visualize the rich exchange and cooperative effects of HGT among microbes."[46]		Modern biotechnology is challenging traditional concepts of organism and species. Cloning is the process of creating a new multicellular organism, genetically identical to another, with the potential of creating entirely new species of organisms. Cloning is the subject of much ethical debate.		In 2008, the J. Craig Venter Institute assembled a synthetic bacterial genome, Mycoplasma genitalium, by using recombination in yeast of 25 overlapping DNA fragments in a single step. The use of yeast recombination greatly simplifies the assembly of large DNA molecules from both synthetic and natural fragments.[47] Other companies, such as Synthetic Genomics, have already been formed to take advantage of the many commercial uses of custom designed genomes.		
Bombesin is a 14-amino acid peptide[1] originally isolated from the skin of the European fire-bellied toad (Bombina bombina).[2] It has two known homologs in mammals called neuromedin B and gastrin-releasing peptide. It stimulates gastrin release from G cells. It activates three different G-protein-coupled receptors known as BBR1, -2, and -3.[3] It also activates these receptors in the brain. Together with cholecystokinin, it is the second major source of negative feedback signals that stop eating behaviour.[4]		Bombesin is also a tumor marker for small cell carcinoma of lung, gastric cancer, pancreatic cancer, and neuroblastoma.[5]		
A suspended meal is a meal which people pay for in advance, to be provided to those that request it later. The extra meal that they purchase is suspended; that is, the restaurant will mark down the sum of money and ‘suspend’ the additional meal for the poor. Suspended meals today range from a cup of coffee to a meal set.						The term originates from a tradition that started in Italy, called pending coffee, or in Italian, caffè sospeso ([kafˈfɛ soˈspeːzo]). Similarly, it refers to a cup of coffee that is paid in advance for someone in need. The trend started in cafes of Naples, where people who had experienced good luck or people of middle class would pay for an extra cup of coffee. Then, the poor would come by later and ask if there was any sospeso that could be given for free to them. Such trend has been adopted by other cafes and restaurants that now incorporate the concept of suspended meals.		The value of this rising trend lies greatly on disseminating the attitude of sharing and giving what we owe to others, so as to bridge the gap between the poor and urban people. However, it must be reminded that the phenomenon should not be viewed as a type of social assistance. In other words, there is no labelling effect in the promotion of the campaign; the focus is not placed on the poor, but rather on the act of citizens suspending meals. This way, individuals can both respect the poor, and promote the natural attitude of helping. Through the collaboration of everyone, the poor can be integrated into the community and feel a sense of belonging. “It’s about communities coming together and looking after people directly in their communities.”, says John Sweeney, the founder of a page advocating suspended coffee, in an interview with TruthAtlas.[1]		Suspended meals in Hong Kong are not as popular as they are in foreign countries. However, they are gradually becoming a trend. It started in a restaurant called Siu Mei Restaurant in Sham Shui Po (北河燒蠟飯店).[2] Originally, the boss, Mr. Chan gave out free meals to the poor in Sham Shui Po, an impoverished district. The news of Mr.Chan’s act soon became publicized and non-governmental Organizations started to cooperate with Mr. Chan to implement the suspended meal scheme. Slowly, more and more food stores, including bakery,dessert, and meal shops in Sham Shui Po adopted the system, too.		In 2012, a TV program called “Rich Mate Poor Mate Series” (窮富翁大作戰) featured Simon Wong,[3] the owner of LHGroup)[4] and expertise in catering services. He spoke about his organization starting at the grassroots, and how he has strived to strengthen the social responsibility of his chain restaurants by launching schemes of suspended meals.[5]		In Taiwan, several methods have been implemented to make the public more familiar with the concept of suspended meals. For example, there is a Facebook page, “Suspended meal, Taiwan”,[6] advocating the ideas of suspended meal. The owners of the restaurants can join this page and inform the people that their restaurants are supporting suspended meals. Also, there are photos and descriptions in the page; more details are exposed to the potential consumers of suspended meals like the location of the restaurants and what kind of meals the restaurants are serving. There are also figures indicating the number of meals left in the restaurant.		Despite sharing the same motivation, restaurants and companies have their own operation style in terms of promoting the suspended meal scheme. Because of such difference, some argue that the restaurant owners are trying to make profits by cheating on the scheme rather than helping the needy. Similarly, some people believe that the companies are more likely to cheat on the scheme since they will charge a sum of administrative expenses after holding the fund-raising events like suspended meals.		Some people argue that suspended meals cannot effectively ameliorate the poverty problem in Hong Kong in terms of effective implementation. A big proportion of the poor, in fact, denies to take the suspended meal because of their strong dignity. According to a survey conducted by Children Welfare League, 32% of needy children dare not to take free meal because they do not want to disclose personal information and feel shameless.[7] Moreover, people may ask for a free meal although they do not have financial difficulty. In other words, the effectiveness of suspended meals cannot be fully estimated and guaranteed.		Although some Hong Kong people are still reserved about the idea of suspended meals, it is worth promoting the scheme as it is both an easy and insightful way in which people can make contributions to the society. It is believed that the number of restaurants adopting the suspended meal scheme will increase steadily in the near future, thus spreading the culture to different sectors of the Hong Kong society.		
Yum cha (simplified Chinese: 饮茶 yǐn chá[1]; traditional Chinese: 飲茶; Jyutping: yam2 cha4; Cantonese Yale: yám chà; lit. "drink tea"), is the Cantonese tradition of brunch involving Chinese tea and dim sum. The practice is popular in Cantonese-speaking regions in China, including the southern provinces of Guangdong and the special administrative regions of Hong Kong and Macau. It is also carried out in other regions worldwide where there are overseas Chinese communities.		Yum cha generally involves small portions of steamed, pan-fried, and deep-fried dim sum dishes served in bamboo steamers, which are designed to be eaten communally and washed down with tea.[2] People often go to yum cha in large groups for family get-togethers or celebrations.						Yum cha in Cantonese Chinese literally means "drink tea". The phrase dim sum is sometimes used in place of yum cha; in Cantonese, dim sum (點心) refers to the range of small dishes, whereas yum cha refers to the entire meal.		Traditionally, yum cha is practiced in the morning or early afternoon, hence the terms chow cha (早茶, "morning tea") or ha ng cha (下午茶, "afternoon tea") when appropriate. The former is also known as yum chow cha (饮早茶), which literally means "drinking morning tea". There has been a recent trend for restaurants to offer dim sum during dinner hours and even late at night, though most venues still generally reserve the serving of dim sum for breakfast and lunch periods. The combination of morning tea, afternoon tea, evening tea, lunch and dinner is known as sam cha leung fan (三茶两饭, "three tea, two meal").[3][4]		The history of the tradition can be traced back to the period of Xianfeng Emperor, who first referred to establishments serving tea as yi li guan (一厘馆, "1 cent house"). These offered a place for people to gossip, which became known as cha waa (茶话, "tea talk"). These tea houses grew to become their own type of restaurant, and the action of going there as yum cha.		The ways in which dim sum is served has varied over the years. The traditional method, known as teoi ce (推車, "push-cart"), dates back to the early 1960s, when dim sum items were pre-cooked in advance in the kitchen and brought out into the dining area in baskets by the restaurant employees. These people are generally called fo gai (夥計,"staff"); however, customers commonly address staff using the slang terms leng zai (靚仔, "handsome guy") or leng leui/leng jie (靚女/靚姐, "pretty girl" or "pretty lady").		Later on, pushable trolleys with a heating function (often using gas) were used, allowing more items to be brought out at once. Employees would call out the items they were carrying, and a customer who want to order items would then notify the server, who would place the desired items on the table. This allows the customers receive hot, fresh items quickly and is efficient during periods of high patronage.		Nowadays, many dim sum restaurants have instead adopted a paper-based à la carte ordering system. This method allows only those items which have been ordered to be prepared in the kitchen, reducing the need for leftovers as well as minimizing waste food or ingredients. A few restaurants use both approaches to serving, making use of push-trolleys during peak hours and switching to on-demand ordering in less busier periods.		The cost of a meal was traditionally calculated by the number and size of dishes left on the patron's table at the end. In modern yum cha restaurants, dim sum servers sometimes mark orders by stamping a card on the table. Servers in some restaurants use different stamps so that sales statistics for each server can be recorded.		It is customary to pour tea for others before filling one's own tea cup. It is considered good manners to be the first to pour tea.		Tea drinkers may tap the table with two (occasionally one) fingers of the same hand in a gesture known as 'finger kowtow', symbolising thanks. According to a just-so story, this gesture recreates a tale of imperial obeisance and can be traced to the Qianlong Emperor of the Qing dynasty, who used to travel incognito. While visiting the Jiangnan region, he once went into a teahouse with his companions. In order to maintain his anonymity, he took his turn at pouring tea. His companions wanted to kowtow, but to do so would have revealed the identity of the emperor. Finally, one of them tapped three fingers on the table (one finger representing their bowed head and the other two representing their prostrate arms).		It is considered rude to have a tea cup full of tea; it is preferred that tea is poured until the cup is about 80% full. The proverb "茶满欺客，酒满敬人"[5] means a full cup of tea is fraud, but a full cup of alcohol is a sign of respect.		
Cookware and bakeware are types of food preparation containers, commonly found in a kitchen. Cookware comprises cooking vessels, such as saucepans and frying pans, intended for use on a stove or range cooktop. Bakeware comprises cooking vessels intended for use inside an oven. Some utensils are considered both cookware and bakeware.		The choice of material for cookware and bakeware items has a significant effect on the item's performance (and cost), particularly in terms of thermal conductivity and how much food sticks to the item when in use. Some choices of material also require special pre-preparation of the surface—known as seasoning—before they are used for food preparation.		Both the cooking pot and lid handles can be made of the same material but will mean that, when picking up or touching either of these parts, oven gloves will need to be worn. In order to avoid this, handles can be made of non-heat-conducting materials, for example bakelite, plastic or wood. It is best to avoid hollow handles because they are difficult to clean or to dry.		A good cooking pot design has an 'overcook edge'which is what the lid lies on. The lid has a dripping edge that avoids condensation fluid from dripping off when handling the lid (taking it off and holding it 45°) or putting it down.						The history of cooking vessels before the development of pottery is minimal due to the limited archaeological evidence. The earliest pottery vessels, dating from 7004196000000000000♠19,600±400 BP, were discovered in Xianrendong Cave, Jiangxi, China. The pottery may have been used as cookware, manufactured by hunter-gatherers.[1] Harvard University archaeologist Ofer Bar-Yosef reported that "When you look at the pots, you can see that they were in a fire."[2] It is also possible to extrapolate likely developments based on methods used by latter peoples. Among the first of the techniques believed to be used by stone age civilizations were improvements to basic roasting. In addition to exposing food to direct heat from either an open fire or hot embers it is possible to cover the food with clay or large leaves before roasting to preserve moisture in the cooked result. Examples of similar techniques are still in use in many modern cuisines.[3]		Of greater difficulty was finding a method to boil water. For people without access to natural heated water sources, such as hot springs, heated stones ("pot boilers") could be placed in a water-filled vessel to raise its temperature (for example, a leaf-lined pit or the stomach from animals killed by hunters).[4] In many locations the shells of turtles or large mollusks provided a source for waterproof cooking vessels. Bamboo tubes sealed at the end with clay provided a usable container in Asia, while the inhabitants of the Tehuacan Valley began carving large stone bowls that were permanently set into a hearth as early as 7,000 BC.		According to Frank Hamilton Cushing, Native American cooking baskets used by the Zuni (Zuñi) developed from mesh casings woven to stabilize gourd water vessels. He reported witnessing cooking basket use by Havasupai in 1881. Roasting baskets covered with clay would be filled with wood coals and the product to be roasted. When the thus fired clay separated from the basket, it would become a usable clay roasting pan in itself. This indicates a steady progression from use of woven gourd casings to waterproof cooking baskets to pottery. Other than in many other cultures, Native Americans used and still use the heat source inside the cookware. Cooking baskets are filled with hot stones and roasting pans with wood coals.[5] Native Americans would form a basket from large leaves to boil water, according to historian and novelist Louis L'Amour. As long as the flames did not reach above the level of water in the basket, the leaves would not burn through.[citation needed]		The development of pottery allowed for the creation of fireproof cooking vessels in a variety of shapes and sizes. Coating the earthenware with some type of plant gum, and later glazes, converted the porous container into a waterproof vessel. The earthenware cookware could then be suspended over a fire through use of a tripod or other apparatus, or even be placed directly into a low fire or coal bed as in the case of the pipkin. Ceramics conduct heat poorly, however, so ceramic pots must cook over relatively low heats and over long periods of time. However, most ceramic pots will crack if used on the stovetop, and are only intended for the oven.		The development of bronze and iron metalworking skills allowed for cookware made from metal to be manufactured, although adoption of the new cookware was slow due to the much higher cost. After the development of metal cookware there was little new development in cookware, with the standard Medieval kitchen utilizing a cauldron and a shallow earthenware pan for most cooking tasks, with a spit employed for roasting.[6][7]		By the 17th century, it was common for a Western kitchen to contain a number of skillets, baking pans, a kettle and several pots, along with a variety of pot hooks and trivets. Brass or copper vessels were common in Asia and Europe, whilst iron pots were common in the American colonies. Improvements in metallurgy during the 19th and 20th centuries allowed for pots and pans from metals such as steel, stainless steel and aluminium to be economically produced.[7]		Pottery has been used to make cookware from before dated history. Pots and pans made with this material are durable (some could last a lifetime or more)and are inert and non-reactive. Heat is also conducted evenly in this material. They can be used for both cooking in a fire pit surrounded with coals and for baking in the oven.		Metal pots are made from a narrow range of metals because pots and pans need to conduct heat well, but also need to be chemically unreactive so that they do not alter the flavor of the food. Most materials that are conductive enough to heat evenly are too reactive to use in food preparation. In some cases (copper pots, for example), a pot may be made out of a more reactive metal, and then tinned or clad with another.		Aluminium is a lightweight metal with very good thermal conductivity. It is resistant to many forms of corrosion. Aluminium is commonly available in sheet, cast, or anodized forms,[8] and may be physically combined with other metals (see below).		Sheet aluminium is spun or stamped into form. Due to the softness of the metal it may be alloyed with magnesium, copper, or bronze to increase its strength. Sheet aluminium is commonly used for baking sheets, pie plates, and cake or muffin pans. Deep or shallow pots may be formed from sheet aluminium.		Cast aluminium can produce a thicker product than sheet aluminium, and is appropriate for irregular shapes and thicknesses. Due to the microscopic pores caused by the casting process, cast aluminium has a lower thermal conductivity than sheet aluminium. It is also more expensive. Accordingly, cast aluminium cookware has become less common. It is used, for example, to make Dutch ovens lightweight and bundt pans heavy duty, and used in ladles and handles and woks to keep the sides at a lower temperature than the center.		Anodized aluminium has had the naturally occurring layer of aluminium oxide thickened by an electrolytic process to create a surface that is hard and non-reactive. It is used for sauté pans, stockpots, roasters, and Dutch ovens.[8]		Uncoated and un-anodized aluminium can react with acidic foods to change the taste of the food. Sauces containing egg yolks, or vegetables such as asparagus or artichokes may cause oxidation of non-anodized aluminium.		Aluminium exposure has been suggested as a risk factor for Alzheimer's disease.[9][10][11] The Alzheimer's Association states that "studies have failed to confirm any role for aluminum in causing Alzheimer's."[12] The link remains controversial.[13]		Copper provides the highest thermal conductivity among non-noble metals and is therefore fast heating with unparalleled heat distribution (see: Copper in heat exchangers). Pots and pans are formed from copper sheets of various thicknesses, with those in excess of 2.5 mm considered commercial (or extra-fort) grade. Between 1mm and 2.5 mm wall thickness is considered utility (fort) grade, with thicknesses below 1.5 mm often requiring tube beading or edge rolling to reinforce structural rigidity in circular configurations. Less than 1mm wall thickness is generally considered decorative, with exception made for the case of .75 - 1mm planished copper, which is work-hardened by hammering and therefore expresses performance and strength characteristic of thicker material.		Copper thickness of less than .25mm is, in the case of cookware, referred to as foil and must be formed to a more structurally rigid metal to produce a serviceable vessel. Such applications of copper are purely aesthetic and do not materially contribute to cookware performance.		Copper is reactive with acidic foods which can result in corrosion, the byproducts of which can foment copper toxicity. In certain circumstances, however, unlined copper is recommended and safe, for instance in the preparation of meringue, where copper ions prompt proteins to denature (unfold) and enable stronger protein bonds across the sulfur contained in egg whites. Unlined copper is also used in the making of preserves, jams and jellies. Copper does not store ("bank") heat, and so thermal flows reverse almost immediately upon removal from heat. This allows precise control of consistency and texture while cooking sugar and pectin-thickened preparations. Alone, fruit acid would be sufficient to cause leaching of copper byproducts, but naturally occurring fruit sugars and added preserving sugars buffer copper reactivity. Unlined pans have thereby been used safely in such applications for centuries.		Lining copper pots and pans prevents copper from contact with acidic foods. The most popular lining types are tin, stainless steel, nickel and silver.		The use of tin dates back many centuries and is the original lining for copper cookware. Although the patent for canning in sheet tin was secured in 1810 in England, legendary French chef Auguste Escoffier experimented with a solution for provisioning the French army while in the field by adapting the tin lining techniques used for his cookware to more robust steel containers (then only lately introduced for canning) which protected the cans from corrosion and soldiers from lead solder and botulism poisoning.		Tin linings sufficiently robust for cooking are wiped onto copper by hand, producing a .25–45-mm-thick lining.[14] Decorative copper cookware, i.e., a pot or pan less than 1mm thick and therefore unsuited to cooking, will often be electroplate lined with tin. Should a wiped tin lining be damaged or wear out the cookware can be re-tinned, usually for much less cost than the purchase price of the pan. Tin presents a smooth crystalline structure and is therefore relatively non-stick in cooking applications. As a relatively soft metal abrasive cleansers or cleaning techniques can accelerate wear of tin linings. Wood, silicone or plastic implements are to preferred over harder stainless steel types.		For a period following the second world war, pure nickel was electroplated as a lining to copper cookware. Nickel had the advantage of being harder and more thermally efficient than tin, with a higher melting point. Despite its hardness nickel's wear characteristics were similar to that of tin, as nickel would be plated only to a thickness of <20 microns, and often even less owing to nickel's tendency to plate somewhat irregularly, requiring milling to produce an even cooking surface, albeit sticky compared to tin and silver. Copper cookware with aged or damaged nickel linings is eligible for retinning, or possibly replating with nickel, although this service is difficult if not impossible to find in the US and Europe in the early 21st century. Nickel linings began to fall out of favor in the 1980s owing to the isolation of nickel as an allergen.		Silver is also applied to copper by means of electroplating, and provides an interior finish that is at once smooth, more durable than either tin or nickel, relatively non-stick and extremely thermally efficient. Copper and silver bond extremely well owing to their shared high electro-conductivity. Lining thickness varies widely by maker, but averages between 7–10 microns. The disadvantages of silver are expense and the tendency of sulfurous foods, especially brassicas, to discolor. Worn silver linings on copper cookware can be restored by stripping and re-electroplating.		Copper cookware lined with a thin layer of stainless steel is available from most modern European manufacturers. Stainless steel is 25 times less thermally conductive than the copper lining, and is sometimes critiqued for compromising the efficacy of copper as the base metal. Among the advantages of stainless steel are its durability and corrosion resistance, and although relatively sticky and subject to food residue adhesions, stainless steel is tolerant of most abrasive cleaning techniques and metal implements. Stainless steel forms a pan's structural element when bonded to copper and is irreparable in the event of wear or damage.		Using modern metal bonding techniques, such as cladding, copper is frequently incorporated into cookware constructed of primarily dissimilar metal, such as stainless steel, often as an enclosed diffusion layer (see Coated and Composite Cookware below).		Cast iron cookware is slow to heat, but once at temperature provides even heating. Cast iron can also withstand very high temperatures, making cast iron pans ideal for searing. Being a reactive material, cast iron can have chemical reactions with high acid foods such as wine or tomatoes. In addition, some foods (such as spinach) cooked on bare cast iron will turn black.		Cast iron is a porous material that rusts easily. As a result, it typically requires seasoning before use. Seasoning creates a thin layer of oxidized fat over the iron that coats and protects the surface, and prevents sticking.		Enameled cast iron cookware was developed in the 1920s. In 1934, the French company Cousances designed the enameled cast iron Doufeu to reduce excessive evaporation and scorching in cast iron Dutch ovens. Modeled on old braising pans in which glowing charcoal was heaped on the lids (to mimic two-fire ovens), the Doufeu has a deep recess in its lid which instead is filled with ice cubes. This keeps the lid at a lower temperature than the pot bottom. Further, little notches on the inside of the lid allow the moisture to collect and drop back into the food during the cooking. Although the Doufeu (literally, "gentlefire") can be used in an oven (without the ice, as a casserole pan), it is chiefly designed for stove top use.		Stainless steel is an iron alloy containing a minimum of 11.5% chromium. Blends containing 18% chromium with either 8% nickel, called 18/8, or with 10% nickel, called 18/10, are commonly used for kitchen cookware. Stainless steel's virtues are resistance to corrosion, non-reactivity with either alkaline or acidic foods, and resistance to scratching and denting. Stainless steel's drawbacks for cooking use is that it is a relatively poor heat conductor and its non-magnetic property, although recent developments have allowed the production of magnetic 18/10 alloys, and which thereby provides compatibility with induction cooktops, which require magnetic cookware. Since the material does not adequately spread the heat itself, stainless steel cookware is generally made as a cladding of stainless steel on both sides of an aluminum or copper core to conduct the heat across all sides, thereby reducing "hot spots", or with a disk of copper or aluminum on just the base to conduct the heat across the base, with possible "hot spots" at the sides. In so-called "tri-ply" cookware, the central aluminum layer is obviously non-magnetic, and the interior 18/10 layer need not be magnetic, but the exterior 18/10 layer must be magnetic to be compatible with induction cooktops.		Carbon steel cookware can be rolled or hammered into relatively thin sheets of dense material, which provides robust strength and improved heat distribution. Carbon steel accommodates high, dry heat for such operations as dry searing. Carbon steel does not conduct heat efficiently, but this may be an advantage for larger vessels, such as woks and paella pans, where one portion of the pan is intentionally kept at a different temperature than the rest. Like cast iron, carbon steel must be seasoned before use, usually by rubbing a fat or oil on the cooking surface and heating the cookware on the stovetop or in the oven. With proper use and care, seasoning oils polymerize on carbon steel to form a low-tack surface, well-suited to browning, Maillard reactions and easy release of fried foods. Carbon steel will easily rust if not seasoned and should be stored seasoned to avoid rusting. Carbon steel is traditionally used for crêpe and fry pans, as well as woks.		Steel or aluminum cooking pans can be coated with a substance such as polytetrafluoroethylene (PTFE) in order to minimize food sticking to the pan surface.		There are advantages and disadvantages to such a coating. Coated pans are easier to clean than most non-coated pans, and require little or no additional oil or fat to prevent sticking.		On the other hand, some sticking is needed to cause sucs to form, so a non-stick pan cannot be used where a pan sauce is desired. Nonstick coatings tend to degrade over time. In order to preserve the coating, it is important never to use metal implements or harsh scouring pads or chemical abrasives when cleaning.		Non-stick pans must not be overheated. The coating is stable at normal cooking temperatures, even at the smoke point of most oils; however, if a non-stick pan is preheated while empty, its temperate may quickly exceed 260 °C (500 °F), above which the non-stick coating may begin to deteriorate, changing color and losing its non-stick properties.[15] Above 350 °C (662 °F), the material decomposes rapidly and emits toxic fumes, which are especially toxic to birds, and can cause polymer fume fever in humans.[15][16]		The main difference in coating quality is due to the formulas of the liquid coating, the thickness of each layer and the number of layers used.[citation needed] Higher-quality non-stick cookware use powdered ceramic or titanium mixed with the non-stick material to strengthen them and to make them more resistant to abrasion and deterioration.[citation needed] Some non-stick coatings contain hardening agents. Some coatings are high enough in quality that they pass the strict standards of the National Sanitation Foundation for approval for restaurant use.[citation needed]		Enameled cast iron cooking vessels are made of cast iron covered with a porcelain surface. This creates a piece that has the heat distribution and retention properties of cast iron combined with a non-reactive, low-stick surface.		The enamel over steel technique creates a piece that has the heat distribution of carbon steel and a non-reactive, low-stick surface. Such pots are much lighter than most other pots of similar size, are cheaper to make than stainless steel pots, and do not have the rust and reactivity issues of cast iron or carbon steel.[citation needed] Enamel over steel is ideal for large stockpots and for other large pans used mostly for water-based cooking. Because of its light weight and easy cleanup, enamel over steel is also popular for cookware used while camping.		Cladding is a technique for fabricating pans with a layer of efficient heat conducting material, such as copper or aluminum, covered on the cooking surface by a non-reactive material such as stainless steel, and often covered on the exterior aspect of the pan ("dual-clad") as well. Some pans feature a copper or aluminum interface layer that extends over the entire pan rather than just a heat-distributing disk on the base. Generally, the thicker the interface layer, especially in the base of the pan, the more improved the heat distribution. Claims of thermal efficiency improvements are, however, controversial, owing in particular to the limiting and heat-banking effect of stainless steel on thermal flows.		Aluminum is typically clad on both the inside and the exterior pan surfaces, providing both a stainless cooking surface and a stainless surface to contact the cooktop. Copper of various thicknesses is often clad on its interior surface only, leaving the more attractive copper exposed on the outside of the pan (see Copper above).		Some cookware uses a dual-clad process, with a thin stainless layer on the cooking surface, a thick core of aluminum to provide structure and improved heat diffusion, and a foil layer of copper on the exterior to provide the "look" of a copper pot at a lower price.[17]		Non-metallic cookware can be used in both conventional and microwave ovens. Non-metallic cookware typically can not be used on the stovetop, although Corningware and Pyroflam are some exceptions.		The size and shape of a cooking vessel is typically determined by how it will be used. Cooking vessels are typically referred to as "pots" and "pans," but there is great variation in their actual shapes. Most cooking vessels are roughly cylindrical.		Bakeware is designed for use in the oven (for baking), and encompasses a variety of different styles of baking pans as cake pans, pie pans, and Bread pans.		A Pyrex chicken roaster.		Römertopf.		A Passover brownie cake baked in a Wonder Pot.		Large and small skillets.		Electric griddle with temperature control.		A copper saucepot (stainless lined, with cast iron handles).		Angel food cake pan.		A springform pan with pizza.		A gugelhupf from Alsace, Unterlinden Museum.		
In vertebrate anatomy, the pituitary gland, or hypophysis, is an endocrine gland about the size of a pea and weighing 0.5 grams (0.018 oz) in humans. It is a protrusion off the bottom of the hypothalamus at the base of the brain. The hypophysis rests upon the hypophysial fossa of the sphenoid bone in the center of the middle cranial fossa and is surrounded by a small bony cavity (sella turcica) covered by a dural fold (diaphragma sellae).[2] The anterior pituitary (or adenohypophysis) is a lobe of the gland that regulates several physiological processes (including stress, growth, reproduction, and lactation). The intermediate lobe synthesizes and secretes melanocyte-stimulating hormone. The posterior pituitary (or neurohypophysis) is a lobe of the gland that is functionally connected to the hypothalamus by the median eminence via a small tube called the pituitary stalk (also called the infundibular stalk or the infundibulum).		Hormones secreted from the pituitary gland help control: growth, blood pressure, certain functions of the sex organs, thyroid glands and metabolism as well as some aspects of pregnancy, childbirth, nursing, water/salt concentration at the kidneys, temperature regulation and pain relief.						The pituitary gland, in humans, is a pea-sized gland that sits in a protective bony enclosure called the sella turcica. It is composed of three lobes: anterior, intermediate, and posterior. In many animals, these three lobes are distinct.The intermediate is avascular and almost absent in human beings. The intermediate lobe is present in many lower animal species, in particular in rodents, mice and rats, that have been used extensively to study pituitary development and function.[3] In all animals, the fleshy, glandular anterior pituitary is distinct from the neural composition of the posterior pituitary, which is an extension of the hypothalamus.[3]		The anterior pituitary arises from an invagination of the oral ectoderm and forms Rathke's pouch. This contrasts with the posterior pituitary, which originates from neuroectoderm.		Endocrine cells of the anterior pituitary are controlled by regulatory hormones released by parvocellular neurosecretory cells in the hypothalamic capillaries leading to infundibular blood vessels, which in turn lead to a second capillary bed in the anterior pituitary. This vascular relationship constitutes the hypothalamo-hypophyseal portal system. Diffusing out of the second capillary bed, the hypothalamic releasing hormones then bind to anterior pituitary endocrine cells, upregulating or downregulating their release of hormones.[4]		The anterior lobe of the pituitary can be divided into the pars tuberalis (pars glandularis) and pars distalis (pars glandularis) that constitutes ~80% of the gland. The pars intermedia (the intermediate lobe) lies between the pars distalis and the pars tuberalis, and is rudimentary in the human, although in other species it is more developed.[3] It develops from a depression in the dorsal wall of the pharynx (stomal part) known as Rathke's pouch.		The anterior pituitary contains several different types of cells[5] that synthesize and secrete hormones. Usually there is one type of cell for each major hormone formed in anterior pituitary. With special stains attached to high-affinity antibodies that bind with distinctive hormone, at least 5 types of cells can be differentiated.		The posterior lobe develops as an extension of the hypothalamus.The Posterior pituitary hormones are synthesized by cell bodies in the hypothalamus. The magnocellular neurosecretory cells of the supraoptic and paraventricular nuclei located in the hypothalamus that project axons down the infundibulum to terminals in the posterior pituitary. This simple arrangement differs sharply from that of the adjacent anterior pituitary, which does not develop from the hypothalamus.		The release of pituitary hormones by both the anterior and posterior lobes is under the control of the hypothalamus, albeit in different ways.[4]		The anterior pituitary synthesizes and secretes hormones. All releasing hormones (-RH) referred to, can also be referred to as releasing factors (-RF).		Somatotrophins:		Thyrotrophins:		Corticotropins:		Lactotrophins:		Gonadotropins:		These hormones are released from the anterior pituitary under the influence of the hypothalamus. Hypothalamic hormones are secreted to the anterior lobe by way of a special capillary system, called the hypothalamic-hypophysial portal system.		The intermediate lobe synthesizes and secretes the following important endocrine hormone:		The posterior pituitary stores and secretes (but does not synthesize) the following important endocrine hormones:		Magnocellular neurons:		Hormones secreted from the pituitary gland help control the following body processes:		Some of the diseases involving the pituitary gland are:		All of the functions of the pituitary gland can be adversely affected by an over- or under-production of associated hormones.		The pituitary gland is important for mediating the stress response, via the hypothalamic–pituitary–adrenal axis (HPA axis) Critically, pituitary gland growth during adolescence can be altered by early life stress such as childhood maltreatment or maternal dysphoric behavior.[10]		It has been demonstrated that, after controlling for age, sex, and BMI, larger quantities of DHEA and DHEA-S tended to be linked to larger pituitary volume.[11] Additionally, a correlation between pituitary gland volume and Social Anxiety subscale scores was identified which provided a basis for exploring mediation. Again controlling for age, sex, and BMI, DHEA and DHEA-S have been found to be predictive of larger pituitary gland volume, which was also associated with increased ratings of social anxiety.[11] This research provides evidence that pituitary gland volume mediates the link between higher DHEA(S) levels (associated with relatively early adrenarche) and traits associated with social anxiety.[11] Children who experience early adrenarcheal development tend to have larger pituitary gland volume compared to children with later adrenarcheal development.[11]		The Greek physician Galen referred to the pituitary gland by only using the (Ancient Greek) name ἀδήν,[12] gland.[13] He described the pituitary gland as part of a series of secretory organs for the excretion of nasal mucus.[12] Anatomist Andreas Vesalius translated ἀδήν with glans, in quam pituita destillat, "gland in which slime (pituita[14]) drips".[12][15] Besides this 'descriptive' name, Vesalius used glandula pituitaria, from which the English name pituitary gland[16] is ultimately derived.		The expression glandula pituitaria is still used as official synonym beside hypophysis in the official Latin nomenclature Terminologia Anatomica.[17] In the seventeenth century the supposed function of the pituitary gland to produce nasal mucus was debunked.[12] The expression glandula pituitaria and its English equivalent pituitary gland can only be justified from a historical point of view.[18] The inclusion of this synonym is merely justified by noting that the main term hypophysis is a much less popular term.[19]		The anatomist Samuel Thomas von Sömmerring coined the name hypophysis.[12] This name consists[12][18] of ὑπό ('under')[13] and φύειν ('to grow').[13] In later Greek ὑπόφυσις is used differently by Greek physicians as outgrowth.[12] Sömmering also used the equivalent expression appendix cerebri,[12][15] with appendix as appendage.[14] In various languages, Hirnanhang[15] in German and hersenaanhangsel[20] in Dutch, the terms are derived from appendix cerebri.		The pituitary gland is found in all vertebrates, but its structure varies among different groups.		The division of the pituitary described above is typical of mammals, and is also true, to varying degrees, of all tetrapods. However, only in mammals does the posterior pituitary have a compact shape. In lungfish, it is a relatively flat sheet of tissue lying above the anterior pituitary, but in amphibians, reptiles, and birds, it becomes increasingly well developed. The intermediate lobe is, in general, not well developed in any species and is entirely absent in birds.[21]		The structure of the pituitary in fish, apart from the lungfish, is generally different from that in other animals. In general, the intermediate lobe tends to be well developed, and may equal the remainder of the anterior pituitary in size. The posterior lobe typically forms a sheet of tissue at the base of the pituitary stalk, and in most cases sends irregular finger-like projection into the tissue of the anterior pituitary, which lies directly beneath it. The anterior pituitary is typically divided into two regions, a more anterior rostral portion and a posterior proximal portion, but the boundary between the two is often not clearly marked. In elasmobranchs there is an additional, ventral lobe beneath the anterior pituitary proper.[21]		The arrangement in lampreys, which are among the most primitive of all fish, may indicate how the pituitary originally evolved in ancestral vertebrates. Here, the posterior pituitary is a simple flat sheet of tissue at the base of the brain, and there is no pituitary stalk. Rathke's pouch remains open to the outside, close to the nasal openings. Closely associated with the pouch are three distinct clusters of glandular tissue, corresponding to the intermediate lobe, and the rostral and proximal portions of the anterior pituitary. These various parts are separated by meningial membranes, suggesting that the pituitary of other vertebrates may have formed from the fusion of a pair of separate, but associated, glands.[21]		Most armadillos also possess a neural secretory gland very similar in form to the posterior pituitary, but located in the tail and associated with the spinal cord. This may have a function in osmoregulation.[21]		There is a structure analogous to the pituitary in the octopus brain.[22]		Although rudimentary in humans (and often considered part of the anterior pituitary), the intermediate lobe located between the anterior and posterior pituitary is important to many animals. For instance, in fish, it is believed to control physiological color change. In adult humans, it is just a thin layer of cells between the anterior and posterior pituitary. The intermediate lobe produces melanocyte-stimulating hormone (MSH), although this function is often (imprecisely) attributed to the anterior pituitary.		The intermediate lobe is, in general, not well developed in tetrapods, and is entirely absent in birds.[21]		Location of the pituitary gland in the human brain		Pituitary and pineal glands		The arteries of the base of the brain.		Mesal aspect of a brain sectioned in the median sagittal plane.		Pituitary		Pituitary gland		Cerebrum.Inferior view.Deep dissection.		
A meal is an eating occasion that takes place at a certain time and includes specific, prepared food, or the food eaten on that occasion.[1][2] The names used for specific meals in English vary greatly, depending on the speaker's culture, the time of day, or the size of the meal.		Meals occur primarily at homes, restaurants, and cafeterias, but may occur anywhere. Regular meals occur on a daily basis, typically several times a day. Special meals are usually held in conjunction with such occasions as birthdays, weddings, anniversaries, and holidays. A meal is different from a snack in that meals are generally larger, more varied, and more filling than snacks.[3]		The type of meal served or eaten at any given time varies by custom and location. In most modern cultures, three main meals are eaten: in the morning, early afternoon, and evening. Further, the names of meals are often interchangeable by custom as well. Some serve dinner as the main meal at midday, with supper as the late afternoon/early evening meal; while others may call their midday meal lunch and their early evening meal supper. Except for "breakfast", these names can vary from region to region or even from family to family.		A study in 2016 by Toluna found that 47% of parents share fewer meals with their families than when growing up, and 58% wished they could do it more frequently, including 66% of dads.[4]						Breakfast is the first meal of a day, most often eaten in the early morning before undertaking the day's work. Some believe it to be the most important meal of the day.[5] The word breakfast literally refers to breaking the fasting period of the prior night.[6]		Breakfast foods vary widely from place to place, but often include a carbohydrate such as grains or cereals, fruit, vegetables, a protein food such as eggs, meat or fish, and a beverage such as tea, coffee, milk, or fruit juice. Coffee, milk, tea, juice, breakfast cereals, pancakes, waffles, sausages, French toast, bacon, sweetened breads, fresh fruits, vegetables, eggs, baked beans, muffins, crumpets and toast with butter, margarine, jam or marmalade are common examples of Western breakfast foods, though a large range of preparations and ingredients are associated with breakfast globally.[7]		A full breakfast is a breakfast meal, usually including bacon, sausages, eggs, and a variety of other cooked foods, with a beverage such as coffee or tea. It is especially popular in the UK and Ireland, to the extent that many cafés and pubs offer the meal at any time of day as an "all-day breakfast". It is also popular in other English-speaking countries.		In England it is usually referred to as a 'full English breakfast' (often shortened to 'full English') or 'fry-up'.[8][9] Other regional names and variants include the 'full Scottish', 'full Welsh', 'full Irish' and the 'Ulster fry'.[10][11][12]		The full breakfast is among the most internationally recognised British dishes, along with such staples as bangers & mash, shepherd's pie, fish and chips and the Christmas dinner.[13] The full breakfast became popular in the British Isles during the Victorian era, and appeared as one among many suggested breakfasts in the home economist Isabella Beeton's The Book of Household Management (1861). A full breakfast is often contrasted (e.g. on hotel menus) with the lighter alternative of a Continental breakfast, traditionally consisting of tea, milk or coffee and fruit juices with bread, croissants, or pastries.		"Instant breakfast" typically refers to breakfast food products that are manufactured in a powdered form, which are generally prepared with the addition of milk and then consumed as a beverage.[14][15] Some instant breakfasts are produced and marketed in liquid form, being pre-mixed. The target market for instant breakfast products includes consumers who tend to be busy, such as working adults.[15]		A champagne breakfast is a breakfast served with champagne or sparkling wine. It is a new concept in some countries[16] and is not typical of the role of a breakfast.		It may be part of any day or outing considered particularly luxurious or indulgent. The accompanying breakfast is sometimes of a similarly high standard [17] and include rich foods such as salmon, caviar,[18] chocolate or pastries, which would not ordinarily be eaten at breakfast[19] or more courses.[20] Instead of as a formal meal the breakfast can be given to the recipient in a basket or hamper.		Lunch, the abbreviation for luncheon, is a light meal typically eaten at midday.[21] The origin of the words lunch and luncheon relate to a small snack originally eaten at any time of the day or night. During the 20th century the meaning gradually narrowed to a small or mid-sized meal eaten at midday. Lunch is commonly the second meal of the day after breakfast. The meal varies in size depending on the culture, and significant variations exist in different areas of the world.		A packed lunch (also called pack lunch, sack lunch or bag lunch in North America, or pack up in the United Kingdom, as well as the regional variations: bagging in Lancashire, Merseyside and Yorkshire,[22]) is a lunch prepared at home and carried to be eaten somewhere else, such as school, a workplace, or at an outing. The food is usually wrapped in plastic, aluminum foil, or paper and can be carried ("packed") in a lunch box, paper bag (a "sack"), or plastic bag. While packed lunches are usually taken from home by the people who are going to eat them, in Mumbai, India, tiffin boxes are most often picked up from the home and brought to workplaces later in the day by so-called dabbawallas. It is also possible to buy packed lunches from stores in several countries. Lunch boxes made out of metal, plastic or vinyl are now popular with today's youth. Lunch boxes provide a way to take heavier lunches in a sturdier box or bag. It is also environmentally friendly.		Dinner usually refers to the most significant and important meal of the day, which can be the noon or the evening meal. However, the term "dinner" can have many different meanings depending on the culture; it may mean a meal of any size eaten at any time of the day.[23][24] Historically, it referred to the first meal of the day, eaten around noon, and is still sometimes used for a noon-time meal, particularly if it is a large or main meal. The meaning as the evening meal, generally the largest of the day, is becoming a standard in many parts of the English-speaking world.		A full course dinner is a dinner consisting of multiple dishes, or courses. In its simplest form, it can consist of three to five courses, such as appetizers, fish course, entrée, main course and dessert.		Meal preparation, sometimes called "meal prep," is the process of planning and preparing meals. It generally involves food preparation, including cooking.		Preparing food for eating generally requires selection, measurement and combination of ingredients in an ordered procedure so as to achieve desired results. Food preparation includes but is not limited to cooking.		Cooking or cookery is the art, technology and craft of preparing food for consumption with the use of heat. Cooking techniques and ingredients vary widely across the world, from grilling food over an open fire to using electric stoves, to baking in various types of ovens, reflecting unique environmental, economic, and cultural traditions and trends. The ways or types of cooking also depend on the skill and type of training an individual cook has. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments. Cooking can also occur through chemical reactions without the presence of heat, most notably with ceviche, a traditional South American dish where fish is cooked with the acids in lemon or lime juice.		
A dietary supplement is either intended to provide nutrients in order to increase the quantity of their consumption, or to provide non-nutrient chemicals which are claimed to have a biologically beneficial effect.		Supplements as generally understood include vitamins, minerals, fiber, fatty acids, or amino acids, among other substances. U.S. authorities define dietary supplements as foods, while elsewhere they may be classified as drugs or other products.		There are more than 50,000 dietary supplements available. More than half of the U.S. adult population (53% – 55%) consume dietary supplements with most common ones being multivitamins.[1]		These products are not intended to prevent or treat any disease and in some circumstances are dangerous, according to the U.S. National Institutes of Health. For those who fail to consume a balanced diet, the agency says that certain supplements "may have value."[2] An exception is vitamin D, which is recommended in Nordic countries[3] due to weak sunlight.						According to the United States Food and Drug Administration (FDA), dietary supplements are products which are not pharmaceutical drugs, food additives like spices or preservatives, or conventional food, and which also meet any of these criteria:[4]		In the United States, the FDA has different monitoring procedures for substances depending on whether they are presented as drugs, food additives, food, or dietary supplements.[4] Dietary supplements are eaten or taken by mouth, and are regulated in United States law as a type of food rather than a type of drug.[5] Like food and unlike drugs, no government approval is required to make or sell dietary supplements; the manufacturer checks the safety of dietary supplements but the government does not; and rather than requiring risk–benefit analysis to prove that the product can be sold like a drug, risk–benefit analysis is only used to petition that food or a dietary supplement is unsafe and should be removed from market.[4]		The intended use of dietary supplements is to ensure that a person gets enough essential nutrients.[6]		Dietary supplements should not be used to treat any disease or as preventive healthcare.[7] An exception to this recommendation is the appropriate use of vitamins.[7]		Supplements may create harm in several ways, including over-consumption, particularly of minerals and fat-soluble vitamins which can build up in the body.[8] The products may also cause harm related to their rapid absorption in a short period of time, quality issues such as contamination, or by adverse interactions with other foods and medications.[9]		There are many types of dietary supplements.		Vitamin is an organic compound required by an organism as a vital nutrient in limited amounts.[10] An organic chemical compound (or related set of compounds) is called a vitamin when it cannot be synthesized in sufficient quantities by an organism, and must be obtained from the diet. Thus, the term is conditional both on the circumstances and on the particular organism. For example, ascorbic acid (vitamin C) is a vitamin for humans, but not for most other animals. Supplementation is important for the treatment of certain health problems but there is little evidence of benefit when used by those who are otherwise healthy.[11]		Dietary elements, commonly called "dietary minerals" or "minerals", are the chemical elements required by living organisms, other than the four elements carbon, hydrogen, nitrogen, and oxygen present in common organic molecules.[citation needed]		Amino acids are biologically important organic compounds composed of amine (-NH2) and carboxylic acid (-COOH) functional groups, along with a side-chain specific to each amino acid. The key elements of an amino acid are carbon, hydrogen, oxygen, and nitrogen, though other elements are found in the side-chains of certain amino acids.		Amino acids can be divided into three categories: essential amino acids, non-essential amino acids, and conditional amino acids. Essential amino acids cannot be made by the body, and must be supplied by food. Non-essential amino acids are made by the body from essential amino acids or in the normal breakdown of proteins. Conditional amino acids are usually not essential, except in times of illness, stress, or for someone challenged with a lifelong medical condition[citation needed].		Essential fatty acids, or EFAs, are fatty acids that humans and other animals must ingest because the body requires them for good health but cannot synthesize them.[12] The term "essential fatty acid" refers to fatty acids required for biological processes but does not include the fats that only act as fuel.		Bodybuilding supplements are dietary supplements commonly used by those involved in bodybuilding and athletics. Bodybuilding supplements may be used to replace meals, enhance weight gain, promote weight loss or improve athletic performance. Among the most widely used are vitamin supplements, protein drinks, branched-chain amino acids (BCAA), glutamine, essential fatty acids, meal replacement products, creatine, weight loss products and testosterone boosters. Supplements are sold either as single ingredient preparations or in the form of "stacks" – proprietary blends of various supplements marketed as offering synergistic advantages. While many bodybuilding supplements are also consumed by the general public their salience and frequency of use may differ when used specifically by bodybuilders.		In 2013, the global market of vitamins, minerals, and nutritional and herbal supplements (VMHS) was valued at $82 billion, with roughly 28 percent of that in the U.S., where sales increased by approximately $6 billion between 2007 and 2012.[13]		The vitamins and dietary supplements sector in the U.S. grew 4% in 2015, to reach US$27.2 billion. The U.S. market was highly competitive in 2015, as no single company accounted for more than a 5% share of value sales.[14]		According to University of Helsinki food safety professor Marina Heinonen, more than 90% of dietary supplement health claims are incorrect.[15] In addition, ingredients listed have been found to be different from the contents. For example, Consumer Reports reported unsafe levels of arsenic, cadmium, lead and mercury in several of the protein powders that were tested.[16] Also, the CBC found that protein spiking (the addition of amino acid filler to manipulate analysis) was not uncommon,[17] however many of the companies involved challenged their claim.[18]		Among general reasons for harmful effects of dietary supplements are: a) absorption in a short time; b) quality and contamination; and c) enhancing both positive and negative effects at the same time.[19] The number of incidents of liver damage from dietary supplements has tripled in a decade. Most of the products causing that effect were bodybuilding supplements. Some of the victims required liver transplants and some died. A third of the supplements involved contained unlisted steroids.[20] Mild to severe toxicity has occurred on many occasions due to dietary supplements, even when the active ingredients were essential nutrients such as vitamins, minerals or amino acids. This has been a result of adulteration of the product, excessive usage on the part of the consumer, or use by persons at risk for the development of adverse effects. In addition, a number of supplements contain psychoactive drugs, whether of natural or synthetic origin.[21][22]		BMC Medicine published a study on herbal supplements in 2013. Most of the supplements studied were of low quality, one third did not contain the active ingredient(s) claimed, and one third contained unlisted substances.[23][24]		An investigation by the New York Attorney General’s office reported in 2015 analyzed 78 bottles of herbal supplements from Walmart, Target, Walgreens and GNC stores in New York State using DNA barcoding, a method used to detect labeling fraud in the seafood industry. Only about 20% contained the ingredient on the label.[25][26]		Some supplements were contamined by rodent feces and urine.[27]		Only 0.3% of the 55,000 U.S. market dietary supplements have been studied regarding their common side effects.[20]		Work done by scientists in the early 20th century on identifying individual nutrients in food and developing ways to manufacture them raised hopes that optimal health could be achieved and diseases prevented by adding them to food and providing people with dietary supplements; while there were successes in preventing vitamin deficiencies, and preventing conditions like neural tube defects by supplementation and food fortification with folic acid, no targeted supplementation or fortification strategies to prevent major diseases like cancer or cardiovascular diseases have proved successful.[28]		For example, while increased consumption of fruits and vegetables are related to decreases in mortality, cardiovascular diseases and cancers, supplementation with key factors found in fruits and vegetable, like antioxidants, vitamins, or minerals, do not help and some have been found to be harmful in some cases.[29][30] In general as of 2016, robust clinical data is lacking, that shows that any kind of dietary supplementation does more good than harm for people who are healthy and eating a reasonable diet but there is clear data showing that dietary pattern and lifestyle choices are associated with health outcomes.[31][32]		As a result of the lack of good data for supplementation and the strong data for dietary pattern, public health recommendations for healthy eating urge people to eat a plant-based diet of whole foods, minimizing processed food, salt and sugar and to get exercise daily, and to abandon Western pattern diets and a sedentary lifestyle.[33][34]:10		The regulation of food and dietary supplements by the U.S. Food and Drug Administration is governed by various statutes enacted by the United States Congress and interpreted by the U.S. Food and Drug Administration ("FDA"). Pursuant to the Federal Food, Drug, and Cosmetic Act ("the Act") and accompanying legislation, the FDA has authority to oversee the quality of substances sold as food in the United States, and to monitor claims made in the labeling about both the composition and the health benefits of foods.		Substances which the FDA regulates as food are subdivided into various categories, including foods, food additives, added substances (man-made substances which are not intentionally introduced into food, but nevertheless end up in it), and dietary supplements. The specific standards which the FDA exercises differ from one category to the next. Furthermore, the FDA has been granted a variety of means by which it can address violations of the standards for a given category of substances.		The European Union's Food Supplements Directive of 2002 requires that supplements be demonstrated to be safe, both in dosages and in purity.[35] Only those supplements that have been proven to be safe may be sold in the bloc without prescription. As a category of food, food supplements cannot be labeled with drug claims but can bear health claims and nutrition claims.[36]		The dietary supplements industry in the United Kingdom (UK), one of the 28 countries in the bloc, strongly opposed the Directive. In addition, a large number of consumers throughout Europe, including over one million in the UK, and various doctors and scientists, had signed petitions by 2005 against what are viewed by the petitioners as unjustified restrictions of consumer choice.[37]		In 2004, along with two British trade associations, the Alliance for Natural Health (ANH) had a legal challenge to the Food Supplements Directive[38] referred to the European Court of Justice by the High Court in London.[39]		Although the European Court of Justice's Advocate General subsequently said that the bloc's plan to tighten rules on the sale of vitamins and food supplements should be scrapped,[40] he was eventually overruled by the European Court, which decided that the measures in question were necessary and appropriate for the purpose of protecting public health. ANH, however, interpreted the ban as applying only to synthetically produced supplements, and not to vitamins and minerals normally found in or consumed as part of the diet.[41]		Nevertheless, the European judges acknowledged the Advocate General's concerns, stating that there must be clear procedures to allow substances to be added to the permitted list based on scientific evidence. They also said that any refusal to add the product to the list must be open to challenge in the courts.[42]		Effects of most dietary supplements have not been determined in randomized clinical trials and manufacturing is lightly regulated; randomized clinical trials of certain vitamins and antioxidants have found increased mortality rates.[43][44]		
Waiting staff are those who work at a restaurant or a bar, and sometimes in private homes, attending customers—supplying them with food and drink as requested. A server or waiting staff takes on a very important role in a restaurant which is to always be attentive and accommodating to the guests. Each waiter follows rules and guidelines that are developed by the manager. Wait staff can abide by this rule by completing many different tasks throughout his or her shift. Such as food-running, polishing dishes and silverware, helping bus tables, and restock working stations with needed supplies.		Waiting on tables is (along with nursing and teaching) part of the service sector, and among the most common occupations in the United States. The Bureau of Labor Statistics estimates that, as of May 2008, there were over 2.2 million persons employed as servers in the U.S.[1]		Many restaurants choose a specific uniform for their wait staff to wear. Waitstaff may receive tips as a minor or major part of their earnings, with customs varying widely from country to country.[2]						An individual waiting tables is commonly called a server, front server, waitress (females only), waiter (referring to males or less commonly either gender), member of the wait staff, waitstaff[3] or serving staff server, waitperson,[4] or less commonly the 1980s American neologism waitron.[5][6][7][8] Archaic terms such as serving girl, serving wench, or serving lad are generally used only within their historical context.		The duties a waiter, wait staff or server partakes in can be tedious, and challenging but are vital to the success of the restaurant. Such duties include, preparing a section of tables before guests sit down (e.g., changing the tablecloth, putting out new utensils, cleaning chairs, etc.), offering cocktails, specialty drinks, wine, beer or other beverages, recommending food options, requesting the chef to make changes in how food is prepared, pre-clearing the tables, and serving food and beverages to customers. In some higher-end restaurants, servers have a good knowledge of the wine list and can recommend food-wine pairings. At more expensive restaurants, servers memorize the ingredient list for the dishes and the manner in which the food is prepared. For example, if the menu lists marinated beef, the customer might ask what the beef is marinated in, for how long, and what cut of beef is used in the dish. Silver service staff are specially trained to serve at banquets or high-end restaurants. These servers follow specific rules and service guidelines which makes it a skilled job. They generally wear black and white with a long, white apron (extending from the waist to ankle).		The head server is in charge of the waiting staff, and is also frequently responsible for assigning seating. The head server must insure that all staff does their duties accordingly. The functions of a head server can overlap to some degree with that of the Maître d'hôtel. Restaurants in North America employ an additional level of waiting staff, known as busboys or busgirls, increasingly referred to as busser or server assistant to clear dirty dishes, set tables, and otherwise assist the waiting staff.[9][10][11]		Emotional labour is often required by waiting staff,[12] particularly at many high-class restaurants.		Restaurant serving positions require on the job training that would be held by an upper level server in the restaurant. The server will be trained to provide good customer service, learn food items and drinks and maintain a neat and tidy appearance. Working, in a role such as captain, in a top rated restaurant requires disciplined role-playing comparable to a theater performance.[13]		In the United States, some states require individuals employed to handle food and beverages to obtain a food handlers card or permit.[14] In these States, servers that do not have a permit or handlers card can not serve. The server can achieve a permit or handlers card online.		No food certification requirements are needed in Canada. However, to serve alcoholic beverages in Canada servers must undergo their province's online training course within a month of being hired.		Different countries maintain different customs regarding tipping, but in the United States, a tip paid in addition to the amount presented on the bill for food and drinks is customary. At most sit-down restaurants, servers and bartenders expect a tip after a patron has paid the check.[15] The minimum legally required hourly wage paid to waiters and waitresses in many U.S. states is lower than the minimum wage employers are required to pay for most other forms of labor in order to account for the tips that form a significant portion of the server's income. If wages and tips do not equal the federal minimum wage of $7.25 per hour during any week, the employer is required to increase cash wages to compensate.[16]		Tips average between 15% and 20% of the bill. 20% is expected for good service, more than 20% is expected for great service, and some patrons tip even more for exceptional service.[17] If the waiter or waitress goes above and beyond to ensure the patron enjoys his/her meal, it is customary to give a higher tip. Some restaurants charge an automatic gratuity for larger parties (usually 6 or more), and the gratuity ranges from 15%-20% depending on the restaurant.		
Vegetarianism /vɛdʒɪˈtɛəriənɪzəm/ is the practice of abstaining from the consumption of meat (red meat, poultry, seafood, and the flesh of any other animal), and may also include abstention from by-products of animal slaughter.[1][2][3][4]		Vegetarianism may be adopted for various reasons. Many people object to eating meat out of respect for sentient life. Such ethical motivations have been codified under various religious beliefs, as well as animal rights advocacy. Other motivations for vegetarianism are health-related, political, environmental, cultural, aesthetic, economic, or personal preference. There are variations of the diet as well: an ovo-lacto vegetarian diet includes both eggs and dairy products, an ovo-vegetarian diet includes eggs but not dairy products, and a lacto-vegetarian diet includes dairy products but not eggs. A vegan diet excludes all animal products, including eggs and dairy. Some vegans also avoid other animal products such as beeswax, leather or silk clothing, and goose-fat shoe polish.		Packaged and processed foods, such as cakes, cookies, candies, chocolate, yogurt, and marshmallows, often contain unfamiliar animal ingredients, so may be a special concern for vegetarians due to the likelihood of such additions.[3][5] Often, prior to purchase or consumption, vegetarians will scrutinize products for animal-derived ingredients.[5] Vegetarians' feelings vary with regard to these ingredients. For example, while some vegetarians may be unaware of animal-derived rennet's role in the production of cheese, and may therefore unknowingly consume the product,[3][6][7] other vegetarians may not take issue with its consumption.[3][4]		Semi-vegetarian diets consist largely of vegetarian foods but may include fish or poultry, or sometimes other meats, on an infrequent basis. Those with diets containing fish or poultry may define meat only as mammalian flesh and may identify with vegetarianism.[8][9] A pescetarian diet has been described as "fish but no other meat".[10] The common use association between such diets and vegetarianism has led vegetarian groups such as the Vegetarian Society to state that diets containing these ingredients are not vegetarian, because fish and birds are also animals.[11]		The term 'vegetarian' has been in use since 1839 to refer to what was previously described as a "vegetable diet". The word is commonly believed to be a compound of vegetable and the suffix -arian (as in agrarian). (John Davis shows that it was probably not derived from the Latin word vegetus.[12]) The term was popularized with the foundation of the Vegetarian Society in Manchester, UK in 1847.[13][14] The earliest occurrences of the term seem to be related to Alcott House, a school on the north side of Ham Common, London, opened in July 1838 by James Pierrepont Greaves. From 1841, it was known as A Concordium, or Industry Harmony College, from which time the institution began to publish its own pamphlet, "The Healthian", which provides some of the earliest appearances of the term "vegetarian".[15][16]		The earliest record of vegetarianism comes from Indus Valley Civilization[citation needed] as early as the 7th century BCE,[17] inculcating tolerance towards all living beings.[18][19] Vegetarianism was also practiced in ancient Greece and the earliest reliable evidence for vegetarian theory and practice in Greece dates from the 6th century BC. The Orphics, a religious movement spreading in Greece at that time, and Pythagoras, a philosopher and religious leader in the area of Southern Italy colonized by Greek settlers, abstained from the flesh of animals.[20]		Vegetarianism was also practiced about six centuries later in another instance (between 30 BCE–50 CE) in northern Thracian region, the Moesi tribe who inhabited present day Serbia and Bulgaria, feeding themselves on honey, milk and cheese.[21]		In the Indian culture, the diet was closely connected with the attitude of nonviolence towards animals (called ahimsa in India) and was promoted by religious groups and philosophers.[22] The ancient Indian work of Tirukkural explicitly and unambiguously emphasizes vegetarianism and non-killing.[23] Chapter 26 of the Tirukkural, through couplets 251 to 260, deals exclusively on vegetarianism or veganism.[23] Among the Hellenes, Egyptians and others, it had medical or ritual purification purposes.		Indian emperor Ashoka asserted protection to fauna:		"Twenty-six years after my coronation various animals were declared to be protected – parrots, mainas, aruna, ruddy geese, wild ducks, nandimukhas, gelatas, bats, queen ants, terrapins, boneless fish, vedareyaka, gangapuputaka, sankiya fish, tortoises, porcupines, squirrels, deer, bulls, okapinda, wild asses, wild pigeons, domestic pigeons and all four-footed creatures that are neither useful nor edible. Those nanny goats, ewes and sows which are with young or giving milk to their young are protected, and so are young ones less than six months old. Cocks are not to be caponized, husks hiding living beings are not to be burnt and forests are not to be burnt either without reason or to kill creatures. One animal is not to be fed to another." —Edicts of Ashoka, Fifth Pillar		Following the Christianization of the Roman Empire in late antiquity, vegetarianism practically disappeared from Europe, as it did on other continents, except India.[25] Several orders of monks in medieval Europe restricted or banned the consumption of meat for ascetic reasons, but none of them eschewed fish.[26] (The medieval definition of "fish" included such animals as seals, porpoises, dolphins, barnacle geese, puffins, and beavers.)[27] It re-emerged during the Renaissance,[28] becoming more widespread in the 19th and 20th centuries. In 1847, the first Vegetarian Society was founded in the United Kingdom;[29] Germany, the Netherlands, and other countries followed. In 1886, the vegetarian colony Nueva Germania was founded in Paraguay, though its vegetarian aspect would prove short-lived.[30]:345–358 The International Vegetarian Union, an association of the national societies, was founded in 1908. In the Western world, the popularity of vegetarianism grew during the twentieth century as a result of nutritional, ethical, and more recently, environmental and economic concerns.		There are a number of vegetarian diets that exclude or include various foods:		Within the "ovo-" groups, there are many who refuse to consume fertilized eggs (with balut being an extreme example); however, such distinction is typically not specifically addressed.		Some vegetarians also avoid products that may use animal ingredients not included in their labels or which use animal products in their manufacturing; for example, sugars that are whitened with bone char, cheeses that use animal rennet (enzymes from animal stomach lining), gelatin (derived from the collagen inside animals' skin, bones and connective tissue), some cane sugar (but not beet sugar) and apple juice/alcohol clarified with gelatin or crushed shellfish and sturgeon, while other vegetarians are unaware of or do not mind such ingredients.[3][4][5][6]		Individuals sometimes label themselves "vegetarian" while practicing a semi-vegetarian diet,[9][33][34] as some dictionary definitions describe vegetarianism as sometimes including the consumption of fish, or only include mammalian flesh as part of their definition of meat,[8][35] while other definitions exclude fish and all animal flesh.[11] In other cases, individuals may describe themselves as "flexitarian".[33][36] These diets may be followed by those who reduce animal flesh consumed as a way of transitioning to a complete vegetarian diet or for health, ethical, environmental, or other reasons. Semi-vegetarian diets include:		Semi-vegetarianism is contested by vegetarian groups, such as the Vegetarian Society, which states that vegetarianism excludes all animal flesh.[11]		Studies on the health effects of vegetarian diets observe heterogeneous effects on mortality. One review found a decreased overall risk of all cause mortality, cancer (except breast) and cardiovascular disease,[38][38] however another meta analysis found lower risk for ischemic heart disease and cancer, but no effect on overall mortality or cerebrovascular disease. Possible limitations include varying definitions used of vegetarianism, and the observation of increased risk of lung cancer mortality in those on a vegetarian diet for less than five years.[39] An analysis pooling 2 large studies found vegetarians in the UK have similar all cause mortality as meat eaters.[40] An older meta analysis found similar results, only finding decreased mortality in vegetarians, pescatarians, and irregular meat eaters in ischemic heart disease, but not from any other cause.[41]		A vegetarian diet which is poorly planned can lead to hyperhomocysteinemia and platelet disorders; this risk may be offset by ensuring sufficient consumption of vitamin B 12 and polyunsaturated fatty acids.[38]		The Academy of Nutrition and Dietetics and Dietitians of Canada have stated that at all stages of life, a properly planned vegetarian diet is "healthful, nutritionally adequate, and provides health benefits in the prevention and treatment of certain diseases".[42] Large-scale studies have shown that mortality from ischemic heart disease was 30% lower among vegetarian men and 20% lower among vegetarian women than in non-vegetarians.[43][44] Vegetarian diets offer lower levels of saturated fat, cholesterol and animal protein, and higher levels of carbohydrates, fibre, magnesium, potassium, folate, and antioxidants such as vitamins C and E and phytochemicals.[45][46]		"Vegetarian diets can meet guidelines for the treatment of diabetes and some research suggests that diets that are more plant-based reduce risk of type-2 diabetes. Rates of self-reported Seventh-day Adventists (SDA) were less than half of those of the general population, and, among SDA, vegetarians had lower rates of diabetes than non-vegetarians. Among possible explanations for a protective effect of vegetarian diet are the Lower BMI of vegetarians and higher fiber intake, both of which improve insulin sensitivity."[47]		The relationship between vegetarian diet and bone health remains unclear. According to some studies, a vegetarian lifestyle can be associated with vitamin B 12 deficiency and low bone mineral density.[48] However, a study of vegetarian and non-vegetarian adults in Taiwan found no significant difference in bone mineral density between the two groups.[49] Other studies, exploring animal protein's negative effects on bone health, suggest that vegetarians may be less prone to osteoporosis than omnivores, as vegetarian subjects had greater bone mineral density[50] and more bone formation.[51]		The China-Cornell-Oxford Project,[52] a 20-year study conducted by Cornell University, the University of Oxford, and the government of China has established a correlation between the consumption of animal products and a variety of chronic illnesses, such as coronary heart disease, diabetes, and cancers of the breast, prostate and bowel (see The China Study).[53]		A British study of almost 10,000 men found that those who gave up meat were almost twice as likely to suffer from depression as people on a conventional balanced diet. The study found that the 350 committed vegetarians studied had a higher average depression score compared to others.[54]		Western vegetarian diets are typically high in carotenoids, but relatively low in omega-3 fatty acids and vitamin B12. Vegans can have particularly low intake of vitamin B and calcium if they do not eat enough items such as collard greens, leafy greens, tempeh and tofu (soy).[citation needed] High levels of dietary fiber, folic acid, vitamins C and E, and magnesium, and low consumption of saturated fat are all considered to be beneficial aspects of a vegetarian diet.[55][56] A well planned vegetarian diet will provide all nutrients in a meat-eater's diet to the same level for all stages of life.[57]		Protein intake in vegetarian diets is lower than in meat diets but can meet the daily requirements for most people.[58] Studies at Harvard University as well as other studies conducted in the United States, United Kingdom, Canada, Australia, New Zealand and various European countries, confirmed vegetarian diets provide sufficient protein intake as long as a variety of plant sources are available and consumed.[59] Pumpkin seeds, peanut butter, hemp seed, almonds, pistachio nuts, flaxseed, tofu, oats, soybeans, walnuts, are great sources of protein for vegetarians. Proteins are composed of amino acids, and a common concern with protein acquired from vegetable sources is an adequate intake of the essential amino acids, which cannot be synthesised by the human body. While dairy and egg products provide complete sources for ovo-lacto vegetarian, several vegetable sources have significant amounts of all eight types of essential amino acids, including lupin beans, soy,[60] hempseed, chia seed,[61] amaranth,[62] buckwheat,[63] pumpkin seeds[64] spirulina,[65] pistachios,[66] and quinoa.[67] However, the essential amino acids can also be obtained by eating a variety of complementary plant sources that, in combination, provide all eight essential amino acids (e.g. brown rice and beans, or hummus and whole wheat pita, though protein combining in the same meal is not necessary[citation needed]). A 1994 study found a varied intake of such sources can be adequate.[68]		Vegetarian diets typically contain similar levels of iron to non-vegetarian diets, but this has lower bioavailability than iron from meat sources, and its absorption can sometimes be inhibited by other dietary constituents.[69] According to the Vegetarian Resource Group, consuming food that contains vitamin C, such as citrus fruit or juices, tomatoes, or broccoli, is a good way to increase the amount of iron absorbed at a meal.[70] Vegetarian foods rich in iron include black beans, cashews, hempseed, kidney beans, broccoli, lentils, oatmeal, raisins, spinach, cabbage, lettuce, black-eyed peas, soybeans, many breakfast cereals, sunflower seeds, chickpeas, tomato juice, tempeh, molasses, thyme, and whole-wheat bread.[71] The related vegan diets can often be higher in iron than vegetarian diets, because dairy products are low in iron.[56] Iron stores often tend to be lower in vegetarians than non-vegetarians, and a few small studies report very high rates of iron deficiency (up to 40%,[72] and 58%[73] of the respective vegetarian or vegan groups). However, the American Dietetic Association states that iron deficiency is no more common in vegetarians than non-vegetarians (adult males are rarely iron deficient); iron deficiency anaemia is rare no matter the diet.[74]		According to the United States National Institutes of Health, vitamin B12 is not generally present in plants and is naturally found in foods of animal origin.[75] Lacto-ovo vegetarians can obtain B12 from dairy products and eggs, and vegans can obtain it from fortified foods (including some soy products and some breakfast cereals) and dietary supplements.[76][77][78][79][80] Vitamin B12 can also be obtained from fortified yeast extract products.[81]		The recommended dietary allowance of B12 in the United States is, per day, 0.4 mcg (0–6 months), rising to 1.8 mcg (9–13 years), 2.4 mcg (14+ years), and 2.8 mcg (lactating female).[75] While the body's daily requirement for vitamin B12 is very small, deficiency of the vitamin is very serious leading to anemia and irreversible nerve damage.[82]		Plant-based, or vegetarian, sources of Omega 3 fatty acids include soy, walnuts, pumpkin seeds, canola oil, kiwifruit, hempseed, algae, chia seed, flaxseed, echium seed and leafy vegetables such as lettuce, spinach, cabbage and purslane. Purslane contains more Omega 3 than any other known leafy green. Olives (and olive oil) are another important plant source of unsaturated fatty acids. Plant foods can provide alpha-linolenic acid which the human body uses to synthesize the long-chain n-3 fatty acids EPA and DHA. EPA and DHA can be obtained directly in high amounts from oily fish or fish oils. Vegetarians, and particularly vegans, have lower levels of EPA and DHA than meat-eaters. While the health effects of low levels of EPA and DHA are unknown, it is unlikely that supplementation with alpha-linolenic acid will significantly increase levels.[83][clarification needed] Recently, some companies have begun to market vegetarian DHA supplements containing seaweed extracts. Similar supplements providing both DHA and EPA have also begun to appear.[84] Whole seaweeds are not suitable for supplementation because their high iodine content limits the amount that may be safely consumed. However, certain algae such as spirulina are good sources of gamma-linolenic acid (GLA), alpha-linolenic acid (ALA), linoleic acid (LA), stearidonic acid (SDA), eicosapentaenoic acid (EPA), docosahexaenoic acid (DHA), and arachidonic acid (AA).[85][86]		Calcium intake in vegetarians and vegans can be similar to non-vegetarians, as long as the diet is properly planned.[87] Lacto-ovo vegetarians that include dairy products can still obtain calcium from dairy sources like milk, yogurt, and cheese.[88]		Non-dairy milks that are fortified with calcium, such as soymilk and almond milk can also contribute a significant amount of calcium in the diet.[89] The calcium found in broccoli, bok choy, and kale have also been found to have calcium that is well absorbed in the body.[87][88][90] Though the calcium content per serving is lower in these vegetables than a glass of milk, the absorption of the calcium into the body is higher.[88][90] Other foods that contain calcium include calcium-set tofu, blackstrap molasses, turnip greens, mustard greens, soybeans, tempeh, almonds, okra, dried figs, and tahini.[87][89] Though calcium can be found in Spinach, swiss chard, beans and beet greens, they are generally not considered to be a good source since the calcium binds to oxalic acid and is poorly absorbed into the body.[88] Phytic acid found in nuts, seeds, and beans may also impact calcium absorption rates.[88] See the National Institutes of Health Office of Dietary Supplements for calcium needs for various ages,[88] the Vegetarian Resource Group[89] and the Vegetarian Nutrition Calcium Fact Sheet from the Academy of Nutrition and Dietetics[87] for more specifics on how to obtain adequate calcium intake on a vegetarian or vegan diet.		Vitamin D needs can be met via the human body's own generation upon sufficient and sensible exposure to ultraviolet (UV) light in sunlight.[91][92] Products including milk, soy milk and cereal grains may be fortified to provide a source of Vitamin D.[93] For those who do not get adequate sun exposure or food sources, Vitamin D supplementation may be necessary.		Vitamin D2, or ergocalciferol is found in fungus (except alfalfa which is a plantae) and created from viosterol, which in turn is created when ultraviolet light activates ergosterol (which is found in fungi and named as a sterol from ergot). Any UV-irradiated fungus including yeast form vitamin D2.[96] Human bioavailability of vitamin D2 from vitamin D2-enhanced button mushrooms via UV-B irradiation is effective in improving vitamin D status and not different from a vitamin D2 supplement according to study.[97] For example, Vitamin D2 from UV-irradiated yeast baked into bread is bioavailable.[98] By visual assessment or using a chromometer, no significant discoloration of irradiated mushrooms, as measured by the degree of "whiteness", was observed[99] making it hard to discover if they have been treated without labeling. Claims have been made that a normal serving (approx. 3 oz or 1/2 cup, or 60 grams) of mushrooms treated with ultraviolet light increase their vitamin D content to levels up to 80 micrograms,[100] or 2700 IU if exposed to just 5 minutes of UV light after being harvested.[101]		There have been many comparative and statistical studies of the relationship between diet and longevity, including vegetarianism and longevity.		A 1999 metastudy combined data from five studies from western countries.[102] The metastudy reported mortality ratios, where lower numbers indicated fewer deaths, for fish eaters to be 0.82, vegetarians to be 0.84, occasional meat eaters (eat meat less than once per week) to be 0.84. Regular meat eaters had the base mortality rate of 1.0, while the number for vegans was very uncertain (anywhere between 0.7 and 1.44) due to too few data points. The study reported the numbers of deaths in each category, and expected error ranges for each ratio, and adjustments made to the data. However, the "lower mortality was due largely to the relatively low prevalence of smoking in these [vegetarian] cohorts". Out of the major causes of death studied, only one difference in mortality rate was attributed to the difference in diet, as the conclusion states: "...vegetarians had a 24% lower mortality from ischaemic heart disease than non-vegetarians, but no associations of a vegetarian diet with other major causes of death were established".[102]		In Mortality in British vegetarians,[103] a similar conclusion is drawn:		The Adventist Health Studies is ongoing research that documents the life expectancy in Seventh-day Adventists. This is the only study among others with similar methodology which had favourable indication for vegetarianism. The researchers found that a combination of different lifestyle choices could influence life expectancy by as much as 10 years. Among the lifestyle choices investigated, a vegetarian diet was estimated to confer an extra 1–1/2 to 2 years of life. The researchers concluded that "the life expectancies of California Adventist men and women are higher than those of any other well-described natural population" at 78.5 years for men and 82.3 years for women. The life expectancy of California Adventists surviving to age 30 was 83.3 years for men and 85.7 years for women.[105]		The Adventist health study is again incorporated into a metastudy titled "Does low meat consumption increase life expectancy in humans?" published in American Journal of Clinical Nutrition, which concluded that low meat eating (less than once per week) and other lifestyle choices significantly increase life expectancy, relative to a group with high meat intake. The study concluded that "The findings from one cohort of healthy adults raises the possibility that long-term (≥ 2 decades) adherence to a vegetarian diet can further produce a significant 3.6-y increase in life expectancy." However, the study also concluded that "Some of the variation in the survival advantage in vegetarians may have been due to marked differences between studies in adjustment for confounders, the definition of vegetarian, measurement error, age distribution, the healthy volunteer effect, and intake of specific plant foods by the vegetarians." It further states that "This raises the possibility that a low-meat, high plant-food dietary pattern may be the true causal protective factor rather than simply elimination of meat from the diet." In a recent review of studies relating low-meat diet patterns to all-cause mortality, Singh noted that "5 out of 5 studies indicated that adults who followed a low meat, high plant-food diet pattern experienced significant or marginally significant decreases in mortality risk relative to other patterns of intake."[106]		Statistical studies, such as comparing life expectancy with regional areas and local diets in Europe also have found life expectancy considerably greater in southern France, where a low meat, high plant Mediterranean diet is common, than northern France, where a diet with high meat content is more common.[107]		A study by the Institute of Preventive and Clinical Medicine, and Institute of Physiological Chemistry looked at a group of 19 vegetarians (lacto-ovo) and used as a comparison a group of 19 omnivorous subjects recruited from the same region. The study found that this group of vegetarians (lacto-ovo) have a significantly higher amount of plasma carboxymethyllysine and advanced glycation endproducts (AGEs) compared to this group of non-vegetarians.[108] Carboxymethyllysine is a glycation product which represents "a general marker of oxidative stress and long-term damage of proteins in aging, atherosclerosis and diabetes" and "[a]dvanced glycation end products (AGEs) may play an important adverse role in process of atherosclerosis, diabetes, aging and chronic renal failure".[108]		In Western medicine, patients are sometimes advised[by whom?] to adhere to a vegetarian diet.[109] According to studies by the Permanente Journal and the National Institute for Health (NIH), vegetarian diets are affordable and can help reduce health risks like high blood pressure, cardiovascular disease, and cholesterol levels. A plant based diet has the potential to lower the risk of heart disease as well as reducing the amount of medications prescribed in instances of chronic illness. A change to a plant based diet, or vegetarianism, has had dramatic positive effects on the health of patients with chronic illnesses, significantly more than exercise alone [110] Vegetarian diets have been used as a treatment for rheumatoid arthritis, but the evidence is inconclusive whether this is effective.[111] Certain alternative medicines, such as Ayurveda and Siddha, prescribe a vegetarian diet as a normal procedure. Maya Tiwari notes that Ayurveda recommends small portions of meat for some people, though "the rules of hunting and killing the animal, practiced by the native peoples, were very specific and detailed". Now that such methods of hunting and killing are not observed, she does not recommend the use of "any animal meat as food, not even for the Vata types".[112]		The human digestive system is omnivorous, capable of consuming a wide variety of plant and animal material.[113][114] Some nutritional experts believe that early hominids evolved into eating meat as a result of huge climatic changes that took place three to four million years ago, when forests and jungles dried up and became open grasslands and opened hunting and scavenging opportunities.[115][116][further explanation needed]		The American Dietetic Association has presented evidence that vegetarian diets may be more common among adolescents with eating disorders. At the same time the association cautions however, that the adoption of a vegetarian diet may not necessarily lead to eating disorders, rather that "vegetarian diets may be selected to camouflage an existing eating disorder".[117] Other studies and statements by dietitians and counselors support this conclusion.[nb 1][119]		Various ethical reasons have been suggested for choosing vegetarianism, usually predicated on the interests of non-human animals. In many societies, controversy and debate have arisen over the ethics of eating animals. Some people, while not vegetarians, refuse to eat the flesh of certain animals due to cultural taboo, such as cats, dogs, horses or rabbits. Others support meat eating for scientific, nutritional and cultural reasons, including religious ones. Some meat eaters abstain from the meat of animals reared in particular ways, such as factory farms, or avoid certain meats, such as veal or foie gras. Some people follow vegetarian or vegan diets not because of moral concerns involving the raising or consumption of animals in general, but because of concerns about the specific treatment and practises involved in the raising and slaughter of animals, i.e. factory farming and the industrialisation of animal slaughter. Others still avoid meat because meat production is claimed to place a greater burden on the environment than production of an equivalent amount of plant protein.		Ethical objections based on consideration for animals are generally divided into opposition to the act of killing in general, and opposition to certain agricultural practices surrounding the production of meat.		Princeton University professor and founder of the animal rights movement, Peter Singer, believes that if alternative means of survival exist, one ought to choose the option that does not cause unnecessary harm to animals. Most ethical vegetarians argue that the same reasons exist against killing animals to eat as against killing humans to eat. Singer, in his book Animal Liberation listed possible qualities of sentience in non-human creatures that gave such creatures the scope to be considered under utilitarian ethics, and this has been widely referenced by animal rights campaigners and vegetarians. Ethical vegetarians also believe that killing an animal, like killing a human, can only be justified in extreme circumstances and that consuming a living creature for its enjoyable taste, convenience, or nutrition value is not a sufficient cause. Another common view is that humans are morally conscious of their behaviour in a way other animals are not, and therefore subject to higher standards.[120]		Opponents of ethical vegetarianism argue that animals are not moral equals to humans and so consider the comparison of eating livestock with killing people to be fallacious. This view does not excuse cruelty, but maintains that animals do not possess the rights a human has.[121]		One of the main differences between a vegan and a typical vegetarian diet is the avoidance of both eggs and dairy products such as milk, cheese, butter and yogurt. Ethical vegans do not consume dairy or eggs because they state that their production causes the animal suffering or a premature death.[122]		To produce milk from dairy cattle, calves are separated from their mothers soon after birth and slaughtered or fed milk replacer in order to retain the cows milk for human consumption.[123] Vegans state that this breaks the natural mother and calf bond.[123] Unwanted male calves are either slaughtered at birth or sent for veal production.[123] To prolong lactation, dairy cows are almost permanently kept pregnant through artificial insemination.[123] After about five years, once the cows milk production has dropped, they are considered "spent" and sent to slaughter for beef and their hides. A dairy cow's natural life expectancy is about twenty years.[122]		In battery cage and free-range egg production, unwanted male chicks are culled or discarded at birth during the process of securing a further generation of egg-laying hens.[124]		Ethical vegetarianism has become popular in developed countries particularly because of the spread of factory farming, faster communications, and environmental consciousness. Some believe that the current mass demand for meat cannot be satisfied without a mass-production system that disregards the welfare of animals, while others believe that practices like well-managed free-ranging and consumption of game, particularly from species whose natural predators have been significantly eliminated, could substantially alleviate the demand for mass-produced meat.[citation needed]		Ancient Greek philosophy has a long tradition of vegetarianism. Pythagoras was reportedly vegetarian (and studied at Mt. Carmel, where some historians say there was a vegetarian community), as his followers were expected to be.		Roman writer Ovid concluded his magnum opus Metamorphoses, in part, with the impassioned argument (uttered by the character of Pythagoras) that in order for humanity to change, or metamorphose, into a better, more harmonious species, it must strive towards more humane tendencies. He cited vegetarianism as the crucial decision in this metamorphosis, explaining his belief that human life and animal life are so entwined that to kill an animal is virtually the same as killing a fellow human.		Everything changes; nothing dies; the soul roams to and fro, now here, now there, and takes what frame it will, passing from beast to man, from our own form to beast and never dies...Therefore lest appetite and greed destroy the bonds of love and duty, heed my message! Abstain! Never by slaughter dispossess souls that are kin and nourish blood with blood![125]		Jainism teaches vegetarianism as moral conduct as do some major[126] sects of Hinduism. Buddhism in general does not prohibit meat eating, while Mahayana Buddhism encourages vegetarianism as beneficial for developing compassion.[127] Other denominations that advocate a vegetarian diet include the Seventh-day Adventists, the Rastafari movement, the Ananda Marga movement and the Hare Krishnas. Sikhism[128][129][130] does not equate spirituality with diet and does not specify a vegetarian or meat diet.[131]		While there are no dietary restrictions in the Bahá'í Faith, `Abdu'l-Bahá, the son of the religion's founder, noted that a vegetarian diet consisting of fruits and grains was desirable, except for people with a weak constitution or those that are sick.[132] He stated that there are no requirements that Bahá'ís become vegetarian, but that a future society should gradually become vegetarian.[132][133][134] `Abdu'l-Bahá also stated that killing animals was contrary to compassion.[132] While Shoghi Effendi, the head of the Bahá'í Faith in the first half of the 20th century, stated that a purely vegetarian diet would be preferable since it avoided killing animals,[135] both he and the Universal House of Justice, the governing body of the Bahá'ís have stated that these teachings do not constitute a Bahá'í practice and that Bahá'ís can choose to eat whatever they wish but should be respectful of others' beliefs.[132]		Theravadins in general eat meat.[136] If Buddhist monks "see, hear or know" a living animal was killed specifically for them to eat, they must refuse it or else incur an offense.[137] However, this does not include eating meat which was given as alms or commercially purchased. In the Theravada canon, Buddha did not make any comment discouraging them from eating meat (except specific types, such as human, elephant meat, horse, dog, snake, lion, tiger, leopard, bear, and hyena flesh[138]) but he specifically refused to institute vegetarianism in his monastic code when a suggestion had been made.[139][140]		In several Sanskrit texts of Mahayana Buddhism, Buddha instructs his followers to avoid meat.[141][142][143][144] However, each branch of Mahayana Buddhism selects which sutra to follow, and some branches, including the majority of Tibetan and Japanese Buddhists, do eat meat, while many Chinese Buddhist branches do not.		Christians have always been free to make their own decisions about what to eat; however, there are groups within Christianity that practice specific dietary restrictions for various reasons.[145] The early sect known as the Ebionites are considered to have practiced vegetarianism. Surviving fragments from their Gospel indicate their belief that – as Christ is the Passover sacrifice and eating the Passover lamb is no longer required – a vegetarian diet may (or should) be observed. However, orthodox Christianity does not accept their teaching as authentic. Indeed, their specific injunction to strict vegetarianism was cited as one of the Ebionites' "errors".[146][147]		At a much later time, the Bible Christian Church founded by Reverend William Cowherd in 1809 followed a vegetarian diet.[148] Cowherd was one of the philosophical forerunners of the Vegetarian Society.[149] Cowherd encouraged members to abstain from eating of meat as a form of temperance.[150]		Seventh-day Adventists are encouraged to engage in healthy eating practices, and ova-lacto-vegetarian diets are recommended by the General Conference of Seventh-day Adventists Nutrition Council (GCNC). They have also sponsored and participated in many scientific studies exploring the impact of dietary decisions upon health outcomes.[151] The GCNC has in addition adapted the USDA's food pyramid for a vegetarian dietary approach.[151][152] However, the only kinds of meat specifically frowned upon by the SDA health message are unclean meats, or those forbidden in scripture.[153]		Additionally, some monastic orders follow a vegetarian diet, and members of the Orthodox Church follow a vegan diet during fasts.[154] There is also a strong association between the Quakers and vegetarianism dating back at least to the 18th century. The association grew in prominence during the 19th century, coupled with growing Quaker concerns in connection with alcohol consumption, anti-vivisection and social purity. The association between the Quaker tradition and vegetarianism, however, becomes most significant with the founding of the Friends' Vegetarian Society in 1902 "to spread a kindlier way of living amongst the Society of Friends."[155]		According to Canon Law, Roman Catholics are required to abstain from meat (defined as all animal flesh excluding water animals) on Ash Wednesday and all Fridays of Lent including Good Friday. Canon Law also obliges Catholics to abstain from meat on the Fridays of the year outside of Lent (excluding certain holy days) unless, with the permission of the local conference of bishops, another penitential act is substituted. The restrictions on eating meat on these days is solely as an act of penance and not because of a religious objection to eating meat.[156]		Since the formation of the Seventh-day Adventist Church in the 1860s when the church began, wholeness and health have been an emphasis of the Adventist church, and has been known as the "health message" belief of the church.[157] Adventists are well known for presenting a health message that recommends vegetarianism and expects adherence to the kosher laws in Leviticus 11. Obedience to these laws means abstinence from pork, shellfish, and other animals proscribed as "unclean". The church discourages its members from consuming alcoholic beverages, tobacco or illegal drugs (compare Christianity and alcohol). In addition, some Adventists avoid coffee, tea, cola, and other beverages containing caffeine. Adventist who follow the vegetarian lifestyle tend to live up to 10 years longer than people who do eat meat.[158] Adventists believe in taking care of one's body and eating as God intended from the Garden of Eden, all natural fruits, grains, nuts and vegetables.		The pioneers of the Adventist Church had much to do with the common acceptance of breakfast cereals into the Western diet, and the "modern commercial concept of cereal food" originated among Adventists.[159] John Harvey Kellogg was one of the early founders of Adventist health work. His development of breakfast cereals as a health food led to the founding of Kellogg's by his brother William. In both Australia and New Zealand, the church-owned Sanitarium Health and Wellbeing Company is a leading manufacturer of health and vegetarian-related products, most prominently Weet-Bix.		Research funded by the U.S. National Institutes of Health has shown that the average Adventist in California lives 4 to 10 years longer than the average Californian. The research, as cited by the cover story of the November 2005 issue of National Geographic, asserts that Adventists live longer because they do not smoke or drink alcohol, have a day of rest every week, and maintain a healthy, low-fat vegetarian diet that is rich in nuts and beans.[160][161] The cohesiveness of Adventists' social networks has also been put forward as an explanation for their extended lifespan.[162] Since Dan Buettner's 2005 National Geographic story about Adventist longevity, his book, The Blue Zones: Lessons for Living Longer From the People Who've Lived the Longest, named Loma Linda, California a "blue zone" because of the large concentration of Seventh-day Adventists. He cites the Adventist emphasis on health, diet, and Sabbath-keeping as primary factors for Adventist longevity.[163][164]		An estimated 35% of Adventists practice vegetarianism or veganism, according to a 2002 worldwide survey of local church leaders.[165][166]		Adventists' clean lifestyles were recognized by the U.S. military in 1954 when 2,200 Adventists volunteered for Operation Whitecoat to be human test subjects for a range of diseases the effects of which were still unknown:		The first task for the scientists was to find people willing to be infected by pathogens that could make them very sick. They found them in the followers of the Seventh-day Adventist faith. Although willing to serve their country when drafted, the Adventists refused to bear arms. As a result many of them became medics. Now the U.S. was offering recruits an opportunity to help in a different manner: to volunteer for biological tests as a way of satisfying their military obligations. When contacted in late 1954, the Adventist hierarchy readily agreed to this plan. For Camp Detrick scientists, church members were a model test population, since most of them were in excellent health and they neither drank, smoked, nor used caffeine. From the perspective of the volunteers, the tests gave them a way to fulfill their patriotic duty while remaining true to their beliefs.[167]		Hinduism is more of a practice that was classified as a religion. Though there is no strict rule on what to consume and what not to, paths of Hinduism hold vegetarianism as an ideal influenced by Jains. Some reasons are: the principle of nonviolence (ahimsa) applied to animals;[168] the intention to offer only "pure" (vegetarian) food to a deity and then to receive it back as prasad; and the conviction that a sentient diet is beneficial for a healthy body and mind and that non-vegetarian food is not recommended for a better mind and for spiritual development. Other reasons maybe lack of availability of meat and forced diet by rulers.		However, the food habits of Hindus vary according to their community, location, custom and varying traditions. Historically and currently, those Hindus who eat meat prescribe Jhatka meat,[169] while some Hindus believe that the cow is a holy animal whose slaughter for meat is forbidden. This belief varies according to region.[170]		Some followers of Islam, or Muslims, chose to be vegetarian for health, ethical, or personal reasons. However, the choice to become vegetarian for non-medical reasons can sometimes be controversial due to conflicting fatwas and differing interpretations of the Quran. Though some more traditional Muslims may keep quiet about their vegetarian diet, the number of vegetarian Muslims is increasing.[171][172]		Vegetarianism has been practiced by some influential Muslims including the Iraqi theologian, female mystic and poet Râbi‘ah al-‘Adawîyah of Basrah, who died in the year 801, and the Sri Lankan Sufi master Bawa Muhaiyaddeen who established The Bawa Muhaiyaddeen Fellowship of North America in Philadelphia. The former Indian president Dr. A. P. J. Abdul Kalam was also famously a vegetarian.[173]		In January 1996, The International Vegetarian Union announced the formation of the Muslim Vegetarian/Vegan Society.[174]		Many non-vegetarian Muslims will select vegetarian (or seafood) options when dining in non-halal restaurants. However, this is a matter of not having the right kind of meat rather than preferring not to eat meat on the whole.[172]		Followers of Jainism believe that all living organisms whether they are micro-organism are living and have a soul, and have one or more senses out of five senses and they go to great lengths to minimise any harm to any living organism. Most Jains are lacto-vegetarians but more devout Jains do not eat root vegetables because they believe that root vegetables contain a lot more micro-organisms as compared to other vegetables, and that, by eating them, violence of these micro-organisms is inevitable. So they focus on eating beans and fruits, whose cultivation do not involve killing of a lot of micro-organisms. No products obtained from dead animals are allowed, because when a living beings dies, a lot of micro-organisms (called as decomposers) will reproduce in the body which decomposes the body, and in eating the dead bodies, violence of decomposers is inevitable. Jain monks usually do a lot of fasting, and when they knew through spiritual powers that their life is very little, they start fasting until death.[175][176] Some particularly dedicated individuals are fruitarians.[177] Honey is forbidden, because honey is the regurgitation of nectar by bees [178] and may also contain eggs, excreta and dead bees. Some Jains do not consume plant parts that grow underground such as roots and bulbs, because tiny animals may be killed when the plants are pulled up.[179]		While it is neither required (required only on special holidays [Pessach, Sukot and Shavuot] according to some traditions, but not on Shabbat [Friday], when just bread and wine/grape juice is required) nor prohibited for Jews to eat meat, a number of medieval scholars of Jewish religion (e.g., Joseph Albo and Isaac Arama) regard vegetarianism as a moral ideal, not just because of a concern for the welfare of animals, but because the slaughter of animals might cause the individual who performs such acts to develop negative character traits. One modern-day scholar who is in favour of vegetarianism is the late Rabbi Abraham Isaac Kook, the Chief Rabbi of Mandate Palestine. In his writings, Rabbi Kook speaks of vegetarianism as an ideal, and points to the fact that Adam did not partake of the flesh of animals, as all humans and animals were originally commanded by God to only eat plants.[180] In context, Rabbi Kook makes those comments in his portrayal of the eschatological (messianic) era. However, he personally refrained from eating meat except on the Sabbath and Festivals, and one of his leading disciples, Rabbi David Cohen, known as the "Nazirite" of Jerusalem, was a devout vegetarian. Several other members of Rabbi Kook's circle were also vegetarians.		According to some Kabbalists, only a mystic, who is able to sense and elevate the reincarnated human souls and "divine sparks", is permitted to consume meat, though eating the flesh of an animal might still cause spiritual damage to the soul. A number of Orthodox Jewish vegetarian groups and activists promote such ideas and believe that the halakhic permission to eat meat is a temporary leniency for those who are not ready yet to accept the vegetarian diet.[181][182] Jewish law also commands people to ritually slaughter animals when killing them, and goes into precise detail on the rituals of both animal sacrifice and ordinary slaughter (shechita). According to medieval sage Rabbi Shlomo Ephraim Luntschitz, author of the Torah commentary Kli Yakar, the complexity of these laws was intended to discourage the consumption of meat and make it less painful for the animals.[183]		Within the Afro-Caribbean community, a minority are Rastafari and follow the dietary regulations with varying degrees of strictness. The most orthodox eat only "Ital" or natural foods, in which the matching of herbs or spices with vegetables is the result of long tradition originating from the African ancestry and cultural heritage of Rastafari.[184] "Ital", which is derived from the word vital, means essential to human existence. Ital cooking in its strictest form prohibits the use of salt, meat (especially pork), preservatives, colorings, flavorings and anything artificial.[185] Most Rastafari are vegetarian.[186]		The tenets of Sikhism do not advocate a particular stance on either vegetarianism or the consumption of meat,[187][188][189][190] but leave the decision of diet to the individual.[191] The tenth guru, Guru Gobind Singh, however, prohibited "Amritdhari" Sikhs, or those that follow the Sikh Rehat Maryada (the Official Sikh Code of Conduct)[192] from eating Kutha meat, or meat which has been obtained from animals which have been killed in a ritualistic way. This is understood to have been for the political reason of maintaining independence from the then-new Muslim hegemony, as Muslims largely adhere to the ritualistic halal diet.[187][191]		"Amritdharis" that belong to some Sikh sects (e.g. Akhand Kirtani Jatha, Damdami Taksal, Namdhari[193] and Rarionwalay,[194] etc.) are vehemently against the consumption of meat and eggs (though they do consume and encourage the consumption of milk, butter and cheese).[195] This vegetarian stance has been traced back to the times of the British Raj, with the advent of many new Vaishnava converts.[191] In response to the varying views on diet throughout the Sikh population, Sikh Gurus have sought to clarify the Sikh view on diet, stressing their preference only for simplicity of diet. Guru Nanak said that over-consumption of food (Lobh, Greed) involves a drain on the Earth's resources and thus on life.[196][197] Passages from the Guru Granth Sahib (the holy book of Sikhs, also known as the Adi Granth) say that it is "foolish" to argue for the superiority of animal life, because though all life is related, only human life carries more importance: "Only fools argue whether to eat meat or not. Who can define what is meat and what is not meat? Who knows where the sin lies, being a vegetarian or a non-vegetarian?"[191] The Sikh langar, or free temple meal, is largely lacto-vegetarian, though this is understood to be a result of efforts to present a meal that is respectful of the diets of any person who would wish to dine, rather than out of dogma.[190][191]		Environmental vegetarianism is based on the concern that the production of meat and animal products for mass consumption, especially through factory farming, is environmentally unsustainable. According to a 2006 United Nations initiative, the livestock industry is one of the largest contributors to environmental degradation worldwide, and modern practices of raising animals for food contribute on a "massive scale" to air and water pollution, land degradation, climate change, and loss of biodiversity. The initiative concluded that "the livestock sector emerges as one of the top two or three most significant contributors to the most serious environmental problems, at every scale from local to global."[198]		In addition, animal agriculture is a large source of greenhouse gases. According to a 2006 report it is responsible for 18% of the world's greenhouse gas emissions as estimated in 100-year CO2 equivalents. Livestock sources (including enteric fermentation and manure) account for about 3.1 percent of US anthropogenic GHG emissions expressed as carbon dioxide equivalents.[199] This EPA estimate is based on methodologies agreed to by the Conference of Parties of the UNFCCC, with 100-year global warming potentials from the IPCC Second Assessment Report used in estimating GHG emissions as carbon dioxide equivalents.		Meat produced in a laboratory (called in vitro meat) may be more environmentally sustainable than regularly produced meat.[200] Reactions of vegetarians vary.[201] Rearing a relatively small number of grazing animals can be beneficial, as the Food Climate Research Network at Surrey University reports: "A little bit of livestock production is probably a good thing for the environment.[202]		In May 2009, Ghent, Belgium, was reported to be "the first [city] in the world to go vegetarian at least once a week" for environmental reasons, when local authorities decided to implement a "weekly meatless day". Civil servants would eat vegetarian meals one day per week, in recognition of the United Nations' report. Posters were put up by local authorities to encourage the population to take part on vegetarian days, and "veggie street maps" were printed to highlight vegetarian restaurants. In September 2009, schools in Ghent are due to have a weekly veggiedag ("vegetarian day") too.[203]		Some groups, such as PETA, promote vegetarianism as a way to offset poor treatment and working conditions of workers in the contemporary meat industry.[204] These groups cite studies showing the psychological damage caused by working in the meat industry, especially in factory and industrialised settings, and argue that the meat industry violates its labourers' human rights by assigning difficult and distressing tasks without adequate counselling, training and debriefing.[205][206][207] However, the working conditions of agricultural workers as a whole, particularly non-permanent workers, remain poor and well below conditions prevailing in other economic sectors.[208] Accidents, including pesticide poisoning, among farmers and plantation workers contribute to increased health risks, including increased mortality.[209] According to the International Labour Organization, agriculture is one of the three most dangerous jobs in the world.[210]		Similar to environmental vegetarianism is the concept of economic vegetarianism. An economic vegetarian is someone who practices vegetarianism from either the philosophical viewpoint concerning issues such as public health and curbing world starvation, the belief that the consumption of meat is economically unsound, part of a conscious simple living strategy or just out of necessity. According to the Worldwatch Institute, "Massive reductions in meat consumption in industrial nations will ease their health care burden while improving public health; declining livestock herds will take pressure off rangelands and grainlands, allowing the agricultural resource base to rejuvenate. As populations grow, lowering meat consumption worldwide will allow more efficient use of declining per capita land and water resources, while at the same time making grain more affordable to the world's chronically hungry."[211]		Prejudice researcher Gordon Hodson observes that vegetarians and vegans frequently face discrimination where eating meat is held as a cultural norm.[212]		A 1992 market research study conducted by the Yankelovich research organisation concluded that "of the 12.4 million people [in the US] who call themselves vegetarian, 68% are female, while only 32% are male".[213]		At least one study indicates that vegetarian women are more likely to have female babies. A study of 6,000 pregnant women in 1998 "found that while the national average in Britain is 106 boys born to every 100 girls, for vegetarian mothers the ratio was just 85 boys to 100 girls".[214] Catherine Collins of the British Dietetic Association has dismissed this as a "statistical fluke" given that it is actually the male's genetic contribution which determines the sex of a baby.[214]		
Drinking is the act of ingesting water or other liquids into the body through the mouth. Water is required for many of life’s physiological processes. Both excessive and inadequate water intake are associated with health problems.						When a liquid is poured into an open human mouth, the swallowing process is completed by peristalsis which delivers the liquid to the stomach; much of the activity is abetted by gravity. The liquid may be poured from the hands or drinkware may be used as vessels. Drinking can also be performed by acts of inhalation, typically when imbibing hot liquids or drinking from a spoon. Infants employ a method of suction wherein the lips are pressed tight around a source, as in breastfeeding: a combination of breath and tongue movement creates a vacuum which draws in liquid.[1]		Amphibians and aquatic animals which live in freshwater do not need to drink: they absorb water steadily through the skin by osmosis.[2][3] Saltwater fish, however, drink through the mouth as they swim, and purge the excess salt through the gills.[3]		By necessity, terrestrial animals in captivity become accustomed to drinking water, but most free-roaming animals stay hydrated through the fluids and moisture in fresh food.[4] When conditions impel them to drink from bodies of water, the methods and motions differ greatly among species.[2] Many desert animals do not drink even if water becomes available, but rely on eating succulent plants.[2]		Cats, canines, and ruminants all lower the neck and lap in water with their powerful tongues.[2] Cats and canines lap up water with the tongue in a spoon-like shape.[5] Ruminants and most other herbivores partially submerge the tip of the mouth in order to draw in water by means of a plunging action with the tongue held straight.[6] Cats drink at a significantly slower pace than ruminants, who face greater natural predation hazards.[2] Uniquely, elephants draw water into their trunks and squirt it into their mouths.[2]		Most birds scoop or draw water into the buccal areas of their bills, raising and tilting their heads back to drink. An exception is the common pigeon which can suck in water directly by inhalation.[2]		Like nearly all other life forms, humans require water for tissue hydration. Lack of hydration causes thirst, a desire to drink which is regulated by the hypothalamus in response to subtle changes in the body's electrolyte levels and blood volume. A decline in total body water is called dehydration and will eventually lead to death by hypernatremia. Methods used in the management of dehydration include assisted drinking or oral rehydration therapy.		An overconsumption of water can lead to water intoxication, which can dangerously dilute the concentration of salts in the body. Overhydration sometimes occurs among athletes and outdoor laborers, but it can also be a sign of disease or damage to the hypothalamus. A persistent desire to drink inordinate quantities of water is a psychological condition termed polydipsia. It is often accompanied by polyuria and may itself be a symptom of Diabetes mellitus or Diabetes insipidus.[7]		A daily intake of water is required for the normal physiological functioning of the human body. The USDA recommends a daily intake of total water: not necessarily by drinking but by consumption of water contained in other beverages and foods. The recommended intake is 3.7 liters (appx. 1 gallon) per day for an adult male, and 2.7 liters (appx. 0.75 gallon) for an adult female.[8] Other sources, however, claim that a high intake of fresh drinking water, separate and distinct from other sources of moisture, is necessary for good health – eight servings per day of eight fluid ounces (1.8 liters, or 0.5 gallon) is the amount recommended by many nutritionists,[9] although there is no scientific evidence supporting this recommendation.[10][11]		The term “drinking” is often used metonymically for the consumption of alcoholic beverages. Most cultures throughout history have incorporated some number of the wide variety of "strong drinks" into their meals, celebrations, ceremonies, toasts and other occasions.[12] Evidence of fermented drinks in human culture goes back as early as the Neolithic Period,[13] and the first pictorial evidence can be found in Egypt around 4,000 BC.[14]		The drinking of alcohol has developed into a variety of well-established drinking cultures around the world. Despite its popularity, alcohol consumption poses significant health risks. Alcohol abuse and the addiction of alcoholism are common maladies in developed countries worldwide.[15] A high rate of consumption can also lead to cirrhosis, gastritis, gout, pancreatitis, hypertension, various forms of cancer, and numerous other illnesses.[16]		
Meal preparation, sometimes called "meal prep," is the process of planning and preparing meals. Most people will do meal preparation on Sunday, but there is an increasing trend of mid-week meal preparation, where meals are prepared once on Sunday, for Mon-Wed, then again on Wednesday for the rest of the week.[citation needed] If one is meal prepping with highly perishable items such as leafy greens, one may want to consider a mid-week meal prep routine.						Sometimes meal preparation involves preparing meals ahead of time for a short or extended period of time.[1] This practice may occur among people who desire to lose weight or maintain a healthy lifestyle. Advance preparation can serve to standardize food portions. Sometimes meals are fully cooked, other times they are not.[2] Meals may be prepared in small containers such as tupperware, and are sometimes labeled.		
A Supra (Georgian: სუფრა [supʰra]) is a traditional Georgian feast and an important part of Georgian social culture. There are two types of supra: a festive supra (ლხინის სუფრა, [lxinis supʰra]), called a keipi, and a sombre supra (ჭირის სუფრა, [tʃʼɪrɪs sʊpʰra]), called a kelekhi, that is always held after burials.						In Georgian, "supra" means "table-cloth". It's likely related to the Arabic sufra (سفرة), Armenian sproc (սփրոց) and Turkish sofra, which are words for traditional eating surfaces.[citation needed] Large public meals are never held in Georgia without a supra; when there are no tables, the supra is laid on the ground.		Regardless of size and type, a supra is always led by a tamada, or toastmaster, who introduces each toast during the feast. The tamada is elected by the banqueting guests or chosen by the host. A successful tamada must possess great rhetorical skill and be able to consume a large amount of alcohol without showing signs of drunkenness.[1] During the meal, the tamada will propose a toast, and then speak at some length about the topic. The guests raise their glasses, but do not drink. After the tamada has spoken, the toast continues, often in a generally counter-clockwise direction (to the right). The next guest who wishes to speak raises their glass, holds forth, and then drains their glass. If a guest does not wish to speak, they may drink from their glass after some words that particularly resonate for him or her. Eating is entirely appropriate during toasts, but talking is frowned upon. Once everyone who wishes to speak on the theme has done so, the tamada proposes a new toast, and the cycle begins again. Some popular traditional themes include toasts to God, Georgia, family, the mother of God, various saints, friends, ancestors, and so on. However, the theme of each toast is up to the tamada, who should be able to tailor his or her toasts to the occasion.		A keipi toast is called sadghegrdzelo (სადღეგრძელო, [sadɣɛɡrdzɛlɔ]), while a kelekhi toast is called a shesandobari (შესანდობარი, [ʃɛsandɔbarɪ]).		
In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the ISO.[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.		A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.		The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.[4][5][6]		The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.						A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]		For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).		DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.		The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.		The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".[15] Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL provides the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18] This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.		Major applications of the DOI system currently include:		In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]		A multilingual European DOI registration agency activity, mEDRA, and a Chinese registration agency, Wanfang Data, are active in non-English language markets.		The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.		The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[20] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[21] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[22]		The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.		The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.		A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[23]		A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[24]		The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).		A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.		DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.		To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.		Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[25] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.		Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[26][27] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[28] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[26][28]		An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[29] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.		The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[30] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.		The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.		Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[31]		Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.		The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[32] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[33] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[34] The final standard was published on 23 April 2012.[1]		DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[35]		The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[36]		The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:		URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.		
Dinner usually refers to the most significant and important meal of the day, which can be the noon or the evening meal. However, the term "dinner" can have many different meanings depending on the culture; it may mean a meal of any size eaten at any time of the day.[1][2] Historically, it referred to the first meal of the day, eaten around noon, and is still sometimes used for a noon-time meal, particularly if it is a large or main meal. The meaning as the evening meal, generally the largest of the day, is becoming a standard in many parts of the English-speaking world.						The word is from the Old French (c. 1300) disner, meaning "dine", from the stem of Gallo-Romance desjunare ("to break one's fast"), from Latin dis- (which indicates the opposite of an action) + Late Latin ieiunare ("to fast"), from Latin ieiunus ("fasting, hungry").[3][4] The Romanian word dejun and the French déjeuner retain this etymology and to some extent the meaning (whereas the Spanish word desayuno and Portuguese desjejum are related but are exclusively used for breakfast). Eventually, the term shifted to referring to the heavy main meal of the day, even if it had been preceded by a breakfast meal (or even both breakfast and lunch).		In Europe, the fashionable hour for dinner began to be incrementally postponed during the 18th century, to two and three in the afternoon, until at the time of the First French Empire an English traveler to Paris remarked upon the "abominable habit of dining as late as seven in the evening".[5]		In many modern usages, the term dinner refers to the evening meal, which is now often the most significant meal of the day in English-speaking cultures. When this meaning is used, the preceding meals are usually referred to as breakfast, lunch and tea. In some areas, the tradition of using dinner to mean the most important meal of the day regardless of time of day leads to a variable name for meals depending on the combination of their size and the time of day, while in others meal names are fixed based on the time they are consumed.		The divide between different meanings of "dinner" is not cut-and-dried based on either geography or socioeconomic class. However, the use of the term dinner for the midday meal is strongest among working-class people, especially in the English Midlands, North of England and the central belt of Scotland.[6] Even in systems in which dinner is the meal usually eaten at the end of the day, an individual dinner may still refer to a main or more sophisticated meal at any time in the day, such as a banquet, feast, or a special meal eaten on a Sunday or holiday, such as Christmas dinner or Thanksgiving dinner. At such a dinner the people who dine together may be formally dressed and consume food with an array of utensils. These dinners are often divided into three or more courses. Appetizers consisting of options such as soup, salad etc., precede the main course, which is followed by the dessert.		A survey by Jacob's Creek, an Australian winemaker, found the average evening meal time in the U.K. to be 7:47pm.[7]		A dinner party is a social gathering at which people congregate to eat dinner.		During the times of Ancient Rome, a dinner party was referred to as a convivia, and was a significant event for Roman emperors and senators to congregate and discuss their relations.[8] The Romans often ate and were also very fond of fish sauce called liquamen (also known as Garum) during said parties.[citation needed]		In greater London, England (c. 1875–c. 1900), dinner parties were sometimes formal occasions that included printed invitations and formal RSVPs.[9] The food served at these parties ranged from large, extravagant food displays and several meal courses to more simple fare and food service.[9] Activities sometimes included singing and poetry reciting, among others.[9]		
A tablecloth is a cloth used to cover a table. Some are mainly ornamental coverings, which may also help protect the table from scratches and stains. Other tablecloths are designed to be spread on a dining table before laying out tableware and food.		Tablecloths can be made of almost any material, including delicate fabrics like embroidered silk. Dining cloths are typically made of cotton, a poly-cotton blend, or a PVC-coated material that can be wiped clean, but they can range from functional coverings to fine textiles, as long as they can be laundered. Some cloths are designed as part of an overall table setting, with coordinating napkins, placemats, or other decorative pieces. Special kinds of tablecloth include runners which overhang the table at two ends only and table protectors to provide a padded layer under a normal cloth.						In many European cultures a white, or mainly white, tablecloth used to be the standard covering for a dinner table. In the later medieval period spreading a high quality white linen or cotton cloth was an important part of preparing for a feast in a wealthy household. Over time the custom of arranging tableware on a cloth became common for most social classes except the very poorest. As eating habits changed in the 20th century, a much greater range of table-setting styles developed. Some formal dinners still use white tablecloths, often with a damask weave, but other colours and patterns are possible.		Perugia tablecloths and napkins have been made since medieval times. White with characteristic woven blue stripes and patterns, the style is also associated with church linen.[1][2]		Victorian interiors were full of thick, fringed draperies in deep colours, including tablecloths reaching to the floor on any kind of table.[3]		A popular "magic trick" involved pulling a loaded tablecloth away from a table but leaving the plates behind. This trick relies on inertia.		
A combination meal, also referred to as a combo meal,[1] is a type of meal that typically includes food items and a beverage. They are a common menu item at fast food restaurants, and other restaurants also purvey them. Combination meals may be priced lower compared to ordering items separately, but this is not always the case. A combination meal is also a meal in which the consumer orders items à la carte to create their own meal combination. The casada is a common type of lunch combination meal in Costa Rica and Panama.						Fast food combination meals typically include an entree such as a hamburger, a side dish such as fries, and a beverage such as a soft drink.[2] Other types of restaurants, such as fast-casual restaurants also offer combination meals.[3]		Combination meals may be priced lower compared to ordering the items separately, and this lower pricing may serve to entice consumers that are budget-minded.[2][4] A 2010 study published in the Journal of Public Policy & Marketing found that some consumers may order a combination meal even if no price discount is applied compared to the price of ordering items separately.[3] The study found that this behavior is based upon consumers perceiving an inherent value in combination meals, and also suggested that the ease and convenience of ordering, such as ordering a meal by number, plays a role compared to ordering items separately.[3] This study also found that the presence of combination meals encourages consumers to increase meal portion size by supersizing their meals.[3]		A combination meal can also comprise a meal in which separate dishes are selected by consumers from an entire menu, and can include à la carte selections that are combined on a plate.[5] A fast food combination meal can contain over 1,300 calories.[6] Fast food restaurants sometimes offer a means to order larger portions of food within the format of the combination meal, such as supersizing.[7]		In the United States in the early 1930s, the combination meal was a popular dish in restaurants and in homes.[a]		In Costa Rican and Panamanian cuisine, a combination meal is referred to as a casado, which means "married".[9][10] It is a typical lunch dish in both countries.[9][10] In Costa Rica, a casado typically consists of a meat dish, rice and beans, and deli salads.[9] Additional foods comprising the Costa Rican casado can include fried plantain, noodles and tomatoes.[11] In Costa Rica, the term plato del diá (plate of the day) is frequently used interchangeably with the term casado.[11]		In Panama, a casado typically consists of an entree, rice and beans, and cabbage.[10] In Panama, the plato executive, which means "executive plate", is a prix fixe (fixed price) lunch menu offered in some upscale restaurants that is similar in concept to the casado.[10]		
Sharia, Sharia law, or Islamic law (Arabic: شريعة‎‎ (IPA: [ʃaˈriːʕa])) is the religious law forming part of the Islamic tradition.[1] It is derived from the religious precepts of Islam, particularly the Quran and the Hadith. In Arabic, the term sharīʿah refers to God's immutable divine law and is contrasted with fiqh, which refers to its human scholarly interpretations.[2][3][4] The manner of its application in modern times has been a subject of dispute between Muslim traditionalists and reformists.[1]		Traditional theory of Islamic jurisprudence recognizes four sources of sharia: the Quran, sunnah (authentic hadith), qiyas (analogical reasoning), and ijma (juridical consensus).[5] Different legal schools—of which the most prominent are Hanafi, Maliki, Shafi'i, Hanbali and Jafari—developed methodologies for deriving sharia rulings from scriptural sources using a process known as ijtihad.[2][3] Traditional jurisprudence distinguishes two principal branches of law, ʿibādāt (rituals) and muʿāmalāt (social relations), which together comprise a wide range of topics.[2][4] Its rulings assign actions to one of five categories: mandatory, recommended, neutral, abhorred, and prohibited.[2][3][4] Thus, some areas of sharia overlap with the Western notion of law while others correspond more broadly to living life in accordance with God’s will.[3]		Historically, sharia was interpreted by independent jurists (muftis). Their legal opinions (fatwas) were taken into account by ruler-appointed judges who presided over qāḍī's courts, and by maẓālim courts, which were controlled by the ruler's council and administered criminal law.[2][3][4] Ottoman rulers achieved additional control over the legal system by promulgating their own legal code (qanun) and turning muftis into state employees.[3] Non-Muslim (dhimmi) communities had legal autonomy, except in cases of interconfessional disputes, which fell under jurisdiction of qadi's courts.[3]		In the modern era, sharia-based criminal laws were widely replaced by statutes inspired by European models.[3][6] Judicial procedures and legal education in the Muslim world were likewise brought in line with European practice.[3] While the constitutions of most Muslim-majority states contain references to sharia, its classical rules were largely retained only in personal status (family) laws.[3] Legislative bodies which codified these laws sought to modernize them without abandoning their foundations in traditional jurisprudence.[3][7] The Islamic revival of the late 20th century brought along calls by Islamist movements for full implementation of sharia, including reinstatement of hudud corporal punishments, such as stoning.[3][7] In some cases, this resulted in traditionalist legal reform,[note 1] while other countries witnessed juridical reinterpretation of sharia advocated by progressive reformers.[3][7][9]		The role of sharia has become a contested topic around the world. Attempts to impose it on non-Muslims have caused intercommunal violence in Nigeria[10][11] and may have contributed to the breakup of Sudan.[3] Some Muslim-minority countries in Asia (such as Israel[12]), Africa, and Europe recognize the use of sharia-based family laws for their Muslim populations.[13][14] Some jurisdictions in North America have passed bans on use of sharia, framed as restrictions on religious or foreign laws.[15] There are ongoing debates as to whether sharia is compatible with secular forms of government, human rights, freedom of thought, and women's rights.[16][17][18]						The word sharīʿah is used by Arabic-speaking peoples of the Middle East to designate a prophetic religion in its totality.[19] For example, sharīʿat Mūsā means law or religion of Moses and sharīʿatu-nā can mean "our religion" in reference to any monotheistic faith.[19] Within Islamic discourse, šarīʿah refers to religious regulations governing the lives of Muslims.[19] For many Muslims, the word means simply "justice", and they will consider any law that promotes justice and social welfare to conform to sharia.[3]		Jan Michiel Otto distinguishes four senses conveyed by the term sharia in religious, legal and political discourse:[20]		A related term al-qānūn al-islāmī (القانون الإسلامي, Islamic law), which was borrowed from European usage in the late 19th century, is used in the Muslim world to refer to a legal system in the context of a modern state.[21]		The primary range of meanings of the Arabic word šarīʿah, derived from the root š-r-ʕ, is related to religion and religious law.[19] The lexicographical tradition records two major areas of use where the word šarīʿah can appear without religious connotation.[22] In texts evoking a pastoral or nomadic environment, the word and its derivatives refer to watering animals at a permanent water-hole or to the seashore, with special reference to animals who come there.[22] Another area of use relates to notions of stretched or lengthy.[22] This range of meanings is cognate with the Hebrew saraʿ and is likely to be the origin of the meaning "way" or "path".[22] Both these areas have been claimed to have given rise to aspects of the religious meaning.[22]		Some scholars describe the word šarīʿah as an archaic Arabic word denoting "pathway to be followed" (analogous to the Hebrew term Halakhah ["The Way to Go"]),[23] or "path to the water hole"[24][25] and argue that its adoption as a metaphor for a divinely ordained way of life arises from the importance of water in an arid desert environment.[25]		In the Quran, šarīʿah and its cognate širʿah occur once each, with the meaning "way" or "path".[19] The word šarīʿah was widely used by Arabic-speaking Jews during the Middle Ages, being the most common translation for the word torah in the 10th century Arabic Old Testament known as Saʿadya Gaon.[19] A similar use of the term can be found in Christian writers.[19] The Arabic expression Sharīʿat Allāh (شريعة الله "God’s Law") is a common translation for תורת אלוהים (‘God’s Law’ in Hebrew) and νόμος τοῦ θεοῦ (‘God’s Law’ in Greek in the New Testament [Rom. 7: 22]).[26] In Muslim literature, šarīʿah designates the laws or message of a prophet or God, in contrast to fiqh, which refers to a scholar's interpretation thereof.[27]		According to the traditional Muslim view, the emergence of Islamic jurisprudence (fiqh) goes back to the lifetime of the Islamic prophet Muhammad.[3][4] In this view, his companions and followers took what he did and approved of as a model (sunnah) and transmitted this information to the succeeding generations in the form of hadith.[3][4] These reports led first to informal discussion and then systematic legal thought, articulated with greatest success in the eighth and ninth centuries by the master jurists Abu Hanifah, Malik ibn Anas, Al-Shafi‘i, and Ahmad ibn Hanbal, who are viewed as the founders of the Hanafi, Maliki, Shafiʿi, and Hanbali legal schools (madhhabs) of Sunni jurisprudence.[4]		Modern historians have presented alternative theories of the formation of fiqh.[3][4] At first Western scholars accepted the general outlines of the traditional account.[28] An influential revisionist hypothesis was advanced by Ignac Goldziher and elaborated by Joseph Schacht in the mid-20th century.[4] Schacht argued that the hadith reflected local practices of early Muslim communities and their chains of transmission were extended back to Muhammad's companions at a later date, when it became accepted that legal norms must be formally grounded in scriptural sources.[4] In his view, the real architect of Islamic jurisprudence was al-Shafi'i, who formulated this and other elements of classical legal theory in his work al-risala.[4][28] Both these accounts gave rise to objections, and modern historians generally adopt more cautious, intermediate positions.[28]		While the origin of hadith remains a subject of scholarly controversy, it is generally accepted that early Islamic jurisprudence developed out of a combination of administrative and popular practices shaped by the religious and ethical precepts of Islam.[29][3][30] It continued some aspects of pre-Islamic laws and customs of the lands that fell under Muslim rule in the aftermath of the early conquests and modified other aspects, aiming to meet the practical need of establishing Islamic norms of behavior and adjudicating disputes arising in the early Muslim communities.[31] Juristic thought gradually developed in study circles, where independent scholars met to learn from a local master and discuss religious topics.[31][32] At first, these circles were fluid in their membership, but with time distinct regional legal schools crystallized around shared sets of methodological principles.[32][3] As the boundaries of the schools became clearly delineated, the authority of their doctrinal tenets came to be vested in a master jurist from earlier times, who was henceforth identified as the school's founder.[32][3] In the course of the first three centuries of Islam, all legal schools came to accept the broad outlines of classical legal theory, according to which Islamic law had to be firmly rooted in the Quran and hadith.[3][33]		Fiqh is traditionally divided into the fields of uṣūl al-fiqh (lit. the roots of fiqh), which studies the theoretical principles of jurisprudence, and furūʿ al-fiqh (lit. the branches of fiqh), which is devoted to elaboration of rulings on the basis of these principles.[4][34]		Classical jurists held that human reason is a gift from God which should be exercised to its fullest capacity.[35] However, they believed that use of reason alone is insufficient to distinguish right from wrong, and that rational argumentation must draw its content from the body of transcendental knowledge revealed in the Quran and through the sunnah of Muhammad.[35]		Traditional theory of Islamic jurisprudence elaborates how scriptures should be interpreted from the standpoint of linguistics and rhetoric.[4] It also comprises methods for establishing authenticity of hadith and for determining when the legal force of a scriptural passage is abrogated by a passage revealed at a later date.[4] In addition to the Quran and sunnah, the classical theory of Sunni fiqh recognizes two other sources of law: juristic consensus (ijmaʿ) and analogical reasoning (qiyas).[29] It therefore studies the application and limits of analogy, as well as the value and limits of consensus, along with other methodological principles, some of which are accepted by only certain legal schools.[4] This interpretive apparatus is brought together under the rubric of ijtihad, which refers to a jurist's exertion in an attempt to arrive at a ruling on a particular question.[4] The theory of Twelver Shia jurisprudence parallels that of Sunni schools with some differences, such as recognition of reason (ʿaql) as a source of law in place of qiyas and extension of the notion of sunnah to include traditions of the imams.[36]		The classical process of ijtihad combined these generally recognized principles with other methods, which were not adopted by all legal schools, such as istihsan (juristic preference), istislah (consideration of public interest) and istishab (presumption of continuity).[29] A jurist who is qualified to practice ijtihad is known as a mujtahid.[30] The use of independent reasoning to arrive at a ruling is contrasted with taqlid (imitation), which refers to following the rulings of a mujtahid.[30] By the beginning of the 10th century, development of Sunni jurisprudence prompted leading jurists to state that the main legal questions had been addressed and the scope of ijtihad was gradually restricted.[30][42] From the 18th century on, leading Muslim reformers began calling for abandonment of taqlid and renewed emphasis on ijtihad, which they saw as a return to the vitality of early Islamic jurisprudence.[42]		Sharia rulings fall into one of five categories known as “the five decisions” (al-aḥkām al-khamsa): mandatory (farḍ or wājib), recommended (mandūb or mustaḥabb), neutral (mubāḥ), reprehensible (makrūh), and forbidden (ḥarām).[3][34] It is a sin or a crime to perform a forbidden action or not to perform a mandatory action.[3] Reprehensible acts should be avoided, but they are not considered to be sinful or punishable in court.[3][43] Avoiding reprehensible acts and performing recommended acts is held to be subject of reward in the afterlife, while allowed actions entail no judgement from God.[3][43] Jurists disagree on whether the term ḥalāl covers the first three or the first four categories.[3] The legal and moral verdict depends on whether the action is committed out of necessity (ḍarūra).[3]		Maqāṣid (aims or purposes) of sharia and maṣlaḥa (welfare or public interest) are two related classical doctrines which have come to play an increasingly prominent role in modern times.[44][45][46] They were first clearly articulated by al-Ghazali (d. 1111), who argued that maslaha was God's general purpose in revealing the divine law, and that its specific aims was preservation of five essentials of human well-being: religion, life, intellect, offspring, and property.[47] Although most classical-era jurists recognized maslaha and maqasid as important legal principles, they held different views regarding the role they should play in Islamic law.[44][46] Some jurists viewed them as auxiliary rationales constrained by scriptural sources and analogical reasoning.[44][48] Others regarded them as an independent source of law, whose general principles could override specific inferences based on the letter of scripture.[44][49] While the latter view was held by a minority of classical jurists, in modern times it came to be championed in different forms by prominent scholars who sought to adapt Islamic law to changing social conditions by drawing on the intellectual heritage of traditional jurisprudence.[44][29][45] These scholars expanded the inventory of maqasid to include such aims of sharia as reform and women's rights (Rashid Rida); justice and freedom (Mohammed al-Ghazali); and human dignity and rights (Yusuf al-Qaradawi).[44]		The domain of furūʿ al-fiqh (lit. branches of fiqh) is traditionally divided into ʿibādāt (rituals or acts of worship) and muʿāmalāt (social relations).[4][30] Many jurists further divided the body of substantive jurisprudence into "the four quarters", called rituals, sales, marriage and injuries.[50] Each of these terms figuratively stood for a variety of subjects.[50] For example, the quarter of sales would encompass partnerships, guaranty, gifts, and bequests, among other topics.[50] Juristic works were arranged as a sequence of such smaller topics, each called a "book" (kitab).[4][50] The special significance of ritual was marked by always placing its discussion at the start of the work.[4][50]		Some historians distinguish a field of Islamic criminal law, which combines several traditional categories.[3][51][34] Several crimes with scripturally prescribed punishments are known as hudud.[3] Jurists developed various restrictions which in many cases made them virtually impossible to apply.[3] Other crimes involving intentional bodily harm are judged according to a version of lex talionis that prescribes a punishment analogous to the crime (qisas), but the victims or their heirs may accept a monetary compensation (diya) or pardon the perpetrator instead; only diya is imposed for non-intentional harm.[3][51] Other criminal cases belong to the category of taʿzīr, where the goal of punishment is correction or rehabilitation of the culprit and its form is largely left to the judge's discretion.[3][51] In practice, since early on in Islamic history, criminal cases were usually handled by ruler-administered courts or local police using procedures which were only loosely related to sharia.[4][51]		The two major genres of furūʿ literature are the mukhtasar (concise summary of law) and the mabsut (extensive commentary).[4] Mukhtasars were short specialized treatises or general overviews that could be used in a classroom or consulted by judges.[4][3][52] A mabsut, which usually provided a commentary on a mukhtasar and could stretch to dozens of large volumes, recorded alternative rulings with their justifications, often accompanied by a proliferation of cases and conceptual distinctions.[4][52] The terminology of juristic literature was conservative and tended to preserve notions which had lost their practical relevance.[4] At the same time, the cycle of abridgement and commentary allowed jurists of each generation to articulate a modified body of law to meet changing social conditions.[52] Other juristic genres include the qawāʿid (succinct formulas meant to aid the student remember general principles) and collections of fatwas by a particular scholar.[3]		The main Sunni schools of law (madhhabs) are the Hanafi, Maliki, Shafi'i and Hanbali madhhabs.[30] They emerged in the ninth and tenth centuries and by the twelfth century almost all jurists aligned themselves with a particular madhhab.[53] These four schools recognize each other's validity and they have interacted in legal debate over the centuries.[53][30] Rulings of these schools are followed across the Muslim world without exclusive regional restrictions, but they each came to dominate in different parts of the world.[53][30] For example, the Maliki school is predominant in North and West Africa; the Hanafi school in South and Central Asia; the Shafi'i school in Lower Egypt, East Africa, and Southeast Asia; and the Hanbali school in North and Central Arabia.[53][30][3] The first centuries of Islam also witnessed a number of short-lived Sunni madhhabs.[4] The Zahiri school, which is commonly identified as extinct, continues to exert influence over legal thought.[4][30][53] The development of Shia legal schools occurred along the lines of theological differences and resulted in formation of the Twelver, Zaidi and Ismaili madhhabs, whose differences from Sunni legal schools are roughly of the same order as the differences among Sunni schools.[4][3] The Ibadi legal school, distinct from Sunni and Shia madhhabs, is predominant in Oman.[30]		The transformations of Islamic legal institutions in the modern era have had profound implications for the madhhab system.[53] Legal practice in most of the Muslim world has come to be controlled by government policy and state law, so that the influence of the madhhabs beyond personal ritual practice depends on the status accorded to them within the national legal system.[53] State law codification commonly utilized the methods of takhayyur (selection of rulings without restriction to a particular madhhab) and talfiq (combining parts of different rulings on the same question).[53] Legal professionals trained in modern law schools have largely replaced traditional ulema as interpreters of the resulting laws.[53] Global Islamic movements have at times drawn on different madhhabs and at other times placed greater focus on the scriptural sources rather than classical jurisprudence.[53] The Hanbali school, with its particularly strict adherence to the Quran and hadith, has inspired conservative currents of direct scriptural interpretation by the Salafi and Wahhabi movements.[53] Other currents, such as networks of Indonesian ulema and Islamic scholars residing in Muslim-minority countries, have advanced liberal interpretations of Islamic law without focusing on traditions of a particular madhhab.[53]		Politics portal		From the 9th century onward, the power to interpret law in traditional Islamic societies was in the hands of the scholars (ulema). This separation of powers served to limit the range of actions available to the ruler, who could not easily decree or reinterpret law independently and expect the continued support of the community.[54] Through succeeding centuries and empires, the balance between the ulema and the rulers shifted and reformed, but the balance of power was never decisively changed.[55] Over the course of many centuries, imperial, political and technological change, including the Industrial Revolution and the French Revolution, ushered in an era of European world hegemony that gradually included the domination of many of the lands which had previously been ruled by Islamic empires.[56][57] At the end of the Second World War, the European powers found themselves too weakened to maintain their empires as before.[58] The wide variety of forms of government, systems of law, attitudes toward modernity and interpretations of sharia are a result of the ensuing drives for independence and modernity in the Muslim world.[59][60]		Most Muslim-majority countries incorporate sharia at some level in their legal framework, with many calling it the highest law or the source of law of the land in their constitution.[61][62] Most use sharia for personal law (marriage, divorce, domestic violence, child support, family law, inheritance and such matters).[63][64] Elements of sharia are present, to varying extents, in the criminal justice system of many Muslim-majority countries.[65] Saudi Arabia, Yemen, Brunei, Qatar, Pakistan, United Arab Emirates, Iraq, Iran, Afghanistan, Sudan and Mauritania apply the code predominantly or entirely while it applies in some parts of Indonesia.[65][66]		Most Muslim-majority countries with sharia-prescribed hudud punishments in their legal code do not prescribe it routinely and use other punishments instead.[61][67] The harshest sharia penalties such as stoning, beheading and other forms of the death penalty are enforced with varying levels of consistency.[68]		Since the 1970s, most Muslim-majority countries have faced vociferous demands from their religious groups and political parties for immediate adoption of sharia as the sole, or at least primary, legal framework.[69] Some moderates and liberal scholars within these Muslim countries have argued for limited expansion of sharia.[70]		With the growing Muslim immigrant communities in Europe, there have been reports in some media of "no-go zones" being established where sharia law reigns supreme.[71][72] However, there is no evidence of the existence of "no-go zones", and these allegations are sourced from anti-immigrant groups falsely equating low-income neighborhoods predominantly inhabited by immigrants as "no-go zones".[73][74] In England, the Muslim Arbitration Tribunal makes use of sharia family law to settle disputes, though this limited adoption of sharia is controversial.[75][76][77]		Sharia is enforced in Islamic nations in a number of ways, including mutaween (police enforcement) and hisbah. mutaween (Arabic: المطوعين، مطوعية‎‎ muṭawwiʿīn, muṭawwiʿiyyah)[78] are the government-authorized or government-recognized religious police (or clerical police) of Saudi Arabia. Elsewhere, enforcement of Islamic values in accordance with sharia is the responsibility of the Polisi Perda Syariah Islam in Aceh province of Indonesia,[79] the Committee for the Propagation of Virtue and the Prevention of Vice (Gaza Strip) in parts of Palestine, and the Basiji Force in Iran.[80]		Hisbah (Arabic: حسبة‎‎ ḥisb(ah), or hisba) is a historic Islamic doctrine which means "accountability".[83] Hisbah doctrine holds that it is a religious obligation of every Muslim that he or she report to the ruler (Sultan, government authorities) any wrong behavior of a neighbor or relative that violates sharia or insults Islam. The doctrine states that it is the divinely sanctioned duty of the ruler to intervene when such charges are made, and coercively "command right and forbid wrong" in order to keep everything in order according to sharia.[84][85][86] Al-Jama'a al-Islamiyya (considered a terrorist organization) suggest that enforcement of sharia under the Hisbah doctrine is the sacred duty of all Muslims, not just rulers.[84]		The doctrine of Hisbah in Islam may allow a Muslim to accuse another Muslim, ex-Muslim or non-Muslim for beliefs or behavior that harms Islamic society. This principle has been used in countries such as Egypt, Pakistan and others to bring blasphemy charges against apostates.[87] For example, in Egypt, sharia was enforced on the Muslim scholar Nasr Abu Zayd, through the doctrine of Hisbah for apostasy.[88][89] Similarly, in Nigeria, after twelve northern Muslim-majority states such as Kano adopted a sharia-based penal code between 1999 and 2000, hisbah became the allowed method of sharia enforcement where Muslim citizens could police compliance of moral order based on sharia.[90] In Aceh province of Indonesia, Islamic vigilante activists have invoked Hisbah doctrine to enforce sharia on fellow Muslims as well as demanding that non-Muslims respect sharia.[91][92] Hisbah has been used in many Muslim majority countries to enforce sharia restrictions on blasphemy and criticism of Islam over internet and social media.[93][94]				Sharia judicial proceedings have significant differences from other legal traditions, including those in both common law and civil law. Sharia courts traditionally do not rely on lawyers; plaintiffs and defendants represent themselves. Trials are conducted solely by the judge, and there is no jury system. There is no pre-trial discovery process, and no cross-examination of witnesses. Unlike common law, judges' verdicts do not set binding precedents[95][96] under the principle of stare decisis,[97] and unlike civil law, sharia is left to the interpretation in each case and has no formally codified universal statutes.[98]		The rules of evidence in sharia courts also maintain a distinctive custom of prioritizing oral testimony.[99] Witnesses, in a sharia court system, must be faithful, that is Muslim.[100] Male Muslim witnesses are deemed more reliable than female Muslim witnesses, and non-Muslim witnesses considered unreliable and receive no priority in a sharia court.[101][102] In civil cases in some countries, a Muslim woman witness is considered half the worth and reliability than a Muslim man witness.[103][104] In criminal cases, women witnesses are unacceptable in stricter, traditional interpretations of sharia, such as those found in Hanbali madhhab.[100]		A confession, an oath, or the oral testimony of Muslim witnesses are the main evidence admissible, in sharia courts, for hudud crimes, that is the religious crimes of adultery, fornication, rape, accusing someone of illicit sex but failing to prove it, apostasy, drinking intoxicants and theft.[105][106][107][108] Testimony must be from at least two free Muslim male witnesses, or one Muslim male and two Muslim females, who are not related parties and who are of sound mind and reliable character. Testimony to establish the crime of adultery, fornication or rape must be from four Muslim male witnesses, with some fiqhs allowing substitution of up to three male with six female witnesses; however, at least one must be a Muslim male.[109] Forensic evidence (i.e., fingerprints, ballistics, blood samples, DNA etc.) and other circumstantial evidence is likewise rejected in hudud cases in favor of eyewitnesses, a practice which can cause severe difficulties for women plaintiffs in rape cases.[110][111]		Muslim jurists have debated whether and when coerced confession and coerced witnesses are acceptable.[citation needed] In the Ottoman Criminal Code, the executive officials were allowed to use torture only if the accused had a bad reputation and there were already indications of his guilt, such as when stolen goods were found in his house, if he was accused of grievous bodily harm by the victim or if a criminal during investigation mentioned him as an accomplice.[112] Confessions obtained under torture could not be used as a ground for awarding punishment unless they were corroborated by circumstantial evidence.[112]		Quran 2:282 recommends written financial contracts with reliable witnesses, although there is dispute about equality of female testimony.[104]		Marriage is solemnized as a written financial contract, in the presence of two Muslim male witnesses, and it includes a brideprice (Mahr) payable from a Muslim man to a Muslim woman. The brideprice is considered by a sharia court as a form of debt. Written contracts are paramount, in sharia courts, in the matters of dispute that are debt-related, which includes marriage contracts.[113] Written contracts in debt-related cases, when notarized by a judge, is deemed more reliable.[114]		In commercial and civil contracts, such as those relating to exchange of merchandise, agreement to supply or purchase goods or property, and others, oral contracts and the testimony of Muslim witnesses triumph over written contracts. Sharia system has held that written commercial contracts may be forged.[114][115] Timur Kuran states that the treatment of written evidence in religious courts in Islamic regions created an incentive for opaque transactions, and the avoidance of written contracts in economic relations. This led to a continuation of a "largely oral contracting culture" in Muslim nations and communities.[115][116]		In lieu of written evidence, oaths are accorded much greater weight; rather than being used simply to guarantee the truth of ensuing testimony, they are themselves used as evidence. Plaintiffs lacking other evidence to support their claims may demand that defendants take an oath swearing their innocence, refusal thereof can result in a verdict for the plaintiff.[117] Taking an oath for Muslims can be a grave act; one study of courts in Morocco found that lying litigants would often "maintain their testimony 'right up to the moment of oath-taking and then to stop, refuse the oath, and surrender the case."[118] Accordingly, defendants are not routinely required to swear before testifying, which would risk casually profaning the Quran should the defendant commit perjury;[118] instead oaths are a solemn procedure performed as a final part of the evidence process.[citation needed]		In classical jurisprudence monetary compensation for bodily harm (diya or blood money) is assessed differently for different classes of victims. For example, for Muslim women the amount was half that assessed for a Muslim man.[119][120] Diya for the death of a free Muslim man is twice as high as for Jewish and Christian victims according to the Maliki and Hanbali madhhabs and three times as high according to Shafi'i rules.[121] Several legals schools assessed diya for Magians (majus) at one-fifteenth the value of a free Muslim male.[121]		Modern countries which incorporate classical diya rules into their legal system treat them in different ways. The Pakistan Penal Code modernized the Hanafi doctrine by eliminating distinctions between Muslims and non-Muslims.[122] In Iran, diya for non-Muslim victims professing one of the faiths protected under the constitution (Jews, Christians, and Zoroastrians) was made equal to diya for Muslims in 2004,[123] though according to a 2006 US State Department report, the penal code still discriminates against other religious minorities and women.[124] According to Human Rights Watch and the US State Department, in Saudi Arabia Jewish or Christian male plaintiffs are entitled to half the amount a Muslim male would receive, while for all other non-Muslim males the proportion is one-sixteenth.[125][126][127]		A 2013 survey based on interviews of 38,000 Muslims, randomly selected from urban and rural parts in 39 countries using area probability designs, by the Pew Forum on Religion and Public Life found that a majority—in some cases "overwhelming" majority—of Muslims in a number of countries support making sharia the law of the land, including Afghanistan (99%), Iraq (91%), Niger (86%), Malaysia (86%), Pakistan (84%), Morocco (83%), Bangladesh (82%), Egypt (74%), Indonesia (72%), Jordan (71%), Uganda (66%), Ethiopia (65%), Mali (63%), Ghana (58%), and Tunisia (56%).[128] In Muslim regions of Southern-Eastern Europe and Central Asia, the support is less than 50%: Russia (42%), Kyrgyzstan (35%), Tajikistan (27%), Kosovo (20%), Albania (12%), Turkey (12%), Kazakhstan (10%), Azerbaijan (8%). Regarding specific averages, in South Asia, Sharia had 84% favorability rating among the respondents; In Southeast Asia 77%; In the Middle-East/North Africa 74%; In Sub-Saharan Africa 64%; In Southern-Eastern Europe 18%; And in Central Asia 12%.[128]		However, while most of those who support implementation of sharia favor using it in family and property disputes, fewer supported application of severe punishments such as whippings and cutting off hands, and interpretations of some aspects differed widely.[128] According to the Pew poll, among Muslims who support making sharia the law of the land, most do not believe that it should be applied to non-Muslims. In the Muslim-majority countries surveyed this proportion varied between 74% (of 74% in Egypt) and 19% (of 10% in Kazakhstan), as percentage of those who favored making sharia the law of the land.[129] Polls demonstrate that for Egyptians, the 'Shariah' is associated with notions of political, social and gender justice.[130]		In 2008, Rowan Williams, the archbishop of Canterbury, has suggested that Islamic and Orthodox Jewish courts should be integrated into the British legal system alongside ecclesiastical courts to handle marriage and divorce, subject to agreement of all parties and strict requirements for protection of equal rights for women.[131] His reference to the sharia sparked a controversy.[131] Later that year, Nicholas Phillips, then Lord Chief Justice of England and Wales, stated that there was "no reason why sharia principles [...] should not be the basis for mediation or other forms of alternative dispute resolution."[132] A 2008 YouGov poll in the United Kingdom found 40% of Muslim students interviewed supported the introduction of sharia into British law for Muslims.[133] Michael Broyde, professor of law at Emory University specializing in alternative dispute resolution and Jewish law,[134] has argued that sharia courts can be integrated into the American religious arbitration system, provided that they adopt appropriate institutional requirements as American rabbinical courts have done.[135]		Fundamentalists, wishing to return to basic Islamic religious values and law, have in some instances imposed harsh sharia punishments for crimes, curtailed civil rights and violated human rights. Extremists have used the Quran and their own particular version of sharia to justify acts of war and terror against Muslim as well as non-Muslim individuals and governments, using alternate, conflicting interpretations of sharia and their notions of jihad.[136][137][138]		The sharia basis of arguments advocating terrorism is controversial. According to Bernard Lewis, "[a]t no time did the classical jurists offer any approval or legitimacy to what we nowadays call terrorism"[139] and the terrorist practice of suicide bombing "has no justification in terms of Islamic theology, law or tradition".[140] In the modern era the notion of jihad has lost its jurisprudential relevance and instead gave rise to an ideological and political discourse.[141] For al-Qaeda ideologues, in jihad all means are legitimate, including targeting Muslim non-combatants and the mass killing of non-Muslim civilians.[136] According to these interpretations, Islam does not discriminate between military and civilian targets, but rather between Muslims and nonbelievers, whose blood can be legitimately spilled.[136]		Some scholars of Islam, such as Yusuf al-Qaradawi and Sulaiman Al-Alwan, have supported suicide attacks against Israeli civilians, arguing that they are army reservists and hence should be considered as soldiers, while Hamid bin Abdallah al-Ali declared that suicide attacks in Chechnya were justified as a "sacrifice".[136][142] Many prominent Islamic scholars, including al-Qaradawi himself, have issued condemnations of terrorism in general terms.[143] For example, Abdul-Aziz ibn Abdullah Al ash-Sheikh, the Grand Mufti of Saudi Arabia has stated that "terrorizing innocent people [...] constitute[s] a form of injustice that cannot be tolerated by Islam", while Muhammad Sayyid Tantawy, Grand Imam of al-Azhar and former Grand Mufti of Egypt has stated that "attacking innocent people is not courageous; it is stupid and will be punished on the Day of Judgment".[136][144]		In the post-9/11 Western world, sharia has been called a source of "hysteria",[145] "more controversial than ever", the one aspect of Islam that inspires "particular dread".[146] On the Internet, "dozens of self-styled `counter-jihadis`" emerged to campaign against sharia law, describing it in strict interpretations resembling those of Salafi Muslims.[146] Several years after 9/11, fear of sharia law and of "the ideology of extremism" among Muslims reportedly spread to mainstream conservative Republicans in the United States.[147] Former House Speaker Newt Gingrich won ovations calling for a federal ban on sharia law.[147] In 2015, the governor of Louisiana (Bobby Jindal) warned of the danger of purported "no-go zones" in European cities allegedly operating under sharia law and where local laws are not applicable.[148] The issue of "liberty versus Sharia" was called a "momentous civilizational debate" by right-wing pundit Diana West.[149] In 2008 in Britain, the future Prime Minister (David Cameron) declared his opposition to "any expansion of Sharia law in the UK."[150] In Germany, in 2014, the Interior Minister (Thomas de Maizière) told a newspaper (Bild), "Sharia law is not tolerated on German soil."[151]		Some countries and jurisdictions have explicit bans on sharia law. In Canada, for example, sharia law has been explicitly banned in Quebec by a 2005 unanimous vote of the National Assembly,[152] while the province of Ontario allows family law disputes to be arbitrated only under Ontario law.[153] In the U.S., opponents of Sharia have sought to ban it from being considered in courts, where it has been routinely used alongside traditional Jewish and Catholic laws to decide legal, business, and family disputes subject to contracts drafted with reference to such laws, as long as they do not violate secular law or the U.S. constitution.[15] After failing to gather support for a federal law making observing Sharia a felony punishable by up to 20 years in prison, anti-Sharia activists have focused on state legislatures.[15] By 2014, bills aimed against use of Sharia have been introduced in 34 states and passed in 11.[15] These bills have generally referred to banning foreign or religious law in order to thwart legal challenges.[15]		According to Jan Michiel Otto, Professor of Law and Governance in Developing Countries at Leiden University, "[a]nthropological research shows that people in local communities often do not distinguish clearly whether and to what extent their norms and practices are based on local tradition, tribal custom, or religion. Those who adhere to a confrontational view of sharia tend to ascribe many undesirable practices to sharia and religion overlooking custom and culture, even if high-ranking religious authorities have stated the opposite."[154]		Ali Khan states that "constitutional orders founded on the principles of sharia are fully compatible with democracy, provided that religious minorities are protected and the incumbent Islamic leadership remains committed to the right to recall".[155][156] Other scholars say sharia is not compatible with democracy, particularly where the country's constitution demands separation of religion and the democratic state.[157][158]		Courts in non-Muslim majority nations have generally ruled against the implementation of sharia, both in jurisprudence and within a community context, based on sharia's religious background. In Muslim nations, sharia has wide support with some exceptions.[159] For example, in 1998 the Constitutional Court of Turkey banned and dissolved Turkey's Refah Party on the grounds that "Democracy is the antithesis of Sharia", the latter of which Refah sought to introduce.[160][161]		On appeal by Refah the European Court of Human Rights determined that "sharia is incompatible with the fundamental principles of democracy".[162][163][164] Refah's sharia-based notion of a "plurality of legal systems, grounded on religion" was ruled to contravene the European Convention for the Protection of Human Rights and Fundamental Freedoms. It was determined that it would "do away with the State's role as the guarantor of individual rights and freedoms" and "infringe the principle of non-discrimination between individuals as regards their enjoyment of public freedoms, which is one of the fundamental principles of democracy".[165]		Several major, predominantly Muslim countries have criticized the Universal Declaration of Human Rights (UDHR) for its perceived failure to take into account the cultural and religious context of non-Western countries. Iran declared in the UN assembly that UDHR was "a secular understanding of the Judeo-Christian tradition", which could not be implemented by Muslims without trespassing the Islamic law.[166] Islamic scholars and Islamist political parties consider 'universal human rights' arguments as imposition of a non-Muslim culture on Muslim people, a disrespect of customary cultural practices and of Islam.[167][168] In 1990, the Organisation of Islamic Cooperation, a group representing all Muslim majority nations, met in Cairo to respond to the UDHR, then adopted the Cairo Declaration on Human Rights in Islam.[169][170]		Ann Elizabeth Mayer points to notable absences from the Cairo Declaration: provisions for democratic principles, protection for religious freedom, freedom of association and freedom of the press, as well as equality in rights and equal protection under the law. Article 24 of the Cairo declaration states that "all the rights and freedoms stipulated in this Declaration are subject to the Islamic shari'a".[171]		In 2009, the journal Free Inquiry summarized the criticism of the Cairo Declaration in an editorial: "We are deeply concerned with the changes to the Universal Declaration of Human Rights by a coalition of Islamic states within the United Nations that wishes to prohibit any criticism of religion and would thus protect Islam's limited view of human rights. In view of the conditions inside the Islamic Republic of Iran, Egypt, Pakistan, Saudi Arabia, the Sudan, Syria, Bangladesh, Iraq, and Afghanistan, we should expect that at the top of their human rights agenda would be to rectify the legal inequality of women, the suppression of political dissent, the curtailment of free expression, the persecution of ethnic minorities and religious dissenters – in short, protecting their citizens from egregious human rights violations. Instead, they are worrying about protecting Islam."[172]		H. Patrick Glenn states that sharia is structured around the concept of mutual obligations of a collective, and it considers individual human rights as potentially disruptive and unnecessary to its revealed code of mutual obligations. In giving priority to this religious collective rather than individual liberty, the Islamic law justifies the formal inequality of individuals (women, non-Islamic people).[173] Bassam Tibi states that sharia framework and human rights are incompatible.[174] Abdel al-Hakeem Carney, in contrast, states that sharia is misunderstood from a failure to distinguish sharia from siyasah (politics).[175]		The Cairo Declaration on Human Rights in Islam conditions free speech with sharia law: Article 22(a) of the Declaration states that "Everyone shall have the right to express his opinion freely in such manner as would not be contrary to the principles of the Shariah."[176]		Blasphemy in Islam is any form of cursing, questioning or annoying God, Muhammad or anything considered sacred in Islam.[177][178][179][180] The sharia of various Islamic schools of jurisprudence specify different punishment for blasphemy against Islam, by Muslims and non-Muslims, ranging from imprisonment, fines, flogging, amputation, hanging, or beheading.[177][181][182][183] In some cases, sharia allows non-Muslims to escape death by converting and becoming a devout follower of Islam.[184]		Blasphemy, as interpreted under sharia, is controversial.[185] Muslim nations have petitioned the United Nations to limit "freedom of speech" because "unrestricted and disrespectful opinion against Islam creates hatred".[186] Other nations, in contrast, consider blasphemy laws as violation of "freedom of speech",[187] stating that freedom of expression is essential to empowering both Muslims and non-Muslims, and point to the abuse of blasphemy laws, where hundreds, often members of religious minorities, are being lynched, killed and incarcerated in Muslim nations, on flimsy accusations of insulting Islam.[188][189]		According to the United Nations' Universal Declaration of Human Rights, every human has the right to freedom of thought, conscience and religion; this right includes freedom to change their religion or belief. Sharia has been criticized for not recognizing this human right. According to scholars[16][191][192] of Islamic law, the applicable rules for religious conversion under sharia are as follows:		According to sharia theory, conversion of disbelievers and non-Muslims to Islam is encouraged as a religious duty for all Muslims, and leaving Islam (apostasy), expressing contempt for Islam (blasphemy), and religious conversion of Muslims is prohibited.[193][194] Not all Islamic scholars agree with this interpretation of sharia theory. In practice, as of 2011, 20 Islamic nations had laws declaring apostasy from Islam as illegal and a criminal offense. Such laws are incompatible with the UDHR's requirement of freedom of thought, conscience and religion.[195][196][197][198] In another 2013 report based on international survey of religious attitudes, more than 50% of Muslim population in 6 out of 49 Islamic countries supported death penalty for any Muslim who leaves Islam (apostasy).[199][200] However it is also shown that the majority of Muslims in the 43 nations surveyed did not agree with this interpretation of sharia.		Some scholars claim sharia allows religious freedom because a sharia verse teaches, "there is no compulsion in religion."[201] Other scholars claim sharia recognizes only one proper religion, considers apostasy as sin punishable with death, and members of other religions as kafir (infidel);[202] or hold that sharia demands that all apostates and kafir must be put to death, enslaved or be ransomed.[203][need quotation to verify][204][205][206] Yet other scholars suggest that sharia has become a product of human interpretation and inevitably leads to disagreements about the “precise contents of the Shari'a." In the end, then, what is being applied is not sharia, but what a particular group of clerics and government decide is sharia. It is these differing interpretations of sharia that explain why many Islamic countries have laws that restrict and criminalize apostasy, proselytism and their citizens' freedom of conscience and religion.[207][208]		Homosexual intercourse is illegal under sharia law, though the prescribed penalties differ from one school of jurisprudence to another. For example, some Muslim-majority countries impose the death penalty for acts perceived as sodomy and homosexual activities: Iran,[209] Saudi Arabia,[210] and in other Muslim-majority countries such as Egypt, Iraq, and the Indonesian province of Aceh,[92][211][212] same-sex sexual acts are illegal,[213] and LGBT people regularly face violence and discrimination.[214]		Many claim sharia law encourages domestic violence against women, when a husband suspects nushuz (disobedience, disloyalty, rebellion, ill conduct) in his wife.[215] Other scholars claim wife beating, for nashizah, is not consistent with modern perspectives of the Quran.[216]		One of the verses of the Quran relating to permissibility of domestic violence is Surah 4:34.[217][218] Sharia has been criticized for ignoring women's rights in domestic abuse cases.[219][220][221][222] Musawah, CEDAW, KAFA and other organizations have proposed ways to modify sharia-inspired laws to improve women's rights in Islamic nations, including women's rights in domestic abuse cases.[223][224][225][226]		Shari'a is the basis for personal status laws in most Islamic majority nations. These personal status laws determine rights of women in matters of marriage, divorce and child custody. A 2011 UNICEF report concludes that sharia law provisions are discriminatory against women from a human rights perspective. In legal proceedings under sharia law, a woman’s testimony is worth half of a man’s before a court.[103]		Except for Iran[citation needed], Lebanon[citation needed] and Bahrain[citation needed] which allow child marriages[citation needed], the civil code in Islamic majority countries do not allow child marriage of girls. However, with sharia personal status laws, sharia courts in all these nations have the power to override the civil code. The religious courts permit girls less than 18 years old to marry. As of 2011, child marriages are common in a few Middle Eastern countries, accounting for 1 in 6 all marriages in Egypt and 1 in 3 marriages in Yemen. UNICEF and other studies state that the top five nations in the world with highest observed child marriage rates – Niger (75%), Chad (72%), Mali (71%), Bangladesh (64%), Guinea (63%) – are Islamic-majority countries where the personal laws for Muslims are sharia-based.[227][228] In his Cairo speech, President Obama spoke out against child marriage.[229]		Rape is considered a crime in all countries, but sharia courts in Bahrain, Iraq, Jordan, Libya, Morocco, Syria and Tunisia in some cases allow a rapist to escape punishment by marrying his victim, while in other cases the victim who complains is often prosecuted with the crime of Zina (adultery).[103][230][231]		Sharia grants women the right to inherit property from other family members, and these rights are detailed in the Quran.[232] A woman's inheritance is unequal and less than a man's, and dependent on many factors.[Quran 4:12][233] For instance, a daughter's inheritance is usually half that of her brother's.[Quran 4:11][233]		Until the 20th century, Islamic law granted Muslim women certain legal rights, such as the right to own property received as Mahr (brideprice) at her marriage.[234][235] However, Islamic law does not grant non-Muslim women the same legal rights as the few it did grant Muslim women. Sharia recognizes the basic inequality between master and women slave, between free women and slave women, between Believers and non-Believers, as well as their unequal rights.[236][237] Sharia authorized the institution of slavery, using the words abd (slave) and the phrase ma malakat aymanukum ("that which your right hand owns") to refer to women slaves, seized as captives of war.[236][238] Under Islamic law, Muslim men could have sexual relations with female captives and slaves.[239][240]		Slave women under sharia did not have a right to own property or to move freely.[241][242] Sharia, in Islam's history, provided a religious foundation for enslaving non-Muslim women (and men), but allowed for the manumission of slaves. However, manumission required that the non-Muslim slave first convert to Islam.[243][244] A non-Muslim slave woman who bore children to her Muslim master became legally free upon her master's death, and her children were presumed to be Muslims like their father, in Africa[243] and elsewhere.[245]		Starting with the 20th century, Western legal systems evolved to expand women's rights, but women's rights under Islamic law have remained tied to the Quran, hadiths and their fundamentalist interpretation as sharia by Islamic jurists.[240][246]		Elements of Islamic law have parallels in western legal systems. As example, the influence of Islam on the development of an international law of the sea can be discerned alongside that of the Roman influence.[247]		Makdisi states Islamic law also parallels the legal scholastic system in the West, which gave rise to the modern university system.[248] He writes that the triple status of faqih ("master of law"), mufti ("professor of legal opinions") and mudarris ("teacher"), conferred by the classical Islamic legal degree, had its equivalents in the medieval Latin terms magister, professor and doctor, respectively, although they all came to be used synonymously in both East and West.[248] Makdisi suggests that the medieval European doctorate, licentia docendi was modeled on the Islamic degree ijazat al-tadris wa-l-ifta’, of which it is a word-for-word translation, with the term ifta’ (issuing of fatwas) omitted.[248][249] He also argues that these systems shared fundamental freedoms: the freedom of a professor to profess his personal opinion and the freedom of a student to pass judgement on what he is learning.[248]		There are differences between Islamic and Western legal systems. For example, sharia classically recognizes only natural persons, and never developed the concept of a legal person, or corporation, i.e., a legal entity that limits the liabilities of its managers, shareholders, and employees; exists beyond the lifetimes of its founders; and that can own assets, sign contracts, and appear in court through representatives.[250] Interest prohibitions imposed secondary costs by discouraging record keeping and delaying the introduction of modern accounting.[251] Such factors, according to Timur Kuran, have played a significant role in retarding economic development in the Middle East.[252]		
Orexin, also called hypocretin, is a neuropeptide that regulates arousal, wakefulness, and appetite.[1] The most common form of narcolepsy, in which the sufferer briefly loses muscle tone (cataplexy), is caused by a lack of orexin in the brain due to destruction of the cells that produce it.[2]		There are approximately 70,000 orexin-producing neurons in the human brain that project from the lateral hypothalamus to neurons and brain regions that modulate wakefulness.[1][2] However, the axons from these neurons extend throughout the entire brain and spinal cord,[3] where there are also receptors for orexin.		Orexin was discovered in 1998 almost simultaneously by two independent groups of rat-brain researchers.[4][5] One group named it orexin, from orexis, meaning "appetite" in Greek; the other group named it hypocretin, because it is produced in the hypothalamus and bears a weak resemblance to secretin, another peptide.[2] The use of both terms is now a practical necessity, as hypocretin is used to refer to the genetic products and orexin is used to refer to the protein products.[6]						In 1998, reports the discovery of the orexins/hypocretins were published nearly simultaneously. Luis de Lecea, Thomas Kilduff, and colleagues reported the discovery of the hypocretin system at the same time as Takeshi Sakurai from Masashi Yanagisawa's lab at the University of Texas Southwestern Medical Center at Dallas reported the discovery of the orexins to reflect the orexigenic (appetite-stimulating) activity of these peptides. In their 1998 paper describing these neuropeptides, they also reported discovery of two orexin receptors, dubbed OX1R and OX2R.[4].		The two groups also took different approaches towards their discovery. One team was interested in finding new genes that were expressed in the hypothalamus. In 1996, scientists from the Scripps Research Institute reported the discovery of several genes in the rat brain, including one they dubbed "clone 35." Their work showed that clone 35 expression was limited to the lateral hypothalamus.[7] They extracted selective DNA found in the lateral hypothalamus. They cloned this DNA and studied it using electron microscopy. Neurotransmitters found in this area were oddly similar to the gut hormone, secretin, a member of the incretin family, so they named hypocretin to stand for a hypothalamic member of the incretin family.[8] These cells were first thought to reside and work only within the lateral hypothalamus area, but immunocytochemistry tactics revealed the various projections this area truly had to other parts of the brain. A majority of these projections reached the limbic system and structures associated with it (including the amygdala, septum, and basal forebrain area).		On the other hand, Sakurai and colleagues were studying the orexin system as orphan receptors. To this end, they used transgenic cell lines that expressed individual orphan receptors and then exposed them to different potential ligands. They found that the orexin peptides activated the cells expressing the orexin receptors and went on find orexin peptide expression specifically in the hypothalamus. Additionally, when either orexin peptide was administered to rats it stimulated feeding, giving rise to the name 'orexin'.[4]		The nomenclature of the orexin/hypocretin system now recognizes the history of its discovery. "Hypocretin" refers to the gene or genetic products and "orexin" refers to the protein, reflecting the differing approaches that resulted in its discovery. The use of both terms is also a practical necessity because "HCRT" is the standard gene symbol in databases like GenBank and "OX" is used to refer to the pharmacology of the peptide system by the International Union of Basic and Clinical Pharmacology.[6]		There are two types of orexin: orexin-A and -B (hypocretin-1 and -2). They are excitatory neuropeptides with approximately 50% sequence identity, produced by cleavage of a single precursor protein. Orexin-A is 33 amino acid residues long and has two intrachain disulfide bonds; orexin-B is a linear 28 amino acid residue peptide. Although these peptides are produced by a very small population of cells in the lateral and posterior hypothalamus, they send projections throughout the brain. The orexin peptides bind to the two G-protein coupled orexin receptors, OX1 and OX2, with orexin-A binding to both OX1 and OX2 with approximately equal affinity while orexin-B binds mainly to OX2 and is 5 times less potent at OX1.[9]		The orexins are strongly conserved peptides, found in all major classes of vertebrates.[10]		The orexin system was initially suggested to be primarily involved in the stimulation of food intake, based on the finding that central administration of orexin-A and -B increased food intake. In addition, it stimulates wakefulness, regulates energy expenditure, and modulates visceral function.		Obesity in orexin knockout mice is a result of inability of brown preadipocytes to differentiate into brown adipose tissue (BAT), which in turn reduces BAT thermogenesis. BAT differentiation can be restored in these knockout mice through injections of orexin. Deficiency in orexin has also been linked to narcolepsy, a sleep disorder. Furthermore, narcoleptic people are more likely to be obese. Hence obesity in narcoleptic patients may be due to orexin deficiency leading to impaired thermogenesis and energy expenditure.[11]		Orexin seems to promote wakefulness. Recent studies indicate that a major role of the orexin system is to integrate metabolic, circadian and sleep debt influences to determine whether an animal should be asleep or awake and active. Orexin neurons strongly excite various brain nuclei with important roles in wakefulness including the dopamine, norepinephrine, histamine and acetylcholine systems[12][13] and appear to play an important role in stabilizing wakefulness and sleep.		The discovery that an orexin receptor mutation causes the sleep disorder canine narcolepsy[14] in Doberman Pinschers subsequently indicated a major role for this system in sleep regulation. Genetic knockout mice lacking the gene for orexin were also reported to exhibit narcolepsy.[15] Transitioning frequently and rapidly between sleep and wakefulness, these mice display many of the symptoms of narcolepsy. Researchers are using this animal model of narcolepsy to study the disease.[16] Narcolepsy results in excessive daytime sleepiness, inability to consolidate wakefulness in the day (and sleep at night), and cataplexy, which is the loss of muscle tone in response to strong, usually positive, emotions. Dogs that lack a functional receptor for orexin have narcolepsy, while animals and people lacking the orexin neuropeptide itself also have narcolepsy.		Central administration of orexin-A strongly promotes wakefulness, increases body temperature and locomotion, and elicits a strong increase in energy expenditure. Sleep deprivation also increases orexin-A transmission. The orexin system may thus be more important in the regulation of energy expenditure than food intake. In fact, orexin-deficient narcoleptic patients have increased obesity rather than decreased BMI, as would be expected if orexin were primarily an appetite stimulating peptide. Another indication that deficits of orexin cause narcolepsy is that depriving monkeys of sleep for 30–36 hours and then injecting them with the neurochemical alleviates the cognitive deficiencies normally seen with such amount of sleep loss.[17][18]		In humans, narcolepsy is associated with a specific variant of the human leukocyte antigen (HLA) complex.[19] Furthermore, genome-wide analysis shows that, in addition to the HLA variant, narcoleptic humans also exhibit a specific genetic mutation in the T-cell receptor alpha locus.[20] In conjunction, these genetic anomalies cause the immune system to attack and kill the critical orexin neurons. Hence the absence of orexin-producing neurons in narcoleptic humans may be the result of an autoimmune disorder.[21]		Orexin increases the craving for food, and correlates with the function of the substances that promote its production. Orexin is also shown to increase meal size by suppressing inhibitory postingestive feedback.[22] However, some studies suggest that the stimulatory effects of orexin on feeding may be due to general arousal without necessarily increasing overall food intake.[23]		Leptin is a hormone produced by fat cells and acts as a long-term internal measure of energy state. Ghrelin is a short-term factor secreted by the stomach just before an expected meal, and strongly promotes food intake.		Orexin-producing cells have recently been shown to be inhibited by leptin (through the leptin receptor pathway), but are activated by ghrelin and hypoglycemia (glucose inhibits orexin production). Orexin, as of 2007, is claimed to be a very important link between metabolism and sleep regulation.[24][25] Such a relationship has been long suspected, based on the observation that long-term sleep deprivation in rodents dramatically increases food intake and energy metabolism, i.e., catabolism, with lethal consequences on a long-term basis. Sleep deprivation then leads to a lack of energy. In order to make up for this lack of energy, many people use high-carbohydrate and high-fat foods that ultimately can lead to poor health and weight gain. Other dietary nutrients, amino acids, also can activate orexin neurons, and they can suppress the glucose response of orexin neurons at physiological concentration, causing the energy balance that orexin maintains to be thrown off its normal cycle.[26]		Preliminary research has been conducted that shows potential for orexin blockers in the treatment of alcoholism. Lab rats given drugs which targeted the orexin system lost interest in alcohol despite being given free access in experiments.[27][28]		Because orexin-A receptors have been shown to regulate relapse to cocaine seeking, a new study investigated its relation to nicotine by studying rats. By blocking the orexin-A receptor with low doses of the selective orexin antagonist SB-334,867, nicotine self-administration decreased and also the motivation to seek and obtain the drug. The study showed that blocking of receptors in the insula decreased self-administration, but not blocking of receptors in the adjacent somatosensory cortex. The greatest decrease in self-administration was found when blocking all orexin-A receptors in the brain as a whole. A rationale for this study was the fact that the insula has been implicated in regulating feelings of craving. The insula contains orexin-A receptors. It has been reported that smokers who sustained damage to the insula lost the desire to smoke.[29]		Orexin-A (OXA) has been recently demonstrated to have a direct effect on an aspect of lipid metabolism. OXA stimulates glucose uptake in 3T3-L1 adipocytes and that increased energy uptake is stored as lipids (triacylglycerol). OXA thus increases lipogenesis. It also inhibits lipolysis and stimulates the secretion of adiponectin. These effects are thought to be mostly conferred via the PI3K pathway because this pathway inhibitor (LY294002) completely blocks OXA effects in adipocytes.[30] The link between OXA and the lipid metabolism is new and currently under more research.		Obesity in orexin-knockout mice is associated with impaired brown adipose tissue thermogenesis.[11]		High levels of orexin-A have been associated with happiness in human subjects, while low levels have been associated with sadness.[31] The finding suggests that boosting levels of orexin-A could elevate mood in humans, being thus a possible future treatment for disorders like depression.		Orexinergic neurons have been shown to be sensitive to inputs from Group III metabotropic glutamate receptors,[32] cannabinoid receptor 1 and CB1–OX1 receptor heterodimers,[33][34][35] adenosine A1 receptors,[36] muscarinic M3 receptors,[37] serotonin 5-HT1A receptors,[38] neuropeptide Y receptors,[39] cholecystokinin A receptors,[40] and catecholamines,[41][42] as well as to ghrelin, leptin, and glucose.[43] Orexinergic neurons themselves regulate release of acetylcholine,[44][45] serotonin, and noradrenaline.[46]		The orexin/hypocretin system the target of the insomnia medication suvorexant (trade name: Belsomra), which works by blocking both orexin receptors. Suvorexant has undergone three phase III trials and was approved on August 13, 2014 by the US Food and Drug Administration (FDA) after being denied approval the year before.[47] It is available as "Belsomra".[48]		In 2016, the University of Texas Health Science Center registered a clinical trial for the use of suvorexant for people with cocaine dependence. They plan to measure cue reactivity, anxiety and stress.[49]		Intranasal orexin is able to increase cognition in primates, especially under sleep deprived situations,[50] which may provide an opportunity for treatment excessive daytime sleepiness.[51]		A study has reported that transplantation of orexin neurons into the pontine reticular formation in rats is feasible, indicating the development of alternative therapeutic strategies in addition to pharmacological interventions to treat narcolepsy.[52]		
An amuse-bouche (/əˌm(j)uːzˈbuːʃ/, French pronunciation: ​[aˌmyzˈbuʃ])[1] or amuse-gueule (/əˌm(j)uːzˈɡəːl/, French pronunciation: ​[aˌmyzˈɡœl]) is a single, bite-sized hors d’œuvre.[2] Amuse-bouches are different from appetizers in that they are not ordered from a menu by patrons but are served gratis and according to the chef's selection alone. These, often accompanied by a complementing wine, are served both to prepare the guest for the meal and to offer a glimpse into the chef's approach to the art of cuisine.		The term is French and literally means "mouth amuser". The plural form is amuse-bouche or amuse-bouches.[3] In France, amuse-gueule is the proper term normally and traditionally employed in conversation and literary writing and amuse-bouche is not even listed in most dictionaries,[4] while amuse-bouche is a euphemistic hypercorrection that appeared in the 1980s[5] on restaurant menus and is used almost only there. In French, bouche refers to the human mouth, while gueule is also used to refer to the mouth or snout of an animal, but it is also a commonly used colloquial and familiar term for mouth that is derogatory in meaning only in certain expressions.[6][7]		The amuse-bouche emerged as an identifiable course during the nouvelle cuisine movement, which emphasized smaller, more intensely flavored courses.[8] It differs from other hors d'œuvres in that it is small, usually just one or two bites, and preselected by the chef and offered free of charge to all present at the table.		The functional role of the amuse-bouche could be played by rather simple offerings, such as a plate of olives or a crock of tapenade. It often becomes a showcase, however, due to the artistry and showmanship of the chef, intensified by the competition among restaurants. According to Jean-Georges Vongerichten, a popular New York celebrity chef with restaurants around the world, "The amuse-bouche is the best way for a great chef to express his or her big ideas in small bites".[9]		At some point, the amuse-bouche transformed from an unexpected bonus to a de rigueur offering at Michelin Guide-starred restaurants and those aspiring to that category (as recently as 1999, The New York Times provided a parenthetical explanation of the course).[10] This in turn created a set of logistical challenges for restaurants: amuse-bouche must be prepared in sufficient quantities to be served to all guests, usually just after the order is taken or between main courses. This often requires a separate cooking station devoted solely to producing the course quickly as well as a large and varied collection of specialized china for serving the amuse. Interesting plates, demitasse cups, and large Asian-style soup spoons are popular choices. In addition, the kitchen must try to accommodate guests that have an aversion or allergy to ingredients in the amuse.[11]		A Japanese-influenced amuse-bouche: hamachi, salmon roe, basil, basil flower		
Instant breakfast typically refers to breakfast food products that are manufactured in a powdered form, which are generally prepared with the addition of milk and then consumed as a beverage.[1][2] Some instant breakfasts are produced and marketed in liquid form, being pre-mixed. The target market for instant breakfast products includes consumers who tend to be busy, such as students and working adults.[2]						Powdered instant breakfast has been described as a breakfast substitute, used as a quick meal replacement in place of traditional quickly prepared breakfast foods[2] such as bacon and eggs,[1][2] oatmeal and pancakes.		Carnation-brand Instant Breakfast was introduced in 1964.[3] It is a powdered instant beverage that is manufactured with protein, vitamins and minerals and sugar.[2] It's typically prepared with milk,[2] and is available in different flavors, such as chocolate, vanilla and strawberry.[4] Powdered forms are marketed in individual-serving packets and in cans. Carnation also manufactures prepared bottled instant breakfast drinks in liquid form.		Another type of instant breakfasts are frozen ready-made meals that are heated in a microwave oven or standard oven.		
Gastrointestinal is an adjective meaning of or pertaining to the stomach and intestines. A tract is a collection of related anatomic structures or a series of connected body organs.		The gastrointestinal tract (digestive tract, GI tract, GIT, gut, or alimentary canal) is an organ system within humans and other animals which takes in food, digests it to extract and absorb energy and nutrients, and expels the remaining waste as feces and urine. The mouth, oesophagus, stomach, and intestines are part of the human alimentary canal.		All bilaterians have a gastrointestinal tract, also called a gut or an alimentary canal. This is a tube that transfers food to the organs of digestion.[1] In large bilaterians, the gastrointestinal tract generally also has an exit, the anus, by which the animal disposes of feces (solid wastes). Some small bilaterians have no anus and dispose of solid wastes by other means (for example, through the mouth).[2]		The gastrointestinal tract contains thousands of different bacteria in their gut flora.[3]		The human gastrointestinal tract consists of the esophagus, stomach, and intestines, and is divided into the upper and lower gastrointestinal tracts.[4] The GI tract includes all structures between the mouth and the anus,[5] forming a continuous passageway that includes the main organs of digestion, namely, the stomach, small intestine, and large intestine. In contrast, the human digestive system comprises the gastrointestinal tract plus the accessory organs of digestion (the tongue, salivary glands, pancreas, liver, and gallbladder).[6] The tract may also be divided into foregut, midgut, and hindgut, reflecting the embryological origin of each segment.		The whole human GI tract is about nine metres (30 feet) long at autopsy. It is considerably shorter in the living body because the intestines, which are tubes of smooth muscle tissue, maintain constant muscle tone, somewhat like a slinky that maintains itself in a halfway-tense state but can relax in spots to allow for local distention, peristalsis, and so on.[citation needed]		The GI tract releases hormones from enzymes to help regulate the digestive process. These hormones, including gastrin, secretin, cholecystokinin, and ghrelin, are mediated through either intracrine or autocrine mechanisms, indicating that the cells releasing these hormones are conserved structures throughout evolution.[7]						The structure and function can be described both as gross anatomy and as microscopic anatomy or histology. The tract itself is divided into upper and lower tracts, and the intestines small and large parts.[8]		The upper gastrointestinal tract consists of the buccal cavity, pharynx, esophagus, stomach, and duodenum.[9] The exact demarcation between the upper and lower tracts is the suspensory muscle of the duodenum. This delineates the embryonic borders between the foregut and midgut, and is also the division commonly used by clinicians to describe gastrointestinal bleeding as being of either "upper" or "lower" origin. Upon dissection, the duodenum may appear to be a unified organ, but it is divided into four segments based upon function, location, and internal anatomy. The four segments of the duodenum are as follows (starting at the stomach, and moving toward the jejunum): bulb, descending, horizontal, and ascending. The suspensory muscle attaches the superior border of the ascending duodenum to the diaphragm.		The suspensory muscle is an important anatomical landmark which shows the formal division between the duodenum and the jejunum, the first and second parts of the small intestine, respectively.[10] This is a thin muscle which is derived from the embryonic mesoderm.		The lower gastrointestinal tract includes most of the small intestine and all of the large intestine.[11] In human anatomy, the intestine (bowel, or gut) is the segment of the gastrointestinal tract extending from the pyloric sphincter of the stomach to the anus and, in humans and other mammals, consists of two segments, the small intestine and the large intestine. In humans, the small intestine is further subdivided into the duodenum, jejunum and ileum while the large intestine is subdivided into the cecum, colon, rectum, and anal canal.[12][13]		The small intestine begins at the duodenum, which receives food from the stomach. It is a tubular structure, usually between 6 and 7 m long.[14] The area of the human, adult small intestinal mucosa is about 30 m2.[15] Its main function is to absorb the products of digestion (including carbohydrates, proteins, lipids, and vitamins) into the bloodstream. It has three major divisions:		The large intestine also called the colon, consists of the cecum, rectum, and anal canal. It also includes the appendix, which is attached to the cecum. The colon is further divided into:		The main function of the large intestine is to absorb water. The area of the large intestinal mucosa of an adult human is about 2 m2.[16]		The gut is an endoderm-derived structure. At approximately the sixteenth day of human development, the embryo begins to fold ventrally (with the embryo's ventral surface becoming concave) in two directions: the sides of the embryo fold in on each other and the head and tail fold toward one another. The result is that a piece of the yolk sac, an endoderm-lined structure in contact with the ventral aspect of the embryo, begins to be pinched off to become the primitive gut. The yolk sac remains connected to the gut tube via the vitelline duct. Usually this structure regresses during development; in cases where it does not, it is known as Meckel's diverticulum.		During fetal life, the primitive gut is gradually patterned into three segments: foregut, midgut, and hindgut. Although these terms are often used in reference to segments of the primitive gut, they are also used regularly to describe regions of the definitive gut as well.		Each segment of the gut is further specified and gives rise to specific gut and gut-related structures in later development. Components derived from the gut proper, including the stomach and colon, develop as swellings or dilatations in the cells of the primitive gut. In contrast, gut-related derivatives — that is, those structures that derive from the primitive gut but are not part of the gut proper, in general develop as out-pouchings of the primitive gut. The blood vessels supplying these structures remain constant throughout development.[17]		The gastrointestinal tract has a form of general histology with some differences that reflect the specialization in functional anatomy.[18] The GI tract can be divided into four concentric layers in the following order:		The mucosa is the innermost layer of the gastrointestinal tract. that is surrounding the lumen, or open space within the tube. This layer comes in direct contact with digested food (chyme). The mucosa is made up of:		The mucosae are highly specialized in each organ of the gastrointestinal tract to deal with the different conditions. The most variation is seen in the epithelium.		The submucosa consists of a dense irregular layer of connective tissue with large blood vessels, lymphatics, and nerves branching into the mucosa and muscularis externa. It contains the submucosal plexus, an enteric nervous plexus, situated on the inner surface of the muscularis externa.		The muscular layer consists of an inner circular layer and a longitudinal outer layer. The circular layer prevents food from traveling backward and the longitudinal layer shortens the tract. The layers are not truly longitudinal or circular, rather the layers of muscle are helical with different pitches. The inner circular is helical with a steep pitch and the outer longitudinal is helical with a much shallower pitch.[19]		Between the two muscle layers is the myenteric plexus. This controls peristalsis. Activity is initiated by the pacemaker cells, (myenteric interstitial cells of Cajal). The gut has intrinsic peristaltic activity (basal electrical rhythm) due to its self-contained enteric nervous system. The rate can be modulated by the rest of the autonomic nervous system.[19]		The coordinated contractions of these layers is called peristalsis and propels the food through the tract. Food in the GI tract is called a bolus (ball of food) from the mouth down to the stomach. After the stomach, the food is partially digested and semi-liquid, and is referred to as chyme. In the large intestine the remaining semi-solid substance is referred to as faeces.[19]		The outermost layer of the gastrointestinal tract consists of several layers of connective tissue.		Intraperitoneal parts of the GI tract are covered with serosa. These include most of the stomach, first part of the duodenum, all of the small intestine, caecum and appendix, transverse colon, sigmoid colon and rectum. In these sections of the gut there is clear boundary between the gut and the surrounding tissue. These parts of the tract have a mesentery.		Retroperitoneal parts are covered with adventitia. They blend into the surrounding tissue and are fixed in position. For example, the retroperitoneal section of the duodenum usually passes through the transpyloric plane. These include the esophagus, pylorus of the stomach, distal duodenum, ascending colon, descending colon and anal canal. In addition, the oral cavity has adventitia.		The time taken for food or other ingested objects to transit through the gastrointestinal tract varies depending on many factors, but roughly, it takes less than an hour after a meal for 50% of stomach contents to empty into the intestines while total emptying takes around 2 hours. Subsequently, 50% emptying of the small intestine takes between 1 and 2 hours. Finally, transit through the colon takes 12 to 50 hours with wide variation between individuals.[20][21]		The gastrointestinal tract forms an important part of the immune system.[22] The surface area of the digestive tract is estimated to be about 32 square meters, or about half a badminton court.[23] With such a large exposure (more than three times larger than the exposed surface of the skin), these immune components function to prevent pathogens from entering the blood and lymph circulatory systems.[24] Fundamental components of this protection are provided by the intestinal mucosal barrier which is composed of physical, biochemical, and immune elements elaborated by the intestinal mucosa.[25] Microorganisms also are kept at bay by an extensive immune system comprising the gut-associated lymphoid tissue (GALT)		There are additional factors contributing to protection from pathogen invasion. For example, low pH (ranging from 1 to 4) of the stomach is fatal for many microorganisms that enter it.[26] Similarly, mucus (containing IgA antibodies) neutralizes many pathogenic microorganisms.[27] Other factors in the GI tract contribution to immune function include enzymes secreted in the saliva and bile.		Beneficial bacteria also can contribute to the gastrointestinal system homeostasis. A case in point is the relationship between human gut and Clostridia, one of the most predominant bacterial groups in the gastrointestinal tract. Clostridia play an important role influencing the dynamics of our immune system in the gut.[28] It has been demonstrated that the intake of a high fiber diet could be the responsible for the induction of Treg cells. This is due to the production of short-chain fatty acids during the fermentation of plant derived nutrients such as butyrate and propionate. Basically, the butyrate induces the differentiation of Treg cells by enhancing histone H3 acetylation in the promoter and conserved non-coding sequence regions of the Foxp3 locus, and thus regulating the T cells, having as a result the reduction of the inflammatory response and allergies.		The large intestine hosts several kinds of bacteria that can deal with molecules that the human body cannot otherwise break down.[29] This is an example of symbiosis. These bacteria also account for the production of gases at host-pathogen interface, inside our intestine(this gas is released as flatulence when eliminated through the anus). However the large intestine is mainly concerned with the absorption of water from digested material (which is regulated by the hypothalamus) and the re absorption of sodium, as well as any nutrients that may have escaped primary digestion in the ileum.[citation needed]		Health-enhancing intestinal bacteria of the gut flora serve to prevent the overgrowth of potentially harmful bacteria in the gut. These two types of bacteria compete for space and "food," as there are limited resources within the intestinal tract. A ratio of 80-85% beneficial to 15–20% potentially harmful bacteria generally is considered normal within the intestines.[citation needed]		Enzymes such as CYP3A4, along with the antiporter activities, are also instrumental in the intestine's role of drug metabolism in the detoxification of antigens and xenobiotics.[30]		There are many diseases and conditions that can affect the gastrointestinal system, including infections, inflammation and cancer.		Various pathogens can cause gastroenteritis an inflammation of the stomach and small intestine. These can include those organisms that cause foodborne illnesses. Gastroenteritis is the most common disease of the GI tract.		Diverticular disease is a condition that is very common in older people in industrialized countries. It usually affects the large intestine but has been known to affect the small intestine as well. Diverticulosis occurs when pouches form on the intestinal wall. Once the pouches become inflamed it is known as diverticulitis.		Inflammatory bowel disease is an inflammatory condition affecting the bowel walls, and includes the subtypes Crohn's disease and ulcerative colitis. While Crohn's can affect the entire gastrointestinal tract, ulcerative colitis is limited to the large intestine. Crohn's disease is widely regarded as an autoimmune disease. Although ulcerative colitis is often treated as though it were an autoimmune disease, there is no consensus that it actually is such.		Functional gastrointestinal disorders the most common of which is irritable bowel syndrome. Functional constipation and chronic functional abdominal pain are other functional disorders of the intestine that have physiological causes, but do not have identifiable structural, chemical, or infectious pathologies.		Several symptoms are used to indicate problems with the gastrointestinal tract:		Gastrointestinal surgery can often be performed in the outpatient setting. In the United States in 2012, operations on the digestive system accounted for 3 of the 25 most common ambulatory surgery procedures and constituted 9.1 percent of all outpatient ambulatory surgeries.[31]		Various methods of imaging the gastrointestinal tract include the upper and lower gastrointestinal series:		Animal intestines have multiple uses. From each species of livestock that is a source of milk, a corresponding rennet is obtained from the intestines of milk-fed calves. Pig and calf intestines are eaten, and pig intestines are used as sausage casings. Calf intestines supply calf-intestinal alkaline phosphatase (CIP), and are used to make goldbeater's skin. Other uses are:		Many birds and other animals have a specialised stomach in the digestive tract called a gizzard used for grinding up food.		Another feature not found in the human but found in a range of other animals is the crop. In birds this is found as a pouch alongside the esophagus.		Other animals including amphibians, birds, reptiles, and egg-laying mammals have a major difference in their GI tract in that it ends in a cloaca and not an anus.		
Apéritifs and digestifs (/əˈpɛrᵻtiːf/ and /diːʒɛˈstiːf/) are drinks, typically alcoholic, that are normally served before (apéritif) or after (digestif) a meal.						An apéritif is an alcoholic beverage usually served before a meal to stimulate the appetite, and is therefore usually dry rather than sweet. Common choices for an apéritif are vermouth; champagne; pastis; gin; rakı; fino, amontillado or other styles of dry sherry (but not usually cream sherry, which is very sweet and rich); and any still, dry, light white wine.		Apéritif may also refer to a snack that precedes a meal. This includes an amuse-bouche, such as crackers, cheese, pâté or olives.[1][2]		Apéritif is a French word derived from the Latin verb aperire, which means "to open". The French slang word for apéritif is apéro, although in France an apéro is also food eaten in the late afternoon or early evening.		St. Diadochos of Photiki, in his writing on the Christian spiritual life, On Spiritual Knowledge, states, "People who wish to discipline the sexual organs should avoid drinking those artificial concoctions which are called 'aperitifs'—presumably because they open a way to the stomach for the vast meal which is to follow." Diadochos was born around 400 A.D. and died sometime before 486 A.D. He was from Northern Greece and was a significant theologian during the Council of Chalcedon (451). Therefore, the use of apéritifs to affect one's consumption of food was a practice in the 5th century.[3]		In 1796, Turin distiller Antonio Carpano invented modern vermouth.[4][5]		The apéritif was introduced in France in 1846[citation needed] when a French chemist, Joseph Dubonnet, created his eponymous wine-based drink as a means of delivering malaria-fighting quinine. The medicine was a bitter brew, so he developed a formula of herbs and spices to mask quinine's sharp flavor, and it worked so well that the recipe has remained well-guarded ever since. French Foreign Legion soldiers made use of it in mosquito-infested Northern Africa. Dubonnet's wife was so fond of the drink that she had all her friends try it, and its popularity spread.[citation needed]		Apéritifs were already widespread in the 19th century in Italy, where they were being served in fashionable cafés in Rome, Genoa, Florence, Milan, Turin, and Venice. Apéritifs became very popular in Europe in the late 19th century. The popularity in Europe crossed the Atlantic and by 1900, they were also commonly served in the United States. The apéritif recrossed the Atlantic in the 1970s: the habit of a substantial food offering with the purchase of a drink during "Happy Hour" in the United States pushed the development of a more food-heavy apéritif in Italy as well.[6] In Spain and in some countries of Latin America, apéritifs have been a staple of tapas for centuries.[citation needed]		There is no single alcoholic drink that is always served as an apéritif. Fortified wine, liqueur, and dry champagne are probably the most common choices. Because it is served before dining, the emphasis is usually on dry rather than sweet, as a general guideline.		A digestif is an alcoholic beverage served after a meal, to aid digestion.[7] When served after a coffee course, it may be called pousse-café.[8] Digestifs are usually taken straight. Common kinds of digestif include:		In certain areas, it is not uncommon for a digestif to be taken before a main course. One example is le trou Normand, a glass of Calvados taken before the main course of a meal.		Bitter digestifs typically contain carminative herbs, which are thought to aid digestion.[9]		In many countries, people drink alcoholic beverages at lunch and dinner. Studies have found that when food is eaten before drinking alcohol, alcohol absorption is reduced[10] and the rate at which alcohol is eliminated from the blood is increased. The mechanism for the faster alcohol elimination appears to be unrelated to the type of food. The likely mechanism is food-induced increases in alcohol-metabolizing enzymes and liver blood flow.[10]		
Soups & stews		Banchan		Tteok		Korean table d'hôte,[1] called han-jeongsik (한정식; 韓定食) in Korean,[1] is a Korean-style full-course meal characterized by the array of small banchan plates in varied colours.[2][3][4][5]		
Activities of daily living (ADLs or ADL) is a term used in healthcare to refer to people's daily self care activities. The concept of ADLs was originally proposed in the 1950s by Sidney Katz and his team at the Benjamin Rose Hospital in Cleveland, OH and has been added to and refined by a variety of researchers since that time.[1] Health professionals often use a person's ability or inability to perform ADLs as a measurement of their functional status, particularly in regard to people post injury, with disabilities and the elderly.[2] Younger children often require help from adults to perform ADLs, as they have not yet developed the skills necessary to perform them independently.		ADLs are defined as "the things we normally do... such as feeding ourselves, bathing, dressing, grooming, work, homemaking, and leisure."[3] A number of national surveys collect data on the ADL status of the U.S. population.[4] While basic definitions of ADLs have been suggested, what specifically constitutes a particular ADL for each individual may vary. Adaptive equipment and devices may be used to enhance and increase independence in performing ADLs.						Basic ADLs consist of self-care tasks that include, but are not limited to:[5]		One way to think about basic ADLs is that they are the things many people do when they get up in the morning and get ready to go out of the house: get out of bed, go to the toilet, bathe, dress, groom, and eat.		There is a hierarchy to the ADLs:" ... the early loss function is hygiene, the mid-loss functions are toilet use and locomotion, and the late loss function is eating. When there is only one remaining area in which the person is independent, there is a 62.9% chance that it is eating and only a 3.5% chance that it is hygiene." [6]		Although not in wide general use, a mnemonic that some find useful is DEATH: dressing/bathing, eating, ambulating (walking), toileting, hygiene.[7]		Instrumental activities of daily living (IADLs) are not necessary for fundamental functioning, but they let an individual live independently in a community:[8][9]		A useful mnemonic is SHAFT: shopping, housekeeping, accounting, food preparation/meds, telephone/transportation.		Occupational therapists often evaluate IADLs when completing patient assessments. The American Occupational Therapy Association identifies 12 types of IADLs that may be performed as a co-occupation with others:[10]		Occupational therapists use exercises to assist patients in maintaining and gaining independence in ADLs. The exercise program is based on what components patients are lacking such as walking speed, strength, balance, and coordination. Slow walking speed is associated with increased risk of falls. Exercise enhances walking speed, allowing for safer and more functional ambulation capabilities. After initiating an exercise program it is important to maintain the routine otherwise the benefits will be lost.[11] Exercise for patients who are frail is essential for preserving functional independence and avoiding the necessity for care from others or placement in a long term care facility.[12]		Assisting in activities of daily living are skills required in nursing and as well as other professions such as nursing assistants. This includes assisting in patient mobility, such as moving an activity intolerant patient within bed. For hygiene, this often involves bed baths and assisting with urinary and bowel elimination.		There are several evaluation tools, such as the Katz ADL scale,[13] the Older Americans Resources and Services (OARS) ADL/IADL scale, the Lawton IADL scale and the Bristol Activities of Daily Living Scale.		Most models of health care service use ADL evaluations in their practice, including the medical (or institutional) models, such as the Roper-Logan-Tierney model of nursing, and the resident-centered models, such as the Program of All-Inclusive Care for the Elderly (PACE).		ADL evaluations are used increasingly in epidemiological studies as an assessment of health in later-life that does not necessarily involve specific ailments. Studies using ADL differ from those investigating specific disease outcomes, as they are sensitive to a broader spectrum of health effects, at lower-levels of impact. ADL is measured on a continuous scale, making the process of investigation fairly straightforward.		Sidney Katz initially studied 64 hip fracture patients over an 18-month period. Comprehensive data on treatments, patient progression, and outcomes were collected during this study. After analyzing the study data, the researchers discovered that the patients they viewed as being most independent could perform a set of basic activities – ranging from the most complex bathing activity, to the least complex feeding activity. From these data, Katz developed a scale to assess patients' ability to live independently.[14] This was first published in the 1963 in the Journal of the American Medical Association; the paper has since been cited over 1,000 times.[15]		Although the scale offers a standardized measure for psychological and biological function, the process of arriving at this assumption has been criticised. Specifically, Porter has argued for a phenomenological approach noting that:		Katz et al. (1963) made a claim that became the basis for the ontological assumptions of the ADL research tradition. In their suggestion that there was an "ordered regression [in skills] as part of the natural process of aging" (p. 918), there was an implicit generalization, from their sample of older persons with fractured hips, to all older persons.[16]		Porter emphasizes the possible disease-specific nature of ADLs (being derived from hip-fracture patients), the need for objective definition of ADLs, and the possible value of adding additional functional measures.[16]		A systematic review examined the effectiveness of imparting activities of daily life skills programmes for people with chronic mental illnesses:		
Antipasto (plural antipasti) is the traditional first course of a formal Italian meal. Typical ingredients of a traditional antipasto include cured meats, olives, peperoncini, mushrooms, anchovies, artichoke hearts, various cheeses (such as provolone or mozzarella), pickled meats, and vegetables in oil or vinegar.		The contents of an antipasto vary greatly according to regional cuisine. It is quite possible to find different preparations of saltwater fish and traditional southern cured meats (like soppressata or 'nduja) in the south of Italy, whereas in northern Italy it will contain different kinds of cured meats and mushrooms and, especially near lakes, preparations of freshwater fish. The cheeses included also vary significantly between regions and backgrounds, and include hard and soft cheeses.		Many compare antipasto to hors d'oeuvre, but antipasto is served at the table and signifies the official beginning of the Italian meal. It may also be referred to as a starter, or an appetizer.		
Child care or childcare, child minding, daycare, or preschool is the caring for and supervision of a child or children, usually from age six weeks to age thirteen. Child care is the action or skill of looking after children by a day-care center, babysitter, or other providers. Child care can also include advanced learning environments that include early childhood education. Child care is a broad topic covering a wide spectrum of contexts, activities, social and cultural conventions, and institutions. The majority of child care institutions that are available require that child care providers have extensive training in first aid and are CPR certified. In addition, background checks, drug testing at all centers, and reference verification are normally a requirement. Child care can cost up to $15,000 for one year in the United States. The average annual cost of full-time care for an infant in center-based care ranges from $4,863 in Mississippi to $16,430 in Massachusetts.[1]		Early child care is a very important and often overlooked component of child development[citation needed]. Child care providers are our children's first teachers, and therefore play an integral role in our systems of early childhood education[citation needed]. Quality care from a young age can have a huge impact on the future successes of children[citation needed]						It is traditional in Western society for children to be taken care of by their parents or their legal guardians. In families where children live with one or both of their parents, the childcare role may also be taken on by the child's extended family. If a parent or extended family is unable to care for the children, orphanages and foster homes are a way of providing for children's care, housing, and schooling.		The two main types of child care options for employed parents needing childcare are center-based care (including creches, daycare, and preschools) and home-based care (also known as nanny or family daycare). As well as these licensed option's parents may also choose to find their own caregiver or arrange childcare exchanges/swaps with another family.[2]		Licensed or unlicensed home day care is also referred to as family child care, or in home care. It refers to the care provided to a group of children in the home of a caregiver. State laws differ regarding rules for licensed versus unlicensed care. In Canada, most home daycares are unlicensed, and this is completely lawful. Licensing home daycares in Canada can help greatly with oversight, but at the cost of a large portion of the daycare provider's pay. Family child cares are small in size and provide families the same securities as a day care center, and also has the benefits of flexible hours, lower costs, accessibility, and cultural compatibility. Home-based providers can give more individualized care and therefore better meet the needs of working families. In addition, family care generally has a small ratio of children in care, allowing for more interaction between child and provider than would be had at a commercial care center. Family child care helps foster emotionally secure interpersonal relationships for everyone involved. The providers are able to communicate each day with parents on a personal level and share information about the development of the child. Providers care for multi-aged groups of children allowing children to remain with one caregiver for many years which helps children develop a sense of trust and security. Multi-aged settings allow children to learn from one another and allow siblings to stay together. Some family child care providers may offer parents more flexibility with hours of operation such as evening, weekend, overnight, and before and after school care.		In home care is typically provided by nannies, au pairs, or friends and family.[3] The child is watched inside their own home or the caregiver's home, reducing exposure to outside children and illnesses. Depending on the number of children in the home, the children utilizing in-home care enjoy the greatest amount of interaction with their caregiver, forming a close bond. There are no required licensing or background checks for in-home care, making parental vigilance essential in choosing an appropriate caregiver. Nanny and au pair services provide certified caregivers and the cost of in-home care is the highest of childcare options per child, though a household with many children may find this the most convenient and affordable option. Many nannies study towards childcare qualifications. This means they are trained to create a safe and stimulating environment for your child to enjoy and thrive in. Typically, au pairs or nannies provide more than routine child care, often assisting with daily household activities, including running errands, shopping, doing laundry, fixing meals, and cleaning house.		At the same time, a nanny or au pair is not always the best methods of childcare. Nanny care is the most expensive form of childcare. Recruiting a nanny can be costly when using a Nanny agency. Nanny agencies will, however, thoroughly check the applicants' references and run a criminal background check on the successful candidate.[4] Weekly salaries for nannies are 2 to 3 times the cost of a week of daycare.[5] It confines the child into a world of their own. It keeps them from interacting with other children a lot of the time. As mentioned the caregivers do not need licenses or background checks so there is no way of telling if a person is really qualified or has a criminal background (unless you live in a country where there is an option of obtaining home-based care through a government licensed and funded agency). These things should be taken in consideration when making a choice.		Family child care providers care for children in the providers' own home. The children are in a mixed age group with a low adult to child ratio. Care can be more personalized and individual. The hours may be more flexible and the provider may offer evening and weekend care for mothers who work second or third shift. The cost of care in a family child care is lower on average than that of a center.		Child care facilities in the US have the option of becoming accredited. This standard is set and regulated by an outside agency. In centers, NAEYC institutes it.[6] For family child care providers, the National Association of Family Child Care Providers award the credentials.[7]		Commercial care center also known as day cares are open for set hours, and provide a standardized and regulated system of care for children. Parents may choose from a commercial care center close to their work, and some companies offer care at their facilities. Active children may thrive in the educational activities provided by a quality commercial care center, but according to the National Center for Early Development and Learning, children from low quality centers may be significantly less advanced in terms of vocabulary and reading skills.[8] Classes are usually largest in this type of care, ratios of children to adult caregivers will vary according to state licensing requirements. Some positives of commercial care are children gain independence, academic achievement, and socialization.[9]		Pre-school is often the term used to refer to child care centers that care primarily for 3 and 4-year old children. Preschool can be based in a center, family child care home or a public school. Head Start is a federally funded program for low income children ages 3 and 4 and their families. Similarly Early Head Start serves low income children birth to 3 years of age.[10] The cost for the Head Start program is estimated at $9,000 per child. Head Start program provides federal grants directly to local agencies to provide comprehensive child development services for low-income children and families. Today, Head Start serves more than one million low-income children. Head Start programs aim to promote school readiness by enhancing the social and cognitive development of children through the provision of educational, health, nutritional, social and other services to enrolled children and families.[11]		Infants may also be cared for in infant and child care centers. Resources for Infant Educators is a non-profit worldwide organization, founded by the late Magda Gerber, a specialist in Infant Care.		Another method of child care is for before and/or after school: the YMCA program. There are buses that bring the child to the location. YMCA website claims that its programs are staffed with people who understand the cognitive, physical, and social development of kids, the need children have to feel connected and supported in trying new things, and the caring and reinforcement parents and families need to help each other. The YMCA aims to enable preschoolers to experience early literacy and learn about their world, and school-age kids make friends, learn new skills, and do homework.[12]		Most western countries also have compulsory education during which the great majority of children are at school starting from five or six years of age. The school will act in loco parentis meaning "in lieu of parent supervision".		In many locales, government is responsible for monitoring the quality of care. For instance, in Scotland Her Majesty's Inspectorate of Education is responsible for improving care and education for children from birth to eighteen. This is implemented by inspections carried out by HMIE itself or by other members of inspection and review teams. Inspection reports include feedback from staff and parents as well as the inspectors, aiming to provide parents and carers information to help them decide whether a particular child care setting is providing good quality child care and meeting government standards.[13]		Informal childcare is a variation of childcare that utilizes family members as a childcare system, for example grandparents and siblings. Informal childcare is an especially inexpensive form of childcare, and is utilized typically by those who are considered poor. Parents may need to utilize informal care for a variety of reasons. Typically informal childcare is necessary for families who do not have enough funds to finance placing their children in a more expensive child care facility. Those low income families are also more apt to work longer hours on an irregular and inflexible schedule, which ultimately makes using a childcare facility that has regular business hours unlikely. A study done by Roberta Iversen and Annie Armstrong explains that due to long and irregular working hours, sometimes including evenings and weekends, poor parents are more likely to utilize informal childcare.[14]		Unlike those children who receive center-based or home based childcare, those children who receive informal childcare do not receive the same educational preparation and school readiness that center-based and home based children receive. In his book Social Inequality and Social Stratification in US Society, sociologist Christopher Doob finds that poor children are less likely to attend the center-based and home based childcare programs, which Doob finds that informal care thus results in the less developed school-related skills children need. Doob concludes that due to a lack of financial capital, poor families are thus subject to substandard amounts of human capital, which results in lower quality childcare programs, and ultimately leaves children at a cognitive disadvantage.[15]		In England, childcare is inspected and regulated by OFSTED (previously this was administered by Local Authority Social Services). Care for children under five is split into Childcare on Domestic Premises which is Childminding and Daycare. In the UK being a ‘Childminder’ is a protected title and can only be used by registered professionals. Registered Childminders are trained, insured and qualified in Pediatric First Aid. They comply/administer/work with The Early Years Foundation Stage EYFS and have the same responsibilities for education as nurseries and reception classes. They generally work from their own homes and are always self-employed setting their own terms and conditions. The basic numbers of children that childminders can care for is 6 children under 8 years of age; of these children, 3 may be under 5 and of these 1 may be under 1. These numbers include the childminders own children (although the childminder’s children will not be included in the childminding ‘Certificate’). Some childminders work with either childminding assistants or with co-childminders, which often increases the number of children that can be cared for and individual childminders can request a ‘variation’ which may increase the children that they care for particularly for ‘continuity of care’ or for twins. There is a professional body – the Professional Association for Childcare & Early Years (formerly the National Childminding Association), which “Promotes and supports quality child-minding expertise” and provides information for Childminders and parents. London has greater pressures on childcare provision than other English regions. A recent study by London’s Poverty Profile found the level of childcare provision in London is lower than the England average. In London, there are 4.4 children aged under 8 per childcare place, compared to the England average of 3.9.[16]		Childcare costs in London significantly hinder the living standards of the capital’s residents. A recent study by Loughborough University, funded by Trust for London, found the minimum budget required for a couple with two children to reach a decent standard of living is 22% more in Inner London and 21% more in Outer London than compared with the rest of the UK. The significantly higher costs of childcare influences this heavily, along with housing and transport.[17]		For many, the use of paid childcare is a matter of choice with arguments on both sides about whether this is beneficial or harmful[18] to children. The parental decisions of leaving a child with someone and who that someone will be are two of the most difficult decisions in the lives of most parents.[19] A parent fears for the safety and security of his/her child. They need to be able trust the person or facility they choose as a provider for childcare. Whether this person is family, friend, live in, center based, young, old, well educated, or barely trained, the parents want to feel comfortable leaving their children with them. To have trust in the caregiver, the parent wants to know what kind of effects the type of service they provide will have on the development of their child. The development of a child has many factors, but it is most directly influenced by the type and quality of care that is most regularly provided to the child.		Child development researcher, Lian Tong, analysed the results from a Haley and Stansbury experiment saying, "Parent responsiveness also facilitates cognitive, social, and emotional development and reduces negative emotions in infants."[20] This study applies to more age groups than just infants. To sum that up, the amount of time that a parent or teacher is willing to spend teaching, listening to, playing with, and exploring with the child the more socially, emotionally, and educationally developed the child will become. Whether that child receives the majority of his or her care at a center or at its house, the biggest factor in deciding what will have the best effect on the child will be those willing to put in the time and effort it takes to properly develop a child's social, physical, and academic skills.		In discussing the numbers it is important to note that in 2001, more than one half of the children in the United States attended childcare facilities. This number has only increased as the number of working parents has increased. The increase in the amount of children that are required to have some sort of childcare service has made childcare facilities more necessary than they have ever been.[21] The quality of childcare given by a facility is generally indicated by the center's cost of enrollment. If the center charges more for the service, it will generally provide better care to the children. Centers that charge more for their services can provide quality education, more current resources, and nicer facilities. These are all helpful when trying to educate a child academically. A higher standard for teachers, such as requiring a degree in early childhood education or a degree of the like, has shown to result in improved growth in the development of a child. The childcare system in France is a great example of this. They have two separate branches of early childhood childcare. These two separate branches are called crèche and école maternelle. Crèche is the program for infants and toddlers and école maternelle is part of the education system. They both require teacher to have a college degree and sometimes a specialized degree on top of that.[19]		Whether at an expensive facility or relatively inexpensive, children who attend daycare facilities tend to develop social skills more quickly than children of the same age group that are reared at home. They communicate better with children of the same age and often try harder to communicate with those that are younger than them, by using patience and taking different approaches at presenting the data.[22] Surprisingly, a study done by Erik Dearing, has proven that negative social behavioral patterns are not directly connected to daycare. By studying a large selection of children from the Norwegian childcare system he concluded that the amount of hours a child spends at a daycare and their behavior have no dependent relations.[23] Though in America, Children who attend childcare systems have a higher risk of externalizing the symptoms of negative social behavior, exhibiting these traits can directly correlate with their time spent in the center.[24]		There are links between the income, education, and importance of consistency and the well being of the child, to the parents, and the development of their child. Higher educated parents place more importance on the education of their children than the parents who do not have a college degree or have not graduated from high school. Likewise, parents who have a higher income level are more willing to part with their money to purchase a private tutor or nanny to assist the parent in the education of their child. They also tend to stress the importance of being socially inept.[20] The first few years of a child's life are important to form a basis for good education, morality, self-discipline and social integration. Consistency of approach, skills and qualifications of caregivers have been shown in many studies to improve the chances of a child reaching his or her full potential. Child care in much of western society is currently in crisis: there are not enough daycare spots, the cost for most parents is beyond their means, and child care staff are grossly underpaid. Starting wages for Early Childcare Educators start at $11 or $12, causing a high turnover rate, and decreases the likelihood of potentially safe, effective, and loving child care providers from even entering the field.		Childcare infection is the spread of infection during childcare, typically because of contact among children in daycare or school.[25] This happens when groups of children meet in a childcare environment, and there any individual with an infectious disease may spread it to the entire group. Commonly spread diseases include influenza-like illness and enteric illnesses, such as diarrhea among babies using diapers. Illnesses and diseases may also include ring-worm, head lice, and hand, feet, mouth disease. It is uncertain how these diseases spread, but hand washing reduces some risk of transmission and increasing hygiene in other ways also reduces risk of infection.[26]		Parents and mothers especially spend a significant amount of time raising their children. These mothers nurture and develop their children into being functional members of society- hard work that is not motivated by monetary gain. For centuries it has been assumed that women will stay home and take care of the children while their husbands go out and work. In most cases, the husbands get all the credit for providing for the family. However, their homemaker wives deserve just as much credit for their care work. Caregivers do not receive monetary compensation and they must pay a ‘care-penalty.[27]		A care-penalty is the price one pays for doing care work for a family member. Care giving demands a lot out of an individual, and as a result there is a high opportunity cost. The opportunity cost can relate to both time and money. Instead of taking care of a family member, a caregiver could spend time working or performing more leisure activities. Care penalties are not strictly related to childcare- they can also refer to taking care of a sick family member, babysitting a younger sibling, or taking an elderly family member to his/her doctor’s appointments.		Studies have been done to get an annual salary estimate for a female caregiver. One survey suggested that the value of a mother's work, if she were paid the average wage for each task she performs in running the household and caring for her children, is $117,867 per year.[28] The reason for the high salary is because mothers typically perform about 10 different job functions throughout the week. Some of these job functions are poorly paid, including cleaning, driving, caring for children, and washing laundry, but others, especially financial and managerial tasks that the survey equated with being the Chief Executive Officer of a company, are highly paid. Neither a nanny nor a housekeeper makes nearly as much money, and almost all of these tasks except direct child care also have to be done by non-parents.		It is important to assess the value of caregivers because they are what truly make society function,[29] and often their work is under-appreciated. They prepare the next generation for school, work, and decision-making. A child’s entire future largely depends on how he/she was nurtured. Not only does the child depend on this care, but the schools and employers also depend on the childcare. The government also benefits because these children will eventually become taxpayers, congressmen, and voters. Eventually, they will be the ones running the country. The value of unpaid childcare is also an important figure in various legal entities. Expert witnesses (most often economists) are occasionally brought into court cases to give estimates on the value of unpaid labor. By giving estimation, the plaintiff or defendant can be fairly compensated for their labor.		Learning Stories [30] are documents that are used by Carers and educators in childcare settings. They use a story- telling format instead of a traditional ‘observation’ report to document the different ways that young children learn, and capture the moment in greater detail and provide parents with a greater insight into the events that occur in their child’s time in childcare.		What they include		Learning stories originate from New Zealand as they use a learning model in their curriculum called "Te Whaariki". It highlights children's learning outcomes as 'disposition' which are “situated learning strategies plus motivation-participation repertoires from which a learner recognize, selects, edits, responds to, resists, searches for and constructs learning opportunities” [32][33]		According to Chris Knight, the first humans were few; then the population "exploded .... Population expansion on such a scale is inconsistent with female tolerance of infanticide, harassment, or the heavy costs to mothers of male philandering and double standards. If unusually large numbers of unusually large-brained offspring were being successfully raised to maturity, the quality of childcare must have been exceptional. We know what the optimal solution would have been. There can be no doubt that mothers would have done best by ... taking advantage of every available childcare resource."[34]		Plato, according to Elaine Hoffman Baruch, around 394 B.C., argued that a system of child care would free women to participate in society.[35] Among the early English authors to devote a book to child care in the modern sense was Elizabeth Dawbarn (The Rights of Infants, or... Nursing of Infants, 1805).[36]		Media related to Child care at Wikimedia Commons		
Eating (also known as consuming) is the ingestion of food, typically to provide a heterotrophic organism with energy and to allow for growth. Animals and other heterotrophs must eat in order to survive — carnivores eat other animals, herbivores eat plants, omnivores consume a mixture of both plant and animal matter, and detritivores eat detritus. Fungi digest organic matter outside of their bodies as opposed to animals that digest their food inside their bodies. For humans, eating is an activity of daily living. Some individuals may limit their amount of nutritional intake. This may be a result of a lifestyle choice, due to hunger or famine, as part of a diet or as religious fasting.						Many homes have a large eating room or outside (in the tropics) kitchen area devoted to preparation of meals and food, and may have a dining room, dining hall, or another designated area for eating. Some trains have a dining car.		Most societies also have restaurants, food courts, and food vendors so that people may eat when away from home, when lacking time to prepare food, or as a social occasion (dining club).[1] At their highest level of sophistication, these places become "theatrical spectacles of global cosmopolitanism and myth."[2] At picnics, potlucks, and food festivals, eating is in fact the primary purpose of a social gathering. At many social events, food and beverages are made available to attendees.		Dishware, silverware, drinkware, and cookware come in a wide array of forms and sizes.		People usually have two or three meals a day regularly. Snacks of smaller amounts may be consumed between meals. Doctors in the UK, recommend three meals a day ( with between 400-600 kcal per meal),[3][4] with four to six hours between.[5] Having three well-balanced meals (thus half of the plate with vegetables,[6] 1/4 protein food as meat, ... and 1/4 carbohydrates as pasta, rice, ...) will then account to some 1800–2000 kcal, which is the average requirement for a regular person.[7]		The issue of healthy eating has long been an important concern to individuals and cultures. Among other practices, fasting, dieting, and vegetarianism are all techniques employed by individuals and encouraged by societies to increase longevity and health. Some religions promote vegetarianism, considering it wrong to consume animals. Leading nutritionists believe that instead of indulging oneself in three large meals each day, it is much healthier and easier on the metabolism to eat five smaller meals each day (e.g. better digestion, easier on the lower intestine to deposit wastes; whereas larger meals are tougher on the digestive tract and may call for the use of laxatives.[8] However, psychiatrists with Yale Medical School have found that people who suffer from Binge Eating Disorder (BED) and consume three meals per day weigh less than those who have meals that are more frequent. Eating can also be a way of making money (see competitive eating). In jurisdictions under sharia law, it may be proscribed for Muslim adults during the daylight hours of Ramadan.[9][10][11]		Newborn babies do not eat adult foods. They survive solely on breast milk or formula. Small amounts of pureed food are sometimes fed to young infants as young as two or three months old, but most infants do not eat adult food until they are between six and eight months old. Young babies eat pureed baby foods because they have few teeth and immature digestive systems. Between 8 and 12 months of age, the digestive system improves, and many babies begin eating finger foods. Their diet is still limited, however, because most babies lack molars or canines at this age, and often have a limited number of incisors. By 18 months, babies often have enough teeth and a sufficiently mature digestive system to eat the same foods as adults. Learning to eat is a messy process for children, and children often do not master neatness or eating etiquette until they are 5 or 6 years old.		Eating positions vary according to the different regions of the world, as culture influences the way people eat their meals. For example, most of the Middle Eastern countries, eating while sitting on the floor is most common, and it is believed to be healthier than eating while sitting to a table.[citation needed][12]		Compulsive overeating, or emotional eating, is "the tendency to eat in response to negative emotions".[13] Empirical studies have indicated that anxiety leads to decreased food consumption in people with normal weight and increased food consumption in the obese.[14]		Many laboratory studies showed that overweight individuals are more emotionally reactive and are more likely to overeat when distressed than people of normal weight. Furthermore, it was consistently found that obese individuals experience negative emotions more frequently and more intensively than do normal weight persons.[15]		The naturalistic study by Lowe and Fisher compared the emotional reactivity and emotional eating of normal and overweight female college students. The study confirmed the tendency of obese individuals to overeat, but these findings applied only to snacks, not to meals. That means that obese individuals did not tend to eat more while having meals; rather, the amount of snacks they ate between meals was greater. One possible explanation that Lowe and Fisher suggest is obese individuals often eat their meals with others and do not eat more than average due to the reduction of distress because of the presence of other people. Another possible explanation would be that obese individuals do not eat more than the others while having meals due to social desirability. Conversely, snacks are usually eaten alone.[15]		There are many physiological mechanisms that control starting and stopping a meal. The control of food intake is a physiologically complex, motivated behavioral system. Hormones such as cholecystokinin, bombesin, neurotensin, anorectin, calcitonin, enterostatin, leptin and corticotropin-releasing hormone have all been shown to suppress food intake.[16][17]		There are numerous signals given off that initiate hunger. There are environmental signals, signals from the gastrointestinal system, and metabolic signals that trigger hunger. The environmental signals come from the body’s senses. The feeling of hunger could be triggered by the smell and thought of food, the sight of a plate, or hearing someone talk about food.[18] The signals from the stomach are initiated by the release of the peptide hormone ghrelin. Ghrelin is a hormone that increases appetite by signaling to the brain that a person is hungry.[19]		Environmental signals and ghrelin are not the only signals that initiate hunger, there are other metabolic signals as well. As time passes between meals, the body starts to take nutrients from long-term reservoirs.[18] When the glucose levels of cells drop (glucoprivation), the body starts to produce the feeling of hunger. The body also stimulates eating by detecting a drop in cellular lipid levels (lipoprivation).[18] Both the brain and the liver monitor the levels of metabolic fuels. The brain checks for glucoprivation on its side of the blood–brain barrier (since glucose is its fuel), while the liver monitors the rest of the body for both lipoprivation and glucoprivation.[20]		There are short-term signals of satiety that arise from the head, the stomach, the intestines, and the liver. The long-term signals of satiety come from adipose tissue.[18] The taste and odor of food can contribute to short-term satiety, allowing the body to learn when to stop eating. The stomach contains receptors to allow us to know when we are full. The intestines also contain receptors that send satiety signals to the brain. The hormone cholecystokinin is secreted by the duodenum, and it controls the rate at which the stomach is emptied.[21] This hormone is thought to be a satiety signal to the brain. Peptide YY 3-36 is a hormone released by the small intestine and it is also used as a satiety signal to the brain.[22] Insulin also serves as a satiety signal to the brain. The brain detects insulin in the blood, which indicates that nutrients are being absorbed by cells and a person is getting full. Long-term satiety comes from the fat stored in adipose tissue. Adipose tissue secretes the hormone leptin, and leptin suppresses appetite. Long-term satiety signals from adipose tissue regulates short-term satiety signals.[18]		The brain stem can control food intake, because it contains neural circuits that detect hunger and satiety signals from other parts of the body.[18] The brain stem’s involvement of food intake has been researched using rats. Rats that have had the motor neurons in the brain stem disconnected from the neural circuits of the cerebral hemispheres (decerebration), are unable to approach and eat food.[18] Instead they have to obtain their food in a liquid form. This research shows that the brain stem does in fact play a role in eating.		There are two peptides in the hypothalamus that produce hunger, melanin concentrating hormone (MCH) and orexin. MCH plays a bigger role in producing hunger. In mice, MCH stimulates feeding and a mutation causing the overproduction of MCH led to overeating and obesity.[23] Orexin plays a greater role in controlling the relationship between eating and sleeping. Other peptides in the hypothalamus that induce eating are neuropeptide Y (NPY) and agouti-related protein (AGRP).[18]		Satiety in the hypothalamus is stimulated by leptin. Leptin targets the receptors on the arcuate nucleus and suppresses the secretion of MCH and orexin. The arcuate nucleus also contains two more peptides that suppress hunger. The first one is cocaine- and amphetamine-regulated transcript (CART), the second is α-MSH (α-melanocyte-stimulating hormone).[18]		Physiologically, eating is generally triggered by hunger, but there are numerous physical and psychological conditions that can affect appetite and disrupt normal eating patterns. These include depression, food allergies, ingestion of certain chemicals, bulimia, anorexia nervosa, pituitary gland malfunction and other endocrine problems, and numerous other illnesses and eating disorders.		A chronic lack of nutritious food can cause various illnesses, and will eventually lead to starvation. When this happens in a locality on a massive scale, it is considered a famine.		If eating and drinking is not possible, as is often the case when recovering from surgery, alternatives are enteral[24] nutrition and parenteral nutrition.[25]		
The plate lunch is a quintessentially Hawaiian meal, roughly analogous to Southern U.S. meat-and-threes. However, the pan-Asian influence on Hawaiian cuisine, and its roots in the Japanese bento, make the plate lunch unique to Hawaii.		Standard plate lunches consist of two scoops of white rice, macaroni salad, and an entrée.[1] A plate lunch with more than one entrée is often called a mixed plate.						Although the exact origin of the Hawai'ian plate lunch is disputed,[1] according to Professor Jon Okamura of the University of Hawai'i, the plate lunch likely grew out of the Japanese bento, because "bentos were take away kinds of eating and certainly the plate lunch continues that tradition".[1] Its appearance in Hawaii in recognizable form goes back to the 1880s when plantation workers were in high demand by the fruit and sugar companies on the islands.[2] Laborers were brought to Hawaii from around the world, including from China, Japan, Portugal, and the Philippines. Kaui Philpotts, former food editor of the Honolulu Advertiser, notes that the laborers "didn’t eat sandwiches or things like that; it was leftover rice and a lot of things like canned meat or teriyaki or cold meat or maybe scrambled eggs or pickles, and almost no salad or vegetable."[2] Later on, macaroni salad was added to the plates, as it seemed to bridge national tastes and also mixed well with gravy-covered slabs of meat.[2] Some locations also include the traditional Korean side dish kimchi.		As the days of the plantations came to an end, plate lunches began to be served on-site by lunch wagons to construction workers and day laborers. Later, local hole-in-the-wall restaurants and other stand-alone plate lunch restaurants began popping up,[2] then plate lunch franchises. Eventually these made their way to the U.S. mainland, beginning with the L&L Drive-Inn chain in California in 1999.[3] At that time, L&L founder Eddie Flores rebranded it "L&L Hawaiian Barbecue", explaining that "When we went to the mainland, the name 'Hawaiian' is a draw, because everyone just fantasized, everyone wants to come to Hawaii."[3]		Overwhelmingly popular plate lunch entrées reflect Asian influence. Of Japanese origin is chicken katsu, fried boneless chicken breaded with Japanese bread crumbs, and beef teriyaki (often shortened to "teri beef"). A common side-dish with plate lunches is fried noodles, often either chow mein, chow fun or saimin noodles.		Entrées of Hawai'ian origin include kalua pork (also called "kalua pig") and lau lau (pork or other meat or fish wrapped in a taro leaf). Some side dishes are lomi salmon (also called "lomi-lomi salmon") and haupia (a coconut dessert).		Korean entrées include kalbi and meat jun. Some side dishes are taegu, a dish made of shredded codfish, and namul, a dish made of seasoned soybean sprouts.		Other Asian ethnic contributions include the Okinawan shoyu pork (Okinawan: rafute), the Chinese-influenced Char siu Pork, and Filipino Chicken Adobo and Longanisa. From Western Europe come dishes with Linguiça, a traditional Portuguese sausage.		A notably American element is the hamburger steak, a ground beef patty smothered with brown gravy served atop rice. Adding a sunny side up egg makes it a Loco Moco.		Traditional plate lunch of ahi poke, lomi lomi salmon, kalua pork, pork lau lau, two scoops rice, and haupia		Plate lunch of lau lau, kalua pork, lomi lomi salmon, poi, haupia, and rice		A shrimp plate lunch		Kalua pork combo plate lunch from a Hawaiian BBQ restaurant in Mountain View, California		
A school meal or school lunch (also known as hot lunch, a school dinner, or school breakfast) is a meal, typically in the middle or beginning of the school day, provided to students at school. Countries all over the world have various kinds of school meal programs. Millions of children from all standards and grades get their meals at their respective schools every day. Scientifically and medically, school meals are regarded as an essential component of children's upbringing and growth.[citation needed] School meals provide high-energy food with high nutritional values either free, or at economical rates.[citation needed]		The benefits of school meals vary from country to country. While in developed countries the school meal is a source of nutritious meals, in developing countries it is an incentive to send children to school and continue their education. In developing countries, school meals provide food security at times of crisis and help children to become healthy and productive adults, thus breaking the cycle of poverty and hunger. In all cases, school meals allow children to focus on their studies, without hunger as a distraction.						Sweden, Finland, Estonia are among the few countries which provide free school meals to all pupils in compulsory education, regardless of their ability to pay.[1][2][3] Many countries do to improve attendance rates such as India, where all the Government School students are provided with free lunch meals( Midday Meal Scheme ),where staple food that varies to different states and region, are provided along with free education.		In high-income countries, free school meals are usually available to children who meet income-based criteria. (The exception is Australia, where free school meals are not available.)[citation needed]		Reduced price meals are also available in some countries to those who need a degree of assistance with costs. Lower-cost meals are available to students in such countries as the France, Italy, Hong Kong, Japan, and the United States.[1]		When they are not provided to all students, free school meals can stigmatize children who do receive them. Studies have shown that many children entitled to free meals do not take them, and those who do may suffer negative consequences. Additionally, not all children who could benefit from free or reduced-price lunch qualify for it. Organisations such as the Child Poverty Action Group have called for school meals to be provided free of charge for all pupils to address these issues. In the United States of America, Share Our Strength has funded free-school-meal-to-all pilot programs in some school districts.[4]		Growing rates of obesity in children have encouraged governments to provide healthier, more balanced school lunches.		For example, in the United Kingdom, significant changes have been made from when school meals were introduced in the nineteenth century. The first National School Meals Policy was published across the United Kingdom in 1941. The Policy set the first nutritional guidelines for school lunches, requiring balanced meals which include the appropriate levels of protein, fat, and calories.[5]		In 1944, the United Kingdom required local authorities to provide school dinners that were consistent with legal nutritional requirements. The government paid the full cost of school meals in 1947.[5] Free school meals were available to children with families on very low incomes.[6] As a result, from the 1950s onward, staple traditional "school dinner" foods became embedded in the national psyche. "School puddings" in particular refers to desserts historically served with school dinners in state and private schools. Examples include tarts such as gypsy tart and Manchester tart, and hot puddings such as spotted dick and treacle sponge pudding.[a]		In the 1980s, Margaret Thatcher's Conservative government ended entitlement to free meals for thousands of children, and obliged local authorities to open up provision of school meals to competitive tender. This was intended to reduce the cost of school meals provided by local authorities. However, it caused a substantial decrease in the standard of school food.[citation needed] A 1999 survey by the Medical Research Council suggested that despite rationing, children in 1950 had healthier diets than their counterparts in the 1990s, with more nutrients and less fat and sugar.[8]		This became a major topic of debate in 2004, when chef Jamie Oliver spearheaded a campaign to improve the quality of school meals. At this time, school dinners at state schools were normally made by outside caterers. The schools sold a lot of deep-fried fast food, like chips, fried turkey nuggets, pizza, and pies. After the "Jamie's School Dinners" programme was shown on Channel 4, sections of the public showed support for increased school meal funding, causing the government to create the School Food Trust. This topic became a factor in the 2005 UK general election.		Martha Payne's blog NeverSeconds, which discussed the quality of school meals at her primary school in Lochgilphead, made national headlines after gaining support from Jamie Oliver.		Since September 2014, all infant pupils in England's schools have been entitled to a free hot meal at lunchtime every day. This was an initiative of Deputy Prime Minister Nick Clegg, who launched the plan at the Liberal Democrats conference in 2013. At the initiative's inception, the government was paying £2.30 for each meal taken by newly eligible pupils.[9]		The British government uses entitlement to free school meals as a measure of deprivation. For the financial year 2014-2015, the government paid schools a premium of £1,300 for primary-aged pupils, or £935 for secondary-aged pupils, for each eligible child. 11% of families entitled to free meals do not claim them, which means that their schools do not receive the extra funding. It is unclear how this will be affected by the introduction of universal free meals for the youngest British schoolchildren.[10]		Since January 2015, the Scottish government has provided free school meals for all children in Primary One to Three.[9]		The National Union of Teachers supports free school meals for all children. Fiona Twycross campaigned to persuade the Labour Party to commit to providing universal free school meals. She argues that according to the Institute for Fiscal Studies and the National Centre for Social Research, free school meals for all students significantly increases attainment in schools.[11]		The Chartwells catering company, which supplies meals to schools in Dorset, was threatened with litigation over its failure to provide hot meals after a fire at one of its kitchens.[12]		In Estonia, free school dinners are served in elementary and secondary schools.[2]		Nutritional guidelines for Estonian school meals are based on the Estonian food pyramid. At the pyramid's base are water and exercise. The next tier up includes starches, fruits, and vegetables. According to Estonia's Food-Based Dietary Guidelines, these foods should comprise the majority of each meal. The middle section of the pyramid includes dairy products and meat. The Guidelines suggest eating these foods in small portions on a daily basis. Just below the top tier are oils, butter and nuts. At the peak of the pyramid are foods like ice cream, soft drinks, honey, and biscuits: high-sugar foods which should be eaten sparingly, as special treats.[13]		Finland provides free, catered hot school meals to all pupils from pre-primary to upper secondary education every school day, as guaranteed by the 1948 Basic Education Act.[14] Section 31 of the Basic Education Act states: "A pupil attending basic education shall be provided with a balanced and appropriately organised and supervised meal on every school day."[15]		Some Finnish cities had offered poor students free school dinners since the beginning of the 20th century. For example, Kuopio did so starting in 1902, and extended school dinners to all students in 1945.[16]		According to Finnish National Board of Education statistics from the year 2014, the average school meal was valued at 2.80 euros per student-school day, totaling 532 euros per student for the school year.[17] This sum included ingredients, labor costs, kitchen equipment, and other fixed expenses, but neither property costs nor taxes.		Children taking part in before- and after-school activities are also served a free healthy snack.[18]		Lunches for higher education students (like those attending universities and polytechnics) are also subsidized in Finland. Kela, the Social Insurance Institution of Finland, compensates student meals that fulfill the nutritional and pricing criteria for government meal subsidies.[19] This program's purpose is to promote positive health and nutritional trends among students. The program accommodates special dietary needs – whether in connection with religion, ethical beliefs, or health issues – without extra costs.		Free school meals in Finland are viewed as an investment for the future; the aim is to maintain and improve children's health, well-being, and learning.[14] The school meal is used as a pedagogical tool for teaching table manners, food culture, nutrition, and healthy eating habits, as well as for increasing the consumption of vegetables, fruits and berries, full corn bread, and skimmed or low-fat milk.[20][21] One of the basic lessons is cooperation between students, head teachers, teachers, parents, and catering staff. In many schools, students participate in the work of the school canteen during their working life practice period. Most schools have a school meal committee where students, teachers, and catering staff develop school catering together. Most schools also welcome parents to come and taste school meals. There are always adults present in the school restaurant. The pedagogical role of the school catering staff is seen as important, as is teachers' knowledge of nutrition. In 2009, Finland began developing school meal and nutrition education for teachers, and pedagogical education for school catering personnel.[22]		National and local regulations form the basis for Finnish school meal practices. Education acts and decrees[15][23][24] and local curricula are central documents governing school meals.[25] Local and school-level curricula define the central principles of arranging school catering. The curricula also describe the objectives for education in health, nutrition, and manners. The health-related and social role of school meals, the objectives of teaching nutrition and manners, and the recreational aspect of lunch breaks are taken into account when arranging school meals and snacks. Students are allowed at least 30 minutes for eating, after which they have a short recess outdoors.		School lunches can also be a channel for empowering local food producers. Introducing locally produced fish to the offerings of institutional kitchens, such as school canteens, is an ethical and ecological alternative to mass-produced meat or imported fish.[26]		School meals generally consist of typical Finnish foods. A basic school meal consists of a warm main course, vegetables, bread, a table spread, and a drink.[27] The school lunch is calculated to equate to about one-third of a child’s daily nutritional needs. School catering is designed to follow the National Nutrition Council's dietary guidelines for schools.[18] The recommendations for school meals are being updated during the year 2016.		Normally, Finnish schools provide lunch at school canteens in the form of a buffet, where pupils serve themselves as much as they want. Schools often use a model plate to guide eating habits towards the following recommendations:[b][18][29]		Children with special dietary needs – whether in connection with religion, ethical beliefs or health issues – are entitled to a special diet without costs. School menus are designed to be suitable for most students, with minor adjustments if needed. If a child has special dietary needs, their school requires specific information about those needs to ensure food safety and the elimination of possible cross contamination. In the case of health-related special diets, schools require the assessment of a doctor, nurse, or dietitian.[30]		As with school lunches, before- and after-school snacks are used as a pedagogical tool in teaching children about proper nutrition, table manners, and food culture. Snacks are designed to offer variety and take into consideration Finnish Nutrition Recommendations as well as children’s individual needs.[18]		Finland has no national accreditation system to evaluate the quality of school lunches. However, by the end of 2015, over 200 schools had been awarded the School Lunch Diploma.[31] The diploma certifies a school's commitment to the national standards and recommendations for nutritionally, educationally, and ecologically sustainable school lunches.[31] It is also an indication of excellent collaboration among interest groups within the school. The School Lunch Diploma is coordinated by the Finnish Kitchen Professionals Association.		School meals in Italy provide regular Italian cuisine, although they may vary among regions and towns. The Italian government is very "down to people" and is doing a large-scale study to measure and involve students in food habits, diets, and food choices.[32]		School dinner has been free in Swedish elementary and secondary schools since 1973.[33] The government or municipality covers all charges. Normally, school lunches are buffet-style. Buffets chiefly include potatoes or rice; meat or fish; and vegetables. Milk and water are usually offered as drinks.[citation needed] There are also vegetarian options, as well as foods that meet religious requirements; these foods are also free of charge.		Usually, each city signs a private contract with a catering company which provides school food. Many of the food products are imported, but still have a good standard. In many schools, teachers or the school principal eat with the pupils, with the goal of creating a stronger connection between students and school authorities.[34] In Swedish schools there are also international food weeks, or vegetarian weeks.		School lunches in Denmark may include items like fruit, roasted duck, potatoes, and red or white cabbage.[35]		Norwegian school lunch was supplied from Sweden during World War II, partly privately financed. Later, all public school lunches were discontinued, so most Norwegian students bring a packed lunch from home. In 2007, schools began providing one free piece of fruit each day for all pupils in grades 8–10. Norwegian schools also sell subsidized milk.		In the 1970s, the French government began to work on improving school lunches. In 1971, the government established nutritional guidelines for French school meals. The 1971 food recommendation guidelines stated that each meal should contain raw vegetables, such as salads and fruits; protein in the form of milk or other dairy products; cooked vegetables twice per week; and carbohydrates on the remaining days.[36]		In France, lunch is considered the most important meal of the day. Students can get lunch at school or go home for it. The lunch break is one to two hours long. French students are taught to take time to savor and enjoy their meals.[37] Students have to pay for cafeteria lunches; the cost of the meal varies by region. A student's family pays for half of the meal, while the school pays for the remainder. For example, a typical meal may cost $6, with the family paying $3 instead of the full price.[36]		The 2001 food recommendation guidelines, signed by the Minister for National Education, state that school lunches must be healthy and balanced. Menus vary daily, and are posted for parents. Specifically, the guidelines state that:[37]		School cafeterias serve five-course meals, even for preschoolers.[38] Schoolchildren eat the same things as adults.[39] A school lunch in France contains an appetizer, salad, main course, cheese plate, and dessert.[38] Bread may accompany each meal. A menu might include potato leek soup, carrot and bean salad, lamb with saffron, an assortment of cheeses, and grapefruit. Each meal is accompanied with water.[40]		French schools do not have vending machines.[40]		A typical school lunch in the People's Republic of China may include items like white rice, fish, a potato and onion mix, and green beans.[35]		In 1925, the Indian government created the Integrated Child Development Services (ICDS) programme. This makes the initiative one of the oldest free food programmes for schoolchildren. Through this initiative, called the Midday Meal Scheme, government high schools and partially aided schools, along with Anganwadis, provide midday meals to students. The meals are free of cost and meet guidelines that have been set by policy.		By 1998, India had deployed the National Programme of Nutritional Support to Primary Education (NP-NSPE) scheme. During the 2013-2014 school year, the scheme covered 104 million children in 1.16 million schools.[41]		The Akshaya Patra Foundation, a public-private partnership in the Midday Meal Program, is a school meal program run by a non-governmental organization. Akshaya Patra started serving 1,500 children in the year 2000. Today it serves lunch to over 1.4 million school children in 10 Indian states every day.[42]		A single afternoon lunch usually contains a cereal which is locally available, made in a way that conforms to prevailing local customs. Each child receives milk, and either soup or vegetables cooked as curry. The menu is occasionally varied to appeal to students.		Children in private schools usually carry their own lunch boxes. Many schools also have canteens, and street food vendors can often be found in front of school campuses.		In the 1960s, Shah Mohammad Reza Pahlavi had intended a non-violent regeneration of Iranian society through economic and social reforms called the White Revolution. The reforms' long-term goal was to transform Iran into a global economic and industrial power. The White Revolution consisted of 19 elements that were introduced over a period of 15 years. In 1975, the Shah started a program for 'Free and Compulsory Education and a daily free meal' for all children from kindergarten to 14 years of age. It provided one-third of a pint of free milk to all children in Iran, as well as pistachios, fresh fruit, and biscuits.		In Japan, the tradition of providing school lunches started in the early 20th century. After World War II, which brought near-famine conditions to the country, the Japanese government re-introduced school lunches in urban areas. School lunch was extended to all elementary schools in Japan in 1952. With the enactment of the School Lunch Law in 1954, school meals were extended to junior high schools as well.		These early lunches initially included items such as bread, bread rolls, and skimmed milk powder (replaced in 1958 by milk bottles and cartons). Later, lunches were expanded to include flour donated by an American charity; a dessert; and a dish (such as daikon) that changed daily.[43] Other dishes included inexpensive protein such as stewed bean dishes, fried white fish, and, until the 1970s, whale meat. Provisions of rice were introduced in 1976, following a surplus of government-distributed Japanese rice, and became increasingly frequent during the 1980s. Hamburg steak, stew and Japanese curry became staples as well.		As of 2004, 99% of elementary school students and 82% of junior high school students in Japan ate kyūshoku (school lunch).[44] The food is grown locally, is almost never frozen, and (barring dietary restrictions) is the same for every student.[43] Children in most districts cannot bring their own meals to school until they reach high school, nor do schools have vending machines; instead, children are taught to eat what they are served.[43]		In both elementary school and middle school, students put on white coats and caps and serve their classmates, who then all eat together in their classrooms instead of in a cafeteria.[43]		To make lunches affordable for students, municipalities pay for the labor costs, while parents, who are billed monthly, pay for the ingredients.[43] These typically cost about 250 to 300 yen (about USD $3) per meal per student. There are reduced-price and free options for poorer families.[43]		The daily bento boxes are designed by nutritionists to provide a balanced yet tasty meal for schoolchildren, working especially to appeal to picky or unhealthy eaters.[43] According to Chico Harlan:[43]		Common dishes range from Asian foods such as naengmyeon, tom yam, and ma po tofu to Western foods such as spaghetti, stew, and clam chowder.		In most Malaysian schools, regardless of whether they are public or private schools, students eat in a canteen where they purchase food and drinks from vendors. School canteens usually offer Malay, Chinese, and Indian foods, with varieties of rice, noodles, and breads. The average Malaysian school canteen offers varieties of Nasi Lemak, Nasi Goreng, Chicken Rice, Popiah, and Laksa.[45]		School canteens sell food and drinks at reduced prices. Underprivileged students can apply for the free-food program – which, depending on the school, is either sponsored by the school's parent-teacher association or by the Ministry of Education.[45] Low-income students may also be eligible for the School Milk Program, which is funded by milk companies and non-governmental organizations.[46]		School meals in the Philippines appear to be relatively simplistic, consisting mainly of rice, meat, and gravy.[47]		School meals in most Singaporean primary and secondary schools, as well as junior colleges, are provided in each school's canteen (or tuckshop). The canteens consist of stalls which sell a variety of foods as well as beverages. To cater to the many races, religions, and cultures in Singapore, canteens often offer a range of cuisines, like Chinese, Indian, Malay, and Western foods.[48]		To encourage healthier eating habits among children, the Health Promotion Board of Singapore launched the Healthy Eating in Schools Programme, which gives an award to schools which serve healthy meals. To receive the award, schools must reduce the sugar content in drinks and desserts, serve fewer deep-fried and fatty foods, and include two servings of greens in their meals.[49]		School lunches in South Korea include traditional foods like rice and kimchi (fermented cabbage).[35] Other dishes may include sesame leaves stuffed with rice and covered with honey sauce; pumpkin potato soup; pancakes made of egg batter and green onions, with optional peppers and octopus; and cucumber-and-carrot salads.[47]		In most schools, the students set and hand out the various dishes, and then clean them up.		Due to the economic boom, obesity has developed into a known health concern amongst adolescents in the United Arab Emirates. The past three decades have seen the largest increases in child obesity. Studies have shown that rates of obesity among the UAE's schoolchildren have surpassed the child obesity rates in both the United States and Europe. Traditional cuisine in the Persian Gulf region, once high-fibre and low-fat, has become Westernized, and now consists of many more high-fat, high-sodium, and high-cholesterol foods. Exercise levels among children have also decreased rapidly, causing the surge of obesity in adolescents.[50]		Canada has no national school meal program,[51] and elementary schools are usually not equipped with kitchen facilities. Parents are generally expected to provide a packed lunch for their children to take to school, or have their children return home to eat during the lunch period. However, some non-profit organizations dedicated to student nutrition programs do exist.[52]		Most Canadian middle schools (grades 6-8) and high schools (grades 9-12) have cafeterias that serve hot meals.		The National School Lunch Program was created in 1946, when President Harry Truman signed the National School Lunch Act into law.[53] This legislation was originally created to aid farms struggling with their surplus provisions, in a way that was also beneficial to society.[54] Truman intended these meals to promote and protect child nutrition, while also supporting the consumption of American farm products.[55]		Today, the National School Lunch Program is a federal nutrition assistance program operating in over 101,000 public schools, non-profit private schools, and residential care institutions. It is regulated and administered at the federal level by the Food and Nutrition Service of the United States Department of Agriculture (USDA). The program provides "nutritionally balanced meals" at low or no cost to more than 31 million children each school day.[53]		Since its inception, the Program has expanded substantially. It now includes the School Breakfast Program, the Snack Program, a Child and Adult Care Food Program, and the Summer Food Service Program. At the State level, the National School Lunch Program is usually administered by state education agencies, which operate the program through agreements with school food authorities.[56]		School meal programs in the United States provide meals free of charge, or at a reduced (government-subsidized) price, to the children of low-income families. Those who do not qualify for free or reduced price are charged a nominal fee.[53]		Generally, private schools cannot participate in the school lunch program. Public or nonprofit private residential child care institutions may or may not participate. School districts and independent schools that choose to take part in the program get minimal cash subsidies and donated commodities from the USDA for each meal they serve. In return, they must serve lunches that meet federal requirements, and they must offer free or reduced-price lunches to eligible children. School food authorities can also be reimbursed for snacks served to children through age 18 in after-school education or enrichment programs.		There is some controversy over the fact that the USDA is simultaneously responsible for promoting health through nutritious school meals and diet guidelines, and for promoting the consumption of major agricultural products such as dairy and pork.[57] Critics say this is an innate conflict of interest, evident in the degree to which setting the National School Lunch Program standards remains a political process, influenced to a degree by food industry lobbyists. These critics argue that school meals' nutritional standards do not include some of the basics needed for a healthy diet, as established by nutrition science.[58]		School lunches must meet the applicable recommendations of the Dietary Guidelines for Americans. These guidelines state that no more than 30 percent of an individual's calories should come from fat, and less than 10 percent from saturated fat. Regulations also state that school lunches must provide one-third of the Recommended Dietary Allowances of protein, Vitamin A, Vitamin C, iron, calcium, and calories. School lunches must meet federal nutrition requirements over the course of one week's worth of lunches. However, local school food authorities may make decisions about which specific foods to serve and how they are prepared.		Vending machines in schools are also a major source of food for students. Under pressure from parents and anti-obesity advocates, many school districts moved to ban sodas, junk foods, and candy from vending machines and cafeterias.[59] Various laws have also been passed to limit foods sold in school vending machines. With increasing concern over traditional vending machines in schools, healthier vending options have gained popularity and are steadily being adopted by schools around the nation.[60][61] Marketing for such "healthy vending machines" states that they allow students to perform better in school while also attaining better health.		In April 2012, the State of Osun in Nigeria pioneered a statewide school meals programme for all public elementary school pupils. It is called the O'Meals programme (an acronym for the Osun Elementary School Feeding and Health Programme). As of July 2014, it was providing lunch to over 252,000 children in 100% of Osun's elementary schools. In addition to staples such as rice, beans, and yams served with stews, soups, and vegetables, the programme provides daily fruits. Its estimated cost is N50 (USD $0.31) per child per day.[62]		According to a report on O'Meals' benefits:[62]		All food items are sourced locally from farmers and others on the supply chain, enhancing employment within the state. Addressing child malnutrition has raised students' academic performance, and has increased school enrollment by 24% compared to figures from before April 2012.[63]		In 2015, the manifesto of the All Progressives Congress (APC) advocated for the adoption of a nationwide free meal plan. Since he became President, Muhammadu Buhari has made the implementation of this policy one of his foremost priorities. A national School Meals programme is the subject of a budgetary proposal before the National Assembly. Also, the government of Kaduna State has implemented a school feeding programme.		The Healthy Kids Association (previously The Healthy Kids School Canteen Association) is a not-for-profit, non-governmental, health promotion organization based in Sydney, Australia.[64] It is a peak organisation for school canteens in New South Wales and the Australian Capital Territory (ACT).[65]		In Australia, many school canteens have returned to offering junk food, or pupils have started buying fast food outside their schools. The association has developed policies intended to counter these trends; in some schools, they have taken over providing school food.[66]		In response to the 2002 Childhood Obesity Summit, former Premier of New South Wales Bob Carr launched the "Fresh Tastes NSW Healthy School Canteen Strategy." Healthy Kids has become a key partner of the Ministry of Health in developing this plan. The strategy is to develop a taste for healthier foods among schoolchildren by promoting and featuring healthier menu options, while limiting the availability of less nutritious foods.[67] The program's menu guide was partially created by Rosemary Stanton.[68]		General:		
Service à la française (French, "service in the French style") is the practice of serving various dishes of a meal at the same time. That contrasts to service à la russe ("service in the Russian style") in which dishes are brought sequentially and served individually.[1] Formal dinners were served à la française from the Middle Ages to the 19th century.						The meal was divided into three courses: soup and fish; roasts; and desserts. Each course included a variety of dishes, all set at the same time at the table with desserts and appetizers. Guests sat around the table and served themselves and their neighbours by choosing foods that suited them. The table was set and served before the arrival of guests; some dishes, the removes, were replaced once eaten, but not the majority, since the table was set with enough dishes to satisfy the number of people seated.[2]		A modified form of service à la française in which several large dishes are brought out for each diner to help themselves from is known as "family-style" in less formal restaurants, as they replicate the typical way in which small family meals are served. The buffet style is essentially a variation of the French service in which all of the food is available, at the correct temperature, in a serving space other than the dining table, and guests commute there to be served or sometimes to serve themselves, and then carry their plate back to the table. Buffets vary from the very informal (a gathering of friends in a home, or the serving of brunch at a hotel) to the rather formal setting of a wedding reception, for example. The buffet format is preferred in occasions where a very large number of guests are to be accommodated efficiently by a small number of service personnel.		On june 23 2017, a group of Headwaiters, University professors, restaurant managers and hospitality management teachers, define in Bordeaux Montaigne University a new version of "Service à la Française" creating La grande charte du Service à la Française[3]		
A food addiction or eating addiction is a behavioral addiction that is characterized[jargon] by the compulsive consumption of palatable (e.g., high fat and high sugar) foods – the types of food which markedly activate the reward system in humans and other animals – despite adverse consequences.[5][6]		Psychological dependence has also been observed with the occurrence of withdrawal symptoms when consumption of these foods stops by replacement with foods low in sugar and fat.[5] Professionals address this by providing behavior therapy.[7]		Sugary and high-fat food have both been shown to increase the expression of ΔFosB, an addiction biomarker, in the D1-type medium spiny neurons of the nucleus accumbens;[5] however, there is very little research on the synaptic plasticity from compulsive food consumption, a phenomenon which is known to be caused by ΔFosB overexpression.[5]						"Food addiction" refers to compulsive overeaters who engage in frequent episodes of uncontrolled eating (binge eating). The term binge eating means eating an unhealthy amount of food while feeling that one's sense of control has been lost.[8] People who engage in binge eating may feel frenzied, and consume a considerable[jargon] amount of calories before stopping. The after effects of bingeing in this way is generally followed by feelings of guilt and depression; for example,[9] some will cancel their plans for the next day because they "feel fat."[10] Binge eating also has implications on physical health, due to excessive intake of fats and sugars, which can cause numerous health problems.		Unlike individuals with bulimia nervosa, compulsive overeaters do not attempt to compensate for their bingeing with purging behaviors, such as fasting, laxative use, or vomiting. When compulsive overeaters overeat through binge eating and experience feelings of guilt after their binges, they can be said to have binge eating disorder (BED).[8]		In addition to binge eating, compulsive overeaters may also engage in grazing behavior, during which they continuously eat throughout the day.[8] These actions result in an excessive overall number of calories consumed, even if the quantities eaten at any one time may be small.		During binges, compulsive overeaters may consume between 5,000 and 15,000 food calories daily (far more than is healthy), resulting in a temporary release from psychological stress through an addictive high not unlike that experienced through drug abuse.[9] Compulsive overeaters tend to show brain changes similar to those of drug addicts, a result of excessive consumption of highly processed foods.[11]		For the compulsive overeater, ingesting trigger foods causes the release of the chemical messengers serotonin and dopamine in the brain.[9] This could be another indicator that neurobiological factors contribute to the addictive process. Conversely, abstaining from addictive food and food eating processes causes withdrawal symptoms for those with eating disorders.[9] The resulting decreased levels of serotonin in the individual may trigger higher levels of depression and anxiety.[12]		Eventually, compulsive overeaters continuously think about food. Food is in the preeminent[jargon] positions of their minds; when deprived of it, the person may engage in actions similar to those of hard drug addicts, including an uncontrollable search for the substance, and in devious behaviour, such as stealing or lying.[13][14][15] The problem of obesity is becoming a worldwide problem. A sugar tax is set to be introduced in Ireland to minimise the consumption of harmful foods and drinks.[16]		A food addiction features compulsive overeating, such as binge eating behavior, as its core and only defining feature. There are several potential signs that a person may be suffering from compulsive overeating. Common behaviors of compulsive overeaters include eating alone, consuming food quickly, and gaining weight rapidly, and eating to the point of feeling sick to the stomach. Other signs include significantly decreased mobility[jargon] and the withdrawal from activities due to weight gain. Emotional indicators can include feelings of guilt, a sense of loss of control, depression and mood swings.[9][17]		Hiding consumption is an emotional indicator of other symptoms that could be a result of having a food addiction. Hiding consumption of food includes eating in secret; late at night while everybody else is asleep, in the car, and hiding certain foods until ready to consume in private. Other signs of hiding consumption are avoiding social interactions to eat the specific foods that are craved. Other emotional indicators are Inner guilt; which includes making up excuses to why the palatable food would be beneficial to consume, and then feeling guilty about it shortly after consuming.[18]		Sense of loss of control is indicated in many ways which includes, going out of the way to obtain specific foods, spending unnecessary amounts of money on foods to satisfy cravings. Difficulty concentrating on things such as a job or career can indicate sense of loss of control by not being to organize thoughts leading to a decrease in efficiency. Other ways to indicate the sense of loss of control, are craving food despite being full. One may set rules to try to eat healthy but the cravings over rule and the rules are failed to be followed. One big indicator of loss of control due to food addiction is even though one knows they have a medical problem caused by the craved foods, they cannot stop consuming the foods, which can be detrimental to their health.[19][18]		Food addiction has some physical signs and symptoms. Decreased energy; not being able to be as active as in the past, not being able to be as active as others around, also a decrease in efficiency due to the lack of energy. Having trouble sleeping; being tired all the time such as fatigue, oversleeping, or the complete opposite and not being able to sleep such as insomnia. Other physical signs and symptoms are restlessness, irritability, digestive disorders, and headaches.[19][18]		In extreme cases food addiction can result in some suicidal thoughts.[19]		A Food addiction, especially long-term, can result in negative consequences to all aspects of a person’s life, creating damaging and chronic symptoms.[18]		The short-term physical effect associated with dopamine and endogenous opiate release in the brain reward center is low level euphoria, a decrease in both anxiety and emotional pain also known as a “food coma.” The long-term physical effects may vary. The health consequences can be severe.		If a food addict has obesity, it can be associated with the following:		The psychological and mental effects can prove intense and plague an individual for years. These include hopelessness, powerlessness, isolation, shame, depression, self-loathing, guilt, suicidal thoughts, suicide attempts, and/or self-injurious behaviors.[18]		Food addiction impacts relationships, especially those within the family. This is because the person with the addiction is vastly more involved with food than with people – it becomes their safest, most important and meaningful relationship. Other connections to friends and family take a back seat. This often leads to a deep sense of isolation from others.[20]		Compulsive overeating is treatable with nutritional assistance and medication. Psychotherapy may also be required, but recent research has proven this to be useful only as a complementary resource, with short-term effectiveness in middle to severe cases.[21][22]		Lisdexamfetamine is an FDA-approved appetite suppressant drug that is indicated[jargon] for the treatment of binge eating disorder.[23] The antidepressant fluoxetine is a medication that is approved by the Food and Drug Administration (FDA) for the treatment of an eating disorder, specifically bulimia nervosa. This medication has been prescribed off-label for the treatment of binge eating disorder (BED). Off-label medications, such as other selective serotonin reuptake inhibitors (SSRIs), have shown some efficacy, as have several atypical[jargon] agents, such as mianserin, trazodone and bupropion.[24][25] Anti-obesity medications[26] have also proven very effective. Studies suggest that anti-obesity drugs, or moderate appetite suppressants, may be key to controlling binge eating.[27]		Many eating disorders are thought to be behavioral patterns that stem from emotional struggles; for the individual to develop lasting improvement and a healthy relationship with food, these affective[jargon] obstacles need to be resolved.[28] Individuals can overcome compulsive overeating through treatment, which should include talk therapy and medical and nutritional counseling. Such counseling has been recently sanctioned by the American Dental Association in their journal article cover-story for the first time in history in 2012: Given "the continued increase in obesity in the United States and the willingness of dentists to assist in prevention and interventional effort, experts in obesity intervention in conjunction with dental educators should develop models of intervention within the scope of dental practice".[29] Moreover, dental appliances such as conventional jaw wiring and orthodontic wiring for controlling compulsive overeating have been shown to be “efficient ways in terms of weight control in properly selected obese patients and usually no serious complications could be encountered through the treatment course.[30]		As well, several twelve-step programs exist to help members recover from compulsive overeating and food addiction,[9] such as Overeaters Anonymous and others.		A review on behavioral addictions listed the estimated the lifetime prevalence rate (i.e., the proportion of individuals in the population that developed the disorder during their lifetime) for food addiction in the United States as 2.8%.[5]		
Dégustation is the careful, appreciative tasting of various foods, focusing on the gustatory system, the senses, high culinary art and good company. Dégustation is more likely to involve sampling small portions of all of a chef's signature dishes in one sitting. Usually consisting of eight or more courses, it may be accompanied by a matching wine degustation which complements each dish.						The French term dégustation is still "commonly" used in English-language contexts, even though a standard Anglicised spelling and pronunciation exist. Modern dégustation probably comes from the French kitchens of the early 20th century and is different from earlier meals with many courses because these meals were served as full-sized meals at each course.		Sampling a selection of cheeses, at home or in a restaurant, may also be called a dégustation.[1] Three to four varieties are normally chosen, generally including a semi-soft cheese, a goat's cheese, and a blue cheese. The stronger varieties are normally tasted last.		A six course dégustation may include two seafood, red meat and dessert items with matching wines while the same menu could have added a vegetarian item, and any other types of dish to expand the menu to (for example) a nine-course dégustation menu.		The popular Spanish style of tapas is similar to the dégustation style, but is not in itself a complete set menu offering the chefs' signature dishes, but instead offers a variety from which the diner can choose.		
Tableware is the dishes or dishware used for setting a table, serving food and dining. It includes cutlery, glassware, serving dishes and other useful items for practical as well as decorative purposes.[1][2] The quality, nature, variety and number of objects varies according to culture, religion, number of diners, cuisine and occasion. For example, Middle Eastern, Indian or Polynesian food culture and cuisine sometimes limits tableware to serving dishes, using bread or leaves as individual plates. Special occasions are usually reflected in higher quality tableware.[3]		"Dinnerware" is another term used to refer to tableware and "crockery" refers to ceramic dishes in everyday use as differentiated them from the fine porcelain and bone china produced by makers such as Sèvres in France, Meissen in Germany, Royal Copenhagen in Denmark, Royal Doulton in England, or Belleek Pottery in Ireland.[4] Sets of dishes are referred to as a table service, dinner service or service set. Table settings or place settings are the dishes, cutlery and glassware used for formal and informal dining. In Ireland such items are normally referred to as delph, the word being an English language phonetic spelling of the word delft, the town from which so much delftware came. Silver service or butler service are methods for a butler or waiter to serve a meal.		Setting the table refers to arranging the tableware, including individual place settings for each diner at the table as well as decorating the table itself in a manner suitable for the occasion. Tableware and table decoration is typically more elaborate for special occasions. Unusual dining locations demand tableware be adapted.						Dishes are usually made of ceramic materials such as earthenware, stoneware, faience, bone china or porcelain. However, they can be made of other materials such as wood, pewter, silver, gold, glass, acrylic and plastic. Before it was possible to purchase mass-produced tableware, it was fashioned from available materials, such as wood. Industrialisation and developments in ceramic manufacture made inexpensive washable tableware available. It is sold either by the piece or as a matched set for a number of diners, normally four, six, eight, or twelve place settings. Large quantities are purchased for use in restaurants. Individual pieces, such as those needed as replacement pieces for broken dishes, can be procured from "open stock" inventory at shops, or from antique dealers if the pattern is no longer in production.		Possession of tableware has to a large extent been determined by individual wealth; the greater the means, the higher was the quality of tableware that was owned and the more numerous its pieces. In the London of the 13th century, the more affluent citizens owned fine furniture and silver, "while those of straiter means possessed only the simplest pottery and kitchen utensils." By the later 16th century, "even the poorer citizens dined off pewter rather than wood" and had plate, jars and pots made from "green glazed earthenware".[5] The nobility often used their arms on heraldic china.		Tableware is generally the functional part of the settings on dining tables but great attention has been paid to the purely decorative aspects, especially when dining is regarded as part of entertainment such as in banquets given by important people or special events, such as State occasions.[6] Table decoration may be ephemeral and consist of items made from confectionery or wax - substances commonly employed in Roman banqueting tables of the 17th century. During the reign of George III of the United Kingdom, ephemeral table decoration was done by men known as "table-deckers" who used sand and similar substances to create marmotinto works (sand painting) for single-use decoration.[6] In modern times, ephemeral table decorations continue to be made from sugar or carved from ice.		In wealthy countries such as 17th century France, table decorations for the aristocracy were sometimes made of silver. One of the most famous table decorations is the Cellini Salt Cellar. Ephemeral and silver table decorations were replaced with porcelain items after its invention in Europe in the 18th century.		A table setting in Western countries is mainly in one of two styles: service à la russe (French for "in the Russian style"), where each course of the meal is brought out in specific order; and service à la française (French for "in the French style"), where all the courses for the meal are arranged on the table and presented at the same time that guests are seated. Service à la russe has become the custom in most restaurants, whereas service à la française is the norm in family settings.		Place settings for service à la russe dining are arranged according to the number of courses in the meal. The tableware is arranged in a particular order. With the first course, each guest at the table begins by using the tableware placed on the outside of place setting. As each course is finished the guest leaves the used cutlery on the used plate or bowl, which are removed from the table by the server. In some case, the original set is kept for the next course. To begin the next course, the diner uses the next item on the outside of the place setting, and so on. Forks are placed on the left of a dinner plate, knives to the right of the plate, and spoons to the outer right side of the place setting.		Items of tableware include a variety of plates, bowls; or cups for individual diners and a range of serving dishes to transport the food from the kitchen or to separate smaller dishes. Plates include charger plates as well as specific dinner plates, lunch plates, dessert plates, salad plates or side plates. Bowls include those used for soup, cereal, pasta, fruit or dessert. A range of saucers accompany plates and bowls, those designed to go with teacups, coffee cups, demitasses and cream soup bowls. There are also individual covered casserole dishes.		Dishes come in standard sizes, which are set according to the manufacturer. They are similar throughout the industry. Plates are standardised in descending order of diameter size according to function. One standard series is charger (12 inches); dinner plate (10.5 inches); dessert plate (8.5 inches) salad plate (7.5 inches); side plate, tea plate (6.75 inches).		Glasses and mugs of various types are an important part of tableware, as beverages are important parts of a meal. Vessels to hold alcoholic beverages such as wine, whether red, white, sparkling tend to be quite specialised in form, with for example Port wine glasses, beer glasses, brandy balloons, aperitif and liqueur glasses all having a different shapes. Water glasses, juice glasses and hot chocolate mugs are also differentiated. Their appearance as part of the tableware depends on the meal and the style of table arrangement.		Tea and coffee tend to involve strong social rituals and so teacups and, coffee cups (including demitasse cups) have a shape that depends on the culture and the social situation in which the drink is taken.		Cutlery is an important part of tableware. A basic formal place setting will usually have a dinner plate at the centre, resting on a charger. The rest of the place setting depends upon the first course, which may be soup, salad or fish.[7]		In either arrangement, the napkin may either rest folded underneath the forks, or it may be folded and placed on the dinner plate.		When more courses are being served, place settings may become more elaborate and cutlery more specialised. Examples include fruit spoon or fruit knife, cheese knife, and pastry fork. Other types of cutlery, such as boning forks, were used when formal meals included dishes that have since become less common. Carving knives and forks are used to carve roasts at the table.		A wide range of serving dishes are used to transport food from kitchen to table or to serve it at table, in order to make food service easier and cleaner or more efficient and pleasant. Serving dishes include: butter dishes; casseroles; fruit bowls; ramekins or lidded serving bowls; compotes; pitchers or jugs; platters, salvers, and trays; salt and pepper shakers or salt cellars; sauce or gravy boats; tureens and tajines; vegetable or salad bowls.		A range of items specific to the serving of tea or coffee also have long cultural traditions. They include teapots and coffee pots as well as samovars, sugar bowls; milk or cream jugs.		Place markers are used to designate assigned seats to guests. They are typically used at large formal functions such as weddings, banquets for dignitaries, politicians or diplomats as well as on special occasions such as large children's parties. Some are collectible[8]		Chinese table settings are traditional in style. Table setting practices in Japan and other parts of East Asia have been influenced by Chinese table setting customs.[9] The emphasis in Chinese table settings is on displaying each individual food in a pleasing way, usually in separate bowls or dishes. Formal table settings are based upon the arrangements used in a family setting, although they can become extremely elaborate with many dishes. Serving bowls and dishes are brought to the table, where guests can choose their own portions. Formal Chinese restaurants often use a large turning wheel in the centre of the table to rotate food for easier service.		In a family setting, a meal typically includes a fan dish, which constitutes the meal's base (much like bread forms the base of various sandwiches), and several accompanying mains, called cai dish (choi or seoung in Cantonese). More specifically, fan usually refers to cooked rice, but can also be other staple grain-based foods. If the meal is a light meal, it will typically include the base and one main dish. The base is often served directly to the guest in a bowl, whereas main dishes are chosen by the guest from shared serving dishes on the table.[10]		An "elaborate" formal meal would include the following place setting:[9]		Japanese ceramic tableware is an industry that is many centuries old. Unlike in Western cultures, where tableware is often produced and bought in matching sets, Japanese tableware is set on the table so that each dish complements the type of food served in it. Since Japanese meals normally include several small amounts of each food per person, this means that each person has a place setting with several different small dishes and bowls for holding individual food and condiments. The emphasis in a Japanese table setting is on enhancing the appearance of the food, which is partially achieved by showing contrasts between the items. Each bowl and dish may have a different shape, colour or pattern.[11]		A basic complete place setting for one person in Japan would include the following:[12]		Not all of these plates and bowls would be necessary for one meal. A rice bowl, a soup bowl, two or three small dishes with accompanying foods, and two or three condiment dishes for person would be typical. Various serving bowls and platters would also be set on a table for a typical meal, along with a soy sauce cruet, a small pitcher for tempura or other sauce, and a tea setting of tea pot, tea cups and tea cup saucers.		Tableware for special circumstances has to be adapted. Dining in the outdoors, for example, whether for recreational purposes, as on a picnic or as part of a journey, project or mission requires specialised tableware. It must be portable, more robust and if possible, lighter in weight than tableware used indoors. It is usually carefully packed for transportation to the place where it will be used.		
Snacking does not have a concrete definition. A study taken by Katherine Chaplin and Andrew Smith from the journal Appetite says, “Participants defined snacking as food or drink eaten between main meals”.[1]		A snack is a portion of food, smaller than a regular meal, generally eaten between meals.[2]						As told in the textbook Nutrition: Concepts and Controversies by Frances Sienkiewicz Sizer and Ellie Whitney, sedentary men have a recommended daily calorie intake of about 2400 kcal. For sedentary women the intake is about 2000 kcal.[3] The average calorie intake during a meal is about 500 kilocalories leaving a range of 300-800 kilocalories for snacks between meals. Overdoing this daily allowance can cause weight gain no matter whether the snack is healthy or unhealthy.		Snacking on foods that are low in energy density, high in nutrient density, and follow the five characteristics of healthy snacking increase satiation and satiety. Sustaining a high level of satiation and satiety helps keep one within the caloric discretionary allowance, and helps one maintain a healthy body weight.		There are five characteristics of healthy snacking: adequacy, balance, calorie control, moderation, and variety. Together, they work to build a nutritious diet.		A healthy snack leaves a feeling of both satiation and satiety.		Discretion should be used to determine whether one snack is a better choice than another based on nutrient density. The Dietary Reference Intake (DRI) establishes the amount of nutrients required daily to avoid deficiencies and allow the body to function properly. Knowing that one snack has more nutrients than another per calorie can help provide required nutrients without exceeding the discretionary calorie allowance. When analyzing the ratio of nutrients to calories in foods, the caloric level must be lower than the nutrient level in order for it to be nutrient dense. Otherwise, it could potentially cause a deficiency in an essential nutrient.		In contrast to nutrient density, energy density is the amount of calories per gram of food.[3] For instance, snacking on two scoops (1 c.) of chocolate ice cream contains 287 calories per 132 grams making the energy density 2.17. As an alternative, one could have a snack containing celery (2 stalks), peanut butter (1 Tbsp), milk (1 c.), and an apple, which would contain similar calorie content (281 calories), but weigh 478 grams making the energy density .59. Other alternatives include salads, fruits, nuts, frozen yogurt, and cereal (1 c.) without milk. Especially when one's under pressure or frustrated, a low energy density is preferable because the food has a low ratio of calories to grams, allowing one to consume more food per calorie. Choosing a healthy snack with lower energy density will increase the amount of food one can ingest, and thus increase satiation and satiety levels, while increasing nutrient intake compared to chocolate ice cream.[3]		There are several forms of unhealthy snacking:		There is a change in the attitude of individuals when they are told that a snack is either healthy or unhealthy. Janet Polivy and C. Peter Herman noted that individuals generally believe that whether a snack is healthy or not is based on its calorie and fat content. A “healthy” food is thought to contain few calories, and an “unhealthy” food is thought to contain many calories. Adding to that, restaurants that claim they serve “healthy” foods sometimes lead their customers to believe their food has low calorie content.[5] Noting the perception of individuals of the healthiness of foods, snacks are perceived similarly to other foods and can be regarded as healthy or unhealthy based on their caloric content. Polivy and Herman found, in a study they performed on the perceived healthiness of a snack, that if the snack were regarded as healthy the participants ate 35% more of it than of snacks regarded as unhealthy.[5]		It was found in a study by Rhonda S. Sebastian, Linda E. Cleveland, and Joseph D. Goldman that snacking occasions for all age groups has increased over the last 25 years.[6] 4,357 adolescents ages 12–19 were surveyed in order to find how the consumption of nutrients and the meeting of recommendations by the U.S. Department of Agriculture’s MyPyramid Food Guidance System are impacted by the adolescents’ snacking level.		Results: As snacking frequency increased in the adolescents, the amount of carbohydrates consumed increased along with sugar consumption. The consumption of fats and energy-adjusted proteins decreased.[6] Increased snacking frequency positively affected the intake of vitamin A, vitamin E, vitamin C, and magnesium in boys and vitamin C in girls. Fruit intake increased as snacking frequency increased for both boys and girls. The three cup daily milk recommendation was met for boys snacking at the highest level, but girls did not meet the milk recommendation.[6] Milk is one of the highest contributors in adolescents’ calcium consumption. Although, during the past 25 years, people have moved toward dietary habits of snacking rather than daily meals, meals generally contribute more nutrient-dense foods to a diet than snacks. In order to make up for this loss of nutrients, snack choices need to consist of nutrient-dense foods. Snacking frequency improved the chance of meeting fruit recommendations for boys and girls, milk and oil recommendations for boys, and affected the intake of all macronutrients and some micronutrients.		Claire A. Zizza, Francis A. Tayie, and Mark Lino studied the effects of snacking on older Americans. As humans age, it is known that their energy (kcal) intake decreases. The study says, “Comparisons between 25- and 70-year-olds showed declines of 1,000 to 1,200 kcal/day for men and 600 to 800 kcal/day for women”.[7] Reasons for this decline include physiological changes, a switch in the sensation of thirst and hunger, chronic diseases, a decline in physical functioning, limited resources and social factors, namely widowhood. Healthy older persons’ low intakes of protein, carbohydrate, fat, and total energy were found to be strong predictors of mortality. These low intakes can also cause unwanted weight loss which is related to potential life-threatening physical limitations.[7] This loss of weight can be prevented by instituting a proper diet.		Results: In a sample of 2,002 older people, ages 65+, 84% were ambitious snackers. Nonsnackers ingested an average of 1,466 kilocalories daily while snackers ingested an average of 1,718 kcal.[7] The US Department of Agriculture’s MyPyramid states that the recommended consumption of energy for older adults is 1,600 kcal.[7] The study shows that, “In this age group, snacking contributed approximately a quarter of their daily energy and carbohydrate intakes and a fifth of their daily fat intake”.[7] Adding healthful snacking to the dietary behavior of older adults in Zizza, Tayie, and Lino’s study proved to increase their total energy intake preventing inadequate diets.[7]		A study done by SL Colles, JB Dixon, and PE O’Brien on snacking at night relates obesity, binge eating disorder (BED), and psychological stress to night eating syndrome (NES). This syndrome constitutes a pattern of eating where most food is consumed late in the day and at night, sometimes including waking up during the night to snack. It causes morning anorexia and evening hyperphagia and insomnia. This condition is observed most frequently in overweight and obese people. Being a recent syndrome to emerge, the study says, “It is currently unclear whether NES, as a discrete condition, is associated with emotional distress, impairment or disability, and thereby represents an eating disorder of clinical significance”.[8] It may just be a behavioral condition linked to obesity and weight gain. The study was conducted by distributing surveys to people ages 18–70 that had not undergone previous bariatric surgery.		Results: NES and BMI are positively related, meaning that the generalized belief that NES is associated with overweight and obese individuals has a stronger basis for being true. To relate NES with BED the study showed, “[Binge-eaters] were almost seven times more likely to manifest NES than non-[binge-eaters]”.[8] This showed that it is common for individuals that show traits of NES also have BED. The results, however, for the relation between NES and psychological distress were found to contrast those for BED. Individuals who exhibited traits of NES showed low psychological distress, whereas those who were binge-eaters showed high psychological distress. This showed that those who exhibited signs of BED were more likely to be depressed and concerned about their weight. This study was the first to observe differences between those who did and did not wake up for nocturnal snacks. The study shows that, “Frequent nocturnal snackers reported higher symptoms of depression and hunger . . . compared to the NES who did not wake to eat”.[8] This shows clinical significance and that nocturnal snackers have a more severe impairment than individuals with NES.		Harriëtte M. Snoek, Tatjana van Strien, Jan M.A.M. Janssens, and Rutger C.M.E. Engels recorded a study of the effect of television viewing on adolescents’ snacking. They explain three theories on the relationship between eating behavior and being overweight and how they relate to watching television. The first theory centers on external eating declaring that some people are more sensitive to food cues than others. Food cues found on television include food advertisements. Certain people can be influenced by these cues regardless of their state of hunger and satiety.[9] The second theory is the restraint theory, which states, “Dieting can lead to overeating”.[9] Weight-related advertisements and images of a stereotypical attractive female on television can generate negative feelings toward the body and low self-esteem in individuals who are dieting causing them to overeat.[9] The third theory is known as the psychosomatic theory, which deals with those who eat in response to their emotions. In these individuals, negative emotions cause excessive eating rather than the normal response of appetite loss.[9] The study used 10,087 Dutch adolescents 11–16 years of age. Snacking was measured as, “The number of sweet and/or savory snacks respondents usually ate per day”.[9] The participants were required to rate their frequency of television viewing and measure their eating behavior through a questionnaire.		Results: Participants who received a high score under the external eating category of the questionnaire ate more snacks than those who did not; those who received a high score under the restrained eating category ate fewer snacks than those who did not; and those who received a high score under the emotional eating category ate more snacks than those who did not.[9] The study showed that external and emotional eaters were more likely to snack when influenced by television and restrained eaters were less likely to snack due to the influence of television. The study shares that, “The interaction between [television viewing], emotional eating and snacking was only significant for boys,” possibly due to the, “inadequacy in dealing with negative emotions,” which might be a personality trait that is different from men to women.		
Human sexual activity, human sexual practice or human sexual behaviour is the manner in which humans experience and express their sexuality. People engage in a variety of sexual acts, ranging from activities done alone (e.g., masturbation) to acts with another person (e.g., sexual intercourse, non-penetrative sex, oral sex, etc.) in varying patterns of frequency, for a wide variety of reasons. Sexual activity usually results in sexual arousal and physiological changes in the aroused person, some of which are pronounced while others are more subtle. Sexual activity may also include conduct and activities which are intended to arouse the sexual interest of another or enhance the sex life of another, such as strategies to find or attract partners (courtship and display behaviour), or personal interactions between individuals (for instance, foreplay or BDSM). Sexual activity may follow sexual arousal.		Human sexual activity has sociological, cognitive, emotional, behavioural and biological aspects; these include personal bonding, sharing emotions and the physiology of the reproductive system, sex drive, sexual intercourse and sexual behaviour in all its forms.		In some cultures, sexual activity is considered acceptable only within marriage, while premarital and extramarital sex are taboo. Some sexual activities are illegal either universally or in some countries or subnational jurisdictions, while some are considered contrary to the norms of certain societies or cultures. Two examples that are criminal offences in most jurisdictions are sexual assault and sexual activity with a person below the local age of consent.						Sexual activity can be classified in a number of ways. It can be divided into acts which involve one person, also called autoeroticism, such as masturbation, or two or more people such as vaginal sex, anal sex, oral sex or mutual masturbation. If there are more than two participants in the sex act, it may be referred to as group sex. Autoerotic sexual activity can involve use of dildos, vibrators, anal beads, and other sex toys, though these devices can also be used with a partner.		Sexual activity can be classified into the gender and sexual orientation of the participants, as well as by the relationship of the participants. For example, the relationships can be ones of marriage, intimate partners, casual sex partners or anonymous. Sexual activity can be regarded as conventional or as alternative, involving, for example, fetishism, paraphilia, or BDSM activities.[1][2] Fetishism can take many forms ranging from the desire for certain body parts, for example large breasts, armpits or foot worship. The object of desire can often be shoes, boots, lingerie, clothing, leather or rubber items. Some non-conventional autoerotic practices can be dangerous. These include erotic asphyxiation and self-bondage. The potential for injury or even death that exists while engaging in the partnered versions of these fetishes (choking and bondage, respectively) becomes drastically increased in the autoerotic case due to the isolation and lack of assistance in the event of a problem.		Sexual activity can be consensual, which means that both or all participants agree to take part and are of the age that they can consent, or it may take place under force or duress, which is often called sexual assault or rape. In different cultures and countries, various sexual activities may be lawful or illegal in regards to the age, gender, marital status or other factors of the participants, or otherwise contrary to social norms or generally accepted sexual morals.		The physiological responses during sexual stimulation are fairly similar for both men and women and there are four phases.[3]		Sexual dysfunction is the inability to react emotionally or physically to sexual stimulation in a way projected of the average healthy person; it can affect different stages in the sexual response cycles, which are desire, excitement and orgasm.[7] In the media, sexual dysfunction is often associated with men, but in actuality, it is more commonly observed in females (43 percent) than males (31 percent).[8]		Sexual activity can lower blood pressure and overall stress levels, regardless of age.[citation needed] It releases tension, elevates mood, and may create a profound sense of relaxation, especially in the postcoital period. From a biochemical perspective, sex causes the release of endorphins and increases levels of white blood cells that actually boost the immune system. A study published in the journal Biological Psychology described how men who had had sex the previous night responded better to stressful situations, it suggested that if a person is regularly sexual, they’re regularly relaxed, and when the person is relaxed, they cope better with stressful situations.[citation needed] A 2007 study published in the Archives of Sexual Behavior 36, (no. 3 (June 2007): 357–68) reported that sexual behavior with a partner on one day significantly predicted lower negative mood and stress, and higher positive mood, on the following day.				People engage in sexual activity for any of a multitude of possible reasons. Although the primary evolutionary purpose of sexual activity is reproduction, research on college students suggested that people have sex for four general reasons: physical attraction, as a means to an end, to increase emotional connection, and to alleviate insecurity.[9]		Most people engage in sexual activity because of pleasure they derive from the arousal of their sexuality, especially if they can achieve orgasm. Sexual arousal can also be experienced from foreplay and flirting, and from fetish or BDSM activities,[1][10] or other erotic activities. Most commonly, people engage in sexual activity because of the sexual desire generated by a person to whom they feel sexual attraction; but they may engage in sexual activity for the physical satisfaction they achieve in the absence of attraction for another, as in the case of casual or social sex.[11] At times, a person may engage in a sexual activity solely for the sexual pleasure of their partner, such as because of an obligation they may have to the partner or because of love, sympathy or pity they may feel for the partner.		A person may engage in sexual activity for purely monetary considerations, or to obtain some advantage from either the partner or the activity. A man and woman may engage in sexual intercourse with the objective of conception. Some people engage in hate sex, which occurs between two people who strongly dislike or annoy each other. It is related to the idea that opposition between two people can heighten sexual tension, attraction and interest.[12]		It has been shown that sexual activity plays a large part in the interaction of social species. Joan Roughgarden, in her book Diversity, Gender, and Sexuality in Nature and People, postulates that this applies equally to humans as it does to other social species. She explores the purpose of sexual activity and demonstrates that there are many functions facilitated by such activity including pair bonding, group bonding, dispute resolution and reproduction.[13]		Research has found that people also engage in sexual activity for reasons associated with self-determination theory. The self-determination theory can be applied to a sexual relationship when the participants have positive feelings associated with the relationship. These participants do not feel guilty or coerced into the partnership.[14] Researchers have proposed the model of self-determined sexual motivation. The purpose of this model is to connect self-determination and sexual motivation.[15] This model has helped to explain how people are sexually motivated when involved in self-determined dating relationships. This model also links the positive outcomes, (satisfying the need for autonomy, competence, and relatedness) gained from sexual motivations.[15]		According to the completed research associated with this model, it was found that people of both sexes who engaged in sexual activity for self-determined motivation had more positive psychological well-being.[15] While engaging in sexual activity for self-determined reasons, the participants also had a higher need for fulfillment. When this need was satisfied, they felt better about themselves. This was correlated with greater closeness to their partner and higher overall satisfaction in their relationship.[15] Though both sexes engaged in sexual activity for self-determined reasons, there were some differences found between males and females. It was concluded that females had more motivation than males to engage in sexual activity for self-determined reasons.[15] Females also had higher satisfaction and relationship quality than males did from the sexual activity.[15] Overall, research concluded that psychological well-being, sexual motivation, and sexual satisfaction were all positively correlated when dating couples partook in sexual activity for self-determined reasons.[15]		The frequency of sexual activity might range from zero (sexual abstinence) to 15 or 20 times a week.[16] In the United States, the average frequency of sexual intercourse for married couples is 2 to 3 times a week.[17][obsolete source] It is generally recognized that postmenopausal women experience declines in frequency of sexual intercourse[18] and that average frequency of intercourse declines with age. According to the Kinsey Institute, the average frequency of sexual intercourse in the US for individuals who have partners is 112 times per year (age 18–29), 86 times per year (age 30–39), and 69 times per year (age 40–49).[19]		The age at which adolescents tend to become sexually active varies considerably between different cultures and from time to time. (See Prevalence of virginity.) The first sexual act of a child or adolescent is sometimes referred to as the sexualization of the child, and may be considered as a milestone or a change of status, as the loss of virginity or innocence. Youth are legally free to have intercourse after they reach the age of consent.		A 1999 survey of students indicated that approximately 40% of ninth graders across the United States report having had sexual intercourse. This figure rises with each grade. Males are more sexually active than females at each of the grade levels surveyed. Sexual activity of young adolescents differs in ethnicity as well. A higher percent of African American and Hispanic adolescents are sexually active than White adolescents.[20]		Research on sexual frequency has also been conducted solely on female adolescents who engage in sexual activity. Female adolescents tended to engage in more sexual activity due to positive mood. In female teenagers, engaging in sexual activity was directly positively correlated with being older, greater sexual activity in the previous week or prior day, and more positive mood the previous day or the same day as the sexual activity occurred.[21] Decreased sexual activity was associated with prior or current day negative mood or menstruating.[21]		Although opinions differ, others[who?] suggest that sexual activity is an essential part of humans, and that teenagers need to experience sex. Sexual experiences help teenagers understand pleasure and satisfaction.[22] In relation to hedonic and eudaimonic well-being, teenagers can positively benefit from sexual activity according to one particular research study. In a rural upstate New York community, a cross-sectional study of teenagers was completed in 2008 and 2009. Teenagers who had their first sexual experience at age 16 revealed a higher well-being than those who were sexually inexperienced or who were first sexually active at a later age of 17.[22] Furthermore, teenagers who had their first sexual experience at age 15 or younger, or who had many sexual partners were not negatively affected and did not have associated lower well-being.[22]		Sexual activity is a normal physiological function,[23] but like other physical activity, it comes with risks. There are four main types of risks that may arise from sexual activity: unwanted pregnancy, contracting a sexually transmitted infection (STI/STD), physical injury, and psychological injury.		Any sexual activity that involves the introduction of semen into a woman's vagina, such as during sexual intercourse, or even contact of semen with her vulva, may result in a pregnancy.[24] To reduce the risk of unintended pregnancies, some people who engage in penile-vaginal sex may use contraception, such as birth control pills, a condom, diaphragms, spermicides, hormonal contraception or sterilization.[25] The effectiveness of the various contraceptive methods in avoiding pregnancy varies considerably.		Sexual activity that involves skin-to-skin contact, exposure to an infected person's bodily fluids or mucosal membranes[26] carries the risk of contracting a sexually transmitted infection. People may not be able to detect that their sexual partner has one or more STIs, for example if they are asymptomatic (show no symptoms).[27][28] The risk of STIs can be reduced by safe sex practices, such as using condoms. Both partners may opt be tested for STIs before engaging in sex.[29] There may also be an increased risk of contracting a STI when having sex with multiple partners.		Some STIs can also be contracted by using IV drug needles after their use by an infected person, as well as through childbirth or breastfeeding.		Typically, older men and women maintaining interest in sexual interest and activity could be therapeutic; it is a way of expressing their love and care for one another. Factors such as biological and psychological factors, diseases, mental conditions, boredom with the relationship, and widowhood have been found to contribute with the common decrease in sexual interest and activity in old age. National sex surveys given in Finland in the 1990s revealed aging men had a higher incidence of sexual intercourse compared to aging women and that women were more likely to report a lack of sexual desire compared to men. Regression analysis, factors considered important to female sexual activity included: sexual desire, valuing sexuality, and a healthy partner, while high sexual self-esteem, good health, and active sexual history were important to male sexual activity. Both aging genders agreed they needed good health, good sexual functioning, positive sexual self-esteem, and a sexually skilful partner to maintain sexual desire.[30]		Heterosexuality is the romantic or sexual attraction to the opposite sex. Heterosexual sexual practices are subject to laws in many places. In some countries, mostly those where religion has a strong influence on social policy, marriage laws serve the purpose of encouraging people to have sex only within marriage. Sodomy laws were seen as discouraging same-sex sexual practices, but may affect opposite-sex sexual practices. Laws also ban adults from committing sexual abuse, committing sexual acts with anyone under an age of consent, performing sexual activities in public, and engaging in sexual activities for money (prostitution). Though these laws cover both same-sex and opposite-sex sexual activities, they may differ in regard to punishment, and may be more frequently (or exclusively) enforced on those who engage in same-sex sexual activities.[31]		Different-sex sexual practices may be monogamous, serially monogamous, or polyamorous, and, depending on the definition of sexual practice, abstinent or autoerotic (including masturbation). Additionally, different religious and political movements have tried to influence or control changes in sexual practices including courting and marriage, though in most countries changes occur at a slow rate.		Homosexuality is the romantic or sexual attraction to the same sex. People with a homosexual orientation can express their sexuality in a variety of ways, and may or may not express it in their behaviors.[32] Research indicates that many gay men and lesbians want, and succeed in having, committed and durable relationships. For example, survey data indicate that between 40% and 60% of gay men and between 45% and 80% of lesbians are currently involved in a romantic relationship.[33]		It is possible for a person whose sexual identity is mainly heterosexual to engage in sexual acts with people of the same sex. For example, mutual masturbation in the context of what may be considered normal heterosexual teen development. Gay and lesbian people who pretend to be heterosexual are often referred to as being closeted (hiding their sexuality in "the closet"). "Closet case" is a derogatory term used to refer to people who hide their sexuality. Making that orientation public can be called "coming out of the closet" in the case of voluntary disclosure or "outing" in the case of disclosure by others against the subject's wishes (or without their knowledge). Among some communities (called "men on the DL" or "down-low"), same-sex sexual behavior is sometimes viewed as solely for physical pleasure. Men who have sex with men, as well as women who have sex with women, or men on the "down-low" may engage in sex acts with members of the same sex while continuing sexual and romantic relationships with the opposite sex.		People who engage exclusively in same-sex sexual practices may not identify themselves as gay or lesbian. In sex-segregated environments, individuals may seek relationships with others of their own gender (known as situational homosexuality). In other cases, some people may experiment or explore their sexuality with same (or different) sex sexual activity before defining their sexual identity. Despite stereotypes and common misconceptions, there are no forms of sexual acts exclusive to same-sex sexual behavior that cannot also be found in opposite-sex sexual behavior, except those involving the meeting of the genitalia between same-sex partners – tribadism (generally vulva-to-vulva rubbing, commonly known by its "scissoring" position) and frot (generally penis-to-penis rubbing).		People who have a romantic or sexual attraction to both sexes are referred to as bisexual.[34][35] People who have a distinct but not exclusive preference for one sex/gender over the other may also identify themselves as bisexual.[36] Like gay and lesbian individuals, bisexual people who pretend to be heterosexual are often referred to as being closeted.		Pansexuality (also referred to as omnisexuality)[37] may or may not be subsumed under bisexuality, with some sources stating that bisexuality encompasses sexual or romantic attraction to all gender identities.[38][39] Pansexuality is characterized by the potential for aesthetic attraction, romantic love, or sexual desire towards people without regard for their gender identity or biological sex.[40] Some pansexuals suggest that they are gender-blind; that gender and sex are insignificant or irrelevant in determining whether they will be sexually attracted to others.[41] As defined in the Oxford English Dictionary, pansexuality "encompasses all kinds of sexuality; not limited or inhibited in sexual choice with regards to gender or practice".[42]		Alex Comfort and others propose three potential social aspects of sexual intercourse in humans, which are not mutually exclusive: reproductive, relational, and recreational.[43] The development of the contraceptive pill and other highly effective forms of contraception in the mid- and late 20th century has increased people's ability to segregate these three functions, which still overlap a great deal and in complex patterns. For example: A fertile couple may have intercourse while using contraception to experience sexual pleasure (recreational) and also as a means of emotional intimacy (relational), thus deepening their bonding, making their relationship more stable and more capable of sustaining children in the future (deferred reproductive). This same couple may emphasize different aspects of intercourse on different occasions, being playful during one episode of intercourse (recreational), experiencing deep emotional connection on another occasion (relational), and later, after discontinuing contraception, seeking to achieve pregnancy (reproductive, or more likely reproductive and relational).[citation needed]		Most world religions have sought to address the moral issues that arise from people's sexuality in society and in human interactions. Each major religion has developed moral codes covering issues of sexuality, morality, ethics etc. Though these moral codes do not address issues of sexuality directly, they seek to regulate the situations which can give rise to sexual interest and to influence people's sexual activities and practices. However, the effect of religious teaching has at times been limited. For example, though most religions disapprove of extramarital sexual relations, it has always been widely practiced. Nevertheless, these religious codes have always had a strong influence on peoples' attitudes to issues of modesty in dress, behavior, speech etc.		On the other hand, some people adopt the view that pleasure is its own justification for sexual activity. Hedonism is a school of thought which argues that pleasure is the only intrinsic good.[44]		Human sexual activity, like many other kinds of activity engaged in by humans, is generally influenced by social rules that are culturally specific and vary widely. These social rules are referred to as sexual morality (what can and can not be done by society's rules) and sexual norms (what is and is not expected).		Sexual ethics, morals, and norms relate to issues including deception/honesty, legality, fidelity and consent. Some activities, known as sex crimes in some locations, are illegal in some jurisdictions, including those conducted between (or among) consenting and competent adults (examples include sodomy law and adult-adult incest).		Some people who are in a relationship but want to hide polygamous activity (possibly of opposite sexual orientation) from their partner, may solicit consensual sexual activity with others through personal contacts, online chat rooms, or, advertising in select media.		Swinging, on the other hand, involves singles or partners in a committed relationship engaging in sexual activities with others as a recreational or social activity.[45] The increasing popularity of swinging is regarded by some as arising from the upsurge in sexual activity during the sexual revolution of the 1960s. Swinging sexual activity can take place in a sex club, also known as a swinger club (not to be confused with a strip club).[46]		Some people engage in various sexual activities as a business transaction. When this involves having sex with, or performing certain actual sexual acts for another person in exchange for money or something of value, it is called prostitution. Other aspects of the adult industry include phone sex operators, strip clubs, and pornography.		Social gender roles can influence sexual behavior as well as the reaction of individuals and communities to certain incidents; the World Health Organization states that, "Sexual violence is also more likely to occur where beliefs in male sexual entitlement are strong, where gender roles are more rigid, and in countries experiencing high rates of other types of violence."[47] Some societies, such as those where the concepts of family honor and female chastity are very strong, may practice violent control of female sexuality, through practices such as honor killings and female genital mutilation.[48][49]		The relation between gender equality and sexual expression is recognized, and promotion of equity between men and women is crucial for attaining sexual and reproductive health, as stated by the UN International Conference on Population and Development Program of Action:[50]		BDSM is a variety of erotic practices or roleplaying involving bondage, dominance and submission, sadomasochism, and other interpersonal dynamics. Given the wide range of practices, some of which may be engaged in by people who do not consider themselves as practicing BDSM, inclusion in the BDSM community or subculture is usually dependent on self-identification and shared experience. BDSM communities generally welcome anyone with a non-normative streak who identifies with the community; this may include cross-dressers, extreme body modification enthusiasts, animal players, latex or rubber aficionados, and others.		B/D, a form of BDSM, is bondage and discipline. Bondage includes the restraint of the body or mind.[51] D/S means "dominant and submissive." A dominant is someone who takes control of someone who wishes to give up control. A submissive is someone who gives up the control to a person who wishes to take control.[51] S/M (sadism and masochism) means an individual who takes pleasure in the humiliation or pain of others. Masochism means an individual who takes pleasure from their own pain or humiliation.[51]		Unlike the usual "power neutral" relationships and play styles commonly followed by couples, activities and relationships within a BDSM context are often characterized by the participants' taking on complementary, but unequal roles; thus, the idea of informed consent of both the partners becomes essential. Participants who exert sexual dominance over their partners are known as dominants or tops, while participants who take the passive, receiving, or obedient role are known as submissives or bottoms.		Individuals are also sometimes abbreviated when referred to in writing, so a dominant person may be referred to as a "dom" for a man or a woman. Sometimes a woman may choose to use the female specific term "Domme". Both terms are pronounced the same when spoken. Individuals who can change between top/dominant and bottom/submissive roles—whether from relationship to relationship or within a given relationship—are known as switches. The precise definition of roles and self-identification is a common subject of debate within the community.[52]		In a 2013 study, the researchers suggest that BDSM is a sexual act where they play role games, use restraint, use power exchange, use suppression and pain is sometimes involved depending on individual(s).[53] The study indicates that, in the past, BDSM has been seen as maladaptive to one's psychological health, but that this may be incorrect. According to the study, one who participates in BDSM can have greater strength socially, mentally and have greater independence than those who do not practice BDSM.[53] It states that people who participate in BDSM play actually have higher subjective well-being, and that this might be due to the fact that BDSM play requires extensive communication. Before any sexual act occurs, the partners must discuss their agreement of their relationship. They discuss how long the play will last, the intensity, their actions, what each participant needs or desires. The sexual acts are all consensual and pleasurable to both parties.[53]		In a 2015 study, BDSM relationships were suggested to have a higher level of connection, intimacy, trust and communication compared to individuals who do not practice BDSM.[51] The study suggests that dominants and submissives exchange control for each other's pleasure and to satisfy a need. They mention that both parties enjoys pleasing their partner in any way they can. Submissive and Dominants who participated in their research, felt that this is one of the best things about BDSM. It gives a submissive pleasure to do things in general for their dominant. Where Dominant enjoys making their encounters all about the submissive. They enjoy doing things that makes their submissive happy. Their findings suggest that submissives and dominants found BDSM play more pleasurable and fun. BDSM was also suggested to improve personal growth, romantic relationships, their sense of community, their sense of self, the dominants confidence, and help an individual cope with everyday things by giving them a psychological release.[51]		There are many laws and social customs which prohibit, or in some way affect sexual activities. These laws and customs vary from country to country, and have varied over time. They cover, for example, a prohibition to non-consensual sex, to sex outside of marriage, to sexual activity in public, besides many others. Many of these restrictions are non-controversial, but some have been the subject of public debate.		Most societies consider it a serious crime to force someone to engage in sexual acts or to engage in sexual activity with someone who does not consent. This is called sexual assault, and if sexual penetration occurs it is called rape, the most serious kind of sexual assault. The details of this distinction may vary among different legal jurisdictions. Also, what constitutes effective consent in sexual matters varies from culture to culture and is frequently debated. Laws regulating the minimum age at which a person can consent to have sex (age of consent) are frequently the subject of debate, as is adolescent sexual behavior in general. Some societies have forced marriage, where consent may not be required.		Many locales have laws that limit or prohibit same-sex sexual activity.		In the West, sex before marriage is not illegal. There are social taboos and many religions condemn pre-marital sex. In many Muslim countries, such as Saudi Arabia, Pakistan,[54] Afghanistan,[55][56][57] Iran,[57] Kuwait,[58] Maldives,[59] Morocco,[60] Oman,[61] Mauritania,[62] United Arab Emirates,[63][64] Sudan,[65] Yemen,[66] any form of sexual activity outside marriage is illegal. Those found guilty, especially women, may be forced to wed the sexual partner, publicly beaten, or stoned to death.[67] In many African and native tribes, sexual activity is not viewed as a privilege or right of a married couple, but rather as the unification of bodies and is thus not frowned upon.[68]		Other studies have analyzed the changing attitudes about sex that American adolescents have outside of marriage. Adolescents were asked how they felt about oral and vaginal sex in relation to their health, social, and emotional well-being. Overall, teenagers felt that oral sex was viewed as more socially positive amongst their demographic.[69] Results stated that teenagers believed that oral sex for dating and non-dating adolescents was less threatening to their overall values and beliefs than vaginal sex was.[69] When asked, teenagers who participated in the research viewed oral sex as more acceptable to their peers, and their personal values than vaginal sex.[69]		The laws of each jurisdiction set the minimum age at which a young person is allowed to engage in sexual activity.[70] This age of consent is typically between 14 and 18 years, but laws vary. In many jurisdictions, age of consent is a person's mental or functional age.[71][71][72][73] As a result, those above the set age of consent may still be considered unable to legally consent due to mental immaturity.[71][72][73][74][75] Many jurisdictions regard any sexual activity by an adult involving a child as child sexual abuse.		Age of consent may vary by the type of sexual act, the sex of the actors, or other restrictions such as abuse of a position of trust. Some jurisdictions also make allowances for young people engaged in sexual acts with each other.[76]		Most jurisdictions prohibit sexual activity between certain close relatives. These laws vary to some extent; such acts are called incestuous.		Non-consensual sexual activity or subjecting an unwilling person to witnessing a sexual activity are forms of sexual abuse, as well as (in many countries) certain non-consensual paraphilias such as frotteurism, telephone scatophilia (indecent phonecalls), and non-consensual exhibitionism and voyeurism (known as "indecent exposure" and "peeping tom" respectively).[77]		People sometimes exchange sex for money or access to other resources. This practice, called prostitution, takes place under many varied circumstances. The person who receives payment for sexual services is called a prostitute and the person who receives such services is known by a multitude of terms, including (and most commonly) "john." Prostitution is one of the branches of the sex industry. The legal status of prostitution varies from country to country, from being a punishable crime to a regulated profession. Estimates place the annual revenue generated from the global prostitution industry to be over $100 billion.[78] Prostitution is sometimes referred to as "the world's oldest profession".[79] Prostitution may be a voluntary individual activity or facilitated or forced by pimps.		Survival sex is a form of prostitution engaged in by people in need, usually when homeless or otherwise disadvantaged people trade sex for food, a place to sleep, or other basic needs, or for drugs.[80] The term is used by sex trade and poverty researchers and aid workers.[81][82]		
A famine is a widespread scarcity of food,[1] caused by several factors including crop failure, population imbalance, or government policies. This phenomenon is usually accompanied or followed by regional malnutrition, starvation, epidemic, and increased mortality. Every inhabited continent in the world has experienced a period of famine throughout history. In the 19th and 20th century, it was generally Eastern Europe and Asia that suffered the most deaths from famine. The numbers dying from famine began to fall sharply from the 1970s.		Some countries, particularly in sub-Saharan Africa, continue to have extreme cases of famine. Since 2010, Africa has been the most affected continent in the world. As of 2017, the United Nations has warned some 20 million are at risk in South Sudan, Somalia, Nigeria and Yemen. Agricultural conditions have been fluctuating more and more due to variations in weather, and the distribution of food has been affected by conflict. Most programmes now direct their aid towards Africa.		According to the United Nations humanitarian criteria, even if there are food shortages with large numbers of people lacking nutrition, a famine is declared only when certain measures of mortality, malnutrition and hunger are met. The criteria are:		The declaration of a famine carries no binding obligations on the UN or member states, but serves to focus global attention on the problem.[2]		The cyclical occurrence of famine has been a mainstay of societies engaged in subsistence agriculture since the dawn of agriculture itself. The frequency and intensity of famine has fluctuated throughout history, depending on changes in food demand, such as population growth, and supply-side shifts caused by changing climatic conditions. Famine was first eliminated in Holland and England during the 17th century, due to the commercialization of agriculture and the implementation of improved techniques to increase crop yields.		In the 16th and 17th century, the feudal system began to break down, and more prosperous farmers began to enclose their own land and improve their yields to sell the surplus crops for a profit. These capitalist landowners paid their labourers with money, thereby increasing the commercialization of rural society. In the emerging competitive labour market, better techniques for the improvement of labour productivity were increasingly valued and rewarded. It was in the farmer's interest to produce as much as possible on their land in order to sell it to areas that demanded that product. They produced guaranteed surpluses of their crop every year if they could.		Subsistence peasants were also increasingly forced to commercialize their activities because of increasing taxes. Taxes that had to be paid to central governments in money forced the peasants to produce crops to sell. Sometimes they produced industrial crops, but they would find ways to increase their production in order to meet both their subsistence requirements as well as their tax obligations. Peasants also used the new money to purchase manufactured goods. The agricultural and social developments encouraging increased food production were gradually taking place throughout the 16th century, but took off in the early 17th century.		By the 1590s, these trends were sufficiently developed in the rich and commercialized province of Holland to allow its population to withstand a general outbreak of famine in Western Europe at that time. By that time, the Netherlands had one of the most commercialized agricultural systems in Europe. They grew many industrial crops such as flax, hemp and hops. Agriculture became increasingly specialized and efficient. The efficiency of Dutch agriculture allowed for much more rapid urbanization in the late sixteenth and early seventeenth centuries than anywhere else in Europe. As a result, productivity and wealth increased, allowing the Netherlands to maintain a steady food supply.[3]		By 1650, English agriculture had also become commercialized on a much wider scale. The last peace-time famine in England was in 1623–24. There were still periods of hunger, as in the Netherlands, but no more famines ever occurred. Common areas for pasture were enclosed for private use and large scale, efficient farms were consolidated. Other technical developments included the draining of marshes, more efficient field use patterns, and the wider introduction of industrial crops. These agricultural developments led to wider prosperity in England and increasing urbanization.[4] By the end of the 17th century, English agriculture was the most productive in Europe.[5] In both England and the Netherlands, the population stabilized between 1650 and 1750, the same time period in which the sweeping changes to agriculture occurred. Famine still occurred in other parts of Europe, however. In East Europe, famines occurred as late as the twentieth century.		Because of the severity of famine, it was a chief concern for governments and other authorities. In pre-industrial Europe, preventing famine, and ensuring timely food supplies, was one of the chief concerns of many governments, although they were severely limited in their options due to limited levels of external trade and an infrastructure and bureaucracy generally too rudimentary to effect real relief. Most governments were concerned by famine because it could lead to revolt and other forms of social disruption.		By the mid-19th century and the onset of the Industrial Revolution, it became possible for governments to alleviate the effects of famine through price controls, large scale importation of food products from foreign markets, stockpiling, rationing, regulation of production and charity. The Great Famine of 1845 in Ireland was one of the first famines to feature such intervention, although the government response was often lacklustre. The initial response of the British government to the early phase of the famine was "prompt and relatively successful," according to F. S. L. Lyons.[6] Confronted by widespread crop failure in the autumn of 1845, Prime Minister Sir Robert Peel purchased £100,000 worth of maize and cornmeal secretly from America. Baring Brothers & Co initially acted as purchasing agents for the Prime Minister. The government hoped that they would not "stifle private enterprise" and that their actions would not act as a disincentive to local relief efforts. Due to weather conditions, the first shipment did not arrive in Ireland until the beginning of February 1846.[7] The maize corn was then re-sold for a penny a pound.[8]		In 1846, Peel moved to repeal the Corn Laws, tariffs on grain which kept the price of bread artificially high. The famine situation worsened during 1846 and the repeal of the Corn Laws in that year did little to help the starving Irish; the measure split the Conservative Party, leading to the fall of Peel's ministry.[9] In March, Peel set up a programme of public works in Ireland.[10]		Despite this promising start, the measures undertaken by Peel's successor, Lord John Russell, proved comparatively "inadequate" as the crisis deepened. Russell's ministry introduced public works projects, which by December 1846 employed some half million Irish and proved impossible to administer. The government was influenced by a laissez-faire belief that the market would provide the food needed. It halted government food and relief works, and turned to a mixture of "indoor" and "outdoor" direct relief; the former administered in workhouses through the Poor Law, the latter through soup kitchens.[11]		A systematic attempt at creating the necessary regulatory framework for dealing with famine was developed by the British Raj in the 1880s. In order to comprehensively address the issue of famine, the British created an Indian Famine commission to recommend steps that the government would be required to take in the event of a famine.[12][13][14] The Famine Commission issued a series of government guidelines and regulations on how to respond to famines and food shortages called the Famine Code. The famine code was also one of the first attempts to scientifically predict famine in order to mitigate its effects. These were finally passed into law in 1883 under Lord Ripon.		The Code introduced the first famine scale: three levels of food insecurity were defined: near-scarcity, scarcity, and famine. "Scarcity" was defined as three successive years of crop failure, crop yields of one-third or one-half normal, and large populations in distress. "Famine" further included a rise in food prices above 140% of "normal", the movement of people in search of food, and widespread mortality.[12] The Commission identified that the loss of wages from lack of employment of agricultural labourers and artisans were the cause of famines. The Famine Code applied a strategy of generating employment for these sections of the population and relied on open-ended public works to do so.[15]		During the 20th century, an estimated 70 million people died from famines across the world, of whom an estimated 30 million died during the famine of 1958–61 in China.[16] The other most notable famines of the century included the Bengal famine of 1943, famines in China in 1928 and 1942, and a sequence of famines in the Soviet Union, including the Soviet famine of 1932–1933, caused by the policies of Stalin.		A few of the great famines of the late 20th century were: the Biafran famine in the 1960s, the Khmer Rouge-caused famine in Cambodia in the 1970s, the North Korean famine of the 1990s and the Ethiopian famine of 1983–85.		The latter event was reported on television reports around the world, carrying footage of starving Ethiopians whose plight was centered around a feeding station near the town of Korem. This stimulated the first mass movements to end famine across the world.		BBC newsreader Michael Buerk gave moving commentary of the tragedy on 23 October 1984, which he described as a "biblical famine". This prompted the Band Aid single, which was organized by Bob Geldof and featured more than 20 pop stars. The Live Aid concerts in London and Philadelphia raised even more funds for the cause. An estimated 900,000 people died within one year as a result of the famine, but the tens of millions of pounds raised by Band Aid and Live Aid are believed to have saved the lives of some Ethiopians who were in danger of dying.[citation needed]		Until 2017, worldwide deaths from famine had been falling dramatically. The world Peace Foundation reported that from the 1870s to the 1970s, great famines killed an average of 928,000 people a year. Since 1980, annual deaths had dropped to an average of 75,000, less than 10% of what they had been until the 1970s. That reduction was achieved despite the approximately 150,000 lives lost in the 2011 Somalia famine. Yet in 2017, the UN officially declared famine had returned to Africa, with about 20 million people at risk at death from starvation in Nigeria, in South Sudan, in Yemen, and in Somalia.[17]		In the mid-22nd century BC, a sudden and short-lived climatic change that caused reduced rainfall resulted in several decades of drought in Upper Egypt. The resulting famine and civil strife is believed to have been a major cause of the collapse of the Old Kingdom. An account from the First Intermediate Period states, "All of Upper Egypt was dying of hunger and people were eating their children." In 1680s, famine extended across the entire Sahel, and in 1738 half the population of Timbuktu died of famine.[18] In Egypt, between 1687 and 1731, there were six famines.[19] The famine that afflicted Egypt in 1784 cost it roughly one-sixth of its population.[20] The Maghreb experienced famine and plague in the late 18th century and early 19th century.[21][22] There was famine in Tripoli in 1784, and in Tunis in 1785.[23]		According to John Iliffe, "Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys."[24]		The first documentation of weather in West-Central Africa occurs around the mid-16th to 17th centuries in areas such as Luanda Kongo, however, not much data was recorded on the issues of weather and disease except for a few notable documents. The only records obtained are of violence between Portuguese and Africans during the Battle of Mbilwa in 1665. In these documents the Portuguese wrote of African raids on Portuguese merchants solely for food, giving clear signs of famine. Additionally, instances of cannibalism by the African Jaga were also more prevalent during this time frame, indicating an extreme deprivation of a primary food source.[25]		A notable period of famine occurred around the turn of the 20th century in the Congo Free State. In forming this state, Leopold used mass labor camps to finance his empire.[26] This period resulted in the death of up to 10 million Congolese from brutality, disease and famine.[27] Some colonial "pacification" efforts often caused severe famine, notably with the repression of the Maji Maji revolt in Tanganyika in 1906. The introduction of cash crops such as cotton, and forcible measures to impel farmers to grow these crops, sometimes impoverished the peasantry in many areas, such as northern Nigeria, contributing to greater vulnerability to famine when severe drought struck in 1913.[28]		A large-scale famine occurred in Ethiopia in 1888 and succeeding years, as the rinderpest epizootic, introduced into Eritrea by infected cattle, spread southwards reaching ultimately as far as South Africa. In Ethiopia it was estimated that as much as 90 percent of the national herd died, rendering rich farmers and herders destitute overnight. This coincided with drought associated with an el Nino oscillation, human epidemics of smallpox, and in several countries, intense war. The Ethiopian Great famine that afflicted Ethiopia from 1888 to 1892 cost it roughly one-third of its population.[29] In Sudan the year 1888 is remembered as the worst famine in history, on account of these factors and also the exactions imposed by the Mahdist state.		Records compiled for the Himba recall two droughts from 1910 to 1917. They were recorded by the Himba through a method of oral tradition. From 1910 to 1911 the Himba described the drought as "drought of the omutati seed" also called omangowi, which means the fruit of an unidentified vine that people ate during the time period. From 1914 to 1916 droughts brought katur' ombanda or kari' ombanda which means "the time of eating clothing".[30]		For the middle part of the 20th century, agriculturalists, economists and geographers did not consider Africa to be especially famine prone. From 1870 to 2010, 87 per cent of deaths from famine occurred in Asia and Eastern Europe, with only 9.2 per cent in Africa.[17] There were notable counter-examples, such as the famine in Rwanda during World War II and the Malawi famine of 1949, but most famines were localized and brief food shortages. Although the drought was brief the main cause of death in Rwanda was due to Belgian prerogatives to acquisition grain from their colony (Rwanda). The increased grain acquisition was related to WW2. This and the drought caused 300,000 Rwandans to perish.[26]		From 1967 to 1969 large scale famine occurred in Biafra and Nigeria due to a government blockade of the Breakaway territory. It is estimated that 1.5 million people died of starvation due to this famine. Additionally, drought and other government interference with the food supply caused 500 thousand Africans to perish in Central and West Africa.[31]		Famine recurred in the early 1970s, when Ethiopia and the west African Sahel suffered drought and famine. The Ethiopian famine of that time was closely linked to the crisis of feudalism in that country, and in due course helped to bring about the downfall of the Emperor Haile Selassie. The Sahelian famine was associated with the slowly growing crisis of pastoralism in Africa, which has seen livestock herding decline as a viable way of life over the last two generations.		Famines occurred in Sudan in the late-1970s and again in 1990 and 1998. The 1980 famine in Karamoja, Uganda was, in terms of mortality rates, one of the worst in history. 21% of the population died, including 60% of the infants.[32] In the 1980s, large scale multilayer drought occurred in the Sudan and Sahelian regions of Africa. This caused famine because even though the Sudanese Government believed there was a surplus of grain, there were local deficits across the region.[33]		In October 1984, television reports describing the Ethiopian famine as "biblical", prompted the Live Aid concerts in London and Philadelphia, which raised large sums to alleviate the suffering. A primary cause of the famine (one of the largest seen in the country) is that Ethiopia (and the surrounding Horn) was still recovering from the droughts which occurred in the mid-late 1970s. Compounding this problem was the intermittent fighting due to civil war, the government's lack of organization in providing relief, and hoarding of supplies to control the population. Ultimately, over 1 million Ethiopians died and over 22 million people suffered due to the prolonged drought, which lasted roughly 2 years.[34]		In 1992 Somalia became a war zone with no effective government, police, or basic services after the collapse of the dictatorship led by Siad Barre and the split of power between warlords. This coincided with a massive drought, causing over 300,000 Somalians to perish.[35]		Since the start of the 21st century, more effective early warning and humanitarian response actions have reduced the number of deaths by famine markedly. That said, many African countries are not self-sufficient in food production, relying on income from cash crops to import food. Agriculture in Africa is susceptible to climatic fluctuations, especially droughts which can reduce the amount of food produced locally. Other agricultural problems include soil infertility, land degradation and erosion, swarms of desert locusts, which can destroy whole crops, and livestock diseases. Desertification is increasingly problematic: the Sahara reportedly spreads up to 48 kilometres (30 mi) per year.[36] The most serious famines have been caused by a combination of drought, misguided economic policies, and conflict. The 1983–85 famine in Ethiopia, for example, was the outcome of all these three factors, made worse by the Communist government's censorship of the emerging crisis. In Sudan at the same date, drought and economic crisis combined with denials of any food shortage by the then-government of President Gaafar Nimeiry, to create a crisis that killed perhaps 250,000 people—and helped bring about a popular uprising that overthrew Nimeiry.		Numerous factors make the food security situation in Africa tenuous, including political instability, armed conflict and civil war, corruption and mismanagement in handling food supplies, and trade policies that harm African agriculture. An example of a famine created by human rights abuses is the 1998 Sudan famine. AIDS is also having long-term economic effects on agriculture by reducing the available workforce, and is creating new vulnerabilities to famine by overburdening poor households. On the other hand, in the modern history of Africa on quite a few occasions famines acted as a major source of acute political instability.[37] In Africa, if current trends of population growth and soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to United Nations University (UNU)'s Ghana-based Institute for Natural Resources in Africa.[38]		Recent famines in Africa include the 2005–06 Niger food crisis, the 2010 Sahel famine and the 2011 East Africa drought, where two consecutive missed rainy seasons precipitated the worst drought in East Africa in 60 years.[39][40] An estimated 50,000 to 150,000 people are reported to have died during the period.[41][42] In 2012, the Sahel drought put more than 10 million people in the western Sahel at risk of famine (according to a Methodist Relief & Development Fund (MRDF) aid expert), due to a month-long heat wave.[43][44]		Today, famine is most widespread in Sub-Saharan Africa, but with exhaustion of food resources, overdrafting of groundwater, wars, internal struggles, and economic failure, famine continues to be a worldwide problem with hundreds of millions of people suffering.[45] These famines cause widespread malnutrition and impoverishment. The famine in Ethiopia in the 1980s had an immense death toll, although Asian famines of the 20th century have also produced extensive death tolls. Modern African famines are characterized by widespread destitution and malnutrition, with heightened mortality confined to young children.		Against a backdrop of conventional interventions through the state or markets, alternative initiatives have been pioneered to address the problem of food security. One pan African example is the Great Green Wall Another example is the "Community Area-Based Development Approach" to agricultural development ("CABDA"), an NGO programme with the objective of providing an alternative approach to increasing food security in Africa. CABDA proceeds through specific areas of intervention such as the introduction of drought-resistant crops and new methods of food production such as agro-forestry. Piloted in Ethiopia in the 1990s it has spread to Malawi, Uganda, Eritrea and Kenya. In an analysis of the programme by the Overseas Development Institute, CABDA's focus on individual and community capacity-building is highlighted. This enables farmers to influence and drive their own development through community-run institutions, bringing food security to their household and region.[46]		The organization of African unity and its role in the African crisis has been interested in the political aspects of the continent, especially the liberation of the occupied parts of it and the elimination of racism. The organization has succeeded in this area but the economic field and development has not succeeded in these fields. African leaders have agreed to waive the role of their organization in the development to the United Nations through the Economic Commission for Africa "ECA".[47]		Chinese scholars had kept count of 1,828 instances of famine from 108 BC to 1911 in one province or another—an average of close to one famine per year.[48] From 1333 to 1337 a terrible famine killed 6 million Chinese. The four famines of 1810, 1811, 1846, and 1849 are said to have killed no fewer than 45 million people.[49]		Japan experienced more than 130 famines between 1603 and 1868.[50]		The period from 1850 to 1873 saw, as a result of the Taiping Rebellion, drought, and famine, the population of China drop by over 30 million people.[51] China's Qing Dynasty bureaucracy, which devoted extensive attention to minimizing famines, is credited with averting a series of famines following El Niño-Southern Oscillation-linked droughts and floods. These events are comparable, though somewhat smaller in scale, to the ecological trigger events of China's vast 19th-century famines.[52] Qing China carried out its relief efforts, which included vast shipments of food, a requirement that the rich open their storehouses to the poor, and price regulation, as part of a state guarantee of subsistence to the peasantry (known as ming-sheng).		When a stressed monarchy shifted from state management and direct shipments of grain to monetary charity in the mid-19th century, the system broke down. Thus the 1867–68 famine under the Tongzhi Restoration was successfully relieved but the Great North China Famine of 1877–78, caused by drought across northern China, was a catastrophe. The province of Shanxi was substantially depopulated as grains ran out, and desperately starving people stripped forests, fields, and their very houses for food. Estimated mortality is 9.5 to 13 million people.[53]		The largest famine of the 20th century, and almost certainly of all time, was the 1958–61 Great Leap Forward famine in China. The immediate causes of this famine lay in Mao Zedong's ill-fated attempt to transform China from an agricultural nation to an industrial power in one huge leap. Communist Party cadres across China insisted that peasants abandon their farms for collective farms, and begin to produce steel in small foundries, often melting down their farm instruments in the process. Collectivisation undermined incentives for the investment of labor and resources in agriculture; unrealistic plans for decentralized metal production sapped needed labor; unfavorable weather conditions; and communal dining halls encouraged overconsumption of available food.[54] Such was the centralized control of information and the intense pressure on party cadres to report only good news—such as production quotas met or exceeded—that information about the escalating disaster was effectively suppressed. When the leadership did become aware of the scale of the famine, it did little to respond, and continued to ban any discussion of the cataclysm. This blanket suppression of news was so effective that very few Chinese citizens were aware of the scale of the famine, and the greatest peacetime demographic disaster of the 20th century only became widely known twenty years later, when the veil of censorship began to lift.		The exact number of famine deaths during 1958–61 is difficult to determine, and estimates range from 18[55] to at least 42 million[56] people, with a further 30 million cancelled or delayed births.[57] It was only when the famine had wrought its worst that Mao reversed agricultural collectivisation policies, which were effectively dismantled in 1978. China has not experienced a famine of the proportions of the Great Leap Forward since 1961.[58]		In 1975, the Khmer Rouge entered the capital of Phnom Penh and took control of Cambodia. The new government under Pol Pot drove all urban residents into the countryside to work on communal farm and civil work projects. No international relief would come until the Vietnamese army invaded in 1979 and liberated the country. While Pol Pot was in power, between one and three million people died out of a total population of eight million. Many were executed. Most died from malnourishment and exhaustion as a result of the famine caused by inept and negligent government officials.[59][60]		Famine struck North Korea in the mid-1990s, set off by unprecedented floods. This autarkic urban, industrial state depended on massive inputs of subsidised goods, including fossil fuels, primarily from the Soviet Union and the People's Republic of China. When the Soviet collapse and China's marketization switched trade to a hard currency, full-price basis, North Korea's economy collapsed. The vulnerable agricultural sector experienced a massive failure in 1995–96, expanding to full-fledged famine by 1996–99.		Estimates based on the North Korean census suggest that 240,000 to 420,000 people died as a result of the famine and that there were 600,000 to 850,000 unnatural deaths in North Korea from 1993 to 2008.[61] North Korea has not yet regained food self-sufficiency and relies on external food aid from China, Japan, South Korea, Russia and the United States. While Woo-Cumings have focused on the FAD side of the famine, Moon argues that FAD shifted the incentive structure of the authoritarian regime to react in a way that forced millions of disenfranchised people to starve to death (Moon, 2009).[62]		According to the UN’s Food and Agriculture Organisation (FAO), North Korea is facing a serious cereal shortfall after the country’s crop harvest was diminished as a result of severe drought.[63] The FAO estimated that early-season production fell by over 30 percent compared to agricultural output from the previous year, leading to the country's worst famine since 2001.[64]		Various famines have occurred in Vietnam. Japanese occupation during World War II caused the Vietnamese Famine of 1945, which caused 2 million deaths, or 10% of the population then.[65] Following the unification of the country after the Vietnam War, Vietnam experienced a food shortage in the 1980s, which prompted many people to flee the country.		Owing to its almost entire dependence upon the monsoon rains, India is vulnerable to crop failures, which upon occasion deepen into famine.[66] There were 14 famines in India between the 11th and 17th centuries (Bhatia, 1985). For example, during the 1022–1033 Great famines in India entire provinces were depopulated. Famine in Deccan killed at least two million people in 1702–1704. B.M. Bhatia believes that the earlier famines were localised, and it was only after 1860, during the British rule, that famine came to signify general shortage of foodgrains in the country. There were approximately 25 major famines spread through states such as Tamil Nadu in the south, and Bihar and Bengal in the east during the latter half of the 19th century.		Romesh Chunder Dutt argued as early as 1900, and present-day scholars such as Amartya Sen agree, that some historic famines were a product of both uneven rainfall and British economic and administrative policies, which since 1857 had led to the seizure and conversion of local farmland to foreign-owned plantations, restrictions on internal trade, heavy taxation of Indian citizens to support British expeditions in Afghanistan (see The Second Anglo-Afghan War), inflationary measures that increased the price of food, and substantial exports of staple crops from India to Britain. (Dutt, 1900 and 1902; Srivastava, 1968; Sen, 1982; Bhatia, 1985.)		Some British citizens, such as William Digby, agitated for policy reforms and famine relief, but Lord Lytton, the governing British viceroy in India, opposed such changes in the belief that they would stimulate shirking by Indian workers. The first, the Bengal famine of 1770, is estimated to have taken around 10 million lives—one-third of Bengal's population at the time. Other notable famines include the Great Famine of 1876–78, in which 6.1 million to 10.3 million people died[67] and the Indian famine of 1899–1900, in which 1.25 to 10 million people died.[68] The famines were ended by the 20th century with the exception of the Bengal famine of 1943 killing an estimated 2.1 million Bengalis during World War II.[69]		The observations of the Famine Commission of 1880 support the notion that food distribution is more to blame for famines than food scarcity. They observed that each province in British India, including Burma, had a surplus of foodgrains, and the annual surplus was 5.16 million tons (Bhatia, 1970). At that time, annual export of rice and other grains from India was approximately one million tons.		The Maharashtra drought in which there were zero deaths and one which is known for the successful employment of famine prevention policies, unlike during British rule.[71]		The Great Persian Famine of 1870–1871 is believed to have caused the death of 1.5 million persons (20–25 percent of the population) in Persia (present-day Iran).[72]		In the early 20th century an Ottoman blockade of food being exported to Lebanon caused a famine which killed up to 450,000 Lebanese (about one-third of the population). The famine killed more people than the Lebanese Civil War. The blockade was caused by uprisings in the Syrian region of the Empire including one which occurred in the 1860s which lead to the massacre of thousands of Lebanese and Syrian by Ottoman Turks and local Druze [73]		The Great Famine of 1315–1317 (or to 1322) was the first major food crisis to strike Europe in the 14th century. Millions in northern Europe died over an extended number of years, marking a clear end to the earlier period of growth and prosperity during the 11th and 12th centuries.[74] Starting with bad weather in the spring of 1315, widespread crop failures lasted until the summer of 1317, from which Europe did not fully recover until 1322. It was a period marked by extreme levels of criminal activity, disease and mass death, infanticide, and cannibalism. It had consequences for Church, State, European society and future calamities to follow in the 14th century. There were 95 famines in medieval Britain,[75] and 75 or more in medieval France.[76] More than 10% of England's population, or at least 500,000 people, may have died during the famine of 1315–1316.[77]		Famine was a very destabilizing and devastating occurrence. The prospect of starvation led people to take desperate measures. When scarcity of food became apparent to peasants, they would sacrifice long-term prosperity for short-term survival. They would kill their draught animals, leading to lowered production in subsequent years. They would eat their seed corn, sacrificing next year's crop in the hope that more seed could be found. Once those means had been exhausted, they would take to the road in search of food. They migrated to the cities where merchants from other areas would be more likely to sell their food, as cities had a stronger purchasing power than did rural areas. Cities also administered relief programs and bought grain for their populations so that they could keep order. With the confusion and desperation of the migrants, crime would often follow them. Many peasants resorted to banditry in order to acquire enough to eat.		One famine would often lead to difficulties in the following years because of lack of seed stock or disruption of routine, or perhaps because of less-available labour. Famines were often interpreted as signs of God's displeasure. They were seen as the removal, by God, of His gifts to the people of the Earth. Elaborate religious processions and rituals were made to prevent God's wrath in the form of famine.		During the Little Ice Age from the 15th century to the 18th century, famines in Europe became more frequent. This often led to a rise in conspiracy theories concerning the causes behind these famines, such as the Pacte de Famine in France.[78]		The 1590s saw the worst famines in centuries across all of Europe. Famine had been relatively rare during the 16th century. The economy and population had grown steadily as subsistence populations tend to when there is an extended period of relative peace (most of the time). Subsistence peasant populations will almost always increase when possible since the peasants will try to spread the work to as many hands as possible. Although peasants in areas of high population density, such as northern Italy, had learned to increase the yields of their lands through techniques such as promiscuous culture, they were still quite vulnerable to famines, forcing them to work their land even more intensively.		The great famine of the 1590s began the period of famine and decline in the 17th century. The price of grain, all over Europe was high, as was the population. Various types of people were vulnerable to the succession of bad harvests that occurred throughout the 1590s in different regions. The increasing number of wage labourers in the countryside were vulnerable because they had no food of their own, and their meager living was not enough to purchase the expensive grain of a bad-crop year. Town labourers were also at risk because their wages would be insufficient to cover the cost of grain, and, to make matters worse, they often received less money in bad-crop years since the disposable income of the wealthy was spent on grain. Often, unemployment would be the result of the increase in grain prices, leading to ever-increasing numbers of urban poor.		All areas of Europe were badly affected by the famine in these periods, especially rural areas. The Netherlands was able to escape most of the damaging effects of the famine, though the 1590s were still difficult years there. Amsterdam's grain trade with the Baltic, guaranteed a food supply.		The years around 1620 saw another period of famine sweep across Europe. These famines were generally less severe than the famines of twenty-five years earlier, but they were nonetheless quite serious in many areas. Perhaps the worst famine since 1600, the great famine in Finland in 1696, killed one-third of the population.[79]		Devastating harvest failures afflicted the northern Italian economy from 1618 to 1621, and it did not recover fully for centuries. There were serious famines in the late-1640s and less severe ones in the 1670s throughout northern Italy.		Over two million people died in two famines in France between 1693 and 1710. Both famines were made worse by ongoing wars.[80]		As late as the 1690s, Scotland experienced famine which reduced the population of parts of Scotland by at least 15%.[81]		The Great Famine of 1695–1697 may have killed a third of the Finnish population.[82] and roughly 10% of Norway's population.[83] Death rates rose in Scandinavia between 1740 and 1800 as the result of a series of crop failures.[84] For instance, the Finnish famine of 1866–1868 killed 15% of the population.		The period of 1740–43 saw frigid winters and summer droughts, which led to famine across Europe and a major spike in mortality.[85] The winter 1740–41 was unusually cold, possibly because of volcanic activity.[86]		According to Scott and Duncan (2002), "Eastern Europe experienced more than 150 recorded famines between AD 1500 and 1700 and there were 100 hunger years and 121 famine years in Russia between AD 971 and 1974."[87]		The Great Famine, which lasted from 1770 until 1771, killed about one tenth of Czech lands' population, or 250,000 inhabitants, and radicalised countrysides leading to peasant uprisings.[88]		There were sixteen good harvests and 111 famine years in northern Italy from 1451 to 1767.[89] According to Stephen L. Dyson and Robert J. Rowland, "The Jesuits of Cagliari [in Sardinia] recorded years during the late 1500s "of such hunger and so sterile that the majority of the people could sustain life only with wild ferns and other weeds" ... During the terrible famine of 1680, some 80,000 persons, out of a total population of 250,000, are said to have died, and entire villages were devastated..."[90]		According to Bryson (1974), there were thirty-seven famine years in Iceland between 1500 and 1804.[91] In 1783 the volcano Laki in south-central Iceland erupted. The lava caused little direct damage, but ash and sulphur dioxide spewed out over most of the country, causing three-quarters of the island's livestock to perish. In the following famine, around ten thousand people died, one-fifth of the population of Iceland. [Asimov, 1984, 152–53]		Other areas of Europe have known famines much more recently. France saw famines as recently as the 19th century. The Great Famine in Ireland, 1846–1851, caused by the failure of the potato crop over a few years, resulted in 1,000,000 dead and another 2,000,000 refugees fleeing to Britain, Australia and the United States.[92]		Famine still occurred in Eastern Europe during the 20th century. Droughts and famines in Imperial Russia are known to have happened every 10 to 13 years, with average droughts happening every 5 to 7 years. Russia experienced eleven major famines between 1845 and 1922, one of the worst being the famine of 1891–92.[93]		Famines continued in the Soviet era, the most notorious being the Holodomor in various parts of the country, especially the Volga, and the Ukrainian and northern Kazakh SSR's during the winter of 1932–1933. The Soviet famine of 1932–1933 is nowadays reckoned to have cost an estimated 6 million lives.[94] The last major famine in the USSR happened in 1947 due to the severe drought and the mismanagement of grain reserves by the Soviet government.[95]		The Hunger Plan, i.e. the Nazi plan to starve large sections of the Soviet population, caused the deaths of many. The Russian Academy of Sciences in 1995 reported civilian victims in the USSR at German hands, including Jews, totalled 13.7 million dead, 20% of the 68 million persons in the occupied USSR. This included 4.1 million famine and disease deaths in occupied territory. There were an additional estimated 3 million famine deaths in areas of the USSR not under German occupation.[96]		The 872 days of the Siege of Leningrad (1941–1944) caused unparalleled famine in the Leningrad region through disruption of utilities, water, energy and food supplies. This resulted in the deaths of about one million people.[97]		Famine even struck in Western Europe during the Second World War. In the Netherlands, the Hongerwinter of 1944 killed approximately 30,000 people. Some other areas of Europe also experienced famine at the same time.		The pre-Columbian Americans often dealt with severe food shortages and famines.[98] The persistent drought around 850 AD coincided with the collapse of Classic Maya civilization, and the famine of One Rabbit (AD 1454) was a major catastrophe in Mexico.[99]		Brazil's 1877–78 Grande Seca (Great Drought), the worst in Brazil's history,[100] caused approximately half a million deaths.[101] The one from 1915 was devastating too.[102]		Easter Island was hit by a great famine between the 15th and 18th centuries. Hunger and subsequent cannibalism was caused by overpopulation and depletion of natural resources as a result of deforestation, partly because work on megalithic monuments required a lot of wood.[103]		There are other documented episodes of famine in various islands of Polynesia, such as occurred in Kau, Hawaii in 1868.[104]		According to Daniel Lord Smail, "'Famine cannibalism' was until recently a regular feature of life in the islands of the Massim near New Guinea and of some other societies of Southeast Asia and the Pacific."[105]		Relief technologies, including immunization, improved public health infrastructure, general food rations and supplementary feeding for vulnerable children, has provided temporary mitigation to the mortality impact of famines, while leaving their economic consequences unchanged, and not solving the underlying issue of too large a regional population relative to food production capability. Humanitarian crises may also arise from genocide campaigns, civil wars, refugee flows and episodes of extreme violence and state collapse, creating famine conditions among the affected populations.		Despite repeated stated intentions by the world's leaders to end hunger and famine, famine remains a chronic threat in much of Africa and Asia. In July 2005, the Famine Early Warning Systems Network labelled Niger with emergency status, as well as Chad, Ethiopia, South Sudan, Somalia and Zimbabwe. In January 2006, the United Nations Food and Agriculture Organization warned that 11 million people in Somalia, Kenya, Djibouti and Ethiopia were in danger of starvation due to the combination of severe drought and military conflicts.[106] In 2006, the most serious humanitarian crisis in Africa was in Sudan's region Darfur.		Frances Moore Lappé, later co-founder of the Institute for Food and Development Policy (Food First) argued in Diet for a Small Planet (1971) that vegetarian diets can provide food for larger populations, with the same resources, compared to omnivorous diets.		Noting that modern famines are sometimes aggravated by misguided economic policies, political design to impoverish or marginalize certain populations, or acts of war, political economists have investigated the political conditions under which famine is prevented. Economist Amartya Sen[note 1] states that the liberal institutions that exist in India, including competitive elections and a free press, have played a major role in preventing famine in that country since independence. Alex de Waal has developed this theory to focus on the "political contract" between rulers and people that ensures famine prevention, noting the rarity of such political contracts in Africa, and the danger that international relief agencies will undermine such contracts through removing the locus of accountability for famines from national governments.		The demographic impacts of famine are sharp. Mortality is concentrated among children and the elderly. A consistent demographic fact is that in all recorded famines, male mortality exceeds female, even in those populations (such as northern India and Pakistan) where there is a male longevity advantage during normal times. Reasons for this may include greater female resilience under the pressure of malnutrition, and possibly female's naturally higher percentage of body fat. Famine is also accompanied by lower fertility. Famines therefore leave the reproductive core of a population—adult women—lesser affected compared to other population categories, and post-famine periods are often characterized a "rebound" with increased births.		Even though the theories of Thomas Malthus would predict that famines reduce the size of the population commensurate with available food resources, in fact even the most severe famines have rarely dented population growth for more than a few years. The mortality in China in 1958–61, Bengal in 1943, and Ethiopia in 1983–85 was all made up by a growing population over just a few years. Of greater long-term demographic impact is emigration: Ireland was chiefly depopulated after the 1840s famines by waves of emigration.		The Guardian reports that in 2007 approximately 40% of the world's agricultural land is seriously degraded.[108] If current trends of soil degradation continue in Africa, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.[38] As of late 2007, increased farming for use in biofuels,[109] along with world oil prices at nearly $100 a barrel,[110] has pushed up the price of grain used to feed poultry and dairy cows and other cattle, causing higher prices of wheat (up 58%), soybean (up 32%), and maize (up 11%) over the year.[111][112] In 2007 Food riots have taken place in many countries across the world.[113][114][115] An epidemic of stem rust, which is destructive to wheat and is caused by race Ug99, has in 2007 spread across Africa and into Asia.[116][117]		Beginning in the 20th century, nitrogen fertilizers, new pesticides, desert farming, and other agricultural technologies began to be used to increase food production, in part to combat famine. Between 1950 and 1984, as the Green Revolution influenced agriculture, world grain production increased by 250%. However, as early as 1995, there were signs that these new developments may contribute to the decline of arable land (e.g. persistence of pesticides leading to soil contamination and decline of area available for farming). Developed nations have shared these technologies with developing nations with a famine problem.		David Pimentel, professor of ecology and agriculture at Cornell University, and Mario Giampietro, senior researcher at the National Research Institute on Food and Nutrition (INRAN), place in their study Food, Land, Population and the U.S. Economy the maximum U.S. population for a sustainable economy at 200 million.[119]		According to geologist Dale Allen Pfeiffer, coming decades could see rising food prices without relief and massive starvation on a global level.[120] Water deficits, which are already spurring heavy grain imports in numerous smaller countries, may soon do the same in larger countries, such as China or India.[121] The water tables are falling in many countries (including Northern China, the US, and India) due to widespread overconsumption. Other countries affected include Pakistan, Iran, and Mexico. This will eventually lead to water scarcity and cutbacks in grain harvest. Even with the overpumping of its aquifers, China has developed a grain deficit, contributing to the upward pressure on grain prices. Most of the three billion people projected to be added worldwide by mid-century will be born in countries already experiencing water shortages.		After China and India, there is a second tier of smaller countries with large water deficits – Algeria, Egypt, Iran, Mexico, and Pakistan. Four of these already import a large share of their grain. Only Pakistan remains marginally self-sufficient. But with a population expanding by 4 million a year, it will also soon turn to the world market for grain.[122] According to a UN climate report, the Himalayan glaciers that are the principal dry-season water sources of Asia's biggest rivers – Ganges, Indus, Brahmaputra, Yangtze, Mekong, Salween and Yellow – could disappear by 2350 as temperatures rise and human demand rises.[note 2][123][124] Approximately 2.4 billion people live in the drainage basin of the Himalayan rivers.[125] India, China, Pakistan, Afghanistan, Bangladesh, Nepal and Myanmar could experience floods followed by severe droughts in coming decades.[126] In India alone, the Ganges provides water for drinking and farming for more than 500 million people.[127][128]		Evan Fraser, a geographer at the University of Guelph in Ontario, Canada, explores the ways in which climate change may affect future famines.[129] To do this he draws on a range of historic cases where relatively small environmental problems triggered famines as a way of creating theoretical links between climate and famine in the future. Drawing on situations as diverse as the Great Irish Potato Famine,[130] a series of weather induced famines in Asia during the late 19th century, and famines in Ethiopia during the 1980s, he concludes there are three "lines of defense" that protect a community's food security from environmental change. The first line of defense is the agro-ecosystem on which food is produced: diverse ecosystems with well managed soils high in organic matter tend to be more resilient. The second line of defense is the wealth and skills of individual households: if those households affected by bad weather such as drought have savings or skills they may be able to do all right despite the bad weather. The final line of defense is created by the formal institutions present in a society. Governments, churches, or NGOs must be willing and able to mount effective relief efforts. Pulling this together, Evan Fraser argues that if an ecosystem is resilient enough, it may be able to withstand weather-related shocks. But if these shocks overwhelm the ecosystem's line of defense, it is necessary for the household to adapt using its skills and savings. If a problem is too big for the family or household, then people must rely on the third line of defense, which is whether or not the formal institutions present in a society are able to provide help. Evan Fraser concludes that in almost every situation where an environmental problem triggered a famine you see a failure in each of these three lines of defense.[131] Hence, understanding how climate change may cause famines in the future requires combining both an assessment of local socio-economic and environmental factors along with climate models that predict where bad weather may occur in the future[132][133][134]		Definitions of famines are based on three different categories—these include food supply-based, food consumption-based and mortality-based definitions. Some definitions of famines are:		Food shortages in a population are caused either by a lack of food or by difficulties in food distribution; it may be worsened by natural climate fluctuations and by extreme political conditions related to oppressive government or warfare. The conventional explanation until 1981 for the cause of famines was the Food availability decline (FAD) hypothesis. The assumption was that the central cause of all famines was a decline in food availability.[141] However, FAD could not explain why only a certain section of the population such as the agricultural laborer was affected by famines while others were insulated from famines.[142] Based on the studies of some recent famines, the decisive role of FAD has been questioned and it has been suggested that the causal mechanism for precipitating starvation includes many variables other than just decline of food availability. According to this view, famines are a result of entitlements, the theory being proposed is called the "failure of exchange entitlements" or FEE.[142] A person may own various commodities that can be exchanged in a market economy for the other commodities he or she needs. The exchange can happen via trading or production or through a combination of the two. These entitlements are called trade-based or production-based entitlements. Per this proposed view, famines are precipitated due to a breakdown in the ability of the person to exchange his entitlements.[142] An example of famines due to FEE is the inability of an agricultural laborer to exchange his primary entitlement, i.e., labor for rice when his employment became erratic or was completely eliminated.[142]		According to the Physicians for Social Responsibility (PSR), global climate change is additionally challenging the Earth's ability to produce food, potentially leading to famine.[143]		Some elements make a particular region more vulnerable to famine. These include poverty, population growth,[144] an inappropriate social infrastructure, a suppressive political regime, and a weak or under-prepared government.[145]		According to a FEWSNET report, "Famines are not natural phenomena, they are catastrophic political failures."[146]		Many famines are caused by imbalance of food production compared to the large populations of countries whose population exceeds the regional carrying capacity.[citation needed] Historically, famines have occurred from agricultural problems such as drought, crop failure, or pestilence. Changing weather patterns, the ineffectiveness of medieval governments in dealing with crises, wars, and epidemic diseases such as the Black Death helped to cause hundreds of famines in Europe during the Middle Ages, including 95 in Britain and 75 in France.[147] In France, the Hundred Years' War, crop failures and epidemics reduced the population by two-thirds.[148]		The failure of a harvest or change in conditions, such as drought, can create a situation whereby large numbers of people continue to live where the carrying capacity of the land has temporarily dropped radically. Famine is often associated with subsistence agriculture. The total absence of agriculture in an economically strong area does not cause famine; Arizona and other wealthy regions import the vast majority of their food, since such regions produce sufficient economic goods for trade.		Famines have also been caused by volcanism. The 1815 eruption of the Mount Tambora volcano in Indonesia caused crop failures and famines worldwide and caused the worst famine of the 19th century. The current consensus of the scientific community is that the aerosols and dust released into the upper atmosphere causes cooler temperatures by preventing the sun's energy from reaching the ground. The same mechanism is theorized to be caused by very large meteorite impacts to the extent of causing mass extinctions.		In certain cases, such as the Great Leap Forward in China (which produced the largest famine in absolute numbers), North Korea in the mid-1990s, or Zimbabwe in the early-2000s, famine can occur because of government policy.		In 1932, under the rule of the USSR, Ukraine experienced one of its largest famines when between 2.4 and 7.5 million peasants died as a result of a state sponsored famine. It was termed the Holodomor, suggesting that it was a deliberate campaign of repression designed to eliminate resistance to collectivization. Forced grain quotas imposed upon the rural peasants and a brutal reign of terror contributed to the widespread famine. The Soviet government continued to deny the problem and it did not provide aid to the victims nor did it accept foreign aid.		In 1958 in China, Mao Zedong's Communist Government launched the Great Leap Forward campaign, aimed at rapidly industrializing the country.[149] The government forcibly took control of agriculture. Barely enough grain was left for the peasants, and starvation occurred in many rural areas. Exportation of grain continued despite the famine and the government attempted to conceal it. While the famine is attributed to unintended consequences, it is believed that the government refused to acknowledge the problem, thereby further contributing to the deaths. In many instances, peasants were persecuted. Between 20 and 45 million people perished in this famine, making it one of the deadliest famines to date.[150]		Malawi ended its famine by subsidizing farmers despite the strictures imposed by the World Bank.[151] During the 1973 Wollo Famine in Ethiopia, food was shipped out of Wollo to the capital city of Addis Ababa, where it could command higher prices. In the late-1970s and early-1980s, residents of the dictatorships of Ethiopia and Sudan suffered massive famines, but the democracy of Botswana avoided them, despite also suffering a severe drop in national food production. In Somalia, famine occurred because of a failed state.		Long term measures to improve food security, include investment in modern agriculture techniques, such as fertilizers and irrigation,[152] but can also include strategic national food storage.		World Bank strictures restrict government subsidies for farmers, and increasing use of fertilizers is opposed by some environmental groups because of its unintended consequences: adverse effects on water supplies and habitat.[151][153]		The effort to bring modern agricultural techniques found in the Western world, such as nitrogen fertilizers and pesticides, to Asia, called the Green Revolution, resulted in decreases in malnutrition similar to those seen earlier in Western nations. This was possible because of existing infrastructure and institutions that are in short supply in Africa, such as a system of roads or public seed companies that made seeds available.[154] Supporting farmers in areas of food insecurity, through such measures as free or subsidized fertilizers and seeds, increases food harvest and reduces food prices.[151][155]		The World Bank and some rich nations press nations that depend on them for aid to cut back or eliminate subsidized agricultural inputs such as fertilizer, in the name of privatization even as the United States and Europe extensively subsidized their own farmers.[156]		There is a growing realization among aid groups that giving cash or cash vouchers instead of food is a cheaper, faster, and more efficient way to deliver help to the hungry, particularly in areas where food is available but unaffordable.[157] The United Nations' World Food Program (WFP), the biggest non-governmental distributor of food, announced that it will begin distributing cash and vouchers instead of food in some areas, which Josette Sheeran, the WFP's executive director, described as a "revolution" in food aid.[157][158] The aid agency Concern Worldwide is piloting a method through a mobile phone operator, Safaricom, which runs a money transfer program that allows cash to be sent from one part of the country to another.[157]		However, for people in a drought living a long way from and with limited access to markets, delivering food may be the most appropriate way to help.[157] Fred Cuny stated that "the chances of saving lives at the outset of a relief operation are greatly reduced when food is imported. By the time it arrives in the country and gets to people, many will have died."[159] US Law, which requires buying food at home rather than where the hungry live, is inefficient because approximately half of what is spent goes for transport.[160] Fred Cuny further pointed out "studies of every recent famine have shown that food was available in-country—though not always in the immediate food deficit area" and "even though by local standards the prices are too high for the poor to purchase it, it would usually be cheaper for a donor to buy the hoarded food at the inflated price than to import it from abroad."[161]		Deficient micronutrients can be provided through fortifying foods.[162] Fortifying foods such as peanut butter sachets (see Plumpy'Nut) have revolutionized emergency feeding in humanitarian emergencies because they can be eaten directly from the packet, do not require refrigeration or mixing with scarce clean water, can be stored for years and, vitally, can be absorbed by extremely ill children.[163]		WHO and other sources recommend that malnourished children—and adults who also have diarrhea—drink rehydration solution, and continue to eat, in addition to antibiotics, and zinc supplements.[164][165][166] There is a special oral rehydration solution called ReSoMal which has less sodium and more potassium than standard solution. However, if the diarrhea is severe, the standard solution is preferable as the person needs the extra sodium.[165] Obviously, this is a judgment call best made by a physician, and using either solution is better than doing nothing. Zinc supplements often can help reduce the duration and severity of diarrhea, and Vitamin A can also be helpful.[167] The World Health Organization underlines the importance of a person with diarrhea continuing to eat, with a 2005 publication for physicians stating: "Food should never be withheld and the child's usual foods should not be diluted. Breastfeeding should always be continued."[164]		Ethiopia has been pioneering a program that has now become part of the World Bank's prescribed recipe for coping with a food crisis and had been seen by aid organizations as a model of how to best help hungry nations. Through the country's main food assistance program, the Productive Safety Net Program, Ethiopia has been giving rural residents who are chronically short of food, a chance to work for food or cash. Foreign aid organizations like the World Food Program were then able to buy food locally from surplus areas to distribute in areas with a shortage of food.[168]		The Green Revolution was widely viewed as an answer to famine in the 1970s and 1980s. Between 1950 and 1984, hybrid strains of high-yielding crops transformed agriculture around the globe and world grain production increased by 250%.[169] Some[who?] criticize the process, stating that these new high-yielding crops require more chemical fertilizers and pesticides, which can harm the environment.[citation needed] Although these high-yielding crops make it technically possible to feed more people, there are indications that regional food production has peaked in many world sectors, due to certain strategies associated with intensive agriculture such as groundwater overdrafting and overuse of pesticides and other agricultural chemicals.		In modern times, local and political governments and non-governmental organizations that deliver famine relief have limited resources with which to address the multiple situations of food insecurity that are occurring simultaneously. Various methods of categorizing the gradations of food security have thus been used in order to most efficiently allocate food relief. One of the earliest were the Indian Famine Codes devised by the British in the 1880s. The Codes listed three stages of food insecurity: near-scarcity, scarcity and famine, and were highly influential in the creation of subsequent famine warning or measurement systems. The early warning system developed to monitor the region inhabited by the Turkana people in northern Kenya also has three levels, but links each stage to a pre-planned response to mitigate the crisis and prevent its deterioration		The experiences of famine relief organizations throughout the world over the 1980s and 1990s resulted in at least two major developments: the "livelihoods approach" and the increased use of nutrition indicators to determine the severity of a crisis. Individuals and groups in food stressful situations will attempt to cope by rationing consumption, finding alternative means to supplement income, etc., before taking desperate measures, such as selling off plots of agricultural land. When all means of self-support are exhausted, the affected population begins to migrate in search of food or fall victim to outright mass starvation. Famine may thus be viewed partially as a social phenomenon, involving markets, the price of food, and social support structures. A second lesson drawn was the increased use of rapid nutrition assessments, in particular of children, to give a quantitative measure of the famine's severity.		Since 2003, many of the most important organizations in famine relief, such as the World Food Programme and the U.S. Agency for International Development, have adopted a five-level scale measuring intensity and magnitude. The intensity scale uses both livelihoods' measures and measurements of mortality and child malnutrition to categorize a situation as food secure, food insecure, food crisis, famine, severe famine, and extreme famine. The number of deaths determines the magnitude designation, with under 1000 fatalities defining a "minor famine" and a "catastrophic famine" resulting in over 1,000,000 deaths.		Famine personified as an allegory is found in some cultures, e.g. one of the Four Horsemen of the Apocalypse in Christian tradition, the fear gorta of Irish folklore, or the Wendigo of Algonquian tradition.		
A dastarkhan (Kazakh: дастарқан; [dɑstɑrqɑ́n], Uzbek: dasturxon dasturxon [dasturxɒ́n], Dari: dastarkhawan, Urdu: دسترخوان‎, Pashto: دسترخوان‎, Tajik: дасторхон, dastarkhān, Azerbaijani: dəstərxan, [dɑstorqón], Kyrgyz: дасторкон dastorqon ) is the name used across Central Asia[1][2][3] to the traditional space where food is eaten. The term may refer to the tablecloth which is spread on the ground, floor, or table and is used as a sanitary surface for food, but it is also used more broadly to refer to the entire meal setting. It is part of traditional Central Asian cuisine.[4][5] The term was introduced in South Asia by the Turkic invaders and conquerors from the Central Asia.		The word Dastarkhan is a Persian word meaning "tablecloth" or "great spread".[6][7]		The food placed on a dastarkhan ranges from simple tea and bread (for small meals shared by a family) to various salads, nuts, candies, sorpa, and meat set out for a feast.		A large cultural significance is placed on the dastarkhan among different groups, and as such, various traditions, customs, values, and prohibitions surround the use of the dastarkhan. These include, for example, that there is usually a dedicated drink (usually tea) pourer, or that one must never step on the dastarkhan.[citation needed]		
Finger food is food meant to be eaten directly using the hands, in contrast to food eaten with a knife and fork, spoon, chopsticks, or other utensils.[1] In some cultures, food is almost always eaten with the hands; for example, Ethiopian cuisine is eaten by rolling various dishes up in injera bread.[2] In the South Asian subcontinent, food is traditionally always eaten with hands. Foods considered street foods are frequently, though not exclusively, finger foods.						In the western world, finger foods are often either appetizers (hors d'oeuvres) or entree/main course items. In the Western world, examples of generally accepted finger food are miniature meat pies, sausage rolls, sausages on sticks, cheese and olives on sticks, chicken drumsticks or wings, spring rolls, miniature quiches, samosas, sandwiches, Merenda or other such based foods, such as pitas or items in buns, bhajjis, potato wedges, vol au vents, several other such small items and risotto balls (arancini). Other well-known foods that are generally eaten with the hands include burger, pizza, hot dogs, fruit and bread.[3] Dessert items such as cookies, pastries, ice cream in cones, or ice pops are often eaten with the hands but are not, in common parlance, considered finger foods. In East Asia, foods like pancakes or flatbreads (bing 饼) and street foods such as chuan (串, also pronounced chuan) are often eaten with the hands.		In many western countries there are catering businesses that supply finger foods for events such as weddings, engagements, birthdays and other milestone celebrations. For weddings, in particular, finger foods are becoming more popular because they are less expensive and offer more flexibility with menu choices.[citation needed] Gourmet hors d'oeuvres such as quiches, pâté, caviar, and tea sandwiches are suitable for a formal event, whereas more familiar food such as sliced fruits, deli trays, crackers, and cookies are preferred at more casual celebrations.		
Convenience food, or tertiary processed food, is food that is commercially prepared (often through processing) to optimise ease of consumption. Such food is usually ready to eat without further preparation. It may also be easily portable, have a long shelf life, or offer a combination of such convenient traits. Although restaurant meals meet this definition, the term is seldom applied to them. Convenience foods include ready-to-eat dry products, frozen foods such as TV dinners, shelf-stable foods, prepared mixes such as cake mix, and snack foods.		Bread, cheese, salted food and other prepared foods have been sold for thousands of years. Other kinds were developed with improvements in food technology. Types of convenience foods can vary by country and geographic region. Some convenience foods have received criticism due to concerns about nutritional content and how their packaging may increase solid waste in landfills. Various methods are used to reduce the unhealthy aspects of commercially produced food and fight childhood obesity.		Convenience food is commercially prepared for ease of consumption.[1] Products designated as convenience food are often sold as hot, ready-to-eat dishes; as room-temperature, shelf-stable products; or as refrigerated or frozen food products that require minimal preparation (typically just heating)[2] Convenience foods have also been described as foods that have been created to "make them more appealing to the consumer."[3] Convenience foods and restaurants are similar in that they save time.[4] They differ in that restaurant food is ready to eat, whilst convenience food usually requires rudimentary preparation. Both typically cost more money and less time compared to home cooking from scratch.[4]						Throughout history, people have bought food from bakeries, creameries, butcher shops and other commercial processors to save time and effort. The Aztec people of Central Mexico utilized several convenience foods that required only adding water for preparation, which were used by travelers.[5] Cornmeal that was ground and dried, referred to as pinolli, was used by travelers as a convenience food in this manner.[5]		Canned food was developed in the 19th century, primarily for military use, and became more popular during World War I. The expansion of canning depended significantly upon the development of a machine for producing large quantities of cans very cheaply. Before the 1850s, making a can for food required a skilled tinsmith; afterwards, an unskilled laborer, operating a can-making machine, could produce 15 times as many cans each day.[6]		One of the earliest industrial-scale processed foods was meatpacking. After the invention of a system of refrigerator cars in 1878, meat could be raised, slaughtered, and butchered hundreds (later thousands) of miles or kilometers away from the consumer.[6]		Experience in World War II contributed to the development of frozen foods and the frozen food industry.[7] Modern convenience food saw its beginnings in the United States during the period that began after World War II.[8] Many of these products had their origins in military-developed foods designed for storage longevity and ease of preparation in the battle field. Following the war, several commercial food companies had leftover manufacturing facilities, and some of these companies created new freeze-dried and canned foods for home use.[9] Like many product introductions, not all were successful—convenience food staples such as fish sticks and canned peaches were counterbalanced by failures such as ham sticks and cheeseburgers-in-a-can.[10]		As of the 2010s due to increased preference for fresh, "natural", whole, and organic food and health concerns the acceptability of processed food to consumers in the United States was dropping and the reputation of major packaged food brands had been damaged. Firms responded by offering "healthier" formulations and acquisition of brands with better reputations.[11]		Convenience foods can include products such as candy; beverages such as soft drinks, juices and milk; fast food; nuts, fruits and vegetables in fresh or preserved states; processed meats and cheeses; and canned products such as soups and pasta dishes. Additional convenience foods include frozen pizza,[12] chips[3] such as potato chips,[12] pretzels,[3] and cookies.[12]		These products are often sold in portion-controlled, single-serving packaging designed for portability.[13][14]		Gristmills have produced flour for baking for thousands of years. In more recent times flour has been sold with other ingredients mixed in, as have other products ready to cook. Packaged mixes are convenience foods[15] which typically require some preparation and cooking either in the oven or on the stove top.		Packaged baked goods mixes typically use chemical leaveners (commonly referred to as baking powder[16]), for a quick, reliable result, avoiding the requirement for time-consuming skilled labor and the climate control needed for traditional yeast breads. These packaged mixes produce a type of quickbread.		Examples include cake mixes,[17] macaroni and cheese,[18] brownie mixes,[19] and gravy mixes.[20] Some packaged mixes may have a high saturated fat content.[21]		In 2007 it was noted in the book Australia's food & nutrition 2012 that a distinct increase in convenience food consumption had been occurring in Australia.[22]		In Japan, onigiri (rice balls) are a popular convenience food[23] that dates for millennia — by the Heian period these were established enough to be mentioned in literature.[24][25] Additional Japanese convenience foods include prepared tofu (bean curd),[26] prepared packages of seafood[27] and instant ramen noodles.[28]		Canned tuna packed in oil is a convenience food in the Solomon Islands.[29]		In Russia, frozen pelmeni, a type of meat dumplings, adopted from the Finno-Ugric Uralian peoples such as Komi, Mansi and Udmurts,[30] are known from at least the 18th century, and industrially produced and prepacked pelmeni are a staple of the supermarket freezer sections.		In Western Africa, processed cassava flour that has been grated and dried is a popular convenience food.[31]		In some instances, retail sales of convenience foods may provide higher profit margins for food retailers compared to the profits attained from sales of the individual ingredients that are present in the convenience foods.[32]		A survey in 1984 attributed over one-third of funds spent by consumers for food in Great Britain to be for convenience food purchases.[33]		Several groups have cited the environmental harm of single serve packaging due to the increased usage of plastics that contributes to solid waste in landfills.[34][35] Due to concerns about obesity and other health problems, some health organizations have criticized the high fat, sugar, salt, food preservatives and food additives that are present in some convenience foods.[13]		In most developed countries, 80% of consumed salt comes from industry-prepared food (5% come from natural salt; 15% comes from salt added during cooking or eating).[36] Health effects of salt concentrate on sodium and depend in part on how much is consumed. A single serving of many convenience foods contains a significant portion of the recommended daily allowance of sodium. Manufacturers are concerned that if the taste of their product is not optimized with salt, it will not sell as well as competing products. Tests have shown that some popular packaged foods depend on significant amounts of salt for their palatability.[37]		In response to the issues surrounding the healthfulness of convenience and restaurant foods, an initiative in the United States, spearheaded by first lady Michelle Obama and her Let's Move! campaign, to reduce the unhealthy aspects of commercially produced food and fight childhood obesity, was unveiled by the White House in February 2010. Mrs. Obama has pushed the industry to cut back on sugars and salts found in many convenience foods, encouraging self-regulation over government intervention through laws and regulations.[38] Despite Mrs. Obama's stated preference on self-regulation, the Food and Drug Administration announced that it was looking into quantifying the guidelines into law while other groups and municipalities are seeking to add other preventative measures such as target taxes and levies onto these products.[39][40]		In response to the attention, in April 2010 a coalition of sixteen manufactures all agreed to reduce salt levels in foods sold in the United States under a program based on a similar effort in the United Kingdom.[39] However, the initiative has met with resistance from some manufacturers, who claim that processed foods require the current high levels of salt to remain appetizing and to mask undesirable effects of food processing such as "warmed-over flavor".[37] The coalition expanded its mission in May 2010 by announcing that it intends to reduce the amount of calories in foods. By introducing lower calorie foods, changing product recipes and reducing portion sizes, the coalition stated that it expected to reduce the caloric content of foods by more than 1.5 trillion calories in total by 2012.[40]		
Money management is the process of managing money which includes expense tracking, investment, budgeting, banking and taxes. It is also called investment management.		Money management is a strategic technique employed to make money yield the highest interest-yielding value for any amount spent. Spending money to satisfy cravings (regardless of whether they can justifiably be included in a budget) is a natural human phenomenon. The idea of money management techniques has been developed to reduce the amount that individuals, firms and institutions spend on items which add no significant value to their living standards, long-term portfolios and assets. Warren Buffett, in one of his documentaries, admonished prospective investors to embrace his highly esteemed "frugality" ideology. This involves making every financial transaction worth the expense:		1. avoid any expense that appeals to vanity or snobbery 2. always go for the most cost-effective alternative (establishing small quality-variance benchmarks, if any) 3. favor expenditures on interest bearing items over all others 4. establish the expected benefits of every desired expenditure using the canon of plus/minus/nil to standard of living value system.		These techniques are investment-boosting and portfolio-multiplying. There are certain companies as well that offer services, provide counselling and different models for managing money. These are designed to manage assets and make them grow. [1]						Money management is used in investment management and deals with the question of how much risk a decision maker should take in situations where uncertainty is present. More precisely what percentage or what part of the decision maker's wealth should be put into risk in order to maximize the decision maker's utility function.[2]		Money management gives practical advice among others for gambling and for stock trading as well.[2]		Money management can mean gaining greater control over outgoings and incomings, both in personal and business perspective. Greater money management can be achieved by establishing budgets and analyzing costs and income etc.		In stock and futures trading, money management plays an important role in every success of a trading system. This is closely related with trading expectancy:		“Expectancy” which is the average amount you can expect to win or lose per dollar at risk. Mathematically:		Expectancy = (Trading system Winning probability * Average Win) – (Trading system losing probability * Average Loss)		So for example even if a trading system has 60% losing probability and only 40% winning of all trades, using money management a trader can set his average win substantially higher compared to his average loss in order to produce a profitable trading system. If he set his average win at around $400 per trade (this can be done using proper exit strategy) and managing/limiting the losses to around $100 per trade; the expectancy is around:		Expectancy = (Trading system Winning probability * Average Win) – (Trading system losing probability * Average Loss) Expectancy = (0.4 x 400) - (0.6 x 100)=$160 - $60 = $100 net average profit per trade (of course commissions are not included in the computations).		Therefore the key to successful money management is maximizing every winning trades and minimizing losses (regardless whether you have winning or losing trading system, such as %Loss probability > %Win probability).[3]		Ethical or religious principles may be used to determine or guide the way in which money is invested. Christians tend to follow the Biblical scripture. Several religions follow Mosaic law which proscribed the charging of interest. The Quakers forbade involvement in the slave trade and so started the concept of ethical investment.		Balsara, Nauzer J. (1992). Money Management Strategies for Futures Traders. Wiley Finance. ISBN 0-471-52215-5. Retrieved 2006-10-29. 		Stephen Petrivy, xBinOp.com (2016). 5 Types of Successful Money Management in Trading.		money tips		
Tea culture is defined by the way tea is made and consumed, by the way the people interact with tea, and by the aesthetics surrounding tea drinking. It includes aspects of tea production, tea brewing, tea arts and ceremony, society, history, health, ethics, education, and communication and media issues.		Tea plays an important role in some countries. It is commonly consumed at social events, and many cultures have created intricate formal ceremonies for these events. Afternoon tea is a British custom with widespread appeal. Tea ceremonies, with their roots in the Chinese tea culture, differ among Asian countries, such as the Japanese or Korean versions. Tea may differ widely in preparation, such as in Tibet, where the beverage is commonly brewed with salt and butter. Tea may be drunk in small private gatherings (tea parties) or in public (tea houses designed for social interaction).		The British Empire spread its own interpretation of tea to its dominions and colonies, including regions that today comprise the states of Hong Kong, India, and Pakistan, which had pre-existing tea customs, as well as regions such as East Africa (modern day Kenya, Tanzania, and Uganda) and the Pacific (Australia and New Zealand) which did not have tea customs. The tea room is found in the US and UK.		Different regions favor different varieties of tea—black, green, or oolong—and use different flavourings, such as herbs, milk, or sugar. The temperature and strength of the tea likewise vary widely.						Bubble tea, pearl milk tea (Chinese: 珍珠奶茶; pinyin: zhēnzhū nǎichá), or boba milk tea (波霸奶茶; bōbà nǎichá) is a tea beverage mixture with milk which may include balls of tapioca. Originating in Taiwan, it is especially popular in Asia (Taiwan, People's Republic of China, Hong Kong, Thailand, South Korea, the Philippines, and Singapore) as well as Europe, Canada, and the United States. It is also known as black pearl tea or tapioca tea.		Due to the importance of tea in Chinese society and culture, tea houses can be found in most Chinese neighbourhoods and business districts. Chinese-style tea houses offer dozens of varieties of hot and cold tea concoctions. They also serve a variety of tea-friendly or tea-related snacks. Beginning in the late afternoon, the typical Chinese tea house quickly becomes packed with students and business people, and later at night plays host to insomniacs and night owls simply looking for a place to relax.		There are formal tea houses. They provide a range of Chinese and Japanese tea leaves, as well as tea making accoutrements and a better class of snack food. Finally there are tea vendors, who specialize in the sale of tea leaves, pots, and other related paraphernalia. Tea is an important item in Chinese culture and is mentioned in the Seven necessities of (Chinese) daily life.		In China, at least as early as the Tang Dynasty, tea was an object of connoisseurship; in the Song Dynasty formal tea-tasting parties were held, comparable to modern wine tastings. As much as in modern wine tastings, the proper vessel was important and much attention was paid to matching the tea to an aesthetically appealing serving vessel.		Historically there were two phases of tea drinking in China based on the form of tea that was produced and consumed, namely: tea bricks versus loose leaf tea.		Tea served before the Ming Dynasty was typically made from tea bricks. Upon harvesting, the tea leaves were either partially dried or were thoroughly dried and ground before being pressed into bricks. The pressing of Pu-erh is likely a vestige of this process. Tea bricks were also sometimes used as currency.[1] Serving the tea from tea bricks required multiple steps:		The ground and whisked teas used at that time called for dark and patterned bowls in which the texture of the tea powder suspension could be enjoyed. The best of these bowls, glazed in patterns with names like oil spot, partridge-feather, hare's fur, and tortoise shell, are highly valued today. The patterned holding bowl and tea mixture were often lauded in the period's poetry with phrases such as "partridge in swirling clouds" or "snow on hare's fur". Tea in this period was enjoyed more for its patterns and less for its flavour. The practice of using powdered tea can still be seen in the Japanese Tea ceremony or Chado.		After 1391, the Hongwu Emperor, the founder of the Ming Dynasty, decreed that tributes of tea to the court were to be changed from brick to loose-leaf form. The imperial decree quickly transformed the tea drinking habits of the people, changing from whisked teas to steeped teas. The arrival of the new method for preparing tea also required the creation or use of new vessels.		Teawares made with a special kind of purple clay (Zisha) from Yixing went on to develop during this period (Ming Dynasty). The structure of purple clay made it advantageous material with tiny and high density, preferred for heat preservation and perviousness. Simplicity and rusticity dominated the idea of purple clay teaware decoration art. It became soon the most popular method of performing Chinese tea ceremony, which often combines literature, calligraphy, painting and seal cutting in Chinese culture.		The loose-leaf tea and the purple clay teaware is still the preferred method of preparing tea in Chinese daily life.		The English-style tea has evolved into a new local style of drink, the Hong Kong-style milk tea, more often simply "milk tea", in Hong Kong by using evaporated milk instead of ordinary milk. It is popular at cha chaan tengs and fast food shops such as Café de Coral and Maxims Express. Traditional Chinese tea, including green tea, flower tea, jasmine tea, and Pu-erh tea, are also common, and are served at dim sum restaurant during yum cha.		Green tea's traditional role in Japanese society is as a drink for special guests and special occasions. Green tea is served in many companies during afternoon breaks. Japanese often buy sweets for their colleagues when on vacation or business trips. These snacks are usually enjoyed with green tea. Tea will also be prepared for visitors coming for meetings to companies and for guests visiting Japanese homes. A thermos full of green tea is a staple on family or school outings as an accompaniment to bento (box lunches). Families often bring along proper Japanese teacups to enhance the enjoyment of the traditional drink.		The strong cultural association the Japanese have with green tea has made it the most popular beverage to drink with traditional Japanese cuisine, such as sushi, sashimi, and tempura. At a restaurant, a cup of green tea is often served with meals at no extra charge, with as many refills as desired. The best traditional Japanese restaurants take as much care in choosing the tea they serve as in preparing the food itself.		Many Japanese are still taught the proper art of the centuries-old tea ceremony as well. Still, the Japanese now enjoy green tea processed using state of the art technology. Today, hand pressing—a method demonstrated to tourists—is taught only as a technique preserved as a part of the Japanese cultural tradition. Most of the ubiquitous vending machines also carry a wide selection of both hot and cold bottled teas. Oolong tea enjoys considerable popularity. Black tea, often with milk or lemon, is served ubiquitously in cafes, coffee shops, and restaurants.		Major tea-producing areas in Japan include Shizuoka Prefecture and the city of Uji in Kyoto Prefecture.		Other infusions bearing the name cha are barley tea (mugi-cha) which is popular as a cold drink in the summer, buckwheat tea (soba-cha), and hydrangea tea (ama-cha).		Myanmar (formerly Burma) is one of very few countries where tea is not only drunk but eaten as lahpet—pickled tea served with various accompaniments.[2][3] It is called lahpet so (tea wet) in contrast to lahpet chauk (tea dry) or akyan jauk (crude dry) with which green tea—yeinway jan or lahpet yeijan meaning plain or crude tea—is made. In the Shan State of Myanmar where most of the tea is grown, and also Kachin State, tea is dry-roasted in a pan before adding boiling water to make green tea.[2] It is the national drink in a predominantly Buddhist country with no national tipple other than the palm toddy. Tea sweetened with milk is known as lahpet yeijo made with acho jauk (sweet dry) or black tea and prepared the Indian way, brewed and sweetened with condensed milk. It is a very popular drink although the middle classes by and large appear to prefer coffee most of the time. It was introduced to Myanmar by Indian immigrants some of whom set up teashops known as kaka hsaing, later evolving to just lahpetyei hsaing (teashop).		Burma's street culture is basically a tea culture[3] as people, mostly men but also women and families, hang out in tea shops reading the paper or chatting away with friends, exchanging news, gossip and jokes, nursing cups of Indian tea served with a diverse range of snacks from cream cakes to Chinese fried breadsticks (youtiao) and steamed buns (baozi) to Indian naan bread and samosas. Green tea is customarily the first thing to be served free of charge as soon as a customer sits down at a table in all restaurants as well as teashops.		Pubs and clubs, unlike in the West, have remained a minority pursuit so far. Teashops are found from the smallest village to major cities in every neighbourhood up and down the country.[3] They are open from the crack of dawn for breakfast till late in the evening, and some are open 24 hours catering for long distance drivers and travellers. One of the most popular teashops in Yangon in the late 1970s was called Shwe Hleiga (Golden Stairs) by popular acclaim as it was just a pavement stall, with low tables and stools for the customers, at the bottom of a stairwell in downtown Yangon. Busy bus stops and terminals as well as markets have several teashops. Train journeys in Myanmar also feature hawkers who jump aboard with giant kettles of tea for thirsty passengers.		Lahpet (pickled tea) is served in one of two ways:		Butter, milk, and salt are added to brewed tea and churned to form a hot drink called Po cha (bod ja, where bod means Tibetan and ja tea) in Tibet. The concoction is sometimes called cha su mar, mainly in Kham, or Eastern Tibet. Traditionally, the drink is made with a domestic brick tea and yak's milk, then mixed in a churn for several minutes. Using a generic black tea, milk and butter, and shaking or blending work well too, although the unique taste of yak milk is difficult to replicate. (see recipe)		Tibet tea drinking has many rules. One such concerns an invitation to a house for tea. The host will first pour some highland barley wine. The guest must dip his finger in the wine and flick some away. This will be done three times to represent respect for the Buddha, Dharma, and Sangha. The cup will then be refilled two more times and on the last time it must be emptied or the host will be insulted. After this the host will present a gift of butter tea to the guest, who will accept it without touching the rim of the bowl. The guest will then pour a glass for himself, and must finish the glass or be seen as rude.		There are two main teas that go with the tea culture. The teas are butter tea and sweet milk tea. These two teas are only found in Tibet.[citation needed] Other teas that the Tibetans enjoy are boiled black teas. There are many tea shops in Tibet selling these teas, which travelers often take for their main hydration source.		Thai tea (also known as Thai iced tea) or "cha-yen" (Thai: ชาเย็น) when ordered in Thailand is a drink made from strongly-brewed red tea that usually contains added anise, red and yellow food colouring, and sometimes other spices as well. This tea is sweetened with sugar and condensed milk and served chilled. Evaporated or whole milk is generally poured over the tea and ice before serving—it is never mixed before serving—to add taste and creamy appearance. Locally, it is served in a traditional tall glass and when ordered take-out, it is poured over the crushed ice in a clear (or translucent) plastic bag. It can be made into a frappé at more westernised vendors.		It is popular in Southeast Asia and in many American restaurants that serve Thai or Vietnamese food, especially on the West Coast. Although Thai tea is not the same as bubble tea, a Southeast and East Asian beverage that contains large black pearls of tapioca starch, Thai tea with pearls is a popular flavour of bubble tea.		Green tea is also very popular in Thailand, spawning many variations such as barley green tea, rose green tea, lemon green tea, etc. Thai green tea, however, is not to be confused with traditional Japanese green tea. Thai green tea tends to be very heavily commercialized and the taste is sweeter and easier to appreciate than bitter variations.		Tea is cultivated extensively in the north of the country, making Vietnam one of the world's largest exporters. The word in the Vietnamese language is trà (pronounced cha/ja) or chè. It is served unsweetened and unaccompanied by milk, cream, or lemon.		Traditionally tea is frequently consumed as green tea (trà xanh). Variants of black tea (chè tàu) is also widely used although frequently scented with Jasminum sambac blossoms (chè nhài, trà lài). Huế is renowned for its tea scented with Nelumbo nucifera stamens (trà sen).		In Vietnamese restaurants, including eateries overseas, a complimentary pot of tea is usually served once the meal has been ordered, with refills free of charge.		One of the world's largest producers of tea, India is a country where tea is popular all over as a breakfast and evening drink. It is often served as masala chai with milk, sugar, and spices such as ginger, cardamom, black pepper and cinnamon. Almost all the tea consumed is black Indian tea, CTC variety. Usually tea leaves are boiled in water while making tea, and milk is added.[6]		Offering tea to visitors is the cultural norm in Indian homes, offices and places of business. Tea is often consumed at small roadside stands, where it is prepared by tea makers known as chai wallahs.[7]		There are three most famous regions in India to produce black teas- Darjeeling, Assam and Nilgiri. "Strong, heavy and fragrant" are 3 criteria for judging black tea. Darjeeling tea is known for its delicate aroma and light colour and is aptly termed as "the champagne of teas", which has high aroma and yellow or brown liquid after brewing. Assam tea is known for its robust taste and dark colour, and Nilgiri tea is dark, intensely aromatic and flavoured. Assam produces the largest quantity of Tea in India, mostly of the CTC variety, and is one of the biggest suppliers of major international brands such as Lipton and Tetley. The Tetley Brand, formerly British owned and one of the largest, is now owned by the Indian Tata Tea Limited company.		On April 21, 2012 the Deputy Chairman of Planning Commission (India), Montek Singh Ahluwalia, said that tea would be declared as national drink by April 2013.[8][9] Speaking on the occasion, former Assam Chief Minister Tarun Gogoi said a special package for the tea industry would be announced in the future to ensure its development.[10] The move was expected to boost the tea industry in the country, but in May 2013 the ministry of commerce decided not to declare a national drink for fear of disrupting the competing coffee industry.[11]		Tea is popular all over Pakistan and is referred to as chai (written as چائے). During British Rule tea became very popular in Lahore. Tea is usually consumed at breakfast, during lunch breaks at the workplace, and in the evening at home. Evening tea may be consumed with biscuits or cake. Guests are typically offered a choice between tea and soft drinks. It is common practice for homeowners to offer tea breaks to hired labour, and sometimes even provide them with tea during the breaks. Tea offered to labour is typically strong and has more sugar in it.		In Pakistan, both black and green teas are popular and are known locally as sabz chai and kahwah, respectively. The popular green tea called kahwah is often served after every meal in Khyber Pakhtunkhwa and the Pashtun belt of Balochistan. In the Kashmir region of Pakistan, Kashmiri chai or "noon chai," a pink, milky tea with pistachios and cardamom, is consumed primarily at special occasions, weddings, and during the winter months when it is sold in many kiosks. In Lahore and other cities of Punjab this Kashmiri Chai or Cha (as pronounced in Punjabi, Kosher as well as in many Chinese dialects ) is common drink in the Punjab, brought by ethnic Kashmiris in the 19th century. Traditionally, it is prepared with Himalayan rock salt, giving it its characteristic pink color. It is taken with Bakar Khani as well as Kashmiri Kulcha (namkeen / salty version of Khand Kulcha). Namkeen Chai or Noon / Loon Cha or commonly called Kashmri Chai and some times Sheer (milk ) Cha or sabz chai(Green Tea as the same tea are used for making Khahwa /Green Tea) is sold and seen Gowal Mandi kiosks with Salt for Kashmiri as well as with sugar and pistachios for Non- Kashmris or those who like it with sugar . In the northern Pakistan regions of Chitral and Gilgit-Baltistan, a salty buttered Tibetan style tea is consumed.		In Sri Lanka, usually black tea is served with milk and sugar, but the milk is always warmed. Tea is a hugely popular beverage among the Sri-Lankan people, and part of its land is surrounded by the many hills of tea plantations that spread for miles. Drinking tea has become part of the culture of Sri Lanka and it is customary to offer a cup of tea to guests. Many working Sri Lankans are used to having a mid-morning cup of tea and another in the afternoon. Black tea is sometimes consumed with ginger. In rural areas some people still have their tea with a piece of sweet jaggery		Tea is the national drink in Egypt, and holds a special position that even coffee cannot rival. In Egypt, tea is called "shai".[12] Tea packed and sold in Egypt is almost exclusively imported from Kenya and Sri Lanka. The Egyptian government considers tea a strategic crop and runs large tea plantations in Kenya. Green tea is a recent arrival to Egypt (only in the late 1990s did green tea become affordable) and is not as popular.		Egyptian tea comes in two varieties: Koshary and Saiidi. Koshary tea, popular in Lower (Northern) Egypt, is prepared using the traditional method of steeping black tea in boiled water and letting it set for a few minutes. It is almost always sweetened with cane sugar and is often flavored with fresh mint leaves. Adding milk is also common. Koshary tea is usually light, with less than a half teaspoonful per cup considered to be near the high end.		Saiidi tea is common in Upper (Southern) Egypt. It is prepared by boiling black tea with water for as long as 5 minutes over a strong flame. Saiidi tea is extremely heavy, with 2 teaspoonfuls per cup being the norm. It is sweetened with copious amounts of cane sugar (a necessity since the formula and method yield a very bitter tea). Saiidi tea is often black even in liquid form.		Besides true tea, herbal teas (or tisanes) are often served at the Egyptian teahouses, with ingredients ranging from mint to cinnamon and ginger to salep; many of these are ascribed medicinal qualities or health benefits in Egyptian folk medicine. Karkade, a tisane of hibiscus flowers, is a particularly popular beverage and is traditionally considered beneficial for the heart.		Tea found its way to Persia (Iran) through the Silk Road from India and soon became the national drink. The whole part of northern Iran along the shores of the Caspian Sea is suitable for the cultivation of tea. Especially in the Gilan province on the slopes of Alborz, large areas are under tea cultivation and millions of people work in the tea industry for their livelihood. That region covers a large part of Iran's need for tea. Iranians have one of the highest per capita rates of tea consumption in the world and from old times every street has had a Châikhâne (Tea House). Châikhânes are still an important social place. Iranians traditionally drink tea by pouring it into a saucer and putting a lump of rock sugar (qand) in the mouth before drinking the tea.		Libyan tea is a strong beverage, black or green, served in small glass cups with foam or froth topping the glass. it is usually sweetened with sugar and traditionally served in three rounds. mint or basil is used for flavoring and traditionally the last round is served with boiled peanuts or almonds.		Tea plays an important part in the island's culture. Tea drinking allows for socialising with it commonly being served to guests and in the workplace.		The Mauritian peoples usually consume black tea, often with milk and sugar. Mauritius is a producer of tea, initially on a small scale when the French introduced the plant into the island around 1765. It was under later British rule that the scale of tea cultivation increased.		Three major tea producers dominate the local market these are Bois Cheri, Chartreuse and Corson. The signature product is the vanilla-flavoured tea which is commonly bought and consumed on the island.		Morocco is considered the largest importer of green tea worldwide.[13]		Tea was introduced to Morocco in the 18th century through trade with Europe.		Morocco consumes green tea with mint rather than black tea. It has become part of the culture and is used widely at almost every meal. The Moroccan people even make tea performance a special culture in the flower country. Moroccan tea is commonly served with rich tea cookies, fresh green mint leaves, local "finger shape" brown sugar, and colorful tea glasses and pots. Drinking Moroccan tea is not only a luxury of tongue, but also the eyes.		In the Sahel region on the southern fringe of the Sahara, green gunpowder tea is prepared with little water and large amounts of sugar. By pouring the tea into the glasses and back, a foam builds on top of the tea. Sahelian tea is a social occasion and three infusions, the first one very bitter, the second in between and the last one rather sweet are taken in the course of several hours.		Tea is an important social beverage to Somali people. It is called shaah in the Somali language. Tea was first introduced to Somalis through ancient trade with the Arabs and Indians. In major Somali towns there are many tea shops and tea stalls around busy market areas. Somalis consume tea at any time of the day but primarily at breakfast, in the late afternoon, called Asariyo, and after or during supper. Any guest to a Somali household would be offered spiced Somali tea, known as Shaah Hawash, as soon as he or she arrives. The tea is spiced with cardamom, cloves and sometimes dry ginger and is usually served milky and sweet. Tea is usually preferred over coffee in Somalia, however some Somalis prefer coffee over tea.		Somalis usually drink tea with camel milk, but it's customary to serve black tea if it is to be consumed after a heavy meal. It is called Shaah Bigaysi.		As of 2016, Turkey tops the per capita tea consumption statistics at 6.96 pounds. [14]		Turkish tea or Çay is produced on the eastern Black Sea coast, which has a mild climate with high precipitation and fertile soil. Turkish tea is typically prepared using çaydanlık, an instrument especially designed for tea preparation. Water is brought to a boil in the larger lower kettle and then some of the water is used to fill the smaller kettle on top and steep several spoons of loose tea leaves, producing a very strong tea. When served, the remaining water is used to dilute the tea on an individual basis, giving each consumer the choice between strong ("koyu"/dark) or weak ("açık"/light). Tea is drunk from small glasses to enjoy it hot in addition to show its colour, with lumps of beetroot sugar.[15] To a lesser extent than in other Muslim countries, tea replaces both alcohol and coffee as the social beverage. Within Turkey the tea is usually known as Rize tea.		In 2004, Turkey produced 205,500 tonnes of tea (6.4% of the world's total tea production), which made it one of the largest tea markets in the world,[16] with 120,000 tons being consumed in Turkey, and the rest being exported.[17] In 2010 Turkey had the highest per capita consumption in the world at 2.7 kg.[18] As of 2013, the per-capita consumption of Turkish tea exceeds 10 cups per day and 13.8 kg per year.[19] Tea is grown mostly in Rize Province on the Black Sea coast.[20]		Specific tea culture has developed in the Czech Republic in recent years,[when?] including many styles of tearooms. Despite having the same name, they differ from British tearooms. Pure teas are usually prepared with respect to their country of origin, and good tea palaces may offer 80 teas from almost all tea-producing countries. Different tea rooms have also created blends and methods of preparation and serving.[citation needed]		The region of East Frisia is noted for its consumption of tea and its tea culture.[21] Strong blends of Assam tea, Ceylon and Darjeeling (East-Frisian Blend) are served whenever there are visitors to an East Frisian home or other gathering, as well as with breakfast, mid-afternoon, and mid-evening.		The traditional preparation is as follows: A Kluntje, a white rock candy sugar that melts slowly, is added to the empty cup (allowing multiple cups to be sweetened) then tea is poured over the Kluntje. A heavy cream "cloud" ("Wölkje"—a diminutive of 'cloud' in Frisian)[22] is added to the tea "water", the sugar represents "land". It is served without a spoon and traditionally drunk unstirred, i. e. in three tiers: In the beginning one predominantly tastes the cream, then the tea and finally the sweet taste of kluntje at the bottom of the cup. Stirring the tea would blend all three tiers into one and spoil the traditional tea savouring. The tea is generally served with small cookies during the week and cakes during special occasions or on weekends as a special treat. The tea is said to cure headaches, stomach problems, and stress, among many other ailments. The tea set is commonly decorated with an East Friesian Rose design.[23] As a guest, it is considered impolite to drink fewer than three cups of tea. Placing your cup upside down on the saucer or your spoon in the cup signals that you are finished and want no more tea.		Although less visible than in the Czech Republic, tea culture exists in Slovakia. Tea rooms are considered an underground environment by many, but they continue to pop up almost in every middle-sized town. These tea rooms are appreciated for offering quiet environments with pleasant music. More importantly, they are usually non-smoking, unlike most pubs and cafés.[citation needed]		The podstakannik ('подстаканник'), or tea glass holder (literally "thing under the glass"), is a part of Russian tea tradition. A Russian tea glass-holder is a traditional way of serving and drinking tea in Russia, Ukraine, Belarus, other CIS and ex-USSR countries. Expensive podstakanniks are made from silver, classic series are made mostly from nickel silver, cupronickel, and other alloys with nickel, silver or gold plating. In Russia, it is customary to drink tea brewed separately in a teapot and diluted with freshly boiled water ('pair-of-teapots tea', 'чай парой чайников'). Traditionally, the tea is very strong, its strength often indicating the hosts' degree of hospitality. The traditional implement for boiling water for tea used to be the samovar (and sometimes it still is, though usually electric). Tea is a family event, and is usually served after each meal with sugar (one to three teaspoonfuls per cup) and lemon (but without milk), and an assortment of jams, pastries and confections. Black tea is commonly used, with green tea gaining popularity as a more healthy, more "Oriental" alternative. Teabags are not used in the traditional Russian tea ceremony, only loose, large-leaf black tea.		In Russian prisons, where alcohol and drugs are prohibited, inmates often brew very strong tea known as 'chifir', in order to experience its mood-altering properties.[24]		While France is well known for its coffee drinking, afternoon tea has long been a social habit of the upper middle class, famously illustrated, for example, by Marcel Proust's novels. Mariage Frères is a famous high-end tea shop from Paris, active since 1854. The French tea market is still only a fraction of the British one (a consumption of 250 grams per person a year compared to about 2 kilos in the UK),[25] but it has doubled from 1995 to 2005 and is still growing steadily.[26] Tea in France is of the black variety, but Asian green teas and fruit-flavoured teas are becoming increasingly popular. French people generally drink tea in the afternoon. It is often taken in salons de thé. Most people will add sugar to their tea (65%), then milk (25%), lemon (30%) or nothing (32%) are about equally popular.[27] Tea is generally served with some pastries, including a variety of not so sweet ones reserved for tea drinking, like the madeleine and the financier.		Ireland has, for a long time,[when?] been one of the biggest per capita consumers of tea in the world. Although broadly similar to tea culture in the United Kingdom, Irish tea culture has a number of distinguishing elements; for example, tea in Ireland is usually taken with milk or sugar and is slightly spicier and stronger than the traditional English Blend. Popular brands of tea sold in Ireland are Lyons, Barry's, and Bewley's.		Tea growing in Portugal takes place in the Azores, a group of islands located 1500 km west of Mainland Portugal. Portugal was the first to introduce the practice of drinking tea to Europe as well as the first European country to produce tea.[citation needed]		In 1750, terrains ranging from the fields of Capelas to those of Porto Formoso on the island of São Miguel were used for the first trial crops of tea. They delivered 10 kg of black tea and 8 kg of green tea. A century later, with the introduction of skilled workers from the Macau Region of China in 1883, production became significant and the culture expanded. Following the instructions of these workers, the species Jasminum grandiflorum and Malva vacciones were introduced to give 'nobility' to the tea aroma, though only the Jasminum was used.[28]		This tea is currently traded under the name of the processed compound, Gorreana, and is produced by independent families. No herbicides or pesticides are allowed in the growing process, and modern consumers associate the production with more recent organic teas. However, production standards concerning the plant itself and its cropping have not changed for the last 250 years.		The British are one of the largest tea consumers in the world, with each person consuming on average 1.9 kg per year.[29] Tea is usually black tea served with milk and sometimes with sugar. Strong tea served with lots of milk and often two teaspoons of sugar, usually in a mug, is commonly referred to as builder's tea for its association with builders and more broadly with the working class. Much of the time in the United Kingdom, tea drinking is not the delicate, refined cultural expression that the rest of the world imagines—a cup (or commonly a mug) of tea is something drunk frequently throughout the day. This is not to say that the British do not have a more formal tea ceremony, but tea breaks are an essential part of the working day. The term is often shortened to 'tea', essentially indicating a break. This term was exported to the game of cricket and consequently to most other countries of the former British Empire.		The popularity of tea dates back to the 19th century when India was part of the British Empire, and British interests controlled tea production in the subcontinent. It was, however, first introduced in the UK by the Portuguese Catherine of Braganza, queen consort of Charles II in the 1660s and 1670s. As tea spread throughout the United Kingdom and through the social classes, tea gardens and tea dances developed. These would include watching fireworks or a dinner party and dance, concluding with an evening tea. The tea gardens lost value after World War II but tea dances are still held today in the UK.[citation needed]		Some scholars suggest that tea played a role in the Industrial Revolution. Afternoon tea possibly became a way to increase the number of hours labourers could work in factories; the stimulants in the tea, accompanied by sugary snacks, would give workers energy to finish out the day's work. Further, tea helped alleviate some of the consequences of the urbanisation that accompanied the industrial revolution: drinking tea required boiling one's water, thereby killing water-borne diseases like dysentery, cholera, and typhoid.[30]		In the United Kingdom tea is not only the name of the beverage, but also the name of a meal. The kind of meal that a person means depends very much on their social background and where they live. The differentiation in usage between dinner, lunch, and tea is one of the classic social markers of British English (see U and non-U English) and is discussed more fully at the article on tea as a meal. Briefly, afternoon tea (one example of which is the cream tea) is sweeter and earlier, while the high tea is the final meal of the day.		Afternoon tea and its variants are the best known "tea ceremony" in the Commonwealth countries, available in homes and commercial establishments. In some varieties of English, "tea" refers to a savoury meal, see Australian usages of the term. Taiwanese bubble tea, known locally as pearl milk tea, has become widely popular in urban Australia, with multiple chains in every major city.		In Canada, various types of tea are used by many different indigenous tribes as healing and ceremonial medicines. For example, Objibwe and Cree tribes in Ontario use Cedar Tea during sweat lodge ceremonies to cleanse and nourish their bodies. When European settlers arrived on North American shores, it was the indigenous people that taught them to make pine needle tea to help cure their scurvy; pine needles are a great source of vitamin C.		In the United States, tea can typically be served at all meals as an alternative to coffee, when served hot, or soft drinks, when served iced. Tea is also consumed throughout the day as a beverage. Afternoon tea, the meal done in the English tradition, is rarely served in the United States, although it remains romanticized by small children; it is usually reserved for special occasions like tea parties.[citation needed]		Rather than drinking tea hot, many Americans prefer tea served with ice. In fact, in the United States, about 80% of the tea consumed is served cold, or "iced".[citation needed] Iced tea has become an iconic symbol of the Southern United States and Southern hospitality, often appearing alongside summer barbecue cooking or grilled foods.[citation needed] Iced tea is often made as sweet tea, which is simply iced tea with copious amounts of sugar or sweetener.[31]		Iced tea can be purchased like soda, in canned or bottled form at vending machines and convenience stores. This pre-made tea is usually sweetened. Sometimes some other flavorings, such as lemon or raspberry, are added. Many restaurants dispense iced tea brewed throughout the day from upright containers.[citation needed]		Decaffeinated tea is widely available in the United States, for those who wish to reduce the physiological effects of caffeine.[citation needed]		Before World War II, the US preference for tea was equally split between green tea and black tea, 40% and 40%, with the remaining 20% preferring oolong tea.[citation needed] The war cut off the United States from its primary sources of green tea, China and Japan, leaving it with tea almost exclusively from British-controlled India, which produced black tea. After the war, nearly 99% of tea consumed was black tea. Green, oolong, and white teas have recently[when?] become more popular again, and are often touted as health foods.[citation needed]		In the past 15 years[when?] fast food coffee chains have made a huge impact on how Americans are exposed to herbal and exotic teas. Once considered a rarity, chai, based on Indian masala chai, has actually become a popular option for people who might drink a caffè latte. Although not as commercialized, Taiwanese-style Bubble tea has also become popular in the United States in recent years,[when?] often served in small local cafes in the same style as many coffee drinks.[citation needed]		Brazilian tea culture has its origins with the infused beverages, or chás (Portuguese pronunciation: [ˈʃas]), made by the indigenous cultures of the Amazon region. It has evolved since the Portuguese colonial period to include imported varieties and tea-drinking customs. There is a folk knowledge in Brazil which says that Brazilians, mainly the urban ones, have a greater taste for using sugar in teas than in other cultures due to the lack of habit to unsweetened drinks.		
Appetite is the desire to eat food, sometimes due to hunger. Appealing foods can stimulate appetite even when hunger is absent. Appetite exists in all higher life-forms, and serves to regulate adequate energy intake to maintain metabolic needs. It is regulated by a close interplay between the digestive tract, adipose tissue and the brain. Appetite has a relationship with every individual's behavior. Appetitive and consummatory behaviours are the only processes that involve energy intake, whereas all other behaviours affect the release of energy. When stressed, appetite levels may increase and result in an increase of food intake. Decreased desire to eat is termed anorexia, while polyphagia (or "hyperphagia") is increased eating. Dysregulation of appetite contributes to anorexia nervosa, bulimia nervosa, cachexia, overeating, and binge eating disorder.						Cannon and Washburn (1912) proposed that eating begins when we have an empty stomach. They suggested that the walls of an empty stomach rub against each other to produce what are commonly called "hunger pangs". Some skeptics called Cannon's explanation of hunger "the rumble theory". However, observations of surgical patients indicated that there was more to the onset of eating than hunger pangs. Removal of the stomach did not abolish hunger pangs, and these patients reported the same feelings of hunger and satiety that they had experienced before surgery (Inglefinger, 1944). (The patients' stomachs had been removed because of cancer or large ulcers, and their esophagi had been attached directly to their small intestines). Although the patients ate small frequent meals because they had no stomachs to hold food, their reports of feelings of hunger and their total food intake were essentially normal.		Depletion of the body's store of nutrients is a more likely cause of hunger. The primary fuels for the cells of our body are glucose (a simple sugar) and fatty acids (compounds produced by the breakdown of fats). If the digestive system contains food, these nutrients are absorbed in the blood and nourish our cells. But the digestive tract is sometimes empty; in fact, it is empty when we wake up every morning. There must be a reservoir that stores nutrients to keep the cells of the body nourished when the gut is empty. Indeed, there are two reservoirs: a short-term reservoir and a long-term reservoir. The short-term reservoir stores carbohydrates, and the long-term reservoir stores fat.		A number of variables have been found to relate to appetite sensation in individuals. The most influential of these is gender and age, with females experiencing greater appetite satisfaction than males and a decrease in appetite with age. Although BMI was not found to influence appetite, tobacco smokers and women ovulating experienced a lower appetite than their counterparts.[1]		The regulation of appetite (the appestat) has been the subject of much research in the last decade[update]. Breakthroughs included the discovery, in 1994, of leptin, a hormone produced by the adipose tissue that appeared to provide negative feedback. Leptin is a peptide hormone that affects homeostasis and immune responses.[2] Lowering food intake can lower leptin levels in the body, while increasing the intake of food can raise leptin levels. Later studies showed that appetite regulation is an immensely complex process involving the gastrointestinal tract, many hormones, and both the central and autonomic nervous systems.[2] The circulating gut hormones that regulate many pathways in the body result in appetite stimulation.[3]		The hypothalamus, a part of the brain, is the main regulatory organ for the human appetite. The neurons that regulate appetite appear to be mainly serotonergic, although neuropeptide Y (NPY) and Agouti-related peptide (AGRP) also play a vital role. Hypothalamocortical and hypothalamolimbic projections contribute to the awareness of hunger, and the somatic processes controlled by the hypothalamus include vagal tone (the activity of the parasympathetic autonomic nervous system), stimulation of the thyroid (thyroxine regulates the metabolic rate), the hypothalamic-pituitary-adrenal axis and a large number of other mechanisms. Opioid receptor-related processes in the nucleus accumbens and ventral pallidum affect the palatability of foods.[4]		The nucleus accumbens (NAc) is the area of the brain that coordinates neurotransmitter, opioid and endocannabinoid signals to control feeding behaviour. The few important signalling molecules inside the NAc shell modulate the motivation to eat and the affective reactions for food. These molecules include the DA, Ach, opioids and cannabinoids and their action receptors inside the brain, DA, muscarinic and MOR and CB1 receptors respectively.[5]		The hypothalamus senses external stimuli mainly through a number of hormones such as leptin, ghrelin, PYY 3-36, orexin and cholecystokinin; all modify the hypothalamic response. They are produced by the digestive tract and by adipose tissue (leptin). Systemic mediators, such as tumor necrosis factor-alpha (TNFα), interleukins 1 and 6 and corticotropin-releasing hormone (CRH) influence appetite negatively; this mechanism explains why ill people often eat less.		In addition, the biological clock (which is regulated by the hypothalamus) stimulates hunger. Processes from other cerebral loci, such as from the limbic system and the cerebral cortex, project on the hypothalamus and modify appetite. This explains why in clinical depression and stress, energy intake can change quite drastically.		A limited or excessive appetite is not necessarily pathological. Abnormal appetite could be defined as eating habits causing malnutrition and related conditions such as obesity and its related problems.		Both genetic and environmental factors may regulate appetite, and abnormalities in either may lead to abnormal appetite. Poor appetite (anorexia) can have numerous causes, but may be a result of physical (infectious, autoimmune or malignant disease) or psychological (stress, mental disorders) factors. Likewise, hyperphagia (excessive eating) may be a result of hormonal imbalances, mental disorders (e.g., depression) and others. Dyspepsia, also known as indigestion, can also affect appetite as one of its symptoms is feeling "overly full" soon after beginning a meal.[6] Taste and smell ("dysgeusia", bad taste) or the lack thereof may also effect appetite.[7]		Abnormal appetite may also be linked to genetics on a chromosomal scale. In the 1950s, the discovery of the Prader Willi Syndrome, a type of obesity, displayed a causation at a gene locus. Additionally, anorexia nervosa and bulimia nervosa are more commonly found in females than males – thus hinting at a possibility of a linkage to the X-chromosome.[8]		Dysregulation of appetite lies at the root of anorexia nervosa, bulimia nervosa, and binge eating disorder. Anorexia nervosa is an eating condition categorized by a penetrating fear of being fat and severe limiting of food consumption. Furthermore, anorexics might do excessive exercise. Individuals who have anorexia have high levels of ghrelin, a hormone that stimulates appetite, so the body is trying to cause hunger, but it is being suppressed by the person.[9] Binge eating disorder (commonly referred to as BED) is described as eating excessively (or uncontrollably) between periodic time intervals. The risk for BED can be present in children and most commonly manifests during adulthood. Studies suggest that the heritability of BED in adults is approximately 50%.[10] Similarly to bulimia some people may be involved in purging and binging. They might puke after food intake or take purgatives. However, the person may still believe they are overweight.[11]		Various hereditary forms of obesity have been traced to defects in hypothalamic signaling (such as the leptin receptor and the MC-4 receptor) or are still awaiting characterization – Prader-Willi syndrome – in addition, decreased response to satiety may promote development of obesity.[12] It has been found that ghrelin-reactive IgG immunoglobulins affect ghrelin's orexigenic response.[13]		Other than genetically-stimulated appetite abnormalities, there are physiological ones that do not require genes for activation. For example, ghrelin and leptin are released from the stomach and pancreas, respectively, into the blood stream at the signal of the hypothalamus. Ghrelin stimulates feelings of hunger, whereas leptin stimulates feelings of satisfaction from food.[14] Any changes in normal production levels of these two hormones can lead to obesity. Looking at leptin, the more cells present in a body, the more adipose tissues there are, and thus, the more leptin would be produced. This overproduction of leptin will cause the hypothalamus to become resistant to leptin and so, although the pancreas is producing leptin, the body will not understand that it should stop eating.[15] This will produce a perpetual cycle for those that are obese.		Eating issues such as "picky eating" affects about 25% of children, but among children with development disorders this number may be significantly higher, which in some cases may be related to the sounds, smells, and tastes (sensory processing disorder).[16]		Glycemic index has been thought to effect satiety; however, a study investigating the effect of satiety found that a high-glycemic food, potatoes, reduced appetite more than a high glycemic index food.[17]		Mechanisms controlling appetite are a potential target for weight loss drugs. Appetite control mechanisms seem to strongly counteract undereating, whereas they appear weak to control overeating. Early anorectics were fenfluramine and phentermine. A more recent addition is sibutramine which increases serotonin and noradrenaline levels in the central nervous system, but had to be withdrawn from the market when it was shown to have an adverse cardiovascular risk profile. Similarly, the appetite suppressant rimonabant (a cannabinoid receptor antagonist) had to be withdrawn when it was linked with worsening depression and increased risk of suicide. Recent reports on recombinant PYY 3-36 suggest that this agent may contribute to weight loss by suppressing appetite.		Given the epidemic proportions of obesity in the Western world, and the fact that it is increasing rapidly in some poorer countries, observers[who?] expect developments in this area to snowball in the near future. Dieting alone is ineffective in most obese adults – and even obese adults who successfully lose weight through dieting overwhelmingly put weight back on afterwards.		Weight loss and loss of appetite ("cachexia") is an effect of some diseases, and a side effect of some drugs. Progestagens such as medroxyprogesterone acetate (MPA) and megestrol acetate (MA) are approved as a treatment in Europe, along with corticosteroids for short-term use.[18] Direct ghrelin administration increases appetite as well.[19]		Rikkunshito, a traditional Japanese Kampo medicine, has been found to stimulate ghrelin and appetite.[20]		In rats, appetizers including a ginger or karpurvalli (Coleus aromaticus) beverage were found to improve food consumption.[21] A subsequent study in human volunteers found that, depending upon the concentration, karpurvalli decreased or increased leptin.[22]		Not only olfaction, There is some visual influence of appetite. Some research has, Red (de:Warme Farbe) sharpens appetite.[23] (regarding Hot food)		Actually, Many inexpensive restaurants (It includes Fast food) use red signs or interior furniture (walls, counters). Complementary colors Red and green combination can be often seen (an example: Tabasco sauce packaging).		Blue (de:Kalte Farbe) is the opposite influence.[24]		
A TV dinner (also called prepackaged meal, ready-made meal,[1] ready meal, frozen dinner, frozen meal and microwave meal) is a pre-packaged frozen or chilled meal that usually comes as an individual portion. It requires very little preparation and contains all the elements for a single-serving meal.		A TV dinner in the United States usually consists of a cut of meat, usually beef or chicken; a vegetable, such as peas, carrots, corn, or potatoes; and sometimes a dessert, such as a brownie or apple cobbler. The entrée could also be pasta or a common type of fish, such as Atlantic cod. Rice is a common side item. In Europe, items such as Indian or Chinese meals are common.		The term TV dinner is a genericized trademark originally used for a brand of packaged meal developed in 1953 by C.A. Swanson & Sons (the name in full was TV Brand Frozen Dinner). The original TV Dinner came in an aluminum tray and was heated in an oven. In the United States the term is synonymous with any prepackaged dinner purchased frozen in a supermarket and heated at home.		Most frozen food trays are now made of microwaveable material, usually plastic.						Several smaller companies had conceived of frozen dinners earlier (see Invention section below), but the first to achieve success was Swanson. The first Swanson-brand TV Dinner was produced in the United States and consisted of a Thanksgiving meal of turkey, cornbread dressing, frozen peas and sweet potatoes[2] packaged in a tray like those used at the time for airline food service. Each item was placed in its own compartment. The trays proved to be useful: the entire dinner could be removed from the outer packaging as a unit; the aluminum tray could be heated directly in the oven without any extra dishes; and one could eat the meal directly from the same tray. The product was cooked for 25 minutes at 425 °F (218 °C) and fit nicely on a TV tray table. The original TV Dinner sold for 98 cents, and had a production estimate of 5,000 dinners for the first year.		The name "TV dinner" supposedly came from the shape of the tray it was served on.[3] The main entrée was in a bigger compartment on one side of the tray and the vegetables lined up in smaller compartments on the other side. The arrangement was similar to that of the front panels of a 1950s television set: a screen on the left and speakers and control on the right. There were other theories about the name of the TV dinner. One reason was that early packaging featured the image of a TV set. Another was that it was marketed a quick alternative to traditional cooking and could be consumed by families at dinnertime while watching TV, a popular pastime in the fifties.[citation needed]		Much has changed since the first TV Dinners were marketed. For instance, a wider variety of main courses – such as fried chicken, spaghetti, Salisbury steak and Mexican combinations – have been introduced. Competitors such as Banquet and Morton began offering prepackaged frozen dinners at a lower price than Swanson.[citation needed] Other changes include:		Modern-day frozen dinners tend to come in microwave-safe containers. Product lines also tend to offer a larger variety of dinner types. These dinners, also known as microwave meals, can be purchased at most supermarkets. They are stored frozen. To prepare them, the plastic cover is removed or vented, and the meal is heated in a microwave oven for a few minutes. They are convenient since they essentially require no preparation time other than the heating, although some frozen dinners may require the preparer to briefly carry out an intermediate step (such as stirring mashed potatoes midway through the heating cycle) to ensure adequate heating and uniform consistency of component items.[citation needed]		In the United Kingdom, pre-prepared frozen meals first became widely available in the late 1970s. Since then they have steadily grown in popularity with the increased ownership of home freezers and microwave ovens. Demographic trends such as the growth of smaller households have also influenced the sale of this and other types of convenience food.[6] In 2003, the United Kingdom spent £5 million a day on ready meals, and was the largest consumer in Europe.[7]		Unfrozen pre-cooked ready meals, which are merely chilled and require less time to reheat, are also popular and are sold by most supermarkets. Chilled ready meals are intended for immediate reheating and consumption. Although most can be frozen by the consumer after purchase, they can either be heated from frozen or may have to be fully defrosted before reheating.[citation needed]		Many different varieties of frozen and chilled ready meals are now generally available in the UK, including "gourmet" recipes, organic and vegetarian dishes, traditional British and foreign cuisine, and smaller children's meals.[citation needed]		The identity of the TV Dinner's inventor has been disputed. In one account, first publicized in 1996,[8] retired Swanson executive Gerry Thomas said he conceived the idea after the company found itself with a huge surplus of frozen turkeys because of poor Thanksgiving sales. Thomas' version of events has been challenged by the Los Angeles Times,[9] members of the Swanson family[10] and former Swanson employees.[11] They credit the Swanson brothers with the invention.		Swanson's concept was not original. In 1944, William L. Maxson's frozen dinners were being served on airplanes.[12] Other prepackaged meals were also marketed before Swanson's TV Dinner.[citation needed] In 1948, plain frozen fruits and vegetables were joined by what were then called 'dinner plates' with a main course, potato, and vegetable. In 1952 the first frozen dinners on oven-ready aluminum trays were introduced by Quaker States Foods under the One-Eye Eskimo label. Quaker States Foods was joined by other companies including Frigi-Dinner, which offered such fare as beef stew with corn and peas, veal goulash with peas and potatoes, and chicken chow mein with egg rolls and fried rice. Swanson, a large producer of canned and frozen poultry in Omaha, Nebraska, was able to promote the widespread sales and adaptation of frozen dinner by using its nationally recognized brand name with an extensive national marketing campaign nicknamed "Operation Smash" and the clever advertising name of "TV Dinner," which tapped into the public's excitement around the new device.[13]		The production process of TV dinners is highly automated and undergoes three major steps. Those steps are food preparation, tray loading, and freezing. During food preparation, vegetables and fruits are usually placed on a movable belt and washed, then are placed into a container to be steamed or boiled for 1–3 minutes. This process is referred to as blanching, and is used as a method to destroy enzymes in the food that can cause chemical changes negatively affecting overall flavor and color of the fruit and vegetables. As for meats, prior to cooking, they are trimmed of fat and cut into proper sizes. The fish is usually cleaned and cut into fillets, and poultry is usually washed thoroughly and dressed. Meats are then seasoned, placed on trays, and are cooked in an oven for a predetermined amount of time. After all the food is ready to be packaged, it is sent to the filling lines. The food is placed in its compartments as the trays pass under numerous filling machines; to ensure that every packaged dinner gets an equal amount of food, the filling devices are strictly regulated.[14]		The food undergoes a process of cryogenic freezing with liquid nitrogen. After the food is placed on the conveyor belt, it is sprayed with liquid nitrogen that boils as it contact the food that is undergoing freezing. This method of flash-freezing fresh foods is used to retain natural quality of the food. When the food is chilled through cryogenic freezing, small ice crystals are formed throughout the food that, in theory, can preserve the food indefinitely if stored safely. Cryogenic freezing is widely used as it is a method for rapid freezing, requires almost no dehydration, excludes oxygen thus decreasing oxidative spoilage, and causes less damage to individual freezing pieces. Due to the fact that the cost of operating cryogenic freezing is high, it is commonly used for high value food products such as TV dinners, which is a $4.5 billion industry a year that is continuing to grow with the constant introduction of new technology.[14]		Following this, the dinners are either covered with aluminum foil or paper, and the product is tightly packed with a partial vacuum created to ensure no evaporation takes place that can cause the food to dry out. Then the packaged dinners are placed in refrigerated storage facility, transported through a refrigerated truck, and are stored in the grocers freezer. TV dinners prepared with the aforementioned steps that is frozen and packaged properly can remain in near-perfect condition for a long time so long as it is stored at -18 °C during shipping and storage.[14]		The freezing process tends to degrade the taste of food[citation needed] and the meals are thus heavily processed with extra salt and fat to compensate.[15] In addition, stabilizing the product for a long period typically means that companies will use partially hydrogenated vegetable oils for some items (typically dessert). Partially hydrogenated vegetable oils are high in trans fats and are shown to adversely affect cardiovascular health.[16] The dinners are almost always significantly less nutritious than fresh food and are formulated to remain edible after long periods of storage, thus often requiring preservatives such as BHT. There is, however, some variability between brands.[17]		In recent years there has been a push by a number of independent manufacturers and retailers to make meals that are low in salt and fat and free of artificial additives. In the UK, most British supermarkets also produce their own "healthy eating" brands.[citation needed] Nearly all chilled or frozen ready meals sold in the UK are now clearly labeled with the salt, sugar and fat content and the recommended daily intake. Concern about obesity and government publicity initiatives such as those by the Food Standards Agency[18][better source needed] and the National Health Service[19][better source needed] have encouraged manufacturers to reduce the levels of salt and fat, but curiously not industrial carbohydrates, in ready prepared food.[improper synthesis?]		More recently, frozen dinners have been created that are designed to be used as a steamer, allowing rapid cooking of essentially raw ingredients (typically fish and vegetables) immediately before consumption.[citation needed]		
A cook is a person who prepares food for consumption.		A cook is sometimes referred to as a chef, although in the professional kitchen, the terms are not interchangeable.						The term "cook" within a restaurant kitchen usually refers to a person with little to no creative influence on a menu and little to no command over others within the kitchen, such as a line cook. These are usually all members of a restaurant kitchen that are underneath the sous chef in the brigade de cuisine. Other establishments may have a relatively constant menu, often only having people that can prepare food quickly and consistently, having little need for an executive chef or sous chef. The kitchens in these particular restaurants would thus be entirely run by cooks intimately acquainted with the menu. This example would not include the short order cook, however, since they are capable and willing to cook items that are not on the menu.		When used as a residential staff, the word cook may refer to the head of the kitchen in a great house or to a cook-housekeeper, responsible for cleaning as well.		Professional cooks were used in Sardinia during the Nuragic Age, as proved by the typical sculptures of the Bronze Age, and in Mycenaean Greece and they are mentioned in Linear b syllabic script.[1][2] The first Olympic champion listed in the records was a cook, Coroebus of Elis, who won the sprint race in 776 BC.[3]		Lawrence of Rome, traditionally a patron saint of cooks and roasters, is reported to have said as he was being burned at the stake in the third century, "I'm roasted on this side. If you want me well done, it's time to turn me over."[4]		
A state dinner or state lunch is a dinner or banquet paid for by a government and hosted by a head of state in his or her official residence in order to renew and celebrate diplomatic ties between the host country and the country of a foreign head of state or head of government who was issued an invitation. It may form part of a state visit or diplomatic conference. In many countries around the world, there are many different rules governed by protocol. State dinners often consist of, but are not limited to, black tie or white tie dress, military honor guards, a four or five course meal, musical entertainment, dancing, and speeches made on behalf of the head of state hosting the state dinner as well as the foreign head of state.						In India, state banquets are held for foreign heads of state and government at the Rashtrapati Bhavan in New Delhi and are hosted by the President of India. Over one hundred guests usually attend state banquets, including members of the Government of India such as the Vice-President of India, the Prime minister of India, and prominent members of the ruling party. Indian and foreign business leaders also attend.		At the beginning of a state banquet, a foreign head of state is greeted by the president in the North Drawing Room. A tent constructed in the Mughal Garden within the environs of the presidential palace is the outdoor setting for state banquets. During the evening, the gardens are lit up with earthen diyas, string lights, and decorated with flowers and rangoli that become a scene for entertainment. After a performance by Rajasthani singers, Indian percussion instruments like the mridangam, tabla, ghatam and khanjeera, as well as India's diverse classical dances in which Bharatnatyam, Odissi and Kathak are carefully choreographed, will all be showcased in front of the guests.		Inside the tent, speeches highlighting bilateral diplomatic relations are delivered by the President of India and the foreign head of state. The guests are then offered a sumptuous meal of Indian delicacies while the Indian Navy Band performs music.		State banquets follow an official arrival ceremony which occurs at the Rashtrapati Bhavan earlier in the day.		In the United Kingdom, state dinners are hosted by the head of state, who is the British sovereign, currently Queen Elizabeth II. Traditionally all state dinners were held at Buckingham Palace because of its position in the heart of London. However, in recent years, banquets have also been held at Windsor Castle in Berkshire. Organisation of the state dinner usually falls to the Master of the Household with a seating plan confirmed both by the Queen and the Foreign and Commonwealth Office. State dinners are usually held for visiting heads of state and are very elaborate; Royal Protocol is generally very strict but this has been played down over recent years. All speeches that are read are again checked and confirmed by the Foreign office, and amended. Gifts are exchanged by both parties.		In the United States, a state dinner is a formal dinner, more often black tie in recent years rather than white tie, which is held in honor of a foreign head of state, such as a king, queen, president, or any head of government. A state dinner is hosted by the President of the United States and held in the State Dining Room at the White House in Washington D.C. Other formal dinners for important people of other nations, such as a prince or princess, are called official dinners, the difference being that the federal government does not pay for them.		State and official dinners are dictated by strict protocol in order to ensure that no diplomatic gaffes occur. The Chief of Protocol of the United States, who is an official within the United States Department of State, the White House Chief Usher, who is head of the household staff at the White House, as well as the White House Social Secretary all oversee the planning of state and official dinners from beginning to end. The Graphics and Calligraphy Office located in the East Wing of the White House also bears numerous responsibilities. The White House Chief Calligrapher creates place cards with the names of the guests who are assigned seats around the tables in the State Dining Room. The Chief Calligrapher also designs and writes formal invitations that are mailed to the postal addresses of the guests. State dinners require close coordination between the White House Executive Chef and the White House Executive Pastry Chef who plan and prepare a four or five-course meal, as well as the White House Chief Floral Designer who arranges flowers and decorations on the candle-lit tables.		As is customary for all incoming state visits by foreign heads of state, a state dinner follows a State Arrival Ceremony which occurs on the South Lawn earlier in the day. In addition, state dinners held in recent years are also given media coverage by the public affairs TV channel, C-SPAN.		In the early 19th century, dinners honoring the president's Cabinet, Congress, or other dignitaries were called 'state dinners' even though they lacked official foreign representation. Under such conditions, large receptions and dinners were a rare occurrence as Washington, D.C., society was a collection of isolated villages widely separated and at times almost inaccessible. Times changed and so did the nation’s capital as a series of state dinners were held every winter social season to honor Congress, the Supreme Court, and members of the diplomatic corps.		In the late 19th century, the term state dinner became synonymous with a dinner hosted by the president honoring a foreign head of state. The first visiting head of state to attend a state dinner at the White House was King David Kalakaua of the Kingdom of Hawaii, hosted by Ulysses S. Grant on December 12, 1874.[1]		The restoration of the White House by the architectural firm McKim, Mead, and White in 1902 created a more proper setting for official entertainment to occur. When the president's office moved to the newly constructed West Wing, the Neoclassical remodeling of the Executive Residence's state rooms gave Theodore Roosevelt a perfect venue reflecting the United States' growing power and influence around the world. While the White House underwent a complete interior reconstruction from 1948 to 1952, Harry S. Truman and Bess Truman lived at Blair House and state dinners were held in local hotels in the nation's capital. Long banquet tables were always used in the State Dining Room prior to the administration of John F. Kennedy. However, these were permanently discarded by Jacqueline Kennedy and replaced with round tables which could seat a far greater number of guests, approximately 120 to 140, in such a tight and confined space. To this day, presidents and first ladies continue to add their own personal touches and flair in entertaining foreign guests of state at the White House, having full access to the vermeil collection of gilded candelabras and flatware, the President's House crystal pattern, as well as the priceless collection of White House china which dates from the James Monroe administration to the George W. Bush administration, for use at a state dinner.[1]		During a state dinner, honor guards and color guards in full dress uniform from all branches of the United States Armed Forces are dispatched for ceremonial duty at the White House. At the North Portico entrance of the White House, the President of the United States and the First Lady of the United States formally greet the visiting head of state and his or her spouse, who have arrived in a motorcade from Blair House, the traditional guest quarters of foreign heads of state and dignitaries, or from a foreign ambassador's residence in the area of Embassy Row in Northwest, Washington, D.C. A brief photo opportunity for the media at the top of the staircase will occur. The president and first lady then escort the visiting head of state and his or her spouse to the Yellow Oval Room for a reception on the residence floor where the president's guests are served hors d'œuvres, cocktails, wine, or champagne. The president and first lady also introduce their guests to a wide array of people from the United States such as ambassadors, diplomats, members of Congress, members of the president's Cabinet, and other prominent people such as celebrities and Hollywood A-list movie stars invited at the discretion of the president and first lady.		After the informal reception in the Yellow Oval Room, the president and the foreign head of state, followed by the first lady and the foreign head of state's spouse, descend the Grand Staircase to the Entrance Hall on the state floor where they are met by the United States Marine Band, "The President's Own". Four ruffles and flourishes, immediately followed by Hail to the Chief, serves as the fanfare for the president's arrival. Often, the national anthem of the foreign head of state's country as well as the Star Spangled Banner are performed.		After a receiving line whereby the president introduces the visiting head of state to all of the invited guests, the president and the visiting head of state, his or her spouse, and the first lady, walk down the Cross Hall and proceed to the State Dining Room where a four or five-course meal, typically consisting of an appetizer/soup, fish, meat, salad and dessert, are served to the guests. The menu planned in advance for a state dinner and prepared by the White House Executive Chef and White House Executive Pastry Chef centers around the national cuisine of the visiting foreign head of state, using local ingredients, flavors, and ethnic foods. Before eating the meal, both the president and the visiting foreign head of state give a speech on a lectern, paying tribute to diplomatic relations between the United States and the foreign head of state's country. Members of the "Strolling Strings," violinists from the United States Marine Band "The President's Own," disperse throughout the State Dining Room and perform for the guests seated around the candle-lit tables. After the meal, the guests are seated in the East Room and are formally entertained by a musical ensemble such as a pianist, a singer, an orchestra, or band of national renown. On past occasions, dancing has also been a component at the conclusion of a state dinner.		In Switzerland, the head of the state is the Swiss Federal Council (not only its president). For this reason, the seven Federal Councillors (and their spouses) are invited to the state dinners organised in Bern during state visits.[2]		President Dwight D. Eisenhower and First Lady Mamie Eisenhower in a photo opportunity with King Bhumibol Adulyadej of Thailand and his consort, Queen Sirikit of Thailand, prior to a state dinner held on June 14, 1960.		President John F. Kennedy, First Lady Jacqueline Kennedy, President Félix Houphouët-Boigny and Madame Marie-Thérèse Houphouët-Boigny of Côte d'Ivoire in the Entrance Hall prior to a state dinner held on May 22, 1962.		President Lyndon B. Johnson, First Lady Lady Bird Johnson, Israeli Prime Minister Levi Eshkol and his spouse Miriam Eshkol, in the Entrance Hall prior to a state dinner held on June 1, 1964.		President Richard Nixon, First Lady Pat Nixon, Romanian President Nicolae Ceaușescu and his spouse First Lady Elena Ceaușescu, in the Entrance Hall prior to a state dinner held on December 4, 1973.		A menu during a state dinner held for King Hussein of Jordan on the evening of August 16, 1974.		President Gerald Ford, First Lady Betty Ford, Japanese Emperor Hirohito, and Japanese Empress Kōjun walking down the Cross Hall to the East Room during a state dinner held on October 2, 1975.		President Jimmy Carter and First Lady Rosalynn Carter dining with German Chancellor Helmut Schmidt and Loki Schmidt during a state dinner held in the State Dining Room on July 13, 1977.		President Ronald Reagan, First Lady Nancy Reagan, Chinese President Li Xiannian and his spouse First Lady Lin Jiamei, at the North Portico entrance of the White House prior to a state dinner held on July 23, 1985.		President George H.W. Bush, Russian President Boris Yeltsin, First Lady Barbara Bush, and Naina Yeltsina descending the Grand Staircase during a state dinner held on June 16, 1992.		President George W. Bush and First Lady Laura Bush in a photo opportunity with Queen Elizabeth II and her consort, Prince Philip, Duke of Edinburgh, at the North Portico entrance of the White House prior to a state dinner held on May 7, 2007.		President Barack Obama and First Lady Michelle Obama greet French President François Hollande at the North Portico entrance of the White House prior to a state dinner held on February 11, 2014.		
Street food is ready-to-eat food or drink sold by a hawker, or vendor, in a street or other public place, such as at a market or fair. It is often sold from a portable food booth,[1] food cart, or food truck and meant for immediate consumption. Some street foods are regional, but many have spread beyond their region of origin. Most street foods are classed as both finger food and fast food, and are cheaper on average than restaurant meals. According to a 2007 study from the Food and Agriculture Organization, 2.5 billion people eat street food every day.[2]		Today, people may purchase street food for a number of reasons, such as to get flavourful food for a reasonable price in a sociable setting, to experience ethnic cuisines, or for nostalgia.[3]						Small fried fish were a street food in ancient Greece;[4] however, Theophrastus held the custom of street food in low regard.[5] Evidence of a large number of street food vendors was discovered during the excavation of Pompeii.[6] Street food was widely consumed by poor urban residents of ancient Rome whose tenement homes did not have ovens or hearths.[7] Here, chickpea soup[8] with bread and grain paste[9] were common meals. In ancient China, street food generally catered to the poor, however, wealthy residents would send servants to buy street food and bring it back for them to eat in their homes.[7]		A traveling Florentine reported in the late 14th century that in Cairo, people brought picnic cloths made of rawhide to spread on the streets and sit on while they ate their meals of lamb kebabs, rice, and fritters that they had purchased from street vendors.[10] In Renaissance Turkey, many crossroads had vendors selling "fragrant bites of hot meat", including chicken and lamb that had been spit-roasted.[11] In 1502, Ottoman Turkey became the first country to legislate and standardize street food.[12]		Aztec marketplaces had vendors who sold beverages such as atolli ("a gruel made from maize dough"), almost 50 types of tamales (with ingredients that ranged from the meat of turkey, rabbit, gopher, frog and fish to fruits, eggs and maize flowers),[13] as well as insects and stews.[14] Spanish colonization brought European food stocks like wheat, sugarcane and livestock to Peru, however, most commoners continued to primarily eat their traditional diets. Imports were only accepted at the margins of their diet, for example, grilled beef hearts sold by street vendors.[15] Some of Lima's 19th-century street vendors such as "Erasmo, the 'negro' sango vendor" and Na Aguedita are still remembered today.[16]		During the American Colonial period, "street vendors sold oysters, roasted corn ears, fruit, and sweets at low prices to all classes." Oysters, in particular, were a cheap and popular street food until around 1910 when overfishing and pollution caused prices to rise.[17] Street vendors in New York City faced a lot of opposition. After previous restrictions had limited their operating hours, street food vendors were completely banned in New York City by 1707.[18] Many women of African descent made their living selling street foods in America in the 18th and 19th centuries, with products ranging from fruit, cakes, and nuts in Savannah, to coffee, biscuits, pralines, and other sweets in New Orleans.[19] Cracker Jack started as one of many street food exhibits at the Columbian Exposition.[20]		In the 19th century, street food vendors in Transylvania sold gingerbread-nuts, cream mixed with corn, as well as bacon and other meat fried on top of ceramic vessels with hot coals inside.[21] French fries, consisting of fried strips of potato, probably originated as a street food in Paris in the 1840s.[22] Street foods in Victorian London included tripe, pea soup, pea pods in butter, whelk, prawns, and jellied eels.[23]		Ramen, originally brought to Japan by Chinese immigrants about 100 years ago, began as a street food for laborers and students. However, it soon became a "national dish" and even acquired regional variations.[24] The street food culture of Southeast Asia today was heavily influenced by coolie workers imported from China during the late 19th century.[25]		In Thailand, although street food did not become popular among native Thai people until the early 1960s, because of rapid urban population growth,[26] by the 1970s it had "displaced home-cooking."[27] The rise of the country's tourism industry is also contributed to the popularity of Thai street food.		In Indonesia — especially Java, travelling food and drink vendor has a long history, as they were described in temples bas reliefs dated from 9th century, as well as mentioned in 14th century inscription as a line of work. During colonial Dutch East Indies period circa 19th century, several street food were developed and documented, including satay and dawet (cendol) street vendors. The current proliferation of Indonesia's vigorous street food culture is contributed by the massive urbanization in recent decades that has opened opportunities in food service sectors. This took place in the country's rapidly expanding urban agglomerations, especially in Greater Jakarta, Bandung and Surabaya.[28]		Street food vending is found all around the world, but varies greatly between regions and cultures.[29] For example, Dorling Kindersley describes the street food of Vietnam as being "fresh and lighter than many of the cuisines in the area" and "draw[ing] heavily on herbs, chile peppers and lime", while street food of Thailand is "fiery" and "pungent with shrimp paste ... and fish sauce." New York City's signature street food is the hot dog, however, New York street food also includes everything from "spicy Middle Eastern falafel or Jamaican jerk chicken to Belgian waffles"[30]		Street food in Thailand offers various selection of ready-to-eat meals, snacks, fruits and drinks sold by hawkers or vendors at food stalls or food carts on the street side. Bangkok is often mentioned as one of the best place for street food.[31][32] Popular street offerings includes pad thai (stir fried rice noodle), som tam (green papaya salad), sour tom yum soup, various selection of Thai curries, to sticky rice mango		Indonesian street food is a diverse mix of local Indonesian, Chinese, and Dutch influences.[33] Indonesian street food often tastes rather strong and spicy. A lot of street food in Indonesia are fried, such as local gorengan (fritters), also nasi goreng and ayam goreng, while bakso meatball soup, skewered chicken satay and gado-gado vegetable salad served in peanut sauce are also popular.[34]		Indian street food is as diverse as Indian cuisine. Every region has its own specialties to offer. Some of the more popular street food dishes are Vada Pav, Cholle Bhature, Parathas, Rolls, Bhel Puri, Sev Puri, Gol Gappa, Aloo tikki, Kebabs, Tandoori chicken, Samosa, Bread omelette, Pav bhaji and Pakora.In India, street food is popularly known as nukkadwala food. There are several restaurants and QSRs in India that have also taken their inspiration from the vibrant street food of India.[35]		In Hawaii, the local street food tradition of "plate lunch" (rice, macaroni salad, and a portion of meat) was inspired by the bento of the Japanese who had been brought to Hawaii as plantation workers.[36] In Denmark, sausage wagons allow passersby to purchase sausages and hot dogs.		In Egypt, a food sold commonly on the street is ful, a slow-cooked fava bean dish.[37]		Mexican street food is known as "antojitos" (translated as "little cravings") which include several varieties of tacos, such as tacos al pastor, huaraches and other maize based foods		Because of differences in culture, social stratification and history, the ways in which family street vendor enterprises are traditionally created and run vary in different areas of the world.[38] For example, few women are street vendors in Bangladesh, but women predominate in the trade in Nigeria and Thailand.[39] Doreen Fernandez says that Filipino cultural attitudes towards meals is one "cultural factor operating in the street food phenomenon" in the Philippines because eating "food out in the open, in the market or street or field" is "not at odds with the meal indoors or at home" where "there is no special room for dining".[21]		Walking on the street while eating is considered rude in some cultures,[40] such as Japan[41] or Swahili cultures, although it is acceptable for children.[42] In India, Henrike Donner wrote about a "marked distinction between food that could be eaten outside, especially by women," and the food prepared and eaten at home, with some non-Indian food being too "strange" or tied too closely to non-vegetarian preparation methods to be made at home.[43]		In Tanzania's Dar es Salaam region, street food vendors produce economic benefits beyond their families. Because street food vendors purchase local fresh foods, urban gardens and small-scale farms in the area have expanded.[44] In the United States, street food vendors are credited with supporting New York City's rapid growth by supplying meals for the city's merchants and workers.[45] Proprietors of street food in the United States have had a goal of upward mobility, moving from selling on the street to their own shops.[3] However, in Mexico, an increase in street vendors has been seen as a sign of deteriorating economic conditions in which food vending is the only employment opportunity that unskilled labor who have migrated from rural areas to urban areas are able to find.[14]		In 2002, Coca-Cola reported that China, India, and Nigeria were some of its fastest-growing markets: markets where the company's expansion efforts included training and equipping mobile street vendors to sell its products.[44]		As early as the 14th century, government officials oversaw street food vendor activities.[10] With the increasing pace of globalization and tourism, the safety of street food has become one of the major concerns of public health, and a focus for governments and scientists to raise public awareness.[47][48][49][50] However, despite concerns about contamination at street food vendors, the incidence of such is low, with studies showing rates comparable to restaurants.[51]		In 2002, a sampling of 511 street foods in Ghana by the World Health Organization showed that most had microbial counts within the accepted limits,[52] and a different sampling of 15 street foods in Calcutta showed that they were "nutritionally well balanced", providing roughly 200 kcal (Cal) of energy per rupee of cost.[53]		In the United Kingdom, the Food Standards Agency provides comprehensive guidance of food safety for the vendors, traders and retailers of the street food sector.[54] Other effective ways of enhancing the safety of street foods include: mystery shopping programs, training, rewarding programs to vendors, regulatory governing and membership management programs, and technical testing programs.[55][56][57][58][59]		Despite knowledge of the risk factors, actual harm to consumers’ health is yet to be fully proven and understood. Due to difficulties in tracking cases and the lack of disease-reporting systems, follow-up studies proving actual connections between street food consumption and food-borne diseases are still very few. Little attention has been devoted to consumers and their eating habits, behaviors and awareness. The fact that social and geographical origins largely determine consumers’ physiological adaptation and reaction to foods—whether contaminated or not—is neglected in the literature.[60]		In the late 1990s, the United Nations and other organizations began to recognize that street vendors had been an underused method of delivering fortified foods to populations, and in 2007, the UN Food and Agriculture Organization recommended considering methods of adding nutrients and supplements to street foods that are commonly consumed by the particular culture.[51]		
Feeding is the process of ingesting food to provide for an animal's nutritional needs.		Feeding may also refer to:		
An energy crop is a plant grown as a low-cost and low-maintenance harvest used to make biofuels, such as bioethanol, or combusted for its energy content to generate electricity or heat. Energy crops are generally categorized as woody or herbaceous plants; many of the latter are grasses of the family Graminaceae.		Commercial energy crops are typically densely planted, high-yielding crop species which are processed to bio-fuel and burnt to generate power. Woody crops such as willow[1] or poplar are widely utilised, as well as temperate grasses such as Miscanthus and Pennisetum purpureum (both known as elephant grass).[2] If carbohydrate content is desired for the production of biogas, whole-crops such as maize, Sudan grass, millet, white sweet clover and many others, can be made into silage and then converted into biogas.[3]		Through genetic modification and application of biotechnology plants can be manipulated to create greater yields, reduce associated costs and require less water. High energy yield can also be realized with existing cultivars.[3]:250						Energy generated by burning plants grown for the purpose, often after the dry matter is pelletized. Energy crops are used for firing power plants, either alone or co-fired with other fuels. Alternatively they may be used for heat or combined heat and power (CHP) production.		To cover the increasing requirements of woody biomass, short rotation coppice (SRC) were applied to agricultural sites. Within this cropping systems fast growing tree species like willows and poplars are planted in growing cycles of three to five years. The cultivation of this cultures is dependent on wet soil conditions and could be an alternative for moist field sieds. However, an influence on local water conditions could not be excluded. This indicates that an establishment should exclude the vicinity to vulnarable wetland ecosystems.[4][5][6]		Anaerobic digesters or biogas plants can be directly supplemented with energy crops once they have been ensiled into silage. The fastest growing sector of German biofarming has been in the area of "Renewable Energy Crops" on nearly 500,000 ha (1,200,000 acres) of land (2006).[7] Energy crops can also be grown to boost gas yields where feedstocks have a low energy content, such as manures and spoiled grain. It is estimated that the energy yield presently of bioenergy crops converted via silage to methane is about 2 GWh/km2 (1.8×1010 BTU/sq mi). Small mixed cropping enterprises with animals can use a portion of their acreage to grow and convert energy crops and sustain the entire farms energy requirements with about one fifth of the acreage. In Europe and especially Germany, however, this rapid growth has occurred only with substantial government support, as in the German bonus system for renewable energy. Similar developments of integrating crop farming and bioenergy production via silage-methane have been almost entirely overlooked in N. America, where political and structural issues and a huge continued push to centralize energy production has overshadowed positive developments.[citation needed]		European production of biodiesel from energy crops has grown steadily in the last decade, principally focused on rapeseed used for oil and energy. Production of oil/biodiesel from rape covers more than 12,000 km² in Germany alone, and has doubled in the past 15 years.[8] Typical yield of oil as pure biodiesel may be is 100,000 L/km2 (68,000 US gal/sq mi; 57,000 imp gal/sq mi) or more, making biodiesel crops economically attractive, provided sustainable crop rotations exist that are nutrient-balanced and preventative of the spread of disease such as clubroot. Biodiesel yield of soybeans is significantly lower than that of rape.[citation needed]		Energy crops for biobutanol are grasses. Two leading non-food crops for the production of cellulosic bioethanol are switchgrass and giant miscanthus. There has been a preoccupation with cellulosic bioethanol in America as the agricultural structure supporting biomethane is absent in many regions, with no credits or bonus system in place.[citation needed] Consequently, a lot of private money and investor hopes are being pinned on marketable and patentable innovations in enzyme hydrolysis and the like.		Bioethanol also refers to the technology of using principally corn (maize seed) to make ethanol directly through fermentation, a process that under certain field and process conditions can consume as much energy as is the energy value of the ethanol it produces, therefore being non-sustainable. New developments in converting grain stillage (referred to as distillers grain stillage or DGS) into biogas energy looks promising as a means to improve the poor energy ratio of this type of bioethanol process.		Dedicated energy crops are non-food energy crops as giant miscanthus, switchgrass, jatropha, fungi, and algae. Dedicated energy crops are promising cellulose sources that can be sustainably produced in many regions of the United States.[9]		Additionally, the green waste byproducts of food and non-food energy crops can be used to produce various biofuels.		
A pu pu platter, pu-pu platter or pupu platter is a tray of American Chinese or Hawaiian food,[1] consisting of an assortment of small meat and seafood appetizers. A typical pupu platter, as found in American Chinese cuisine, might include an egg roll, spare ribs, chicken wings, chicken fingers, beef teriyaki, skewered beef, fried wontons, crab rangoon, fried shrimp, among other items, accompanied by a small hibachi grill.		The pupu platter was probably first introduced to restaurants on the United States mainland by Don the Beachcomber in 1934.[1] It has since became a standard at most Polynesian themed restaurants such as Don's and Trader Vic's.[2][3] The earliest known print reference to a pupu platter served at a Chinese restaurant is from 1969.[4]		Later, other types of restaurants used pu pu platter to mean an appetizer combination platter.[n 1] However, pu pu platters are currently more closely associated with American Chinese restaurants.[5][6]						In the Hawaiian language, pū-pū denotes a relish, appetizer, canapé, or hors d'oeuvre; it originally meant "shell fish', but also referred to small bits of fish, chicken, or banana relish served with kava[7] and beans.		Since the introduction of commercial dining and drinking establishments in Hawaii, pūpū were, and remain, standard fare in island establishments.[n 2] An establishment that serves "heavy pupus" will often have a buffet table with warming trays full of chicken, tempura vegetables, shrimp, poke (cubed and seasoned raw fish), small skewers of teriyaki meat or chicken, sushi, and other similar finger foods. An establishment that serves "light pupus" usually will offer only the cold foods such as poke, sushi, and vegetables. Some establishments will serve pūpū to the table.		At Hawaiian bar, restaurants, catered events such as political rallies, and private parties, establishments and hosts are known in "local" circles by the quality of their pupus. Event invitations often will state that "light pupus" or "heavy pupus" will be served so that attendees will know whether they should plan to have a full meal before the event or not.		Today, the simple platter of dried fish, grilled chicken, and slices of banana has evolved into chefs' offerings of international delicacies arranged for visual as well as gustatory pleasure. Modern pupu platters can hold offerings of anything from traditional Hawaiian fare to exotic combinations.		At the height of the tiki bar/restaurant craze, the New York Herald Tribune published several articles concerning the opening and the ambiance of one of the first Hawaiian themed restaurants in New York City, Luau 400, on East 57th Street. At the time of the restaurant's opening in 1957, pu pu platters were considered a part of the luau feast.[8] A typical platter at this establishment could have include baked clams, rumaki, Shrimp Vela (battered fried shrimp with coconut), chicken wings, egg rolls, spare ribs, or Javanese sate (satay) on skewers.[9] The appetizers were served on "a Lazy Susan made of monkey pod wood and equipped with a little stove fired with charcoal briquettes."[8] Recipes for some of the pu pu items were later published in the Herald Tribune in 1960.[10]		At one 21st-century tiki bar, the pu pu platter includes "Samoan deviled eggs, Chinese sausage and stick rice arancini, coconut shrimp and chilies stuffed with pork sausage."[11]		
A sittning ("sitting", often shortened to sits or sitz,[citation needed] Finnish: sitsit) is in Sweden and Finland a seated meal held within a set time frame. In restaurants it may refer to a seating, i.e. the time given for a crowd to have their meal. The term is also used to denote the part of a party that is a seated meal. Though it can refer to any kind of meal, it is often used to refer to a student sittning.						A student sittning in Sweden or Finland is usually a dinner had at the student union's or nation's property, usually a pub room, or banquet hall if the student union is fortunate enough to have one. In academic environments some of the tradition is carried on even after one is no longer a student.		The dress code is different depending on the occasion, everything from white tie to student boilersuit can be the evening's dress code. Sittnings often have themes, and the guests are encouraged to dress to match the theme.		The meal is prepared and served by students who often spend some time working at their union or nation. The number of dishes vary, but three courses is common, and alcohol is usually included in the price (beer or cider and snaps along with Punsch). If snaps or punsch is not included, there will often be tickets for sale.		A vital part of a sittning is singing. The guests are usually given a booklet with songs that will be sung during the sittning. Everyone sings when a song is brought up, and songs are often related to the current progress of the sitting. Sittnings often begin with a certain song and end with another, songs are sung when drinking snaps, and there is often a song to honor the students serving and cooking. These songs differ depending on student union or nation. In some locations, such as Uppsala or Stockholm, it is common for a sittning to end with the song "O gamla klang och jubeltid". During the last verse of the song, the guests stand on their chairs and after the song is finished you are no longer supposed to sit down.		During Finnish-language sitsit, many of the classic Swedish drinking songs are also sung in Swedish or in translated versions. In more formal sitsit, academic and patriotic songs, such as De Brevitate Vitae, Finlandia Hymn and the Jäger March are sung.		After a song is sung, glasses are usually raised for a skål, or in English, a toast. Many places in Sweden and Finland have an etiquette concerning how to toast. In Uppsala, for example, the tradition is usually to raise your glass and to nod first to your table partner (for ladies, the gentleman to your left, and for gentlemen, the lady to your right), then with the person next to you who is not your table partner, and lastly with the person sitting across from you. After the three nods, you drink and then nod to the same people in the reverse order. Finally, you set your glass back down on the table (ladies first). During some sittnings this begins very formally but then becomes more and more sloppy (relaxed).		Often sittnings will involve sitting at long tables (långbord). In some cases, the guests will be designated seats, if possible every other lady and gentleman, and shifted on opposite sides of the table so that the ladies will sit opposite a gentleman, and vice versa.		Other performances such as short spex (theatre), choir performances, speeches, and pretty much anything that may be entertaining to the guests may also take place during the sittning.		The sittning is usually guided by a toastmaster and often also by a sånganförare ("song master"). The toastmaster keeps note of and introduces everyone who wants to perform something, communicates with the serving staff, and generally makes sure that the sittning runs smoothly. The sånganförare chooses which songs to sing and starts them off.		Some students are very relaxed about all the traditions that come with a sittning and do as they please so as to have a very nice evening. Others recognize that the traditions in fact enhance the festivity of the sittning and adhere to them rigorously. Strangely, when Swedish students from different sittning traditions meet they will often find each other's traditions quite remarkable, even absurd.		A gask or a ball usually starts with a sittning and then continues on with a party after the sittning is over. Usually there is dancing, either to live music or a DJ, and a pub. Formal etiquette suggests that a gentleman at a ball with a sittning should dance the two first dances with his table partner (the lady on the right), the following two dances with the lady who was on his left, and finally two dances with the lady sitting across from him.		
Detritivores, also known as detrivores, detritophages, detritus feeders, or detritus eaters, are heterotrophs that obtain nutrients by consuming detritus (decomposing plant and animal parts as well as feces).[1] There are many kinds of invertebrates, vertebrates and plants that carry out coprophagy. By doing so, all these detritivores contribute to decomposition and the nutrient cycles. They should be distinguished from other decomposers, such as many species of bacteria, fungi and protists, which are unable to ingest discrete lumps of matter, but instead live by absorbing and metabolizing on a molecular scale (saprotrophic nutrition). However, the terms detritivore and decomposer are often used interchangeably.		Detritivores are an important aspect of many ecosystems. They can live on any soil with an organic component, including marine ecosystems, where they are termed interchangeably with bottom feeders.		Typical detritivorous animals include millipedes, springtails, woodlice, dung flies, slugs, many terrestrial worms, sea stars, sea cucumbers, fiddler crabs, and some sedentary polychaetes such as amphitrites (Amphitritinae, worms of the family Terebellidae) and other terebellids.		Scavengers are typically not thought to be detritivores, as they generally eat large quantities of organic matter, but both detritivores and scavengers are specific cases of consumer-resource systems.[2] The eating of wood, whether alive or dead, is known as xylophagy. Τhe activity of animals feeding only on dead wood is called sapro-xylophagy and those animals, sapro-xylophagous.		In food webs, detritivores generally play the roles of decomposers. Detritivores are often eaten by consumers and therefore commonly play important roles as recyclers in ecosystem energy flow and biogeochemical cycles.		Many detritivores live in mature woodland, though the term can be applied to certain bottom-feeders in wet environments. These organisms play a crucial role in benthic ecosystems, forming essential food chains and participating in the nitrogen cycle.[3]		Fungi, acting as decomposers, are important in today's terrestrial environment. During the Carboniferous period, fungi and bacteria had yet to evolve the capacity to digest lignin, and so large gokul of dead plant tissue accumulated during this period, later becoming the fossil fuels.[citation needed]		By feeding on sediments directly to extract the organic component, some detritivores accidentally concentrate toxic pollutants.		
Baby food is any soft, easily consumed food, other than breastmilk or infant formula, that is made specifically for babies, roughly between the ages of four to six months and two years. The food comes in multiple varieties and tastes; it may be table food that the rest of the family is eating that has been mashed or otherwise broken down, or it can be purchased ready-made from producers.						As of 2011, the World Health Organization, UNICEF and many national health agencies recommended waiting until six months of age before starting a child on food;[1] individual babies may differ greatly from this guideline based on their unique developmental progress. Baby food can be given when the child is developmentally ready to eat. Signs of readiness include the ability to sit without help, loss of tongue thrust, and the display of active interest in food that others are eating.		As a global public health recommendation, the World Health Organization recommends that infants should be exclusively breastfed for the first six months of life to achieve optimal growth, development and health. Most six-month-old infants are physiologically and developmentally ready for new foods, textures and modes of feeding.[2] Experts advising the World Health Assembly have provided evidence that introducing solids earlier than six months increases babies' chances of illness, without improving growth.[3]		One of the health concerns associated with the introduction of solid foods before six months is iron deficiency. The early introduction of complementary foods may satisfy the hunger of the infant, resulting in less frequent breastfeeding and ultimately less milk production in the mother. Because iron absorption from human milk is depressed when the milk is in contact with other foods in the proximal small bowel, early use of complementary foods may increase the risk of iron depletion and anemia.[2]		In Canada sodium content in infant food is regulated; strained fruit, fruit juice, fruit drink, and cereal cannot be sold if sodium has been added (excluding strained desserts). Foods naturally containing sodium are limited to 0.05 - 0.25 grams per 100 grams of food, depending on the type of infant food.[4]		If there is a family history of allergies, one may wish to introduce only one new food at a time, leaving a few days in between to notice any reactions that would indicate a food allergy or sensitivity. This way, if the child is unable to tolerate a certain food, it can be determined which food is causing the reaction.[citation needed]		Newborns need a diet of breastmilk or infant formula. About 40% of the food energy in these milks comes from carbohydrates, mostly from a simple sugar called lactose.[5]		As shown in the 2008 Feeding Infants and Toddlers study, the overall diet of babies and toddlers, the primary consumers of baby food, generally meets or significantly exceeds the recommended amount of macronutrients.[6] Toddlers and preschoolers generally ate too little dietary fiber, and preschoolers generally ate too much saturated fat, although the overall fat intake was lower than recommended.[6] Micronutrient levels were typically within the recommended levels. A small group of older infants in the American study needed more iron and zinc, such as from iron-fortified baby foods.[6] A substantial proportion of toddlers and preschoolers exceeded the upper recommended level of synthetic folate, preformed vitamin A, zinc, and sodium (salt).[6]		The World Health Organization recommends starting in small amounts that gradually increase as the child gets older: 2 to 3 meals per day for infants 6 to 8 months of age and 3 to 4 meals per day for infants 9 to 23 months of age, with 1 or 2 additional snacks as required.		Baby foods are either a soft, liquid paste or an easily chewed food since babies lack developed muscles and teeth to effectively chew. Babies typically move to consuming baby food once nursing or formula is not sufficient for the child's appetite. Babies do not need to have teeth to transition to eating solid foods. Teeth, however, normally do begin to show up at this age. Care should be taken with certain foods that pose a choking hazard, such as undercooked vegetables, grapes, or food that may contain bones. Babies begin eating liquid style baby food consisting of pureed vegetables and fruits, sometimes mixed with rice cereal and formula, or breastmilk. Then, as the baby is better able to chew, small, soft pieces or lumps may be included. Care should be taken, as babies with teeth have the ability to break off pieces of food but they do not possess the back molars to grind, so food can be carefully mashed or prechewed, or broken into manageable pieces for their baby. Around 6 months of age, babies may begin to feed themselves (picking up food pieces with hands, using the whole fist, or later the pincer grasp [the thumb and forefinger]) with help from parents.		Homemade baby food is less expensive than commercial baby foods.[7] Homemade food is appropriate only when the family has a sufficient and varied diet, as well as access to refrigeration and basic sanitation.[7] It is important to follow proper sanitation methods when preparing homemade baby food such as washing and rinsing vegetables or fruit, as well as the cooking and packaging materials that will be used.		Homemade food requires more preparation time than opening a jar or box of ready-to-eat commercial baby food. Food may need to be minced or pureed for young babies, or cooked separately without the salt, intense spices, or sugar that the family chooses to eat.[7] Avocados and bananas are foods that can be easily mashed and are high in vitamins and nutrients, making them ideal starter foods for an infant 6 months in age or older.		Through the first year, breastmilk or infant formula is the main source of calories and nutrients.		Babies may be started directly on normal family food if attention is given to choking hazards; this is called baby-led weaning. Because breastmilk takes on the flavor of foods eaten by the mother,[8] these foods are especially good choices.[9]		Nestlé's Feeding Infants and Toddlers Study (FITS) of 2008 indicates that few American babies are fed baby food before the age of four months.[10]		Some commercial baby food companies have expanded their lines to produce specialty foods for toddlers from the age of about 12 months to two and a half years old.[11] These include juice, cereal, small microwaveable meals, baked goods, and other foods that have been formulated and marketed for toddlers.		In the late 1940s, Gerber Products Company and Beech-Nut produced special cookbooks to promote the sale of commercial baby foods for use by elderly, sick, or disabled people.[12]		Parents and/or caregivers may perceive up to half of toddlers as being "picky" or "faddy", with the peak around 24 months.[13][14] Adults who hold this opinion often stop offering new foods to the child after only three to five attempts, rather than continuing to offer the food until the child has tasted it eight to fifteen times. They may also engage in counterproductive behaviors, such as offering appetite-suppressing milk or other favorite foods as an alternative, or trying to force or bribe the child into eating.[15]		Baby food varies from culture to culture. In many cultures, pastes of a grain and liquids are the first baby food. In human history and presently with many cultures around the world, babies are fed food premasticated by the caretaker of the baby in order to pulverise the food and start the digestion process.[16] An infant's first bite of solid food is ceremonial and holds religious importance in many cultures. An example of this is annaprashan, a Hindu ritual where the infant is fed a sweetened rice porridge, usually blessed, by an elder family member. Similar rites of passage are practiced across Asia, including the Bengal region, Vietnam, and Thailand.[citation needed]		In the Western world until the mid-1900s, baby food was generally made at home. The industrial revolution saw the beginning of the baby food market which promoted commercial baby foods as convenience items.[17] In developed countries, babies are now often started with commercially produced iron-fortified infant cereals,[2] and then move on to mashed fruits and vegetables. Commercial baby foods are widely available in dry, ready-to-feed and frozen forms, often in small batches (e.g. small jars) for convenience of preparation.		Commercially prepared baby foods in the Netherlands were first prepared by Martinus van der Hagen through his NV Nutricia company in 1901.[18] In United States they were first prepared by Harold Clapp who sold Clapp's Baby Food in the 1920s.[19] The Fremont Canning Company, now called the Gerber Products Company, started in 1927.[11] The Beech-Nut company entered the U.S. baby food market in 1931.[20] The first precooked dried baby food was Pablum which was originally made for sick children in the 1930s. Other commercial baby food manufacturers include H. J. Heinz Company, Nestlé, Nutricia and Organix. Heinz produced dehydrated baby food in the 1980s.[21] The demand from parents for organic food began to grow in the 1960s[citation needed]; since then, many larger commercial manufacturers have introduced organic lines of infant food.		At the beginning of the 20th century in America, most babies began eating baby food around the age of seven months.[12] During and shortly after World War II, the age at which solid food was first introduced dropped to just six weeks.[12] This age has since increased to four to six months.[12] By the mid-20th century, manufactured baby food was readily used and supplemented previous infant feeding practices. Author of Inventing Baby Food, Amy Bentley argues that the excessive additives of sugar, salt, and MSG in overused manufactured baby food conditioned infants to prefer processed foods later in life. This subsequent misuse of salt and sugar was also feared to effect issues of weight and nutrition based diseases.[12]		In China and other east Asian countries, homemade baby food remains common, and babies are started on rice porridge called xifan, then move on to mashed fruits, soft vegetables, tofu and fish.[22] In Sweden, it is common to start with mashed fruit, such as bananas, as well as oatmeal and mashed vegetables. In western Africa, maize porridge is often the first solid food given to young children.[23]		Some commercial baby foods have been criticized for their contents and cost.[25]		Over the decades, there have been multiple recalls of baby foods because of concerns about contamination or spoilage. In 1984 and 1986, Gerber was involved in a scandal over glass baby food jars breaking in transit, which dramatically affected its sales and profitability, although the US Food and Drug Administration later concluded that the company was not at fault.[11] In 1987, Beechnut paid US $25 million to resolve charges of selling adulterated apple juice in the early 1980s.[11] In 2011, Nestlé France decided to recall a batch of P'tit pot baby food as a precautionary measure after a customer reportedly found glass shards in one of their jars. An investigation into the incident's scope led the company to conclude that it had been an isolated occurrence and that the rest of the batch had not been affected.[26]		Commercial baby food in the United States is dominated by Gerber, which had about 70% of the American market share in 1996.[11] Beechnut had about 15% of the market, and Heinz had about 10%. Heinz's Earth's Best, the largest brand of organic baby food, had about 2% of the American market share.[11]		In Australia, Canada, and New Zealand, Heinz had about 90% of the market share in 1996.[11] Heinz is also the market leader in the UK, Italy, and several eastern European countries.[11]		
Soups & stews		Banchan		Tteok		Banchan (/ˈbɑːnˌtʃɑːn/,[1] from Korean: 반찬; 飯饌; banchan [pan.tɕʰan]) refers to small dishes of food served along with cooked rice in Korean cuisine. This word is used both in the singular and plural. The basic table setting for a meal called bansang (반상) usually consists of bap (밥, cooked rice), guk or tang (soup), gochujang or ganjang, jjigae, and kimchi. According to the number of banchan added, the table setting is called 3 cheop (삼첩), 5 cheop (오첩), 7 cheop (칠첩), 9 cheop (구첩), 12 cheop (십이첩) bansang, with the 12 cheop used in Korean royal cuisine.[2]		Banchan are set in the middle of the table to be shared. At the center of the table is the secondary main course, such as galbi or bulgogi, and a shared pot of jjigae. Bowls of cooked rice and guk (soup) are set individually. Banchan are served in small portions, meant to be finished at each meal and are replenished during the meal if not enough. Usually, the more formal the meals are, the more banchan there will be. Jeolla province is particularly famous for serving many different varieties of banchan in a single meal.[3]						Banchan is thought to be a result of Buddhist influence at around the mid-Three Kingdoms period and the subsequent proscription against eating meat by the monarchies of these kingdoms.[4] Thus, with the ban on meat-containing dishes, vegetable-based dishes rose in prominence and became the centrepoint of Korean cuisine;[4] court kitchens developed various methods for cooking, preparing and presenting these dishes, while less-affluent commoners produced smaller, simpler arrays of these vegetable-based dishes.[4]		Although the Mongol invasions of Korea ended the ban on meat-containing dishes, as well as meat offerings for rituals such as jesa, approximately six centuries of vegetable-based cuisine in the form of banchan had imprinted itself into Korean cuisine.[4]		Kimchi is fermented vegetables, usually baechu (Napa cabbage), seasoned with chili peppers and salt. This is the essential banchan of a standard Korean meal. Some Koreans do not consider a meal complete without kimchi. Kimchi can be made with other vegetables as well, including scallions, gat (갓), and radish (무; mu).		Namul (나물) refers to steamed, marinated, or stir-fried vegetables usually seasoned with sesame oil, salt, vinegar, minced garlic, chopped green onions, dried chili peppers, and soy sauce.		Bokkeum (볶음) is a dish stir-fried with sauce.		Jorim is a dish simmered in a seasoned broth.		Jjim is a steamed dish.		Jeon denotes a variety of pan-fried, pancake-like dishes.[23] Buchimgae is a near synonym.		Various banchan served at a table		Table setting in Jeolla province with many banchan		Ojingeochae bokkeum (오징어채볶음)		Yeongeun jorim (연근조림)		Gyeran jjim (계란찜) in a hot pot		Samsaek jeon (삼색전), any three different colored jeon are referred to as such.		Japchae		
A multicourse meal is a meal of multiple courses, almost invariably eaten in the evening or afternoon. Most Western-world multicourse meals follow a standard sequence, influenced by traditional French haute cuisine. Each course is supposed to be designed with a particular size and genre that befits its place in the sequence. There are variations depending on location and custom. The following is a common sequence for multicourse meals:		Meals like this are generally very formal as well as very expensive.		
A snack is a portion of food, smaller than a regular meal, generally eaten between meals.[1] Snacks come in a variety of forms including packaged snack foods and other processed foods, as well as items made from fresh ingredients at home.		Traditionally, snacks are prepared from ingredients commonly available in the home. Often cold cuts, fruits, leftovers, nuts, sandwiches, and the like are used as snacks. The Dagwood sandwich was originally the humorous result of a cartoon character's desire for large snacks. With the spread of convenience stores, packaged snack foods became a significant business. Snack foods are typically designed to be portable, quick, and satisfying. Processed snack foods, as one form of convenience food, are designed to be less perishable, more durable, and more portable than prepared foods. They often contain substantial amounts of sweeteners, preservatives, and appealing ingredients such as chocolate, peanuts, and specially-designed flavors (such as flavored potato chips).		Beverages, such as coffee, are not generally considered snacks though they may be consumed along with or in lieu of snack foods.[2]		A snack eaten shortly before going to bed or during the night may be called a (mid)night snack.						In the United States, the first snack food was the peanut. Peanuts first arrived from South America via slave ships, and became incorporated into African-inspired cooking on southern plantations. After the Civil War, the taste for peanuts spread north, where they were incorporated into the culture of baseball games and vaudeville theaters.[3]		Along with popcorn (also of South American origin), snacks bore the stigma of being sold by unhygienic street vendors. The middle-class etiquette of the Victorian era (1837–1901) categorized any food that did not require proper usage of utensils as lower-class.[3]		Pretzels were introduced to North America by the Dutch via New Amsterdam in the 17th century. In the 1860s, the snack was still associated with immigrants, unhygienic street vendors, and saloons. Due to loss of business during the Prohibition era (1920-1933), pretzels underwent rebranding to make them more appealing to the public. Packaging revolutionized snack foods, allowing sellers to reduce contamination risk while making it easy to advertise brands with a logo. Pretzels boomed in popularity, bringing many other types of snack foods with it. By the 1950s, snacking had become an all-American pastime, becoming an internationally recognized emblem of middle American life.[3]		Healthy snacks include those that have significant vitamins, are low in saturated fat and added sugars, and have a low sodium content. [4] Examples of healthy snacks include:		Government bodies like Health Canada are recommending that people make a conscious effort to eat more healthy, natural snacks – such as fruit, vegetables, nuts and cereal grains – while avoiding high-calorie, low-nutrient junk food.[6]		A 2010 study showed that children in the United States snacked on average six times per day, approximately twice as often as American children in the 1970s.[7] This represents consumption of roughly 570 calories more per day than U.S. children consumed in the 1970s.[8]		A Tufts University, Department of Psychology empirical study titled, "Effect of an afternoon confectionery snack on cognitive processes critical to learning" found that a consumption of a confectionery snack in the afternoon improved spatial memory in the study's sample group, but in the area of attention performance had a mixed effect.[9]		A rack of snack foods		Popcorn		Trail mix		Cheese-flavored crackers of the Cheez-It brand		Candy		A candy bar of the Snickers brand		Chocolate chip cookie		Fruit		Potato chips		Pretzels		Doughnuts		A blueberry muffin		Ants on a log		Dutch bitterballen		Peanuts		
Force-feeding is the practice of feeding a human or other animal against their will. The term "gavage" ( /ɡəˈvɑːʒ/) refers to the supplying of a nutritional substance by means of a small plastic feeding tube passed through the nose (nasogastric) or mouth (orogastric) into the stomach. In hospitals, some psychiatric patients can also be restrained so that sedatives can be injected into them; this happens if patients have been non-compliant with their instructions.						Soviet dissident Vladimir Bukovsky described how he was force-fed: "The feeding pipe was thick, thicker than my nostril, and would not go in. Blood came gushing out of my nose and tears down my cheeks, but they kept pushing until the cartilages cracked. I guess I would have screamed if I could, but I could not with the pipe in my throat. I could breathe neither in nor out at first; I wheezed like a drowning man — my lungs felt ready to burst. The doctor also seemed ready to burst into tears, but she kept shoving the pipe farther and farther down. Only when it reached my stomach could I resume breathing, carefully. Then she poured some slop through a funnel into the pipe that would choke me if it came back up. They held me down for another half-hour so that the liquid was absorbed by my stomach and could not be vomited back, and then began to pull the pipe out bit by bit."[2]		"The unfortunate patients had their mouth clamped shut, had a rubber tube inserted into their mouth or nostril. They keep on pressing it down until it reaches your esophagus. A china funnel is attached to the other end of the tube and a cabbage-like mixture poured down the tube and through to the stomach. This was an unhealthy practice, as the food might have gone into their lungs and caused pneumonia."[3]		Some countries force-feed prisoners when they go on hunger strike. It has been prohibited since 1975 by the Declaration of Tokyo of the World Medical Association, provided that the prisoner is "capable of forming an unimpaired and rational judgment". The violation of this prohibition may be carried out in a manner that can be categorised as torture, as it may be extremely painful and result in severe bleeding and spreading of various diseases via the exchanged blood and mucus, especially when conducted with dirty equipment on a prison population.[4] Large feeding pipes are traditionally used on hunger striking prisoners[5] whereas thin pipes are preferred in hospitals.		Suffragettes who had been imprisoned while campaigning for votes for women went on hunger strike and were force fed. (This lasted until the Prisoners (Temporary Discharge for Ill Health) Act of 1913, better known as the Cat and Mouse Act, whereby debilitated prisoners would be released, allowed to recover, and then re-arrested.) Rubber tubes were inserted through the mouth (only occasionally through the nose) and into the stomach, and food poured down; the suffragettes were held down by force while the instruments were inserted into their bodies, an experience which has been likened to rape.[6] In a smuggled letter, Sylvia Pankhurst described how the warders held her down and forced her mouth open with a steel gag. Her gums bled, and she vomited most of the liquid up afterwards.[7]		Emmeline Pankhurst, founder of the Women's Social and Political Union, was horrified by the screams of women being force-fed in HM Prison Holloway. She wrote: "Holloway became a place of horror and torment. Sickening scenes of violence took place almost every hour of the day, as the doctors went from cell to cell performing their hideous office. …I shall never while I live forget the suffering I experienced during the days when those cries were ringing in my ears." When prison officials tried to enter her cell, Pankhurst, in order to avoid being force-fed, raised a clay jug over her head and announced: "If any of you dares so much as to take one step inside this cell I shall defend myself."[8]		Djuna Barnes, the American journalist, agreed to submit force-feeding for a 1914 New York World magazine article. Barnes wrote, "If I, play acting, felt my being burning with revolt at this brutal usurpation of my own functions, how they who actually suffered the ordeal in its acutest horror must have flamed at the violation of the sanctuaries of their spirits." She concluded, "I had shared the greatest experience of the bravest of my sex."[9]		The United Kingdom also used forcible feeding techniques against Irish Republicans during their struggle for independence. In 1917 Irish prisoner Thomas Ashe died as a result of complications from such a feeding while incarcerated at Dublin's Mountjoy Jail.[10]		Under United States jurisdiction, force-feeding is frequently used in the U.S. military prison in Guantanamo Bay, prompting in March 2006 an open letter by 250 doctors in The Lancet, warning that, in their opinion, the participation of any doctor is contrary to the rules of the World Medical Association.[11][5][12] Retired Major General Paul E. Vallely visited Guantanamo and reported on the process of force-feeding:[13]		They have to restrain the prisoners when they feed them because they attack the nurses. They spit in their faces. They're simply restrained for 20 minutes so they can be fed Ensure. They get their choice of four flavors of Ensure. It's put in a very unobtrusive feeding tube smaller than a normal straw and it's put in there for 20 minutes, so they get breakfast, lunch, and dinner.		In the 2009 case Lantz v. Coleman,[14] the Connecticut Superior Court authorized the state Department of Correction to force-feed a competent prisoner who had refused to eat voluntarily.[15] In 2009, terrorist Richard Reid, known as the "shoe bomber," was force-fed while on a hunger strike at the United States Penitentiary, Florence ADX, the federal supermax prison in Colorado.[16] Hundreds of force-feedings have been reported at ADX Florence.[17] Ethel Byrne was the first female political prisoner in the United States to be subjected to force feeding [18] after she was jailed at Blackwell Island workhouse on January 22, 1917 for her activism in advocating for the legalization of birth control. She subsequently went on a hunger strike and refused to drink water for 185 hours. [19]		On December 6, 2006, the United Nations War Crimes Tribunal at The Hague approved the use of force-feeding of Serbian politician Vojislav Šešelj. They decided it was not "torture, inhuman or degrading treatment if there is a medical necessity to do so... and if the manner in which the detainee is force-fed is not inhuman or degrading".[20]		In 2015, the Knesset passed a law allowing the force-feeding of prisoners in response to a hunger strike by a Palestinian terrorist who had been held for months in administrative detention. Israel doctors refused to feed Mohammad Allan against his will, and he resumed eating after the Supreme Court temporarily released him.[21]		Force-feeding of pernicious substances may be used as a form of torture and/or physical punishment. While in prison in northern Bosnia in 1996, some Serbian prisoners have described being forced to eat paper and soap.[22]		Sometimes it has been alleged that prisoners are forced to eat foods forbidden by their religion. The Washington Post has reported that Muslim prisoners in Abu Ghraib prison under the U.S.-led coalition described in sworn statements having been forced to eat pork and drink alcohol, both of which are strictly forbidden in Islam (see Abu Ghraib torture and prisoner abuse).[23]		Force-feeding used to be practiced in the Middle East and still is in Mauritania. Fatness was considered a marriage asset in women; culturally, voluptuous figures were perceived as indicators of wealth. In this tradition, some girls are forced by their mothers or grandmothers to overeat, often accompanied by physical punishment (e.g., pressing a finger between two pieces of wood) should the girl not eat. The intended result is a rapid onset of obesity, and the practice may start at a young age and continue for years. This is still the tradition in the rather undernourished Sahel country Mauritania (where it is called leblouh), where it induces major health risks in the female population; some younger men no longer insist on voluptuous brides, but traditional beauty norms remain part of the culture.[24][25]		Force-feeding has been used to prepare animals for slaughter. In some cases, such as is the case with geese raised for foie gras, it is still practiced today.		Force-feeding is also known as gavage, from a French word meaning "to gorge". This term specifically refers to force-feeding of ducks or geese in order to fatten their livers in the production of foie gras. In modern Egypt, the practice of fattening geese and male Muscovy ducks by force-feeding[dubious – discuss] them various grains is present, mostly by individuals, unrelated to foie gras production, but for general consumption of those birds later. It is not widespread on commercial farms however. The term used for such a practice is called "Tazgheet" تزغيط from the verb "Zaghghat" زغَط.		Force-feeding of birds is practiced mostly on geese or male Moulard ducks, a Muscovy/Pekin hybrid. Preparation for gavage usually begins 4–5 months before slaughter. For geese, after an initial free-range period and treatment to assist in esophagus dilation (eating grass, for example), the force-feeding commences. Gavage is performed 2–4 times a day for 2–5 weeks, depending on the size of the fowl, using a funnel attached to a slim metal or plastic feeding tube inserted into the bird's throat to deposit the food into the bird's crop (the storage area in the esophagus). A grain mash, usually maize mixed with fats and vitamin supplements, is the feed of choice. Waterfowl are suited to the tube method due to a non-existent gag reflex and an extremely flexible esophagus, unlike other fowl such as chickens. These migratory waterfowl are also said to be ideal for gavage because of their natural ability to gain large amounts of weight in short periods of time before cold seasons.		Shen Dzu is a similar practice of force-feeding pigs.		Gavage is used in some scientific studies such as those involving the rate of metabolism. It is practiced upon various laboratory animals, such as mice. Liquids such as medicines may be administered to the animals via a tube or syringe.[26]		
Melanin-concentrating hormone (MCH) is a cyclic 19-amino acid orexigenic hypothalamic peptide originally isolated from the pituitary gland of teleost fish where it controls skin pigmentation.		In mammals it is involved in the regulation of feeding behavior, mood, sleep-wake cycle[1] and energy balance. MCH expressing neurons are located within the lateral hypothalamus and zona incerta. Despite this restricted distribution MCH neurons project widely throughout the brain. MCH knockout mice are hypophagic (eat less) and lean and are hyperactive.[2][3] When administered centrally it increases food intake and weight gain.		
Silver service (in British English) is a method of foodservice that usually includes serving food at the table. It is a technique of transferring food from a service dish to the guest's plate from the left.		It is performed by a waiter by using service forks and spoons from the diner's left. In France, it is known as service à l'anglaise ("English service").[1]						Silver service, like all formal food service, is oriented for a right-handed waiter, left-handed waiters may use their right hand: to serve the food, the waiter stands behind the guest and to the guest's left, holds or supports the platter with their left hand and serves the food with their right hand. It is common for the waiter to hold the serving-fork above the serving-spoon both in the right hand, and use the fingers to manipulate the two as a pincer for picking up, holding and transferring the food. This technique or form requires much practice and dexterity.		A modification of silver service is known as butler service.[3]		In butler service, the diner helps himself from a serving plate held by the waiter (butler). Traditionally, this type of service was used on Sunday evenings, when the waiting staff had the evening off and the butler helped out at dinner.[citation needed] In France, this kind of service is known as service à la francaise ("French service").[4]		
À la carte /ɑːləˈkɑːrt/[1] is an English language loan phrase meaning "according to the menu."[2][3] It was adopted from French in the early 19th century and refers to "food that can be ordered as separate items, rather than part of a set meal."[4]		The phrase is used in reference to a menu of items priced and ordered separately, which is the usual operation of restaurants. That is in contrast to a table d'hôte, in which a menu has limited or no choice of items and is served at a fixed price.[5] It may also be used to order an item from the menu on its own: a steak without potatoes and other vegetables is steak à la carte.						The earliest examples of à la carte in writing are from 1816 for the adjectival use ("à la carte meal", for example) and from 1821 for the adverbial use ("meals were served à la carte").[2] These pre-date the use of the word menu which came into English in the 1830s.[6][7][2]		More broadly, the term is not exclusive to food. Today, it can be used in reference to things such as television. To watch television à la carte refers to paying for a provider where the viewer can choose from an option of programs to watch (e.g. Netflix or Hulu), instead of watching from set programs.[8]		
EAT or eat may refer to eating, it is the process of consuming food, for the purpose of providing for the nutritional needs of an animal. The term may also refer to:		
65–75% Sunni Islam[4][note 1] 10–13% Shia Islam[4] 15–20% Non-denominational Islam[5] ~1% Ahmadiyya[6]		A Muslim is someone who follows or practices Islam, a monotheistic Abrahamic religion. Muslims consider the Quran (Koran), their holy book, to be the verbatim word of God as revealed to the Islamic prophet and messenger Muhammad. The majority of Muslims also follow the teachings and practices of Muhammad (sunnah) as recorded in traditional accounts (hadith).[10] "Muslim" is an Arabic word meaning "one who submits (to God)".[11]		The beliefs of Muslims include: that God (Arabic: الله‎‎ Allāh) is eternal, transcendent and absolutely one (tawhid or monotheism); that God is incomparable, self-sustaining and neither begets nor was begotten; that Islam is the complete and universal version of a primordial faith that has been revealed before through many prophets including Abraham, Moses, Ishmael and Jesus;[12] that these previous messages and revelations have been partially changed or corrupted over time (tahrif)[13] and that the Qur'an is the final unaltered revelation from God (The Final Testament).[14]						The religious practices of Muslims are enumerated in the Five Pillars of Islam: the declaration of faith (shahadah), daily prayers (salat), fasting during the month of Ramadan (sawm), almsgiving (zakat), and the pilgrimage to Mecca (hajj) at least once in a lifetime.[15][16]		To become a Muslim and to convert to Islam is essential to utter the Shahada, one of the Five Pillars of Islam, a declaration of faith and trust that professes that there is only one God (Allah) and that Muhammad is God's messenger.[17] It is a set statement normally recited in Arabic: lā ʾilāha ʾillā-llāhu muḥammadun rasūlu-llāh (لَا إِلٰهَ إِلَّا الله مُحَمَّدٌ رَسُولُ الله) "There is no god but God (and) Muhammad is the messenger of God."[18]		In Sunni Islam, the shahada has two parts: la ilaha illa'llah (there is no god but God), and Muhammadun rasul Allah (Muhammad is the messenger of God),[19] which are sometimes referred to as the first shahada and the second shahada.[20] The first statement of the shahada is also known as the tahlīl.[21]		In Shia Islam, the shahada also has a third part, a phrase concerning Ali, the first Shia Imam and the fourth Rashid caliph of Sunni Islam: وعليٌ وليُّ الله (wa ʿalīyyun walīyyu-llāh), which translates to "Ali is the wali of God.[22]		There are customs stating that anyone above the age of fifteen who possesses the faculties of rationality, logic or sanity, but misses numerous successive Friday prayers (jumu'ah) without a valid excuse, no longer qualifies as a Muslim.[23][24]		The word muslim (Arabic: مسلم‎‎, IPA: [ˈmʊslɪm]; English: /ˈmʌzlᵻm/, /ˈmʊzlᵻm/, /ˈmʊslᵻm/ or moslem /ˈmɒzləm/, /ˈmɒsləm/[25]) is the active participle of the same verb of which islām is a verbal noun, based on the triliteral S-L-M "to be whole, intact".[26][27] A female adherent is a muslima (Arabic: مسلمة‎‎) (also transliterated as "Muslimah"[28] ). The plural form in Arabic is muslimūn (مسلمون) or muslimīn (مسلمين), and its feminine equivalent is muslimāt (مسلمات). The Arabic form muslimun is the stem IV participle[note 2] of the triliteral S-L-M.		The ordinary word in English is "Muslim". It is sometimes transliterated as "Moslem", which is an older spelling.[citation needed] The word Mosalman (Persian: مسلمان‎‎, alternatively Mussalman) is a common equivalent for Muslim used in Central Asia. Until at least the mid-1960s, many English-language writers used the term Mohammedans or Mahometans.[29] Although such terms were not necessarily intended to be pejorative, Muslims argue that the terms are offensive because they allegedly imply that Muslims worship Muhammad rather than God.[30] Other obsolete terms include Muslimite[31] and Muslimist.[32]		Musulmán/Mosalmán (Persian: مسلمان‎‎) is a synonym for Muslim and is modified from Arabic. It is the origin of the Spanish word musulmán, the (dated) German Muselmann, the French word musulman, the Polish words muzułmanin and muzułmański, the Portuguese word muçulmano, the Italian word mussulmano or musulmano, the Romanian word musulman and the Greek word μουσουλμάνος (all used for a Muslim).[33] In English it was sometimes spelled Mussulman and has become archaic in usage.		Apart from Persian, Spanish, Polish, Portuguese, Italian, and Greek, the term could be found, with obvious local differences, in Armenian, Dari, Pashto, Urdu, Hindi, Bengali, Marathi, Panjabi, Turkish, Kazakh, Uzbek, Kyrgyz, Azeri, Maltese, Hungarian, Czech, Bosnian, Bulgarian, Russian, Serbian, Ukrainian, Romanian, Dutch, and Sanskrit.		The Muslim philosopher Ibn Arabi said:		A Muslim is a person who has dedicated his worship exclusively to God...Islam means making one's religion and faith God's alone.[34]		The Qur'an describes many prophets and messengers within Judaism and Christianity, and their respective followers, as Muslim: Adam, Noah, Abraham, Jacob, Moses, Ishmael, and Jesus and his apostles are all considered to be Muslims in the Qur'an. The Qur'an states that these men were Muslims because they submitted to God, preached His message and upheld His values, which included praying, charity, fasting and pilgrimage. Thus, in Surah 3:52 of the Qur'an, Jesus' disciples tell him, "We believe in God; and you be our witness that we are Muslims (wa-shahad be anna muslimūn)." In Muslim belief, before the Qur'an, God had given the Tawrat (Torah, Old Testament) to Moses, the Zabur (Psalms) to David and the Injil (Gospel, New Testament) to Jesus, who are all considered important Muslim prophets.		The most populous Muslim-majority country is Indonesia, home to 12.7% of the world's Muslims,[4] followed by Pakistan (11.0%), Bangladesh (9.2%), and Egypt (4.9%).[35] About 20% of the world's Muslims lives in the Middle East and North Africa,[4][36]		Sizable minorities are also found in India, China, Russia, Ethiopia, the Americas, Australia and parts of Europe. The country with the highest proportion of self-described Muslims as a proportion of its total population is Morocco.[1] Converts and immigrant communities are found in almost every part of the world.		Over 75–90% of Muslims are Sunni.[37][38] The second and third largest sects, Shia and Ahmadiyya, make up 10–20%,[39][40] and 1%[6] respectively.		With about 1.6 billion followers, almost a quarter of earth's population,[41][4][42] Islam is the second-largest and the fastest-growing religion in the world.[43] due primarily to the young age and high fertility rate of Muslims,[44] with Muslim having a rate of (3.1) compared to the world average of (2.5). According to the same study religious switching has no impact on Muslim population, since the number of people who embrace Islam and those who leave Islam are roughly equal.[44]		A Pew Center study in 2011, found that Muslims have the highest number of adherents under the age of 15 (or- 34% of the total Muslim population) of any major religion, while only 7% are aged 60+ (the smallest percentage of any major religion). According to the same study Muslims have also the highest fertility rates (3.1) than any other major religious group.[45]		
Table manners are the rules used while eating, which may also include the appropriate use of utensils. Different cultures observe different rules for table manners. Each family or group sets its own standards for how strictly these rules are to be enforced.						Traditionally in Western Europe, the host or hostess takes the first bite[citation needed] unless he or she instructs otherwise. The host begins after all food for that course has been served and everyone is seated. In religious households, a family meal may commence with saying Grace, or at dinner parties the guests might begin the meal by offering some favourable comments on the food and thanks to the host. In a group dining situation it is considered impolite to begin eating before all the group have been served their food and are ready to start.		Napkins should be placed on the lap and not tucked into clothing. They should not be used for anything other than wiping your mouth and should be placed unfolded on the seat of your chair should you need to leave the table during the meal or placed unfolded on the table when the meal is finished.[1]		The fork is held with the left hand and the knife held with the right. The fork is held generally with the tines down[citation needed], using the knife to cut food or help guide food on to the fork. When no knife is being used, the fork can be held with the tines up. With the tines up, the fork balances on the side of the index finger, held in place with the thumb and index finger. Under no circumstances should the fork be held like a shovel, with all fingers wrapped around the base. A single mouthful of food should be lifted on the fork and you should not chew or bite food from the fork. The knife should be held with the base into the palm of the hand, not like a pen with the base resting between the thumb and forefinger. The knife must never enter the mouth or be licked.[1][reliable source??] When eating soup, the spoon is held in the right hand and the bowl tipped away from the diner, scooping the soup in outward movements. The soup spoon should never be put into the mouth, and soup should be sipped from the side of the spoon, not the end.[2] Food should always be chewed with the mouth closed.[3] Talking with food in one's mouth is seen as very rude.[1] Licking one's fingers and eating slowly can also be considered impolite.		Food should always be tasted before salt and pepper are added. Applying condiments or seasoning before the food is tasted is viewed as an insult to the cook, as it shows a lack of faith in the cook's ability to prepare a meal.[4]		Butter should be cut, not scraped, from the butter dish using a butter knife or side plate knife and put it onto a side plate, not spread directly on to the bread. This prevents the butter in the dish from gathering bread crumbs as it is passed around. Bread rolls should be torn with the hands into mouth-sized pieces and buttered individually, from the butter placed on the side plate, using a knife. Bread should not be used to dip into soup or sauces. As with butter, cheese should be cut and placed on your plate before eating.		Only white wine or rosé is held by the stem of the glass; red by the bowl.[citation needed] Pouring one's own drink when eating with other people is acceptable, but it is more polite to offer to pour drinks to the people sitting on either side.[1] Wine bottles should not be upturned in an ice bucket when empty.		It is impolite to reach over someone to pick up food or other items. Diners should always ask for items to be passed along the table to them.[1] In the same vein, diners should pass those items directly to the person who asked.[4] It is also rude to slurp food, eat noisily or make noise with cutlery.		Elbows should remain off the table		When one has finished eating, this should be communicated to other diners and waiting staff by placing the knife and fork together on the plate, at approximately 6 o'clock position, with the fork tines facing upwards.		At family meals, children are often expected to ask permission to leave the table at the end of the meal.		Should a mobile telephone (or any other modern device) ring or if a text message is received, the diner should ignore the call. In exceptional cases where the diner feels the call may be of an urgent nature, he should ask to be excused, leave the room and take the call (or read the text message) out of earshot of the other diners. Placing a phone, keys, handbag or wallet on the dinner table is considered rude.[citation needed]		Modern etiquette provides the smallest numbers and types of utensils necessary for dining. Only utensils which are to be used for the planned meal should be set. Even if needed, hosts should not have more than three utensils on either side of the plate before a meal. If extra utensils are needed, they may be brought to the table along with later courses.[5]		A table cloth extending 10–15 inches past the edge of the table should be used for formal dinners, while placemats may be used for breakfast, lunch, and informal suppers.[6] Candlesticks, even if not lit, should not be on the table while dining during daylight hours.[7]		Men's and unisex hats should never be worn at the table. Ladies' hats may be worn during the day if visiting others.[8]		Phones and other distracting items should not be used at the dining table. Reading at a table is permitted only at breakfast, unless the diner is alone.[9] Urgent matters should be handled, after an apology, by stepping away from the table.		If food must be removed from the mouth for some reason—a pit, bone, or gristle—the rule of thumb according to Emily Post, is that it comes out the same way it went in. For example, if olives are eaten by hand, the pit may be removed by hand. If an olive in a salad is eaten with a fork, the pit should be deposited back onto the fork inside one's mouth, and then placed onto a plate. The same applies to any small bone or piece of gristle in food. A diner should never spit things into a napkin, certainly not a cloth napkin. Since the napkin is always laid in the lap and brought up only to wipe one's mouth, hidden food may be accidentally dropped into the lap or onto the host's floor. Food that is simply disliked should be swallowed.[10]		The fork may be used in the American style (in the left hand while cutting and in the right hand to pick up food) or the European Continental style (fork always in the left hand). (See Fork etiquette) The napkin should be left on the seat of a chair only when leaving temporarily.[11] Upon leaving the table at the end of a meal, the napkin is placed loosely on the table to the left of the plate.[12]		In formal settings, the host asks the guests to start the meal. Similarly, one should not leave the table before the host or the eldest person finishes his or her food. It is also considered impolite to leave the table without asking for the host's or the elder's permission. Normally whoever completes first will wait for others and after everybody is finished all leave the table.[13]		In a traditional Indian meal setup, the following is observed. Normally the plate is served with small quantities of all the food items.		A cardinal rule of dining is to use the right hand when eating or receiving food.[14] Hand washing, both before sitting at a table and after eating, is important. Cleaning with cloth or paper tissue may be considered unhygienic.[15]		Small amounts of food are taken at a time, ensuring that food does not reach the palms of the hands. It is considered important to finish each item on the plate out of respect for the food being served.[16] Traditionally, food should be eaten as it is served, without asking for salt or pepper. It is however, now acceptable to express a personal preference for salt or pepper and to ask for it.		Distorting or playing with food is unacceptable. Eating at a moderate pace is important, as eating too slowly may imply a dislike of the food and eating too quickly is considered rude. Generally it is not acceptable to burp, slurp, or spit. Staring at another diner's plate is also considered rude. It is inappropriate to make sounds while chewing. Certain Indian food items can create sounds, so it is important to close the mouth and chew at a moderate pace.		At the dining table, attention must be paid to specific behaviors that may indicate distraction or rudeness. Answering phone calls, sending messages and using inappropriate language are considered inappropriate while dining and while elders are present.		Seating and serving customs play important roles in Chinese dining etiquette. For example, the diners should not sit down or begin to eat before the host (or guest of honor) has done so. When everyone is seated, the host offers to pour tea, beginning with the cup of the eldest person. The youngest person is served last as a gesture of respect for the elders.		Just as in Western cultures, communal utensils (chopsticks and spoons) are used to bring food from communal dishes to an individual's own bowl (or plate). It is considered rude and unhygienic for a diner to use his or her own chopsticks to pick up food from communal plates and bowls when such utensils are present. Other potentially rude behaviors with chopsticks include playing with them, separating them in any way (such as holding one in each hand), piercing food with them, or standing them vertically in a plate of food. (The latter is especially rude, evoking images of incense or 'joss' sticks used ceremoniously at funerals).[17] A rice bowl may be lifted with one hand to scoop rice into the mouth with chopsticks. It is also considered rude to look for a piece one would prefer on the plate instead of picking up the piece that is closest to the diner as symbol of fairness and sharing to the others.		The last piece of food on a communal dish is never served to oneself without asking for permission. When offered the last bit of food, it is considered rude to refuse the offer. It is considered virtuous for diners to not leave any bit of food on their plates or bowls. Condiments, such as soy sauce or duck sauce, may not be routinely provided at high-quality restaurants. The assumption is that perfectly prepared food needs no condiments and the quality of the food can be best appreciated.		In formal settings, a meal is commenced when the eldest or most senior diner at the table partakes of any of the foods on the table. Before partaking, intention to enjoy their meal should be expressed. Similarly, satisfaction or enjoyment of that meal should be expressed at its completion. On occasion, there are some dishes which require additional cooking or serving at the table. In this case, the youngest or lowest-ranked adult diner should perform this task. When serving, diners are served food and drink in descending order starting with the eldest or highest-ranked diner to the youngest or lowest-ranked.		Usually, diners will have a bowl of soup on the right with a bowl of rice to its left. Alternatively, soup may be served in a single large communal pot to be consumed directly or ladled into individual bowls. Dining utensils will include a pair of chopsticks and a spoon. Common chopstick etiquette should be followed (See Chopstick Etiquette), but rice is generally eaten with the spoon instead of chopsticks (as eating rice with chopsticks is considered rude). Often some form of protein (meat, poultry, fish) will be served as a main course and placed at the center of the table within reach of the diners. Banchan will also be distributed throughout the table. If eaten with spoon, banchan is placed on the spoonful of rice before entering the mouth. With chopsticks, however, it is fed to the mouth directly. The last piece of food on a communal dish should not be served to oneself without first asking for permission, but, if offered the last bit of food in the communal dish, it is considered rude to refuse the offer. Bowls of rice or soup should not be picked up off the table while dining, an exception being made for large bowls of Korean noodle soup. Slurping while eating noodles and soup is generally acceptable. It is not uncommon to chew with the mouth open.		If alcohol is served with the meal, it is common practice that when alcohol is first served for the eldest/highest-ranked diner to make a toast and for diners to clink their glasses together before drinking. The clinking of glasses together is often done throughout the meal. A diner should never serve alcohol to themselves. Likewise, it is considered rude to drink alone. Instead, keep pace with other diners and both serve and be served the alcohol. Alcohol should always be served to older and higher-ranked diners with both hands, and younger or lower-ranked diners may turn their face away from other diners when drinking the alcohol.		
The kids' meal or children's meal is a fast food combination meal tailored to and marketed to youngsters. Most kids' meals come in colourful bags or cardboard boxes with depictions of activities on the bag or box and a plastic toy inside.[1][2] The standard kids' meal comprises a burger, a side item, and a soft drink.[2]						The first kids' meal, Funmeal,[3] emerged at Burger Chef in 1973 and succeeded. Discerning the popularity of the kids' meal, McDonald's introduced its Happy Meal in 1978,[note 1] and other fast food corporations, including Burger King, followed suit with their own kids' meals.[5]		Some fast food corporations considered youngsters their "most important" customers, owing to the success of the kids' meal.[5] Their effectiveness has been ascribed to the fact that the patronage of youngsters often means the patronage of a family and to the allure of the toys, which often are in collectable series.[1] In 2006, $360 million of the expenditures of fast food corporations was for toys in kids' meals, which numbered over 1.2 billion.[5]		In recent years, the popularity of the kids' meal has receded, with a study by NPD Group indicating that there was a 6% decrease in kids' meals sales in 2011.[6] Explanations include parents' realization that kids' meals are unhealthy, parents' desire to save money (opting instead to order from the value menu), as well as kids' outgrowing the meals earlier than before. Youngsters have "become more sophisticated in their palates" and seek items from the regular menu but in smaller servings.[6] Kids' meal toys are also no longer appealing to the increasingly technology-oriented youth,[7] who prefer video games.[8] Also, the kids meal toys of Wendy's and Chick-Fil-A are no longer appealing to ages 8–12.		Kids' meals have evolved in response to critics, offering healthier selections and greater variety.[8] In 2011, nineteen food chains participating in the Kids Live Well initiative—including Burger King, Denny's, IHOP, Chili's, Friendly's, Chevy's, and El Pollo Loco—pledged to "offer at least one children's meal that has fewer than 600 calories, no soft drinks and at least two items from the following food groups: fruits, vegetables, whole grains, lean proteins or low-fat dairy".[9]		There have been concerns from food critics about the nutritional value of the kids' meal. A 2010 study by the Rudd Center for Food Policy and Obesity inspecting the kids' meals of twelve US food chains[10] concluded that of 3,039 entrée combinations, twelve satisfied the advised levels of fat, sodium, and calories for preschool kids and fifteen those for older kids.[5]		Burger King has run kids meal promotions featuring toys of characters from PG-13 movies at least four times, such as Small Soldiers in 1998,[11] Star Wars: Episode III – Revenge of the Sith in 2005,[12] and the Simpsons movie in 2007,[13] One of the McDonald's toys featuring Minions characters in 2015 has caused controversy because parents have confused the gibberish talking minion with profanity. As a result, the minion toys promotion was ended early, in July 2015.[14][15]		In the United States, kids' meals have been blamed for ingraining unhealthy dietary habits in youngsters and augmenting child obesity.[16] In 2010, Santa Clara County, California implemented a ban on toys accompanying kids' meals that fail nutritional standards.[17] San Francisco County enacted the same ban,[18] and similar ones have been proposed or considered in other cities or states across the country.[note 2] Conversely, legislators in Arizona prohibited such restrictions, and Florida state senators proposed the same.[22]		Outside the United States, Spain[23] and Brazil[24] have also considered such measures. Chile has banned toys in kids' meals altogether.[25]		
A dining room is a room for consuming food. In modern times it is usually adjacent to the kitchen for convenience in serving, although in medieval times it was often on an entirely different floor level. Historically the dining room is furnished with a rather large dining table and a number of dining chairs; the most common shape is generally rectangular with two armed end chairs and an even number of un-armed side chairs along the long sides.						In the Middle Ages, upper class Britons and other European nobility in castles or large manor houses dined in the great hall. This was a large multi-function room capable of seating the bulk of the population of the house. The family would sit at the head table on a raised dais, with the rest of the population arrayed in order of diminishing rank away from them. Tables in the great hall would tend to be long trestle tables with benches. The sheer number of people in a Great Hall meant it would probably have had a busy, bustling atmosphere. Suggestions that it would also have been quite smelly and smoky are probably, by the standards of the time, unfounded. These rooms had large chimneys and high ceilings and there would have been a free flow of air through the numerous door and window openings.		It is true that the owners of such properties began to develop a taste for more intimate gatherings in smaller 'parlers' or 'privee parlers' off the main hall but this is thought to be due as much to political and social changes as to the greater comfort afforded by such rooms. Over time, the nobility took more of their meals in the parlour, and the parlour became, functionally, a dining room (or was split into two separate rooms). It also migrated farther from the Great Hall, often accessed via grand ceremonial staircases from the dais in the Great Hall. Eventually dining in the Great Hall became something that was done primarily on special occasions.		Toward the beginning of the 18th Century, a pattern emerged where the ladies of the house would withdraw after dinner from the dining room to the drawing room. The gentlemen would remain in the dining room having drinks. The dining room tended to take on a more masculine tenor as a result.		A typical North American dining room will contain a table with chairs arranged along the sides and ends of the table, as well as other pieces of furniture, (often used for storing formal china), as space permits. Often tables in modern dining rooms will have a removable leaf to allow for the larger number of people present on those special occasions without taking up extra space when not in use. Although the "typical" family dining experience is at a wooden table or some sort of kitchen area, some choose to make their dining rooms more comfortable by using couches or comfortable chairs.		In modern American and Canadian homes, the dining room is typically adjacent to the living room, being increasingly used only for formal dining with guests or on special occasions. For informal daily meals, most medium size houses and larger will have a space adjacent to the kitchen where table and chairs can be placed, larger spaces are often known as a dinette while a smaller one is called a breakfast nook.[1] Smaller houses and condos may have a breakfast bar instead, often of a different height than the regular kitchen counter (either raised for stools or lowered for chairs). If a home lacks a dinette, breakfast nook, or breakfast bar, then the kitchen or family room will be used for day-to-day eating.		This[clarification needed] was traditionally the case in Britain, where the dining room would for many families be used only on Sundays, other meals being eaten in the kitchen.		In Australia, the use of a dining room is still prevalent, yet not an essential part of modern home design. For most, it is considered a space to be used during formal occasions or celebrations. Smaller homes, akin to the USA and Canada, use a breakfast bar or table placed within the confines of a kitchen or living space for meals.		
An hors d'oeuvre (/ɔːr ˈdɜːrv, ˈdɜːrvrə/; French: hors d'œuvre, [also hors d'oevre] [ɔʁ dœvʁ] ( listen)), appetizer,[1] or starter[2] is a small dish served before a meal.[3] Some hors d'oeuvres are served cold, others hot.[4] Hors d'oeuvres may be served at the dinner table as a part of the meal, or they may be served before seating. Formerly, hors d'oeuvres were also served between courses.[5]		Typically smaller than a main dish, it is often designed to be eaten by hand (with minimal use of cutlery).[6]						Hors d'oeuvre [hors d'oevre] in French means "outside the work"—that is, "not part of the ordinary set of courses in a meal".[1][7] The French spelling is the same for singular and plural usage; in English, the ⟨œ⟩ typographic ligature is usually replaced by the digraph ⟨oe⟩, with the plural commonly written hors d'oeuvres and pronounced /ɔːr ˈdɜːrvz/.[citation needed]		"Appetizer" is a synonym for hors d'oeuvre.		"Starter" is sometimes used to denote an hors d'oeuvre, sometimes to denote more substantial courses, known in Europe as entrées.		A small number of food historians believe that the tradition may have begun in Russia, where small snacks of fish, caviar and meats were common after long travels.[8] However, it may be that the custom originated in China, possibly coming through Steppes, into Russia, Scandinavia, France and other European countries. The tradition may have reached Italy, Greece and the Balkan nations through Russia or Persia. Many national customs are related, including the Swedish smörgåsbord, Russian zakuska, Lebanese mezze, and Italian antipasto.[9] During the Roman Period the meal practice was to have two main courses which were supplemented before the meal with small amounts of fish, vegetables, cheeses, olives[10][11] and even stuffed dormice.[12] These would be served at the start of the meal known as either gustatio or promulsis. The Greeks called the appetizer course propoma.[11]		During the Middle Ages formal French meals were served with entremets between the serving of plates. These secondary dishes could be either actual food dishes, or elaborate displays and even dramatic or musical presentations. In the 14th century, recipes for entremets were mostly made with meat, fish, pork and vegetables. By the 15th century the elaborate display and performances were served up between courses, and could be edible or displays of subjects relevant to the host, created in butter sculpture or other types of crafted work.[13] With the introduction in the 17th century of service à la française, where all the dishes are laid out at once in very rigid symmetrical fashion, entremets began to change in meaning but were still mainly savoury. Along with this came elaborate silver and ceramic table displays as well as pièces montées. The entremets were placed between the other dishes within the main work of the meal.[13]		At about this time in the 17th century, smaller dishes began to be served by being placed outside the main work of symmetrically placed dishes. These were known as hors d'oeuvre.[10][13] Hors d'oeuvres were originally served as a canapé of small toasted bread with a savoury topping before a meal.[14] The first mention of the food item was by François Massialot in 1691, mentioned in his book: Le cuisinier roial et bourgeois (The Royal and Bourgeois Cook) and explained as "Certain dishes served in addition to those one might expect in the normal composition of the feast".[15] In the French publication Les plaisirs de la table, Edouard Nignon stated that hors d'oeuvres originated in Asia. He went on to state that the French considered hors-d'oeuvres to be superfluous to a well cooked meal.[16] Service à la française continued in Europe until the early 19th century.[10][13] After the 19th century the entremet would become almost exclusively a sweet dish or dessert with the British custom of the "savoury" being the only remaining tradition of the savoury entremet.[13]		The style of formal dining changed drastically in the 19th century, becoming successive courses served one after the other over a period of time.[10][12] Some traditional hors d'oeuvres would remain on the table throughout the meal. These included olives, nuts, celery and radishes. The changing, contemporary hors d'oeuvres, sometimes called "dainty dishes" became more complicated in preparation. Pastries, with meat and cream sauces among other elaborate items, had become a course served after the soup.[10]		Food in England is heavily influenced by other countries due to the island nation's dependence on importing outside goods and sometimes, inspiration.[17] Many English culinary words and customs have been directly borrowed from the original French (some completely Anglicized in spelling) such as: cuisine, sirloin, pastry and omelette which came from the 18th century and earlier. In the late 19th and early 20th century, even more words, foods and customs from culinary France made their way into England, such as éclair, casserole, à la carte, rôtisserie and hors d'oeuvre.[18]		The custom of the savoury course is of British origin and comes towards the end of the meal, before dessert or sweets[19] or even after the dessert, in contrast to the hors d'oeuvre, which is served before the meal.[20] The British favored the savoury course as a palate cleanser before drinking after the meal, which made the hors d'oeuvre before the meal unnecessary.[21] The savoury is generally small, well spiced and often served hot, requiring cooking just before serving.[19] In the Victorian and Edwardian periods, savouries included such toppings as fried oysters wrapped in bacon, and Scotch woodcock,[14] which was a savoury made of scrambled eggs, cayenne pepper and Gentleman's Relish on buttered toast, served hot.[22] In France, cheese was often part of the savoury course or added with simple fruit as a dessert.[23] A typical Edwardian dinner might consist of up to four courses[24] that include two soups, two types of fish, two meats, ending with several savouries then sweets.[25]		The term appetizer is a synonym for hors d'oeuvre. It was first used in the United States and England simultaneously in 1860. Americans also use the term to define the first of three courses in a meal, which were optional and generally set on the table before guests were seated.[10] Drinks before dinner became a custom towards the end of the 19th century. As this new fashion caught on, the British took inspiration from the French to begin serving hors d'oeuvres before dinner.[26] A cocktail party is considered a small gathering with mixed drinks and light snacks.[27] Hors d'oeuvres may be served as the only food offering at cocktail parties and receptions, where no dinner is served afterward.[28] After the end of prohibition in the United States, the cocktail party gained acceptance.[9][10] Prior to the First World War, American dinner guests would be expected to enter the dining room immediately where drinks would be served at the table with appetizers. This changed by the 1920s, when hors d'oeuvres were served prior to a non-alcoholic cocktail; however, after the repeal of Prohibition in the United States, cocktail parties became popular with many different hors d'oeuvres meant as something to help counter the stronger drinks.[10][12] It is the cocktail party that helped transfer the hors d'oeuvres from the formal dining table to the mobility of the serving tray. These appetizers passed around the cocktail party may also be referred to as canapés.[12]		In restaurants or large estates, hors d'oeuvres are prepared in a garde manger which is a cool room.[29][page needed] Hors d'oeuvres are often prepared in advance. Some types may be refrigerated or frozen and then precooked and then reheated in an oven or microwave oven as necessary before serving.[30]		If there is an extended period between when guests arrive and when the meal is eaten, for example during a cocktail hour, these might serve the purpose of sustaining guests during the wait, in the same way that apéritifs are served as a drink before meals.[31]		It is also an unwritten rule that the dishes served as hors d'oeuvres do not give any clue to the main meal.[32] They are served with the main meal menu in view either in hot, room temperature or cold forms; when served hot they are brought out after all the guests arrive so that everyone gets to taste the dishes.		Hors d'oeuvres before a meal may be rotated by waiters or passed. Stationary hors d'oeuvres served at the table on a tray may be referred to as table hors d'oeuvres or as buffet-style.[33] Passed hors d'oeuvres provided by servers are part of butler-style service.[33] or butlered hors d'oeuvres.[34]		Though any food served before the main course is technically an hors d'oeuvre, the phrase is generally limited to individual items, such as cheese or fruit. A glazed fig topped with mascarpone and wrapped with prosciutto is an hors d'oeuvre, and plain figs served on a platter may also be served as hors d'oeuvres.[35] It could be pickled beets or anchovy eggs as topping over tomatoes as part of the initial "drinks" session such as of alcoholic or non alcoholic beverages. They are also served in the forms of dips, spreads, pastries, olives or nuts with or without a base of egg, cheese, meats, vegetables, seafood or breads.[30] Single cold items served are smoked salmon, avocado pear, caviar, pâté, shellfish cocktails and melon with garnishes and decorations. Seasoned hot dishes served are of vegetables, meat, fish, egg, pasta, cheese, soufflés, tartlets, puff pastry or choux pastry.[36]		Appetizers in a restaurant		Tomato bruschetta		Various crudités		Deviled eggs, a cold hors d'oeuvre		Obložené chlebíčky, a Czech and Slovak appetizer or snack		A selection of modern hors-d'oeuvres		Hors d'oeuvres in Azerbaijani cuisine		In Mexico botanas refers to the vegetarian varieties[48] commonly served in small portions in wine bars.[49] In many Central American countries, hors d'oeuvres are known as bocas (lit. "mouthfuls").[50] Pasapalos (lit. "drink passer") is Venezuelan for an hors d'oeuvre.[51]		In Arabic moqabbelat (مقبلات, "things which make one accept what is to come". From root قبل lit. "to accept") is the term for an hors d'oeuvre.[52] In India it is known as chaat which is served throughout the day.[53] Dahi puri is another snack from India which is especially popular from the city of Mumbai in the state of Maharashtra. Chaat is the snack food consumed separately and not part of main course meals.[54] Zensai (前菜, lit. before dish) or zenzai is Japanese for an hors d'oeuvre; more commonly, ōdoburu (オードブル), which is a direct transcription of hors d'oeuvre, is used.[55][56][57] In Korea, banchan (반찬) is a small serving of vegetables, cereals or meats. Additional Korean terms for hors d'oeuvres include jeonchae (전채), meaning "before dish" or epita-ijeo (에피타이저), meaning "appetizer".[53] In Vietnamese Đồ nguội khai vị ("cold plate first course") is the name for an hors d'oeuvre.[citation needed] In Mandarin, lěng pán 冷盘 ("cold plate") or qián cài 前菜 ("before dish") are terms used for hors d'oeuvres, which are served in steamer baskets or on small plates. [53] Meze is a selection of small dishes[58] served in Mediterranean cuisine, Middle Eastern cuisine, and Balkan cuisine. Mezedakia is a term for small mezes.[58] Pembuka (lit. "opening") is Indonesian for an hors d'oeuvre.[59] Yemekaltı is Turkish for an hors d'oeuvre.[60] Zakuskis are hors d'oeuvres in Russian cuisine and other post-Soviet cuisines, served in the form of a buffet of cured meats and fishes.[53] Caviar served in Iran and Russia is the traditional roe from wild sturgeon in the Caspian and Black Seas. [61]		Chaat, a starter in Indian cuisine		A sampling of starters in Northern Thai cuisine (Lanna cuisine)		Zensai in Japanese cuisine		In England, devils on horseback is a hot hors d'oeuvre in different recipes, but in general they are a variation on angels on horseback, made by replacing oysters with dried fruit. The majority of recipes contain a pitted date (though prunes are sometimes used).[62] Starters is the colloquial term for hors d'oeuvres in the UK, Ireland and India.[63] Crudités from France are a blend of salads of raw vegetables and the serving has a minimum of three vegetables of striking colors.[64][65] In Italian antipasto means it is served cold in the form of olive, cheese, pickled vegetables[53] or entrada (Portuguese, Spanish) are served as hors d'oeuvres in Southern Europe.[66][67] Voorgerecht in Dutch means the dish ("gerecht") before ("voor") the main course.[68] Fattoush is a bread salad in Levantine cuisine made from toasted or fried pieces of pita bread (khubz 'arabi) combined with mixed greens and other vegetables. It belongs to the family of dishes known as fattat (plural) or fatta, which use stale flatbread as a base.[69]		Various hors d'oeuvres at a banquet of Romanian cuisine		An appetizer served at a restaurant serving Swiss cuisine		Typical Carinthian "brettljause", composed of different kinds of cold meat, horseradish, hard-boiled egg, meat paste, vegetables, butter and curd cheese		In the United States the custom appears to have come from California, where a foreign saloon owner may have put out trays of simple hors d'oeuvres to serve his customers. This tradition soon became the 5-cent beer and free lunch in early America before prohibition ended the custom.[9]		In the U.S., appetizers,[70] referring to anything served before a meal, is the most common term for hors d'oeuvres. Light snacks served outside of the context of a meal are called hors d'oeuvres (with the English-language pluralization). Chicken fingers from the United States are made from chicken meat from the pectoralis minor muscles of the animal. These strips of white meat are located on either side of the breastbone, under the breast meat (pectoralis major).[71][72]		In the Hawaiian language hors d'oeuvres and appetizers are called pūpū.[73] Hawaìian culinary influences are very diverse due to the multiple ethnicities living in the islands. This diversity, along with the Americanization of entertaining in the mid 20th century led to the Hawaiian Cocktail and the pūpū (hors-d'oeuvre) served at the beginning of luaus.[74] This invention of a faux Polynesian experience is heavily influenced by Don the Beachcomber, who is credited for the creation of the pūpū platter and the drink named the Zombie for his Hollywood restaurant.[75][76] At Don's the food was traditional Cantonese cuisine served with a fancy presentation. The first pūpū platters were eggrolls, chicken wings, spare ribs as well as other Chinese-American foods.[77] Eventually Trader Vic would create the Mai Tai in his restaurants in the San Francisco Bay Area and the Tiki bar would become an American cocktail tradition.[76]		Hors d'oeuvres, also called amuse-bouches, served around bars in Australia are oysters and alsace foie gras.[78] Appetizers in New Zealand are lamb skewer or blue cod sliders.[79] In New Zealand the Māori call their snacks Kai Timotimo.[80]Kiribati appetizers served include pastes made from chickpeas and eggplant, meat dishes with spices and wheat.[81] Samoan foil chicken and roast pork, tidbits of meat in a smoky, spicy sauce are appetizers in Samoa.[82] In Tonga, puu-puus or appetizers served are Waikiki shrimp and grilled pineapple with dipping sauce.[83]		In Algeria, apart from fish, a dozen different hors d'oeuvres served include artichokes à la grecque, Algerian salad, salami sausage and prawns.[84] Appetizers served in Kenya are raw and fresh vegetables and assorted dips with decorations.[85] Before modern-day hors d'oeuvre were introduced from Europe into South Africa, starters served consisted of eastern fish sambals and cooked bone marrow served with bread.[86]		
A value meal is a group of menu items at a restaurant offered together at a lower price than they would cost individually. They are common at fast food restaurants. Value meals are a common merchandising tactic to facilitate bundling, up-selling, and price discrimination. The perceived creation of a "discount" on individual menu items in exchange for the purchase of a "meal" is also consistent with the Loyalty Marketing school of thought.[1] Additionally, the term is based on value theory, which utilizes certain marketing tactics to encourage people to spend more money than they originally intended on their purchase.		
A picnic is an excursion at which a meal is eaten outdoors (al fresco), ideally taking place in a scenic landscape such as a park, beside a lake, or with an interesting view and possibly at a public event such as before an open-air theatre performance, and usually in summer. Picnics are usually meant for the late mornings or midday breakfasts, but of course could also be held as a luncheonette or a dinner event. Descriptions of picnics show that the idea of a meal that was jointly contributed and was enjoyed out-of-doors was essential to a picnic from the early 19th century.[1]		Picnics are often family-oriented but can also be an intimate occasion between two people or a large get together such as company picnics and church picnics. It is also sometimes combined with a cookout, usually a form of barbecue; either grilling (griddling, gridironing, or charbroiling), braising (by combining a charbroil or gridiron grill with a broth-filled pot), baking, or a combination of all of the above.		On romantic and family picnics, a picnic basket and a blanket (to sit or recline on) are usually brought along. Outdoor games or some other form of entertainment are common at large picnics. In established public parks, a picnic area generally includes picnic tables and possibly other items related to eating outdoors, such as built-in grills, water faucets, garbage containers, and restrooms.		Some picnics are a potluck, an entertainment at which each person contributed some dish to a common table for all to share. When the picnic is not also a cookout, the food eaten is rarely hot, instead taking the form of deli sandwiches, finger food, fresh fruit, salad, cold meats and accompanied by chilled wine or champagne or soft drinks.						The first usage of the word is traced to the 1692 edition of Tony Willis, Origines de la Langue Française, which mentions pique-nique as being of recent origin; it marks the first appearance of the word in print. The term was used to describe a group of people dining in a restaurant who brought their own wine. The concept of a picnic long retained the connotation of a meal to which everyone contributed something. Whether picnic is actually based on the verb piquer which means 'pick' or 'peck' with the rhyming nique meaning "thing of little importance" is doubted; the Oxford English Dictionary says it is of unknown provenance.[2]		The word picnic first appeared in English in a letter of the Gallicized Lord Chesterfield in 1748 (OED), who associates it with card-playing, drinking and conversation, and may have entered the English language from this French word.[3] The practice of an elegant meal eaten out-of-doors, rather than an agricultural worker's dinner in a field, was connected with respite from hunting from the Middle Ages; the excuse for the pleasurable outing of 1723 in François Lemoyne's painting (illustration, left) is still offered in the context of a hunt.		In approximately 1999 rumors began to be spread claiming a racist origin for the word "picnic", and connecting it with the lynching of blacks in the American South. Despite the spurious etymology behind this rumor having been thoroughly debunked, it occasionally resurfaces.[4]		Though it may have appeared in a 17th century dictionary as "pique-nique," the actual usage began as "pique un niche" meaning to "pick a place," an isolated spot (a nest) where family or friends could enjoy a meal together away from the distractions, demands, and public nature of a communal life. The term morphed into "pique-nique" and after years of usage entered the official French language.		After the French Revolution in 1789, royal parks became open to the public for the first time. Picnicking in the parks became a popular activity amongst the newly enfranchised citizens.		Early in the 19th century, a fashionable group of Londoners (including Edwin Young) formed the 'Picnic Society'. Members met in the Pantheon on Oxford Street. Each member was expected to provide a share of the entertainment and of the refreshments with no one particular host. Interest in the society waned in the 1850s as the founders died.[5]		From the 1830s, Romantic American landscape painting of spectacular scenery often included a group of picnickers in the foreground. An early American illustration of the picnic is Thomas Cole's The Pic-Nic of 1846 (Brooklyn Museum of Art).[6] In it, a guitarist serenades the genteel social group in the Hudson River Valley with the Catskills visible in the distance. Cole's well-dressed young picnickers having finished their repast, served from splint baskets on blue-and-white china, stroll about in the woodland and boat on the lake.		The image of picnics as a peaceful social activity can be utilised for political protest, too. In this context, a picnic functions as a temporary occupation of significant public territory. A famous example of this is the Pan-European Picnic held on both sides of the Hungarian/Austrian border on the 19 August 1989 as part of the struggle towards German reunification.		In 2000, a 600-mile-long picnic took place from coast to coast in France to celebrate the first Bastille Day of the new Millennium. In the United States, likewise, the 4 July celebration of American independence is a popular day for a picnic. In Italy, the favorite picnic day is Easter Monday.		A book of verse beneath the bough, A loaf of bread, a jug of wine, and thou Beside me singing in the Wilderness – Ah, wilderness were paradise enow!		"The Rat brought the boat alongside the bank, tied it up, helped awkward Mole safely ashore, and swung out the picnic basket. The Mole begged to be allowed to unpack it all by himself. He took out all the mysterious packets one by one and arranged their contents, gasping 'Oh my! Oh my!' at each fresh surprise."		A table on the dock awaits picnickers at Lake Providence, Louisiana, 2013.		United States Army officers and their families having a picnic at Fort Thomas, Arizona, in 1886		Picnic in a wooded area (Harry Walker, photographer, circa 1900–1949)		Picnic area in Haute-Savoie (east of France), autumn 2012		Picnic area next to the Nature Center at YMCA Camp Bernie		Picnic shelter at Indian Springs State Park		
Food presentation is the art of modifying, processing, arranging, or decorating food to enhance its aesthetic appeal.		The visual presentation of foods is often considered by chefs at many different stages of food preparation, from the manner of tying or sewing meats, to the type of cut used in chopping and slicing meats or vegetables, to the style of mold used in a poured dish. The food itself may be decorated as in elaborately iced cakes, topped with ornamental sometimes sculptural consumables, drizzled with sauces, sprinkled with seeds, powders, or other toppings, or it may be accompanied by edible or inedible garnishes.		Historically, the presentation of food has been used as shows of wealth and power. Such displays often emphasize the complexity of a dishes composition as opposed to its flavors. For instance, ancient sources recall the hosts of Roman banquets adding precious metals and minerals to food in order to enhance its aesthetic appeal. Additionally, Medieval aristocrats hosted feasts involving sculptural dishes and shows of live animals. These banquets existed to show the culture and affluence of its host, and were therefore tied to social class. Contemporary food aesthetics reflect the autonomy of the chef, such as in nouvelle cuisine and Japanese bento boxes. Dishes often involve both simplistic and complex designs. Some schools of thought, like French nouvelle cuisine, emphasize minimalism while others create complicated compositions based on modern aesthetic principles.[1] Overall, the presentation of food reflects societal trends and beliefs.						The arrangement and overall styling of food upon bringing it to the plate is termed plating.[1] Some common styles of plating include a 'classic' arrangement of the main item in the front of the plate with vegetables or starches in the back, a 'stacked' arrangement of the various items, or the main item leaning or 'shingled' upon a vegetable bed or side item.[2] Item location on the plate is often referenced as for the face of a clock, with six o'clock the position closest to the diner. A basic rule of thumb upon plating, and even in some cases prepping, is to make sure you have the 5 components to a dish; protein, traditionally at a 6 o'clock position, vegetable, at a 2 o'clock position, starch at an 11 o'clock position, sauce and garnish.		Banquets were important social events, usually hosted in private residences for friends and clients. The Romans placed great focus on the appearance of their dining room (triclinium), decorating it with murals and mosaics, as well as lavish sculptures and furniture.[3] The overall purpose of a private banquet was entertainment, not only through live performances, but also through the presentation of the food itself. The meal consisted of three courses- appetizers, main course, and dessert- brought out in elaborate rituals.[2] For instance, the main course was sometimes served to the tune of trumpets at particularly luxurious events. Foods that were particularly valued were wild game, such as pheasant and boar, certain kinds of fish, and wild berries, mainly because of their exoticism and high price. Some ancient writers recount Emperor Claudius adding crushed pearls to wine and flecks of gold to peas solely to increase their cost. Others recall live animals being served as shows of entertainment and richness. For instance, at one event mackerels were pickled live in order to showcase their silvery bodies thrashing in vinegar. Overall, Roman aristocrats desired to showcase their wealth and luxury through the presentation of food.[4]		Medieval aristocrats also desired to entertain and impress through food. Banquets were usually huge feasts with diverse choices of dishes. Social etiquette dictated that the wealthy and powerful be given beautiful and elaborate dishes while the poor be given simple food, usually scraps.[5] Such banquets not only entertained guests, but also showed the wealth of the host. In particular, the patron sometimes commissioned artists to create complicated sculptures made from food items to awe and inspire.[6] A popular sculpture was a pregnant woman modeled in marzipan or sugar paste. Other favorites were pies or cakes designed to expel live birds when cut open and multicolored jellies stacked together, dyed with spices and vegetable matter. Furthermore, since these foods often were not preserved properly and were somewhat rotted when stored, the mask of decaying food was covered with spices and other aromas. This indicates the medieval hosts concern with presentation as opposed to flavor.[4]		In the same way, contemporary food reflects both personal and societal aesthetic beliefs. While cuisine in the past was intrinsically related to wealth and social status, contemporary cuisine is much less distinguished by class. The disintegration of highbrow and lowbrow foods has led to increased accessibility of various foods.[7] Now, it is possible to find a hamburger at a five star restaurants and exotic cuisines on street corners. Therefore, contemporary food presentation is determined much more by modern aesthetics and creativity than displays of wealth and power.		Nouvelle cuisine is a school of French cooking that rejects ostentatious displays of food in favor of simple presentation and high-quality ingredients. In contrast to historical chefs that obeyed the orders of patrons, this manner of cooking elevates the chef from a skilled worker to an inventor and artist. The aesthetic of nouvelle cuisine emphasizes minimalism, serving fewer courses and utilizing simple plating.[8] Chefs were extremely creative in constructing innovative recipes and plating.		A bento box is a Japanese meal traditionally consisting of rice, meat/fish, and vegetables served in a portable box. In Japan, as well as in the United States, a large focus is placed on the aesthetic arrangement of the food. There have even been contests to see who can come up with the most inventive way of creating bento boxes, allowing for creativity in amateur chefs and everyday people. Sometimes bento boxes are used to make sculptural designs, such as rice shaped to look like animals.[6] These specific types of bento boxes are known as Kyaraben or charaben, (キャラ弁) a shortened form of character bento. (キャラクター弁当 kyarakutā bentō) Kyaraben are most often made by mothers to encourage their children to eat more nutritious diets and as a way of showing their love and dedication. [9]		Kaiseki (懐石) is a Japanese multi-course haute cuisine dinner consisting of 7-14 courses, often served at ryokan, but also in small restaurants known as ryotei, particularly in Kyoto.[10] A large focus of kaiseki is in the elaborate preparation and aesthetic presentation of these meals to enhance the natural flavors of fresh, local ingredients. [11] Meals are often garnished with edible leaves and flowers to enhance the seasonality of the meal and its ingredients and are arranged to resemble natural plants and animals.		Kaiseki dinners most commonly involve an appetizer, sashimi, a simmered dish, a grilled dish, and a steamed dish. Other dishes may be added or omitted depending on the chef. [12]		Modern science can illuminate how and why people respond in certain ways to food plating and presentation. According to a sociological study, people react differently to various aesthetic principles such as color, composition (including a number of components, placement of components, and use of negative space), design, and the organization of a plate. They found that participants responded best to plates with four different colors, three different components, some empty space, and with a disorganized and casual design. This research is particularly important because understanding how food presentation affects how people eat can be used in the study of health and nutrition. For instance, another study showed that participants who ate off of uncleaned tables (i.e all uneaten food was left on the table) ate less than those that had their tabled periodically cleaned. This is presumably because those that could see the leftovers of what they had eaten were less likely to take more food. This could be useful, for instance, in combatting the obesity endemic.[13] Another example of science in food aesthetics is the development of molecular food in Spain, which emphasizes the essence of food using scientific elements.[14] Molecular science can break down the roles of carbohydrates and protein in order to isolate what creates particular tastes. For instance, modern science makes it possible to freeze ice cream using liquid nitrogen and create wine using sugars, creating efficient and visually interesting new dishes.[15]		Like other aspects of culture, food presentation is subject to trends and fads. For instance, "unicorn food", a style of presentation that uses a rainbow color palette to decorate food, became popular in 2017.[16][6][17] The pastel-like hues are supposed to represent the colors of the mythical unicorn.[6]		Olives and pickles attractively served on purple cabbage leaves		Molded seafood dip garnished with chicory "seaweed" and appetizers served in large sea shells		Brightly colored veggie platter arranged over purple cabbage with "rosebud radishes" and sweet peppers used as serving containers		An artful arrangement of cheese, fruit and bread		Fruit and vegetable "flowers"		
Urination is the release of urine from the urinary bladder through the urethra to the outside of the body. It is the urinary system's form of excretion. It is also known medically as micturition, voiding, uresis, or, rarely, emiction, and known colloquially by various names including peeing, weeing, and pissing.		In healthy humans (and many other animals) the process of urination is under voluntary control. In infants, some elderly individuals, and those with neurological injury, urination may occur as a reflex. It is normal for adult humans to urinate up to seven times during the day.[1]		In some animals, in addition to expelling waste material, urination can mark territory or express submissiveness. Physiologically, urination involves coordination between the central, autonomic, and somatic nervous systems. Brain centers that regulate urination include the pontine micturition center, periaqueductal gray, and the cerebral cortex. In placental mammals, urine is drained through the urinary meatus, a urethral opening in the male penis or female vulval vestibule.[2][3]:38,364		The main organs involved in urination are the urinary bladder and the urethra. The smooth muscle of the bladder, known as the detrusor, is innervated by sympathetic nervous system fibers from the lumbar spinal cord and parasympathetic fibers from the sacral spinal cord.[4] Fibers in the pelvic nerves constitute the main afferent limb of the voiding reflex; the parasympathetic fibers to the bladder that constitute the excitatory efferent limb also travel in these nerves. Part of the urethra is surrounded by the male or female external urethral sphincter, which is innervated by the somatic pudendal nerve originating in the cord, in an area termed Onuf's nucleus.[5]		Smooth muscle bundles pass on either side of the urethra, and these fibers are sometimes called the internal urethral sphincter, although they do not encircle the urethra. Further along the urethra is a sphincter of skeletal muscle, the sphincter of the membranous urethra (external urethral sphincter). The bladder's epithelium is termed transitional epithelium which contains a superficial layer of dome-like cells and multiple layers of stratified cuboidal cells underneath when evacuated. When the bladder is fully distended the superficial cells become squamous (flat) and the stratification of the cuboidal cells is reduced in order to provide lateral stretching.		The physiology of micturition and the physiologic basis of its disorders are subjects about which there is much confusion, especially at the supraspinal level. Micturition is fundamentally a spinobulbospinal reflex facilitated and inhibited by higher brain centers such as the pontine micturition center and, like defecation, subject to voluntary facilitation and inhibition.[6]		In healthy individuals, the lower urinary tract has two discrete phases of activity: the storage (or guarding) phase, when urine is stored in the bladder; and the voiding phase, when urine is released through the urethra. The state of the reflex system is dependent on both a conscious signal from the brain and the firing rate of sensory fibers from the bladder and urethra.[6] At low bladder volumes, afferent firing is low, resulting in excitation of the outlet (the sphincter and urethra), and relaxation of the bladder.[7] At high bladder volumes, afferent firing increases, causing a conscious sensation of urinary urge. When the individual is ready to urinate, he or she consciously initiates voiding, causing the bladder to contract and the outlet to relax. Voiding continues until the bladder empties completely, at which point the bladder relaxes and the outlet contracts to re-initiate storage.[6] The muscles controlling micturition are controlled by the autonomic and somatic nervous systems. During the storage phase the internal urethral sphincter remains tense and the detrusor muscle relaxed by sympathetic stimulation. During micturition, parasympathetic stimulation causes the detrusor muscle to contract and the internal urethral sphincter to relax. The external urethral sphincter (sphincter urethrae) is under somatic control and is consciously relaxed during micturition.		It is commonly believed that in infants, voiding occurs involuntarily (as a reflex). However, the practice of elimination communication suggests otherwise.[citation needed] The ability to voluntarily inhibit micturition develops by the age of 2–3 years, as control at higher levels of the central nervous system develops. In the adult, the volume of urine in the bladder that normally initiates a reflex contraction is about 300–400 millilitres (11–14 imp fl oz; 10–14 US fl oz).		During storage, bladder pressure stays low, because of the bladder's highly compliant nature. A plot of bladder (intravesical) pressure against the depressant of fluid in the bladder (called a cystometrogram), will show a very slight rise as the bladder is filled. This phenomenon is a manifestation of the law of Laplace, which states that the pressure in a spherical viscus is equal to twice the wall tension divided by the radius. In the case of the bladder, the tension increases as the organ fills, but so does the radius. Therefore, the pressure increase is slight until the organ is relatively full. The bladder's smooth muscle has some inherent contractile activity; however, when its nerve supply is intact, stretch receptors in the bladder wall initiate a reflex contraction that has a lower threshold than the inherent contractile response of the muscle.		Action potentials carried by sensory neurons from stretch receptors in the urinary bladder wall travel to the sacral segments of the spinal cord through the pelvic nerves.[6] Since bladder wall stretch is low during the storage phase, these afferent neurons fire at low frequencies. Low-frequency afferent signals cause relaxation of the bladder by inhibiting sacral parasympathetic preganglionic neurons and exciting lumbar sympathetic preganglionic neurons. Conversely, afferent input causes contraction of the sphincter through excitation of Onuf's nucleus, and contraction of the bladder neck and urethra through excitation of the sympathetic preganglionic neurons.		Diuresis (production of urine by the kidney) occurs constantly, and as the bladder becomes full, afferent firing increases, yet the micturition reflex can be voluntarily inhibited until it is appropriate to begin voiding.		Voiding begins when a voluntary signal is sent from the brain to begin urination, and continues until the bladder is empty.		Bladder afferent signals ascend the spinal cord to the periaqueductal gray, where they project both to the pontine micturition center and to the cerebrum.[8] At a certain level of afferent activity, the conscious urge to void becomes difficult to ignore. Once the voluntary signal to begin voiding has been issued, neurons in pontine micturition center fire maximally, causing excitation of sacral preganglionic neurons. The firing of these neurons causes the wall of the bladder to contract; as a result, a sudden, sharp rise in intravesical pressure occurs. The pontine micturition center also causes inhibition of Onuf's nucleus, resulting in relaxation of the external urinary sphincter.[9] When the external urinary sphincter is relaxed urine is released from the urinary bladder when the pressure there is great enough to force urine to flow out of the urethra. The micturition reflex normally produces a series of contractions of the urinary bladder.		The flow of urine through the urethra has an overall excitatory role in micturition, which helps sustain voiding until the bladder is empty.[10]		After urination, the female urethra empties partially by gravity, with assistance from muscles.[clarification needed] Urine remaining in the male urethra is expelled by several contractions of the bulbospongiosus muscle, and, by some men, manual squeezing along the length of the penis to expel the rest of the urine.		For land mammals over 1 kilogram, the duration of urination does not vary with body mass, being dispersed around an average of 21 seconds (standard deviation 13 seconds), despite a 4 order of magnitude (1000×) difference in bladder volume.[11][12] This is due to increased urethra length of large animals, which amplifies gravitational force (hence flow rate), and increased urethra width, which increases flow rate. For smaller mammals a different phenomenon occurs, where urine is discharged as droplets, and urination in smaller mammals, such as mice and rats, can occur in less than a second.[12] The posited benefits of faster voiding are decreased risk of predation (while voiding) and decreased risk of urinary tract infection.		The mechanism by which voluntary urination is initiated remains unsettled.[13] One possibility is that the voluntary relaxation of the muscles of the pelvic floor causes a sufficient downward tug on the detrusor muscle to initiate its contraction.[14] Another possibility is the excitation or disinhibition of neurons in the pontine micturition center, which causes concurrent contraction of the bladder and relaxation of the sphincter.[6]		There is an inhibitory area for micturition in the midbrain. After transection of the brain stem just above the pons, the threshold is lowered and less bladder filling is required to trigger it, whereas after transection at the top of the midbrain, the threshold for the reflex is essentially normal. There is another facilitatory area in the posterior hypothalamus. In humans with lesions in the superior frontal gyrus, the desire to urinate is reduced and there is also difficulty in stopping micturition once it has commenced. However, stimulation experiments in animals indicate that other cortical areas also affect the process.		The bladder can be made to contract by voluntary facilitation of the spinal voiding reflex when it contains only a few milliliters of urine. Voluntary contraction of the abdominal muscles aids the expulsion of urine by increasing the pressure applied to the urinary bladder wall, but voiding can be initiated without straining even when the bladder is nearly empty.		Voiding can also be consciously interrupted once it has begun, through a contraction of the perineal muscles. The external sphincter can be contracted voluntarily, which will prevent urine from passing down the urethra.		The need to urinate is experienced as an uncomfortable, full feeling. It is highly correlated with the fullness of the bladder.[15] In many males the feeling of the need to urinate can be sensed at the base of the penis as well as the bladder, even though the neural activity associated with a full bladder comes from the bladder itself, and can be felt there as well. In females the need to urinate is felt in the lower abdomen region when the bladder is full. When the bladder becomes too full, the sphincter muscles will involuntarily relax, allowing urine to pass from the bladder. Release of urine is experienced as a lessening of the discomfort.		Many clinical conditions can cause disturbances to normal urination, including:		A drug that increases urination is called a diuretic, whereas antidiuretics decrease the production of urine by the kidneys.		There are three major types of bladder dysfunction due to neural lesions: (1) the type due to interruption of the afferent nerves from the bladder; (2) the type due to interruption of both afferent and efferent nerves; and (3) the type due to interruption of facilitatory and inhibitory pathways descending from the brain. In all three types the bladder contracts, but the contractions are generally not sufficient to empty the viscus completely, and residual urine is left in the bladder. Paruresis, also known as shy bladder syndrome, is an example of a bladder interruption from the brain that often causes total interruption until the person has left a public area. As these people may have difficulty urinating in the presence of others and will consequently avoid using urinals directly adjacent to another person. Alternatively, they may opt for the privacy of a stall or simply avoid public toilets altogether.		When the sacral dorsal roots are cut in experimental animals or interrupted by diseases of the dorsal roots such as tabes dorsalis in humans, all reflex contractions of the bladder are abolished. The bladder becomes distended, thin-walled, and hypotonic, but there are some contractions because of the intrinsic response of the smooth muscle to stretch.		When the afferent and efferent nerves are both destroyed, as they may be by tumors of the cauda equina or filum terminale, the bladder is flaccid and distended for a while. Gradually, however, the muscle of the "decentralized bladder" becomes active, with many contraction waves that expel dribbles of urine out of the urethra. The bladder becomes shrunken and the bladder wall hypertrophied. The reason for the difference between the small, hypertrophic bladder seen in this condition and the distended, hypotonic bladder seen when only the afferent nerves are interrupted is not known. The hyperactive state in the former condition suggests the development of denervation hypersensitization even though the neurons interrupted are preganglionic rather than postganglionic.		During spinal shock, the bladder is flaccid and unresponsive. It becomes overfilled, and urine dribbles through the sphincters (overflow incontinence). After spinal shock has passed, a spinally mediated voiding reflex ensues, although there is no voluntary control and no inhibition or facilitation from higher centers. Some paraplegic patients train themselves to initiate voiding by pinching or stroking their thighs, provoking a mild mass reflex. In some instances, the voiding reflex becomes hyperactive. Bladder capacity is reduced and the wall becomes hypertrophied. This type of bladder is sometimes called the spastic neurogenic bladder. The reflex hyperactivity is made worse, and may be caused, by infection in the bladder wall.		Due to the positions where the urethra exits the body, males and females often use different techniques for urination.		Some males prefer to urinate standing while others prefer to urinate sitting or squatting. Elderly males with prostate gland enlargement may benefit from sitting down while in healthy males, no difference is found in the ability to urinate.[16][17] For practising Muslim men, the genital modesty of squatting is also associated with proper cleanliness requirements or awrah.[18]				In human females, the urethra opens straight into the vulva. Hence, urination can take place while sitting or squatting for defecation. It is also possible for females to urinate while standing, and while clothed.[19] It is common for women in various regions of Africa to use this method when they urinate,[20][21][22][need quotation to verify][23][24][25] as do women in Laos.[26][not in citation given] Herodotus described a similar custom in ancient Egypt.[27] An alternative method for women to urinate standing is to use a tool known as a female urination device to assist.[28]		A common technique used in many undeveloped nations involves holding the child by the backs of the thighs, above the ground, facing outward, in order to urinate.[citation needed]		The fetus urinates hourly and produces most of the amniotic fluid in the second and third trimester of pregnancy. The amniotic fluid is then recycled by fetal swallowing.[29]		Occasionally, if a male's penis is damaged or removed, or a female's genitals/urinary tract is damaged, other urination techniques must be used. Most often in such cases, doctors will reposition the urethra to a location where urination can still be accomplished, usually in a position that would only promote urination while seated/squatting, though a permanent urinary catheter may be used in rare cases.[citation needed]		Sometimes urination is done in a container such as a bottle, urinal, bedpan, or chamber pot (also known as a gazunder). A container may be used so that the urine can be examined for medical reasons or for a drug test, for a bedridden patient, when no toilet is available, or there is no other possibility to dispose of the urine immediately.		An alternative solution (for traveling, stakeouts, etc.) is a special disposable bag containing absorbent material that solidifies the urine within seconds, making it convenient and safe to store and dispose of later.[citation needed]		It is possible for both genders to urinate into bottles in case of emergencies. The technique can help children to urinate discreetly inside cars and in other places without being seen by others.[30]		Babies have little socialized control over urination within traditions or families that do not practice elimination communication and instead use diapers. Toilet training is the process of learning to restrict urination to socially approved times and situations. Consequently, young children sometimes suffer from nocturnal enuresis.[31]		It is socially more accepted and more environmentally hygienic for those who are able, to urinate in a toilet. Public toilets may have urinals, usually for males, although female urinals exist, designed to be used in various ways.[19]		Acceptability of outdoor urination in a public place other than at a public urinal varies with the situation and with customs. Potential disadvantages include a dislike of the smell of urine, and some exposure of genitals.[citation needed] The latter can be unpleasant for the one who exposes them (modesty, lack of privacy) and/or those who can see them;[citation needed] it can be avoided or mitigated by going to a quiet place and/or facing a tree or wall if urinating standing up, or while squatting, hiding the back behind walls, bushes, or a tree.[citation needed]		Portable toilets (port-a-potties) are frequently placed in outdoor situations where no immediate facility is available. These need to be serviced (cleaned out) on a regular basis. Urination in a heavily wooded area is generally harmless, actually saves water, and may be condoned for males (and less commonly, females) in certain situations as long as common sense is used. Examples (depending on circumstances) include activities such as camping, hiking, cross country running, rural fishing, amateur baseball, golf, etc.		The more developed and crowded a place is, the more public urination tends to be objectionable. In the countryside, it is more acceptable than in a street in a town, where it may be a common transgression. Often this is done after the consumption of alcoholic beverages, which causes production of additional urine as well as a reduction of inhibitions. One proposed way to inhibit public urination due to drunkenness is the Urilift, which is disguised as a normal manhole by day but raises out of the ground at night to provide a public restroom for bar-goers.		In many places, public urination is punishable by fines, though attitudes vary widely by country. In general, women are less likely to urinate in public than men. Depending on the culture, adult women, unlike men, are restricted in where they can urinate.[32]		The 5th-century BC historian Herodotus, writing on the culture of the ancient Persians and highlighting the differences with those of the Greeks, noted that to urinate in the presence of others was prohibited among Persians.[33][34]		There was[when?] a popular belief in the UK, that it was legal for a man to urinate in public so long as it occurred on the rear wheel of his vehicle and he had his right hand on the vehicle, but this is not true.[35] Public urination still remains more accepted by males in the UK, although British cultural tradition itself seems to find such practices objectionable.[36]		In Islamic toilet etiquette, it is haram to urinate while facing the Qibla, or to turn one's back to it when urinating or relieving bowels but modesty requirements for females make it impossible for girls to relieve themselves without facilities.[37][38] When toilets are unavailable, females can relieve themselves in Laos, Russia and Mongolia in emergency [39] but it remains utterly unacceptable for females in India even when circumstances make this a highly desirable option.[40]		Women generally need to urinate more frequently than men due to having smaller bladders.[41] Resisting the urge to urinate because of lack of facilities can promote urinary tract infections which can lead to more serious infections and, in rare situations, can cause renal damage in women.[42][43] Female urination devices are available to help women to urinate discreetly, as well to help them urinate while standing.		In Western culture, the standing position is regarded by some as superior and more masculine than the sitting or squatting option, and as a way to differentiate men from women.[citation needed] However, in public restrooms without urinals and sometimes at home, men may be urged to use the sitting position as to diminish spattering of urine.[17]		A meta-analysis on these studies showed that males with an enlarged prostate urinated better in the sitting position compared to the standing. The amount of residual urine in the bladder was significantly reduced, and there was a trend towards a more powerful flow and shorter voiding time. Combined, this reduces the risk of bladder stones and urinary tract infections. The same study showed that healthy males were not influenced by position, meaning that they could urinate in either position.[16]		A systematic review meta-analysis on the effect of voiding position on the quality of urination found that in elderly males with benign prostate hyperplasia, the sitting position was superior compared with the standing.[44][45] Healthy males were not influenced by voiding position.		A literature review found cultural differences in socially accepted voiding positions around the world found many differences in preferred position: in the Middle-East and Asia, the squatting position was more prevalent, while in the Western world the standing and sitting positions were more common.[46]		While it is uncommon in Western cultures for women to stand while urinating, this practice is becoming more common. Denise Decker, a nurse who advocates for this practice, surveyed 600 women to discover how interested they were in having female urination devices that would allow them to urinate in a standing position, and the majority of respondents indicated a desire to have such a device.[47]		A partial squatting position (or "hovering") by females while urinating is often done to avoid sitting on a potentially contaminated toilet seat, although it may leave urine behind in the bladder.[48] It can also result in urine landing on the toilet seat.		In many societies and in many social classes, even mentioning the need to urinate is seen as a social transgression, despite it being a universal need. Even today, many adults avoid stating that they need to urinate.[49][50]		Many expressions exist, some euphemistic and some vulgar. For example, centuries ago the standard English word (both noun and verb, for the product and the activity) was "piss", but subsequently "pee", formerly associated with children, has become more common in general public speech. Since elimination of bodily wastes is, of necessity, a subject talked about with toddlers during toilet training, other expressions considered suitable for use by and with children exist, and some continue to be used by adults, e.g. "weeing", "doing/having a wee-wee", "to tinkle", "potty".[citation needed]		Other expressions include "squirting" and "taking a leak", and, predominantly by younger persons for outdoor female urination, "popping a squat", referring to the position many women adopt in such circumstances. National varieties of English show creativity. American English uses "to whiz".[51] Australian English has coined "I am off to take a Chinese singing lesson", derived from the tinkling sound of urination against the China porcelain of a toilet bowl.[52] British English uses "going to see my aunt", "going to see a man about a dog", "to piddle", "to splash (one's) boots", as well as "to have a slash", which originates from the Scottish term for a large splash of liquid.[53] One of the most common, albeit old-fashioned, euphemisms in British English is "to spend a penny", a reference to coin-operated pay toilets, which used (pre-decimalisation) to charge that sum.[54]		References to urination are commonly used in slang. Usage in English includes:		Urolagnia is an inclination to obtain sexual enjoyment by looking at or thinking of urine or urination.[55] As a paraphilia, urine may be consumed, or the person may bathe in it. Drinking urine is known as urophagia, though uraphagia refers to the consumption of urine regardless of whether the context is sexual. Involuntary urination during sexual intercourse is common, but rarely acknowledged. In one survey, 24% of women reported involuntary urination during sexual intercourse; in 66% of sufferers urination occurred on penetration, while in 33% urine leakage was restricted to orgasm.[56]		Female kob may exhibit urolagnia during sex; one female will urinate while the other sticks her nose in the stream.[57][58]		A male Patagonian mara, a type of rodent, will stand on his hind legs and urinate on a female's rump, to which the female may respond by spraying a jet of urine backwards into the face of the male.[59] The male's urination is meant to repel other males from his partner while the female's urination is a rejection of any approaching male when she is not receptive.[59] Both anal digging and urination are more frequent during the breeding season and are more commonly done by males.[60]		A male porcupine urinates on a female porcupine prior to mating, spraying the urine at high velocity.[61][62][63][64][65]						While the primary purpose of urination is the same across the animal kingdom, urination often serves a social purpose beyond the expulsion of waste material. In dogs and other animals, urination can mark territory or express submissiveness.[66] In small rodents such as rats and mice, it marks familiar paths.		The urine of animals of differing physiology or sex sometimes has different characteristics. For example, the urine of birds and reptiles is whitish, consisting of a pastelike suspension of uric acid crystals, and discharged with the feces of the animal via the cloaca, whereas mammals' urine is a yellowish colour, with mostly urea instead of uric acid, and is discharged via the urethra, separately from the feces. Some animals' (example: carnivores') urine possesses a strong odour, especially when it is used to mark territory or communicate in other ways.[clarify]		Stallions sometimes exhibit the Flehmen response by smelling the urine of a mare in heat.[67] A stallion sometimes scent marks his urination spots to make his position as herd stallion clear.[68] A male horse's penis is protected by a sheath when it is not in use for urination.[69]		Ring-tailed lemurs have also been shown to mark using urine. Behaviorally, there is a difference between regular urination, where the tail is slightly raised and a stream of urine is produced, and the urine marking behavior, where the tail is held up in display and only a few drops of urine are used.[70][71] The urine-marking behavior is typically used by females to mark territory, and has been observed primarily at the edges of the troop's territory and in areas where other troops may frequent.[72] The urine marking behavior is also most frequent during the mating season, and may play a role in reproductive communication between groups.[70] Some other primate species use also urine for scent-marking. The white-headed capuchin sometimes engages in a practice known as "urine washing", in which the monkey rubs urine on its feet.[73] Urine washing, in which urine is rubbed on the hands and feet, is also used by the Panamanian night monkey.[74] In some cases, strepsirrhines may also anoint themselves with urine.[75]		Hyenas do not raise their legs as canids do when urinating, as urination serves no territorial function for them. Instead, hyenas mark their territories using their anal glands, a trait found also in viverrids and mustelids, but not canids and felids.[76] Unlike other female mammals, female spotted hyenas urinate, copulate, and give birth through an organ called the pseudo-penis.[77][78]		All canids (with the possible exception of dholes[79]) use urine (combined with preputial gland secretions) to mark their territories. Many species of canids, including hoary foxes,[80] cape foxes,[81] and golden jackals,[82] use a raised-leg posture when urinating.[83][84] The scent of their urine is usually strongest in the winter, before the mating season.[84]		Domestic dogs mark their territories by urinating on vertical surfaces (usually at nose level), sometimes marking over the urine of other dogs.[83] When one dog marks over another dog's urine, this is known as "counter-marking" or "overmarking".[85][86] Male dogs urine-mark more frequently than female dogs,[87] typically beginning after the onset of sexual maturity.[88] Male dogs, as well as wolves, sometimes lift a leg and attempt to urinate even when their bladders are empty – this is known as a "raised-leg display",[89][90][91][91][92] "shadow-urination",[93] or "pseudo-urination".[94] They typically mark their territory due to the presence of new stimuli or social triggers in a dog's environment, as well as out of anxiety.[95] Marking behavior is present in both male and female dogs, and is especially pronounced in male dogs that have not been neutered.[95]		Raised-leg urination is the most significant form of scent marking in wolves, and is most frequent around the breeding season.[96] Wolves urine-mark more frequently when they detect the scent of other wolves, or other canid species.[97] Leg-lifting is more common in male wolves than female wolves, although dominant females also use the raised-leg posture.[98] Other types of urine-marking in wolves are FLU (flexed-leg urination), STU (standing urination), and SQU (squatting urination).[99] Breeding pairs of wolves will sometimes urinate on the same spot: this is known as "double-marking".[100][101][102][103][104][105] Double-marking is practiced by both coyotes and wolves.,[106][107][108] and also by foxes.[109]		Coyotes mark their territories by urinating on bushes, trees, or rocks.[110] All male coyotes lift their legs when urinating.[111] However, females sometimes also raise their legs, and males sometimes squat.[112] Urine marking is also associated with pair bonding in coyotes[clarification needed][113] Coyotes sometimes urinate on their food, possibly to claim ownership over it.[114]		Red foxes use their urine to mark their territories.[115][116][117][118][119] A male fox raises one hind leg and his urine is sprayed forward in front of him, whereas a female fox squats down so that the urine is sprayed in the ground between the hind legs.[120][121] Urine is also used to mark empty cache sites, as reminders not to waste time investigating them.[122][123][124] Red foxes use various postures[clarify] to urinate, depending on where they are leaving a scent mark.[120][125]		As in most other canids, male bush dogs lift their hind legs when urinating. However, female bush dogs use a kind of handstand posture, which is less common in other canids.[126][127] When male bush dogs urinate, they create a spray instead of a stream.[128]		Both male and female maned wolves use their urine to communicate, e.g. to mark their hunting paths, or the places where they have buried hunted prey.[129] The urine has a very distinctive smell, which some people liken to hops or cannabis. The responsible substance is very likely a pyrazine, which occurs in both plants.[130] (At the Rotterdam Zoo, this smell once set the police on a hunt for cannabis smokers.[130][131])		Within the Felidae, male felids can urinate backwards by curving the tip of the glans penis backward.[132][133] Urine marking by felids is also known as "spray-urinating"[134] or "spray-marking".[135] To identify their territories, male tigers mark trees by spraying urine[136][137] and anal gland secretions, as well as marking trails with scat. Males show a grimacing face, called the Flehmen response, when identifying a female's reproductive condition by sniffing their urine markings.		Lions use urine to mark their territories. They often scrape the ground while urinating, and the urine often flows in short spurts, instead of flowing continuously. They often urinate on vegetation, or on tree trunks at least one meter high.[138] Male lions spray 1–20 jets of urine at an angle of 20–30 degrees upward, at a range of up to 4 meters behind them.[139]		Male cheetahs mark their territory by urinating on objects that stand out, such as trees, logs, or termite mounds. The whole coalition contributes to the scent. Males will attempt to kill any intruders, and fights result in serious injury or death.[140] When male cheetahs urine-mark their territories, they stand a meter away from a tree or rock surface with the tail raised, pointing the penis either horizontally backward or 60° upward.[141] The odor of cheetah urine (unlike that of other large felids) cannot be easily detected by humans.[135]		Black-footed cats use scent marking throughout their ranges, with males spraying urine up to 12 times an hour.[142]		
Aphagia is the inability or refusal to swallow.[1][2] The word is derived from the Ancient Greek prefix α, meaning "not" or "without," and the suffix φαγία, derived from the verb φαγεῖν, meaning "to eat." It is related to dysphagia which is difficulty swallowing (Greek prefix δυσ, dys, meaning difficult, or defective), and odynophagia, painful swallowing (from ὀδύνη, odyn(o), meaning "pain"). Aphagia may be temporary or long term, depending on the affected organ. It is an extreme, life-threatening case of dysphagia. Depending on the cause, untreated dysphagia may develop into aphagia.						The following behavioural classifications result from studies performed on rats, in which lesions were made on the lateral hypothalamus region in the brain.		These studies point to the function of the hypothalamus in regulating food intake. Animals in this study also demonstrated several other types of eating behaviour: "weak eating," in which the animal slowly approaches, chews, and swallows small observable amounts of food for a brief period; "good eating," in which the animal reaches normally for the food and eats reliably; and "vigorous eating," in which the animal gluttonously reaches for and devours the food. In these cases, there was either minor or no damage to the lateral hypothalamus.[3]		Aphagia not classified under behavioral aphagia typically has a structural cause, see causes.		Aphagia is usually the result of many different diseases as well as different medical treatments. The most common things that can lead to aphagia are:		It is important to note that all this causes (except due to the damage of the lateral hypothalamus) are indirect causes of aphagia.		During the treatment of aphagia (or dysphagia), it is important to provide adequate nutrition and hydration. If a person is not able to tolerate a regular diet, diet modifications and alternative means of nutrition may be considered. These include thickening liquids (typical thickening hierarchy is nectar/syrup thick, honey thick, and pudding thick) or by changing the texture of the solid foods to reduce the required amount of mastication (chewing) needed or to reduce other symptoms of oral dysphagia (such as buccal pocketing or anterior loss). Alternative means of nutrition may also be needed in more severe cases (such as when a person is deemed NPO and is not safe to eat anything orally). In these cases, nasogastric (NG) or percutaneous endoscopic gastronomy (PEG) tubes may be placed. Other compensatory measures may include reducing the bolus size (small bites/sips) or postural strategies (such as tucking the chin, turning the head to one side or the other). A speech-lanuguage pathologist is one professional who evaluates and treats aphagia and dysphagia and can recommend these strategies depending on the etiology of the deficit and the location of the breakdown within the swallowing mechanism. True treatment of aphagia/dysphagia comes from neuromuscular re-education and strengthening/coordination in most cases. This can be achieved by use of pharyngeal strengthening exercises, thermal stimulation of the swallowing trigger and oral motor exercises. In some cases, it is also appropriate to complete therapeutic exercises in conjunction with neuromuscular electrical stimulation (NMES) which utilizes low-level electrical currents to target muscle fibers from an external source (electrodes placed on the surface of the skin in strategic places to target muscles and nerves needed during the swallowing process).		
A restaurant (/ˈrɛstərənt/ or /ˈrɛstərɒnt/; French: [ʀɛs.to.ʁɑ̃] ( listen)), or an eatery, is a business which prepares and serves food and drinks to customers in exchange for money. Meals are generally served and eaten on the premises, but many restaurants also offer take-out and food delivery services, and some offer only take-out and delivery. Restaurants vary greatly in appearance and offerings, including a wide variety of cuisines and service models ranging from inexpensive fast food restaurants and cafeterias to mid-priced family restaurants, to high-priced luxury establishments.		In Western countries, most mid- to high-range restaurants serve alcoholic beverages such as beer, wine and light beer. Some restaurants serve all the major meals, such as breakfast, lunch, and dinner (e.g., major fast food chains, diners, hotel restaurants, and airport restaurants). Other restaurants may only serve a single meal (e.g., a pancake house may only serve breakfast) or they may serve two meals (e.g., lunch and dinner) or even a kids' meal.						Restaurants may be classified or distinguished in many different ways. The primary factors are usually the food itself (e.g. vegetarian, seafood, steak); the cuisine (e.g. Italian, Chinese, Japanese, Indian, French, Mexican, Thai) and/or the style of offering (e.g. tapas bar, a sushi train, a tastet restaurant, a buffet restaurant or a yum cha restaurant). Beyond this, restaurants may differentiate themselves on factors including speed (see fast food), formality, location, cost, service, or novelty themes (such as automated restaurants).		Restaurants range from inexpensive and informal lunching or dining places catering to people working nearby, with modest food served in simple settings at low prices, to expensive establishments serving refined food and fine wines in a formal setting. In the former case, customers usually wear casual clothing. In the latter case, depending on culture and local traditions, customers might wear semi-casual, semi-formal or formal wear. Typically, at mid- to high-priced restaurants, customers sit at tables, their orders are taken by a waiter, who brings the food when it is ready. After eating, the customers then pay the bill. In some restaurants, such as workplace cafeterias, there are no waiters; the customers use trays, on which they place cold items that they select from a refrigerated container and hot items which they request from cooks, and then they pay a cashier before they sit down. Another restaurant approach which uses few waiters is the buffet restaurant. Customers serve food onto their own plates and then pay at the end of the meal. Buffet restaurants typically still have waiters to serve drinks and alcoholic beverages. Fast food restaurants are also considered a restaurant.		The travelling public has long been catered for with ship's messes and railway restaurant cars which are, in effect, travelling restaurants. Many railways, the world over, also cater for the needs of travellers by providing railway refreshment rooms, a form of restaurant, at railway stations. In the 2000s, a number of travelling restaurants, specifically designed for tourists, have been created. These can be found on trams, boats, buses, etc.		A Salaama Hut restaurant at a Somali strip mall in Toronto		The kitchen of Pétrus, located in Knightsbridge, Central London		A restaurant's proprietor is called a restaurateur /ˌrɛstərəˈtɜːr/; like 'restaurant', this derives from the French verb restaurer, meaning "to restore". Professional cooks are called chefs, with there being various finer distinctions (e.g. sous-chef, chef de partie). Most restaurant (other than fast food restaurants and cafeterias) will have various waiting staff to serve food, beverages and alcoholic drinks, including busboys who remove used dishes and cutlery. In finer restaurants, this may include a host or hostess, a maître d'hôtel to welcome customers and to seat them, and a sommelier or wine waiter to help patrons select wines. A new route to becoming a restauranter, rather than working one's way up through the stages, is to operate a food truck. Once a sufficient following has been obtained, a permanent restaurant site can be opened. This trend has become common in the UK and the US.		A chef's table is a table located in the kitchen of a restaurant,[2][3] reserved for VIPs and special guests.[4] Patrons may be served a themed[4] tasting menu prepared and served by the head chef. Restaurants can require a minimum party[5] and charge a higher flat fee.[6] Because of the demand on the kitchen's facilities, chef's tables are generally only available during off-peak times.[7]		In Ancient Greece and Ancient Rome, thermopolia (singular thermopolium) were small restaurant-bars that offered food and drinks to customers. A typical thermopolium had little L-shaped counters in which large storage vessels were sunk, which would contain either hot or cold food. Their popularity was linked to the lack of kitchens in many dwellings and the ease with which people could purchase prepared foods. Furthermore, eating out was considered a very important aspect of socializing.		In Pompeii, 158 thermopolia with a service counter have been identified across the whole town area. They were concentrated along the main axis of the town and the public spaces where they were frequented by the locals.[8]		A Roman thermopolium in Pompeii		In China, food catering establishments that may be described as restaurants have been known since the 11th century in Kaifeng, China's capital during the first half of the Song dynasty (960–1279). Probably growing out of the tea houses and taverns that catered to travellers, Kaifeng's restaurants blossomed into an industry catering to locals as well as people from other regions of China.[9] There is a direct correlation between the growth of the restaurant businesses and institutions of theatrical stage drama, gambling and prostitution which served the burgeoning merchant middle class during the Song dynasty.[10] Restaurants catered to different styles of cuisine, price brackets, and religious requirements. Even within a single restaurant much choice was available, and people ordered the entree they wanted from written menus.[9] An account from 1275 writes of Hangzhou, the capital city for the last half of the dynasty:		The people of Hangzhou are very difficult to please. Hundreds of orders are given on all sides: this person wants something hot, another something cold, a third something tepid, a fourth something chilled; one wants cooked food, another raw, another chooses roast, another grill.[11]		The restaurants in Hangzhou also catered to many northern Chinese who had fled south from Kaifeng during the Jurchen invasion of the 1120s, while it is also known that many restaurants were run by families formerly from Kaifeng.[12]		The modern idea of a restaurant – as well as the term itself – appeared in Paris in the 18th century.[13] For centuries Paris had taverns which served food at large common tables, but they were notoriously crowded, noisy, not very clean, and served food of dubious quality. In about 1765 a new kind of eating establishment, called a "Bouillon", was opened on rue des Poulies, near the Louvre, by a man named Boulanger. It had separate tables, a menu, and specialized in soups made with a base of meat and eggs, which were said to be restaurants or, in English "restoratives". Other similar bouillons soon opened around Paris.[14] Thanks to Boulanger and his imitators, these soups moved from the category of remedy into the category of health food and ultimately into the category of ordinary food. Their existence was predicated on health, not gustatory, requirements.[15]		The first luxury restaurant in Paris, called the Taverne Anglaise, was opened at the beginning of 1786, shortly before the French Revolution, by Antoine Beauvilliers, the former chef of the Count of Provence, at the Palais-Royal. It had mahogany tables, linen tablecloths, chandeliers, well-dressed and trained waiters, a long wine list and an extensive menu of elaborately prepared and presented dishes. In June 1786 the Provost of Paris issued a decree giving the new kind of eating establishment official status, authorizing restaurateurs to receive clients and to offer them meals until eleven in the evening in winter and midnight in summer. A rival restaurant was started in 1791 by Méot, the former chef of the Duke of Orleans, which offered a wine list with twenty-two choices of red wine and twenty-seven of white wine. By the end of the century there were other luxury restaurants at the Grand-Palais: Huré, the Couvert espagnol; Février; the Grotte flamande; Véry, Masse and the Café de Chartres (still open, now Le Grand Vefour).[14]		In the United States, it was not until the late 18th century that establishments that provided meals without also providing lodging began to appear in major metropolitan areas in the form of coffee and oyster houses. The actual term "restaurant" did not enter into the common parlance until the following century. Prior to being referred to as "restaurants" these eating establishments assumed regional names such as "eating house" in New York City, "restorator" in Boston, or "victualing house" in other areas. Restaurants were typically located in populous urban areas during the 19th century and grew both in number and sophistication in the mid-century due to a more affluent middle class and to suburbanization. The highest concentration of these restaurants were in the West, followed by industrial cities on the Eastern Seaboard. [16]		In Brazil, restaurants varieties mirrors the multitude of nationalities that arrived in the country: Japanese, "Arab" (Lebanese), German, Italian, Portuguese and many more.		In Colombia, a piqueteadero is a type of casual or rustic eatery.[17] Meals are often shared, and typical offerings include dishes such as chorizo, chicharron, fried organs, fried yuca, maduro and corn on the cob. Customers order the foods they want and the prepared foods are served together on a platter to be shared.[17] The word piquete can be used to refer to a common Colombian type of meal that includes meat, yuca and potatoes, which is a type of meal served at a piqueteaderos. The verb form of the word piquete, piquetear, means to participate in binging, liquor drinking, and leisure activities in popular areas or open spaces.[18]		In the 1970s, there was one restaurant for every 7,500 persons. In 2016, there were 1,000,000 restaurants; one for every 310 people. The average person eats out five to six times weekly. 10% of the nations workforce is composed of restaurant workers.[19]		Restaurant guides review restaurants, often ranking them or providing information to guide consumers (type of food, handicap accessibility, facilities, etc.). One of the most famous contemporary guides is the Michelin series of guides which accord from 1 to 3 stars to restaurants they perceive to be of high culinary merit. Restaurants with stars in the Michelin guide are formal, expensive establishments; in general the more stars awarded, the higher the prices.		The main competitor to the Michelin guide in Europe is the guidebook series published by Gault Millau. Unlike the Michelin guide which takes the restaurant décor and service into consideration with its rating, Gault Millau only judges the quality of the food. Its ratings are on a scale of 1 to 20, with 20 being the highest.		In the United States, the Forbes Travel Guide (previously the Mobil travel guides) and the AAA rate restaurants on a similar 1 to 5 star (Forbes) or diamond (AAA) scale. Three, four, and five star/diamond ratings are roughly equivalent to the Michelin one, two, and three star ratings while one and two star ratings typically indicate more casual places to eat. In 2005, Michelin released a New York City guide, its first for the United States. The popular Zagat Survey compiles individuals' comments about restaurants but does not pass an "official" critical assessment. FreshNYC recommends plausible New York City restaurants for busy New Yorkers and visitors alike.[21]		The Good Food Guide, published by the Fairfax Newspaper Group in Australia,[22] is the Australian guide listing the best places to eat. Chefs Hats are awarded for outstanding restaurants and range from one hat through three hats. The Good Food Guide also incorporates guides to bars, cafes and providers. The Good Restaurant Guide is another Australian restaurant guide that has reviews on the restaurants as experienced by the public and provides information on locations and contact details. Any member of the public can submit a review.		Nearly all major American newspapers employ food critics and publish online dining guides for the cities they serve. Some news sources provide customary reviews of restaurants, while others may provide more of a general listings service.		More recently Internet sites have started up that publish both food critic reviews and popular reviews by the general public.		Many restaurants are small businesses, and franchise restaurants are common. There is often a relatively large immigrant representation, reflecting both the relatively low start-up costs of the industry (thus making restaurant ownership an option for immigrants with relatively few resources) and the cultural importance of food.		There are 86,915 commercial foodservice units in Canada, or 26.4 units per 10,000 Canadians. By segment, there are:[23]		Fully 63% of restaurants in Canada are independent brands. Chain restaurants account for the remaining 37%, and many of these are locally owned and operated franchises.[24]		The EU-27 has an estimated 1.6m businesses involved in 'accommodation & food services', more than 75% of which are small and medium enterprises.[25]		As of 2006, there are approximately 215,000 full-service restaurants in the United States, accounting for $298 billion in sales, and approximately 250,000 limited-service (fast food) restaurants, accounting for $260 billion.[26] Starting in 2016, Americans spent more on restaurants than groceries.[27]		One study of new restaurants in Cleveland, Ohio found that 1 in 4 changed ownership or went out of business after one year, and 6 out of 10 did so after three years. (Not all changes in ownership are indicative of financial failure.)[28] The three-year failure rate for franchises was nearly the same.[29]		Restaurants employed 912,100 cooks in 2013, earning an average $9.83 per hour.[30] The waiting staff numbered 4,438,100 in 2012, earning an average $8.84 per hour.[31]		Jiaxi Lu of the Washington Post reports in 2014 that, "Americans are spending $683.4 billion a year dining out, and they are also demanding better food quality and greater variety from restaurants to make sure their money is well spent."[32]		Dining in restaurants has become increasingly popular, with the proportion of meals consumed outside the home in restaurants or institutions rising from 25% in 1950 to 46% in 1990. This is caused by factors such as the growing numbers of older people, who are often unable or unwilling to cook their meals at home and the growing number of single-parent households. It is also caused by the convenience that restaurants can afford people; the growth of restaurant popularity is also correlated with the growing length of the work day in the US, as well as the growing number of single parent households.[33] Eating in restaurants has also become more popular with the growth of higher income households. At the same time, less expensive establishments such as fast food establishments can be quite inexpensive, making restaurant eating accessible to many.		In many countries, restaurants are subject to inspections by health inspectors to maintain standards for public health, such as maintaining proper hygiene and cleanliness. As part of these inspections, cooking and handling practices of ground beef are taken into account to protect against the spread of E coli poisoning. The most common kind of violations of inspection reports are those concerning the storage of cold food at appropriate temperatures, proper sanitation of equipment, regular hand washing and proper disposal of harmful chemicals. Simple steps can be taken to improve sanitation in restaurants. As sickness is easily spread through touch, restaurants are encouraged to regularly wipe down tables, door knobs and menus.[34]		Depending on local customs and the establishment, restaurants may or may not serve alcoholic beverages. Restaurants are often prohibited from selling alcoholic beverages without a meal by alcohol sale laws; such sale is considered to be activity for bars, which are meant to have more severe restrictions. Some restaurants are licensed to serve alcohol ("fully licensed"), and/or permit customers to "bring your own" alcohol (BYO / BYOB). In some places restaurant licenses may restrict service to beer, or wine and beer.[35]		
An anorectic or anorexic (from the Greek an- = "without" and orexis = "appetite"), also known as anorexigenic, anorexiant, or appetite suppressant, is a dietary supplement or drug which reduces appetite, resulting in lower food consumption, leading to weight loss.[1] By contrast, an appetite stimulant is referred to as orexigenic.						Numerous pharmaceutical compounds are marketed as appetite suppressants.		The following drugs listed as "centrally-acting antiobesity preparations" in the Anatomical Therapeutic Chemical Classification System:[2]		The following are listed as appetite depressants by MeSH, an index of medical journal articles and books.[3]		Other compounds with known appetite suppressant activity include:		Where † indicates drugs that have been since withdrawn from the market because of adverse effects.		Epidemics of fatal pulmonary hypertension and heart valve damage associated with pharmaceutical anorectic agents have led to the withdrawal of products from the market. This was the case with aminorex in the 1960s, and again in the 1990s with fenfluramine (see: Fen-phen).[6] Likewise, association of the related appetite suppressant phenylpropanolamine with hemorrhagic stroke led the Food and Drug Administration (FDA) to request its withdrawal from the market in the United States in 2000, and similar concerns regarding ephedrine resulted in an FDA ban on its inclusion in dietary supplements in 2004. A Federal judge later overturned this ban in 2005 during a challenge by supplement maker Nutraceuticals. It is also debatable as to whether the ephedrine ban had more to do with its use as a precursor in methamphetamine manufacture rather than health concerns with the ingredient as such.		Weight loss effects of water have been subject to some scientific research.[7] Drinking water prior to each meal may help in appetite suppression. Consumption of 500 mL (approximately 17 fl oz) of water 30 minutes before meals has been correlated with modest weight loss (1–2 kg) in obese men and women over a period of 8 to 12 weeks.[8][9]		Used on a short-term basis clinically to treat obesity, some appetite suppressants are also available over-the-counter. Most common natural appetite suppressants are based on Hoodia, a genus of 13 species in the flowering plant family Apocynaceae, under the subfamily Asclepiadoideae. Several appetite suppressants are based on a mix of natural ingredients, mostly using green tea as its basis, in combination with other plant extracts such as fucoxanthin, found naturally in seaweed. Drugs of this class are frequently stimulants of the phenethylamine family, related to amphetamine (informally known as speed).		The German and Finnish[10] militaries issued amphetamines to soldiers commonly to enhance warfare during the Second World War.[11] Following the war, amphetamines were redirected for use on the civilian market. Indeed, amphetamine itself was sold commercially as an appetite suppressant until it was outlawed in most parts of the world in the late 1950s because of safety issues. Many amphetamines produce side effects, including addiction, tachycardia and hypertension,[12] making prolonged unsupervised use dangerous.		
The Indonesian rijsttafel (Dutch pronunciation: [ˈrɛisttaːfəl]), a Dutch word that literally translates to "rice table", is an elaborate meal adapted by the Dutch following the hidang presentation of nasi Padang from the Padang region of West Sumatra.[1] It consists of many (forty is not an unusual number) side dishes served in small portions, accompanied by rice prepared in several different ways. Popular side dishes include egg rolls, sambals, satay, fish, fruit, vegetables, pickles, and nuts. In most areas where it is served, such as the Netherlands, and other areas of heavy Dutch influence (such as parts of the West Indies), it is known under its Dutch name.		Although the dishes served are undoubtedly Indonesian, the rijsttafel’s origins were colonial. During their presence in Indonesia, the Dutch introduced the rice table not only so they could enjoy a wide array of dishes at a single setting but also to impress visitors with the exotic abundance of their colony.[2]		Rijsttafels strive to feature an array of not only flavors and colors and degrees of spiciness but also textures, an aspect that is not commonly discussed in Western food. Such textures may include crispy, chewy, slippery, soft, hard, velvety, gelatinous, and runny.						The Dutch colonial feast, the rijsttafel, was created to provide a festive and official type of banquet that would represent the multi-ethnic nature of the Indonesian archipelago. Dishes were assembled from many of the far flung regions of Indonesia, where many different cuisines exist, often determined by ethnicity and culture of the particular island or island group — from Javanese favourite sateh, tempeh and seroendeng, to vegetarian cuisine gado-gado and lodeh with sambal lalab from Batavia and Preanger. From spicy rendang and gulai curry from the Minangkabau region in Sumatra, to East Indies ubiquitous dishes nasi goreng, soto ayam, and kroepoek crackers. Also Indonesian dishes from hybrid influences; such as Chinese babi ketjap, loempia, and bamie to European beef smoor. And there are many others from the hundreds of inhabited islands, which contain more than 300 regional and ethnic language groups.		During its centuries of popularity in Dutch East Indies, lines of servants or sarong-clad waitresses ceremoniously served the marathon meal on platters laden with steaming bowls of fragrant foods. The first to be served was a cone-shaped pile of rice on a large platter, which the server placed in the center of the table. The servers then surrounded the rice platter with as many as 40 small bowls holding meat and vegetable dishes as well as condiments. During its colonial heyday, the most celebrated rijsttafel in the Indies was served for Sunday luncheon at the Hotel des Indes in Batavia and the Savoy Homann Hotel in Bandung, where the rice was accompanied by sixty different dishes.[3]		Brought back to the Netherlands by former colonials and exiled Indonesians and Indo-Europeans (Eurasians) after Indonesia gained its independence in 1945, the rijsttafel was predominantly popular with Dutch families with colonial roots. On the other hand, when Indonesia proclaimed its independence in 1945, nationalist sentiment promoted the rejection of Dutch colonial culture and customs, including the flamboyant rice table. Today, the rice table has practically disappeared from Indonesia's restaurants and is served only by a handful of fine-dining restaurants in Indonesia.		More of a banquet than a regular meal, the rijsttafel has survived Indonesia's independence, composed as it is of indigenous Indonesian dishes, and is served in some mainstream restaurants in Indonesia. A typical rijsttafel will have several dining tables covered with different dishes; while in some fancy settings in Indonesia, each dish may be served by a separate waitress. Since about 1990, Indonesian food has become part of a mainstream interest in South East Asian cuisine, and there has been a proliferation of Indonesian restaurants in the Netherlands.		The following is a brief, but not nearly complete, list of examples of foods that may be found on a rijsttafel:		Despite its popularity in the Netherlands and abroad, the rijsttafel is rarely found in Indonesia. That is probably because most Indonesian meals consist of rice accompanied by only one, two or three dishes, mostly consisting of lauk (fish, chicken, meat, egg, or other source of protein), sayur (vegetable), and other side dishes. To consume more than that number of dishes at once (a rijsttafel might range from seven to forty dishes) is considered too extravagant and too expensive. The closest versions to rice table dishes readily available in Indonesia are local nasi Padang and nasi campur. However, in Indonesian restaurants around the world, especially in the Netherlands and South Africa, the rijsttafel is still popular.		Today only a handful of dining establishments in Indonesia ceremoniously serve elaborate colonial-style rijsttafel. In July 2011, the airline Garuda Indonesia launched Indonesian rijsttafel in Executive Class as its signature in-flight service.[4] This Indonesian signature dining was meant to introduce the passenger to a wide array of Indonesian cuisine in a single setting as part of Garuda Indonesia experience. This in-flight Indonesian rijsttafel includes Indonesian signature dishes; choices of nasi kuning or regular steamed rice, accompanied with choices of dishes such as satay, rendang, gado-gado grilled chicken rica, red snapper in yellow acar sauce, fried shrimp in sambal, potato perkedel and tempeh, also with kerupuk or rempeyek crackers.[5][6]		
Food is any substance[1] consumed to provide nutritional support for an organism. It is usually of plant or animal origin, and contains essential nutrients, such as carbohydrates, fats, proteins, vitamins, or minerals. The substance is ingested by an organism and assimilated by the organism's cells to provide energy, maintain life, or stimulate growth.		Historically, humans secured food through two methods: hunting and gathering and agriculture. Today, the majority of the food energy required by the ever increasing population of the world is supplied by the food industry.		Food safety and food security are monitored by agencies like the International Association for Food Protection, World Resources Institute, World Food Programme, Food and Agriculture Organization, and International Food Information Council. They address issues such as sustainability, biological diversity, climate change, nutritional economics, population growth, water supply, and access to food.		The right to food is a human right derived from the International Covenant on Economic, Social and Cultural Rights (ICESCR), recognizing the "right to an adequate standard of living, including adequate food", as well as the "fundamental right to be free from hunger".		Most food has its origin in plants. Some food is obtained directly from plants; but even animals that are used as food sources are raised by feeding them food derived from plants. Cereal grain is a staple food that provides more food energy worldwide than any other type of crop.[2] Corn (maize), wheat, and rice – in all of their varieties – account for 87% of all grain production worldwide.[3][4][5] Most of the grain that is produced worldwide is fed to livestock.		Some foods not from animal or plant sources include various edible fungi, especially mushrooms. Fungi and ambient bacteria are used in the preparation of fermented and pickled foods like leavened bread, alcoholic drinks, cheese, pickles, kombucha, and yogurt. Another example is blue-green algae such as Spirulina.[6] Inorganic substances such as salt, baking soda and cream of tartar are used to preserve or chemically alter an ingredient.		Many plants and plant parts are eaten as food and around 2,000 plant species are cultivated for food. Many of these plant species have several distinct cultivars.[7]		Seeds of plants are a good source of food for animals, including humans, because they contain the nutrients necessary for the plant's initial growth, including many healthful fats, such as omega fats. In fact, the majority of food consumed by human beings are seed-based foods. Edible seeds include cereals (corn, wheat, rice, et cetera), legumes (beans, peas, lentils, et cetera), and nuts. Oilseeds are often pressed to produce rich oils - sunflower, flaxseed, rapeseed (including canola oil), sesame, et cetera.[8]		Seeds are typically high in unsaturated fats and, in moderation, are considered a health food, although not all seeds are edible. Large seeds, such as those from a lemon, pose a choking hazard, while seeds from cherries and apples contain cyanide which could be poisonous only if consumed in large volumes.[9]		Fruits are the ripened ovaries of plants, including the seeds within. Many plants and animals have coevolved such that the fruits of the former are an attractive food source to the latter, because animals that eat the fruits may excrete the seeds some distance away. Fruits, therefore, make up a significant part of the diets of most cultures. Some botanical fruits, such as tomatoes, pumpkins, and eggplants, are eaten as vegetables.[10] (For more information, see list of fruits.)		Vegetables are a second type of plant matter that is commonly eaten as food. These include root vegetables (potatoes and carrots), bulbs (onion family), leaf vegetables (spinach and lettuce), stem vegetables (bamboo shoots and asparagus), and inflorescence vegetables (globe artichokes and broccoli and other vegetables such as cabbage or cauliflower).[11]		Animals are used as food either directly or indirectly by the products they produce. Meat is an example of a direct product taken from an animal, which comes from muscle systems or from organs.		Food products produced by animals include milk produced by mammary glands, which in many cultures is drunk or processed into dairy products (cheese, butter, etc.). In addition, birds and other animals lay eggs, which are often eaten, and bees produce honey, a reduced nectar from flowers, which is a popular sweetener in many cultures. Some cultures consume blood, sometimes in the form of blood sausage, as a thickener for sauces, or in a cured, salted form for times of food scarcity, and others use blood in stews such as jugged hare.[12]		Some cultures and people do not consume meat or animal food products for cultural, dietary, health, ethical, or ideological reasons. Vegetarians choose to forgo food from animal sources to varying degrees. Vegans do not consume any foods that are or contain ingredients from an animal source.		Most food has always been obtained through agriculture. With increasing concern over both the methods and products of modern industrial agriculture, there has been a growing trend toward sustainable agricultural practices. This approach, partly fueled by consumer demand, encourages biodiversity, local self-reliance and organic farming methods.[13] Major influences on food production include international organizations (e.g. the World Trade Organization and Common Agricultural Policy), national government policy (or law), and war.[14]		In popular culture, the mass production of food, specifically meats such as chicken and beef, has come under fire from various documentaries, most recently Food, Inc, documenting the mass slaughter and poor treatment of animals, often for easier revenues from large corporations. Along with a current trend towards environmentalism, people in Western culture have had an increasing trend towards the use of herbal supplements, foods for a specific group of people (such as dieters, women, or athletes), functional foods (fortified foods, such as omega-3 eggs), and a more ethnically diverse diet.[15]		Several organisations have begun calling for a new kind of agriculture in which agroecosystems provide food but also support vital ecosystem services so that soil fertility and biodiversity are maintained rather than compromised. According to the International Water Management Institute and UNEP, well-managed agroecosystems not only provide food, fiber and animal products, they also provide services such as flood mitigation, groundwater recharge, erosion control and habitats for plants, birds, fish and other animals.[16]		Animals, specifically humans, have five different types of tastes: sweet, sour, salty, bitter, and umami. As animals have evolved, the tastes that provide the most energy (sugar and fats) are the most pleasant to eat while others, such as bitter, are not enjoyable.[17] Water, while important for survival, has no taste.[18] Fats, on the other hand, especially saturated fats, are thicker and rich and are thus considered more enjoyable to eat.		Generally regarded as the most pleasant taste, sweetness is almost always caused by a type of simple sugar such as glucose or fructose, or disaccharides such as sucrose, a molecule combining glucose and fructose.[19] Complex carbohydrates are long chains and thus do not have the sweet taste. Artificial sweeteners such as sucralose are used to mimic the sugar molecule, creating the sensation of sweet, without the calories. Other types of sugar include raw sugar, which is known for its amber color, as it is unprocessed. As sugar is vital for energy and survival, the taste of sugar is pleasant.		The stevia plant contains a compound known as steviol which, when extracted, has 300 times the sweetness of sugar while having minimal impact on blood sugar.[20]		Sourness is caused by the taste of acids, such as vinegar in alcoholic beverages. Sour foods include citrus, specifically lemons, limes, and to a lesser degree oranges. Sour is evolutionarily significant as it is a sign for a food that may have gone rancid due to bacteria.[21] Many foods, however, are slightly acidic, and help stimulate the taste buds and enhance flavor.		Saltiness is the taste of alkali metal ions such as sodium and potassium. It is found in almost every food in low to moderate proportions to enhance flavor, although to eat pure salt is regarded as highly unpleasant. There are many different types of salt, with each having a different degree of saltiness, including sea salt, fleur de sel, kosher salt, mined salt, and grey salt. Other than enhancing flavor, its significance is that the body needs and maintains a delicate electrolyte balance, which is the kidney's function. Salt may be iodized, meaning iodine has been added to it, a necessary nutrient that promotes thyroid function. Some canned foods, notably soups or packaged broths, tend to be high in salt as a means of preserving the food longer. Historically salt has long been used as a meat preservative as salt promotes water excretion. Similarly, dried foods also promote food safety.[22]		Bitterness is a sensation often considered unpleasant characterized by having a sharp, pungent taste. Unsweetened dark chocolate, caffeine, lemon rind, and some types of fruit are known to be bitter.		Umami, the Japanese word for delicious, is the least known in Western popular culture but has a long tradition in Asian cuisine. Umami is the taste of glutamates, especially monosodium glutamate (MSG).[19] It is characterized as savory, meaty, and rich in flavor.[23] Salmon and mushrooms are foods high in umami.[24]		Many scholars claim that the rhetorical function of food is to represent the culture of a country, and that it can be used as a form of communication. According to Goode, Curtis and Theophano, food "is the last aspect of an ethnic culture to be lost".[25]		Many cultures have a recognizable cuisine, a specific set of cooking traditions using various spices or a combination of flavors unique to that culture, which evolves over time. Other differences include preferences (hot or cold, spicy, etc.) and practices, the study of which is known as gastronomy. Many cultures have diversified their foods by means of preparation, cooking methods, and manufacturing. This also includes a complex food trade which helps the cultures to economically survive by way of food, not just by consumption.		Some popular types of ethnic foods include Italian, French, Japanese, Chinese, American, Cajun, Thai, African cuisine, Indian andNepalese cuisine. Various cultures throughout the world study the dietary analysis of food habits. While evolutionarily speaking, as opposed to culturally, humans are omnivores, religion and social constructs such as morality, activism, or environmentalism will often affect which foods they will consume. Food is eaten and typically enjoyed through the sense of taste, the perception of flavor from eating and drinking. Certain tastes are more enjoyable than others, for evolutionary purposes.		Aesthetically pleasing and eye-appealing food presentations can encourage people to consume foods. A common saying is that people "eat with their eyes". Food presented in a clean and appetizing way will encourage a good flavor, even if unsatisfactory.[26][27]		Texture plays a crucial role in the enjoyment of eating foods. Contrasts in textures, such as something crunchy in an otherwise smooth dish, may increase the appeal of eating it. Common examples include adding granola to yogurt, adding croutons to a salad or soup, and toasting bread to enhance its crunchiness for a smooth topping, such as jam or butter.[28]		Another universal phenomenon regarding food is the appeal of contrast in taste and presentation. For example, such opposite flavors as sweetness and saltiness tend to go well together, as in kettle corn and nuts.		While many foods can be eaten raw, many also undergo some form of preparation for reasons of safety, palatability, texture, or flavor. At the simplest level this may involve washing, cutting, trimming, or adding other foods or ingredients, such as spices. It may also involve mixing, heating or cooling, pressure cooking, fermentation, or combination with other food. In a home, most food preparation takes place in a kitchen. Some preparation is done to enhance the taste or aesthetic appeal; other preparation may help to preserve the food; others may be involved in cultural identity. A meal is made up of food which is prepared to be eaten at a specific time and place.[29]		The preparation of animal-based food usually involves slaughter, evisceration, hanging, portioning, and rendering. In developed countries, this is usually done outside the home in slaughterhouses, which are used to process animals en masse for meat production. Many countries regulate their slaughterhouses by law. For example, the United States has established the Humane Slaughter Act of 1958, which requires that an animal be stunned before killing. This act, like those in many countries, exempts slaughter in accordance to religious law, such as kosher, shechita, and dhabīḥah halal. Strict interpretations of kashrut require the animal to be fully aware when its carotid artery is cut.[30]		On the local level, a butcher may commonly break down larger animal meat into smaller manageable cuts, and pre-wrap them for commercial sale or wrap them to order in butcher paper. In addition, fish and seafood may be fabricated into smaller cuts by a fish monger. However fish butchery may be done on board a fishing vessel and quick-frozen for preservation of quality.[31]		The term "cooking" encompasses a vast range of methods, tools, and combinations of ingredients to improve the flavor or digestibility of food. Cooking technique, known as culinary art, generally requires the selection, measurement, and combining of ingredients in an ordered procedure in an effort to achieve the desired result. Constraints on success include the variability of ingredients, ambient conditions, tools, and the skill of the individual cook.[32] The diversity of cooking worldwide is a reflection of the myriad nutritional, aesthetic, agricultural, economic, cultural, and religious considerations that affect it.[33]		Cooking requires applying heat to a food which usually, though not always, chemically changes the molecules, thus changing its flavor, texture, appearance, and nutritional properties.[34] Cooking certain proteins, such as egg whites, meats, and fish, denatures the protein, causing it to firm. There is archaeological evidence of roasted foodstuffs at Homo erectus campsites dating from 420,000 years ago.[35] Boiling as a means of cooking requires a container, and has been practiced at least since the 10th millennium BC with the introduction of pottery.[36]		There are many different types of equipment used for cooking.		Ovens are mostly hollow devices that get very hot (up to 500 °F (260 °C)) and are used for baking or roasting and offer a dry-heat cooking method. Different cuisines will use different types of ovens. For example, Indian culture uses a tandoor oven, which is a cylindrical clay oven which operates at a single high temperature.[37] Western kitchens use variable temperature convection ovens, conventional ovens, toaster ovens, or non-radiant heat ovens like the microwave oven. Classic Italian cuisine includes the use of a brick oven containing burning wood. Ovens may be wood-fired, coal-fired, gas, electric, or oil-fired.[38]		Various types of cook-tops are used as well. They carry the same variations of fuel types as the ovens mentioned above. Cook-tops are used to heat vessels placed on top of the heat source, such as a sauté pan, sauce pot, frying pan, or pressure cooker. These pieces of equipment can use either a moist or dry cooking method and include methods such as steaming, simmering, boiling, and poaching for moist methods, while the dry methods include sautéing, pan frying, and deep-frying.[39]		In addition, many cultures use grills for cooking. A grill operates with a radiant heat source from below, usually covered with a metal grid and sometimes a cover. An open pit barbecue in the American south is one example along with the American style outdoor grill fueled by wood, liquid propane, or charcoal along with soaked wood chips for smoking.[40] A Mexican style of barbecue is called barbacoa, which involves the cooking of meats such as whole sheep over an open fire. In Argentina, an asado (Spanish for "grilled") is prepared on a grill held over an open pit or fire made upon the ground, on which a whole animal or smaller cuts are grilled.[41]		Certain cultures highlight animal and vegetable foods in a raw state. Salads consisting of raw vegetables or fruits are common in many cuisines. Sashimi in Japanese cuisine consists of raw sliced fish or other meat, and sushi often incorporates raw fish or seafood. Steak tartare and salmon tartare are dishes made from diced or ground raw beef or salmon, mixed with various ingredients and served with baguettes, brioche, or frites.[42] In Italy, carpaccio is a dish of very thinly sliced raw beef, drizzled with a vinaigrette made with olive oil.[43] The health food movement known as raw foodism promotes a mostly vegan diet of raw fruits, vegetables, and grains prepared in various ways, including juicing, food dehydration, sprouting, and other methods of preparation that do not heat the food above 118 °F (47.8 °C).[44] An example of a raw meat dish is ceviche, a Latin American dish made with raw meat that is "cooked" from the highly acidic citric juice from lemons and limes along with other aromatics such as garlic.		Restaurants employ trained chefs who prepare food, and trained waitstaff to serve the customers. The term restaurant is credited to the French from the 19th century, as it relates to the restorative nature of the bouillons that were once served in them. However, the concept pre-dates the naming of these establishments, as evidence suggests commercial food preparation may have existed during the age of the city of Pompeii, and urban sales of prepared foods may have existed in China during the Song dynasty. The coffee shops or cafés of 17th century Europe may also be considered an early version of the restaurant.[45] In 2005, the population of the United States spent $496 billion for out-of-home dining. Expenditures by type of out-of-home dining were as follows: 40% in full-service restaurants, 37.2% in limited service restaurants (fast food), 6.6% in schools or colleges, 5.4% in bars and vending machines, 4.7% in hotels and motels, 4.0% in recreational places, and 2.2% in others, which includes military bases.[46]		Packaged foods are manufactured outside the home for purchase. This can be as simple as a butcher preparing meat, or as complex as a modern international food industry. Early food processing techniques were limited by available food preservation, packaging, and transportation. This mainly involved salting, curing, curdling, drying, pickling, fermenting, and smoking.[47] Food manufacturing arose during the industrial revolution in the 19th century.[48] This development took advantage of new mass markets and emerging technology, such as milling, preservation, packaging and labeling, and transportation. It brought the advantages of pre-prepared time-saving food to the bulk of ordinary people who did not employ domestic servants.[49]		At the start of the 21st century, a two-tier structure has arisen, with a few international food processing giants controlling a wide range of well-known food brands. There also exists a wide array of small local or national food processing companies.[50] Advanced technologies have also come to change food manufacture. Computer-based control systems, sophisticated processing and packaging methods, and logistics and distribution advances can enhance product quality, improve food safety, and reduce costs.[49]		The World Bank reported that the European Union was the top food importer in 2005, followed at a distance by the USA and Japan. Britain's need for food was especially well illustrated in World War II. Despite the implementation of food rationing, Britain remained dependent on food imports and the result was a long term engagement in the Battle of the Atlantic.		Food is traded and marketed on a global basis. The variety and availability of food is no longer restricted by the diversity of locally grown food or the limitations of the local growing season.[51] Between 1961 and 1999, there was a 400% increase in worldwide food exports.[52] Some countries are now economically dependent on food exports, which in some cases account for over 80% of all exports.[53]		In 1994, over 100 countries became signatories to the Uruguay Round of the General Agreement on Tariffs and Trade in a dramatic increase in trade liberalization. This included an agreement to reduce subsidies paid to farmers, underpinned by the WTO enforcement of agricultural subsidy, tariffs, import quotas, and settlement of trade disputes that cannot be bilaterally resolved.[54] Where trade barriers are raised on the disputed grounds of public health and safety, the WTO refer the dispute to the Codex Alimentarius Commission, which was founded in 1962 by the United Nations Food and Agriculture Organization and the World Health Organization. Trade liberalization has greatly affected world food trade.[55]		Food marketing brings together the producer and the consumer. It is the chain of activities that brings food from "farm gate to plate".[56] The marketing of even a single food product can be a complicated process involving many producers and companies. For example, fifty-six companies are involved in making one can of chicken noodle soup. These businesses include not only chicken and vegetable processors but also the companies that transport the ingredients and those who print labels and manufacture cans.[57] The food marketing system is the largest direct and indirect non-government employer in the United States.		In the pre-modern era, the sale of surplus food took place once a week when farmers took their wares on market day into the local village marketplace. Here food was sold to grocers for sale in their local shops for purchase by local consumers.[33][49] With the onset of industrialization and the development of the food processing industry, a wider range of food could be sold and distributed in distant locations. Typically early grocery shops would be counter-based shops, in which purchasers told the shop-keeper what they wanted, so that the shop-keeper could get it for them.[33][58]		In the 20th century, supermarkets were born. Supermarkets brought with them a self service approach to shopping using shopping carts, and were able to offer quality food at lower cost through economies of scale and reduced staffing costs. In the latter part of the 20th century, this has been further revolutionized by the development of vast warehouse-sized, out-of-town supermarkets, selling a wide range of food from around the world.[59]		Unlike food processors, food retailing is a two-tier market in which a small number of very large companies control a large proportion of supermarkets. The supermarket giants wield great purchasing power over farmers and processors, and strong influence over consumers. Nevertheless, less than 10% of consumer spending on food goes to farmers, with larger percentages going to advertising, transportation, and intermediate corporations.[60]		It is rare for price spikes to hit all major foods in most countries at once. Food prices rose 4% in the United States in 2007, the highest increase since 1990, and are expected to climb as much again in 2008. As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. In China, the price of pork jumped 58% in 2007. In the 1980s and 1990s, farm subsidies and support programs allowed major grain exporting countries to hold large surpluses, which could be tapped during food shortages to keep prices down. However, new trade policies have made agricultural production much more responsive to market demands, putting global food reserves at their lowest since 1983.[62]		Rising food prices over recent years have been linked with social unrest around the world, including rioting in Bangladesh and Mexico,[63] and the Arab Spring.[64] Food prices worldwide increased in 2008.[65][66] One cause of rising food prices is wealthier Asian consumers are westernizing their diets, and farmers and nations of the third world are struggling to keep up the pace. The past five years have seen rapid growth in the contribution of Asian nations to the global fluid and powdered milk manufacturing industry, which in 2008 accounted for more than 30% of production, while China alone accounts for more than 10% of both production and consumption in the global fruit and vegetable processing and preserving industry.[67]		In 2013 Overseas Development Institute researchers showed that rice has more than doubled in price since 2000, rising by 120% in real terms. This was as a result of shifts in trade policy and restocking by major producers. More fundamental drivers of increased prices are the higher costs of fertiliser, diesel and labour. Parts of Asia see rural wages rise with potential large benefits for the 1.3 billion (2008 estimate) of Asia's poor in reducing the poverty they face. However, this negatively impacts more vulnerable groups who don't share in the economic boom, especially in Asian and African coastal cities. The researchers said the threat means social-protection policies are needed to guard against price shocks. The research proposed that in the longer run, the rises present opportunities to export for Western African farmers with high potential for rice production to replace imports with domestic production.[68]		Institutions such as hedge funds, pension funds and investment banks like Barclays Capital, Goldman Sachs and Morgan Stanley[63] have been instrumental in pushing up prices in the last five years, with investment in food commodities rising from $65bn to $126bn (£41bn to £79bn) between 2007 and 2012, contributing to 30-year highs. This has caused price fluctuations which are not strongly related to the actual supply of food, according to the United Nations.[63] Financial institutions now make up 61% of all investment in wheat futures. According to Olivier De Schutter, the UN special rapporteur on food, there was a rush by institutions to enter the food market following George W Bush's Commodities Futures Modernization Act of 2000.[63] De Schutter told the Independent in March 2012: "What we are seeing now is that these financial markets have developed massively with the arrival of these new financial investors, who are purely interested in the short-term monetary gain and are not really interested in the physical thing – they never actually buy the ton of wheat or maize; they only buy a promise to buy or to sell. The result of this financialisation of the commodities market is that the prices of the products respond increasingly to a purely speculative logic. This explains why in very short periods of time we see prices spiking or bubbles exploding, because prices are less and less determined by the real match between supply and demand."[63] In 2011, 450 economists from around the world called on the G20 to regulate the commodities market more.[63]		Some experts have said that speculation has merely aggravated other factors, such as climate change, competition with bio-fuels and overall rising demand.[63] However, some such as Jayati Ghosh, professor of economics at Jawaharlal Nehru University in New Delhi, have pointed out that prices have increased irrespective of supply and demand issues: Ghosh points to world wheat prices, which doubled in the period from June to December 2010, despite there being no fall in global supply.[63]		Food deprivation leads to malnutrition and ultimately starvation. This is often connected with famine, which involves the absence of food in entire communities. This can have a devastating and widespread effect on human health and mortality. Rationing is sometimes used to distribute food in times of shortage, most notably during times of war.[14]		Starvation is a significant international problem. Approximately 815 million people are undernourished, and over 16,000 children die per day from hunger-related causes.[69] Food deprivation is regarded as a deficit need in Maslow's hierarchy of needs and is measured using famine scales.[70]		Food aid can benefit people suffering from a shortage of food. It can be used to improve peoples' lives in the short term, so that a society can increase its standard of living to the point that food aid is no longer required.[71] Conversely, badly managed food aid can create problems by disrupting local markets, depressing crop prices, and discouraging food production. Sometimes a cycle of food aid dependence can develop.[72] Its provision, or threatened withdrawal, is sometimes used as a political tool to influence the policies of the destination country, a strategy known as food politics. Sometimes, food aid provisions will require certain types of food be purchased from certain sellers, and food aid can be misused to enhance the markets of donor countries.[73] International efforts to distribute food to the neediest countries are often coordinated by the World Food Programme.[74]		Foodborne illness, commonly called "food poisoning", is caused by bacteria, toxins, viruses, parasites, and prions. Roughly 7 million people die of food poisoning each year, with about 10 times as many suffering from a non-fatal version.[75] The two most common factors leading to cases of bacterial foodborne illness are cross-contamination of ready-to-eat food from other uncooked foods and improper temperature control. Less commonly, acute adverse reactions can also occur if chemical contamination of food occurs, for example from improper storage, or use of non-food grade soaps and disinfectants. Food can also be adulterated by a very wide range of articles (known as "foreign bodies") during farming, manufacture, cooking, packaging, distribution, or sale. These foreign bodies can include pests or their droppings, hairs, cigarette butts, wood chips, and all manner of other contaminants. It is possible for certain types of food to become contaminated if stored or presented in an unsafe container, such as a ceramic pot with lead-based glaze.[75]		Food poisoning has been recognized as a disease since as early as Hippocrates.[76] The sale of rancid, contaminated, or adulterated food was commonplace until the introduction of hygiene, refrigeration, and vermin controls in the 19th century. Discovery of techniques for killing bacteria using heat, and other microbiological studies by scientists such as Louis Pasteur, contributed to the modern sanitation standards that are ubiquitous in developed nations today. This was further underpinned by the work of Justus von Liebig, which led to the development of modern food storage and food preservation methods.[77] In more recent years, a greater understanding of the causes of food-borne illnesses has led to the development of more systematic approaches such as the Hazard Analysis and Critical Control Points (HACCP), which can identify and eliminate many risks.[78]		Recommended measures for ensuring food safety include maintaining a clean preparation area with foods of different types kept separate, ensuring an adequate cooking temperature, and refrigerating foods promptly after cooking.[79]		Foods that spoil easily, such as meats, dairy, and seafood, must be prepared a certain way to avoid contaminating the people for whom they are prepared. As such, the rule of thumb is that cold foods (such as dairy products) should be kept cold and hot foods (such as soup) should be kept hot until storage. Cold meats, such as chicken, that are to be cooked should not be placed at room temperature for thawing, at the risk of dangerous bacterial growth, such as Salmonella or E. coli.[80]		Some people have allergies or sensitivities to foods which are not problematic to most people. This occurs when a person's immune system mistakes a certain food protein for a harmful foreign agent and attacks it. About 2% of adults and 8% of children have a food allergy.[81] The amount of the food substance required to provoke a reaction in a particularly susceptible individual can be quite small. In some instances, traces of food in the air, too minute to be perceived through smell, have been known to provoke lethal reactions in extremely sensitive individuals. Common food allergens are gluten, corn, shellfish (mollusks), peanuts, and soy.[81] Allergens frequently produce symptoms such as diarrhea, rashes, bloating, vomiting, and regurgitation. The digestive complaints usually develop within half an hour of ingesting the allergen.[81]		Rarely, food allergies can lead to a medical emergency, such as anaphylactic shock, hypotension (low blood pressure), and loss of consciousness. An allergen associated with this type of reaction is peanut, although latex products can induce similar reactions.[81] Initial treatment is with epinephrine (adrenaline), often carried by known patients in the form of an Epi-pen or Twinject.[82][83]		Human diet was estimated to cause perhaps around 35% of cancers in a human epidemiological analysis by Richard Doll and Richard Peto in 1981.[84] These cancer may be caused by carcinogens that are present in food naturally or as contaminants. Food contaminated with fungal growth may contain mycotoxins such as aflatoxins which may be found in contaminated corn and peanuts. Other carcinogens identified in food include heterocyclic amines generated in meat when cooked at high temperature, polyaromatic hydrocarbons in charred meat and smoked fish, and nitrosamines generated from nitrites used as food preservatives in cured meat such as bacon.[85]		Anticarcinogens that may help prevent cancer can also be found in many food especially fruit and vegetables. Antioxidants are important groups of compounds that may help remove potentially harmful chemicals. It is however often difficult to identify the specific components in diet that serve to increase or decrease cancer risk since many food, such as beef steak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.[85]		Dietary habits are the habitual decisions a person or culture makes when choosing what foods to eat.[89] Many cultures hold some food preferences and some food taboos. Dietary choices can also define cultures and play a role in religion. For example, only kosher foods are permitted by Judaism, halal foods by Islam, and in Hinduism beef is restricted.[90] In addition, the dietary choices of different countries or regions have different characteristics. This is highly related to a culture's cuisine.		Dietary habits play a significant role in the health and mortality of all humans. Imbalances between the consumed fuels and expended energy results in either starvation or excessive reserves of adipose tissue, known as body fat.[91] Poor intake of various vitamins and minerals can lead to diseases that can have far-reaching effects on health. For instance, 30% of the world's population either has, or is at risk for developing, iodine deficiency.[92] It is estimated that at least 3 million children are blind due to vitamin A deficiency.[93] Vitamin C deficiency results in scurvy.[94] Calcium, Vitamin D, and phosphorus are inter-related; the consumption of each may affect the absorption of the others. Kwashiorkor and marasmus are childhood disorders caused by lack of dietary protein.[95]		Many individuals limit what foods they eat for reasons of morality, or other habit. For instance, vegetarians choose to forgo food from animal sources to varying degrees. Others choose a healthier diet, avoiding sugars or animal fats and increasing consumption of dietary fiber and antioxidants.[96] Obesity, a serious problem in the western world, leads to higher chances of developing heart disease, diabetes, cancer and many other diseases.[97] More recently, dietary habits have been influenced by the concerns that some people have about possible impacts on health or the environment from genetically modified food.[98] Further concerns about the impact of industrial farming (grains) on animal welfare, human health, and the environment are also having an effect on contemporary human dietary habits. This has led to the emergence of a movement with a preference for organic and local food.[99]		Between the extremes of optimal health and death from starvation or malnutrition, there is an array of disease states that can be caused or alleviated by changes in diet. Deficiencies, excesses, and imbalances in diet can produce negative impacts on health, which may lead to various health problems such as scurvy, obesity, or osteoporosis, diabetes, cardiovascular diseases as well as psychological and behavioral problems. The science of nutrition attempts to understand how and why specific dietary aspects influence health.		Nutrients in food are grouped into several categories. Macronutrients are fat, protein, and carbohydrates. Micronutrients are the minerals and vitamins. Additionally, food contains water and dietary fiber.		As previously discussed, the body is designed by natural selection to enjoy sweet and fattening foods for evolutionary diets, ideal for hunters and gatherers. Thus, sweet and fattening foods in nature are typically rare and are very pleasurable to eat. In modern times, with advanced technology, enjoyable foods are easily available to consumers. Unfortunately, this promotes obesity in adults and children alike.		Some countries list a legal definition of food, often referring them with the word foodstuff. These countries list food as any item that is to be processed, partially processed, or unprocessed for consumption. The listing of items included as food include any substance intended to be, or reasonably expected to be, ingested by humans. In addition to these foodstuffs, drink, chewing gum, water, or other items processed into said food items are part of the legal definition of food. Items not included in the legal definition of food include animal feed, live animals (unless being prepared for sale in a market), plants prior to harvesting, medicinal products, cosmetics, tobacco and tobacco products, narcotic or psychotropic substances, and residues and contaminants.[100]		
Elderly care, or simply eldercare (also known in parts of the English speaking world as aged care), is the fulfillment of the special needs and requirements that are unique to senior citizens. This broad term encompasses such services as assisted living, adult day care, long term care, nursing homes (often referred to as residential care), hospice care, and home care. Because of the wide variety of elderly care found nationally, as well as differentiating cultural perspectives on elderly citizens, cannot to be limited to any one practice. For example, many countries in Asia use government-established elderly care quite infrequently, preferring the traditional methods of being cared for by younger generations of family members.		Elderly care emphasizes the social and personal requirements of senior citizens who need some assistance with daily activities and health care, but who desire to age with dignity. It is an important distinction, in that the design of housing, services, activities, employee training and such should be truly customer-centered. It is also noteworthy that a large amount of global elderly care falls under the unpaid market sector.[1]						The form of elderly care provided varies greatly among countries and is changing rapidly.[2] Even within the same country, regional differences exist with respect to the care for the elderly.[3] However, it has been observed that the global elderly consume the most health expenditures out of any other age group,[4] observation that shows comprehensive eldercare may be very similar. One must also account for an increasingly large proportion of global elderly, especially in developing nations, as continued pressure is put on limiting fertility and decreasing family size.[5]		Traditionally, elderly care has been the responsibility of family members and was provided within the extended family home.[6] Increasingly in modern societies, elderly care is now being provided by state or charitable institutions.[6] The reasons for this change include decreasing family size, the greater life expectancy of elderly people, the geographical dispersion of families, and the tendency for women to be educated and work outside the home.[6] Although these changes have affected European and North American countries first, they are now increasingly affecting Asian countries as well.[7]		In most western countries, elderly care facilities are residential family care homes, freestanding assisted living facilities, nursing homes, and Continuing care retirement communities (CCRCs).[8] A family care home is a residential home with support and supervisory personnel by an agency, organization, or individual that provides room and board, personal care and habilitation services in a family environment for at least two and no more than six persons.[9]		According to Family Caregiver Alliance, the majority of family caregivers are women:[10]		“Many studies have looked at the role of women and family caregiving. Although not all have addressed gender issues and caregiving specifically, the results are still generalizable [sic] to		According to the United States Department of Health and Human Services the older population—persons 65 years or older—numbered 39.6 million in 2009.[11] They represented 12.9% of the U.S. population, about one in every eight Americans.[11] By 2030, there will be about 72.1 million older persons, more than twice their number in 2000.[11] People 65-plus years old represented 12.4% of the population in the year 2000, but that is expected to grow to be 19% of the population by 2030.[11] This will mean more demand for elderly care facilities in the coming years. There were more than 36,000 assisted living facilities in the United States in 2009, according to the Assisted Living Federation of America [12] in 2009. More than 1 million senior citizens are served by these assisted living facilities.[12]		Last-year-of-life expenses represent 22% of all medical spending in the United States, 26% of all Medicare spending, 18% of all non-Medicare spending, and 25 percent of all Medicaid spending for the poor.[13]		In the United States, most of the large multi-facility providers are publicly owned and managed as for-profit businesses.[10] There are exceptions; the largest operator in the US is the Evangelical Lutheran Good Samaritan Society, a not-for-profit organization that manages 6,531 beds in 22 states, according to a 1995 study by the American Health Care Association.[14]		Given the choice, most elders would prefer to continue to live in their homes (aging in place).[15] Many elderly people gradually lose functioning ability and require either additional assistance in the home or a move to an eldercare facility.[15] The adult children of these elders often face a difficult challenge in helping their parents make the right choices.[16] Assisted living is one option for the elderly who need assistance with everyday tasks. It costs less than nursing home care but is still considered expensive for most people.[17] Home care services may allow seniors to live in their own home for a longer period of time.		One relatively new service in the United States that can help keep the elderly in their homes longer is respite care.[18] This type of care allows caregivers the opportunity to go on vacation or a business trip and know that their elder has good quality temporary care, for without this help the elder might have to move permanently to an outside facility. Another unique type of care cropping in U.S. hospitals is called acute care of elder units, or ACE units, which provide "a homelike setting" within a medical center specifically for the elderly.[19]		Information about long-term care options in the United States can be found by contacting the local Area Agency on Aging[20] or elder referral agencies such as Silver Living, or A Place for Mom. Furthermore, the U.S. government recommends evaluation of health care facilities through websites using data collected from sources such as Medicare records.[21]		In Canada, such privately run for-profit and not-for-profit facilities also exist. Because of cost factors, some provinces operate government-funded public facilities run by each province's or territory's Ministry of Health, or the government may subsidize the cost of the facility. In these care homes, elderly Canadians may pay for their care on a sliding scale based on annual income. The scale that they are charged on depends on whether they are considered “Long Term Care” or “Assisted Living.” For example, commencing in January 2010 seniors living in British Columbia’s government subsidized “Long Term Care” (also called “Residential Care”) will pay 80% of their after-tax income unless their After Tax Income is less than $16,500. The “Assisted Living” tariff is calculated more simply as 70% of the After-Tax Income.[22] As seen in the province of Ontario, there are waiting lists for many long-term care homes, though, so families may need to resort to hiring home health care or paying for a stay in a private retirement home.[23]		Aged care in Australia is designed to make sure that every Australian can contribute as much as possible toward their cost of care, depending on their individual income and assets.[24] This means that residents pay only what they can afford, and the Commonwealth government pays what a resident cannot. An Australian statutory authority, the Productivity Commission, conducted a review of aged care commencing in 2010 and reporting in 2011. That review concluded that approximately 80% of care for older Australians is informal care provided by family, friends and neighbours. Around a million people received government-subsidised aged care services, most of these receiving low-level community care support, with 160 000 people in permanent residential care. Expenditure on aged care by all governments in 2009-10 was approximately $11 billion.[25]		The need for increasing amounts of care, and known weaknesses in the care system (such as skilled workforce shortages and rationing of available care places), led several reviews in the 2000s to conclude that Australia's aged care system needs reform. This culminated in the 2011 Productivity Commission report and subsequent reform proposals.[26] In accordance with the Living Longer, Living Better amendments of 2013, assistance is provided in accordance with assessed care needs, with additional supplements available for people experiencing homelessness, dementia and veterans.[27]		Australian Aged Care is often considered complicated due to various state and federal funding. Furthermore, there are many acronyms that customers need to be aware of, including ACAT, ACAR, NRCP, HACC, CACP, EACH, EACH-D and CDC (Consumer Directed Care) to name a few.[26]		Care for the elderly in England is increasingly rationed according to a joint report by the King's Fund and Nuffield Trust. People are left to struggle without support on their own. Larger numbers of old people need help due to an aging population but less is being paid out to help them. A million people who need care get neither formal nor informal help.[28]		Due to health and economic benefits, the life expectancy in Nepal jumped from 27 years in 1951 to 65 in 2008.[29] Most elderly Nepali citizens, roughly 85%, live in rural areas.[29] Because of this, there is a significant lack of government sponsored programs or homes for the elderly. Traditionally, parents live with their children, and today, it is estimated that 90% of the elderly do live in the homes of their family.[29] This number is changing as more children leave home for work or school, leading to loneliness and mental problems in Nepali elderly.[29]		The Ninth Five-Year Plan included policies in an attempt to care for the elderly left without children as caretakers.[29] A Senior Health Facilities Fund has been established in each district.[29] The Senior Citizens Health Facilities Program Implementation Guideline, 2061BS provides medical facilities to the elderly, and to those that are poverty stricken, free medicine and health care in all districts.[29] In its yearly budget, the government has planned to fund free health care to all heart and kidney patients older than 75.[29] Unfortunately, many of these plans are overly ambitious, which has been recognized by the Nepali government.[29] Nepal is a developing nation and may not be able to fund all of these programs after the development of an Old Age Allowance, or OAA. OAA provides a monthly stipend to all citizens over 70 and widows over 60.[29]		There are a handful of private day care facilities for elderly, but it is limited to the capital city. These day care services are very expensive and out of reach for the general public.		Thailand has observed global patterns of an enlarging elderly class: as fertility control is encouraged and medical advances are made, births shrink and lives age.[5] The Thai government is noticing and concerned about this trend, but tends to let families care for their elderly members rather than create extraneous policies for them.[30] As of 2011, there are only 25 state sponsored homes for the elderly, with no more than a few thousand members of each home.[30] Such programs are largely run by volunteers and are questionable by quality of care, considering there is not always a guarantee care will be available. Private care is tough to follow, often based on assumptions. Because children are less likely to care for their parents, private caretakers are in demand.[30] Volunteer NGOs are available but in very limited quantities.[30]		While there are certainly programs available for use by the elderly in Thailand, questions of equity have risen since their introduction.[31] The rich elderly in Thailand are much more likely to have access to care resources, while the poor elderly are more likely to actually use their acquired health care, as observed in a study by Bhumisuk Khananurak.[31] However, over 96% of the nation has health insurance with varying degrees of care available.[31]		India's cultural view of elderly care is similar to that of Nepal's. Parents are typically cared for by their children into old age, most commonly by their sons.[32] It should be noted that in these countries, elderly citizens, especially men, are viewed in very high regard. Traditional values demand honor and respect for older, wiser people.[33] India is facing the same problem as many developing nations in that its elderly population is increasing tremendously, with a current estimate of 90 million over the age of 60.[34] Using data on health and living conditions from the India's 60th National Sample Survey, a study found that almost a quarter of the elderly reported poor health. Reports of poor health were clustered among the poor, single, lower-educated and economically inactive groups.[35]		Under its eleventh Five-Year plan, the Indian government has made many strides similar to that of Nepal. Article 41 of the Indian Constitution states that elderly citizens will be guaranteed Social Security support for health care and welfare.[34] A section of the 1973 Criminal Procedure Code, alluding to its traditional background, mandates that children support their parents if they no longer can themselves.[34] NGOs, however, are prevalent in Indian elderly care, providing homes and volunteer care, but governmental policies and organizations are accessible.[34]		A distinction is generally made between medical and non-medical care, care provided by people who are not medical professionals. The latter is much less likely to be covered by insurance or public funds. In the US, 67% of the one million or so residents in assisted living facilities pay for care out of their own funds.[36] The rest get help from family and friends and from state agencies. Medicare does not pay unless skilled-nursing care is needed and given in certified skilled nursing facilities or by a skilled nursing agency in the home. Assisted living facilities usually do not meet Medicare's requirements. However, Medicare does pay for some skilled care if the elderly person meets the requirements for the Medicare home health benefit. [37]		Thirty-two U.S. states pay for care in assisted living facilities through their Medicaid waiver programs. Similarly, in the United Kingdom the National Health Service provides medical care for the elderly, as for all, free at the point of use, but social care is only paid for by the state in Scotland; England, Wales and Northern Ireland are yet to introduce any legislation on the matter, so currently social care is only funded by public authorities when a person has exhausted their private resources, for example, by selling their home. Money provided for supporting elderly people in the UK has fallen by 20% per person during the ten years from 2005 to 2015 and in real terms the fall was greater. Experts claim that vulnerable UK people do not get what they need.[38]		However, elderly care is focused on satisfying the expectations of two tiers of customers: the resident customer and the purchasing customer, who are often not identical, since relatives or public authorities rather than the resident may be providing the cost of care. Where residents are confused or have communication difficulties, it may be very difficult for relatives or other concerned parties to be sure of the standard of care being given, and the possibility of elder abuse is a continuing source of concern. The Adult Protective Services Agency — a component of the human service agency in most states — is typically responsible for investigating reports of domestic elder abuse and providing families with help and guidance. Other professionals who may be able to help include doctors or nurses, police officers, lawyers, and social workers.[39]		Promoting independence in self-care can provide older adults with the capability to maintain independence longer and can leave them with a sense of achievement when they complete a task unaided. Older adults that require assistance with activities of daily living are at a greater risk of losing their independence with self-care tasks as dependent personal behaviours are often met with reinforcement from caregivers.[40] It is important for caregivers to ensure that measures are put into place to preserve and promote function rather than contribute to a decline in status in an older adult that has physical limitations. Caregivers need to be conscious of actions and behaviors that cause older adults to become dependent on them and need to allow older patients to maintain as much independence as possible. Providing information to the older patient on why it is important to perform self-care may allow them to see the benefit in performing self-care independently. If the older adult is able to complete self-care activities on their own, or even if they need supervision, encourage them in their efforts as maintaining independence can provide them with a sense of accomplishment and the ability to maintain independence longer.[41]		Impaired mobility is a major health concern for older adults, affecting 50% of people over 85 and at least a quarter of those over 75. As adults lose the ability to walk, to climb stairs, and to rise from a chair, they become completely disabled. The problem cannot be ignored because people over 65 constitute the fastest growing segment of the U.S. population.		Therapy designed to improve mobility in elderly patients is usually built around diagnosing and treating specific impairments, such as reduced strength or poor balance. It is appropriate to compare older adults seeking to improve their mobility to athletes seeking to improve their split times. People in both groups perform best when they measure their progress and work toward specific goals related to strength, aerobic capacity, and other physical qualities. Someone attempting to improve an older adult’s mobility must decide what impairments to focus on, and in many cases, there is little scientific evidence to justify any of the options. Today, many caregivers choose to focus on leg strength and balance. New research suggests that limb velocity and core strength may also be important factors in mobility.[42]		The family is one of the most important providers for the elderly. In fact, the majority of caregivers for the elderly are often members of their own family, most often a daughter or a granddaughter. Family and friends can provide a home (i.e. have elderly relatives live with them), help with money and meet social needs by visiting, taking them out on trips, etc.		One of the major causes of elderly falls is hyponatremia, an electrolyte disturbance when the level of sodium in a person's serum drops below 135 mEq/L. Hyponatremia is the most common electrolyte disorder encountered in the elderly patient population. Studies have shown that older patients are more prone to hyponatremia as a result of multiple factors including physiologic changes associated with aging such as decreases in glomerular filtration rate, a tendency for defective sodium conservation, and increased vasopressin activity. Mild hyponatremia ups the risk of fracture in elderly patients because hyponatremia has been shown to cause subtle neurologic impairment that affects gait and attention, similar to that of moderate alcohol intake.[43]		Legal incapacity is an invasive and sometimes, difficult legal procedure. It requires that a person file a petition with the local courts, stating the elderly person lacks the capacity to carry out activities that include making medical decisions, voting, making gifts, seeking public benefits, marrying, managing property and financial affairs, choosing where to live and who they socialize with. Most states' laws require that a minimum of two doctors or other health professionals, provide reports as evidence of such incompetence and the person must be represented by an attorney. Only then can the individual's legal rights be removed and legal supervision by a guardian or conservator be initiated. The legal guardian or conservator is the person to whom the court delegates the responsibility of acting on the incapacitated person's behalf and must report regularly his or her activities to the court.		A less restrictive alternative to legal incapacity is the use of "Advance Directives"; powers of attorney, trusts, living wills and health care directives. The person who has these documents in place, should have prepared them with their attorney when that person had capacity. So, if the time comes when that person lacks capacity to carry out those tasks laid out in the documents, the person they named (their agent) can step in to make decisions on their behalf. The agent has a duty to act as that person would have and in their best interest.		 This article incorporates public domain material from the United States Government document "A Profile of Older Americans: 2010, Department of Health & Human Services".				
A free lunch is a sales enticement that offers a meal at no cost in order to attract customers and increase revenues from other offerings. It was a tradition once common in saloons in many places in the United States, with the phrase appearing in U.S. literature from about 1870 to the 1920s. These establishments included a "free" lunch, varying from rudimentary to quite elaborate, with the purchase of at least one drink. These free lunches were typically worth far more than the price of a single drink.[1] The saloon-keeper relied on the expectation that most customers would buy more than one drink, and that the practice would build patronage for other times of day.		Free food or drink is sometimes supplied in contemporary times, often by gambling establishments such as casinos.		The saying "there ain't no such thing as a free lunch" refers to this custom, meaning that things which appear to be free are always paid for in some way.						In 1875, The New York Times wrote of elaborate free lunches as a "custom peculiar to the Crescent City" (New Orleans), saying, "In every one of the drinking saloons which fill the city a meal of some sort is served free every day. The custom appears to have prevailed long before the war.... I am informed that there are thousands of men in this city who live entirely on the meals obtained in this way." As described by this reporter,		A free lunch-counter is a great leveler of classes, and when a man takes up a position before one of them he must give up all hope of appearing either dignified or consequential. In New-Orleans all classes of the people can be seen partaking of these free meals and pushing and scrambling to be helped a second time. [At one saloon] six men were engaged in preparing drinks for the crowd that stood in front of the counter. I noticed that the price charged for every kind of liquor was fifteen cents, punches and cobblers costing no more than a glass of ale.		The repast included "immense dishes of butter," large baskets of bread, "a monster silver boiler filled with a most excellent oyster soup," "a round of beef that must have weighed at least forty pounds," vessels filled with potatoes, stewed mutton, stewed tomatoes, and macaroni à la Français. The proprietor said that the patrons included "at least a dozen old fellows who come here every day, take one fifteen cent drink, eat a dinner which would have cost them $1 in a restaurant, and then complain that the beef is tough or the potatoes watery."[1] ($0.15 in 1875 is roughly equivalent to $2.94 today; $1 in 1875 to $19.62 today)[2]		The nearly indigent "free-lunch fiend" was a recognized social type. An 1872 New York Times story about "loafers and free-lunch men" who "toil not, neither do they spin, yet they 'get along,'" visiting saloons, trying to bum drinks from strangers; "should this inexplicable lunch-fiend not happen to be called to drink, he devours whatever he can, and, while the bartender is occupied, tries to escape unnoticed."[3]		In American saloon bars from the late 19th century until Prohibition, bouncers had, in addition to their role of removing drunks who were too intoxicated to keep buying drinks, fighters, and troublemakers, the unusual role of protecting the saloon's free buffet. To attract business, "...many saloons lured customers with offers of a "free lunch"—usually well salted to inspire drinking, and the saloon "bouncer" was generally on hand to discourage [those with too] hearty appetites".[4]		The custom was well-developed in San Francisco. An 1886 story on the fading of the days of the 1849 California Gold Rush calls "the free lunch fiend the only landmark of the past." It asks "How do all these idle people live" and asserts, "It is the free lunch system that keeps them alive. Take away that peculiarly California institution and they would all starve."[5] Rudyard Kipling, writing in 1891, noted how he		came upon a barroom full of bad Salon pictures in which men with hats on the backs of their heads were wolfing food from a counter. It was the institution of the "free lunch" I had struck. You paid for a drink and got as much as you wanted to eat. For something less than a rupee a day a man can feed himself sumptuously in San Francisco, even though he be a bankrupt. Remember this if ever you are stranded in these parts.[6]		A 1919 novel compared a war zone to the free lunch experience by saying "the shells and shrapnels was flyin round and over our heads thicker than hungry bums [(homeless people)] around a free lunch counter."[7]		The temperance movement opposed the free lunch as promoting the consumption of alcohol. An 1874 history of the movement writes:		In the cities, there are prominent rooms on fashionable streets that hold out the sign "Free Lunch." Does it mean that some [philanthropist]... has gone systematically to work setting out tables... placing about them a score of the most beautiful and winning young ladies... hiring a band of music? Ah, no!... there are men who do all this in order to hide the main feature of their peculiar institution. Out of sight is a well-filled bar, which is the centre about which all these other things are made to revolve. All the gathered fascinations and attractions are as so many baits to allure men into the net that is spread for them. Thus consummate art plies the work of death, and virtue, reputation, and every good are sacrificed at these worse than Moloch shrines.[8]		A number of writers, however, suggest that the free lunch actually performed a social relief function. Reformer William T. Stead commented that in winter in 1894 the suffering of the poor in need of food		would have been very much greater had it not been for the help given by the labor unions to their members and for an agency which, without pretending to be of much account from a charitable point of view, nevertheless fed more hungry people in Chicago than all the other agencies, religious, charitable, and municipal, put together. I refer to the Free Lunch of the saloons. There are from six to seven thousand saloons in Chicago. In one half of these a free lunch is provided every day of the week.		He states that "in many cases the free lunch is really a free lunch," citing an example of a saloon which did not insist on a drink purchase, although commenting that this saloon was "better than its neighbors." Stead cites a newspaper's estimate that the saloon keepers fed 60,000 people a day and that this represented a contribution of about $18,000 a week toward the relief of the destitute in Chicago.[9]		In 1896, the New York State legislature passed the Raines law which was intended to regulate liquor traffic. Among its many provisions, one forbade the sale of liquor unless accompanied by food; another outlawed the free lunch. In 1897, however, it was amended to allow free lunches again.[10]		
A food court (in Asia-Pacific also called food hall or hawker centre)[1] is generally an indoor plaza or common area within a facility that is contiguous with the counters of multiple food vendors and provides a common area for self-serve dinner.[2][3]		Food courts may be found in shopping malls, airports, and parks. In various regions (such as Asia, the Americas, and Africa), it may be a standalone development. In some places of learning such as high schools and universities, food courts have also come to replace or complement traditional cafeterias.[4][5][6]		The average cost of a meal per person in an American food court in 2004 was US$6 (equivalent to $7.61 in 2016).[7]						Food courts consist of a number of vendors at food stalls or service counters. Meals are ordered at one of the vendors and then carried to a common dining area. The food may also be ordered as takeout for consumption at another location, such as a home, or workplace. In this case, it may be packaged in foam food containers, though one common food tray used by all the stalls might also be utilitzed to allow the food to be carried to the table. Food courts may also have shops which sell prepared meals for consumers to take home and reheat, making the food court a daily stop for some.[7]		Food is usually eaten with plastic cutlery, and sporks are sometimes used to avoid the necessity of providing both forks and spoons. There are exceptions: Carrefour Laval requires its food court tenants to use solid dinnerware and cutlery which it provides.[8]		Typical North American and European food courts have mostly fast food chains such as McDonald's and Sbarro, with perhaps a few smaller private vendors. Berkshire Hathaway is also a frequent presence at food courts via their Dairy Queen and Orange Julius divisions. Cuisines and choices are varied, with larger food courts offering more global choices. Asian and African food courts are mostly private vendors that offer local cuisine. In Singapore, food courts and hawker centres are the people's main eating choice when dining out.[9]		Common materials used in constructing food courts are tile, linoleum, Formica, stainless steel, and glass, all of which facilitate easy cleanup.[7]		The second-floor food court at the Paramus Park shopping mall in Paramus, New Jersey, which opened in March 1974, has been credited as the first successful shopping mall food court in America. However, the food court at Sherway Gardens shopping center in Toronto, Canada was constructed three years earlier. [10] Built by The Rouse Company, one of the leading mall building companies of the time, it followed an unsuccessful attempt at the Plymouth Meeting Mall in 1971, which reportedly failed because it was "deemed too small and insufficiently varied."[10][11]		
A dining club is a social group, usually requiring membership (which may, or may not be available only to certain people), which meets for dinners and discussion on a regular basis. They may also often have guest speakers.						A dining club differs from a gentlemen's club in that it does not have permanent premises, often changing the location of its meetings and dinners.		Clubs may limit their membership to those who meet highly specific membership requirements, for example the Coningsby Club requires that one was a member of either OUCA or CUCA, the Conservative Associations at the Universities of Oxford and Cambridge respectively.[citation needed] Others may require applicants to pass an interview, or simply pay a membership fee.		In the United States, similar clubs that limit membership to students of a particular university are referred to as eating clubs. Replaced largely by the modern fraternity and sorority system in the United States, eating clubs are now limited to a few colleges and universities, most notably Princeton University.		Dining clubs often have reciprocity with other dining clubs across the nation or even worldwide. Some are able to arrange reciprocity with other private social clubs with more facilities besides dining such as overnight guest rooms and a gym. Examples of such social clubs include Penn Club of New York City that has reciprocity with India House Club at 1 Hanover Square.		This list is incomplete. Date of founding in brackets		18th-century foundations		19th-century foundations		20th- and 21st-century foundations		The Thursday Club - a monthly dining club featured in the novel The Three Hostages by John Buchan.		The Twelve True Fishermen is the name of a fictional club, the title of a short story by G. K. Chesterton in which his detective Father Brown solves the riddle of the disappearance of the club's silver.		
Bandeja paisa, (Paisa refers to a person from the Paisa Region and bandeja is Spanish for platter) with variations known as bandeja de arriero, bandeja montañera, or bandeja antioqueña, is a typical meal popular in Colombian cuisine, especially of the Antioquia department and the Paisa Region, as well as with the Colombian Coffee-Growers Axis, (Caldas Department, Quindío, Risaralda) and part of Valle del Cauca.		The main characteristic of this dish is the generous amount and variety of food in a traditional bandeja paisa: red beans cooked with pork, white rice, carne molida (ground meat), chicharrón, fried egg, plantain (plátano maduro), chorizo, arepa, hogao sauce, black pudding (morcilla), avocado and lemon.[1] It is served in a platter or a tray.[2]						The origin of the bandeja paisa was influenced by several different cultures that inhabited Colombia throughout the centuries, including the indigenous peoples of Colombia, as well as colonial Spaniards and Africans. In the 19th century, French and British colonialists also brought their cuisine with them.[3]		The current form and presentation of the Paisa platter is relatively recent. There are no references in the food writing about this dish before 1950. It is probably an interpretation of the local restaurants of simpler peasant dishes. One of its most prominent features is the juxtaposition of native American and European ingredients, which is also observed in other mestizo dishes of Latin American cuisine, such as Venezuelan pabellón criollo or Costa Rican gallo pinto.		A Paisa platter is traditionally served in a large, oval-shaped tray due to the amount of food that is served. Side dishes include mazamorra (a maize-derived beverage similar to atole) with milk and ground panela.		There are several variants of the dish all over the country with deletion or addition of ingredients, which cannot be recognized as bandeja paisa in the strictest sense. Some Antioquian restaurants offer an "extended" bandeja paisa, also known as "seven meats platter", which contains, besides the aforementioned ingredients, grilled steak, grilled pork and liver. A diet- friendly version of the dish is very popular in Bogotá, which replaces pork with grilled chicken breast, black pudding with salad and chorizo with a wiener.[4]		Is advisable to drink with this dinner something soft, like a lemonade, water or fruit juice without milk.		In 2005, the Colombian government planned to make bandeja paisa the national dish, with name changed to "bandeja montañera" (mountain tray) to avoid the exclusion of people outside the Paisa Region.[5] A number of people opposed this designation, arguing that only a small percentage of the Colombian population consumes it in regular basis, that it is originated in a single region of Colombia (Antioquia).[6] However, the suggested alternative, sancocho, is not a distinctively Colombian dish, as it is known and enjoyed in many other countries, such as Cuba, Venezuela, the Canary Islands, the Dominican Republic and Panama.[7] Due to the widespread ubiquity of sancocho, often Colombian ajiaco is instead considered the most indicative Colombian dish.		Nonetheless, the commercial Colombian tourism industry has pushed ahead without official government sanction by emblazoning ads, menus, and brochure information with imagery of the bandeja paisa as the single most typical Colombian dish.[8]		
A retailer or a shop is a business that presents a selection of goods and offers to trade or sell them to customers for money or other goods. Shopping is an activity in which a customer browses the available goods or services presented by one or more retailers with the intent to purchase a suitable selection of them. In some contexts it may be considered a leisure activity as well as an economic one.		In modern days customer focus is more transferred towards online shopping; worldwide people order products from different regions and online retailers deliver their products to their homes, offices or wherever they want. The B2C (business to consumer) process has made it easy for consumers to select any product online from a retailer's website and have it delivered to the consumer within no time. The consumer does not need to consume his energy by going out to the stores and saves his time and cost of travelling.		The shopping experience can range from delightful to terrible, based on a variety of factors including how the customer is treated, convenience, the type of goods being purchased, and mood.[1]		The shopping experience can also be influenced by other shoppers. For example, research from a field experiment found that male and female shoppers who were accidentally touched from behind by other shoppers left a store earlier than people who had not been touched and evaluated brands more negatively, resulting in the Accidental Interpersonal Touch effect.[2]		According to a 2000 report, in the U.S. state of New York, women purchase 80% of all consumer goods and influence 80% of health-care decisions.[3]						In ancient Greece, the agora served as a marketplace where merchants kept stalls or shops to sell their goods. Ancient Rome utilized a similar marketplace known as the forum. For example, there was Trajan's Market with tabernae that served as retailing units.		Shopping lists are known to have been used by Romans, as one was discovered near Hadrian's wall dated back to 75–125 CE written for a soldier.[4]		Fairs and markets were established to facilitate the exchange of goods and services. People would shop for goods at a weekly market in nearby towns.		The modern phenomenon of shopping is closely linked to the emergence of the consumer society in the 18th century. Over the course of the two centuries from 1600 onwards, the purchasing power of the average Englishman steadily rose. Sugar consumption doubled in the first half of the 18th century and the availability of a wide range of luxury goods, including tea and cotton saw a sustained increase.[5]		Marketplaces dating back to the Middle Ages, expanded as shopping centres, such as the New Exchange, opened in 1609 by Robert Cecil in the Strand. Shops started to become important as places for Londoners to meet and socialise and became popular destinations alongside the theatre. Restoration London also saw the growth of luxury buildings as advertisements for social position with speculative architects like Nicholas Barbon and Lionel Cranfield.		Much pamphleteering of the time was devoted to justifying conspicuous consumption and private vice for luxury goods for the greater public good. This then scandalous line of thought caused great controversy with the publication of Bernard Mandeville's influential work Fable of the Bees in 1714, in which he argued that a country's prosperity ultimately lay in the self-interest of the consumer.[6]		These trends were vastly accelerated in the 18th century, as rising prosperity and social mobility increased the number of people with disposable income for consumption. Important shifts included the marketing of goods for individuals as opposed to items for the household, and the new status of goods as status symbols, related to changes in fashion and desired for aesthetic appeal, as opposed to just their utility. The pottery inventor and entrepreneur, Josiah Wedgewood, pioneered the use of marketing techniques to influence and manipulate the direction of the prevailing tastes.[7]		As the century wore on a tremendous variety of goods and manufactures were steadily made available for the urban middle and upper classes. This growth in consumption led to the rise of 'shopping' - a proliferation of retail shops selling particular goods and the acceptance of shopping as a cultural activity in its own right. Specific streets and districts became devoted to retail, including the Strand and Picadilly in London.[5]		The first display windows in shops were installed in the late 18th century in London. Retailer Francis Place was one of the first to experiment with this new retailing method at his tailoring establishment in Charing Cross, where he fitted the shop-front with large plate glass windows. Although this was condemned by many, he defended his practice in his memoirs, claiming that he:		Retailers designed attractive shop fronts to entice patronage, using bright lights, advertisements and attractively arranged goods. The goods on offer were in a constant state of change, due to the frenetic change in fashions. A foreign visitor thought that London was "A world of gold and silver plate, then pearls and gems shedding their dazzling lustre, home manufactures of the most exquisite taste, an ocean of rings, watches, chains, bracelets, perfumes, ready-dresses, ribbons, lace, bonnets, and fruits from all the zones of the habitable world".[5]		The next stage in shopping was the transition from 'single-function' shops selling one type of good, to the department store where a large variety of goods were sold. As economic growth, fueled by the Industrial Revolution at the turn of the 19th-century, steadily expanded, the affluent bourgeois middle-class grew in size and wealth. This urbanized social group was the catalyst for the emergence of the retail revolution of the period. The first reliably dated department store to be established, was Harding, Howell & Co, which opened in 1796 on Pall Mall, London.[9]		This venture was described as being a public retail establishment offering a wide range of consumer goods in different departments. This pioneering shop was closed down in 1820 when the business partnership was dissolved. Department stores were established on a large scale from the 1840s and 50s, in France, the United Kingdom and the US		A larger commercial zone can be found in many cities, more formally called a central business district, but more commonly called "downtown" in the United States, or in Arab cities, souks. Shopping hubs, or shopping centers, are collections of stores; that is a grouping of several businesses.		Typical examples include shopping malls, town squares, flea markets and bazaars.		A shopping hub or centre, is a collection of retail, entertainment and service stores designed to serve products and services to the surrounding region. Traditionally, shopping hubs were called bazaars or marketplaces which were generally an assortment of stalls lining streets selling a large variety of goods.[10] The modern shopping centre is now different from its antecedents, the stores are commonly in individual buildings or compressed into one large structure (Mall).[11] The first modern shopping mall was The Country Club Plaza in Kansas City which opened in 1922, from there the first enclosed mall was designed by Victor Gruen and opened in 1956 as Southdale Centre in Edina, Minnesota, a suburb of Minneapolis. Malls peaked in America in the 1980s-1990s when many larger malls (more than 37,000 sq m in size) were built, attracting consumers from within a 32 km radius with their luxurious department stores.[12] There are different types of malls around the world, the Superregional malls are very large malls that contain at least five department stores and 300 shops, this mall can appeal to a broad radius (up to a 160-km). A regional mall can contain at least two department stores or "anchor stores".[13] The smaller malls are often called open-air strip centres or mini-marts and are typically attached to a grocery store or supermarket. The smaller malls are less likely to include the same features of a large mall such as an indoor concourse, but are beginning to evolve to become enclosed to comply with all weather and customer preferences.[12]		Stores are divided into multiple categories of stores which sell a selected set of goods or services. Usually they are tiered by target demographics based on the disposable income of the shopper. They can be tiered from cheap to pricey.		Some shops sell secondhand goods. Often the public can also sell goods to such shops. In other cases, especially in the case of a nonprofit shop, the public donates goods to these shops, commonly known as thrift stores in the United States, charity shops in the United Kingdom, or op shops in Australia and New Zealand. In give-away shops goods can be taken for free. In antique shops, the public can find goods that are older and harder to find. Sometimes people are broke and borrow money from a pawn shop using an item of value as collateral. College students are known to resell books back through college textbook bookstores. Old used items are often distributed through surplus stores.		Various types of retail stores that specialize in the selling of goods related to a theme include bookstores, boutiques, candy shops, liquor stores, gift shops, hardware stores, hobby stores, pet stores, pharmacies, sex shops and supermarkets.		Other stores such as big-box stores, hypermarkets, convenience stores, department stores, general stores, dollar stores sell a wider variety of products not horizontally related to each other.		Home mail delivery systems and modern technology (such as television, telephones, and the Internet), in combination with electronic commerce, allow consumers to shop from home. There are three main types of home shopping: mail or telephone ordering from catalogs; telephone ordering in response to advertisements in print and electronic media (such as periodicals, TV and radio); and online shopping. Online shopping has completely redefined the way people make their buying decisions; the Internet provides access to a lot of information about a particular product, which can be looked at, evaluated, and comparison-priced at any given time. Online shopping allows the buyer to save the time and expense, which would have been spent traveling to the store or mall. According to technology and research firm Forrester, mobile purchases or mcommerce will account for 49% of ecommerce, or $252 billion in sales, by 2020[14]		Convenience stores are common in North America, and are often called "bodegas" in Spanish-speaking communities or "dépanneurs" in French-speaking ones. Sometimes peddlers and ice cream trucks pass through neighborhoods offering goods and services. Also, garage sales are a common form of second hand resale.		Neighbourhood shopping areas and retailers give value to a community by providing various social and community services (like a library), and a social place to meet. Neighbourhood retailing differs from other types of retailers such as destination retailers because of the difference in offered products and services, location and popularity.[15] Neighbourhood retailers include stores such as; Food shops/marts, dairies, Pharmacies, Dry cleaners, Hairdressers/barbers, Bottle shops, Cafés and take-away shops . Destination retailers include stores such as; Gift shops, Antique shops, Pet groomers, Engravers, Tattoo parlour, Bicycle shops, Herbal dispensary clinics, Art galleries, Office Supplies and framers. The neighbourhood retailers sell essential goods and services to the residential area they are located in. There can be many groups of neighbourhood retailers in different areas of a region or city, but destination retailers are often part of shopping malls where the numbers of consumers is higher than that of a neighbourhood retail area. The destination retailers are becoming more prevalent as they can provide a community with more than the essentials, they offer an experience, and a wider scope of goods and services.		The party plan is a method of marketing products by hosting a social event, using the event to display and demonstrate the product or products to those gathered, and then to take orders for the products before the gathering ends.		Shopping frenzies are periods of time where a burst of spending occurs, typically near holidays in the United States, with Christmas shopping being the biggest shopping spending season, starting as early as October and continuing until after Christmas.		Some religions regard such spending seasons as being against their faith and dismiss the practice. Many contest the over-commercialization and the response by stores that downplay the shopping season often cited in the War on Christmas.		The National Retail Federation (NRF) also highlights the importance of back-to-school shopping for retailers which comes second behind holiday shopping, when buyers often buy clothing and school supplies for their children.[16] In 2006, Americans spend over $17 billion on their children, according to a NRF survey.[citation needed]		Seasonal shopping consists of buying the appropriate clothing for the particular season. In winter people bundle up in warm layers and coats to keep warm, while in summer people wear less clothing to stay cooler in the heat. Seasonal shopping now revolves a lot around holiday sales and buying more for less. Stores need to get rid of all of their previous seasonal clothing to make room for the new trends of the upcoming season.[17] The end-of-season sales usually last a few weeks with prices lowering further towards the closing of the sale. During sales items can be discounted from 10% up to as much as 50%, with the biggest reduction sales occurring at the end of the season. Holiday shopping periods are extending their sales further and further with holidays such as Black Friday becoming a month-long event stretching promotions across November . These days shopping doesn't stop once the mall closes, as people have more access to stores and their sales than ever before with the help of the internet and apps.[18] Today many people research their purchases online to find the cheapest and best deal with one third of all shopping searches on Google happen between 10:00 pm and 4:00 am.[19] Shoppers are now spending more time consulting different sources before making a final purchasing decision. Shoppers once used an average of five sources for information before making a purchase, but numbers have risen to as high as 12 sources in 2014.[20]		The pricing technique used by most retailers is cost-plus pricing. This involves adding a markup amount (or percentage) to the retailers' cost. Another common technique is manufacturers suggested list pricing. This simply involves charging the amount suggested by the manufacturer and usually printed on the product by the manufacturer.		In Western countries, retail prices can be referred to as psychological prices or odd prices: a little less than a round number, e.g. $6.95. In Chinese societies, prices are generally either a round number or sometimes some lucky number. This creates price points.		Often, prices are fixed and price discrimination can lead to a bargaining situation often called haggling, a negotiation about the price. Economists see this as determining how the transaction's total economic surplus will be divided between consumers and producers. Neither party has a clear advantage because the threat of no sale exists, in which case the surplus would vanish for both.		When shopping online, it can be more difficult to negotiate price given that you are not directly interacting with a sales person. Some consumers use price comparison websites to find the best price and/or to make a decision about who or where to buy from to save money.		"Window shopping" is a term referring to the browsing of goods by a consumer with no intent to purchase, either as a recreational activity or to plan a later purchase.		Showrooming, the practice of examining merchandise in a traditional brick and mortar retail store without purchasing it, but then shopping online to find a lower price for the same item, has become an increasingly prevalent problem for traditional retailers as a result of online competitors, so much so that some have begun to take measures to combat it.[21]		In countries like Denmark, the Netherlands and Germany the high levels of utility cycling also includes shopping trips e.g. 9% of all shopping trips in Germany are by bicycle.[22]		
Defecation is the final act of digestion, by which organisms eliminate solid, semisolid, or liquid waste material from the digestive tract via the anus.		Humans expel feces with a frequency varying from a few times daily to a few times weekly.[citation needed] Waves of muscular contraction (known as peristalsis) in the walls of the colon move fecal matter through the digestive tract towards the rectum. Undigested food may also be expelled this way, in a process called egestion.		Open defecation, the practice of defecating outside without using a toilet of any kind, is still widespread in some developing countries, including, for example, India, Indonesia, Pakistan, Nigeria, and Ethiopia.[1]						The rectum ampulla (anatomically also: ampulla recti) temporarily stores fecal waste. As the waste fills the rectum and expands the rectal walls, nervous system stretch receptors in the rectal walls stimulate the desire to defecate. This urge to defecate arises from the reflex contraction of rectal muscles, relaxation of the internal anal sphincter, and an initial contraction of the skeletal muscle of the external anal sphincter. If the urge is not acted upon, the material in the rectum is often returned to the colon by reverse peristalsis, where more water is absorbed and the faeces is stored until the next mass peristaltic movement of the transverse and descending colon. If defecation is delayed for a prolonged period the fecal matter may harden, resulting in constipation. If defecation occurs too fast, before excess liquid is absorbed, diarrhea may occur.[2]		When the rectum is full, an increase in intra-rectal pressure forces apart the walls of the anal canal, allowing the fecal matter to enter the canal. The rectum shortens as material is forced into the anal canal and peristaltic waves push the feces out of the rectum. The internal and external anal sphincters along with the puborectalis muscle allow the feces to be passed by muscles pulling the anus up over the exiting feces.[citation needed]		Defecation is normally assisted by taking a deep breath and trying to expel this air against a closed glottis (Valsalva maneuver). This contraction of expiratory chest muscles, diaphragm, abdominal wall muscles, and pelvic diaphragm exerts pressure on the digestive tract. Ventilation at this point temporarily ceases as the lungs push the chest diaphragm down to exert the pressure. Thoracic blood pressure rises and as a reflex response the amount of blood pumped by the heart decreases. Death has been known to occur in cases where defecation causes the blood pressure to rise enough to cause the rupture of an aneurysm or to dislodge blood clots (see thrombosis). Also, in releasing the Valsalva maneuver blood pressure falls; this, coupled with standing up quickly to leave the toilet, can result in a blackout.[citation needed] [3]		During defecation, the external sphincter muscles relax. The anal and urethral sphincter muscles are closely linked. Experiments by Harrison Weed at the Ohio State University Medical Center have shown they can only be contracted together, not individually, and that both show relaxation during urination.[citation needed] This explains why defecation is frequently accompanied by urination.		Defecation may be involuntary or voluntary.  Young children learn voluntary control through the process of toilet training.  Once trained, loss of control, called fecal incontinence, may be caused by physical injury, nerve injury, prior surgeries (such as an episiotomy), constipation, diarrhea, loss of storage capacity in the rectum, intense fright, inflammatory bowel disease, psychological or neurological factors, childbirth, or death.[4]		The anus and buttocks may be cleansed after defecation with toilet paper, similar paper products, or other absorbent material. In many cultures, such as Hindu and Muslim, water is used for anal cleansing after defecation, either in addition to using toilet paper or exclusively. When water is used for anal cleansing after defecation, toilet paper may be used for drying the area afterwards.		The positions and modalities of defecation are culture-dependent. Squat toilets are used by the vast majority of the world, including most of Africa, Asia, and the Middle East.[5] The use of sit-down toilets in the Western world is a relatively recent development, beginning in the 19th century with the advent of indoor plumbing.[6]		Attempting forced expiration of breath against a closed airway (the valsalva maneuver) is sometimes practiced to induce defecation while on a toilet. Cardiac arrest[7] and other cardiovascular complications[8] can in rare cases occur due to attempting to defecate using the valsalva maneuver. Valsalva retinopathy is another pathological syndrome associated with the Valsalva maneuver.[9][10]		Some peoples have culturally significant stories in which defecation plays a role. For example:		
Ingestion is the consumption of a substance by an organism. In animals, it normally is accomplished by taking in the substance through the mouth into the gastrointestinal tract, such as through eating or drinking. In single-celled organisms, ingestion can take place through taking the substance through the cell membrane.		Besides nutritional items, other substances which may be ingested include medication (where ingestion is termed oral administration), recreational drugs, and substances considered inedible such as foreign bodies or excrement. Ingestion is a common route taken by pathogenic organisms and poisons entering the body.		Ingestion can also refer to a mechanism picking up something and making it enter an internal hollow of that mechanism, e.g. "a grille was fitted to prevent the pump from ingesting driftwood".						Some pathogens are transmitted via ingestion, including viruses, bacteria, and parasites. Most commonly, this takes place via the faecal-oral route. An intermediate step is often involved, such as drinking water contaminated by faeces or food prepared by workers who fail to practice adequate hand-washing, and is more common in regions where untreated sewage is common. Diseases transmitted via the fecal-oral route include hepatitis A, polio, and cholera.		Some pathogenic organisms are typically ingested by other routes.		Disk batteries, also called button cells, are often mistakenly ingested, particularly by children and the elderly. They may be mistaken for a medication pill because of their size and shape, or they may be swallowed after being held in the mouth while the battery is being changed. Battery ingestion can cause medical problems including blocked airway, vomiting, irritability, persistent drooling, and rash (due to nickel metal allergy).[4]		Pica is an abnormal appetite for non-nutritive objects or for food items in a form not normally eaten, such as flour. Coprophagia is the consumption of feces, an abnormal ingestive behavior common in some animals.				
Allergies, also known as allergic diseases, are a number of conditions caused by hypersensitivity of the immune system to something in the environment that usually causes little or no problem in most people.[10] These diseases include hay fever, food allergies, atopic dermatitis, allergic asthma, and anaphylaxis.[2] Symptoms may include red eyes, an itchy rash, sneezing, a runny nose, shortness of breath, or swelling.[1] Food intolerances and food poisoning are separate conditions.[5][4]		Common allergens include pollen and certain food.[10] Metals and other substances may also cause problems.[10] Food, insect stings, and medications are common causes of severe reactions.[3] Their development is due to both genetic and environmental factors.[3] The underlying mechanism involves immunoglobulin E antibodies (IgE), part of the body's immune system, binding to an allergen and then to a receptor on mast cells or basophils where it triggers the release of inflammatory chemicals such as histamine.[11] Diagnosis is typically based on a person's medical history.[4] Further testing of the skin or blood may be useful in certain cases.[4] Positive tests, however, may not mean there is a significant allergy to the substance in question.[12]		Early exposure to potential allergens may be protective.[6] Treatments for allergies include avoiding known allergens and the use of medications such as steroids and antihistamines.[7] In severe reactions injectable adrenaline (epinephrine) is recommended.[8] Allergen immunotherapy, which gradually exposes people to larger and larger amounts of allergen, is useful for some types of allergies such as hay fever and reactions to insect bites.[7] Its use in food allergies is unclear.[7]		Allergies are common.[9] In the developed world, about 20% of people are affected by allergic rhinitis,[13] about 6% of people have at least one food allergy,[4][6] and about 20% have atopic dermatitis at some point in time.[14] Depending on the country about 1–18% of people have asthma.[15][16] Anaphylaxis occurs in between 0.05–2% of people.[17] Rates of many allergic diseases appear to be increasing.[8][18] The word "allergy" was first used by Clemens von Pirquet in 1906.[3]						Many allergens such as dust or pollen are airborne particles. In these cases, symptoms arise in areas in contact with air, such as eyes, nose, and lungs. For instance, allergic rhinitis, also known as hay fever, causes irritation of the nose, sneezing, itching, and redness of the eyes.[19] Inhaled allergens can also lead to increased production of mucus in the lungs, shortness of breath, coughing, and wheezing.[20]		Aside from these ambient allergens, allergic reactions can result from foods, insect stings, and reactions to medications like aspirin and antibiotics such as penicillin. Symptoms of food allergy include abdominal pain, bloating, vomiting, diarrhea, itchy skin, and swelling of the skin during hives. Food allergies rarely cause respiratory (asthmatic) reactions, or rhinitis.[21] Insect stings, food, antibiotics, and certain medicines may produce a systemic allergic response that is also called anaphylaxis; multiple organ systems can be affected, including the digestive system, the respiratory system, and the circulatory system.[22][23][24] Depending on the rate of severity, it can cause a skin reactions, bronchoconstriction, swelling, low blood pressure, coma, and death. This type of reaction can be triggered suddenly, or the onset can be delayed. The nature of anaphylaxis is such that the reaction can seem to be subsiding, but may recur throughout a period of time.[24]		Substances that come into contact with the skin, such as latex, are also common causes of allergic reactions, known as contact dermatitis or eczema.[25] Skin allergies frequently cause rashes, or swelling and inflammation within the skin, in what is known as a "wheal and flare" reaction characteristic of hives and angioedema.[26]		With insect stings a large local reaction may occur (an area of skin redness greater than 10 cm in size).[27] It can last one to two days.[27] This reaction may also occur after immunotherapy.[28]		Risk factors for allergy can be placed in two general categories, namely host and environmental factors.[29] Host factors include heredity, sex, race, and age, with heredity being by far the most significant. However, there have been recent increases in the incidence of allergic disorders that cannot be explained by genetic factors alone. Four major environmental candidates are alterations in exposure to infectious diseases during early childhood, environmental pollution, allergen levels, and dietary changes.[30]		A wide variety of foods can cause allergic reactions, but 90% of allergic responses to foods are caused by cow's milk, soy, eggs, wheat, peanuts, tree nuts, fish, and shellfish.[31] Other food allergies, affecting less than 1 person per 10,000 population, may be considered "rare".[32] The use of hydrolysed milk baby formula versus standard milk baby formula does not appear to change the risk.[33]		The most common food allergy in the US population is a sensitivity to crustacea.[32] Although peanut allergies are notorious for their severity, peanut allergies are not the most common food allergy in adults or children. Severe or life-threatening reactions may be triggered by other allergens, and are more common when combined with asthma.[31]		Rates of allergies differ between adults and children. Peanut allergies can sometimes be outgrown by children. Egg allergies affect one to two percent of children but are outgrown by about two-thirds of children by the age of 5.[34] The sensitivity is usually to proteins in the white, rather than the yolk.[35]		Milk-protein allergies are most common in children.[36] Approximately 60% of milk-protein reactions are immunoglobulin E-mediated, with the remaining usually attributable to inflammation of the colon.[37] Some people are unable to tolerate milk from goats or sheep as well as from cows, and many are also unable to tolerate dairy products such as cheese. Roughly 10% of children with a milk allergy will have a reaction to beef. Beef contains a small amount of protein that is present in cow's milk.[38] Lactose intolerance, a common reaction to milk, is not a form of allergy at all, but rather due to the absence of an enzyme in the digestive tract.		Those with tree nut allergies may be allergic to one or to many tree nuts, including pecans, pistachios, pine nuts, and walnuts.[35] Also seeds, including sesame seeds and poppy seeds, contain oils in which protein is present, which may elicit an allergic reaction.[35]		Allergens can be transferred from one food to another through genetic engineering; however genetic modification can also remove allergens. Little research has been done on the natural variation of allergen concentrations in the unmodified crops.[39][40]		Latex can trigger an IgE-mediated cutaneous, respiratory, and systemic reaction. The prevalence of latex allergy in the general population is believed to be less than one percent. In a hospital study, 1 in 800 surgical patients (0.125 percent) reported latex sensitivity, although the sensitivity among healthcare workers is higher, between seven and ten percent. Researchers attribute this higher level to the exposure of healthcare workers to areas with significant airborne latex allergens, such as operating rooms, intensive-care units, and dental suites. These latex-rich environments may sensitize healthcare workers who regularly inhale allergenic proteins.[41]		The most prevalent response to latex is an allergic contact dermatitis, a delayed hypersensitive reaction appearing as dry, crusted lesions. This reaction usually lasts 48–96 hours. Sweating or rubbing the area under the glove aggravates the lesions, possibly leading to ulcerations.[41] Anaphylactic reactions occur most often in sensitive patients who have been exposed to a surgeon's latex gloves during abdominal surgery, but other mucosal exposures, such as dental procedures, can also produce systemic reactions.[41]		Latex and banana sensitivity may cross-react. Furthermore, those with latex allergy may also have sensitivities to avocado, kiwifruit, and chestnut.[42] These people often have perioral itching and local urticaria. Only occasionally have these food-induced allergies induced systemic responses. Researchers suspect that the cross-reactivity of latex with banana, avocado, kiwifruit, and chestnut occurs because latex proteins are structurally homologous with some other plant proteins.[41]		About 10% of people report that they are allergic to penicillin; however, 90% turn out not to be.[43] Serious allergies only occur in about 0.03%.[43]		Another non-food protein reaction, urushiol-induced contact dermatitis, originates after contact with poison ivy, eastern poison oak, western poison oak, or poison sumac. Urushiol, which is not itself a protein, acts as a hapten and chemically reacts with, binds to, and changes the shape of integral membrane proteins on exposed skin cells. The immune system does not recognize the affected cells as normal parts of the body, causing a T-cell-mediated immune response.[44] Of these poisonous plants, sumac is the most virulent.[45] The resulting dermatological response to the reaction between urushiol and membrane proteins includes redness, swelling, papules, vesicles, blisters, and streaking.[46]		Estimates vary on the percentage of the population that will have an immune system response. Approximately 25 percent of the population will have a strong allergic response to urushiol. In general, approximately 80 percent to 90 percent of adults will develop a rash if they are exposed to .0050 milligrams (7.7×10−5 gr) of purified urushiol, but some people are so sensitive that it takes only a molecular trace on the skin to initiate an allergic reaction.[47]		Allergic diseases are strongly familial: identical twins are likely to have the same allergic diseases about 70% of the time; the same allergy occurs about 40% of the time in non-identical twins.[48] Allergic parents are more likely to have allergic children,[49] and those children's allergies are likely to be more severe than those in children of non-allergic parents. Some allergies, however, are not consistent along genealogies; parents who are allergic to peanuts may have children who are allergic to ragweed. It seems that the likelihood of developing allergies is inherited and related to an irregularity in the immune system, but the specific allergen is not.[49]		The risk of allergic sensitization and the development of allergies varies with age, with young children most at risk.[50] Several studies have shown that IgE levels are highest in childhood and fall rapidly between the ages of 10 and 30 years.[50] The peak prevalence of hay fever is highest in children and young adults and the incidence of asthma is highest in children under 10.[51]		Overall, boys have a higher risk of developing allergies than girls,[49] although for some diseases, namely asthma in young adults, females are more likely to be affected.[52] These differences between the sexes tend to decrease in adulthood.[49]		Ethnicity may play a role in some allergies; however, racial factors have been difficult to separate from environmental influences and changes due to migration.[49] It has been suggested that different genetic loci are responsible for asthma, to be specific, in people of European, Hispanic, Asian, and African origins.[53]		Allergic diseases are caused by inappropriate immunological responses to harmless antigens driven by a TH2-mediated immune response. Many bacteria and viruses elicit a TH1-mediated immune response, which down-regulates TH2 responses. The first proposed mechanism of action of the hygiene hypothesis was that insufficient stimulation of the TH1 arm of the immune system leads to an overactive TH2 arm, which in turn leads to allergic disease.[54] In other words, individuals living in too sterile an environment are not exposed to enough pathogens to keep the immune system busy. Since our bodies evolved to deal with a certain level of such pathogens, when they are not exposed to this level, the immune system will attack harmless antigens and thus normally benign microbial objects—like pollen—will trigger an immune response.[55]		The hygiene hypothesis was developed to explain the observation that hay fever and eczema, both allergic diseases, were less common in children from larger families, which were, it is presumed, exposed to more infectious agents through their siblings, than in children from families with only one child. The hygiene hypothesis has been extensively investigated by immunologists and epidemiologists and has become an important theoretical framework for the study of allergic disorders. It is used to explain the increase in allergic diseases that have been seen since industrialization, and the higher incidence of allergic diseases in more developed countries. The hygiene hypothesis has now expanded to include exposure to symbiotic bacteria and parasites as important modulators of immune system development, along with infectious agents.		Epidemiological data support the hygiene hypothesis. Studies have shown that various immunological and autoimmune diseases are much less common in the developing world than the industrialized world and that immigrants to the industrialized world from the developing world increasingly develop immunological disorders in relation to the length of time since arrival in the industrialized world.[56] Longitudinal studies in the third world demonstrate an increase in immunological disorders as a country grows more affluent and, it is presumed, cleaner.[57] The use of antibiotics in the first year of life has been linked to asthma and other allergic diseases.[58] The use of antibacterial cleaning products has also been associated with higher incidence of asthma, as has birth by Caesarean section rather than vaginal birth.[59][60]		Chronic stress can aggravate allergic conditions. This has been attributed to a T helper 2 (TH2)-predominant response driven by suppression of interleukin 12 by both the autonomic nervous system and the hypothalamic–pituitary–adrenal axis. Stress management in highly susceptible individuals may improve symptoms.[61]		International differences have been associated with the number of individuals within a population have allergy. Allergic diseases are more common in industrialized countries than in countries that are more traditional or agricultural, and there is a higher rate of allergic disease in urban populations versus rural populations, although these differences are becoming less defined.[62]		Alterations in exposure to microorganisms is another plausible explanation, at present, for the increase in atopic allergy.[30] Endotoxin exposure reduces release of inflammatory cytokines such as TNF-α, IFNγ, interleukin-10, and interleukin-12 from white blood cells (leukocytes) that circulate in the blood.[63] Certain microbe-sensing proteins, known as Toll-like receptors, found on the surface of cells in the body are also thought to be involved in these processes.[64]		Gutworms and similar parasites are present in untreated drinking water in developing countries, and were present in the water of developed countries until the routine chlorination and purification of drinking water supplies.[65] Recent research has shown that some common parasites, such as intestinal worms (e.g., hookworms), secrete chemicals into the gut wall (and, hence, the bloodstream) that suppress the immune system and prevent the body from attacking the parasite.[66] This gives rise to a new slant on the hygiene hypothesis theory—that co-evolution of humans and parasites has led to an immune system that functions correctly only in the presence of the parasites. Without them, the immune system becomes unbalanced and oversensitive.[67] In particular, research suggests that allergies may coincide with the delayed establishment of gut flora in infants.[68] However, the research to support this theory is conflicting, with some studies performed in China and Ethiopia showing an increase in allergy in people infected with intestinal worms.[62] Clinical trials have been initiated to test the effectiveness of certain worms in treating some allergies.[69] It may be that the term 'parasite' could turn out to be inappropriate, and in fact a hitherto unsuspected symbiosis is at work.[69] For more information on this topic, see Helminthic therapy.		In the early stages of allergy, a type I hypersensitivity reaction against an allergen encountered for the first time and presented by a professional antigen-presenting cell causes a response in a type of immune cell called a TH2 lymphocyte, which belongs to a subset of T cells that produce a cytokine called interleukin-4 (IL-4). These TH2 cells interact with other lymphocytes called B cells, whose role is production of antibodies. Coupled with signals provided by IL-4, this interaction stimulates the B cell to begin production of a large amount of a particular type of antibody known as IgE. Secreted IgE circulates in the blood and binds to an IgE-specific receptor (a kind of Fc receptor called FcεRI) on the surface of other kinds of immune cells called mast cells and basophils, which are both involved in the acute inflammatory response. The IgE-coated cells, at this stage, are sensitized to the allergen.[30]		If later exposure to the same allergen occurs, the allergen can bind to the IgE molecules held on the surface of the mast cells or basophils. Cross-linking of the IgE and Fc receptors occurs when more than one IgE-receptor complex interacts with the same allergenic molecule, and activates the sensitized cell. Activated mast cells and basophils undergo a process called degranulation, during which they release histamine and other inflammatory chemical mediators (cytokines, interleukins, leukotrienes, and prostaglandins) from their granules into the surrounding tissue causing several systemic effects, such as vasodilation, mucous secretion, nerve stimulation, and smooth muscle contraction. This results in rhinorrhea, itchiness, dyspnea, and anaphylaxis. Depending on the individual, allergen, and mode of introduction, the symptoms can be system-wide (classical anaphylaxis), or localized to particular body systems; asthma is localized to the respiratory system and eczema is localized to the dermis.[30]		After the chemical mediators of the acute response subside, late-phase responses can often occur. This is due to the migration of other leukocytes such as neutrophils, lymphocytes, eosinophils and macrophages to the initial site. The reaction is usually seen 2–24 hours after the original reaction.[70] Cytokines from mast cells may play a role in the persistence of long-term effects. Late-phase responses seen in asthma are slightly different from those seen in other allergic responses, although they are still caused by release of mediators from eosinophils and are still dependent on activity of TH2 cells.[71]		Although allergic contact dermatitis is termed an "allergic" reaction (which usually refers to type I hypersensitivity), its pathophysiology actually involves a reaction that more correctly corresponds to a type IV hypersensitivity reaction.[72] In type IV hypersensitivity, there is activation of certain types of T cells (CD8+) that destroy target cells on contact, as well as activated macrophages that produce hydrolytic enzymes.		Effective management of allergic diseases relies on the ability to make an accurate diagnosis.[73] Allergy testing can help confirm or rule out allergies.[74][75] Correct diagnosis, counseling, and avoidance advice based on valid allergy test results reduces the incidence of symptoms and need for medications, and improves quality of life.[74] To assess the presence of allergen-specific IgE antibodies, two different methods can be used: a skin prick test, or an allergy blood test. Both methods are recommended, and they have similar diagnostic value.[75][76]		Skin prick tests and blood tests are equally cost-effective, and health economic evidence shows that both tests were cost-effective compared with no test.[74] Also, early and more accurate diagnoses save cost due to reduced consultations, referrals to secondary care, misdiagnosis, and emergency admissions.[77]		Allergy undergoes dynamic changes over time. Regular allergy testing of relevant allergens provides information on if and how patient management can be changed, in order to improve health and quality of life. Annual testing is often the practice for determining whether allergy to milk, egg, soy, and wheat have been outgrown, and the testing interval is extended to 2–3 years for allergy to peanut, tree nuts, fish, and crustacean shellfish.[75] Results of follow-up testing can guide decision-making regarding whether and when it is safe to introduce or re-introduce allergenic food into the diet.[78]		Skin testing is also known as "puncture testing" and "prick testing" due to the series of tiny punctures or pricks made into the patient's skin. Small amounts of suspected allergens and/or their extracts (e.g., pollen, grass, mite proteins, peanut extract) are introduced to sites on the skin marked with pen or dye (the ink/dye should be carefully selected, lest it cause an allergic response itself). A small plastic or metal device is used to puncture or prick the skin. Sometimes, the allergens are injected "intradermally" into the patient's skin, with a needle and syringe. Common areas for testing include the inside forearm and the back.		If the patient is allergic to the substance, then a visible inflammatory reaction will usually occur within 30 minutes. This response will range from slight reddening of the skin to a full-blown hive (called "wheal and flare") in more sensitive patients similar to a mosquito bite. Interpretation of the results of the skin prick test is normally done by allergists on a scale of severity, with +/− meaning borderline reactivity, and 4+ being a large reaction. Increasingly, allergists are measuring and recording the diameter of the wheal and flare reaction. Interpretation by well-trained allergists is often guided by relevant literature.[79] Some patients may believe they have determined their own allergic sensitivity from observation, but a skin test has been shown to be much better than patient observation to detect allergy.[80]		If a serious life-threatening anaphylactic reaction has brought a patient in for evaluation, some allergists will prefer an initial blood test prior to performing the skin prick test. Skin tests may not be an option if the patient has widespread skin disease, or has taken antihistamines in the last several days.		Patch testing is a method used to determine if a specific substance causes allergic inflammation of the skin. It tests for delayed reactions. It is used to help ascertain the cause of skin contact allergy, or contact dermatitis. Adhesive patches, usually treated with a number of common allergic chemicals or skin sensitizers, are applied to the back. The skin is then examined for possible local reactions at least twice, usually at 48 hours after application of the patch, and again two or three days later.		An allergy blood test is quick and simple, and can be ordered by a licensed health care provider (e.g., an allergy specialist), GP, or PED. Unlike skin-prick testing, a blood test can be performed irrespective of age, skin condition, medication, symptom, disease activity, and pregnancy. Adults and children of any age can take an allergy blood test. For babies and very young children, a single needle stick for allergy blood testing is often more gentle than several skin tests.		An allergy blood test is available through most laboratories. A sample of the patient's blood is sent to a laboratory for analysis, and the results are sent back a few days later. Multiple allergens can be detected with a single blood sample. Allergy blood tests are very safe, since the person is not exposed to any allergens during the testing procedure.		The test measures the concentration of specific IgE antibodies in the blood. Quantitative IgE test results increase the possibility of ranking how different substances may affect symptoms. A rule of thumb is that the higher the IgE antibody value, the greater the likelihood of symptoms. Allergens found at low levels that today do not result in symptoms can nevertheless help predict future symptom development. The quantitative allergy blood result can help determine what a patient is allergic to, help predict and follow the disease development, estimate the risk of a severe reaction, and explain cross-reactivity.[81][82]		A low total IgE level is not adequate to rule out sensitization to commonly inhaled allergens.[83] Statistical methods, such as ROC curves, predictive value calculations, and likelihood ratios have been used to examine the relationship of various testing methods to each other. These methods have shown that patients with a high total IgE have a high probability of allergic sensitization, but further investigation with allergy tests for specific IgE antibodies for a carefully chosen of allergens is often warranted.		Laboratory methods to measure specific IgE antibodies for allergy testing include enzyme-linked immunosorbent assay (ELISA, or EIA),[84] radioallergosorbent test (RAST)[84] and fluorescent enzyme immunoassay (FEIA).[85]		Challenge testing: Challenge testing is when small amounts of a suspected allergen are introduced to the body orally, through inhalation, or via other routes. Except for testing food and medication allergies, challenges are rarely performed. When this type of testing is chosen, it must be closely supervised by an allergist.		Elimination/Challenge tests: This testing method is used most often with foods or medicines. A patient with a suspected allergen is instructed to modify his diet to totally avoid that allergen for a set time. If the patient experiences significant improvement, he may then be "challenged" by reintroducing the allergen, to see if symptoms are reproduced.		Unreliable tests: There are other types of allergy testing methods that are unreliable, including applied kinesiology (allergy testing through muscle relaxation), cytotoxicity testing, urine autoinjection, skin titration (Rinkel method), and provocative and neutralization (subcutaneous) testing or sublingual provocation.[86]		Before a diagnosis of allergic disease can be confirmed, other possible causes of the presenting symptoms should be considered.[87] Vasomotor rhinitis, for example, is one of many maladies that shares symptoms with allergic rhinitis, underscoring the need for professional differential diagnosis.[88] Once a diagnosis of asthma, rhinitis, anaphylaxis, or other allergic disease has been made, there are several methods for discovering the causative agent of that allergy.		Some foods during pregnancy has been linked to allergies in the child. Vegetable oil, nuts and fast food may increase the risk while fruits, vegetables and fish may decrease it.[89] Another review found no effect of eating fish during pregnancy on allergy risk.[90]		Probiotic supplements taken during pregnancy or infancy may help to prevent atopic dermatitis.[91]		Management of allergies typically involves avoiding what triggers the allergy and medications to improve the symptoms.[7] Allergen immunotherapy may be useful for some types of allergies.[7]		Several medications may be used to block the action of allergic mediators, or to prevent activation of cells and degranulation processes. These include antihistamines, glucocorticoids, epinephrine (adrenaline), mast cell stabilizers, and antileukotriene agents are common treatments of allergic diseases.[92] Anti-cholinergics, decongestants, and other compounds thought to impair eosinophil chemotaxis, are also commonly used. Though rare, the severity of anaphylaxis often requires epinephrine injection, and where medical care is unavailable, a device known as an epinephrine autoinjector may be used.[24]		Allergen immunotherapy is useful for environmental allergies, allergies to insect bites, and asthma.[7][93] Its benefit for food allergies is unclear and thus not recommended.[7] Immunotherapy involves exposing people to larger and larger amounts of allergen in an effort to change the immune system's response.[7]		Meta-analyses have found that injections of allergens under the skin is effective in the treatment in allergic rhinitis in children[94][95] and in asthma.[93] The benefits may last for years after treatment is stopped.[96] It is generally safe and effective for allergic rhinitis and conjunctivitis, allergic forms of asthma, and stinging insects.[97]		The evidence also supports the use of sublingual immunotherapy for rhinitis and asthma but it is less strong.[96] For seasonal allergies the benefit is small.[98] In this form the allergen is given under the tongue and people often prefer it to injections.[96] Immunotherapy is not recommended as a stand-alone treatment for asthma.[96]		An experimental treatment, enzyme potentiated desensitization (EPD), has been tried for decades but is not generally accepted as effective.[99] EPD uses dilutions of allergen and an enzyme, beta-glucuronidase, to which T-regulatory lymphocytes are supposed to respond by favoring desensitization, or down-regulation, rather than sensitization. EPD has also been tried for the treatment of autoimmune diseases but evidence does not show effectiveness.[99]		A review found no effectiveness of homeopathic treatments and no difference compared with placebo. The authors concluded that, based on rigorous clinical trials of all types of homeopathy for childhood and adolescence ailments, there is no convincing evidence that supports the use of homeopathic treatments.[100]		According to the NCCIH, the evidence is relatively strong that saline nasal irrigation and butterbur are effective, when compared to other alternative medicine treatments, for which the scientific evidence is weak, negative, or nonexistent, such as honey, acupuncture, omega 3's, probiotics, astragalus, capsaicin, grape seed extract, Pycnogenol, quercetin, spirulina, stinging nettle, tinospora or guduchi. [101][102]		The allergic diseases—hay fever and asthma—have increased in the Western world over the past 2–3 decades.[103] Increases in allergic asthma and other atopic disorders in industrialized nations, it is estimated, began in the 1960s and 1970s, with further increases occurring during the 1980s and 1990s,[104] although some suggest that a steady rise in sensitization has been occurring since the 1920s.[105] The number of new cases per year of atopy in developing countries has, in general, remained much lower.[104]		Although genetic factors govern susceptibility to atopic disease, increases in atopy have occurred within too short a time frame to be explained by a genetic change in the population, thus pointing to environmental or lifestyle changes.[104] Several hypotheses have been identified to explain this increased rate; increased exposure to perennial allergens due to housing changes and increasing time spent indoors, and changes in cleanliness or hygiene that have resulted in the decreased activation of a common immune control mechanism, coupled with dietary changes, obesity and decline in physical exercise.[103] The hygiene hypothesis maintains[116] that high living standards and hygienic conditions exposes children to fewer infections. It is thought that reduced bacterial and viral infections early in life direct the maturing immune system away from TH1 type responses, leading to unrestrained TH2 responses that allow for an increase in allergy.[67][117]		Changes in rates and types of infection alone however, have been unable to explain the observed increase in allergic disease, and recent evidence has focused attention on the importance of the gastrointestinal microbial environment. Evidence has shown that exposure to food and fecal-oral pathogens, such as hepatitis A, Toxoplasma gondii, and Helicobacter pylori (which also tend to be more prevalent in developing countries), can reduce the overall risk of atopy by more than 60%,[118] and an increased rate of parasitic infections has been associated with a decreased prevalence of asthma.[119] It is speculated that these infections exert their effect by critically altering TH1/TH2 regulation.[120] Important elements of newer hygiene hypotheses also include exposure to endotoxins, exposure to pets and growing up on a farm.[120]		The concept of "allergy" was originally introduced in 1906 by the Viennese pediatrician Clemens von Pirquet, after he noted that some of his patients were hypersensitive to normally innocuous entities such as dust, pollen, or certain foods.[121] Pirquet called this phenomenon "allergy" from the Ancient Greek words ἄλλος allos meaning "other" and ἔργον ergon meaning "work".[122]		All forms of hypersensitivity used to be classified as allergies, and all were thought to be caused by an improper activation of the immune system. Later, it became clear that several different disease mechanisms were implicated, with the common link to a disordered activation of the immune system. In 1963, a new classification scheme was designed by Philip Gell and Robin Coombs that described four types of hypersensitivity reactions, known as Type I to Type IV hypersensitivity.[123] With this new classification, the word "allergy" was restricted to type I hypersensitivities (also called immediate hypersensitivity), which are characterized as rapidly developing reactions.		A major breakthrough in understanding the mechanisms of allergy was the discovery of the antibody class labeled immunoglobulin E (IgE). IgE was simultaneously discovered in 1966–67 by two independent groups:[124] Ishizaka's team at the Children's Asthma Research Institute and Hospital in Denver, Colorado,[125] and by Gunnar Johansson and Hans Bennich in Uppsala, Sweden.[126] Their joint paper was published in April 1969.[127]		Radiometric assays include the radioallergosorbent test (RAST test) method, which uses IgE-binding (anti-IgE) antibodies labeled with radioactive isotopes for quantifying the levels of IgE antibody in the blood.[128] Other newer methods use colorimetric or fluorescence-labeled technology in the place of radioactive isotopes.[citation needed]		The RAST methodology was invented and marketed in 1974 by Pharmacia Diagnostics AB, Uppsala, Sweden, and the acronym RAST is actually a brand name. In 1989, Pharmacia Diagnostics AB replaced it with a superior test named the ImmunoCAP Specific IgE blood test, which uses the newer fluorescence-labeled technology.[citation needed]		American College of Allergy Asthma and Immunology (ACAAI) and the American Academy of Allergy Asthma and Immunology (AAAAI) issued the Joint Task Force Report "Pearls and pitfalls of allergy diagnostic testing" in 2008, and is firm in its statement that the term RAST is now obsolete:		The term RAST became a colloquialism for all varieties of (in vitro allergy) tests. This is unfortunate because it is well recognized that there are well-performing tests and some that do not perform so well, yet they are all called RASTs, making it difficult to distinguish which is which. For these reasons, it is now recommended that use of RAST as a generic descriptor of these tests be abandoned.[129]		The new version, the ImmunoCAP Specific IgE blood test, is the only specific IgE assay to receive FDA approval to quantitatively report to its detection limit of 0.1kU/l.[citation needed]		An allergist is a physician specially trained to manage and treat allergies, asthma and the other allergic diseases. In the United States physicians holding certification by the American Board of Allergy and Immunology (ABAI) have successfully completed an accredited educational program and evaluation process, including a proctored examination to demonstrate knowledge, skills, and experience in patient care in allergy and immunology.[130] Becoming an allergist/immunologist requires completion of at least nine years of training. After completing medical school and graduating with a medical degree, a physician will undergo three years of training in internal medicine (to become an internist) or pediatrics (to become a pediatrician). Once physicians have finished training in one of these specialties, they must pass the exam of either the American Board of Pediatrics (ABP), the American Osteopathic Board of Pediatrics (AOBP), the American Board of Internal Medicine (ABIM), or the American Osteopathic Board of Internal Medicine (AOBIM). Internists or pediatricians wishing to focus on the sub-specialty of allergy-immunology then complete at least an additional two years of study, called a fellowship, in an allergy/immunology training program. Allergist/immunologists listed as ABAI-certified have successfully passed the certifying examination of the ABAI following their fellowship.[131]		In the United Kingdom, allergy is a subspecialty of general medicine or pediatrics. After obtaining postgraduate exams (MRCP or MRCPCH), a doctor works for several years as a specialist registrar before qualifying for the General Medical Council specialist register. Allergy services may also be delivered by immunologists. A 2003 Royal College of Physicians report presented a case for improvement of what were felt to be inadequate allergy services in the UK.[132] In 2006, the House of Lords convened a subcommittee. It concluded likewise in 2007 that allergy services were insufficient to deal with what the Lords referred to as an "allergy epidemic" and its social cost; it made several recommendations.[133]		Low-allergen foods are being developed, as are improvements in skin prick test predictions; evaluation of the atopy patch test; in wasp sting outcomes predictions and a rapidly disintegrating epinephrine tablet, and anti-IL-5 for eosinophilic diseases.[134]		Aerobiology is the study of biological particles passively dispersed through the air. One aim is the prevention of allergies due to pollen.[135][136]						
An emergency is a serious, unexpected, often dangerous situation that requires immediate action. The emergency procedure is a plan of actions to be conducted in a certain order or manner, in response to an emergency event.						Organizations are frequently required to have written emergency procedures in place to comply with statutory requirements;[1] demands from their insurers, their regulatory agency, shareholders, stakeholders and unions; to protect staff, the public, the environment, the business, their property and their reputation.		Before preparing a procedure, it may be appropriate to carry out a risk assessment, estimating how likely it is for an emergency event to occur and if it does, how serious or damaging the consequences would be. The emergency procedure should provide an appropriate and proportionate response to this situation.		A Risk assessment is usually in the style of a table, this table rates a risk on its likelihood and severity using the numbers 1-10. these numbers are usually multiplied then to give the final number quoted for a given risk.		An emergency procedure identifies the responsibilities, actions and resources necessary to deal with an emergency. Once drafted, a procedure may require a consultative period with those who could be involved or affected by the emergency, and a programme set out for testing, training and periodic review.		When an emergency procedure is revised and reissued, previous versions must be withdrawn from point of use to avoid confusion. For the same reason, a revision numbering system and a schedule of amendments are frequently used with procedures to reduce the potential for errors and misunderstandings.[2]		The document itself may be just a few lines, perhaps using bullet points, flow charts or it may be a detailed set of instructions and diagrams, dependant on the complexity of the situation and the capabilities of those responsible for implementing the procedure during the emergency.[3]		Business continuity planning may also feed off of the emergency procedures, enabling an organization to identify points of vulnerability and minimise the risk to the business by preparing backup plans and improving resilience. The act of producing the procedures may also highlight failings in current arrangements that if corrected, could reduce the risk levels.		Even with a well documented and well practised procedure using trained staff, there is still the potential for events to spiral out of control, often due to unpredicted scenarios or a coincidence of events. There are many well documented examples of this such as: Three Mile Island accident, the Chernobyl disaster and the Deepwater Horizon drilling platform explosion in April 2010. In a press release by BP on the 8 September 2010, BP’s outgoing chief executive Tony Hayward said of this: “The investigation report provides critical new information on the causes of this terrible accident. It is evident that a series of complex events, rather than a single mistake or failure, led to the tragedy”.		It is common practise with emergency procedures to have review processes where the lessons learnt from previous emergencies, changing circumstances, changes in personnel, contact details, etc. can be incorporated into the latest version of the documentation.		Some typical emergency procedure are:		Other potential emergencies that may affect the business activity of an organisation are:		Governments often have web sites giving advice on emergencies at both a personal and national level. UK Examples: [1] UK Government advice on dealing with emergencies [2] UK Government advice for fire safety in the home - Emergency Evacuation Plan		
Tiffin is an Indian English word for a light meal generally in the midday (luncheon) in most regions of the peninsular India.[1] When used in place of the word "lunch", it does not necessarily mean a light meal.[2] In Telugu usage tiffin is mostly synonymous with light breakfast in the morning.[3]						In the British Raj, where the British custom of afternoon tea was supplanted by the local Indian practice of taking a light meal at that hour, it came to be called tiffin.[4] It is derived from English colloquial or slang tiffing meaning to take a little drink, and had by 1867 become naturalised among Anglo-Indians in the north of British India to mean luncheon.[5]		In South India and in Nepal, tiffin is generally a snack between meals: dosas, idlis, vadas etc.[6] In other parts of India, such as Mumbai, the word mostly refers to a packed lunch of some sort.[7] In Mumbai, it is often forwarded to them by dabbawalas, sometimes known as tiffin wallahs, who use a complex system to get thousands of tiffin-boxes to their destinations. In Mumbai, a school going child's lunch box is fondly called a Tiffin box.[8]		Tiffin often consists of rice, dal, curry, vegetables, chapatis or "spicy meats".[9] In addition, the lunch boxes are themselves called tiffin carriers, tiffin-boxes or just tiffins.		
A tasting menu is a collection of small portions of several dishes served by a restaurant as a single meal.[1] The French name for a tasting menu is menu dégustation.[1] Some restaurants and chefs specialize in tasting menus, while in other cases, it is a special or a menu option. Tasting menus may be offered to provide a sample of a type of cuisine, or house specialties,[1] or to take advantage of fresh seasonal ingredients.		Coming to the mainstream in the 1990s, tasting menus evolved into elaborate showcases highlighting the culinary artistry of the chef. The trend traces back centuries, but some trace the latest evolution to the mid-1990s and two highly lauded restaurants, Chef Ferran Adrià's El Bulli in Spain, and Chef Thomas Keller's French Laundry, in Napa Valley, north of San Francisco in the U.S., that offered tasting menus of 40 courses or more.[2] Tasting menus have since become increasingly popular, to the point where, in 2013, New York Times food critic Pete Wells noted, "Across the country, expensive tasting-menu-only restaurants are spreading like an epidemic."[3]		
Take-out or takeout (in North American (U.S and Canada) and Philippine English); also carry-out (in some dialects in the U.S. and Scotland);[1] take-away (in the United Kingdom other than Scotland, Australia, New Zealand, South Africa, Hong Kong, and Ireland),[1] parcel (in Indian English and Pakistani English),[2] refers to prepared meals or other food items, purchased at a restaurant, that the purchaser intends to eat elsewhere. A concept found in many ancient cultures, take-out food is now common worldwide, with a number of different cuisines and dishes on offer.						The concept of prepared meals to be eaten elsewhere dates back to antiquity. Market and roadside stalls selling food were common in both Ancient Greece and Ancient Rome.[3] In Pompeii, archaeologists have found a number of thermopolia. These were service counters, opening onto the street, which provided food to be taken away and eaten elsewhere. There is a distinct lack of formal dining and kitchen area in Pompeian homes, which may suggest that eating, or at least cooking, at home was unusual. Over 200 thermopolia have been found in the ruins of Pompeii.[4]		In the cities of medieval Europe there were a number of street vendors selling take-out food. In medieval London, street vendors sold hot meat pies, geese, sheep's feet and French wine, while in Paris roasted meats, squab, tarts and flans, cheeses and eggs were available. A large strata of society would have purchased food from these vendors, but they were especially popular amongst the urban poor, who would have lacked kitchen facilities in which to prepare their own food.[5] However, these vendors often had a bad reputation, often being in trouble with civic authorities reprimanding them for selling infected meat or reheated food. The cooks of Norwich often defended themselves in court against selling such things as "pokky pies" and "stynkyng mackerelles".[6] In 10th and 11th century China, citizens of cities like Kaifeng and Hangzhou were able to buy pastries such as yuebing and congyoubing to take away. By the early 13th century, the two most successful such shops in Kaifeng had "upwards of fifty ovens".[7] A traveling Florentine reported in the late 14th century that in Cairo, people carried picnic cloths made of raw hide to spread on the streets and eat their meals of lamb kebabs, rice and fritters that they had purchased from street vendors.[8] In Renaissance Turkey, many crossroads saw vendors selling "fragrant bites of hot meat", including chicken and lamb that had been spit roasted.[9]		Aztec marketplaces had vendors that sold beverages such as atolli ("a gruel made from maize dough"), almost 50 types of tamales (with ingredients that ranged from the meat of turkey, rabbit, gopher, frog, and fish to fruits, eggs, and maize flowers),[10] as well as insects and stews.[11] After Spanish colonization of Peru and importation of European food stocks like wheat, sugarcane and livestock, most commoners continued primarily to eat their traditional diets, but did add grilled beef hearts sold by street vendors.[12] Some of Lima's 19th century street vendors such as "Erasmo, the 'negro' sango vendor" and Na Aguedita are still remembered today.[13]		During the American colonial period, street vendors sold "pepper pot soup" (tripe) "oysters, roasted corn ears, fruit and sweets," with oysters being a low-priced commodity until the 1910s when overfishing caused prices to rise.[14] In 1707, after previous restrictions that had limited their operating hours, street food vendors had been banned in New York City.[15] Many women of African descent made their living selling street foods in America in the 18th and 19th centuries; with products ranging from fruit, cakes and nuts in Savannah, Georgia, to coffee, biscuits, pralines and other sweets in New Orleans.[16] In the 19th century, street food vendors in Transylvania sold gingerbread-nuts, cream mixed with corn, and bacon and other meat fried on tops of ceramic vessels with hot coals inside.[17]		The Industrial Revolution saw an increase in the availability of take-out food. By the early 20th Century, fish and chips was considered an "established institution" in Britain. The hamburger was introduced to America around this time. The diets of industrial workers were often poor, and these meals provided an "important component" to their nutrition.[18] In India, local businesses and cooperatives, had begun to supply workers in the city of Mumbai with tiffin boxes by the end of the 19th century.[19]		Take-out food can be purchased from restaurants that also provide sit-down table service or from establishments specialising in food to be taken away.[20] Providing a take-out service often saves business operators having to spend money on things like cutlery, crockery and wages for servers and hosts; it also means that a large number of customers can be served in a relatively short amount of time, compared to a traditional dine-in restaurant.[21]		Although once popular in Europe and America,[5] street food has declined in popularity. In part, this can be attributed to a combination of the proliferation of specialized takeaway restaurants and legislation relating to health and safety.[5] Vendors selling street food are still common in parts of Asia, Africa and the Middle East,[22] with the annual turnover of street food vendors in Bangladesh and Thailand being described as particularly important to the local economy.[23]		Many restaurants and take-out establishments have benefit from the invention of the car. Drive-throughs (also drive-thru[24]) allowed customers to order, purchase and receive their products without leaving their cars. The idea was pioneered in 1931 in a California fast food restaurant, Pig Stand Number 21. By 1988, 51% of McDonald's turnover was being generated by drive-throughs, with 31% of all US take-out turnover being generated by them by 1990.[25] Some take-out businesses offer food for delivery, which usually involves contacting a local business by telephone or online. Online ordering is available in some countries such as Australia, Canada, India, Brazil, Japan, the European Union and the United States, where some pizza chains offer online menu and ordering.[26] The industry has kept pace with technological developments since the 1980s beginning with the rise of the personal computer. Specialized computer software for the pizza delivery business helps determine the most efficient routes for carriers, track exact order and delivery times, manage calls and orders with PoS software, and other functions. Since 2008 GPS tracking technology has been used for real-time monitoring of delivery vehicles by customers over the Internet.[27]		Some businesses such as Pizza Pizza in Ontario, Canada will incorporate a guarantee to deliver within a predetermined period of time, or late deliveries will be free of charge.[28] For example, Domino's Pizza had a commercial campaign in the 1980s and early 1990s which promised "30 minutes or it's free". This was discontinued in the United States in 1993 due to the number of lawsuits arising from accidents caused by hurried delivery drivers.[29]		Take-out food is packaged in paper, paperboard, corrugated fiberboard, plastic, or foam food containers. One common container is the oyster pail, a folded, waxed or plastic coated, paperboard container. The oyster pail was quickly adopted, especially in the West, for Chinese food, "Chinese takeout".[30]		Corrugated fiberboard and foam containers are to some extent self-insulating, and could be used for a wide variety of foods including cooked rice, moist dishes. Thermal bags and other insulated shipping containers have increased ability to control temperatures during transit.		Aluminium containers are also popular for take-out packaging due to their low cost. These containers can often be customized with by being coated with unique designs, to develop or further a brand identity.[31] Expanded polystyrene is often used for hot drinks containers and food trays because it is lightweight and heat-absorbing.[32]		A bacon cheeseburger with fries served in an aluminum tray.		Fish and chips served in a polystyrene clamshell tray.		Pizza served in a cardboard box.		Boiled rice served in an oyster pail.		Leaf-wrapped rice dish (nasi kuning)		Paper-wrapped food		Take-out food in Thailand is often packaged in plastic bags		Packaging of fast food and take-out food is necessary for the customer but involves a significant amount of material that ends up in landfills, recycling, composting, or litter.[33] Fast-food foam food containers were the target of environmentalists in the U.S. and were largely replaced with paper wrappers among large restaurant chains.[34]		In 2002, Taiwan began taking action to reduce the use of disposable tableware at institutions and businesses, and to reduce the use of plastic bags. Yearly, the nation of 17.7 million people was producing 59,000 tons of disposable tableware waste and 105,000 tons of waste plastic bags, and increasing measures have been taken in the years since then to reduce the amount of waste.[35] In 2013 Taiwan's Environmental Protection Administration (EPA) banned outright the use of disposable tableware in the nation's 968 schools, government agencies and hospitals. The ban is expected to eliminate 2,600 metric tons of waste yearly.[36]		In Germany, Austria, and Switzerland, laws banning use of disposable food and drink containers at large scale events have been enacted. Such a ban has been in place in Munich, Germany since 1991, applying to all city facilities and events. This includes events of all sizes, including very large ones (Christmas market, Auer-Dult Faire, Oktoberfest and Munich City Marathon). For small events of a few hundred people, the city has arranged for a corporation to offer rental of crockery and dishwasher equipment. In part through this regulation, Munich reduced the waste generated by Oktoberfest, which attracts tens of thousands of people, from 11,000 metric tons in 1990 to 550 tons in 1999.[37]		China produces about 57 billion pairs of single-use chopsticks yearly, of which half are exported. About 45 percent are made from trees – about 3.8 million of them – mainly cotton wood, birch, and spruce, the remainder being made from bamboo. Japan uses about 24 billion pairs of these disposables per year, and globally the use is about 80 billion pairs are thrown away by an estimated 1.4 billion people. Reusable chopsticks in restaurants have a lifespan of 130 meals. In Japan, with disposable ones costing about 2 cents and reusable ones costing typically $1.17, the reusables better the $2.60 breakeven cost. Campaigns in several countries to reduce this waste are beginning to have some effect.[38][39]		
A tableround is a traditional academic feast known at universities in most Middle and Eastern European countries. At a tableround, tables usually are placed in the form of a U or a W, the participants drink beer and sing commercium songs. A more formal form of the tableround is the commercium. Tableround probably shares the same roots with Cantus and Sitsit.				
Bulimia nervosa, also known as simply bulimia, is an eating disorder characterized by binge eating followed by purging. Binge eating refers to eating a large amount of food in a short amount of time. Purging refers to the attempts to get rid of the food consumed. This may be done by vomiting or taking laxatives.[2] Other efforts to lose weight may include the use of diuretics, stimulants, water fasting, or excessive exercise.[2][4] Most people with bulimia are at a normal weight.[1] The forcing of vomiting may result in thickened skin on the knuckles and breakdown of the teeth. Bulimia is frequently associated with other mental disorders such as depression, anxiety, and problems with drugs or alcohol.[2] There is also a higher risk of suicide and self-harm.[3]		Bulimia is more common among those who have a close relative with the condition.[2] The percentage risk that is estimated to be due to genetics is between 30% and 80%.[4] Other risk factors for the disease include psychological stress, cultural pressure to attain a certain body type, poor self-esteem, and obesity.[2][4] Living in a culture that promotes dieting and having parents that worry about weight are also risks.[4] Diagnosis is based on a person's medical history,[5] however this is difficult as people are usually secretive about their binge eating and purging habits.[4] Furthermore, the diagnosis of anorexia nervosa takes precedence over that of bulimia.[4] Other similar disorders include binge eating disorder, Kleine-Levin syndrome, and borderline personality disorder.[5]		Cognitive behavioral therapy is the primary treatment for bulimia.[2][6] Antidepressants of the selective serotonin reuptake inhibitors (SSRI) or tricyclic antidepressant class may have a modest benefit.[4][7] While outcomes with bulimia are typically better than in those of anorexia, the risk of death among those affected is higher than that of the general population.[3] At 10 years after receiving treatment about 50% of people are fully recovered.[4]		Globally, bulimia was estimated to affect 3.6 million people in 2015.[8] About 1% of young women have bulimia at a given point in time and about 2% to 3% of women have the condition at some point in their lives.[3] The condition is less common in the developing world.[4] Bulimia is about nine times more likely to occur in women than men. Among women, rates are highest in young adults.[5] Bulimia was named and first described by the British psychiatrist Gerald Russell in 1979.[9][10]						Bulimia typically involves rapid and out-of-control eating, which may stop when the bulimic is interrupted by another person or the stomach hurts from over-extension, followed by self-induced vomiting or other forms of purging. This cycle may be repeated several times a week or, in more serious cases, several times a day[12] and may directly cause:		These are some of the many signs that may indicate whether someone has bulimia nervosa:[19]		As with many psychiatric illnesses, delusions can occur, in conjunction with other signs and symptoms, leaving the person with a false belief that is not ordinarily accepted by others.[21]		People with bulimia nervosa may also exercise to a point that excludes other activities.[21]		With regards to interoception, people with bulimia report reduced sensitivity to many kinds of internal and external sensations. For example, some show increased thresholds to heat pain compared and report the same level of satiety after consuming more calories than do healthy subjects.[22]		Bulimics are much more likely than non-bulimics to have an affective disorder, such as depression or general anxiety disorder: A 1985 Columbia University study on female bulimics at New York State Psychiatric Institute found 70% had suffered depression some time in their lives (as opposed to 25.8% for adult females in a control sample from the general population), rising to 88% for all affective disorders combined.[23] Another study by the Royal Children's Hospital in Melbourne on a cohort of 2,000 adolescents similarly found that those meeting at least two of the DSM-IV criteria for bulimia nervosa or anorexia nervosa had a sixfold increase in risk of anxiety and a doubled risk for substance dependency.[24] Some sufferers of anorexia nervosa exhibit episodes of bulimic tendencies through purging (either through self-induced vomiting or laxatives) as a way to quickly remove food in their system.[25] Bulimia also has negative effects on the sufferer's dental health due to the acid passed through the mouth from frequent vomiting causing acid erosion, mainly on the posterior dental surface.		The onset of bulimia nervosa is often during adolescence, between 13 and 20 years of age, and many cases have previously suffered from obesity, with many sufferers relapsing in adulthood into episodic bingeing and purging even after initially successful treatment and remission.[26] A lifetime prevalence of 0.5 percent and 0.9 percent for adult and adolescent sufferers, respectively, is estimated among the United States population.[27] Bulimia nervosa may affect up to 1% of young women and, after 10 years of diagnosis, half will recover fully, a third will recover partially, and 10–20% will still have symptoms.[4]		Adolescents with bulimia nervosa are more likely to have self-imposed perfectionism and compulsivity issues in eating compared to their peers. This means that the high expectations and unrealistic goals that these individuals set for themselves are internally motivated rather than by social views or expectations.[28]		Bulimia nervosa can be difficult to detect, compared to anorexia nervosa, because bulimics tend to be of average or slightly above or below average weight. Many bulimics may also engage in significantly disordered eating and exercise patterns without meeting the full diagnostic criteria for bulimia nervosa.[29] Recently, the Diagnostic and Statistical Manual of Mental Disorders was revised, which resulted in the loosening of criteria regarding the diagnoses of bulimia nervosa and anorexia nervosa.[30] The diagnostic criteria utilized by the DSM-5 includes repetitive episodes of binge eating (a discrete episode of overeating during which the individual feels out of control of consumption) compensated for by excessive or inappropriate measures taken to avoid gaining weight.[31] The diagnosis also requires the episodes of compensatory behaviors and binge eating to happen a minimum of once a week for a consistent time period of 3 months.[32] The diagnosis is made only when the behavior is not a part of the symptom complex of anorexia nervosa and when the behavior reflects an overemphasis on physical mass or appearance. Purging often is a common characteristic of a more severe case of bulimia nervosa.[33]		As with anorexia nervosa, there is evidence of genetic predispositions contributing to the onset of this eating disorder.[34] Abnormal levels of many hormones, notably serotonin, have been shown to be responsible for some disordered eating behaviors. Brain-derived neurotrophic factor (BDNF) is under investigation as a possible mechanism.[35][36]		There is evidence that sex hormones may influence appetite and eating in women, and the onset of bulimia nervosa. Studies have shown that women with hyperandrogenism and polycystic ovary syndrome have a dysregulation of appetite, along with carbohydrates and fats. This dysregulation of appetite is also seen in women with bulimia nervosa. In addition, gene knockout studies in mice have shown that mice that have the gene encoding estrogen receptors have decreased fertility due to ovarian dysfunction and dysregulation of androgen receptors. In humans, there is evidence that there is an association between polymorphisms in the ERβ (estrogen receptor β) and bulimia, suggesting there is a correlation between sex hormones and bulimia nervosa.[37]		Bulimia has been compared to drug addiction, though the empirical support for this characterization is limited.[38] However, people with bulimia nervosa may share dopamine D2 receptor-related vulnerabilities with those with substance abuse disorders.[39]		Dieting, a common behaviour in bulimics, is associated with lower plasma tryptophan levels.[40] Decreased tryptophan levels in the brain, and thus the synthesis of serotonin, increases bulimic urges in currently and formerly bulimic individuals within hours.[41][42]		Media portrayals of an 'ideal' body shape are widely considered to be a contributing factor to bulimia.[21] In a 1991 study by Weltzin, Hsu, Pollicle, and Kaye, it was stated that 19% of bulimics undereat, 37% of bulimics eat an amount of food that is normal for an average human being, and 44% of bulimics overeat.[43] A survey of 15- to 18-year-old high school girls in Nadroga, Fiji, found the self-reported incidence of purging rose from 0% in 1995 (a few weeks after the introduction of television in the province) to 11.3% in 1998.[44] In addition, the suicide rate among people with bulimia nervosa is 7.5 times higher than in the general population.[45]		When attempting to decipher the origin of bulimia nervosa in a cognitive context, Christopher Fairburn et al.'s cognitive behavioral model is often considered the golden standard. Fairburn et al.'s model discusses the process in which an individual falls into the binge-purge cycle and thus develops bulimia. Fairburn et al. argue that extreme concern with weight and shape coupled with low self-esteem will result in strict, rigid, and inflexible dietary rules. Accordingly, this would lead to unrealistically restricted eating, which may consequently induce an eventual "slip" where the individual commits a minor infraction of the strict and inflexible dietary rules. Moreover, the cognitive distortion due to dichotomous thinking leads the individual to binge. The binge subsequently should trigger a perceived loss of control, promoting the individual to purge in hope of counteracting the binge. However, Fairburn et al. assert the cycle repeats itself, and thus consider the binge-purge cycle to be self-perpetuating.[46][citation needed]		In contrast, Byrne and Mclean's findings differed slightly from Fairburn et al.'s cognitive behavioral model of bulimia nervosa in that the drive for thinness was the major cause of purging as a way of controlling weight. In turn, Byrne and Mclean argued that this makes the individual vulnerable to binging, indicating that it is not a binge-purge cycle but rather a purge-binge cycle in that purging comes before bingeing. Similarly, Fairburn et al.'s cognitive behavioral model of bulimia nervosa is not necessarily applicable to every individual and is certainly reductionist. Everyone differs from another, and taking such a complex behavior like bulimia and applying the same one theory to everyone would certainly be invalid. In addition, the cognitive behavioral model of bulimia nervosa is very cultural bound in that it may not be necessarily applicable to cultures outside of the Western society. To evaluate, Fairburn et al..'s model and more generally the cognitive explanation of bulimia nervosa is more descriptive than explanatory, as it does not necessarily explain how bulimia arises. Furthermore, it is difficult to ascertain cause and effect, because it may be that distorted eating leads to distorted cognition rather than vice versa.[47][48]		A considerable amount of literature has identified a correlation between sexual abuse and the development of bulimia nervosa. The reported incident rate of unwanted sexual contact is higher among those with bulimia nervosa than anorexia nervosa.[49]		When exploring the etiology of bulimia through a socio-cultural perspective, the "thin ideal internalization" is significantly responsible. The thin ideal internalization is the extent to which individuals adapt to the societal ideals of attractiveness. Studies have shown that young females that read fashion magazines tend to have more bulimic symptoms than those females who do not. This further demonstrates the impact of media on the likelihood of developing the disorder.[50] Individuals first accept and "buy into" the ideals, and then attempt to transform themselves in order to reflect the societal ideals of attractiveness. J. Kevin Thompson and Eric Stice claim that family, peers, and most evidently media reinforce the thin ideal, which may lead to an individual accepting and "buying into" the thin ideal. In turn, Thompson and Stice assert that if the thin ideal is accepted, one could begin to feel uncomfortable with their body shape or size since it may not necessarily reflect the thin ideal set out by society. Thus, people feeling uncomfortable with their bodies may result in suffering from body dissatisfaction and may develop a certain drive for thinness. Consequently, body dissatisfaction coupled with a drive for thinness is thought to promote dieting and negative effects, which could eventually lead to bulimic symptoms such as purging or bingeing. Binges lead to self-disgust which causes purging to prevent weight gain.[51]		A study dedicated to investigating the thin ideal internalization as a factor of bulimia nervosa is Thompson's and Stice's research. The aim of their study was to investigate how and to what degree does media affect the thin ideal internalization. Thompson and Stice used randomized experiments (more specifically programs) dedicated to teaching young women how to be more critical when it comes to media, in order to reduce thin ideal internalization. The results showed that by creating more awareness of the media's control of the societal ideal of attractiveness, the thin ideal internalization significantly dropped. In other words, less thin ideal images portrayed by the media resulted in less thin ideal internalization. Therefore, Thompson and Stice concluded that media affected greatly the thin ideal internalization.[52] Papies showed that it is not the thin ideal itself, but rather the self-association with other persons of a certain weight that decide how someone with bulimia nervosa feels. People that associate themselves with thin models get in a positive attitude when they see thin models and people that associate with overweight get in a negative attitude when they see thin models. Moreover, it can be taught to associate with thinner people.[53]		There are two main types of treatment given to those suffering with bulimia nervosa; psychopharmacological and psychosocial treatments.[54]		There are several supported psychosocial treatments for bulimia. Cognitive behavioral therapy (CBT), which involves teaching a person to challenge automatic thoughts and engage in behavioral experiments (for example, in session eating of "forbidden foods") has a small amount of evidence supporting its use.[55]		By using CBT people record how much food they eat and periods of vomiting with the purpose of identifying and avoiding emotional fluctuations that bring on episodes of bulimia on a regular basis.[56] Barker (2003) states that research has found 40–60% of people using cognitive behaviour therapy to become symptom free. He states in order for the therapy to work, all parties must work together to discuss, record and develop coping strategies. Barker (2003) claims by making people aware of their actions they will think of alternatives.[57][58] People undergoing CBT who exhibit early behavioral changes are most likely to achieve the best treatment outcomes in the long run.[59] Researchers have also reported some positive outcomes for interpersonal psychotherapy and dialectical behavior therapy.[60][61]		Maudsley family therapy, developed at the Maudsley Hospital in London for the treatment of anorexia has been shown promising results in bulimia.[62]		The use of Cognitive Behavioral Therapy (CBT) has been shown to be quite effective for treating bulimia nervosa (BN) in adults, but little research has been done on effective treatments of BN for adolescents.[63] Although CBT is seen as more cost efficient and helps individuals with BN in self-guided care, Family Based Treatment (FBT) might be more helpful to younger adolescents who need more support and guidance from their families. Adolescents are at the stage where their brains are still quite malleable and developing gradually.[64] Therefore, young adolescents with BN are less likely to realize the detrimental consequences of becoming bulimic and have less motivation to change,[65] which is why FBT would be useful to have families intervene and support the teens.[63] Working with BN patients and their families in FBT can empower the families by having them involved in their adolescent's food choices and behaviors, taking more control of the situation in the beginning and gradually letting the adolescent become more autonomous when they have learned healthier eating habits.[63]		Antidepressants of the selective serotonin reuptake inhibitors (SSRI) class may have a modest benefit.[7] This includes fluoxetine, which is FDA approved, for the treatment of bulimia, other antidepressants such as sertraline may also be effective against bulimia. Topiramate may also be useful but has greater side effects.[7]		It is not known if combining medication with counseling improves the outcomes. Any trials which originally suggested that such combinations should improve the outcome have not proven to be exceptionally powerful. Some positive outcomes of treatments can include: abstinence from binge eating, a decrease in obsessive behaviors to lose weight and in shape preoccupation, less severe psychiatric symptoms, a desire to counter the effects of binge eating, as well as an improvement in social functioning and reduced relapse rates.[4]		Some researchers have also claimed positive outcomes in hypnotherapy.[66]		There is little data on the percentage of people with bulimia in general populations. Most studies conducted thus far have been on convenience samples from hospital patients, high school or university students. These have yielded a wide range of results: between 0.1% and 1.4% of males, and between 0.3% and 9.4% of females.[67] Studies on time trends in the prevalence of bulimia nervosa have also yielded inconsistent results.[68] According to Gelder, Mayou and Geddes (2005) bulimia nervosa is prevalent between 1 and 2 percent of women aged 15–40 years. Bulimia nervosa occurs more frequently in developed countries[56] and in cities, with one study finding that bulimia is five times more prevalent in cities than in rural areas.[69] There is a perception that bulimia is most prevalent amongst girls from middle-class families;[70] however, in a 2009 study girls from families in the lowest income bracket studied were 153 percent more likely to be bulimic than girls from the highest income bracket.[71]		There are higher rates of eating disorders in groups involved in activities which idealize a slim physique, such as dance,[72] gymnastics, modeling, cheerleading, running, acting, swimming, diving, rowing and figure skating. Bulimia is thought to be more prevalent among Caucasians;[73] however, a more recent study showed that African-American teenage girls were 50 percent more likely than white girls to exhibit bulimic behavior, including both binging and purging.[74]		The term bulimia comes from Greek βουλιμία boulīmia, "ravenous hunger", a compound of βοῦς bous, "ox" and λιμός, līmos, "hunger".[84] Literally, the scientific name of the disorder, bulimia nervosa, translates to "nervous ravenous hunger".		Although diagnostic criteria for bulimia nervosa did not appear until 1979, evidence suggests that binging and purging were popular in certain ancient cultures. The first documented account of behavior resembling bulimia nervosa was recorded in Xenophon's Anabasis around 370 B.C, in which Greek soldiers purged themselves in the mountains of Asia Minor. It is unclear whether this purging was preceded by binging.[85] In ancient Egypt, physicians recommended purging once a month for three days in order to preserve health.[86] This practice stemmed from the belief that human diseases were caused by the food itself. In ancient Rome, elite society members would vomit in order to "make room" in their stomachs for more food at all day banquets.[86] Emperors Claudius and Vitellius both were gluttonous and obese, and they often resorted to habitual purging.[86]		Historical records also suggest that some saints who developed anorexia (as a result of a life of asceticism) may also have displayed bulimic behaviors.[86] Saint Mary Magdalen de Pazzi (1566–1607) and Saint Veronica Giuliani (1660–1727) were both observed binge eating—giving in, as they believed, to the temptations of the devil.[86] Saint Catherine of Siena (1347–1380) is known to have supplemented her strict abstinence from food by purging as reparation for her sins. Catherine died from starvation at age thirty-three.[86]		While the psychological disorder "bulimia nervosa" is relatively new, the word "bulimia," signifying overeating, has been present for centuries.[86] The Babylon Talmud referenced practices of "bulimia," yet scholars believe that this simply referred to overeating without the purging or the psychological implications bulimia nervosa.[86] In fact, a search for evidence of bulimia nervosa from the 17th to late 19th century revealed that only a quarter of the overeating cases they examined actually vomited after the binges. There was no evidence of deliberate vomiting or an attempt to control weight.[86]		At the turn of the century, bulimia (overeating) was described as a clinical symptom, but rarely in the context of weight control.[87] Purging, however, was seen in anorexic patients and attributed to gastric pain rather than another method of weight control.[87]		In 1930, admissions of anorexia nervosa patients to the Mayo Clinic from 1917 to 1929 were compiled. Fifty-five to sixty-five percent of these patients were reported to be voluntarily vomiting in order to relieve weight anxiety.[87] Records show that purging for weight control continued throughout the mid-1900s. Several case studies from this era reveal patients suffering from the modern description of bulimia nervosa.[87] In 1939, Rahman and Richardson reported that out of their six anorexic patients, one had periods of overeating and another practiced self-induced vomiting.[87] Wulff, in 1932, treated "Patient D," who would have periods of intense cravings for food and overeat for weeks, which often resulted in frequent vomiting.[86] Patient D, who grew up with a tyrannical father, was repulsed by her weight and would fast for a few days, rapidly losing weight. Ellen West, a patient described by Ludwig Binswanger in 1958, was teased by friends for being fat and excessively took thyroid pills to lose weight, later using laxatives and vomiting.[86] She reportedly consumed dozens of oranges and several pounds of tomatoes each day, yet would skip meals. After being admitted to a psychiatric facility for depression, Ellen ate ravenously yet lost weight, presumably due to self-induced vomiting.[86] However, while these patients may have met modern criteria for bulimia nervosa, they cannot technically be diagnosed with the disorder, as it had not yet appeared in the Diagnostic and Statistical Manual of Mental Disorders at the time of their treatment.[86]		An explanation for the increased instances of bulimic symptoms may be due to the 20th century's new ideals of thinness.[87] The shame of being fat emerged in the 1940s, when teasing remarks about weight became more common. The 1950s, however, truly introduced the trend of an aspiration for thinness.[87]		In 1979, Gerald Russell first published a description of bulimia nervosa, in which he studied patients with a "morbid fear of becoming fat" who overate and purged afterwards.[9] He specified treatment options and indicated the seriousness of the disease, which can be accompanied by depression and suicide.[9] In 1980, bulimia nervosa first appeared in the DSM-III.[9]		After its appearance in the DSM-III, there was a sudden rise in the documented incidences of bulimia nervosa.[86] In the early 1980s, incidences of the disorder rose to about 40 in every 100,000 people.[86] This decreased to about 27 in every 100,000 people at the end of the 1980s/early 1990s.[86] However, bulimia nervosa's prevalence was still much higher than anorexia nervosa's, which at the time occurred in about 14 people per 100,000.[86]		In 1991, Kendler et al. documented the cumulative risk for bulimia nervosa for those born before 1950, from 1950 to 1959, and after 1959.[88] The risk for those born after 1959 is much higher than those in either of the other cohorts.[88]								
Dim sum /ˈdimˈsʌm/ (Chinese: 點心; pinyin: diǎnxīn; Cantonese Yale: dímsām) is a style of Chinese cuisine (particularly Cantonese but also other varieties) prepared as small bite-sized portions of food served in small steamer baskets or on small plates. Dim sum dishes are usually served with tea, and together form a full tea brunch. Dim sum traditionally are served as fully cooked, ready-to-serve dishes. In Cantonese teahouses, carts with dim sum will be served around the restaurant for diners to order from without leaving their seats. The Cantonese tradition of having endless cups of tea and dim sum is also called Yum Cha (饮茶).[1]						Dim sum is usually linked with the older tradition from yum cha (Chinese: 飲茶; Cantonese Yale: yám chàh; pinyin: yǐnchá; literally: "drink tea"), which has its roots in travelers on the ancient Silk Road needing a place to rest. Thus, teahouses were established along the roadside. An imperial physician in the third century wrote that combining tea with food would lead to excessive weight gain[citation needed]. People later discovered that tea can aid in digestion, so teahouse owners began adding various snacks.[citation needed][2]		The unique culinary art dim sum originated with the Cantonese in southern China, who over the centuries transformed yum cha from a relaxing respite to a loud and happy dining experience. In Hong Kong, and in most cities and towns in Guangdong province, many restaurants start serving dim sum as early as five in the morning. It is a tradition for the elderly to gather to eat dim sum after morning exercises. For many in southern China, yum cha is treated as a weekend family day. More traditional dim sum restaurants typically serve dim sum until mid-afternoon. However, in modern society, it has become commonplace for restaurants to serve dim sum at dinner time; various dim sum items are even sold as take-out for students and office workers on the go.		A traditional dim sum brunch includes various types of steamed buns such as cha siu bao (a steamed bun filled with barbecue pork), dumplings and rice noodle rolls, which contain a range of ingredients, including beef, chicken, pork, prawns, and vegetarian options. Many dim sum restaurants also offer plates of steamed green vegetables, roasted meats, congee and other soups. Dessert dim sum is also available and many places offer the customary egg tart. Dim sum is usually eaten as breakfast.		Dim sum can be cooked by steaming and frying, among other methods. The serving sizes are usually small and normally served as three or four pieces in one dish. It is customary to order family style, sharing dishes among all members of the dining party. Because of the small portions, people can try a wide variety of food.		Dim sum brunch restaurants have a wide variety of dishes, usually several dozen. Among the standard fare of dim sum are the following:		The drinking of tea is as important to dim sum as the food. The type of tea to serve on the table is typically one of the first things the server asks dining customers. Several types of tea are served during dim sum :		Chrysanthemum tea – Chrysanthemum tea does not actually contain any tea leaves. Instead it is a flower-based tisane made from chrysanthemum flowers of the species Chrysanthemum morifolium or Chrysanthemum indicum, which are most popular in East Asia. To prepare the tea, chrysanthemum flowers (usually dried) are steeped in hot water (usually 90 to 95 °C (194 to 203 °F) after cooling from a boil) in either a teapot, cup, or glass. However, Chrysanthemum flowers are often paired with Pu-erh tea, and this is often referred to as guk pou (Chinese: 菊普; pinyin: jú pǔ; Cantonese Yale: gūk póu).		Green tea – Freshly picked leaves only go through heating and drying processes, but do not undergo oxidation. This enables the leaves to keep their original green color and retain most natural substances like polyphenols and chlorophyll contained within the leaves. This kind of tea is produced all over China and is the most popular category of tea. Representative varieties include Dragon Well (Chinese: 龍井; pinyin: lóngjǐng; Cantonese Yale: lùhngjéng) and Biluochun from Zhejiang and Jiangsu Provinces respectively.		Oolong tea – The tea leaves are partially oxidized, imparting to them the characteristics of both green and black teas. Its taste is more similar to green tea than black tea, but has a less "grassy" flavor than green tea. The three major oolong-tea producing areas are on the southeast coast of China e.g. Fujian, Guangdong and Taiwan. Tieguanyin (Chinese: 鐵觀音; pinyin: tiěguānyīn; Cantonese Yale: titgūnyām), is one of the most popular choice of tea. It is originally cultivated in Fujian province and is a premium variety of Oolong tea with delightful fragrance.		Pounei tea (Cantonese) or Pu-erh tea (Mandarin) – The tea has undergone years of fermentation, giving them a unique earthy flavor. This variety of tea is usually compressed into different shapes like bricks, discs and bowls.		Scented teas – There can be various mixtures of flowers with green tea, black tea or oolong tea. Flowers used include jasmine, gardenia, magnolia, grapefruit flower, sweet-scented osmanthus and rose. There are strict rules about the proportion of flowers to tea. Jasmine tea is the most popular type of scented tea, and is often the most popular type of tea served at yum cha establishments.		The above teas are produced in most of China. Chinese tea bushes (Camellia sinensis) are cultivated in the mountain areas of tropical and subtropical regions or wherever there is a proper climate, sufficient humidity, adequate sunshine and fertile soil. Chinese tea is classified in many ways, e.g., quality, method of preparation or place of production. The main processing methods include oxidation, fermentation, heating, drying and addition of other ingredients like flowers, herbs or fruits. These help to develop the special flavor of the raw tea leaves.		One aspect unique to dim sum is its method of serving in specialized dim sum brunch restaurants or teahouses.[4] Here, dishes are pushed around the restaurant in steam-heated carts, with servers offering the dishes to customers.[4][5] Pricing of dishes at these types of restaurants may vary, but traditionally the dishes are classified as "small", "medium", "large", "extra-large", or "special". For example, a basket of dumplings may be considered a small dish, while a bowl of congee or plate of lo mai gai may be considered a large dish. Dishes are then priced accordingly by size, with orders typically recorded with a rubber stamp onto a bill card that remains on the table. Servers in some restaurants use distinctive stamps, so that sales statistics for each server can be recorded. Menu items not typically considered dim sum fare, such as a plate of chow mein, are often available; they are typically branded as "kitchen" dishes on menus and are individually priced.		As a means of attracting customers at less busy times, many restaurants have promotional periods on certain days, usually in the early morning or late afternoon. This typically involves charging a fixed price for certain dishes. Typically, the "small", "medium" and "large" items are all charged at the usual price for "small" items; the rule would not apply to "extra large", "special", or "kitchen" dishes. In some places the "tea fee" may also be waived, or a discount be placed on the entire order. Where such periods exist the bill card would have a designated section that allows the items that were ordered during the period and to be easily recognized and charged appropriately.		Another way of pricing the food consumed is to use the number and color of the dishes left on the patron's table as a guide, similar to the method used in some Japanese conveyor belt sushi restaurants. Some newer restaurants offer a "conveyor belt dim sum" format, similar to the conveyor belt sushi eating places.		Other Chinese restaurants may not offer dim sum on moving platforms, and instead take orders, generally on a pre-printed sheet of paper, and serve à la carte, in a manner similar to a Spanish tapas restaurant. Prices of each dim sum dish may then vary depending on the actual dish ordered. This procedure may be used at less busier times in specialized restaurants, as it saves time and resources by chefs not having to anticipate which dishes would be ordered and prepare them in advance; though for the customer, it may take longer for the dishes to be delivered after ordering.		There are common tea-drinking and eating practices or etiquette that Chinese people commonly recognize and use. These are practiced not only during dim sum meals but during other types of Chinese meals as well.		It is customary to pour tea for others before filling one's own cup during a meal. When pouring tea for people on one's left side, the right hand should be used to hold the teapot and vice versa. A common custom among the Cantonese is to thank the person pouring the tea by tapping the bent index finger (if you are single), or by tapping both the index and middle finger (if you are married), which symbolizes the gesture of bowing.[6][7]		This custom is said to be analogous to the ritual of bowing to someone in appreciation. The origin of this gesture is described anecdotally: The Qianlong Emperor went to yum cha with his friends, outside the palace; not wanting to attract attention to himself, the Emperor was disguised. While at yum cha, the Emperor poured his companion some tea, which was a great honor. The companion, not wanting to give away the Emperor's identity in public by bowing, instead tapped his index and middle finger on the table as a sign of appreciation.		Given the number of times tea is poured in a meal, the tapping is a timesaver in loud restaurants or lively company, as an individual being served might be speaking to someone else or have food in their mouth. If a diner does not wish a refill being offered at that time, the fingers are used to "wave off" or politely decline more tea. This does not preclude taking more fresh hot tea at a later time during the meal.		Leaving the lid balanced on the side of the tea pot is a common way of attracting a server's attention, and indicates a silent request that the tea pot be refilled.		Instant dim sum as a fast food has come onto the market in Hong Kong,[8] Mainland China,[9] Taiwan, Indonesia, Singapore, and Malaysia. People can enjoy snacks after a three-minute defrosting or reheating of the instant dim sum in a microwave oven.[8]		In many cities, "street dim sum" is sold from mobile carts and usually consists of dumplings or meatballs steamed in a large container and served on a bamboo skewer. The customer can dip the whole skewer into a sauce bowl and then eat while standing or walking.		Dim sum can be purchased from major grocery stores in most countries with a Chinese population. These dim sum can be easily cooked by steaming or microwaving. Major grocery stores in Hong Kong, Philippines, Singapore, Taiwan, Mainland China, Indonesia, Malaysia, Brunei, Thailand, Australia, United States, and Canada have a variety of frozen or fresh dim sum stocked at the shelves. These include dumplings, shaomai, baozi, rice noodle roll, turnip cake and steamed spare ribs.		In Singapore, as well as other countries, dim sum can be purchased from convenience stores, coffee shops and other eateries. There is halal certified dim sum available (with chicken taking the place of pork) which is very popular in Malaysia, Indonesia and Brunei.				
3EHU, 3EHT, 1GOE		1392		12918		ENSG00000147571		ENSMUSG00000049796		P06850		Q8CIT0		NM_000756		NM_205769		NP_000747		NP_991338		Corticotropin-releasing hormone (CRH) (also known as corticotropin-releasing factor (CRF) or corticoliberin; corticotropin may also be spelled corticotrophin) is a peptide hormone involved in the stress response. It is a releasing hormone that belongs to corticotropin-releasing factor family. In humans, it is encoded by the CRH gene.[3]		Its main function is the stimulation of the pituitary synthesis of ACTH, as part of the HPA Axis.		Corticotropin-releasing hormone (CRH) is a 41-amino acid peptide derived from a 196-amino acid preprohormone. CRH is secreted by the paraventricular nucleus (PVN) of the hypothalamus in response to stress. Increased CRH production has been observed to be associated with Alzheimer's disease and major depression,[4] and autosomal recessive hypothalamic corticotropin deficiency has multiple and potentially fatal metabolic consequences including hypoglycemia.[3] In addition to being produced in the hypothalamus, CRH is also synthesized in peripheral tissues, such as T lymphocytes, and is highly expressed in the placenta. In the placenta, CRH is a marker that determines the length of gestation and the timing of parturition and delivery. A rapid increase in circulating levels of CRH occurs at the onset of parturition, suggesting that, in addition to its metabolic functions, CRH may act as a trigger for parturition.[3]		A recombinant version for diagnostics is called corticorelin (INN).						CRH is produced by parvocellular neuroendocrine cells within the paraventricular nucleus of the hypothalamus and is released at the median eminence from neurosecretory terminals of these neurons into the primary capillary plexus of the hypothalamo-hypophyseal portal system. The portal system carries the CRH to the anterior lobe of the pituitary, where it stimulates corticotropes to secrete adrenocorticotropic hormone (ACTH) and other biologically-active substances (β-endorphin). ACTH stimulates the synthesis of cortisol, glucocorticoids, mineralocorticoids and DHEA.[5]		In the short term, CRH can suppress appetite, increase subjective feelings of anxiety, and perform other functions like boosting attention. Although the distal action of CRH is immunosuppression via the action of cortisol, CRH itself can actually heighten inflammation, a process being investigated in multiple sclerosis research.[6]		The CRH-1 receptor antagonist pexacerfont is currently under investigation for the treatment of generalized anxiety disorder.[7] Another CRH-1 antagonist antalarmin has been researched in animal studies for the treatment of anxiety, depression and other conditions, but no human trials with this compound have been carried out.		Also, abnormally high levels of CRH have been found in the cerebrospinal fluid of people that have committed suicide.[8]		Recent research has linked the activation of the CRH1 receptor with the euphoric feelings that accompany alcohol consumption. A CRH1 receptor antagonist developed by Pfizer, CP-154,526 is under investigation for the potential treatment of alcoholism.[9][10]		Alpha-helical CRH-(9–41) acts as a CRH antagonist.[11]		CRH is also synthesized by the placenta and seems to determine the duration of pregnancy.[12]		Levels rise towards the end of pregnancy just before birth and current theory suggests three roles of CRH in parturition:[13]		In culture, trophoblast CRH is inhibited by progesterone, which remains high throughout pregnancy. Its release is stimulated by glucocorticoids and catecholamines, which increase prior to parturition lifting this progesterone block.[14]		The 41-amino acid sequence of CRH was first discovered in sheep by Vale et al. in 1981.[15] Its full sequence is:		The rat and human peptides are identical and differ from the ovine sequence only by 7 amino acids.[16]		In mammals, studies suggest that CRH has no significant thyrotropic effect. However, in representatives of all non-mammalian vertebrates, it has been found that, in addition to its corticotropic function, CRH has a potent thyrotropic function, acting with TRH to control the thyroid axis (TRH has been found to be less potent than CRH in some species).[17][18]		Corticotropin-releasing hormone has been shown to interact with corticotropin-releasing hormone receptor 1.[19][20]		
Sadhya (Malayalam: സദ്യ) is a variety of dishes traditionally served on a banana leaf in Kerala, India.[1] Sadhya means banquet in Malayalam. It is a vegetarian feast prepared by both, men and women, especially when needed in large quantities, for weddings and other special events.		During a traditional Sadhya celebration people are seated cross-legged on mats. Food is eaten with the right hand, without cutlery. The fingers are cupped to form a ladle. A normal Sadhya can have about 24-28 dishes served as a single course. In cases where it is a much larger one it can have over 64 items in a Sadya like the Sadya for Aranmula Boatrace (Valla Sadhya).suhaim		The main dish is plain boiled rice, served along with other dishes collectively called Kootan (കൂട്ടാന്‍) which include curries like Parippu, Sambar, Rasam, Pulisseri and others like Kaalan, Avial, Thoran, Olan, Pachadi, Kichadi, Koottukari, Elissery, Mango pickle, Pulinji, Naranga achaar (Lime Pickle), as well as Papadam, plantain chips, Sharkara Upperi, Banana, plain curd and Buttermilk. The traditional dessert called Payasam served at the end of the meal is of many kinds and usually three or more are served. The 'Kootan' are made with different vegetables and have different flavours; some say the reason for including so many dishes in the Sadhya is to ensure that the diners will like at least a few dishes.		The dishes are served on specific places on the banana leaf in specific order. For example, the pickles are served on the top left corner and the banana on the bottom left corner, which helps the waiters to easily identify and decide on offering additional servings. The most common ingredients in all the dishes are rice, vegetables, coconut and coconut oil as they are abundant in Kerala. Coconut milk is used in some dishes and coconut oil is used for frying and also as an ingredient in others.		There are variations in the menu depending on the place and religion. Some communities, especially those in the northern part of Kerala, include non-vegetarian dishes in the sadhya. Although custom was to use traditional and seasonal vegetables, it has become common practice to include vegetables such as carrots, pineapples, beans in the dishes. Tradition has it that Onion and garlic are not typically used in the sadhya. Conventionally, the meal may be followed by vettila murukkan, chewing of betel leaf with lime and arecanut. This helps digestion of the meal and also cleanses the palate.						The sadhya is usually served for lunch. Preparations begin the night before, and the dishes are prepared before ten o' clock in the morning on the day of the celebration. On many occasions, sadhya is served on tables, as people no longer find it convenient to sit on the floor.		Traditionally, the people of the neighborhood spent the night helping the cooks in cooking. They also volunteer to serve the food for the hosts to the guests. This involves a fair amount of social interaction which help build rapport with the neighbors.		Sadhya is served in Pankthi (Sanskrit) - panti in Malayalam - meaning in lines or rounds where sets of people are served in sitting lines, on the floor earlier, now on benches and desks. There can be many Pankti's depending upon total size of the crowd and the capacity of the place. The hosts normally sits only during the last pankti.		In a Sadhya, the meals are served on a Banana leaf. The leaf is folded and closed once the meal is finished. Closing the leaf away from you signifies complete satisfaction with the food and closing it towards you would mean a signal to the cooks that it needs improvement.		The Travancore-style sadhya is renowned to be the most disciplined and tradition-bound.[2] There is usually an order followed in serving the dishes, starting from the chips and pickles first. However, different styles and approaches to making and serving the dishes are adopted in various parts of Kerala depending on local preferences.		The items include:[2][3]		These side dishes are followed by desserts like Prathaman and Payasams.		Prathaman is a sweet dish in the form of a thick liquid; similar to payasam, but with more variety in terms of ingredients and more elaborately made. It is made with white sugar or jaggery to which coconut milk is added. The main difference between a prathaman and a payasam is that the former uses coconut milk, while the liquid versions of the latter use cow's milk.		Media related to Sadya at Wikimedia Commons		
885		12424		ENSG00000187094		ENSMUSG00000032532		P06307		P09240		NM_000729 NM_001174138		NM_031161 NM_001284508		NP_000720 NP_001167609		NP_001271437 NP_112438		Cholecystokinin (CCK or CCK-PZ; from Greek chole, "bile"; cysto, "sac"; kinin, "move"; hence, move the bile-sac (gallbladder)) is a peptide hormone of the gastrointestinal system responsible for stimulating the digestion of fat and protein. Cholecystokinin, previously called pancreozymin, is synthesized and secreted by enteroendocrine cells in the duodenum, the first segment of the small intestine. Its presence causes the release of digestive enzymes and bile from the pancreas and gallbladder, respectively, and also acts as a hunger suppressant.[3][4]						The existence of CCK was first suggested in 1905 by the British physiologist Joy Simcha Cohen. It is a member of the gastrin/cholecystokinin family of peptide hormones and is very similar in structure to gastrin, another gastrointestinal hormone. CCK and gastrin share the same five C-terminal amino acids. CCK is composed of varying numbers of amino acids depending on post-translational modification of the 150-amino acid precursor, preprocholecystokinin.[5] Thus, the CCK peptide hormone exists in several forms, each identified by the number of amino acids it contains, e.g., CCK58, CCK33, CCK22 and CCK8. CCK58 assumes a helix-turn-helix configuration.[6] Biological activity resides in the C-terminus of the peptide. Most CCK peptides have a sulfate-group attached to a tyrosine located seven residues from the C-terminus.[5] This modification is crucial for the ability of CCK to activate the cholecystokinin A receptor. Nonsulfated CCK peptides also occur, which consequently cannot activate the CCK-A receptor.[7]		CCK plays important physiologic roles both as a neuropeptide in the central nervous system and as a peptide hormone in the gut.[8] It participates in a number of physiological processes such as digestion, satiety and anxiety.		CCK is synthesized and released by enteroendocrine cells in the mucosal lining of the small intestine (mostly in the duodenum and jejunum), called I cells, neurons of the enteric nervous system, and neurons in the brain.[3] It is released rapidly into the circulation in response to a meal. The greatest stimulator of CCK release is the presence of fatty acids and/or certain amino acids in the chyme entering the duodenum.[5] In addition, release of CCK is stimulated by monitor peptide (released by pancreatic acinar cells), CCK-releasing protein (via paracrine signalling mediated by enterocytes in the gastric and intestinal mucosa), and acetylcholine (released by the parasympathetic nerve fibers of the vagus nerve).[9]		Once in the circulatory system, CCK has a relatively short half-life.[10]		CCK mediates digestion in the small intestine by inhibiting gastric emptying. It stimulates the acinar cells of the pancreas to release a juice rich in pancreatic digestive enzymes (hence an alternate name, pancreozymin) that catalyze the digestion of fat, protein, and carbohydrates. Thus, as the levels of the substances that stimulated the release of CCK drop, the concentration of the hormone drops as well. The release of CCK is also inhibited by somatostatin and pancreatic peptide. Trypsin, a protease released by pancreatic acinar cells, hydrolyzes CCK-releasing peptide and monitor peptide, in effect turning off the additional signals to secrete CCK.[11]		CCK also causes the increased production of hepatic bile, and stimulates the contraction of the gall bladder and the relaxation of the sphincter of Oddi (Glisson's sphincter), resulting in the delivery of bile into the duodenal part of the small intestine.[3][4] Bile salts form amphipathic lipids, micelles that emulsify fats, aiding in their digestion and absorption.[3]		As a peptide hormone, CCK mediates satiety by acting on the CCK receptors distributed widely throughout the central nervous system. The mechanism for hunger suppression is thought to be a decrease in the rate of gastric emptying.[12] CCK also has stimulatory effects on the vagus nerve, effects that can be inhibited by capsaicin.[13] The stimulatory effects of CCK oppose those of ghrelin, which has been shown to inhibit the vagus nerve.[14]		The effects of CCK vary between individuals. For example, in rats, CCK administration significantly reduces hunger in adult males, but is slightly less effective in younger subjects, and even slightly less effective in females. The hunger-suppressive effects of CCK also are reduced in obese rats.[15]		CCK is found extensively throughout the central nervous system, with high concentrations found in the limbic system.[16] CCK is synthesized as a 115 amino acid preprohormone, that is then converted into multiple isoforms.[16] The predominant form of CCK in the central nervous system is the sulfated octapeptide, CCK-8S.[16]		In both humans and rodents, studies clearly indicate that elevated CCK levels causes increased anxiety.[10] The site of the anxiety-inducing effects of CCK seems to be central with specific targets being the basolateral amygdala, hippocampus, hypothalamus, peraqueductal grey, and cortical regions.[10][17]		The CCK tetrapeptide fragment CCK-4 (Trp-Met-Asp-Phe-NH2) reliably causes anxiety and panic attacks (panicogenic effect) when administered to humans and is commonly used in scientific research for this purpose of in order to test new anxiolytic drugs.[17][18] Positron emission tomography visualization of regional cerebral blood flow in patients undergoing CCK-4 induced panic attacks show changes in the anterior cingulate gyrus, the claustrum-insular-amygdala region, and cerebellar vermis.[16]		Several studies have implicated CCK as a cause of visual hallucinations in Parkinson’s disease. Mutations in CCK receptors in combination with mutated CCK genes potentiate this association. These studies also uncovered potential racial/ethnic differences in the distribution of mutated CCK genes.[8]		CCK has been shown to interact with the Cholecystokinin A receptor located mainly on pancreatic acinar cells and Cholecystokinin B receptor mostly in the brain and stomach. CCKB receptor also binds gastrin, a gastrointestinal hormone involved in stimulating gastric acid release and growth of the gastric mucosa.[19][20][21] CCK has also been shown to interact with calcineurin in the pancreas. Calcineurin will go on to activate the transcription factors NFAT 1–3, which will stimulate hypertrophy and growth of the pancreas. CCK can be stimulated by a diet high in protein, or by protease inhibitors.[22] CCK has been shown to interact with orexin neurons, which control appetite and wakefulness (sleep).[23] CCK can have indirect effects on sleep regulation.[24]		CCK in the body cannot cross the blood-brain barrier, but certain parts of the hypothalamus and brainstem are not protected by the barrier.		
The endocrine system is the collection of glands of an organism that secrete hormones directly into the circulatory system to be carried towards distant target organs. The phenomenon of biochemical processes' serving to regulate distant tissues by means of secretions directly into the circulatory system is called endocrine signaling. The major endocrine glands include the pineal gland, pituitary gland, pancreas, ovaries, testes, thyroid gland, parathyroid gland, and adrenal glands. The endocrine system is in contrast to the exocrine system, which secretes its hormones to the outside of the body using ducts. The endocrine system is an information signal system like the nervous system, yet its effects and mechanism are classifiably different. The endocrine system's effects are slow to initiate, and prolonged in their response, lasting from a few hours up to weeks. The nervous system sends information very quickly, and responses are generally short lived. In vertebrates, the hypothalamus is the neural control center for all endocrine systems. The field of study dealing with the endocrine system and its disorders is endocrinology, a branch of internal medicine.[1] Special features of endocrine glands are, in general, their ductless nature, their vascularity, and commonly the presence of intracellular vacuoles or granules that store their hormones. In contrast, exocrine glands, such as salivary glands, sweat glands, and glands within the gastrointestinal tract, tend to be much less vascular and have ducts or a hollow lumen.		In addition to the specialized endocrine organs mentioned above, many other organs that are part of other body systems, such as bone, kidney, liver, heart and gonads, have secondary endocrine functions. For example, the kidney secretes endocrine hormones such as erythropoietin and renin. Hormones can consist of either amino acid complexes, steroids, eicosanoids, leukotrienes, or prostaglandins.[1]		A number of glands that signal each other in sequence are usually referred to as an axis, for example, the hypothalamic-pituitary-adrenal axis.		As opposed to endocrine factors that travel considerably longer distances via the circulatory system, other signaling molecules, such as paracrine factors involved in paracrine signalling diffuse over a relatively short distance.		The word endocrine derives from the Greek words ἐνδο- endo- "inside, within," and κρίνειν krinein "to separate, distinguish".						The pituitary gland (or hypophysis) is an endocrine gland about the size of a pea and weighing 0.5 grams (0.018 oz) in humans. It is a protrusion off the bottom of the hypothalamus at the base of the brain, and rests in a small, bony cavity (sella turcica) covered by a dural fold (diaphragma sellae). The pituitary is functionally connected to the hypothalamus by the median eminence via a small tube called the infundibular stem or pituitary stalk.[2] The anterior pituitary (adenohypophysis) is connected to the hypothalamus via the hypothalamo–hypophyseal portal vessels, which allows for quicker and more efficient communication between the hypothalamus and the pituitary.[3]		Oxytocin and anti-diuretic hormone are not secreted in the posterior lobe, merely stored.		Lowers rate of gastric emptying		Reduces smooth muscle contractions and blood flow within the intestine.[4]		Enhances effects of cholecystokinin, stops production of gastric juice		Release of bile from gallbladder, hunger suppressant		regulate cell growth and development		release of aldosterone from adrenal cortex dipsogen.		The pancreas is a mixocrine gland and it secretes both enzymes and hormones.		Intake of lipids and synthesis of triglycerides in adipocytes. Other anabolic effects		Increases blood glucose level.		Inhibit release of glucagon[7] Suppress the exocrine secretory action of pancreas.		Increase absorption of calcium and phosphate from gastrointestinal tract and kidneys inhibit release of PTH		Virilizing: maturation of sex organs, formation of scrotum, deepening of voice, growth of beard and axillary hair.		Other:		Anti-inflammatory		Protein synthesis:		Coagulation:		Fluid balance:		Gastrointestinal tract:		Melanin:		Cancer:		Lung function:		Other effects on mother similar to ovarian follicle-progesterone		Inhibit immune response, towards the human embryo.		Increase insulin resistance and carbohydrate intolerance		Phosphate:		reducing systemic vascular resistance, reducing blood water, sodium and fats		reducing systemic vascular resistance, reducing blood water, sodium and fats		In 1998, skeletal muscle was identified as an endocrine organ[14] due to its now well-established role in the secretion of myokines.[14][15] The use of the term myokine to describe cytokines and other peptides produced by muscle as signalling molecules was proposed in 2003.[16]		Signalling molecules released by adipose tissue are referred to as adipokines.		The human endocrine system consists of several systems that operate via feedback loops. Several important feedback systems are mediated via the hypothalamus and pituitary.[18]		Extensive bidirectional interactions exist between the endocrine system and the immune system.[19] Cortisol has major immunosuppressive effects,[20][21] and dopamine has immunomodulatory functions.[22] On the other hand, cytokines produced during inflammation activate the HPA axis at all three levels, sensible to negative feedback.[23] Moreover, cytokines stimulate hepcidin release from the liver, which is eventually responsible for the anemia of chronic disease.[24]		The typical mode of cell signaling in the endocrine system is endocrine signaling, that is, using the circulatory system to reach distant target organs. However, there are also other modes, i.e., paracrine, autocrine, and neuroendocrine signaling. Purely neurocrine signaling between neurons, on the other hand, belongs completely to the nervous system.		Autocrine signaling is a form of signaling in which a cell secretes a hormone or chemical messenger (called the autocrine agent) that binds to autocrine receptors on the same cell, leading to changes in the cells.		Some endocrinologists and clinicians include the paracrine system as part of the endocrine system, but there is not consensus. Paracrines are slower acting, targeting cells in the same tissue or organ. An example of this is somatostatin which is released by some pancreatic cells and targets other pancreatic cells.[1]		Juxtacrine signaling is a type of intercellular communication that is transmitted via oligosaccharide, lipid, or protein components of a cell membrane, and may affect either the emitting cell or the immediately adjacent cells.[25]		It occurs between adjacent cells that possess broad patches of closely opposed plasma membrane linked by transmembrane channels known as connexons. The gap between the cells can usually be between only 2 and 4 nm.[3]		Diseases of the endocrine system are common,[27] including conditions such as diabetes mellitus, thyroid disease, and obesity. Endocrine disease is characterized by irregulated hormone release (a productive pituitary adenoma), inappropriate response to signaling (hypothyroidism), lack of a gland (diabetes mellitus type 1, diminished erythropoiesis in chronic renal failure), or structural enlargement in a critical site such as the thyroid (toxic multinodular goitre). Hypofunction of endocrine glands can occur as a result of loss of reserve, hyposecretion, agenesis, atrophy, or active destruction. Hyperfunction can occur as a result of hypersecretion, loss of suppression, hyperplastic or neoplastic change, or hyperstimulation.		Endocrinopathies are classified as primary, secondary, or tertiary. Primary endocrine disease inhibits the action of downstream glands. Secondary endocrine disease is indicative of a problem with the pituitary gland. Tertiary endocrine disease is associated with dysfunction of the hypothalamus and its releasing hormones.[citation needed]		As the thyroid, and hormones have been implicated in signaling distant tissues to proliferate, for example, the estrogen receptor has been shown to be involved in certain breast cancers. Endocrine, paracrine, and autocrine signaling have all been implicated in proliferation, one of the required steps of oncogenesis.[28]		Other common diseases that result from endocrine dysfunction include Addison’s disease, Cushing’s disease and Grave’s disease. Cushing's disease and Addison's disease are pathologies involving the dysfunction of the adrenal gland. Dysfunction in the adrenal gland could be due to primary or secondary factors and can result in hypercortisolism or hypocortisolism . Cushing’s disease is characterized by the hypersecretion of the adrenocorticotropic hormone (ACTH) due to a pituitary adenoma that ultimately causes endogenous hypercortisolism by stimulating the adrenal glands.[29] Some clinical signs of Cushing’s disease include obesity, moon face, and hirsutism.[2] Addison's disease is an endocrine disease that results from hypocortisolism caused by adrenal gland insufficiency. Adrenal insufficiency is significant because it is correlated with decreased ability to maintain blood pressure and blood sugar, a defect that can prove to be fatal.[30]		Graves' disease involves the hyperactivity of the thyroid gland which produces the T3 and T4 hormones.[2]  Graves' disease effects range from excess sweating, fatigue, heat intolerance and high blood pressure to swelling of the eyes that causes redness, puffiness and in rare cases reduced or double vision.[3]		A neuroendocrine system has been observed in all animals with a nervous system and all vertebrates have an hypothalamus-pituitary axis.[31] All vertebrates have a thyroid, which in amphibians is also crucial for transformation of larvae into adult form.[32][33] All vertebrates have adrenal gland tissue, with mammals unique in having it organized into layers.[34] All vertebrates have some form of renin-angiotensin axis, and all tetrapods have aldosterone as primary mineralocorticoid.[35][36]		Female endocrine system.		Male endocrine system		
A side dish, sometimes referred to as a side order, side item, or simply a side, is a food item that accompanies the entrée or main course at a meal.[1]						Side dishes such as salad, potatoes and bread are commonly used with main courses throughout many countries of the western world. New side orders introduced within the past decade[citation needed], such as rice and couscous, have grown to be quite popular throughout Europe, especially at formal occasions (with couscous appearing more commonly at dinner parties with Middle Eastern dishes).		When used as an adjective qualifying the name of a dish, the term "side" usually refers to a smaller portion served as a side dish, rather than a larger, main dish-sized serving. For example, a "side salad" usually served in a small bowl or salad plate, in contrast to a large dinner-plate-sized entrée salad.		A typical American meal with a meat-based main dish might include one vegetable side dish, sometimes in the form of a salad, and one starch side dish, such as bread, potatoes, rice, or pasta.		Some common side dishes include:		Some restaurants offer a limited selection of side dishes which are included with the price of the entrée as a combination meal. In contrast, sometimes side dishes are ordered separately from an a la carte menu. The term may or may not imply that the dish can only be ordered with other food.[citation needed]		French fries are the most common side dish served at fast-food restaurants and other American cuisine restaurants.[citation needed] In response to criticism about the high fat and calorie content of French fries, some fast-food chains have recently begun to offer other side dishes, such as salads, as substitutes for the standard French fries with their combination meals.		The related phrase on the side may be synonymous with "side dish" – as in "French fries on the side" – or may refer to a complimentary sauce or condiment served in a separate dish. For example, a diner may request a salad be served with its dressing "on the side".		
Service à la russe (French, "service in the Russian style") is a manner of dining that involves courses being brought to the table sequentially. It contrasts with service à la française ("service in the French style") in which all the food is brought out at once, in an impressive display.		Russian Ambassador Alexander Kurakin is credited with bringing service à la russe to France in the early 19th century. It later caught on in England[citation needed] and is now the style in which most modern Western restaurants serve food (with some significant modifications).						For the most correct service à la russe, the following must be observed:		The place setting (called a cover) for each guest includes a service plate, all the necessary cutlery except those required for dessert, and stemmed glasses for water, wines and champagne. Atop the service plate is a rolled napkin, and atop that is the place card. Above the plate is a saltcellar, nut dish, and a menu.		The cutlery to the right of the service plate is, from the outside in, the oyster fork resting in the bowl of the soup spoon, the fish knife, the meat knife and the salad knife (or fruit knife). On the left, from the outside in, are the fish fork, the meat fork and a salad fork (or fruit fork). (If both a salad and a fruit course are served, the necessary extra flatware must be brought out on a platter, as it is bad form to have more than three knives or forks on the table at once, the oyster fork excepted.)[citation needed]		Guests are seated according to their place cards and immediately remove their napkins and place them in their laps. Another view maintains that the napkin is only removed after the host has removed his or hers. In the same manner, the host is first to begin eating, and guests follow. Then the oyster plate is placed atop the service plate. Once that is cleared, the soup plate replaces it. After the soup course is finished, both the soup plate and service plate are removed from the table, and a heated plate is put in their place. (The rule is as such: a filled plate is always replaced with an empty one, and no place goes without a plate until just before the dessert course.)		The fish and meat courses are always served from platters because in correct service a filled plate is never placed before a guest, as this would indirectly dictate how much food the guest is to eat.		Directly before dessert, everything is removed from the place settings except the wine and water glasses. Crumbs are cleared now. The dessert plate is then brought out with a doily on top of it, a finger bowl on top of that, and a fork and spoon, the former balanced on the left side of the plate and the latter on the right. Guests remove the doily and finger bowls, move them to the left of the plate and place the fork to the left side of the plate and the spoon to its right. Guests do not actually need to use the finger bowl, since they may have not used their fingers to eat with, unless they also had bread with the meal.		The number of dishes (or courses) served at a meal à la russe has changed over time; but an underlying pattern of service—beginning with soup, then moving through fish and various entrées, then to the roast or game, and then to salad, sweets and coffee—persisted from the mid-19th century, when this type of service was introduced into France, up to the Second World War, continuing in a much-reduced form from the War into the 21st century. The order of dishes descends directly from the much older service à la française. In that style of service, all sorts of dishes were arranged on the table and guests served themselves; but as Jean-Louis Flandrin has shown, the order of consumption—known to the guests of the time but rarely evident from contemporary menus or descriptions of meals—was essentially the same as the order of presentation in service à la russe.[1]		The most elaborate version of service à la russe, which reached its pinnacle in the last decades of the Victorian era, was described by Sarah Tyson Rorer in 1886. Rorer was critical of this elaborate service and offered a much simpler alternative, which in fact represents the core principals of this style of service.		The elaborate and conventional dinner, complete at all points, which the dinner-giving of a century and a half has evolved, is beyond any but the very wealthy. Very few of them succeed in giving it, and still fewer of their guests enjoy it. Its triple triplets of oysters, soup, and fish, the relevé, entrées, and roast, a pause of rum punch to stimulate languishing digestion, game with salad, sweets and ice, coffee to close, and a bewildering series of wines, with an alcoholic appetizer to begin and end, have, however, had their effect in making many feel that a formal dinner must only follow this model from afar. So, with only the resources of a simple household, they compass, with infinite labor, oysters, soup, and fish, add some made dish to the meat, and put salad before and ice cream after the pudding or sweets. But success here, with a moderate income, is as rare as success with the long dinner at the complete table. Try to grasp the theory of the elaborate edifice which custom and convention has piled up, and see if your own resources cannot reproduce its purpose with better success. After having carefully analyzed it, you will see at once that the most complex dinner simply aims to begin with something of easy digestion, slide by some transition to the roast, and make sure that through salad, sweets and coffee, the last half of your dinner shall interest the appetite as well as satisfy hunger. You, have, therefore, soup, roast, dessert, which make up the usual dinner of thoroughly civilized people, and below you will see how, with but moderate resources, you may so vary this as to make a “little dinner” complete and satisfying in itself; more, the most elaborate meal at Delmonico’s cannot do.[2]:247-248		In Britain and the United States, at the time Rorer was writing, fish was a distinct course; relevés meant large, solid joints of meat or whole fowl, generally baked, braised, or boiled but not roasted (in France at the time, relevés encompassed whole fish and fowl and solid joints indiscriminately)[3]:2; entrées meant elaborate "made dishes" of, typically, fillets of beef or other butcher's meat (and sometimes fowl, but not fish), served in fine sauces; roasts were roasted, solid joints of meat (and sometimes fowl) other than feathered game; in general, game was feathered, rather than furred, spit-roasted whole and served rather simply.		Near the end of the 19th century, Charles Ranhofer, chef at Delmonico's Restaurant in New York, in his cookbook The Epicurean, outlined in great detail the dishes necessary for dinners ranging from five to fourteen courses. The five-course dinner was very much like Rorer's "little dinner": soup, fish, entrée, roast, salad, and dessert. Longer dinners were arranged by adding side dishes, removes, and various cold dishes, and by serving a greater number of entrées and desserts. The longest and "most elaborate meal at Delmonico’s", as Rorer put it, was as follows:		Figure 1—36 covers:		S.D. means side dishes, such as little timbales or croquettes; removes are relevés. For the "roast", Ranhofer and many others at the time made no distinctions among butchers' meat, fowl, and game. Cold dishes, such as mayonnaise salads and aspics, had also become very popular. When more than one dish was appointed for a course (e.g. 2 Soups, 2 Fish, 2 roasts, 2 colds), the guest was expected to choose one or the other, not both; a guest might also decline one or more of the courses.		Ranhofer also gives elaborate instructions for the service of wine.		FIRST SERVICE.		With Oysters.—Sauterne, Barsac, Graves, Mont Rachet, Chablis. After the Soup.—Madeira, Sherry or Xeres. With Fish.—(Rhine wines) Johannisberger, Marcobrunner, Hochheimer, Laubenheimer, Liebfraumilch, Steinberger. (Moselle) Brauneberger, Zeltinger, Berncasteler. With Removes.—Côte St. Jacques, Moulin-à-vent, Macon, Clos de Vougeot, Beaune. With Entrées.—St. Émilion, Médoc du Bordelais, St. Julien. Dry champagnes for certain countries. Iced Punches and Sherbets, Rum, Madeira.		SECOND SERVICE. With Roasts.—(Burgundies) Pommard, Nuits, Corton, Chambertin, Romanée Conti. Cold Roasts.—Vin de Paille, Steinberger. With Hot Desserts.—(Bordeaux) Château Margaux, Léoville, Laffitte, Château Larose, Pontet-Canet, St. Pierre, Côtes de Rhone, Hermitage and Côte-Rôtie. (Red Champagne) Bouzy, Verzenay, Porto Première.		THIRD SERVICE. With Dessert.—(Burgundy) Volnay, Mousseux. (Champagnes) Delmonico, Roederer, Rosé Mousseux, Pommery, Cliquot, Perrier-Jouët, Moët, Mumm. Wine Liquors.—Muscatel, Malaga, Alicante, Malvoisie of Madeira, Lacryma Christi, red and white Cape, Tokay, Constance, Schiraz. Cordials.—Curaçoa [sic], Kirsch, Cognac, Chartreuse, Maraschino, Prunelle, Anisette, Bénédictine.		Several decades later, shorter meals had become the norm and the extravagant dinners of the Victorian period were considered vulgar, as noted by Emily Post in 1922:		Under no circumstances would a private dinner, no matter how formal, consist of more than:		The menu for an informal dinner would leave out the entrée, and possibly either the hors-d’oeuvre or the soup.		At the time Post was writing, hors-d’œuvre meant rather narrowly light cold dishes like oysters, clams, melon, or citrus; entrées meant elaborate "made dishes" of fillets of beef or other butcher's meat served in a fine sauces, or some sort of pastry dish; roasts could be of any meat, but the preferred dish of a truly fine dinner was wild feathered game, spit-roasted and served rather simply; and dessert was molded ice-cream only, to the exclusion of all other sweets.[4]:207		Post's first book was published during Prohibition, and she noted, "A water glass standing alone at each place makes such a meager and untrimmed looking table that most people put on at least two wine glasses, sherry and champagne, or claret and sherry, and pour something pinkish or yellowish into them. [...] Those few who still have cellars, serve wines exactly as they used to, white wine, claret, sherry and Burgundy warm, champagne ice cold; and after dinner, green mint poured over crushed ice in little glasses, and other liqueurs of room temperature."[4]:205		After World War II, dinners were curtailed even more. As Post writes in the 1950 edition of her book, the shorter "informal" meal of her earlier book had become the norm for formal dinners:		Under no circumstances does a modern dinner, no matter how formal, consist of more than:		In addition to the set courses, little relish dishes of radishes, celery, olives, or almonds could be set on the table as "hors-d'œuvre". Wines, too, were often greatly reduced in number. Amy Vanderbilt noted in her book, The Complete Book of Etiquette, "At a formal dinner champagne may be the only wine served after the service of sherry with the soup."[6]:350		This five-course service might be further reduced by serving either soup or fish (or shellfish) as a first course, but not both. Dinners in the French style usually include a cheese course after the roast, generally resulting in a 6-course meal (see, for example, the formal menus in Richard Olney's The French Menu Cookbook[7]); alternatively, one or more of the other courses can be omitted (see, for example, the formal menus in Simone Beck's Simca's Cuisine[8]). Dinners in the American style often place the salad as a first course instead of soup, an innovation that appeared in the 1950s in California and was noted by Vanderbilt [6]:340; in this arrangement, dessert is served immediately after the roast. Wine service may include a separate wine for each course, or simply be champagne throughout; or, most commonly, service may be limited to three wines: a white for the soup and fish, a red for the roast, and a sweet wine or champagne for dessert.		These and similar arrangements of four- and five-course formal dinners were the norm throughout the second half of the 20th century.		
Kaiseki (懐石) or kaiseki-ryōri (懐石料理) is a traditional multi-course Japanese dinner. The term also refers to the collection of skills and techniques that allow the preparation of such meals, and is analogous to Western haute cuisine.[1]		There are basically two kinds of traditional Japanese meal styles called kaiseki or kaiseki-ryōri. The first, where kaiseki is written as 会席 and kaiseki-ryōri as 会席料理, refers to a set menu of select food served on an individual tray (to each member of a gathering).[2] The second, written as 懐石 and as 懐石料理, refers to the simple meal that the host of a chanoyu gathering serves to the guests before a ceremonial tea,[2] and is also known as cha-kaiseki (茶懐石).[3]						The kanji characters 懐石 used to write kaiseki literally mean "bosom-pocket stone". These kanji are thought to have been incorporated by Sen no Rikyū (1522–1591), to indicate the frugal meal served in the austere style of chanoyu (Japanese tea ceremony). The idea came from the practice where Zen monks would ward off hunger by putting warm stones into the front folds of their robes, near their stomachs. Before these kanji started to be used, the kanji for writing the word were simply ones indicating that the cuisine was for a get-together (会席料理).[4] Both sets of kanji remain in use today to write the word; the authoritative Japanese dictionary Kōjien describes the "cuisine for a get-together" as a banquet meal where the main beverage is sake (Japanese rice wine), and the "bosom-stone" cuisine as the simple meal served in chanoyu. To distinguish between the two in speech and if necessary in writing, the chanoyu meal may be referred to as "tea" kaiseki, or cha-kaiseki.[5][6]		Modern kaiseki draws on a number of traditional Japanese haute cuisines, notably the following four traditions: imperial court cuisine (有職料理, yūsoku ryōri), from the 9th century in the Heian period; Buddhist cuisine of temples (精進料理, shōjin ryōri), from the 12th century in the Kamakura period; samurai cuisine of warrior households (本膳料理, honzen ryōri), from the 14th century in the Muromachi period; and tea ceremony cuisine (茶懐石, cha kaiseki), from the 15th century in the Higashiyama period of the Muromachi period. All of these individual cuisines were formalized and developed over time, and continue in some form to the present day, but have also been incorporated into kaiseki cuisine. Different chefs weight these differently – court and samurai cuisine are more ornate, while temple and tea ceremony cuisine are more restrained.		In the present day, kaiseki is a type of art form that balances the taste, texture, appearance, and colors of food.[5] To this end, only fresh seasonal ingredients are used and are prepared in ways that aim to enhance their flavor. Local ingredients are often included as well.[7] Finished dishes are carefully presented on plates that are chosen to enhance both the appearance and the seasonal theme of the meal. Dishes are beautifully arranged and garnished, often with real leaves and flowers, as well as edible garnishes designed to resemble natural plants and animals.		Originally, kaiseki comprised a bowl of miso soup and three side dishes;[8] this is now instead the standard form of Japanese-style cuisine generally, referred to as a セット (setto, "set"). Kaiseki has since evolved to include an appetizer, sashimi, a simmered dish, a grilled dish, and a steamed course,[8] in addition to other dishes at the discretion of the chef.[9]		Sakizuke (先附)		Hassun (八寸)		Owan (お椀)		Otsukuri (お造り)		Agemono (揚げ物)		Futamono (蓋物)		Dai no mono (台の物)		Gohan, Kō no mono, Tomewan (御飯・香の物・止椀)		Mizumono (水物)		This is the meal served in the context of chanoyu (Japanese tea ceremony). It precedes the serving of the tea at a formal tea function (chaji). The basic constituents of a cha-kaiseki meal are the ichijū sansai or "one soup, three side dishes", and the rice, plus the following: suimono, hassun, yutō, and kōnomono. The one soup referred to here is usually miso soup, and the basic three side dishes are the following:		Here under is a description of the additional items mentioned above:		Extra items that may be added to the menu are generally referred to as shiizakana, and these attend further rounds of sake. Because the host leaves them with the first guest, they are also referred to as azukebachi (lit., "bowl left in another's care").[10]		Casual kaiseki meals theatrically arrange ingredients in dishes and combine rough textured pottery with fine patterned bowls or plates for effect. The bento box is another casual, common form of popular Kaiseki.		Kaiseki is often served in ryokan in Japan, but it is also served in small restaurants, known as ryōtei (料亭). Kyoto is well known for its kaiseki, as it was the home of the imperial court and nobility for over a millennium. In Kyoto, kaiseki-style cooking is sometimes known as Kyoto cooking (京料理, kyō-ryōri), to emphasize its traditional Kyoto roots, and includes some influence from traditional Kyoto home cooking, notably obanzai (おばんざい), the Kyoto term for sōzai (惣菜) or okazu (おかず).		Kaiseki is often very expensive – kaiseki dinners at top traditional restaurants generally cost from 15,000 yen to upwards of 40,000 per person[11] (about US $125 to $340 at 2015 exchange rates), without drinks. Cheaper options are available, notably lunch (from around 4,000 to 8,000 yen (US $34 to $68)), and in some circumstances bento (around 2,000 to 4,000 yen (US $17 to $34)). In some cases counter seating is cheaper than private rooms. At ryokan, the meals may be included in the price of the room or optional, and may be available only to guests, or served to the general public (some ryokan are now primarily restaurants). Traditional menu options offer three price levels, Sho Chiku Bai (traditional trio of pine, bamboo, and plum), with pine being most expensive, plum least expensive; this is still found at some restaurants.		
Omnivore /ˈɒmnivɔər/ is a consumption classification for animals that have the capability to obtain chemical energy and nutrients from materials originating from plant and animal origin. Often, omnivores also have the ability to incorporate food sources such as algae, fungi, and bacteria into their diet as well.[3][4][5]		Omnivores come from diverse backgrounds that often independently evolved sophisticated consumption capabilities. For instance, dogs evolved from primarily carnivorous organisms (Carnivora) while pigs evolved from primarily herbivorous organisms (Artiodactyla).[6][7][8] What this means is that physical characteristics are often not reliable indicators of whether an animal has the ability to obtain energy and nutrients from both plant and animal matter. Due to the wide range of entirely unrelated organisms independently evolving the capability to obtain energy and nutrients from both plant and animal materials, no generalizations about the anatomical features of all omnivores can realistically be made.[9]		The variety of different animals that are classified as omnivores can be placed into further categories depending on their feeding behaviors. Frugivores include maned wolves and orangutans;[10][11] insectivores include swallows and pink fairy armadillos;[12][13] granivores include large ground finches and humans. (This is due to the average human diet mainly consisting of grains, with rice, maize and wheat comprising two-thirds of human food consumption).[14]		All of these animals are omnivores, yet still fall into special niches in terms of feeding behavior and preferred foods. Being omnivores gives these animals more food security in stressful times or makes possible living in less consistent environments.[15]						The word omnivore derives from the Latin omnis (all), and vora, from vorare, (to eat or devour), having been coined by the French and later adopted by the English in the 1800s.[16] Traditionally the definition for omnivory was entirely behavioral by means of simply "including both animal and vegetable tissue in the diet.[17]" In more recent times, with the advent of advanced technological capabilities in fields like gastroenterology, biologists have formulated a standardized variation of omnivore used for labeling a species actual ability to obtain energy and nutrients from materials.[18][19] This has subsequently conditioned two context specific definitions.		The taxonomic utility of omnivore's traditional and behavioral definition is limited, since the diet, behavior, and phylogeny of one omnivorous species might be very different from that of another: for instance, an omnivorous pig digging for roots and scavenging for fruit and carrion is taxonomically and ecologically quite distinct from an omnivorous chameleon that eats leaves and insects. The term "omnivory" is also not always comprehensive because it does not deal with mineral foods such as salt licks and the consumption of plant and animal material for medical purposes which would not otherwise be consumed (i.e. zoopharmacognosy) within non-omnivores.		Though Carnivora is a taxon for species classification, no such equivalent exists for omnivores, as omnivores are widespread across multiple taxonomic clades. The Carnivora order does not include all carnivorous species, and not all species within the Carnivora taxon are carnivorous.[25] It is common to find physiological carnivores consuming materials from plants or physiological herbivores consuming material from animals, e.g. felines eating grass and deer eating birds.[26][27] From a behavioral aspect, this would make them omnivores, but from the physiological standpoint, this may be due to zoopharmacognosy. Physiologically, animals must be able to obtain both energy and nutrients from plant and animal materials to be considered omnivorous. Thus, such animals are still able to be classified as carnivores and herbivores when they are just obtaining nutrients from materials originating from sources that do not seemingly complement their classification. For instance, it is well documented that animals such as giraffes, camels, and cattle will gnaw on bones, preferably dry bones, for particular minerals and nutrients.[28] Felines, which are usually regarded as obligate carnivores, occasionally eat grass to regurgitate indigestibles (e.g. hair, bones), aid with hemoglobin production, and as a laxative.[29]		Occasionally, it is found that animals historically classified as carnivorous may deliberately eat plant material. For example, in 2013 it was considered that American alligators (Alligator mississippiensis) may be physiologically omnivorous once investigations had been conducted on why they occasionally eat fruits. It was suggested that alligators probably ate fruits both accidentally but also deliberately.[30]		"Life-history omnivores" is a specialized classification given to organisms that change their eating habits during their life cycle.[31] Some species, such as grazing waterfowl like geese, are known to eat mainly animal tissue at one stage of their lives, but plant matter at another.[32] The same is true for many insects, such as beetles in the family Meloidae,[33] which begin by eating animal tissue as larvae, but change to eating plant matter after they mature. Likewise, many mosquito species in early life eat plants or assorted detritus, but as they mature, males continue to eat plant matter and nectar whereas the females (such as those of Anopheles, Aedes and Culex) also eat blood to reproduce effectively.[34]		Although cases exist of carnivores eating plant matter and herbivores eating meat, the classification "omnivore" refers to the adaptations and main food source of the species in general, so these exceptions do not make either individual animals or the species as a whole omnivorous. For the concept of "omnivore" to be regarded as a scientific classification, some clear set of measurable and relevant criteria would need to be considered to differentiate between an "omnivore" and other categories, e.g. faunivore, folivore, and scavenger.[35] Some researchers argue that evolution of any species from herbivory to carnivory or carnivory to herbivory would be rare except via an intermediate stage of omnivory.[36]		Various mammals are omnivorous in the wild, such as species of pigs,[37] badgers, bears, coatis, civets, hedgehogs, opossums, skunks, sloths, squirrels,[38] raccoons, chipmunks,[39] mice,[40] and rats.[41] Hominidae, including humans and chimpanzees, are also omnivores.[5][42][43]		Most bear species are omnivores,[44] but individual diets can range from almost exclusively herbivorous to almost exclusively carnivorous, depending on what food sources are available locally and seasonally. Polar bears are classified as carnivores, both taxonomically (they are in the order Carnivora), and behaviorally (they subsist on a largely carnivorous diet). Depending on the species of bear, there is generally a preference for one class of food, as plants and animals are digested differently. Wolf subspecies (including wolves, dogs, dingoes, and coyotes) have a general preference and are evolutionarily geared towards meat, but also will voluntarily eat plant material like fruits, vegetables, and grasses, and can live on such indefinitely.[45] Also, the maned wolf is a canid whose diet is naturally 50% plant matter.		While most mammals may display "omnivorous" behavior patterns depending on conditions of supply, culture, season and so on, they will generally prefer a particular class of food, to which their digestive processes are adapted. Like most arboreal species, most squirrels are primarily granivores, subsisting on nuts and seeds.[46] But like virtually all mammals, squirrels avidly consume some animal food when it becomes available. For example, the American eastern gray squirrel has been introduced by humans to parts of Britain, continental Europe and South Africa. Where it flourishes, its effect on populations of nesting birds is often serious, largely because of consumption of eggs and nestlings.[47][48]		Various birds are omnivorous, with diets varying from berries and nectar to insects, worms, fish, and small rodents. Examples include cassowaries, chickens, crows[49] and related corvids, keas, rallidae, and rheas. In addition, some lizards, turtles, fish (such as piranhas and catfish), and invertebrates are also omnivorous.		Quite often, mainly herbivorous creatures will eagerly eat small quantities of animal food when it becomes available. Although this is trivial most of the time, omnivorous or herbivorous birds, such as sparrows, often will feed their chicks insects while food is most needed for growth.[50] On close inspection it appears that nectar-feeding birds such as sunbirds rely on the ants and other insects that they find in flowers, not for a richer supply of protein, but for essential nutrients such as cyanocobalamin that are absent from nectar. Similarly, monkeys of many species eat maggoty fruit, sometimes in clear preference to sound fruit.[51] When to refer to such animals as omnivorous, or otherwise, is a question of context and emphasis, rather than of definition.		
A break at work is a period of time during a shift in which an employee is allowed to take time off from his/her job. There are different types of breaks, and depending on the length and the employer's policies, the break may or may not be paid.		Meal breaks or lunch breaks usually range from twenty minutes to one hour. Their purpose is to allow the employee to have a meal that is regularly scheduled during the work day. For a typical daytime job, this is lunch, but this may vary for those with other work hours. It is not uncommon for this break to be unpaid, and for the entire work day from start to finish to be longer than the number of hours paid in order to accommodate this time.		According to a study, the amount of time people are taking for lunch breaks in the United States is shrinking, thereby making the term "lunch hour" a misnomer.[1] Some employers request the lunch to be taken at their work station or do not offer lunch breaks at all. Many employees are taking shorter lunch breaks in order to compete with other employees for a better position, and to show their productivity.[2]		In some places, such as the state of California, meal breaks are legally mandated.[1] Penalties can be severe for failing to adequately staff one's business premises so that all employees can rotate through their mandatory meal and rest breaks. For example, on April 16, 2007, the Supreme Court of California unanimously affirmed a trial court judgment requiring Kenneth Cole Productions to pay an additional hour of pay for each day that a store manager had been forced to work a nine-hour shift without a break.[3] On April 12, 2012 the Supreme Court of California issued its long-awaited opinion in Brinker Restaurant Corp., et al. v. Superior Court.,[4] which addressed a number of issues that have been the subject of much litigation in California for many years. The California Supreme court ruled that employers satisfy their California Labor Code section 512 obligation to "provide" meal periods to nonexempt employees by (1) relieving employees of all duty; (2) relinquishing control over their activities and permitting them a reasonable opportunity to take an uninterrupted 30-minute break; and (3) not impeding or discouraging them from doing so. Importantly, the court agreed that employers are not obliged to "police" meal breaks to ensure that no work is performed. Even if an employee chooses to work during a properly provided meal period, an employer will not be liable for any premium pay, and will only be liable to pay for the time worked during a meal period so long as the employer knew or reasonably should have known that the employee was working during the meal period.[5]						A short break to allow an employee to use a restroom or WC and will generally last less than 10 minutes. Many employers expect their employees to use the facilities during their regularly scheduled breaks and lunches. Denying employees rights to use the facilities as needed could adversely affect workplace sanitation and workers' health and could create legal issues for both these and other reasons.[6] Employers and co-workers often frown on employees who are seen as taking too many of these breaks, and this could be a cause for progressive discipline from a written warning up to termination. In today's setting however, restroom breaks are generally accepted and not tracked by employers. In 2017, an official in the Swedish town of Overtornea proposed an hour-long break for sexual activity.[7]		A coffee break in the United States and elsewhere is a short mid-morning rest period granted to employees in business and industry, corresponding with the Commonwealth terms "elevenses", "smoko" (in Australia), "morning tea", "tea break", or even just "tea". An afternoon coffee break, or afternoon tea, often occurs as well.		The origin of the tea break as is now incorporated into the law of most countries, stems from research undertaken in England in the early 1900s. A F Stanley Kent, an Oxford graduate and the first Professor of Physiology at University College, Bristol, undertook scientific research on Industrial Fatigue at the request of the Home Office (UK). This work followed the International Congress of Hygiene and Demography held in Brussels in 1903 where a resolution was passed that "the various governments should facilitate as far as possible investigation into the subject of Industrial Fatigue". This was due to its noted bearing on incidence of accidents and excessive sickness. The monotony of work and the effect of alcohol on muscular activity and mental fatigue were also mentioned. The Tea Break came as a direct result of this work.		When Kent was sent by the Home Secretary to stop wartime munitions production as a trial to test the effect of a tea break on productivity, the factory manager refused on the grounds that he had a production schedule within which he must comply. Meeting this challenge, Kent showed the letter from the Home Secretary and observed that if necessary he would have the police called to arrest the manager who blocked the Home Office directive. The results of Kent's study were presented to both Houses of Parliament on 17 August 1915 in an "Interim Report on Industrial Fatigue by Physiological Methods". It was the first time that the government had owned and operated factories and therefore had the right to intervene in their operational methods. Again presenting to both Houses of Parliament on 16 August 1916, Kent read from his "Blue Book" that during his research it had been "possible to obtain information upon...such [matters] as the need to provide canteens in munitions factories, the question of proper feeding of the factory worker, provision of accommodation in factories for the changing and drying of shoes and clothing, and the proper use of appliances provided for ventilating the work-rooms".[8]		The coffee break allegedly originated in the late 19th century in Stoughton, Wisconsin, with the wives of Norwegian immigrants. The city celebrates this every year with the Stoughton Coffee Break Festival.[9] In 1951, Time noted that "[s]ince the war, the coffee break has been written into union contracts".[10] The term subsequently became popular through a Pan-American Coffee Bureau ad campaign of 1952 which urged consumers, "Give yourself a Coffee-Break — and Get What Coffee Gives to You."[11] John B. Watson, a behavioral psychologist who worked with Maxwell House later in his career, helped to popularize coffee breaks within the American culture.[12]		Coffee breaks usually last from 10 to 20 minutes and frequently occur at the end of the first third of the work shift. In some companies and some civil service, the coffee break may be observed formally at a set hour. In some places, a "cart" with hot and cold beverages and cakes, breads and pastries arrives at the same time morning and afternoon, an employer may contract with an outside caterer for daily service, or coffee breaks may take place away from the actual work-area in a designated cafeteria or tea room.		More generally, the phrase "coffee break" has also come to denote any break from work.		Snack breaks are usually shorter than meal breaks, and allow an employee to have a quick snack, or to accomplish other personal needs. Similar types of breaks include restroom and smoke breaks. These breaks are also required in the state of California; one 10-15 minute break for every 3.5 hours worked. A few other states have similar laws, but most do not.[citation needed] Some employers allow employees to stop their work for short durations at any time to take care of these needs.		Many companies in the 21st century do not allow smoking on their property, although some employers allow workers to leave the premises to smoke, and some jurisdictions have laws prohibiting smoking in an enclosed place where others are employed. Smoke breaks can be of different lengths but for the most part are shorter than lunch breaks. Some employers are very strict about smoking. A criticism of smoking breaks is that non-smoking employees do not receive the small respite because they simply do not smoke. To certain working environments, however, smoking breaks are widely accepted and seen by some as a good way to network with colleagues (and the management).		
A drink or beverage is a liquid intended for human consumption.		In addition to their basic function of satisfying thirst, drinks play important roles in human culture. Common types of drinks include plain water, milk, juices, coffee, tea, and soft drinks. In addition, alcoholic drinks such as wine, beer, and liquor, which contain the drug ethanol, have been part of human culture and development for 8,000 years.		Non-alcoholic drinks often signify drinks that would normally contain alcohol, such as beer and wine, but are made with less than .5 percent alcohol by volume. The category includes drinks that have undergone an alcohol removal process such as non-alcoholic beers and de-alcoholized wines.						When the human body becomes dehydrated it experiences the sensation of thirst. This craving of fluids results in an instinctive need to drink. Thirst is regulated by the hypothalamus in response to subtle changes in the body's electrolyte levels, and also as a result of changes in the volume of blood circulating. The complete elimination of drinks, i.e. water, from the body will result in death faster than the removal of any other substance.[1] Water and milk have been basic drinks throughout history.[1] As water is essential for life, it has also been the carrier of many diseases.[2]		As mankind evolved, new techniques were discovered to create drinks from the plants that were native to their areas. The earliest archaeological evidence of wine production yet found has been at sites in Georgia (c. 6000 BCE)[3][4][5] and Iran (c. 5000 BCE).[6] Beer may have been known in Neolithic Europe as far back as 3000 BCE,[7] and was mainly brewed on a domestic scale][8] The invention of beer (and bread) has been argued to be responsible for humanity's ability to develop technology and build civilization.[9][10][11] Tea likely originated in Yunnan, China during the Shang Dynasty (1500 BCE–1046 BCE) as a medicinal drink.[12]		Drinking has been a large part of socialising throughout the centuries. In Ancient Greece, a social gathering for the purpose of drinking was known as a symposium, where watered down wine would be drunk. The purpose of these gatherings could be anything from serious discussions to direct indulgence. In Ancient Rome, a similar concept of a convivium took place regularly.		Many early societies considered alcohol a gift from the gods,[13] leading to the creation of gods such as Dionysus. Other religions forbid, discourage, or restrict the drinking of alcoholic drinks for various reasons. In some regions with a dominant religion the production, sale, and consumption of alcoholic drinks is forbidden to everybody, regardless of religion.		Toasting is a method of honouring a person or wishing good will by taking a drink.[13] Another tradition is that of the loving cup, at weddings or other celebrations such as sports victories a group will share a drink in a large receptacle, shared by everyone until empty.[13]		In East Africa and Yemen, coffee was used in native religious ceremonies. As these ceremonies conflicted with the beliefs of the Christian church, the Ethiopian Church banned the secular consumption of coffee until the reign of Emperor Menelik II.[14] The drink was also banned in Ottoman Turkey during the 17th century for political reasons[15] and was associated with rebellious political activities in Europe.		A drink is a form of liquid which has been prepared for human consumption. This can include a number of different steps, some prior to transport, others immediately prior to consumption.		Water is the chief constituent in all drinks, and the primary ingredient in most. Water is purified prior to drinking. Methods for purification include filtration and the addition of chemicals, such as chlorination. The importance of purified water is highlighted by the World Health Organisation, who point out 94% of deaths from diarrhea – the third biggest cause of infectious death worldwide at 1.8 million annually – could be prevented by improving the quality of the victim's environment, particularly safe water.[16]		Pasteurisation is the process of heating a liquid for a period of time at a specified temperature, then immediately cooling. The process reduces the growth of micro-organisms within the liquid, thereby increasing the time before spoilage. It is primarily used on milk, which prior to pasteurisation is commonly infected with pathogenic bacteria and therefore the more likely than any other part of the common diet in the developed world to cause illness.[17]		The process of extracting juice from fruits and vegetables can take a number of forms. Simple crushing of most fruits will provide a significant amount of liquid, though a more intense pressure can be applied to get the maximum amount of juice from the fruit. Both crushing and pressing are processes used in the production of wine.		Infusion is the process of extracting flavours from plant material by allowing the material to remain suspended within water. This process is used in the production of teas, herbal teas and can be used to prepare coffee (when using a coffee press).		The name is derived from the word "percolate" which means to cause (a solvent) to pass through a permeable substance especially for extracting a soluble constituent.[18] In the case of coffee-brewing the solvent is water, the permeable substance is the coffee grounds, and the soluble constituents are the chemical compounds that give coffee its color, taste, aroma, and stimulating properties.		Carbonation is the process of dissolving carbon dioxide into a liquid, such as water.		Fermentation is a metabolic process that converts sugar to ethanol. Fermentation has been used by humans for the production of drinks since the Neolithic age. In winemaking, grape juice is combined with yeast in an anaerobic environment to allow the fermentation.[19] The amount of sugar in the wine and the length of time given for fermentation determine the alcohol level and the sweetness of the wine.[20]		When brewing beer, there are four primary ingredients – water, grain, yeast and hops. The grain is encouraged to germinate by soaking and drying in heat, a process known as malting. It is then milled before soaking again to create the sugars needed for fermentation. This process is known as mashing. Hops are added for flavouring, then the yeast is added to the mixture (now called wort) to start the fermentation process.[21]		Distillation is a method of separating mixtures based on differences in volatility of components in a boiling liquid mixture. It is one of the methods used in the purification of water. It is also a method of producing spirits from milder alcoholic drinks.		An alcoholic mixed drink that contains two or more ingredients is referred to as a cocktail. Cocktails were originally a mixture of spirits, sugar, water, and bitters.[22] The term is now often used for almost any mixed drink that contains alcohol, including mixers, mixed shots, etc.[23] A cocktail today usually contains one or more kinds of spirit and one or more mixers, such as soda or fruit juice. Additional ingredients may be sugar, honey, milk, cream, and various herbs.[24]		A non-alcoholic drink is one that contains little or no alcohol. This category includes low-alcohol beer, non-alcoholic wine, and apple cider if they contain less than 0.5% alcohol by volume. The term "soft drink" specifies the absence of alcohol in contrast to "hard drink" and "drink". The term "drink" is theoretically neutral, but often is used in a way that suggests alcoholic content. Drinks such as soda pop, sparkling water, iced tea, lemonade, root beer, fruit punch, milk, hot chocolate, tea, coffee, milkshakes, and tap water and energy drinks are all soft drinks.		Water is the world's most consumed drink[25] however, 97% of water on Earth is non-drinkable salt water.[26] Fresh water is found in rivers, lakes, wetlands, groundwater, and frozen glaciers.[27] Less than 1% of the Earth's fresh water supplies are accessible through surface water and underground sources which are cost effective to retrieve.[28]		In western cultures, water is often drunk cold. In the Chinese culture, it is typically drunk hot.[29]		Regarded as one of the "original" drinks,[30] milk is the primary source of nutrition for babies. In many cultures of the world, especially the Western world, humans continue to consume dairy milk beyond infancy, using the milk of other animals (especially cattle, goats and sheep) as a drink. Plant milk, a general term for any milk-like product that is derived from a plant source, also has a long history of consumption in various countries and cultures. The most popular varieties internationally are soy milk, almond milk, rice milk and coconut milk.		Tea, the second most consumed drink in the world, is produced from infusing dried leaves of the camellia sinensis shrub, in boiling water.[31] There are many ways in which tea is prepared for consumption: lemon or milk and sugar are among the most common additives worldwide. Other additions include butter and salt in Bhutan, Nepal, and Tibet; bubble tea in Taiwan; fresh ginger in Indonesia, Malaysia and Singapore; mint in North Africa and Senegal; cardamom in Central Asia; rum to make Jagertee in Central Europe; and coffee to make yuanyang in Hong Kong. Tea is also served differently from country to country: in China and Japan tiny cups are used to serve tea; in Thailand and the United States tea is often served cold (as "iced tea") or with a lot of sweetener; Indians boil tea with milk and a blend of spices as masala chai; tea is brewed with a samovar in Iran, Kashmir, Russia and Turkey; and in the Australian Outback it is traditionally brewed in a billycan.[32] Tea leaves can be processed in different ways resulting in a drink which appears and tastes different. Chinese yellow and green tea are steamed, roasted and dried; Oolong tea is semi-fermented and appears green-black and black teas are fully fermented.[33]		Around the world, people refer to other herbal infusions as "teas"; it is also argued that these were popular long before the Camellia sinensis shrub was used for tea making.[34] Leaves, flowers, roots or bark can be used to make a herbal infusion and can be bought fresh, dried or powdered.[35]		Coffee is a brewed drink prepared from the roasted seeds of several species of an evergreen shrub of the genus Coffea. The two most common sources of coffee beans are the highly regarded Coffea arabica, and the "robusta" form of the hardier Coffea canephora. Coffee plants are cultivated in more than 70 countries Once ripe, coffee "berries" are picked, processed, and dried to yield the seeds inside. The seeds are then roasted to varying degrees, depending on the desired flavor, before being ground and brewed to create coffee.		Coffee is slightly acidic (pH 5.0–5.1[36]) and can have a stimulating effect on humans because of its caffeine content. It is one of the most popular drinks in the world.[37] It can be prepared and presented in a variety of ways. The effect of coffee on human health has been a subject of many studies; however, results have varied in terms of coffee's relative benefit.[38]		Coffee cultivation first took place in southern Arabia;[39] the earliest credible evidence of coffee-drinking appears in the middle of the 15th century in the Sufi shrines of Yemen.[39]		Carbonated drinks refer to drinks which have carbon dioxide dissolved into them. This can happen naturally through fermenting and in natural water spas or artificially by the dissolution of carbon dioxide under pressure. The first commercially available artificially carbonated drink is believed to have been produced by Thomas Henry in the late 1770s.[40] Cola, orange, various roots, ginger, and lemon/lime are commonly used to create non-alcoholic carbonated drinks; sugars and preservatives may be added later.[41]		The most consumed carbonated soft drinks are produced by three major global brands: Coca-Cola, PepsiCo and the Dr Pepper Snapple Group.[42]		Fruit juice is a natural product that contains few or no additives. Citrus products such as orange juice and tangerine juice are familiar breakfast drinks, while grapefruit juice, pineapple, apple, grape, lime, and lemon juice are also common. Coconut water is a highly nutritious and refreshing juice. Many kinds of berries are crushed; their juices are mixed with water and sometimes sweetened. Raspberry, blackberry and currants are popular juices drinks but the percentage of water also determines their nutritive value. Grape juice allowed to ferment produces wine.		Fruits are highly perishable so the ability to extract juices and store them was of significant value. Some fruits are highly acidic and mixing them with water and sugars or honey was often necessary to make them palatable. Early storage of fruit juices was labor-intensive, requiring the crushing of the fruits and the mixing of the resulting pure juices with sugars before bottling.		Vegetable juices are usually served warm or cold. Different types of vegetables can be used to make vegetable juice such as carrots, tomatoes, cucumbers, celery and many more. Some vegetable juices are mixed with some fruit juice to make the vegetable juice taste better. Many popular vegetable juices, particularly ones with high tomato content, are high in sodium, and therefore consumption of them for health must be carefully considered. Some vegetable juices provide the same health benefits as whole vegetables in terms of reducing risks of cardiovascular disease and cancer.		A drink is considered "alcoholic" if it contains ethanol, commonly known as alcohol (although in chemistry the definition of "alcohol" includes many other compounds). Beer has been a part of human culture for 8,000 years.[48]		In many countries, imbibing alcoholic drinks in a local bar or pub is a cultural tradition.[49]		Beer is an alcoholic drink produced by the saccharification of starch and fermentation of the resulting sugar. The starch and saccharification enzymes are often derived from malted cereal grains, most commonly malted barley and malted wheat.[50] Most beer is also flavoured with hops, which add bitterness and act as a natural preservative, though other flavourings such as herbs or fruit may occasionally be included. The preparation of beer is called brewing. Beer is the world's most widely consumed alcoholic drink,[51] and is the third-most popular drink overall, after water and tea.[52] It is thought by some to be the oldest fermented drink.[53][54][55][56]		Some of humanity's earliest known writings refer to the production and distribution of beer: the Code of Hammurabi included laws regulating beer and beer parlours,[57] and "The Hymn to Ninkasi", a prayer to the Mesopotamian goddess of beer, served as both a prayer and as a method of remembering the recipe for beer in a culture with few literate people.[58][59] Today, the brewing industry is a global business, consisting of several dominant multinational companies and many thousands of smaller producers ranging from brewpubs to regional breweries.		Cider is a fermented alcoholic drink made from fruit juice, most commonly and traditionally apple juice, but also the juice of peaches, pears ("Perry" cider) or other fruit. Cider may be made from any variety of apple, but certain cultivars grown solely for use in cider are known as cider apples.[60] The United Kingdom has the highest per capita consumption of cider, as well as the largest cider-producing companies in the world,[61] As of 2006[update], the U.K. produces 600 million litres of cider each year (130 million imperial gallons).[62]		Wine is an alcoholic drink made from fermented grapes or other fruits. The natural chemical balance of grapes lets them ferment without the addition of sugars, acids, enzymes, water, or other nutrients.[63] Yeast consumes the sugars in the grapes and converts them into alcohol and carbon dioxide. Different varieties of grapes and strains of yeasts produce different styles of wine. The well-known variations result from the very complex interactions between the biochemical development of the fruit, reactions involved in fermentation, terroir and subsequent appellation, along with human intervention in the overall process. The final product may contain tens of thousands of chemical compounds in amounts varying from a few percent to a few parts per billion.		Wines made from produce besides grapes are usually named after the product from which they are produced (for example, rice wine, pomegranate wine, apple wine and elderberry wine) and are generically called fruit wine. The term "wine" can also refer to starch-fermented or fortified drinks having higher alcohol content, such as barley wine, huangjiu, or sake.		Wine has a rich history dating back thousands of years, with the earliest production so far discovered having occurred c. 6000 BC in Georgia.[4][64][65] It had reached the Balkans by c. 4500 BC and was consumed and celebrated in ancient Greece and Rome.		From its earliest appearance in written records, wine has also played an important role in religion. Red wine was closely associated with blood by the ancient Egyptians, who, according to Plutarch, avoided its free consumption as late as the 7th-century BC Saite dynasty, "thinking it to be the blood of those who had once battled against the gods".[66] The Greek cult and mysteries of Dionysus, carried on by the Romans in their Bacchanalia, were the origins of western theater. Judaism incorporates it in the Kiddush and Christianity in its Eucharist, while alcohol consumption was forbidden in Islam.		The term "spirit" refers to a distilled drink that contains no added sugar and has at least 20% alcohol by volume (ABV). Popular spirits include borovička, brandy, gin, rum, slivovitz, tequila, vodka, and whisky. Brandy is a spirit created by distilling wine, whilst vodka may be distilled from any starch- or sugar-rich plant matter; most vodka today is produced from grains such as sorghum, corn, rye or wheat.		Throughout history, people have come together in establishments to socialise whilst drinking. This includes cafés and coffeehouses, focus on providing hot drinks as well as light snacks. Many coffee houses in the Middle East, and in West Asian immigrant districts in the Western world, offer shisha (nargile in Turkish and Greek), flavored tobacco smoked through a hookah. Espresso bars, such as Starbucks and Costa Coffee are a type of coffeehouse that specialize in serving espresso and espresso-based drinks.		In China and Japan, the establishment would be a tea house, were people would socialise whilst drinking tea. Chinese scholars have used the teahouse for places of sharing ideas.		Alcoholic drinks are served in drinking establishments, which have different cultural connotations. For example, pubs are fundamental to the culture of Britain,[67][68] Ireland,[69] Australia,[70] Atlantic Canada, New England, Metro Detroit, South Africa and New Zealand. In many places, especially in villages, a pub can be the focal point of the community. The writings of Samuel Pepys describe the pub as the heart of England. Many pubs are controlled by breweries, so cask ale or keg beer may be a better value than wines and spirits.		In contrast, types of bars range from seedy bars or nightclubs, sometimes termed "dive bars",[71] to elegant places of entertainment for the elite. Bars provide stools or chairs that are placed at tables or counters for their patrons. The term "bar" is derived from the specialized counter on which drinks are served. Some bars have entertainment on a stage, such as a live band, comedians, go-go dancers, or strippers. Patrons may sit or stand at the bar and be served by the bartender, or they may sit at tables and be served by cocktail servers.		Food and drink are often paired together to enhance the taste experience. This primarily happens with wine and a culture has grown up around the process. Weight, flavors and textures can either be contrasted or complemented.[72] In recent years, food magazines began to suggest particular wines with recipes and restaurants would offer multi-course dinners matched with a specific wine for each course.[73]		Different drinks have unique receptacles for their consumption. This is sometimes purely for presentations purposes, such as for cocktails. In other situations, the drinkware has practical application, such as coffee cups which are designed for insulation or brandy snifters which are designed to encourage evaporation but trap the aroma within the glass.		Many glasses include a stem, which allows the drinker to hold the glass without affecting the temperature of the drink. In champagne glasses, the bowl is designed to retain champagne's signature carbonation, by reducing the surface area at the opening of the bowl. Historically, champagne has been served in a champagne coupe, the shape of which allowed carbonation to dissipate even more rapidly than from a standard wine glass.		An important export commodity, coffee was the top agricultural export for twelve countries in 2004,[74] and it was the world's seventh-largest legal agricultural export by value in 2005.[75] Green (unroasted) coffee is one of the most traded agricultural commodities in the world.[76]		Some drinks, such as wine, can be used as an alternative investment.[77] This can be achieved by either purchasing and reselling individual bottles or cases of particular wines, or purchasing shares in an investment wine fund that pools investors' capital.[78]		
