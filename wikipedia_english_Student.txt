In Australia, technical and further education or TAFE (/ˈteɪf/) institutions provide a wide range of predominantly vocational tertiary education courses, mostly qualifying courses under the National Training System/Australian Qualifications Framework/Australian Quality Training Framework. Fields covered include business, finance, hospitality, tourism, construction, engineering, visual arts, information technology and community work.		Individual TAFE institutions (usually with numerous campuses) are known as either colleges or institutes, depending on the state or territory. TAFE colleges are owned, operated and financed by the various state and territory governments. This is in contrast to the university sector, whose funding is predominantly the domain of the federal government and whose universities are predominantly owned by the state governments.						In NZ the equivalent of a TAFE is a Polytech.		T.A.F.E colleges award Australian Qualifications Framework (AQF) qualifications accredited in the Vocational Education and Training (VET) sector that align to Certificate I, Certificate II, Certificate III, Certificate IV, Diploma, Advanced Diploma, Graduate Certificate and Graduate Diploma qualifications.[1] In many instances TAFE study can be used as partial credit towards bachelor's degree-level university programs.		From 2002 the TAFE education sector has been able to offer bachelor's degrees and post-graduate diploma courses to fill niche areas, particularly vocationally focused areas of study based on industry needs. As at June 2009 10 TAFE colleges (mainly in New South Wales,[2] Victoria, but also Western Australia, ACT, and Queensland) now confer their own degree-level awards and post graduate diplomas, though initially not beyond the level of bachelor's degree. However Melbourne Polytechnic has been accredited in 2015 to offer two master's degree courses.[3] Similarly, some universities, e.g. Charles Darwin University and Royal Melbourne Institute of Technology, offer vocational education courses (traditionally the domain of TAFE); these are funded by the local state and territory governments. Some high schools also deliver courses developed and accredited by TAFEs.[4][5]		Students who enrol in these undergraduate degree courses at TAFE are required to pay full fees and are not entitled to Commonwealth Government supported student fee loans, known as HECS loans, but may access a FEE-HELP loan scheme.[6] While Universities have the ability and power to design and offer their own degree courses, each TAFE degree course must be assessed and approved by the Higher Education Accreditation Committee (HEAC).[4]		TAFEs in some states can also teach senior high school qualifications, like the VCE, Victorian Certificate of Applied Learning, and the Higher School Certificate. Some universities, e.g. Charles Darwin University and Royal Melbourne Institute of Technology, offer TAFE courses; these are funded by the local state and territory governments. Some high schools also deliver courses developed and accredited by TAFEs.		Some private institutions also offer courses from TAFEs, however they more commonly offer other vocational education and training courses. Many Australians refer to all sub-degree courses as "TAFE" courses, no matter what institution creates or delivers the course. Before the 1990s, the TAFEs had a near monopoly in the sector. TAFE courses provide students an opportunity for certificate, diploma, and advanced diploma qualifications in a wide range of areas.		In most cases, TAFE campuses are grouped into TAFE institutions along geographic lines. Most TAFEs are given a locally recognised region of the country where they exclusively operate covering a wide range of subjects.		A few TAFEs specialise in a single area of study. These are usually found near the middle of the capital cities, and service the whole state or territory. For example, the Trade and Technician Skills Institute in Brisbane, (from 1 July 2006), specialises in automotive, building and construction, manufacturing and engineering, and electrical/electronic studies for students throughout Queensland. Or the William Angliss Institute of TAFE in Melbourne which specialises in food, hospitality and tourism courses for Victoria.		In the Australian Capital Territory these include:		There are ten TAFE NSW Institutes in NSW which include:		In the Northern Territory these include:		In Queensland, TAFE Queensland includes:		As of May 2014, the TAFE institutes have amalgamated into six regions of the central TAFE Queensland (parent body). The regions of TAFE Queensland are:[7]		In South Australia:		In Tasmania, there are two government TAFE organisations:		In Victoria these include:		In Western Australia, this includes:		
College education, informally referred to as College and often informally by the umbrella acronym CEGEP in the Canadian province of Quebec is the post-secondary level immediately after high-school but required for university admissions. The Quebec education system is unique in North America.		The college level is a separate and distinct step in Quebec. For students graduating from high school in Quebec, a college diploma is required for admission into university. In the rest of Canada, colleges have historically been technical schools that offer specialized professional or vocational education in specific employment fields. In the USA, the term "college" is synonymous with university.						Quebec high school starts at grade 7 and ends at grade 11, one year earlier than in English North America. Most Quebec university programs are three years in length. With a collegiate level between high school and university bridging the gap:[1]		Two main college paths are possible:		Pre-University programs of two years, leading to a college diploma required for university admissions.		Specialized vocational programs of either three years—leading to a college diploma and entry to the job market, with a possibility of university admissions—or one year, leading to a college certificate and direct entry into the workforce.		There are three types of colleges in Quebec: public colleges, private colleges, and government colleges.		The majority of college students attend a public General and Vocational College, also known as the French acronym "cegep" (French: Collège d'Enseignement General et Professionel). These colleges do not charge tuition to Quebec residents, although small administrative fees are charged.		A large number of private colleges also exist at the collegiate level. Some of these schools receive funding from the government, others do not, and therefore tuition can vary greatly between schools.		There are a small number of collegiate-level government institutions that are not private colleges, yet also not public colleges, as defined under Quebec's General and Vocational College law. One example is the Quebec Music Conservatory.		
Freshman is a term commonly used to refer to a person that is in their first year at an educational institution, usually a secondary or post-secondary school. In the US, freshman usually denotes a first year student, whereas in Canada, the word 'frosh' describes a first year student.						In the Gulf Council countries (GCC), a freshman is called Mustajid (مُستجد), which means one who is new to something. Informally a freshman is called a Sanfoor (سنفور), which is the Arabic word for the Smurf.		The term 'freshman' is not commonly used in Australia or New Zealand. The term first year is used within Australia and New Zealand universities primarily to describe students in their first year of tertiary education direct from secondary school, the exception being the University of Otago where the term 'fresher' is employed. In Australia, Year 7 (8 in some South Australian schools) is the first year of high school education; in New Zealand, Year Nine is the first year of Secondary Education—in contrast to North America, where the ninth grade or "freshman year" is the first year. In New Zealand, year nine students are sometimes referred to as "Turds" as a derogatory reference to the old form system, as year nine was previously known as third form.		The exception to this general norm is the use of the terms 'Fresher', 'Freshman', or simply 'Fresh' to describe first year residents at the University of Sydney residential colleges.[citation needed]		The first year students who go to university starting their bachelor are called "generatiestudenten" (in Dutch). This is the official way of referring to first-year students. In Dutch-Belgium (officially known as Flanders) students choose to become a member in a 'studentenclub'. In their first year of membership the male students are called 'schachten' and the female students are called 'porren'. They are inferior to other members, and often tasked with all the dirty jobs. This is part of becoming a 'full' member. They keep this title during the entire academic year, until the 'ontgroening', when they become a full member of the club. The Belgian tradition in the north of the country has heavily influenced the French practices, mainly because the largest French university (UCL) was relocalised from the Belgian city of Leuven only in 1968. Of course, the French tradition differs on various points. The correct terminology for first year students aspiring to become a member of a club in French-Belgium is 'bleus'. Some universities may have other names according to their own traditions.		In Brazil, students that pass the vestibulares and begin studying in a college or university are called "calouros" or more informally "bixos" ("bixetes" for girls), an alternate spelling of "bicho", which means "animal". Calouros are often subject to hazing, which is known as "trote" (lit. "prank") there. The first known hazing episode in Brazil happened 1831 at the Law School of Olinda and resulted in the death of a student.[1] In 1999, a Chinese Brazilian calouro of the University of São Paulo Medicine School named Edison Tsung Chi Hsueh was found dead at the institution's swimming pool; this has since become one of the most well known episodes of violent hazing and has received extensive national media coverage since that year.[1][2][3][4]		In Chile, during the first year of University, the student is called "mechón". During the firsts weeks of classes, 2nd year students perform "mechoneo" on 1st students. This "mechoneo" usually consist of several activities that are generally gruesome, like having to kiss a pig head and collect money in the street to get the freshmen belongings back. This tradition is every year less common, as universities and student federations started to prohibit it after several incidents. In 2017 only a few faculties of the University of Chile allowed this activity.		In Colombia, during the first year of University, the student is called "Primíparo".[citation needed]		At Danish universities, first year students are called "russer" (singular "rus"). The term is believed to be an abbreviation of the Latin "depositurus", which means "the man, who is about to give way". The term can be used both affectionately or derogatorily. The first year students are introduced to their respective institutes and subjects through activities held by older students. These activities are called "rusture" (singular "rustur") and the older students are called "rusvejledere" (lit. first year student tutor). These activities usually last a few days, up to a week in some cases, usually just days before the first lecture. While the term "rus" nor "rustur" have official status, the term "rusvejleder" usually does, as some faculties educate them for next year's new students.		At some Danish secondary education institutions (gymnasium), first year pupils are called "putter" (singular "put" or "putte"). This is not an official title, but it's a tradition among pupils in gymnasium to call the first year pupils this. It is in some cases used affectionately or derogatorily. The first year pupils, however, often identify themselves as "putter". First year of gymnasium is also sometimes called "putår" (lit. put year).		The term 'freshman' is not used in the UK, although first-year university students are referred to as "freshers" particularly in the first few weeks of their first term. The term first year is commonly still used in the pre-University and college English education system, either as the first year in primary and secondary education. In England and Wales a student's school career (not including pre-school nursery education) now begins with Reception, usually at the age of four, and continues up to either Year 11 or Year 13 depending on whether the student is going on to further education. Before the introduction of the "Year [number]" in most secondary schools in September 1990, the first year or first form almost always referred to the first year of secondary education. Years 12 and 13 are known as Sixth Form or "lower sixth" and "upper sixth" respectively.		In Estonia the freshmen are called rebased (sing. rebane; "foxes") in both high schools and universities. The term derives from the Baltic German and more distantly from German student corps of the 19th century, where young members are also still called "foxes". In universities, the term esmakursuslane (pl. esmakursuslased; "first-course") is also used.		In French most public universities, freshmen are called "premières années" (meaning first year) There is no direct equivalent in French for the terms 'freshman', 'sophomore', 'junior' or 'senior'. In French some public universities, private schools (business schools, engineer school...), freshmen are called Bizuth. Until the 90's, at the beginning of the year, in most schools, the first week or month was called "bizutage". Older students (especially 2nd year students) were bullying all Bizuths. In French high schools, the first year is called "seconde" and is the equivalent of the sophomore year. The French equivalent of the freshman year is "sixième", in middle school. read also - education in France -		At German high schools (Gymnasium), students in the initial fifth form (formerly "Sexta" in the old Latin numbering system, which counted backwards) were traditionally called "Sextaner" formerly. At German universities, freshers are called "Erstis" (abbreviation for" Erstsemester" (first semester). They are also the target group of fraternities (Studentenverbindungen, Burschenschaften) looking for new members; the minute percentage of students who do join one, are then called "Füchse" (literally "foxes", singular "Fuchs" or "Fux") and have to undergo training and a number of tests (usually fencing or drinking duels, depending on the type of fraternity) before they are formally received as full members.		In India, during the first year of College or University, students are called Freshers & from second year onward, they are called Seniors. Sometimes, to more specifically emphasize third year or fourth year students are called Super seniors.		The first year of university is called freshman year and only those who studied abroad undergo it. Freshman year is a preparatory year and the students are major-less. Students who have finished high school in Lebanon enter the sophomore year where they study their major.		In the Netherlands it is common in the highschool as "brugklasser" and in the major university cities, Utrecht, Amsterdam, Leiden, Groningen and a few others to call first year students "sjaars".		The first year university student is called "Cachimbo".		In the Portuguese Praxe, referring to all student and academic traditions of Portuguese universities, a major component is the hazing of freshmen (known in Portuguese as Caloiros). There are also many music festivals and a great deal of partying.		There is an actual "Praxe Code" that describes the entire set of traditions, including the Freshman's rights. These include the obligation, of the Freshman to be present in every initiation, having to respect and obey the seniors who are initiating him/her. One of the traditions includes forcing the freshmen to sing university songs and paint their faces and nails with several colors and partake in various games. The tradition requires that hazing be moderate and not endanger anyone. It is also tradition to host friendly dinners for the freshman so they can meet fellow students. It is usually the third year students who "guide" the freshmen, and there´s a symbolic ceremony where the freshmen must choose a "Godmother, Godfather, two Godmothers, two Godfather or both a Godmother and a Godfather" (mentor) from the second and third year students to "guide" them throughout their university years. After the mentor is formally chosen, the freshman can no longer be hazed except with the mentor's permission and he can then after a while be baptized on the "Baptism Ceremony". Second and Third year students wear the traditional university outfit most of the time but it is a must to wear it at freshmen ceremonies.		Sophomores are usually not allowed to haze freshmen or to join second or third year students, they are also not permitted to wear the traditional university outfit during their first sophomore semester and until the "Traçar da Capa Ceremony", where their Godfather and/or Godmother "traçam a capa" to them.		Throughout Russian Federation and most of the former Soviet Union territories with high prevalence of Russian-speaking population, the first-year higher education students are referred to (somewhat derogatorily) as "abitura" (абитура), from the official term "abiturient" which denotes a person seeking an admission to the university.		This term, however, doesn't apply to high school pupils of any year, as in Soviet times the school education was not separated into stages as in the West (elementary, middle, high) and pupils of all ages attended the same school (called Secondary school). It is only in the last decade or so the Russian school system (particularly private schools) started to adopt Western structure, and along with it - some of the westernized (anglicized) slang and terminology.		In Scotland, the first year of compulsory education is Primary 1 (P1). The first year of secondary school is known as S1 but one can freely use first year.		At the four ancient Scottish universities the traditional names for the four years at university are Bejan ("Bejant" at the University of St Andrews)(1st),[5] Semi (2nd), Tertian (3rd) and Magistrand (4th), though all Scottish universities will have a "freshers' week" and the term is as widely used with more traditional terms.		Freshman is commonly in use as a US English idiomatic term to describe a beginner or novice, someone who is naive, a first effort, instance, or a student in the first year of study (generally referring to high school or university study).[6]		New members of Congress in their first term are referred to as freshmen senators or freshmen congressmen or congresswoman, no matter how experienced they were in previous government positions.		High School first year students are almost exclusively referred to as Freshmen, or in some cases by their grade year, 9th graders. Second year students are Sophomores, or 10th graders, then Juniors or 11th graders, and finally Seniors or 12th graders.		At college or university, Freshman denotes students in their first year of study. The grade designations of high school are not used, but the terms Sophomores, Juniors, and Seniors are kept at most schools. Some colleges do not use the term Freshman, but use the perceived gender neutral term: First Year, instead.[7][8] Beyond the fourth year, students are simply classified as fifth years, sixth years, etc. Some institutions use the term freshman for specific reporting purposes.[9]		
Student orientation or new student orientation (often encapsulated into an Orientation week, Frosh Week, Welcome Week[1] or Freshers' Week) is a period of time at the beginning of the academic year at a university or other tertiary institution during which a variety of events are held to orient and welcome new students. The name of the period varies by country.		Although usually described as a week, the length of this period varies widely from university to university and country to country, ranging from about three days to a month or even more (e.g. four or five weeks, depending on program, at Chalmers). The length of the week is often affected by each university's tradition as well as financial and physical constraints. During this period, students participate in a wide range of social activities, including live music and other performances, sports challenges, stunts, and open-air markets.						The week before the term starts is known as: Frosh (or frosh week) in some[citation needed] colleges and universities in Canada. In the US, most call it by the acronym SOAR for Student Orientation And Registration;[2] Freshers' week in the majority of the United Kingdom and Ireland and Orientation week or O-week in countries such as Australia, South Africa and New Zealand, and also in many Canadian universities. In Sweden, it is known as nollning (from nolla, "zero", in this case meaning the students have not earned any credit points yet) or inspark (being "kicked in" to university life). Orientation week is the coming phrase[clarification needed] in the United States. Some schools use the acronym WOW for Week of Welcome.		In Canada, first year students are called "Frosh" or "first years". The terms "freshies" and "freshers" are also emerging. In the United States, first year university students are typically[citation needed] referred to as freshmen. In Australia and New Zealand first year students are known simply as "first years", although in some the colleges of the University of Melbourne and the University of Sydney they are also called "Freshers". In the U.K. and Ireland first year students are known as freshers or first years. Freshies is also an emerging term in New Zealand. In Sweden, the student is a nolla (a "zero") during the orientation period and usually upgraded to the status of an etta (student who is in her/his first college term) at a ceremony involving a fancy three-course dinner and a lots of singing.		In Australia, some universities require students to arrive at university a week before classes start in order to gain course approval. This also allows students a chance to orient themselves to student life without the pressure of lectures—hence the term Orientation week is used to describe this week of induction into university life.		In Australian universities, such as the University of Melbourne, University of New South Wales and University of Sydney, the last or second last night is usually celebrated with a large-scale event such as a famous band playing at an entertainment venue on campus. This is generally followed by continued partying and drinking, especially among students living in residential colleges such as Janet Clarke Hall and Ormond College.		The Adelaide University O-Week[3] runs from Monday to Thursday in the week before lectures begin. During O-Week sporting clubs and societies set up a variety of tented areas where clubs display their activities. The Adelaide University Union coordinates a variety of events centering around beer, bands and barbecues on the lawns near the Union complex. A major event for the week is the O-Ball (live entertainment and licensed areas) which takes place in the Cloisters (Union House). The O-Ball attracts many thousands of revellers, not all of whom are Adelaide University students. In recent times Sports and Clubs have sought to distance themselves from the student union and student association controlled activities and have set themselves up on the Maths lawns.		The Australian National University has a full week (Sunday to Sunday)[4] of events, parties and social activities open to all students of the university, organised by the Australian National University Students Association. The residential colleges often have their own "O-week" activities catered primarily for residents as well as the annual "Burgmann Toga Party" held at Burgmann College open to students from all residential colleges. "Burgmann Toga" is the largest party held at a university residence in the Southern Hemisphere.		In Canada, the nature and length of orientation week varies considerably between Universities. For instance, Ottawa, has two universities within its urban centre; the University of Ottawa and Carleton University, both with orientations spanning approximately 7 days. At The University of Ottawa, Frosh Week is Called 101 week. At Carleton University there are multiple orientations, SPROSH (Sprott Frosh), ENG Frosh, Radical Frosh, and the largest, CUSA/RRRA/SEO Frosh. In the province of Quebec, because of the CEGEP system, "froshies" are of legal drinking age and Frosh activities may include the option to drink alcohol. Moreover, the proximity of the two Ottawa universities also allows them to take advantage of the drinking age in neighbouring Gatineau, Quebec. The University of British Columbia cancels the first day of class for all students, and hosts an orientation day for new students, called Imagine Day. As of 2007, the Faculty of Science also holds an annual, day-long Science Frosh event for approximately 300 first-year students, while the commerce faculty holds a 3-day-long frosh weekend before classes begin. The University of Toronto has a number of different "Frosh Weeks" organized concurrently by different student groups within the university; including college societies, professional faculties (perhaps the best known being organized by Engineering Society, Skule (engineering society), in which 'F!ROSH' and 'F!ROSH Leedurs' dye their bodies purple) and the University of Toronto Students' Union. Similarly, Ryerson University also has a number of "Frosh Weeks" organized by different student groups, although it also has a central frosh team known as the 'Ryerson Orientation Crew'. At the Friday of frosh week, the Ryerson Students' Union holds a concert that is free for all Ryerson students; the headliners for the 2015 concert included Drake and Future. McMaster University also organizes many events during what they term "Welcome Week". The week strongly encourages solidarity, first with members of one's own residence or for off-campus students, and later the members of a student's faculty. University of Guelph holds many orientation activities for its incoming students. The main event is the pep rally in which students from each residence perform a dance on the football field. The Guelph Engineering Society also hosts a series of special events for Engineering Frosh including frosh olympics, beach day, and a scavenger hunt. Western University hosts the largest orientation program in Canada, involving 1200 student volunteers and an entire week of activities. St Thomas University, in Fredericton, New Brunswick, hosts a week-long event including activities for each residence and activities for new students. As a rule, Frosh week at Queen's University is so secretive and confidential that no one knows what happens during the week long adventure except Queen's Students and Alumni. Wilfred Laurier University has by definition a Lit[disambiguation needed] orientation week.		In Finnish universities, the student organizations for each department independently organize orientation activities for the new students in their respective departments. New students are often assigned in groups to an upperclassman tutor and participate in many activities with their tutoring group. New students may be referred to as piltti (child), fuksi (freshman), fetus or other names according to their major subject. Activities for new students may include "orienteering", pub crawls, sporting events, swimming in fountains or other forms of "baptism", sitsit parties and saunas, often done wearing homemade fancy-dress costumes. It is also considered important for the new students to participate in the regular activities of the student department organizations.		In past years a typical orientation may consist of verbal harassment as well as initiation leading to humiliation. An orientation of freshers in Indonesia is usually called OSPEK for some universities and MOS in middle and high school. Orientations in Indonesia has event organizers that consists of seniors and the presidium of universities. The most basic form of orientation in Indonesia consist of an educational board run and introduction of campus cultural behavior.[5] What makes orientation in Indonesia (for some universities and schools) distinctive to other countries would arguably be the freshmens' requirement to wear unusual accessories or hairstyles (i.e. Freshmens were asked to wear hats made of bird's nest, necktie made of folded paper, military hairstyle for male students or intricate braids for females, and the usage of a sack instead of a rucksack). Harsh physical punishments were not uncommon during the Suharto era, and mass media continues to report inhumane activities during those orientation that led to a few cases of death.		Nowadays, however, orientation is more tolerable as physical abuse is now forbidden by the law, however it is still criticized by many psychologists and people as 'too much' because of excessive verbal harassment like dissing and insulting the juniors, and the usage of unusual and humiliating attributes typically found in orientations on Junior High and High Schools. As well, it is also criticized by many parents for being economically inconvenient. The reason cited by psychologists is that orientation is often used as a tool of revenge done by the board of organizers for what the seniors did to them during their freshman year.[6] And because of this there are so many people who believes that "MOS" or "OSPEK" is a useless traditions that needs to be erased.[7][8] The 'cruelty' of MOS and OSPEK varies between universities and schools in Indonesia,[9] although in (most) major universities and institutes that kind of humiliation and harassment doesn't exist anymore, or greatly limited to pending applicants or pledges for certain campus organizations.[10]		As in Australia, in New Zealand students have a week to orient themselves to university life before the start of formal classes. This orientation week is a time for many social events, and is often a reason for alcohol fests.[11] Flat warmings are often held within the time limit to couple the alcohol oriented event with the general party week.		In New Zealand's main university towns such as Dunedin and Palmerston North (where students make up around one fifth of the population) orientation week leads a wide range of events. Many top overseas and local bands tour the country at this time, and the orientation tour is one of the highlights of the year's music calendar. The University of Otago in the Scottish-settled city of Dunedin traditionally holds a parody of the Highland Games called the Lowland Games, including such esoteric events as porridge wrestling.		Student pranks were once common during orientation week, but have fallen out of favour in recent years.[citation needed] Until recent years, many halls of residence also inducted new residents with "Initiation" (a form of hazing, though considerably milder than the rituals found among American college fraternities).		Although officially designated as a week, in several New Zealand universities and polytechnics orientation week stretches to over ten days.		Most Swedish universities have some kind of nollning ("zeroing") or "inspark" ("kicking-in"). This is most extensive at the technical faculties and at the student nation communities of Uppsala and Lund. Since student union membership was mandatory in Sweden (until July 2010), the nollning is usually centrally organized from the student union with support from the universities.		At the old universities, these traditions have often turned civilized after a dark history of hazing. Today, many student unions have strict rules against inappropriate drunkenness, sexual harassment and other problematic behaviour.		At the technical faculties, the people who organize the nollning play roles in a theatrical manner and often wear sunglasses and some form of weird clothes. Most senior students who are mentors during the nollning wear their student boilersuits or the b-frack (a worn tailcoat). This kind of organized nollning developed at KTH and Chalmers and spread to the rest of the country.		In Thailand, the activity is commonly called rapnong (รับน้อง), translated as "welcoming of freshmen". It takes place in the first week or month of the academic year at universities and some high schools. The purpose is to adapt new students to university culture. Activities include games, entertainment and recreation. These let the newcomers get to know other members of the university and reduce tension in the changing environment. It sometimes includes alcohol. The main object is to let juniors carry on the universities' tradition and identity and to bind together the new generation into one. Long-term activity often includes seniors taking freshman or older years to meals and meetings, usually the most senior pays for it all. Hazing is a concern in this activity, as many students have been humiliated, abused, and dehumanized by their upperclassmen.		For over 50 years, SOTUS – a Hazing based system used for college initiation in Thailand – has been involved in Thai universities. It stands for Seniority, Order, Tradition, Unity, and Spirit.[12] It is the system for freshmen to bring harmony to their friends and to show their pride through their institute. By seniors, freshmen have to do activities such as singing university songs. Moreover, freshmen are required to do a lot of things; for example, wear a nametag, and show respect to seniors. These requirements lead seniors to try to make their juniors do what they desire and punish them if they don't do seniors' orders.		Presently, there are adolescents and adults opposing those who had committed unethical or deadly actions to juniors. This group of adolescents has distributed "Anti-SOTUS"[13] group and it becomes one of the main issues in Thailand recently. They consider the SOTUS system to be "old-fashioned and source of brutality". Since it was established, this has become the group of people who share their opinions about SOTUS system based on how they have encountered it.		On the other hand, some seniors that support this system resisting the anti SOTUS attitude for many years. They tend to say that SOTUS makes them get along together and feel proud of themselves by becoming part of their institute. Some seniors, however, coerce their freshmen to attend every activity held by them as parts of preparing them to be able to live happily in university. These become worse when some freshmen suffer from what their senior have done to them.		In Thai society, news related to this system has been reported almost every year. For example, recent news about a male freshman[14] who died in this tradition. This news has resulted in people thinking that rapnong should end or, at least, be controlled.		As well as providing a chance to learn about the university, Freshers' week allows students to become familiar with the representatives of their Student Union and to get to know the city or town which is home to the university, often through some form of pub crawl (the legal drinking age is 18 in the UK).		Live music is also common, as are a number of organized social gatherings especially designed to allow freshers to make new friends and to get to know their course colleagues. Because of the intensity of activities, there are often many new friendships made, especially in group accommodation, some not lasting past Freshers' Week and others lasting for the whole University career and longer.		Typically a Freshers' Fair for student clubs and societies is included as part of the activities to introduce new students to facilities on offer, typically outside their course of study, such as societies, clubs and sports. The various societies and clubs available within the University have stalls and aim to entice freshers to join. Most campuses take the opportunity to promote safe sex to their students and sometimes offer leaflets on the subject and free condoms, as well as promoting the Drinksafe campaign. The aim is to lower the rate of sexually transmitted disease and to reduce the level of intoxication commonly witnessed in Freshers' Week.		Freshers' Flu is a predominately British term which describes the increased rates of illness during the first few weeks of university. Although called Freshers' Flu, it is often not a flu at all.		"Freshmen" is the traditional term for first-year students arriving at school in the United States, but the slang term 'frosh'[15] is also used. Due to the perceived gender exclusiveness of the term, some institutions including the University of North Carolina have adopted "first-year student" as the preferred nomenclature.[16] Lasting between a few days and a week, the orientation is these students' informal introduction and inauguration to the institution. Typically, the first-year students are led by fellow students from upper years over the course of the week through various events ranging from campus tours, games, competitions, and field trips. At smaller liberal arts colleges, the faculty may also play a central role in orientation.		In many colleges, incoming freshmen are made to perform activities such as singing of songs, engaging in group physical activities, and playing games. These activities are often done to help freshmen make friends at their new establishment, and also to bond with each other and the upperclassmen.		Despite the fact that most first-year students are below the legal drinking age (currently 21 years in all states), heavy drinking and binge drinking may occur outside the orientation curriculum. Some programs require their organizers to sign waivers stating they will not be under the influence of any substances over the course of the week as they are responsible for the well-being of the students. Most programs have one final party on the final night to finish off the week of celebrating, in which the organizers join in.[citation needed]		Although it has been officially banned at many schools, hazing is not uncommon during the week. This can be anywhere from the organizers treating the first-year students in a playfully discouraging manner to forcing them to endure rigorous trials.		The attitude of the events also depends on the school. Many colleges encourage parents to come to the first day to help new students move into their dormitory, fill out paper work, and get situated.[17] Some schools view their week as an initiation or rite of passage while others view it as a time to build school spirit and pride. In towns with more than one university, there may be a school rivalry that is reflected in the events throughout the week.		At most schools, incoming freshmen arrive at the school for a couple of days during the summer and are put into orientation groups led by an upperclassman trained for the position. Their Orientation Leader will take them around campus, do activities with them, have discussions with them, help them register for the next semester's classes and make them feel comfortable about coming to school in the fall.		Freshmen orientation is usually mandatory for all new students, especially international students which is one way to activate the status of their visa.		After first-year students have completed some time at their university, they may find that they did not make the right choice, miss being close to home, or simply want to attend a different institution. When this occurs, they may transfer to another university, usually after their first year.		Many universities will hold another student orientation similar to freshman orientation for these transfer students. Freshman orientation lasts a few days or a week, on the other hand, transfer student orientation will typically last between one and three days. Transfer orientation's purpose is to acquaint transfer students with their new university. This usually includes campus tours, introducing transfer students to their adviser or perhaps a few of their teachers, and filling out paperwork for proper enrollment. At some colleges, transfer orientation is mandatory for all transfer students.[18]		Unlike freshmen, transfer students are already familiar with the independence of college life. Therefore, their orientation focuses mostly on becoming familiar with the layout and policies of their new institution, providing information about essential campus resources, and getting acquainted with other transfer students so they may make friends at their new university.[19] Transfer students may engage in games, conversations with University faculty, and discussions with current students to make acquaintances and learn more about the university.		At Roskilde University in Denmark, orientation week (in Danish rusvejledning) normally lasts from 1 week and a half to two whole weeks. During the period, approximately 14 teams consisting of 10–16 tutors each takes care of an individual house in which the new students have been allocated. There's normally 1 house of Natural Sciences, 4 of Social Studies and Economics, 4 houses of Arts and Language and 2 of technology and design. Each of the first 3 houses described has an International version as well, where the courses are taught in English instead of Danish.		Each tutor group spends roughly 14 days (and 3–5 days of preeducation in the spring semester) living on campus before the arrival of the new students (also called ruslings). These periods usually involve heavy amounts of drinking, partying and sexual activity among the tutors themselves. However most festive activities including alcohol only occurs until after 4 pm, due to the alcohol policies of the university. Because of this policy, most of the daily activity is spent on planning and preparing activities for the new students.		When the students arrive all tutor groups welcomes the ruslings with the infamous Marbjergmark show- usually a display of wacky sketches such as naked people playing chess, smashing rotten eggs at bystanders or themselves or guys chasing midgets with a butcher's knife (to name a few examples).		During the two-week period the tutor group teach and introduce the new students to life at campus. Both the social and educational aspects. As it is with the preparation period, festive activities take place after 4 PM, and educational activities are held during the day.		The two-week period ends in a four-day period in which the house will leave campus to varied destinations. During these days mostly social activities are held, including the more secret hazing rituals of the university.		The tutors uphold a strict set of rules to maintain a safe and pleasant tutorship to prevent harmful and humiliating hazing rituals. Examples are the presence of minimum two sober tutors at each party (In Danish Ædruvagter). Engaging in sexual relations with new students is also strongly discouraged. Also it is generally not seen as appropriate to force people to drink alcohol through various games and activities. Furthermore, the university dictates that each tutor must be taught basic first aid, as well as a couple of courses in conflict management and basic education psychology.		At DTU (Danish Faculty of Technology and Engineering), Copenhagen Business School and Copenhagen University similar periods are held. They however vary, and are significantly shorter than the overall orientation period spent on Roskilde University.		
Liceo scientifico (literally scientific lyceum) is a type of secondary school in Italy. It is designed to give students the skills to progress to any university or higher educational institution.[1] Students can attend the liceo scientifico after successfully completing middle school (scuola media).		The curriculum is devised by the Ministry of Education, and emphasises the link between the humanistic tradition and scientific culture.[1] It covers a complete and widespread range of disciplines, including Italian language and literature, mathematics, physics, chemistry, biology, history, philosophy, Latin language and culture, English language and culture, art history and technical drawing.[2] Students typically study for five years, and attend the school from the age of 14 to 19. At the end of the fifth year all students sit for the esame di Stato, a final examination which leads to the maturità scientifica.		A student attending a liceo is called liceale, although the more generic terms studente (male) and studentessa (female) are also in common use. Teachers are known as professore (male) or professoressa (female).						Liceo scientifico was created with the Gentile reform in 1923.[3] Originally, students attended the school for four years and the curriculum was derived from that of the ginnasio, a humanity-centered type of secondary school similar to today's liceo classico.		This meant that, despite its name, the liceo scientifico had more teaching hours dedicated to Latin language and literature than mathematics.[4] Moreover, students that completed the school were not allowed to progress to university courses in the humanities or jurisprudence, while students that completed the liceo classico were allowed to progress to any university course. These limitations drew several criticisms from the academic world.[5]		The curriculum and structure underwent several changes over the course of the following century. Most importantly, in 1969 the ban preventing students of the liceo scientifico from entering many university courses was finally lifted.[6] In 1991 and again later in 1995, school reforms introduced an alternative curriculum, Piano Nazionale di Informatica (PNI, literally National Plan of Computer Studies).[7][8] The new curriculum differed from the old one not only by including computer programming, but also in having more teaching hours dedicated to mathematics and physics. This new curriculum was optional for schools to implement. Other alternative curricula were also made available for schools to choose from, such as a bilingual curriculum introducing the teaching of a second language in addition to English.[9]		The latest major reform, in 2008, eliminated all previous alternative curricula.[10] It significantly increased the teaching hours dedicated to scientific subjects and decreased those dedicated to Latin. In addition to the standard curriculum, it also created the alternative curriculum opzione scienze applicate (applied sciences option) which involves more teaching and laboratory hours for the sciences and does not include Latin.		The tables below show the hours of teaching dedicated to each subject per week for the two available curricula.		The final exam is officially called esame di Stato, although the old name esame di maturità is still in common use. It is composed of three written and one oral part:[13]		Students are examined by an exam committee which is formed in equal parts by their own teachers and teachers coming from other schools. The first and second tests are written by the Ministry of Education, while the third test and the oral exam are prepared and administered by the exam committee.[13]		The final grade is given as a number out of 100, and is the sum of the points obtained by students in the final exam and the credit they accumulated over the previous years.		
A Doctor of Philosophy (PhD, Ph.D., or DPhil; Latin Philosophiae Doctor) is a type of doctoral degree awarded by universities in many countries. Ph.D.s are awarded for programs across the whole breadth of academic fields. The completion of a Ph.D. is often a requirement for employment as a university professor, researcher, or scientist in many fields. Individuals who have earned a Doctor of Philosophy degree may, in most jurisdictions, use the title of "Doctor" (often abbreviated "Dr") or, in non-English speaking countries, variants such as "Dr. phil." with their name, and may use post-nominal letters such as "Ph.D.", "PhD" or "DPhil" (depending on the awarding institute).[1]		The requirements to earn a Ph.D. degree vary considerably according to the country, institution, and time period, from entry-level research degrees to higher doctorates. During the studies that lead to the degree, the student is called a doctoral student or Ph.D. student; a student who has completed all of their coursework and comprehensive examinations and is working on their thesis/dissertation is sometimes known as a doctoral candidate or Ph.D. candidate (see: all but dissertation). A student attaining this level may be granted a Candidate of Philosophy degree at some institutions.		A Ph.D. candidate must submit a project, thesis or dissertation often consisting of a body of original academic research, which is in principle worthy of publication in a peer-reviewed journal.[2] In many countries, a candidate must defend this work before a panel of expert examiners appointed by the university. Universities sometimes award other types of doctorate besides the Ph.D., such as the Doctor of Musical Arts (D.M.A.) for music performers and the Doctor of Education (Ed.D.) for professional educators. In 2005 the European Universities Association defined the Salzburg Principles, ten basic principles for third-cycle degrees (doctorates) within the Bologna Process.[3] These were followed in 2016 by the Florence Principles, seven basic principles for doctorates in the arts laid out by the European League of Institutes of the Arts, which have been endorsed by the European Association of Conservatoires, the International Association of Film and Television Schools, the International Association of Universities and Colleges of Art, Design and Media, and the Society for Artistic Research.[4]		In the context of the Doctor of Philosophy and other similarly titled degrees, the term "philosophy" does not refer to the field or academic discipline of philosophy, but is used in a broader sense in accordance with its original Greek meaning, which is "love of wisdom". In most of Europe, all fields (history, philosophy, social sciences, mathematics, and natural philosophy/natural sciences)[5] other than theology, law, and medicine (the so-called professional, vocational, or technical curriculum) were traditionally known as philosophy, and in Germany and elsewhere in Europe the basic faculty of liberal arts was known as the "faculty of philosophy".						The degree is abbreviated PhD (sometimes Ph.D. in North America), from the Latin Philosophiae Doctor, pronounced as three separate letters (/piːeɪtʃˈdiː/).[6][7][8] The abbreviation DPhil, from the English 'Doctor of Philosophy',[9] is used by a small number of British universities, including Oxford and formerly York and Sussex, as the abbreviation for degrees from those institutions.[10]		In the universities of Medieval Europe, study was organized in four faculties: the basic faculty of arts, and the three higher faculties of theology, medicine, and law (canon law and civil law). All of these faculties awarded intermediate degrees (bachelor of arts, of theology, of laws, of medicine) and final degrees. Initially, the titles of master and doctor were used interchangeably for the final degrees—the title Doctor was merely a formality bestowed on a Teacher/Master of the art—but by the late Middle Ages the terms Master of Arts and Doctor of Theology/Divinity, Doctor of Law, and Doctor of Medicine had become standard in most places (though in the German and Italian universities the term Doctor was used for all faculties).		The doctorates in the higher faculties were quite different from the current Ph.D. degree in that they were awarded for advanced scholarship, not original research. No dissertation or original work was required, only lengthy residency requirements and examinations. Besides these degrees, there was the licentiate. Originally this was a license to teach, awarded shortly before the award of the master or doctor degree by the diocese in which the university was located, but later it evolved into an academic degree in its own right, in particular in the continental universities.		According to Keith Allan Noble (1994), the first doctoral degree was awarded in medieval Paris around 1150.[11] The doctorate of philosophy developed in Germany as the terminal Teacher's credential in the 17th century (c. 1652). There were no PhDs in Germany before the 1650s (when they gradually started replacing the MA as the highest academic degree; arguably one of the earliest German PhD holders is Erhard Weigel (Dr. phil. hab., Leipzig, 1652).[citation needed]		In theory, the full course of studies might, for example, lead in succession to the degrees of Bachelor of Arts, Licentiate of Arts, Master of Arts or Bachelor of Medicine, Licentiate of Medicine, Doctor of Medicine. But before the early modern era, there were many exceptions to this. Most students left the university without becoming masters of arts, whereas regulars (members of monastic orders) could skip the arts faculty entirely.[12][13][14]		This situation changed in the early 19th century through the educational reforms in Germany, most strongly embodied in the model of the University of Berlin, founded and controlled by the Prussian government in 1810. The arts faculty, which in Germany was labelled the faculty of philosophy, started demanding contributions to research,[15] attested by a dissertation, for the award of their final degree, which was labelled Doctor of Philosophy (abbreviated as Ph.D.)—originally this was just the German equivalent of the Master of Arts degree. Whereas in the Middle Ages the arts faculty had a set curriculum, based upon the trivium and the quadrivium, by the 19th century it had come to house all the courses of study in subjects now commonly referred to as sciences and humanities.[16] Professors across the humanities and sciences focused on their advanced research.[17] Practically all the funding came from the central government, and it could be cut off if the professor was politically unacceptable.[relevant? – discuss][18]		These reforms proved extremely successful, and fairly quickly the German universities started attracting foreign students, notably from the United States. The American students would go to Germany to obtain a Ph.D. after having studied for a bachelor's degrees at an American college. So influential was this practice that it was imported to the United States, where in 1861 Yale University started granting the Ph.D. degree to younger students who, after having obtained the bachelor's degree, had completed a prescribed course of graduate study and successfully defended a thesis or dissertation containing original research in science or in the humanities.[19] In Germany, the name of the doctorate was adapted after the philosophy faculty started being split up − e.g. Dr. rer. nat. for doctorates in the faculty of natural sciences − but in most of the English-speaking world the name "Doctor of Philosophy" was retained for research doctorates in all disciplines.		The PhD degree and similar awards spread across Europe in the 19th and early 20th centuries. The degree was introduced in France in 1808, replacing diplomas as the highest academic degree; into Russia in 1819, when the Doktor Nauk degree, roughly equivalent to a PhD, gradually started replacing the specialist diploma, roughly equivalent to the MA, as the highest academic degree; and in Italy in 1927, when PhDs gradually started replacing the Laurea as the highest academic degree.[citation needed]		Research degrees first appeared in the UK in the late 19th century in the shape of the Doctor of Science (DSc or ScD) and other such "higher doctorates". The University of London introduced the DSc in 1860, but as an advanced study course, following on directly from the BSc, rather than a research degree. The first higher doctorate in the modern sense was Durham University's DSc, introduced in 1882.[20] This was soon followed by other universities, including the University of Cambridge establishing its ScD in the same year and the University of London transforming its DSc into a research degree in 1885. These were, however, very advanced degrees, rather than research-training degrees at the PhD level—Harold Jeffreys said that getting a Cambridge ScD was "more or less equivalent to being proposed for the Royal Society".[21]		Finally, in 1917 the current degree of Ph.D. was introduced, along the lines of the American and German model, and quickly became popular with both British and foreign students.[22] The slightly older degrees of Doctor of Science and Doctor of Literature/Letters still exist at British universities; together with the much older degrees of Doctor of Divinity (DD), Doctor of Music (DMus), Doctor of Civil Law (DCL) and Doctor of Medicine (MD) they form the higher doctorates, but apart from honorary degrees they are only infrequently awarded.		It should be noted that in the English (but not the Scottish) universities the Faculty of Arts had become dominant by the early 19th century. Indeed, the higher faculties had largely atrophied, since medical training had shifted to teaching hospitals,[23] the legal training for the common law system was provided by the Inns of Court (with some minor exceptions, see Doctors' Commons), and few students undertook formal study in theology. This contrasted with the situation in the continental European universities at the time, where the preparatory role of the Faculty of Philosophy or Arts was to a great extent taken over by secondary education: in modern France, the Baccalauréat is the examination taken at the end of secondary studies. The reforms at the Humboldt University transformed the Faculty of Philosophy or Arts (and its more recent successors such as the Faculty of Sciences) from a lower faculty into one on a par with the Faculties of Law and Medicine.		There were similar developments in many other continental European universities, and at least until reforms in the early 21st century many European countries (e.g. Belgium, Spain, and the Scandinavian countries) had in all faculties triple degree structures of bachelor (or candidate) − licentiate − doctor as opposed to bachelor − master − doctor; the meaning of the different degrees varied a lot from country to country however. To this day this is also still the case for the pontifical degrees in theology and canon law: for instance, in Sacred theology the degrees are Bachelor of Sacred Theology (STB), Licentiate of Sacred Theology (STL), and Doctor of Sacred Theology (STD), and in Canon law: Bachelor of Canon Law (JCB), Licentiate of Canon Law (JCL), and Doctor of Canon Law (JCD).		Until the mid-19th century, advanced degrees were not a criterion for professorships at most colleges. That began to change as the more ambitious scholars at major schools went to Germany for 1 to 3 years to obtain a Ph.D. in the sciences or humanities.[24][25] Graduate schools slowly emerged in the United States. In 1861, Yale awarded the first three earned Ph.D.s in North America to Eugene Schuyler, Arthur Williams Wright, and James Morris Whiton,[26] although honorary Ph.D.s had been awarded in the U.S. for almost a decade, with Bucknell University awarding the first to Ebenezer Newton Elliott in 1852.[27]		In the next two decades, NYU, the University of Pennsylvania, Harvard, and Princeton also began granting the degree. Major shifts toward graduate education were foretold by the opening of Clark University in 1887, which only offered graduate programs and the Johns Hopkins University which focused on its Ph.D. program. By the 1890s, Harvard, Columbia, Michigan and Wisconsin were building major graduate programs, whose alumni were hired by new research universities. By 1900, 300 Ph.D.s were awarded annually, most of them by six universities. It was no longer necessary to study in Germany.[28][29] However, half of the institutions awarding earned Ph.D.s in 1899 were undergraduate institutions that granted the degree for work done away from campus.[27] Degrees awarded by universities without legitimate Ph.D. programs accounted for about a third of the 382 doctorates recorded by the U.S. Department of Education in 1900, of which another 8–10% were honorary.[30]		At the start of the 20th century, U.S. universities were held in low regard internationally and many American students were still traveling to Europe for Ph.D.s. The lack of centralised authority meant anyone could start a university and award Ph.D.s. This led to the formation of the Association of American Universities by 14 leading research universities (producing nearly 90% of the approximately 250 legitimate research doctorates awarded in 1900), with one of the main goals being to "raise the opinion entertained abroad of our own Doctor's Degree".[30]		In Germany, the national government funded the universities and the research programs of the leading professors. It was impossible for professors who were not approved by Berlin to train graduate students. In the United States, by contrast, private universities and state universities alike were independent of the federal government. Independence was high, but funding was low. The breakthrough came from private foundations, which began regularly supporting research in science and history; large corporations sometimes supported engineering programs. The postdoctoral fellowship was established by the Rockefeller Foundation in 1919. Meanwhile, the leading universities, in cooperation with the learned societies, set up a network of scholarly journals. "Publish or perish" became the formula for faculty advancement in the research universities. After World War II, state universities across the country expanded greatly in undergraduate enrollment, and eagerly added research programs leading to masters or doctorate degrees. Their graduate faculties had to have a suitable record of publication and research grants. Late in the 20th century, "publish or perish" became increasingly important in colleges and smaller universities.[31]		Detailed requirements for the award of a Ph.D. degree vary throughout the world and even from school to school. It is usually required for the student to hold an Honours degree or a Master's Degree with high academic standing, in order to be considered for a Ph.D. program.[citation needed] In the US, Canada, India, and Denmark, for example, many universities require coursework in addition to research for Ph.D. degrees. In other countries (such as the UK) there is generally no such condition, though this varies by university and field.[32] Some individual universities or departments specify additional requirements for students not already in possession of a bachelor's degree or equivalent or higher. In order to submit a successful Ph.D. admission application, copies of academic transcripts, letters of recommendation, a research proposal, and a personal statement are often required. Most universities also invite for a special interview before admission.		A candidate must submit a project or thesis or dissertation often consisting of a body of original academic research, which is in principle worthy of publication in a peer-reviewed context.[2] In many countries a candidate must defend this work before a panel of expert examiners appointed by the university; in other countries, the dissertation is examined by a panel of expert examiners who stipulate whether the dissertation is in principle passable and any issues that need to be addressed before the dissertation can be passed.		Some universities in the non-English-speaking world have begun adopting similar standards to those of the anglophone Ph.D. degree for their research doctorates (see the Bologna process).[33]		A Ph.D. student or candidate is conventionally required to study on campus under close supervision. With the popularity of distance education and e-learning technologies, some universities now accept students enrolled into a distance education part-time mode.		In a "sandwich Ph.D." program, Ph.D. candidates do not spend their entire study period at the same university. Instead, the Ph.D. candidates spend the first and last periods of the program at their home universities, and in between conduct research at another institution or field research.[34] Occasionally a "sandwich Ph.D." will be awarded by two universities.[35]		A PhD confirmation is a preliminary presentation or lecture that a PhD candidate presents to faculty and possibly other interested members.[where?] The lecture follows after a suitable topic has been identified, and can include such matters as the aim of the research, methodology, first results, planned (or finished) publications, etc.		The confirmation lecture can be seen as a trial run for the final public defense, though faculty members at this stage can still largely influence the direction of the research. At the end of the lecture, the PhD candidate can be seen as "confirmed" – faculty members give their approval and trust that the study is well directed and will with high probability result in the candidate being successful.		In the United States, this is generally called advancing to Candidacy, the confirmation event being called the Candidacy Examination.		Ph.D. students are often motivated to pursue the Ph.D. by scientific and humanistic curiosity, the desire to contribute to the academic community, service to others, or personal development. A career in academia generally requires a Ph.D., though, in some countries, it is possible to reach relatively high positions without a doctorate. In North America, professors are increasingly being required to have a Ph.D., because the percentage of faculty with a Ph.D. is used as a university ratings measure.[36]		The motivation may also include increased salary, but in many cases, this is not the result. Research by Casey suggests that, over all subjects, Ph.D.s provide an earnings premium of 26% over non-accredited graduates, but notes that master's degrees provide a premium of 23% and a bachelor's 14%. While this is a small return to the individual (or even an overall deficit when tuition and lost earnings during training are accounted for), he claims there are significant benefits to society for the extra research training.[37] However, some research suggests that overqualified workers are often less satisfied and less productive at their jobs.[38] These difficulties are increasingly being felt by graduates of professional degrees, such as law school, looking to find employment. Ph.D. students often have to take on debt to undertake their degree.[citation needed]		A Ph.D. is also required in some positions outside academia, such as research jobs in major international agencies. In some cases, the Executive Directors of some types of foundations may be expected to hold a Ph.D.[citation needed] A Ph.D. is sometimes felt to be a necessary qualification in certain areas of employment, such as in foreign policy think-tanks: U.S. News wrote in 2013 that "[i]f having a master's degree at the minimum is de rigueur in Washington's foreign policy world, it is no wonder many are starting to feel that the Ph.D. is a necessary escalation, another case of costly signaling to potential employers."[39] Similarly, an article on the Australian public service states that "credentialism in the public service is seeing a dramatic increase in the number of graduate positions going to PhDs and masters degrees becoming the base entry level qualification."[40]		The Economist published an article in 2010 citing various criticisms against the state of Ph.D.s. These included a prediction by economist Richard B. Freeman that, based on pre-2000 data, only 20% of life science Ph.D. students would gain a faculty job in the U.S., and that in Canada 80% of postdoctoral research fellows earned less than or equal to an average construction worker ($38,600 a year). According to the article, only the fastest developing countries (e.g. China or Brazil) have a shortage of Ph.D.s.[38]		The U.S. higher education systems often offers little incentive to move students through Ph.D. programs quickly, and may even provide incentive to slow them down. To counter this, the United States introduced the Doctor of Arts degree in 1970 with seed money from the Carnegie Foundation for the Advancement of Teaching. The aim of the Doctor of Arts degree was to shorten the time needed to complete the degree by focusing on pedagogy over research, although the Doctor of Arts still contains a significant research component. Germany is one of the few nations engaging these issues, and it has been doing so by reconceptualising Ph.D. programs to be training for careers, outside academia, but still at high-level positions. This development can be seen in the extensive number of Ph.D. holders, typically from the fields of law, engineering, and economics, at the very top corporate and administrative positions. To a lesser extent, the UK research councils have tackled the issue by introducing, since 1992, the EngD.[citation needed][clarification needed]		Mark C. Taylor opined in 2011 in Nature that total reform of Ph.D. programs in almost every field is necessary in the U.S. and that pressure to make the necessary changes will need to come from many sources (students, administrators, public and private sectors, etc.).[41] Other articles in Nature have also examined the issue of PhD reform.[42][43][44]		In German-speaking nations; most Eastern European nations; successor states of the former Soviet Union; most parts of Africa, Asia, and many Spanish-speaking countries, the corresponding degree to a Doctor of Philosophy is simply called "Doctor" (Doktor), and the subject area is distinguished by a Latin suffix (e.g., "Dr. med." for Doctor medicinae, Doctor of Medicine; "Dr. rer. nat." for Doctor rerum naturalium, Doctor of the Natural Sciences; "Dr. phil." for Doctor philosophiae, Doctor of Philosophy; "Dr. iur." for Doctor iuris, Doctor of Laws).[45]		The UNESCO, in its International Standard Classification of Education (ISCED), states that: "Programmes to be classified at ISCED level 8 are referred to in many ways around the world such as PhD, DPhil, D.Lit, D.Sc, LL.D, Doctorate or similar terms. However, it is important to note that programmes with a similar name to 'doctor' should only be included in ISCED level 8 if they satisfy the criteria described in Paragraph 263. For international comparability purposes, the term 'doctoral or equivalent' is used to label ISCED level 8".[46]		In Argentina, the admission to a Ph.D. program at public Argentine University requires the full completion of a Master's degree or a Licentiate degree. Non-Argentine Master's titles are generally accepted into a Ph.D. program when the degree comes from a recognized university.		While a significant portion of postgraduate students finance their tuition and living costs with teaching or research work at private and state-run institutions, international institutions, such as the Fulbright Program and the Organization of American States (OAS), have been known to grant full scholarships for tuition with apportions for housing.[47]		Upon completion of at least two years' research and coursework as a graduate student, a candidate must demonstrate truthful and original contributions to his or her specific field of knowledge within a frame of academic excellence.[48] The doctoral candidate's work should be presented in a dissertation or thesis prepared under the supervision of a tutor or director, and reviewed by a Doctoral Committee. This Committee should be composed of examiners that are external to the program, and at least one of them should also be external to the institution. The academic degree of Doctor, respective to the correspondent field of science that the candidate has contributed with original and rigorous research, is received after a successful defense of the candidate's dissertation.[49]		Admission to a Ph.D. program in Australia requires applicants to demonstrate capacity to undertake research in the proposed field of study. The standard requirement is a bachelor's degree with either first-class or upper second-class honors. Research master's degrees and coursework master's degrees with a 25% research component are usually considered equivalent. It is also possible for research master's degree students to 'upgrade' to Ph.D. candidature after demonstrating sufficient progress.		Ph.D. students are sometimes offered a scholarship to study for their Ph.D. degree. The most common of these are the government-funded Australian Postgraduate Award (APA), which provides a living stipend to students of approximately A$25,800 a year (tax-free). APAs are paid for a duration of 3 years, while a 6-month extension is usually possible upon citing delays out of the control of the student.[50] Some universities also fund a similar scholarship that matches the APA amount. Due to a continual increase in living costs, many Ph.D. students are forced to live under the poverty line.[51] In addition to the more common APA and university scholarships, Australian students have other sources of scholarship funding.		Australian citizens, permanent residents, and New Zealand citizens are not charged course fees for their Ph.D. or research master's degree, with exception to the student services and amenities fee (SSAF) which is set by each university and typically involves the largest amount allowed by the Australian government. All fees are paid for by the Australian government, except for the SSAF, under the Research Training Scheme.[52] International students and coursework master's degree students must pay course fees unless they receive a scholarship to cover them.		Completion requirements vary. Most Australian Ph.D. programs do not have a required coursework component. The credit points attached to the degree are all in the product of the research, which is usually an 80,000-word thesis that makes a significant new contribution to the field. The Ph.D. thesis is sent to external examiners who are experts in the field of research and who have not been involved in the work. Examiners are nominated by the candidate's university and their identities are often not revealed to the candidate until the examination is complete. A formal oral defence is generally not part of the examination of the thesis, largely because of the distances that would need to be traveled by the overseas examiners. Recent pressure on higher degree by research (HDR) students to publish has resulted in increasing interest in Ph.D by publication as opposed to the more traditional Ph.D by dissertation [53].		Admission to a doctoral programme at a Canadian university usually requires completion of a Master's degree in a related field, with sufficiently high grades and proven research ability. In some cases, a student may progress directly from an Honours Bachelor's degree to a Ph.D. program; other programs allow a student to fast-track to a doctoral program after one year of outstanding work in a Master's program (without having to complete the Master's).		An application package typically includes a research proposal, letters of reference, transcripts, and in some cases, a writing sample or Graduate Record Examinations scores. A common criterion for prospective Ph.D. students is the comprehensive or qualifying examination, a process that often commences in the second year of a graduate program. Generally, successful completion of the qualifying exam permits continuance in the graduate program. Formats for this examination include oral examination by the student's faculty committee (or a separate qualifying committee), or written tests designed to demonstrate the student's knowledge in a specialized area (see below) or both.		At English-speaking universities, a student may also be required to demonstrate English language abilities, usually by achieving an acceptable score on a standard examination (for example the Test of English as a Foreign Language). Depending on the field, the student may also be required to demonstrate ability in one or more additional languages. A prospective student applying to French-speaking universities may also have to demonstrate some English language ability.		While some students work outside the university (or at student jobs within the university), in some programs students are advised (or must agree) not to devote more than ten hours per week to activities (e.g., employment) outside of their studies, particularly if they have been given funding. For large and prestigious scholarships, such as those from NSERC and Fonds québécois de la recherche sur la nature et les technologies, this is an absolute requirement.		At some Canadian universities, most Ph.D. students receive an award equivalent to part or all of the tuition amount for the first four years (this is sometimes called a tuition deferral or tuition waiver). Other sources of funding include teaching assistantships and research assistantships; experience as a teaching assistant is encouraged but not requisite in many programs. Some programs may require all Ph.D. candidates to teach, which may be done under the supervision of their supervisor or regular faculty. Besides these sources of funding, there are also various competitive scholarships, bursaries, and awards available, such as those offered by the federal government via NSERC, CIHR, or SSHRC.		In general, the first two years of study are devoted to completion of coursework and the comprehensive examinations. At this stage, the student is known as a "Ph.D. student" or "doctoral student". It is usually expected that the student will have completed most of his or her required coursework by the end of this stage. Furthermore, it is usually required that by the end of eighteen to thirty-six months after the first registration, the student will have successfully completed the comprehensive exams.		Upon successful completion of the comprehensive exams, the student becomes known as a "Ph.D. candidate". From this stage on, the bulk of the student's time will be devoted to his or her own research, culminating in the completion of a Ph.D. thesis or dissertation. The final requirement is an oral defense of the thesis, which is open to the public in some, but not all, universities. At most Canadian universities, the time needed to complete a Ph.D. degree typically ranges from four to six years.[citation needed] It is, however, not uncommon for students to be unable to complete all the requirements within six years, particularly given that funding packages often support students for only two to four years; many departments will allow program extensions at the discretion of the thesis supervisor and/or department chair. Alternate arrangements exist whereby a student is allowed to let their registration in the program lapse at the end of six years and re-register once the thesis is completed in draft form. The general rule is that graduate students are obligated to pay tuition until the initial thesis submission has been received by the thesis office. In other words, if a Ph.D. student defers or delays the initial submission of their thesis they remain obligated to pay fees until such time that the thesis has been received in good standing.		In Colombia, the Ph.D. course admission may require a master's degree (Magíster) in some universities, specially public universities. However, it could also be applied for a direct doctorate in specific cases, according to the jury's recommendations on the thesis proposal.		Most of postgraduate students in Colombia must finance their tuition fees by means of teaching assistant seats or research works. Some institutions such as Colciencias, Colfuturo, and Icetex grant scholarships or provide awards in the form of forgivable loans.[54]		After two or two and a half years it is expected the research work of the doctoral candidate to be submitted in the form of oral qualification, where suggestions and corrections about the research hypothesis and methodology, as well as on the course of the research work are performed. The Ph.D. degree is only received after a successful defense of the candidate's thesis is performed (four or five years after the enrollment), and most of the times also requiring the most important results having been published in at least one peer-reviewed high impact international journal.		In Finland, the degree of filosofian tohtori (abbreviated FT) is awarded by traditional universities, such as University of Helsinki. A Master's degree is required, and the doctorate combines approximately 4–5 years of research (amounting to 3–5 scientific articles, some of which must be first-author) and 60 ECTS points of studies.[55] Other universities such as Aalto University award degrees such as tekniikan tohtori (TkT, engineering), taiteen tohtori (TaT, art), etc., which are translated in English to Doctor of Science (D.Sc.), and they are formally equivalent. The licentiate (filosofian lisensiaatti or FL) requires only 2–3 years of research and is sometimes done before an FT.		Before 1984 three research doctorates existed in France: the State doctorate (doctorat d'État, the old doctorate introduced in 1808), the third cycle doctorate (doctorat de troisième cycle, created in 1954 and shorter than the State doctorate) and the diploma of doctor-engineer (diplôme de docteur-ingénieur created in 1923), for technical research. After 1984, only one type of doctoral degree remained, called "doctorate" (Doctorat). The latter is equivalent to the Ph.D.		Students pursuing the Ph.D. degree must first complete a master's degree program, which takes two years after graduation with a bachelor's degree (five years in total). The candidate must find funding and a formal doctoral advisor (Directeur de thèse) with an habilitation throughout the doctoral program.		The Ph.D. admission is granted by a graduate school (in French, "école doctorale"). A Ph.D. candidate can follow some in-service training offered by the graduate school while continuing his or her research at laboratory. His or her research may be carried out in a laboratory, at a university, or in a company. In the last case, the company hires the candidate and he or she is supervised by both the company's tutor and a labs' professor. The validation of the Ph.D. degree requires generally 3 to 4 years after the master's degree.		The financing of Ph.D. research comes mainly from funds for research of the French Ministry of Higher Education and Research. The most common procedure is a short-term employment contract called doctoral contract: the institution of higher education is the employer and the Ph.D. candidate the employee. However, the candidate can apply for funds from a company who can host him or her at its premises (as in the case where Ph.D. candidates do their research in a company). As another encountered situation, the company and the institute can sign together a funding agreement so that the candidate still has a public doctoral contract, but is daily located in the company (for example, it is particularly the case of (French) Scientific Cooperation Foundation). Many other resources come from some regional/city projects, some associations, etc.		In India, generally, a master's degree is required to gain admission to a doctoral program. Direct admission to a Ph.D. programme after bachelors is also offered by the IITs, the IIITs, the NITs and the Academy of Scientific and Innovative Research. In some subjects, doing a Masters in Philosophy (M.Phil.) is a prerequisite to starting a Ph.D. For funding/fellowship, it is required to qualify for the National Eligibility Test for Lectureship and Junior Research fellowship (NET for LS and JRF)[56] conducted by the federal research organisation Council of Scientific and Industrial Research (CSIR) and University Grants Commission (UGC).		In the last few years, there have been many changes in the rules relating to a Ph.D. in India.[citation needed] According to the new rules described by UGC, universities must have to conduct entrance exams in general ability and the selected subject. After clearing these tests, the shortlisted candidates need to appear for an interview by the available supervisor/guide. After successful completion of the coursework, the students are required to give presentations of the research proposal (plan of work or synopsis) at the beginning, submit progress reports, give a pre-submission presentation and finally defend the thesis in an open defence viva-voce.[citation needed]		In Germany, admission to a doctoral program is generally on the basis of having an advanced degree (i.e., a master's degree, diplom, magister, or staatsexamen), mostly in a related field and having above-average grades. A candidate must also find a tenured professor from a university to serve as the formal advisor and supervisor (Betreuer) of the dissertation throughout the doctoral program called Promotion. This supervisor is informally referred to as Doktorvater or Doktormutter, which literally translate to "doctor's father" and "doctor's mother" respectively.		While most German doctorates are considered equivalent to the PhD, an exception is the medical doctorate, where "doctoral" dissertations are often written alongside undergraduate study. The European Research Council decided in 2010 that those doctorates do not meet the international standards of a PhD research degree.[57][58] There are different forms of university-level institution in Germany, but only professors from "Universities" (Univ.-Prof.) can serve as doctoral supervisors – "Universities of Applied Sciences" (Fachhochschulen) are not entitled to award doctorates,[59] although some exceptions apply to this rule.[60]		Depending on the university, doctoral students (Doktoranden) can be required to attend formal classes or lectures, some of them also including exams or other scientific assignments, in order to get one or more certificates of qualification (Qualifikationsnachweise). Depending on the doctoral regulations (Promotionsordnung) of the university and sometimes on the status of the doctoral student, such certificates may not be required. Usually, former students, research assistants or lecturers from the same university, may be spared from attending extra classes. Instead, under the tutelage of a single professor or advisory committee, they are expected to conduct independent research. In addition to doctoral studies, many doctoral candidates work as teaching assistants, research assistants, or lecturers.		Many universities have established research-intensive Graduiertenkollegs ("graduate colleges"), which are graduate schools that provide funding for doctoral studies.		The usual duration of a doctoral program largely depends on the subject and area of research; but, often three to five years of full-time research work are required.		In 2014, the median age of new Ph.D. graduates was 30.4 years of age.[61]		The degree of Candidate of Sciences (Russian: кандидат наук, Kandidat Nauk) was the first advanced research qualification in the former USSR (it was introduced there in 1934) and some Eastern Bloc countries (Czechoslovakia, Hungary) and is still awarded in some post-Soviet states (Russian Federation, Belarus, and others). According to "Guidelines for the recognition of Russian qualifications in the other countries", in countries with a two-tier system of doctoral degrees (like Russian Federation, some post-Soviet states, Germany, Poland, Austria and Switzerland), should be considered for recognition at the level of the first doctoral degree, and in countries with only one doctoral degree, the degree of Kandidat Nauk should be considered for recognition as equivalent to this Ph.D. degree.		As most education systems only have one advanced research qualification granting doctoral degrees or equivalent qualifications (ISCED 2011,[62] par.270), the degree of Candidate of Sciences (Kandidat Nauk) of the former USSR counties is usually considered at the same level as the doctorate or Ph.D. degrees of those countries.[63][64]		According to the Joint Statement by the Permanent Conference of the Ministers for Education and Cultural Affairs of the Länder of the Federal Republic of Germany (Kultusministerkonferenz, KMK), German Rectors' Conference (HRK) and the Ministry of General and Professional Education of the Russian Federation, the degree of Kandidat Nauk is recognised in Germany at the level of the German degree of Doktor and the degree of Doktor Nauk at the level of German Habilitation.[65][66] The Russian degree of Kandidat Nauk is also officially recognised by the Government of the French Republic as equivalent to French doctorate.[67][68]		According to the International Standard Classification of Education (ISCED) 2011, for purposes of international educational statistics, Kandidat Nauk (Candidate of Sciences) belongs to ISCED level 8, or "doctoral or equivalent", together with Ph.D., D.Phil., D.Litt., D.Sc., LL.D., Doctorate or similar. It is mentioned in the Russian version of ISCED 2011 (par.262) on the UNESCO website as an equivalent to Ph.D. belonging to this level.[62] In the same way as Ph.D. degrees awarded in many English-speaking countries, Kandidat Nauk (Candidate of Sciences) allows its holders to reach the level of the Docent.[69] The second doctorate[63] (or post-doctoral degree)[70][71] in some post-Soviet states called Doctor of Sciences (Russian: доктор наук, Doktor Nauk) is given as an example of second advanced research qualifications or higher doctorates in ISCED 2011[62] (par.270) and is similar to Habilitation in Germany, Poland and several other countries.[63][71] It constitutes a higher qualification compared to Ph.D. as against the European Qualifications Framework (EQF) or Dublin Descriptors.[71]		About 88% of Russian students studying at state universities study at the expense of budget funds.[72] The average stipend in Russia (as of August 2011) is $430 a year ($35/month).[73] The average tuition fee in graduate school is $2,000 per year.[74]		The Dottorato di ricerca (research doctorate), abbreviated to "Dott. Ric." or "Ph.D.", is an academic title awarded at the end of a course of not less than three years, admission to which is based on entrance examinations and academic rankings in the Bachelor of Arts ("Laurea Triennale") and Master of Arts ("Laurea Magistrale" or "Laurea Specialistica"). While the standard Ph.D. follows the Bologna process, the M.D.-Ph.D. programme may be completed in two years.		The first institution in Italy to create a doctoral program (Ph.D.) was Scuola Normale Superiore di Pisa in 1927 under the historic name "Diploma di Perfezionamento".[75][76] Further, the research doctorates or Ph.D. (Dottorato di ricerca) in Italy were introduced by law and Presidential Decree in 1980,[77][78] referring to the reform of academic teaching, training and experimentation in organisation and teaching methods.[79][80]		Hence, the Superior Graduate Schools in Italy[81] (Scuola Superiore Universitaria),[82] also called Schools of Excellence (Scuole di Eccellenza)[81][83] such as Scuola Normale Superiore di Pisa and Sant'Anna School of Advanced Studies still keep their reputed historical "Diploma di Perfezionamento" Ph.D. title by law[76][84] and MIUR Decree.[85][86]		Doctorate courses are open, without age or citizenship limits, to all those who already hold a "laurea magistrale" (master degree) or similar academic title awarded abroad which has been recognised as equivalent to an Italian degree by the Committee responsible for the entrance examinations.		The number of places on offer each year and details of the entrance examinations are set out in the examination announcement.		A doctoral degree (Pol. doktor), abbreviated to Ph.D. (Pol. dr) is an advanced academic degree awarded by universities in most fields[87][88][89][90][91] as well as by the Polish Academy of Sciences,[92] regulated by the Polish parliament acts[93] and the government orders, in particular by the Ministry of Science and Higher Education of the Republic of Poland. Commonly, students with a master's degree or equivalent are accepted to a doctoral entrance exam. The title of Ph.D. is awarded to a scientist who 1) completed a minimum of 3 years of Ph.D. studies (Pol. studia doktoranckie; not required to obtain Ph.D.), 2) finished his/her theoretical and/or laboratory's scientific work, 3) passed all Ph.D. examinations, 4) submitted his/her dissertation, a document presenting the author's research and findings,[94] 5) successfully defended his/her doctoral thesis. Typically, upon completion, the candidate undergoes an oral examination, always public, by his/her supervisory committee with expertise in the given discipline.		Starting in 2016,[95] in Ukraine Doctor of Philosophy (PhD, Ukrainian: Доктор філософії) is the highest education level and the first science degree. PhD is awarded in recognition of a substantial contribution to scientific knowledge, origination of new directions and visions in science. A PhD degree is a prerequisite for heading a university department in Ukraine. Upon completion of a PhD, a PhD holder can elect to continue his studies and get a post-doctoral degree called "Doctor of Sciences" (DSc. Ukrainian: Доктор наук), which is the second and the highest science degree in Ukraine.		The doctorate was introduced in Sweden in 1477 and in Denmark-Norway in 1479 and awarded in theology, law, and medicine, while the magister's degree was the highest degree at the Faculty of Philosophy, equivalent to the doctorate.		Scandinavian countries were among the early adopters of a degree known as a doctorate of philosophy, based upon the German model. Denmark and Norway both introduced the Dr. Phil(os). degree in 1824, replacing the Magister's degree as the highest degree, while Uppsala University of Sweden renamed its Magister's degree Filosofie Doktor (fil. dr) in 1863. These degrees, however, became comparable to the German Habilitation rather than the doctorate, as Scandinavian countries did not have a separate Habilitation.[96]		The degrees were uncommon and not a prerequisite for employment as a professor; rather, they were seen as distinctions similar to the British (higher) doctorates (D.Litt., D.Sc.). Denmark introduced an American-style Ph.D. in 1989; it formally replaced the Licentiate's degree and is considered a lower degree than the dr. phil. degree; officially, the ph.d. is not considered a doctorate, but unofficially, it is referred to as "the smaller doctorate", as opposed to the dr. phil., "the grand doctorate". Holders of a ph.d. degree are not entitled to style themselves as "Dr."[97] Currently Denmark distinctions between the dr. phil. as the proper doctorate and a higher degree than the ph.d., whereas in Norway, the historically analogous dr. philos. degree is officially regarded as equivalent to the new ph.d.		In Sweden, the doctorate of philosophy was introduced at Uppsala University's Faculty of Philosophy in 1863. In Sweden, the Latin term is officially translated into Swedish filosofie doktor and commonly abbreviated fil. dr or FD. The degree represents the traditional Faculty of Philosophy and encompasses subjects from biology, physics, and chemistry, to languages, history, and social sciences, being the highest degree in these disciplines. Sweden currently has two research-level degrees, the Licentiate's degree, which is comparable to the Danish degree formerly known as the Licentiate's degree and now as the ph.d., and the higher doctorate of philosophy, Filosofie Doktor. Some universities in Sweden also use the term teknologie doktor for doctorates awarded by institutes of technology (for doctorates in engineering or natural science related subjects such as materials science, molecular biology, computer science etc.). The Swedish term fil. dr is often also used as a translation of corresponding degrees from e.g. Denmark and Norway.		Doctoral degrees are regulated by Real Decreto (Royal Decree in Spanish) R.D. 99/2011 from the 2014/2015 academic year.[98] They are granted by a university on behalf of the King, and its diploma has the force of a public document. The Ministry of Science keeps a National Registry of Theses called TESEO.[99]		All doctoral programs are of a research nature. A minimum of three years of study are required, in one stage only:		A doctoral degree is required to apply to a long-term teaching position at a university.		The social standing of doctors in Spain is evidenced by the fact that only Ph.D. holders, Grandees and Dukes can take seat and cover their heads in the presence of the King.[101] All Doctor Degree holders are reciprocally recognized as equivalent in Germany and Spain ("Bonn Agreement of November 14, 1994").[102]		Universities admit applicants to Ph.D. programs on a case-by-case basis; depending on the university, admission is typically conditional on the prospective student having completed an undergraduate degree with at least upper second-class honours or a postgraduate master's degree but requirements can vary.		In the case of the University of Oxford, for example, "The one essential condition of being accepted … is evidence of previous academic excellence, and of future potential."[103] Some UK universities (e.g. Oxford) abbreviate their Doctor of Philosophy degree as "DPhil", while most use the abbreviation "PhD"; these are in all other respects equivalent. Commonly, students are first accepted onto an MPhil or MRes programme and may transfer to Ph.D. regulations upon satisfactory progress, this is sometimes referred to as APG (Advanced Postgraduate) status. This is typically done after one or two years and the research work done may count towards the Ph.D. degree. If a student fails to make satisfactory progress, he or she may be offered the opportunity to write up and submit for an MPhil degree as is the case at the King's College London and University of Manchester. In many universities, the MPhil is also offered as a stand-alone research degree.		Ph.D. students from countries outside the EU/EFTA area are required to comply with the Academic Technology Approval Scheme (ATAS), which involves undergoing a security clearance process with the Foreign Office for certain courses in medicine, mathematics, engineering and material sciences.[104][105] This requirement was introduced in 2007 due to concerns about overseas terrorism and weapons proliferation.[105]		In the United Kingdom, funding for Ph.D. students is sometimes provided by government-funded Research Councils or the European Social Fund, usually in the form of a tax-free bursary which consists of tuition fees together with a stipend.[106] Tuition fees are charged at different rates for "Home/EU" and "Overseas" students, generally £3,000–£6,000 per year for the former and £9,000–14,500 for the latter (which includes EU citizens who have not been normally resident in the EEA for the last three years), although this can rise to over £16,000 at elite institutions. Higher fees are often charged for laboratory-based degrees.[107][108]		The stipend is around £13,000 per year for three years,[106] (sometimes higher by £2,000–3,000 in London), whether or not the degree continues for longer (within the usual four-year span). This implies that the fourth year of Ph.D. work is often unfunded. A very small number of scientific studentships are sometimes paid at a higher rate - for example, in London, Cancer Research UK, the ICR and the Wellcome Trust stipend rates start at around £19,000 and progress annually to around £23,000 a year; an amount that is tax and national insurance free. Research Council funding is sometimes 'earmarked' for a particular department or research group, who then allocate it to a chosen student, although in doing so they are generally expected to abide by the usual minimum entry requirements (typically a first degree with upper second class honours, although successful completion of a postgraduate master's degree is usually counted as raising the class of the first degree by one division for these purposes). The availability of funding in many disciplines (especially humanities, social studies and pure science[citation needed] subjects) means that in practice only those with the best research proposals, references and backgrounds are likely to be awarded a studentship. The ESRC (Economic and Social Science Research Council) explicitly state that a 2.1 minimum (or 2.2 plus additional master's degree) is required—no additional marks are given for students with a first class honours or a distinction at masters level. Since 2002, there has been a move by research councils to fund interdisciplinary doctoral training centres which concentrate resources on fewer higher quality centres.		Many students who are not in receipt of external funding may choose to undertake the degree part-time, thus reducing the tuition fees, as well as creating free time in which to earn money for subsistence. Students may also take part in tutoring, work as research assistants, or (occasionally) deliver lectures, at a rate of typically £12-14 per hour, either to supplement existing low income or as a sole means of funding.[109]		There is usually a preliminary assessment to remain in the program and the thesis is submitted at the end of a three- to four-year program. These periods are usually extended pro rata for part-time students. With special dispensation, the final date for the thesis can be extended for up to four additional years, for a total of seven, but this is rare.[110] For full-time Ph.D.s, a 4-year time limit has now been fixed and students must apply for an extension to submit a thesis past this point. Since the early 1990s, British funding councils have adopted a policy of penalising departments where large proportions of students fail to submit their theses in four years after achieving Ph.D.-student status (or pro rata equivalent) by reducing the number of funded places in subsequent years.[111] Inadvertently, this leads to significant pressure on the candidate to minimise the scope of projects with a view on thesis submission, regardless of quality, and discouradge time spent on activities that would otherwise further the impact of the research on the community (e.g. publications in high impact journals, seminars, workshops). Furthermore, supervising staff are encouraged in their career progression to ensure that the Ph.D. students under their supervision finalise the projects in three rather than the four years that the program is permitted to cover. These issues contribute to an overall discrepancy between supervisors and Ph.D. candidates in the priority they assign to the quality and impact of the research contained in a Ph.D. project, the former favouring quick Ph.D. projects over several students and the latter favouring a larger scope for their own ambitious project, training, and impact.[citation needed]		There has recently been an increase in the number of Integrated Ph.D. programs available, such as at the University of Southampton. These courses include a Master of Research (MRes) in the first year, which consists of a taught component as well as laboratory rotation projects. The Ph.D. must then be completed within the next 3 years. As this includes the MRes all deadlines and timeframes are brought forward to encourage completion of both MRes and Ph.D. within 4 years from commencement. These programs are designed to provide students with a greater range of skills than a standard Ph.D., and for the university, they are a means of gaining an extra years' fees from public sources.		In the United Kingdom, Ph.D. degrees are distinct from other doctorates, most notably the higher doctorates such as D.Litt. (Doctor of Letters) or D.Sc. (Doctor of Science), which may be granted on the recommendation of a committee of examiners on the basis of a substantial portfolio of submitted (and usually published) research. However, some UK universities still maintain the option of submitting a thesis for the award of a higher doctorate.		Recent years have seen the introduction of professional doctorates (D.Prof or ProfD), which are the same level as Ph.D.s but more specific in their field.[112] These tend not to be solely academic, but combine academic research, a taught component and a professional qualification. These are most notably in the fields of engineering (Eng.D.), education (Ed.D.), educational psychology (D.Ed.Psych), occupational psychology (D.Occ Psych.) clinical psychology (D.Clin.Psych.), health psychology (DHealthPsy), social work (DSW), nursing (DNP), public administration (DPA), business administration (DBA), and music (DMA). These typically have a more formal taught component consisting of smaller research projects, as well as a 40,000–60,000-word thesis component, which together are officially considered equivalent to a Ph.D. degree.		In the United States, the Ph.D. degree is the highest academic degree awarded by universities in most fields of study. There are 282 universities in the United States that award the Ph.D. degree, and those universities vary widely in their criteria for admission, as well as the rigor of their academic programs.[113]		Requirements. U.S. students typically undergo a series of three phases in the course of their work toward the Ph.D. degree. The first phase consists of coursework in the student's field of study and requires one to three years to complete. This often is followed by a preliminary, a comprehensive examination, or a series of cumulative examinations where the emphasis is on breadth rather than depth of knowledge. The student is often later required to pass oral and written examinations in the field of specialization within the discipline, and here, depth is emphasized. Some Ph.D. programs require the candidate to successfully complete requirements in pedagogy (taking courses on higher level teaching and teaching undergraduate courses) or applied science (e.g., clinical practice and predoctoral clinical internship in Ph.D. programs in clinical, counseling, or school psychology).[citation needed]		Another two to eight years are usually required for the composition of a substantial and original contribution to human knowledge in the form of a written dissertation, which in the social sciences and humanities typically ranges from 50 to 450 pages. In many cases, depending on the discipline, a dissertation consists of a comprehensive literature review, an outline of methodology, and several chapters of scientific, social, historical, philosophical, or literary analysis. Typically, upon completion, the candidate undergoes an oral examination, sometimes public, by his or her supervisory committee with expertise in the given discipline.		Typically, Ph.D. programs require applicants to have a bachelor's degree in a relevant field (and, in many cases in the humanities, a master's degree), reasonably high grades, several letters of recommendation, relevant academic coursework, a cogent statement of interest in the field of study, and satisfactory performance on a graduate-level exam specified by the respective program (e.g., GRE, GMAT).[114][115]		Depending on the specific field of study, completion of a Ph.D. program usually takes four to eight years of study after the Bachelor's Degree; those students who begin a Ph.D. program with a master's degree may complete their Ph.D. degree a year or two sooner.[116] As Ph.D. programs typically lack the formal structure of undergraduate education, there are significant individual differences in the time taken to complete the degree. Overall, 57% of students who begin a Ph.D. program in the US will complete their degree within ten years, approximately 30% will drop out or be dismissed, and the remaining 13% of students will continue on past ten years.[117]		The number of Ph.D. diplomas awarded by US universities has risen nearly every year since 1957, according to data compiled by the US National Science Foundation. In 1957, US universities awarded 8,611 Ph.D. diplomas; 20,403 in 1967; 31,716 in 1977; 32,365 in 1987; 42,538 in 1997; 48,133 in 2007,[118] and 55,006 in 2015.[119]		Funding. Ph.D. students at U.S. universities typically receive a tuition waiver and some form of annual stipend.[citation needed] Many U.S. Ph.D. students work as teaching assistants or research assistants. Graduate schools increasingly[citation needed] encourage their students to seek outside funding; many are supported by fellowships they obtain for themselves or by their advisers' research grants from government agencies such as the National Science Foundation and the National Institutes of Health. Many Ivy League and other well-endowed universities provide funding for the entire duration of the degree program (if it is short) or for most of it.[citation needed]		At some universities, there may be training for those wishing to supervise Ph.D. studies. There is now a lot of literature published for academics who wish to do this, such as Delamont, Atkinson, and Parry (1997). Indeed, Dinham and Scott (2001) have argued that the worldwide growth in research students has been matched by increase in a number of what they term "how-to" texts for both students and supervisors, citing examples such as Pugh and Phillips (1987). These authors report empirical data on the benefits that a Ph.D. candidate may gain if he or she publishes work, and note that Ph.D. students are more likely to do this with adequate encouragement from their supervisors.		Wisker (2005) has noticed how research into this field has distinguished between two models of supervision: The technical-rationality model of supervision, emphasising technique; The negotiated order model, being less mechanistic and emphasising fluid and dynamic change in the Ph.D. process. These two models were first distinguished by Acker, Hill and Black (1994; cited in Wisker, 2005). Considerable literature exists on the expectations that supervisors may have of their students (Phillips & Pugh, 1987) and the expectations that students may have of their supervisors (Phillips & Pugh, 1987; Wilkinson, 2005) in the course of Ph.D. supervision. Similar expectations are implied by the Quality Assurance Agency's Code for Supervision (Quality Assurance Agency, 1999; cited in Wilkinson, 2005).		
Learning is the act of acquiring new or modifying and reinforcing existing knowledge, behaviors, skills, values, or preferences which may lead to a potential change in synthesizing information, depth of the knowledge, attitude or behavior relative to the type and range of experience.[1] The ability to learn is possessed by humans, animals, plants[2] and some machines. Progress over time tends to follow a learning curve. Learning does not happen all at once, but it builds upon and is shaped by previous knowledge. To that end, learning may be viewed as a process, rather than a collection of factual and procedural knowledge. Learning produces changes in the organism and the changes produced are relatively permanent.[3]		Human learning may occur as part of education, personal development, schooling, or training. It may be goal-oriented and may be aided by motivation. The study of how learning occurs is part of educational psychology, neuropsychology, learning theory, and pedagogy. Learning may occur as a result of habituation or classical conditioning, seen in many animal species, or as a result of more complex activities such as play, seen only in relatively intelligent animals.[4][5] Learning may occur consciously or without conscious awareness. Learning that an aversive event can't be avoided nor escaped is called learned helplessness.[6] There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.[7]		Play has been approached by several theorists as the first form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games.						Non-associative learning refers to "a relatively permanent change in the strength of response to a single stimulus due to repeated exposure to that stimulus. Changes due to such factors as sensory adaptation, fatigue, or injury do not qualify as non-associative learning."[8]		Non-associative learning can be divided into habituation and sensitization.		Habituation is an example of non-associative learning in which the strength or probability of a response diminishes when the response is repeated. The response is typically a reflex or unconditioned response. Thus, habituation must be distinguished from extinction, which is an associative process. In operant extinction, for example, a response declines because it is no longer followed by reward. An example of habituation can be seen in small song birds—if a stuffed owl (or similar predator) is put into the cage, the birds initially react to it as though it were a real predator. Soon the birds react less, showing habituation. If another stuffed owl is introduced (or the same one removed and re-introduced), the birds react to it again as though it were a predator, demonstrating that it is only a very specific stimulus that is habituated to (namely, one particular unmoving owl in one place). Habituation has been shown in essentially every species of animal, as well as the sensitive plant Mimosa pudica[9] and the large protozoan Stentor coeruleus.[10]		Sensitization is an example of non-associative learning in which the progressive amplification of a response follows repeated administrations of a stimulus (Bell et al., 1995).[citation needed] An everyday example of this mechanism is the repeated tonic stimulation of peripheral nerves that occurs if a person rubs their arm continuously. After a while, this stimulation creates a warm sensation that eventually turns painful. The pain results from the progressively amplified synaptic response of the peripheral nerves warning that the stimulation is harmful.[clarification needed] Sensitisation is thought to underlie both adaptive as well as maladaptive learning processes in the organism.[citation needed]		Active learning occurs when a person takes control of his/her learning experience. Since understanding information is the key aspect of learning, it is important for learners to recognize what they understand and what they do not. By doing so, they can monitor their own mastery of subjects. Active learning encourages learners to have an internal dialogue in which they verbalize understandings. This and other meta-cognitive strategies can be taught to a child over time. Studies within metacognition have proven the value in active learning, claiming that the learning is usually at a stronger level as a result.[12] In addition, learners have more incentive to learn when they have control over not only how they learn but also what they learn.[13] Active learning is a key characteristic of student-centered learning. Conversely, passive learning and direct instruction are characteristics of teacher-centered learning (or traditional education).		Associative learning is the process by which a person or animal learns an association between two stimuli. In classical conditioning a previously neutral stimulus is repeatedly paired with a reflex eliciting stimulus until eventually the neutral stimulus elicits a response on its own. In operant conditioning, a behavior that is reinforced or punished in the presence of a stimulus becomes more on less likely to occur in the presence of that stimulus.		In operant conditioning, the consequences (reinforcement or punishment) of a behavior change the frequency and/or form of that behavior. Stimulus present when the behavior/consequence occurs come to control these behavior modifications.		The typical paradigm for classical conditioning involves repeatedly pairing an unconditioned stimulus (which unfailingly evokes a reflexive response) with another previously neutral stimulus (which does not normally evoke the response). Following conditioning, the response occurs both to the unconditioned stimulus and to the other, unrelated stimulus (now referred to as the "conditioned stimulus"). The response to the conditioned stimulus is termed a conditioned response. The classic example is Ivan Pavlov and his dogs. Pavlov fed his dogs meat powder, which naturally made the dogs salivate—salivating is a reflexive response to the meat powder. Meat powder is the unconditioned stimulus (US) and the salivation is the unconditioned response (UR). Pavlov rang a bell before presenting the meat powder. The first time Pavlov rang the bell, the neutral stimulus, the dogs did not salivate, but once he put the meat powder in their mouths they began to salivate. After numerous pairings of bell and food, the dogs learned that the bell signaled that food was about to come, and began to salivate when they heard the bell. Once this occurred, the bell became the conditioned stimulus (CS) and the salivation to the bell became the conditioned response (CR). Classical conditioning has been demonstrated in many species. For example, it is seen in honeybees, in the proboscis extension reflex paradigm.[14] and recently, it was demonstrated in garden pea plants.[15]		Another influential person in the world of classical conditioning is John B. Watson. Watson's work was very influential and paved the way for B.F. Skinner's radical behaviorism. Watson's behaviorism (and philosophy of science) stood in direct contrast to Freud and other accounts based largely on introspection. Watson's view was that the introspective method was too subjective, and that we should limit the study of human development to directly observable behaviors. In 1913, Watson published the article "Psychology as the Behaviorist Views," in which he argued that laboratory studies should serve psychology best as a science. Watson's most famous, and controversial, experiment, "Little Albert", where he demonstrated how psychologists can account for the learning of emotion through classical conditioning principles.		Imprinting is a kind of learning occurring at a particular life stage that is rapid and apparently independent of the consequences of behavior. In filial imprinting, young animals, particularly birds, form an association with another individual or in some cases, an object, that they respond to as they would to a parent. In 1935, the Austrian Zoologist Konrad Lorenz discovered that certain birds follow and form a bond if the object makes sounds.		Play generally describes behavior with no particular end in itself, but that improves performance in similar future situations. This is seen in a wide variety of vertebrates besides humans, but is mostly limited to mammals and birds. Cats are known to play with a ball of string when young, which gives them experience with catching prey. Besides inanimate objects, animals may play with other members of their own species or other animals, such as orcas playing with seals they have caught. Play involves a significant cost to animals, such as increased vulnerability to predators and the risk of injury and possibly infection. It also consumes energy, so there must be significant benefits associated with play for it to have evolved. Play is generally seen in younger animals, suggesting a link with learning. However, it may also have other benefits not associated directly with learning, for example improving physical fitness.		Play, as it pertains to humans as a form of learning is central to a child's learning and development. Through play, children learn social skills such as sharing and collaboration. Children develop emotional skills such as learning to deal with the emotion of anger, through play activities. As a form of learning, play also facilitates the development of thinking and language skills in children.[16]		There are five types of play:		These five types of play are often intersecting. All types of play generate thinking and problem-solving skills in children. Children learn to think creatively when they learn through play.[17] Specific activities involved in each type of play change over time as humans progress through the lifespan. Play as a form of learning, can occur solitarily, or involve interacting with others.		Enculturation is the process by which people learn values and behaviors that are appropriate or necessary in their surrounding culture.[18] Parents, other adults, and peers shape the individual's understanding of these values.[18] If successful, enculturation results in competence in the language, values and rituals of the culture.[18] This is different from acculturation, where a person adopts the values and societal rules of a culture different from their native one.		Multiple examples of enculturation can be found cross-culturally. Collaborative practices in the Mazahua people have shown that participation in everyday interaction and later learning activities contributed to enculturation rooted in nonverbal social experience.[19] As the children participated in everyday activities, they learned the cultural significance of these interactions. The collaborative and helpful behaviors exhibited by Mexican and Mexican-heritage children is a cultural practice known as being "acomedido".[20] Chillihuani girls in Peru described themselves as weaving constantly, following behavior shown by the other adults.[21]		Episodic learning is a change in behavior that occurs as a result of an event.[22] For example, a fear of dogs that follows being bitten by a dog is episodic learning. Episodic learning is so named because events are recorded into episodic memory, which is one of the three forms of explicit learning and retrieval, along with perceptual memory and semantic memory.[23]		Multimedia learning is where a person uses both auditory and visual stimuli to learn information (Mayer 2001). This type of learning relies on dual-coding theory (Paivio 1971).		Electronic learning or e-learning is computer-enhanced learning. A specific and always more diffused e-learning is mobile learning (m-learning), which uses different mobile telecommunication equipment, such as cellular phones.		When a learner interacts with the e-learning environment, it's called augmented learning. By adapting to the needs of individuals, the context-driven instruction can be dynamically tailored to the learner's natural environment. Augmented digital content may include text, images, video, audio (music and voice). By personalizing instruction, augmented learning has been shown to improve learning performance for a lifetime.[24] See also minimally invasive education.		Moore (1989)[25] purported that three core types of interaction are necessary for quality, effective online learning:		In his theory of transactional distance, Moore (1993)[26] contented that structure and interaction or dialogue bridge the gap in understanding and communication that is created by geographical distances (known as transactional distance).		Rote learning is memorizing information so that it can be recalled by the learner exactly the way it was read or heard. The major technique used for rote learning is learning by repetition, based on the idea that a learner can recall the material exactly (but not its meaning) if the information is repeatedly processed. Rote learning is used in diverse areas, from mathematics to music to religion. Although it has been criticized by some educators, rote learning is a necessary precursor to meaningful learning.		Meaningful learning is the concept that learned knowledge (e.g., a fact) is fully understood to the extent that it relates to other knowledge. To this end, meaningful learning contrasts with rote learning in which information is acquired without regard to understanding. Meaningful learning, on the other hand, implies there is a comprehensive knowledge of the context of the facts learned.[27]		Informal learning occurs through the experience of day-to-day situations (for example, one would learn to look ahead while walking because of the danger inherent in not paying attention to where one is going). It is learning from life, during a meal at table with parents, play, exploring, etc.		Formal learning is learning that takes place within a teacher-student relationship, such as in a school system. The term formal learning has nothing to do with the formality of the learning, but rather the way it is directed and organized. In formal learning, the learning or training departments set out the goals and objectives of the learning.[28]		Nonformal learning is organized learning outside the formal learning system. For example, learning by coming together with people with similar interests and exchanging viewpoints, in clubs or in (international) youth organizations, workshops.		The educational system may use a combination of formal, informal, and nonformal learning methods. The UN and EU recognize these different forms of learning (cf. links below). In some schools, students can get points that count in the formal-learning systems if they get work done in informal-learning circuits. They may be given time to assist international youth workshops and training courses, on the condition they prepare, contribute, share and can prove this offered valuable new insight, helped to acquire new skills, a place to get experience in organizing, teaching, etc.		To learn a skill, such as solving a Rubik's Cube quickly, several factors come into play at once:		Tangential learning is the process by which people self-educate if a topic is exposed to them in a context that they already enjoy. For example, after playing a music-based video game, some people may be motivated to learn how to play a real instrument, or after watching a TV show that references Faust and Lovecraft, some people may be inspired to read the original work.[29] Self-education can be improved with systematization. According to experts in natural learning, self-oriented learning training has proven an effective tool for assisting independent learners with the natural phases of learning.[30]		Dialogic learning is a type of learning based on dialogue.		This learning is not planned by the instructor or the student, but occurs as a byproduct of another activity—an experience, observation, self-reflection, interaction, unique event, or common routine task. This learning happens in addition to or apart from the instructor's plans and the student's expectations.		Incidental learning is an occurrence that is not generally accounted for using the traditional methods of instructional objectives and outcomes assessment. This type of learning occurs in part as a product of social interaction and active involvement in both online and onsite courses. Research implies that some un-assessed aspects of onsite and online learning challenge the equivalency of education between the two modalities. Both onsite and online learning have distinct advantages with traditional on-campus students experiencing higher degrees of incidental learning in three times as many areas as online students. Additional research is called for to investigate the implications of these findings both conceptually and pedagogically.[31]		Benjamin Bloom has suggested three domains of learning:		These domains are not mutually exclusive. For example, in learning to play chess, the person must learn the rules (cognitive domain)—but must also learn how to set up the chess pieces and how to properly hold and move a chess piece (psychomotor). Furthermore, later in the game the person may even learn to love the game itself, value its applications in life, and appreciate its history (affective domain).[32]		Transfer of learning is the application of skill, knowledge or understanding to resolve a novel problem or situation that happens when certain conditions are fulfilled. Research indicates that learning transfer is infrequent; most common when "... cued, primed, and guided..."[33] and has sought to clarify what it is, and how it might be promoted through instruction.		Over the history of its discourse, various hypotheses and definitions have been advanced. First, it is speculated that different types of transfer exist, including: near transfer, the application of skill to solve a novel problem in a similar context; and far transfer, the application of skill to solve novel problem presented in a different context.[34] Furthermore, Perkins and Salomon (1992) suggest that positive transfer in cases when learning supports novel problem solving, and negative transfer occurs when prior learning inhibits performance on highly correlated tasks, such as second or third-language learning.[35] Concepts of positive and negative transfer have a long history; researchers in the early 20th century described the possibility that "...habits or mental acts developed by a particular kind of training may inhibit rather than facilitate other mental activities".[36] Finally, Schwarz, Bransford and Sears (2005) have proposed that transferring knowledge into a situation may differ from transferring knowledge out to a situation as a means to reconcile findings that transfer may both be frequent and challenging to promote.[37]		A significant and long research history has also attempted to explicate the conditions under which transfer of learning might occur. Early research by Ruger, for example, found that the "level of attention", "attitudes", "method of attack" (or method for tackling a problem), a "search for new points of view", "a careful testing of hypothesis" and "generalization" were all valuable approaches for promoting transfer.[38] To encourage transfer through teaching, Perkins and Salomon recommend aligning ("hugging") instruction with practice and assessment, and "bridging", or encouraging learners to reflect on past experiences or make connections between prior knowledge and current content.[35]		There are several internal factors that affect learning.[43][44] They are		Animals gain knowledge in two ways. First is learning—in which an animal gathers information about its environment and uses this information. For example, if an animal eats something that hurts its stomach, it learns not to eat that again. The second is innate knowledge that is genetically inherited. An example of this is when a horse is born and can immediately walk. The horse has not learned this behavior; it simply knows how to do it.[45] In some scenarios, innate knowledge is more beneficial than learned knowledge. However, in other scenarios the opposite is true—animals must learn certain behaviors when it is disadvantageous to have a specific innate behavior. In these situations, learning evolves in the species.		In a changing environment, an animal must constantly gain new information to survive. However, in a stable environment, this same individual needs to gather the information it needs once, and then rely on it for the rest of its life. Therefore, different scenarios better suit either learning or innate knowledge. Essentially, the cost of obtaining certain knowledge versus the benefit of already having it determines whether an animal evolved to learn in a given situation, or whether it innately knew the information. If the cost of gaining the knowledge outweighes the benefit of having it, then the animal does not evolve to learn in this scenario—but instead, non-learning evolves. However, if the benefit of having certain information outweighs the cost of obtaining it, then the animal is far more likely to evolve to have to learn this information.[45]		Non-learning is more likely to evolve in two scenarios. If an environment is static and change does not or rarely occurs, then learning is simply unnecessary. Because there is no need for learning in this scenario—and because learning could prove disadvantageous due to the time it took to learn the information—non-learning evolves. However, if an environment is in a constant state of change, then learning is disadvantageous. Anything learned is immediately irrelevant because of the changing environment.[45] The learned information no longer applies. Essentially, the animal would be just as successful if it took a guess as if it learned. In this situation, non-learning evolves. In fact, a study of Drosophila melanogaster showed that learning can actually lead to a decrease in productivity, possibly because egg-laying behaviors and decisions were impaired by interference from the memories gained from the new learned materials or because of the cost of energy in learning.[46]		However, in environments where change occurs within an animal's lifetime but is not constant, learning is more likely to evolve. Learning is beneficial in these scenarios because an animal can adapt to the new situation, but can still apply the knowledge that it learns for a somewhat extended period of time. Therefore, learning increases the chances of success as opposed to guessing.[45] An example of this is seen in aquatic environments with landscapes subject to change. In these environments, learning is favored because the fish are predisposed to learn the specific spatial cues where they live.[47]		Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages.		
The Massachusetts Institute of Technology (MIT) is a private research university in Cambridge, Massachusetts, often cited as one of the world's most prestigious universities.[10][11][12][13]		Founded in 1861 in response to the increasing industrialization of the United States, MIT adopted a European polytechnic university model and stressed laboratory instruction in applied science and engineering. Researchers worked on computers, radar, and inertial guidance during World War II and the Cold War. Post-war defense research contributed to the rapid expansion of the faculty and campus under James Killian. The current 168-acre (68.0 ha) campus opened in 1916 and extends over 1 mile (1.6 km) along the northern bank of the Charles River basin.		The Institute is traditionally known for its research and education in the physical sciences and engineering, and more recently in biology, economics, linguistics, and management as well. MIT is a member of the Association of American Universities (AAU) and founder of the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute). For several years, MIT's School of Engineering has been ranked first in various international and national university rankings, and the Institute is also often ranked among the world's top universities overall.[10][11][12][13][14] The "Engineers" compete in 31 sports, most teams of which compete in the NCAA Division III's New England Women's and Men's Athletic Conference; the Division I rowing programs compete as part of the EARC and EAWRC.		As of 2015[update], 85 Nobel laureates, 52 National Medal of Science recipients, 65 Marshall Scholars, 45 Rhodes Scholars, 38 MacArthur Fellows, 34 astronauts, 19 Turing award winners, 16 Chief Scientists of the U.S. Air Force, and 6 Fields Medalists have been affiliated with MIT. The school has a strong entrepreneurial culture, and the aggregated revenues of companies founded by MIT alumni would rank as the eleventh-largest economy in the world.[15][16]						In 1859, a proposal was submitted to the Massachusetts General Court to use newly filled lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed.[18][19] A charter for the incorporation of the Massachusetts Institute of Technology, proposed by William Barton Rogers, was signed by the governor of Massachusetts on April 10, 1861.[20]		Rogers, a professor from the University of Virginia, wanted to establish an institution to address rapid scientific and technological advances.[21][22] He did not wish to found a professional school, but a combination with elements of both professional and liberal education,[23] proposing that:		The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws.[24]		The Rogers Plan reflected the German research university model, emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories.[25][26]		Two days after the charter was issued, the first battle of the Civil War broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865.[27] The new institute was founded as part of the Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes", and was a land-grant school.[28][29] In 1863 under the same act, the Commonwealth of Massachusetts founded the Massachusetts Agricultural College, which developed as the University of Massachusetts Amherst. In 1866, the proceeds from land sales went toward new buildings in the Back Bay.[30]		MIT was informally called "Boston Tech".[30] The institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date.[31] Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President Francis Amasa Walker.[32] Programs in electrical, chemical, marine, and sanitary engineering were introduced,[33][34] new buildings were built, and the size of the student body increased to more than one thousand.[32]		The curriculum drifted to a vocational emphasis, with less focus on theoretical science.[35] The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these "Boston Tech" years, MIT faculty and alumni rebuffed Harvard University president (and former MIT faculty) Charles W. Eliot's repeated attempts to merge MIT with Harvard College's Lawrence Scientific School.[36] There would be at least six attempts to absorb MIT into Harvard.[37] In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually the MIT Corporation approved a formal agreement to merge with Harvard, over the vehement objections of MIT faculty, students, and alumni.[37] However, a 1917 decision by the Massachusetts Supreme Judicial Court effectively put an end to the merger scheme.[37]		In 1916, the MIT administration and the MIT charter crossed the Charles River on the ceremonial barge Bucentaur built for the occasion,[38][39] to signify MIT's move to a spacious new campus largely consisting of filled land on a mile-long tract along the Cambridge side of the Charles River.[40][41] The neoclassical "New Technology" campus was designed by William W. Bosworth[42] and had been funded largely by anonymous donations from a mysterious "Mr. Smith", starting in 1912. In January 1920, the donor was revealed to be the industrialist George Eastman of Rochester, New York, who had invented methods of film production and processing, and founded Eastman Kodak. Between 1912 and 1920, Eastman donated $20 million ($236.6 million in 2015 dollars) in cash and Kodak stock to MIT.[43]		In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios.[44] The Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering."[45] Unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants for its funding.[46] The school was elected to the Association of American Universities in 1934.[47]		Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities.[48][49] The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs.[50][51] The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.[52]		MIT's involvement in military science surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT.[53] Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area.[54] Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory;[55][56] the development of a digital computer for flight simulations under Project Whirlwind;[57] and high-speed and high-altitude photography under Harold Edgerton.[58][59] By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush),[53] employing nearly 4000 in the Radiation Laboratory alone[54] and receiving in excess of $100 million ($1.2 billion in 2015 dollars) before 1946.[45] Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.[60]		These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquility of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities.[62] The faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948; James Rhyne Killian, president from 1948 to 1957; and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government.[63]		In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research.[64][65] In this period MIT's various departments were researching helicopters, smart bombs and counterinsurgency techniques for the war in Vietnam as well as guidance systems for nuclear missiles.[66] The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems.[67] MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the MIT Lincoln Laboratory facility in 1973 in response to the protests.[68][69] The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities.[64] Johnson was seen to be highly successful in leading his institution to "greater strength and unity" after these times of turmoil.[70] However six MIT students were sentenced to prison terms at this time and some former student leaders, such as Michael Albert and George Katsiaficas, are still indignant about MIT's role in military research and its suppression of these protests.[71] (Richard Leacock's film, November Actions, records some of these tumultuous events.[72])		In the 1980s, there was more controversy at MIT over its involvement in SDI (space weaponry) and CBW (chemical and biological warfare) research.[73] More recently, MIT’s research for the military has included work on robots, drones and ‘battle suits’.[74]		MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies,[75][76] students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like Spacewar! and created much of modern hacker slang and culture.[77] Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology;[78] the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee;[79] the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002;[80] and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.[81]		MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs.[82][83] Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center.[84] Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest.[85][86] In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.[87]		In 2001, inspired by the open source and open access movements,[88] MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabuses, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed.[89] While the cost of supporting and hosting the project is high,[90] OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages.[91] In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee.[92] The "edX" online platform supporting MITx was initially developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities have already joined and added their own course content.[93]		Three days after the Boston Marathon bombing of April 2013, MIT Police patrol officer Sean Collier was fatally shot by the suspects Dzhokhar and Tamerlan Tsarnaev, setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day.[94] One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada.[95][96][97] On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to "an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life". The announcement further stated that "Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness".[98][99][100]		MIT's 168-acre (68.0 ha) campus in the city of Cambridge spans approximately a mile along the north side of the Charles River basin.[6] The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length – the smoot.[101][102]		The Kendall MBTA Red Line station is located on the northeastern edge of the campus, in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings, as well as socio-economically diverse residential neighborhoods.[103][104] In early 2016, MIT presented its updated Kendall Square Initiative to the City of Cambridge, with plans for mixed-use educational, retail, residential, startup incubator, and office space in a dense high-rise transit-oriented development plan.[105][106] The MIT Museum will eventually be moved immediately adjacent to a Kendall Square subway entrance, joining the List Visual Arts Center on the eastern end of the campus.[106][107]		Each building at MIT has a number (possibly preceded by a W, N, E, or NW) designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings.[108] Many of the buildings are connected above ground as well as through an extensive network of underground tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.[109][110]		MIT's on-campus nuclear reactor[111] is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial,[112] but MIT maintains that it is well-secured.[113] In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building", and designed by architect Frank Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.[114]		Other notable campus facilities include a pressurized wind tunnel and a towing tank for testing ship and ocean structure designs.[115][116] MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering 9,400,000 square feet (870,000 m2) of campus.[117]		In 2001, the Environmental Protection Agency sued MIT for violating the Clean Water Act and the Clean Air Act with regard to its hazardous waste storage and disposal procedures.[118] MIT settled the suit by paying a $155,000 fine and launching three environmental projects.[119] In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.[120]		The MIT Police with state and local authorities, in the 2009-2011 period, have investigated reports of 12 forcible sex offenses, 6 robberies, 3 aggravated assaults, 164 burglaries, 1 case of arson, and 4 cases of motor vehicle theft on campus; affecting a community of around 22,000 students and employees.[121]		MIT has substantial commercial real estate holdings in Cambridge on which it pays property taxes, plus an additional voluntary payment in lieu of taxes (PILOT) on academic buildings which are legally tax-exempt. As of 2017[update], it is the largest taxpayer in the city, contributing approximately 14% of the city's annual revenues.[122] Holdings include Technology Square, parts of Kendall Square, and many properties in Cambridgeport and Area 4 neighboring the educational buildings.[123] The land is held for investment purposes and potential long-term expansion.		MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States,[124] and it has a history of commissioning progressive buildings.[125][126] The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial – much less university – building in the US.[127] Bosworth's design was influenced by the City Beautiful Movement of the early 1900s,[127] and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where graduation ceremonies are held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers.[a] The spacious Building 7 atrium at 77 Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.[104]		Alvar Aalto's Baker House (1947), Eero Saarinen's MIT Chapel and Kresge Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture.[130][131][132] More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005), and Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture".[125][133] These buildings have not always been well received;[134][135] in 2010, The Princeton Review included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".[136]		Undergraduates are guaranteed four-year housing in one of MIT's 12 undergraduate dormitories.[137] Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters.[138] Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the Yale Daily News staff's The Insider's Guide to the Colleges, 2010, "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture."[139] MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.[140]		MIT has an active Greek and co-op housing system, including thirty-six fraternities, sororities, and independent living groups (FSILGs).[141] As of 2015[update], 98% of all undergraduates lived in MIT-affiliated housing; 54% of the men participated in fraternities and 20% of the women were involved in sororities.[142] Most FSILGs are located across the river in Back Bay near where MIT was founded, and there is also a cluster of fraternities on MIT's West Campus that face the Charles River Basin.[143] After the 1997 alcohol-related death of Scott Krueger, a new pledge at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002.[144] Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy could not be implemented until Simmons Hall opened in that year.[145]		MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation.[146] The current board consists of 43 members elected to five-year terms,[147] 25 life members who vote until their 75th birthday,[148] 3 elected officers (President, Treasurer, and Secretary),[149] and 4 ex officio members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court).[150][151] The board is chaired by Robert Millard, a co-founder of L-3 Communications Holdings.[152][153] The Corporation approves the budget, new programs, degrees and faculty appointments, and elects the President to serve as the chief executive officer of the university and preside over the Institute's faculty.[104][154] MIT's endowment and other financial assets are managed through a subsidiary called MIT Investment Management Company (MITIMCo).[155] Valued at $13.182 billion in 2016, MIT's endowment is the sixth-largest among American colleges and universities.[3]		MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Whitaker College of Health Sciences and Technology), but no schools of law or medicine.[156][b] While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs,[158] the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President.[159] The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.[160][161]		MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs.[162] The university has been accredited by the New England Association of Schools and Colleges since 1929.[163][164] MIT operates on a 4–1–4 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester beginning in early February and ending in late May.[165]		MIT students refer to both their majors and classes using numbers or acronyms alone.[166] Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is Course 1, while Linguistics and Philosophy is Course 24.[167] Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; the introductory calculus-based classical mechanics course is simply "8.01" at MIT.[168][c]		The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and has been dubbed "most selective" by U.S. News,[171] admitting few transfer students[162] and 8.0% of its applicants in the 2015 admissions cycle.[172] MIT offers 44 undergraduate degrees across its five schools.[173] In the 2010–2011 academic year, 1,161 bachelor of science degrees (abbreviated "SB") were granted, the only type of undergraduate degree MIT now awards.[needs update][174][175] In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%).[needs update] The largest undergraduate degree programs were in Electrical Engineering and Computer Science (Course 6–2), Computer Science and Engineering (Course 6–3), Mechanical Engineering (Course 2), Physics (Course 8), and Mathematics (Course 18).[169]		All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs).[176] The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive",[177] including "substantial instruction and practice in oral presentation".[178] Finally, all students are required to complete a swimming test;[179] non-varsity athletes must also take four quarters of physical education classes.[176]		Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and periodic quizzes or tests. While the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose",[180][181] the freshmen retention rate at MIT is similar to other research universities.[171] The "pass/no-record" grading system relieves some pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded.[182] (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.[183]) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.[182]		In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly.[184] A substantial majority of undergraduates participate.[185][186] Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.[187][188]		In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published The Hidden Curriculum, arguing that education at MIT was often slighted in favor of following a set of unwritten expectations, and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest.[189][190]		MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees.[162] The Institute offers graduate programs leading to academic degrees such as the Master of Science (MS), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD) and interdisciplinary graduate programs such as the MD-PhD (with Harvard Medical School).[191][192]		Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs).[193]		MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 2010–11.[needs update][174] In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%),[d] and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.[169]		MIT also places among the top ten in many overall rankings of universities (see right) and rankings based on students' revealed preferences.[202][203][204] For several years, U.S. News & World Report, the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report.[205] In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, architecture, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.[10][11][12][13][14]		In 2014, Money magazine ranked MIT as third in the US "Best Colleges for Your Money", based on its assessment of "the most bang for your tuition buck", factoring in quality of education, affordability, and career outcomes.[206] As of 2014[update], Forbes magazine rated MIT as the second "Most Entrepreneurial University", based on the percentage of alumni and students self-identifying as founders or business owners on LinkedIn.[207] In 2015, Brookings Fellow Jonathan Rothwell issued a report "Beyond College Rankings", placing MIT as third in the US, with an estimated 45% value-added to mid-career salary.[208]		Times Higher Education has recognized MIT as one of the world's "six super brands" on its World Reputation Rankings, along with Berkeley, Cambridge, Harvard, Oxford and Stanford.[209]		The university historically pioneered research and training collaborations between academia, industry and government.[210][211]  In 1946, President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research and Development Corporation, the first American venture-capital firm.[212][213]  In 1948, Compton established the MIT Industrial Liaison Program.[214] Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international – especially Japanese – firms that were competing with struggling American businesses.[215][216] On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as presidential scientific advisers since 1940.[e] MIT established a Washington Office in 1991 to continue effective lobbying for research funding and national science policy.[218][219]		The U.S. Justice Department began an investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing during their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships.[220][221] While the Ivy League institutions settled,[222] MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students.[223][224] MIT ultimately prevailed when the Justice Department dropped the case in 1994.[225][226]		MIT's proximity[f] to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and the Broad Institute.[227] In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees.[227] A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the Cambridge–MIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge.[227] MIT has more modest cross-registration programs with Boston University, Brandeis University, Tufts University, Massachusetts College of Art, and the School of the Museum of Fine Arts, Boston.[227]		MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the Charles Stark Draper Laboratory, the Whitehead Institute for Biomedical Research, and the Woods Hole Oceanographic Institution. Ongoing international research and educational collaborations include the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute), Singapore-MIT Alliance, MIT-Politecnico di Milano,[227][228] MIT-Zaragoza International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program.[227][229]		The mass-market magazine Technology Review is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine.[230][231] The MIT Press is a major university press, publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events, and social issues.[232]		The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries.[233] Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music,[234] the List Visual Arts Center's rotating exhibitions of contemporary art,[235] and the Compton Gallery's cross-disciplinary exhibitions.[236] MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.[237][238]		The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and history of MIT. The museum now engages in significant educational outreach programs for the general public, including the annual Cambridge Science Festival, the first celebration of this kind in the United States. Since 2005, its official mission has been, "to engage the wider community with MIT's science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century".[239]		MIT was elected to the Association of American Universities in 1934 and remains a research university with a very high level of research activity;[47][162] research expenditures totaled $718.2 million in 2009.[needs update][240] The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million.[240] MIT employs approximately 1300 researchers in addition to faculty.[241] In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties.[242] Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.[243]		In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers.[244][245] Harold Eugene Edgerton was a pioneer in high speed photography and sonar.[246][247] Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory.[248] In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography.[245][249] At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.[250][251]		Current and previous physics faculty have won eight Nobel Prizes,[252] four Dirac Medals,[253] and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory.[254] Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods.[252] MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology.[252] Professor Eric Lander was one of the principal leaders of the Human Genome Project.[255][256] Positronium atoms,[257] synthetic penicillin,[258] synthetic self-replicating molecules,[259] and the genetic bases for Amyotrophic lateral sclerosis (also known as ALS or Lou Gehrig's disease) and Huntington's disease were first discovered at MIT.[260] Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain".[261] Researchers developed a system to convert MRI scans into 3D printed physical models.[262]		In the domain of humanities, arts, and social sciences, MIT economists have been awarded five Nobel Prizes and nine John Bates Clark Medals.[252][263] Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology.[264][265] The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research,[266][267] has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.[268]		Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 38 people associated with MIT.[269] Four Pulitzer Prize–winning writers currently work at or have retired from MIT.[270] Four current or former faculty are members of the American Academy of Arts and Letters.[271]		Allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991.[272][273] Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed.[274][275] Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.[276][277]		[290][291]		The faculty and student body place a high value on meritocracy and on technical proficiency.[292][293] MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation.[294] However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.[295]		Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat".[296][297] Originally created in 1929, the ring's official name is the "Standard Technology Ring."[298] The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver.[296] The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise," "Institute Has The Finest Professors," "It's Hard to Fondle Penguins," and other variations, has occasionally been featured on the ring given its historical prominence in student culture.[299]		MIT has over 500 recognized student activity groups,[300] including a campus radio station, The Tech student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.[301]		The Independent Activities Period is a four-week-long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are the 6.270, 6.370, and MasLab competitions,[302] the annual "mystery hunt",[303] and Charm School.[304][305] More than 250 students pursue externships annually at companies in the US and abroad.[306][307]		Many MIT students also engage in "hacking", which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes.[308][309] Recent high-profile hacks have included the abduction of Caltech's cannon,[310] reconstructing a Wright Flyer atop the Great Dome,[311] and adorning the John Harvard statue with the Master Chief's Mjölnir Helmet.[312]		MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs.[313][314]  MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, NCAA's Division I Eastern Association of Women's Rowing Colleges (EAWRC) for women's crew, and the Collegiate Water Polo Association (CWPA) for Men's Water Polo. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). In April 2009, budget cuts led to MIT eliminating eight of its 41 sports, including the mixed men's and women's teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men's programs in golf and wrestling.[315][316]		MIT enrolled 4,384 undergraduates and 6,510 graduate students in 2011–2012.[needs update][169] Women constituted 45 percent of undergraduate students.[needs update][169][319] Undergraduate and graduate students were drawn from all 50 states as well as 115 foreign countries.[320]		MIT received 17,909 applications for admission to the undergraduate Class of 2015; 1,742 were admitted (9.7 percent) and 1128 enrolled (64.8 percent).[needs update][142] 19,446 applications were received for graduate and advanced degree program across all departments; 2,991 were admitted (15.4 percent) and 1,880 enrolled (62.8 percent).[needs update][321]		The interquartile range on the SAT was 2090–2340 and 97 percent of students ranked in the top tenth of their high school graduating class.[needs update][142] 97 percent of the Class of 2012 returned as sophomores; 82 percent of the Class of 2007 graduated within 4 years, and 93 percent (91 percent of the men and 95 percent of the women) graduated within 6 years.[142][322]		Undergraduate tuition and fees total $40,732 and annual expenses are estimated at $52,507 as of 2012.[needs update] 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student.[needs update][323] Students were awarded a total of $102 million in scholarships and grants, primarily from institutional support ($84 million).[142] The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".[324]		MIT has been nominally co-educational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry.[325][326] Female students remained a minority prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1963.[327][328][329] Between 1993 and 2009, the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students.[169][330] Women currently outnumber men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning, and Biological Engineering.[169][319]		A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention to MIT's culture and student life.[331][332] After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity,[333] MIT began requiring all freshmen to live in the dormitory system.[333][334] The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate.[335][336] In late 2001 a task force's recommended improvements in student mental health services were implemented,[337][338] including expanding staff and operating hours at the mental health center.[339] These and later cases were significant as well because they sought to prove the negligence and liability of university administrators in loco parentis.[335]		As of 2013[update], MIT had 1,030 faculty members, of whom 225 were women.[4] Faculty are responsible for lecturing classes, advising both graduate and undergraduate students, and sitting on academic committees, as well as conducting original research. Between 1964 and 2009, a total of seventeen faculty and staff members affiliated with MIT were awarded Nobel Prizes (thirteen in the last 25 years).[340] MIT faculty members past or present have won a total of twenty-seven Nobel Prizes, the majority in Economics or Physics.[341] As of October 2013[update], among current faculty and teaching staff there are 67 Guggenheim Fellows, 6 Fulbright Scholars, and 22 MacArthur Fellows.[4] Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.		A 1998 MIT study concluded that a systemic bias against female faculty existed in its School of Science,[342] although the study's methods were controversial.[343][344] Since the study, though, women have headed departments within the Schools of Science and of Engineering, and MIT has appointed several female vice presidents, although allegations of sexism continue to be made.[345] Susan Hockfield, a molecular neurobiologist, was MIT's president from 2004 to 2012 and was the first woman to hold the post.[161]		Tenure outcomes have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble, a historian of technology, became a cause célèbre about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military.[346] Former materials science professor Gretchen Kalonji sued MIT in 1994 alleging that she was denied tenure because of sexual discrimination. Several years later, the lawsuit was settled with undisclosed payments, and establishment of a project to encourage women and minorities to seek faculty positions.[345][347][348] In 1997, the Massachusetts Commission Against Discrimination issued a probable cause finding supporting UMass Boston Professor James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure.[349]		In 2006–2007, MIT's denial of tenure to African-American stem cell scientist professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger strike, and the resignation of Professor Frank L. Douglas in protest.[350][351] The Boston Globe reported on February 6, 2007: "Less than half of MIT's junior faculty members are granted tenure. After Sherley was initially denied tenure, his case was examined three times before the university established that neither racial discrimination nor conflict of interest affected the decision. Twenty-one of Sherley's colleagues later issued a statement saying that the professor was treated fairly in tenure review."[352]		MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty member Charles W. Eliot was recruited in 1869 to become president of Harvard University, a post he would hold for 40 years, during which he wielded considerable influence on both American higher education and secondary education. MIT alumnus and faculty member George Ellery Hale played a central role in the development of the California Institute of Technology (Caltech), and other faculty members have been key founders of Franklin W. Olin College of Engineering in nearby Needham, Massachusetts.		As of 2014[update], former provost Robert A. Brown is president of Boston University; former provost Mark Wrighton is chancellor of Washington University in St. Louis; former associate provost Alice Gast is president of Lehigh University; and former professor Suh Nam-pyo is president of KAIST. Former dean of the School of Science Robert J. Birgeneau was the chancellor of the University of California, Berkeley (2004–2013); former professor John Maeda was president of Rhode Island School of Design (RISD, 2008–2013); former professor David Baltimore was president of Caltech (1997–2006); and MIT alumnus and former assistant professor Hans Mark served as chancellor of the University of Texas system (1984–1992).		In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt is president of the National Academy of Sciences,[353] urban studies professor Xavier de Souza Briggs is currently the associate director of the White House Office of Management and Budget,[354] and biology professor Eric Lander was a co-chair of the President's Council of Advisors on Science and Technology.[355] In 2013, faculty member Ernest Moniz was nominated by President Obama and later confirmed as United States Secretary of Energy.[356][357] Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense.		As of 2017[update], MIT was the second-largest employer in the city of Cambridge.[122] Based on feedback from employees, MIT was ranked #7 as a place to work, among US colleges and universities as of 2013[update].[358] Surveys cited a "smart", "creative", "friendly" environment, noting that the work-life balance tilts towards a "strong work ethic" but complaining about "low pay" compared to an industry position.[359]		Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. As of 2014[update], 27 MIT alumni have won the Nobel Prize, 47 have been selected as Rhodes Scholars, and 61 have been selected as Marshall Scholars.[360]		Alumni in American politics and public service include former Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, former CA-13 Representative Pete Stark, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairwoman Christina Romer. MIT alumni in international politics include Foreign Affairs Minister of Iran Ali Akbar Salehi, Israeli Prime Minister Benjamin Netanyahu, President of Colombia Virgilio Barco Vargas, President of the European Central Bank Mario Draghi, former Governor of the Reserve Bank of India Raghuram Rajan, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, former Iraqi Deputy Prime Minister Ahmed Chalabi, former Minister of Education and Culture of The Republic of Indonesia Yahya Muhaimin.		MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, The Guardian, "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year. If MIT were a country, it would have the 11th highest GDP of any nation in the world."[361][362][363]		Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, New York Institute of Technology, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), Northeastern University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnológico de Monterrey, Purdue University, Virginia Polytechnic Institute, KAIST, and Quaid-e-Azam University. Berklee College of Music, the largest independent college of contemporary music in the world, was founded and led by MIT alumnus Lawrence Berk for more than three decades.		More than one third of the United States' manned spaceflights have included MIT-educated astronauts (among them Apollo 11 Lunar Module Pilot Buzz Aldrin), more than any university excluding the United States service academies.[364] Alumnus and former faculty member Qian Xuesen was instrumental in the PRC rocket program.[365]		Noted alumni in non-scientific fields include author Hugh Lofting,[366] sculptor Daniel Chester French, guitarist Tom Scholz of the band Boston, the British BBC and ITN correspondent and political advisor David Walter, The New York Times columnist and Nobel Prize Winning economist Paul Krugman, The Bell Curve author Charles Murray, United States Supreme Court building architect Cass Gilbert,[367] Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.		Apollo 11 astronaut Buzz Aldrin, ScD 1963 (Aero & Astro)		Former UN Secretary-General Kofi Annan, SM 1972 (Management)		President of Colombia 1986-1990 Virgilio Barco Vargas, SB 1943 (Civil Engineering)		Former Federal Reserve Bank chairman Ben Bernanke, PhD 1979 (Economics)		Physicist Nobel laureate Richard Feynman, SB 1939 (Physics)		Economics Nobel laureate Paul Krugman, PhD 1977 (Economics)		Biologist, suffragist, philanthropist Katherine Dexter McCormick (left), SB 1904 (Biology)		Challenger astronaut and physicist Ronald McNair, PhD 1976 (Physics)		Israeli Prime Minister Benjamin Netanyahu, SB 1975 (Architecture), SM 1976 (Management)		Architect I. M. Pei, BArch 1940 (Architecture)		CEO of General Motors Alfred P. Sloan, SB 1895 (Electrical Engineering)		"Boston" guitarist Tom Scholz, SB 1969, SM 1970 (Mechanical Engineering)				Coordinates: 42°21′35″N 71°05′32″W﻿ / ﻿42.35982°N 71.09211°W﻿ / 42.35982; -71.09211		
School discipline is a required set of actions by a teacher towards a student (or groups of students) after the student's behavior disrupts the ongoing educational activity or breaks a pre-established rule created by the school system. Discipline guides the children's behaviour or sets limits to help them learn to take care of themselves, other people and the world around them.[1]		School systems set rules, and if students break these rules they are subject to discipline. These rules may, for example, define the expected standards of clothing, timekeeping, social conduct, and work ethic. The term 'discipline is applied to the punishment that is the consequence of breaking the rules. The aim of discipline is to set limits restricting certain behaviors or attitudes that are seen as harmful or going against school policies, educational norms, school traditions, etc.[1] The focus of discipline is shifting and alternative approaches are emerging due to notably high dropout rates and disproportionate punishment upon minority students. Discipline often has a negative connotation, but discipline can be a positive way of instilling community values upon youth.						Disciplining children is important to create a safe and fun learning environment. Discipline requires knowledge, skill, sensitivity and self-confidence; like any art, it is something that one will acquire through training and experience; it becomes easier with practice. Many people confuse discipline with classroom management; discipline is one dimension of classroom management and classroom management is a general term.[2] Discipline can also have a positive influence on both the individual and classroom environment. Utilizing disciplinary actions can be an opportunity to reflect and learn about consequences, instill collective values, and encourage behavior that is acceptable for the classroom. Recognition of the diversity of values within communities can increase understanding and tolerance of different disciplinary techniques.[3] Promoting positive correction of questionable behavior within the classroom dynamic, as opposed to out-of-class punishments like detention, suspension, or expulsion, can encourage learning and discourage future misbehavior.[4] Learning to own one’s bad behavior can also contribute to positive growth in social emotional learning.[5]		Discipline is a set of actions determined by the school district to remedy actions taken by a student that are deemed inappropriate. Some scholars think students misbehave because of the lack of engagement and stimulation in typical school settings, a rigid definition of acceptable behaviors and/or a lack of attention and love in a student's personal life. Recently, scholars have begun to explore alternative explanations for why students are being disciplined, in particular the disproportionate rate of discipline towards African American and Minority students.		School discipline practices are generally informed by theory from psychologists and educators. There are a number of theories to form a comprehensive discipline strategy for an entire school or a particular class.		Detention is one of the most common punishments in schools in the United States, the United Kingdom, Ireland, Singapore, Canada, Australia, New Zealand, South Africa and some other countries. It requires the pupil to report to a designated area of the school during a specified time on a school day (typically either recess or after school) and remain there for a specified period of time, but also may require a pupil to report to that part of school at a certain time on a non-school day, e.g. "Saturday detention" at some US, UK, and Irish schools (especially for serious offenses not quite serious enough for suspension). [clarification needed]		Typically, in schools in the US, UK, and Singapore, if one misses a detention, then another is added or the student gets a more serious punishment. In UK schools, for offenses too serious for a normal detention but not serious enough for a detention requiring the pupil to return to school at a certain time on a non-school day, a detention can require a pupil to return to school 1–2 hours after school ends on a school day, e.g. "Friday Night Detention".[17]		In Germany detention is less common. In some states like Baden-Württemberg there is detention to rework missed school hours, but in others like Rheinland-Pfalz it is prohibited by law. In schools where some classes are held on Saturdays, pupils may get detention on a Saturday even if it is a non-school day for them.		In China, long-time detention is less common than in the US, the UK, Ireland, Singapore, Canada, Australia, New Zealand, South Africa and some other countries. Short-time detention by schoolteachers is still common, but usually lasts no more than 3 to 5 hours.[citation needed]		In Australia,[18] the policy for school detention: the principal must consider circumstances when determining what a reasonable time and place for detention entails and make sure that any special conditions relating to the imposition of detention are specified in the school's 'Student Engagement Policy'. The conditions that schools must ensure are that: no more than half the time for recess is used for detention, when students are kept after school, parents should be informed at least the day before detention, and detention should not exceed 45 minutes.[19]		Counseling is also provided when a kid/teen/adult will have to see a school counselor if they behave badly. The purpose of counseling is to help the student recognize their mistakes and find positive ways to make changes in the student’s life. Counseling can also help the student clarify the expectations and standards of the school, as well as understand the consequences of failing to meet those standards.		Suspension or temporary exclusion is mandatory leave assigned to a student as a form of punishment that can last anywhere from one day to a few weeks, during which time the student is not allowed to attend regular lessons. In some US, UK, Australian and Canadian schools, there are two types of suspension: In-School (ISS, Internal Exclusion or Isolation) and Out-of-School (OSS, Off-Campus Suspension, External Exclusion). In-school requires the student to report to school as usual but attend a designated suspension classroom or room all day.[20] Out-of-school suspension bans the student from being on school grounds during school hours while school is in session. Students who breach a suspension by attending school may be arrested for, and charged with trespassing. This could result in an extension of suspension, community service, and sometimes jail time. Students who continue to breach a suspension could be expelled from school and sentenced to longer, more severe punishments. Students are also not allowed to attend after school activities (such as proms, sporting events, etc.) while suspended from school.[21] Schools are often required to notify the student's parents/guardians of the reason for and duration of the out-of-school suspension, and usually also for in-school suspensions.[22] Suspended students are often required to continue to learn and complete assignments from the days in which they miss instruction.[22]		Throughout the history of education the most common means of maintaining discipline in schools was corporal punishment. While a child was in school, a teacher was expected to act as a substitute parent, with many forms of parental discipline or rewards open to them. This often meant that students were commonly chastised with the birch, cane, paddle, strap or yardstick if they did something wrong.		Corporal punishment in schools has now disappeared from most Western countries, including all European countries. In the United States, corporal punishment is not used in public schools in 34 states, banned in 31, permitted in 19, of which only 16 actually have school districts actively administering corporal punishment. Every U.S. state except New Jersey and Iowa permits corporal punishment in private schools, however an increasing number of private schools have abandoned the practice, especially Catholic schools, nearly all of which now ban. Thirty-one U.S. states as well as the District of Columbia have banned it from public schools, most recently New Mexico in 2011. The other 19 states (mostly in the South) continue to allow corporal punishment in public schools. Of the 19 which permit the practice, three - Arizona, Colorado, and Wyoming have no public schools which actually use corporal punishment as of 2016. Paddling is still used to a significant (though declining) degree in some public schools in Alabama, Arkansas, Georgia, Louisiana, Mississippi, Missouri, Oklahoma, Tennessee and Texas. Private schools in these and most other states may also use it, though many choose not to do so.		Official corporal punishment, often by caning, remains commonplace in schools in some Asian, African and Caribbean countries.		Most mainstream schools in most other countries retain punishment for misbehavior, but it usually takes non-corporal forms such as detention and suspension.		In China, school corporal punishment was completely banned under the Article 29 of the Compulsory Education Act of the People's Republic of China, but in practice, beating by schoolteachers is still common, especially in rural areas.		In Australia, school corporal punishment has been banned in most states.		Expulsion, exclusion, withdrawing, or permanent exclusion terminates the student's education. This is the ultimate last resort, when all other methods of discipline have failed. However, in extreme situations, it may also be used for a single offense.[24] Some education authorities have a nominated school in which all excluded students are collected; this typically has a much higher staffing level than mainstream schools. In some US public schools, expulsions and exclusions are so serious that they require an appearance before the Board of Education or the court system. In the UK, head teachers may make the decision to exclude, but the student's parents have the right of appeal to the local education authority. It was completely banned for compulsory schools in China. This has proved controversial in cases where the head teacher's decision has been overturned (and his or her authority thereby undermined), and there are proposals to abolish the right of appeal. In the United States, when it comes to student discipline, there is a marked difference in procedure between public and private institutions. With public schools, the school must provide the student with constitutional due process protections as public educational institutions operate as an extension of state governments. With private schools, on the other hand, the student can be expelled for any reason – so long as the expulsion was not “arbitrary and capricious.” Generally, as long as a private school follows the procedures in its student handbook, a court will not view its actions as arbitrary and capricious.[25] Expulsion from a private school is a more straightforward matter, since the school can merely terminate its contract with the parents if the pupil does not have siblings in the same school.		In schools, restorative justice is an offshoot of the model used by some courts and law enforcement; it seeks to repair the harm that has been done by acknowledging the impact on the victim, community, and offender, accepting responsibility for the wrongdoing, and repairing the harm that was caused. Restorative practices can “also include preventive measures designed to build skills and capacity in students as well as adults." Some examples of preventative measures in restorative practices might include teachers and students devising classroom expectations together or setting up community building in the classroom. Restorative justice also focuses on justice as needs and obligations, expands justice as conversations between the offender, victim and school, and recognizes accountability as understanding the impact of actions and repairing the harm. Traditional styles of discipline do not always work well for students across every cultural community. As an alternative to the normative approaches of corporal punishment, detention, counseling, suspension, and expulsion, restorative justice was established to give students a voice in their consequences, as well as an opportunity to make a positive contribution to their community.[26] This method of discipline typically involves peer-mediation or adult-supervised conversations surrounding a perceived offence. Each student has the ability to contribute to the conversation, the person who has misbehaved has the opportunity not only to give their side of the story but also has a say in their consequence. Consequences defy the traditional methods of punitive punishment and instead give students an opportunity for restoration.[27] Restorative justice focuses on relationship building and the community as a whole over the individual student and their offence, creating a sense that everyone has a part in the community and it is everyone’s responsibility to uphold the values of the particular community.[28] This is a method that not only increases an understanding of perceived community values, but is also a method thought to work well in cultures and communities where there is a high value on the community, rather than just on the individual.		
Sophism is a method of teaching. In ancient Greece, sophists were a category of teachers who specialized in using the techniques of philosophy and rhetoric for the purpose of teaching arete—"excellence" or "virtue"—predominantly to young statesmen and nobility. The practice of charging money for education and providing wisdom only to those who could pay led to the condemnations made by Socrates (as he is portrayed by Plato in his dialogues) as well as Xenophon's Memorabilia. Through works such as these, Sophists were portrayed as "specious" or "deceptive", hence the modern meaning of the term.		The term originated from Greek σόφισμα, sophisma, from σοφίζω, sophizo "I am wise"; confer σοφιστής, sophistēs, meaning "wise-ist, one who does wisdom," and σοφός, sophós means "wise man".						The Greek word sophist (sophistēs) derives from the words sophia and sophos, which had meant "wisdom" or “wise” (respectively) since the time of Homer. The words were originally used to describe expertise in a particular knowledge or craft.[1] Gradually, however, the word also came to denote general wisdom and especially wisdom about human affairs (for example, in politics, ethics, or household management). This was the meaning ascribed to the Greek Seven Sages of 7th and 6th century BC (such as Solon and Thales), and it was the meaning that appeared in the histories of Herodotus. Richard Martin refers to the seven sages as "performers of political poetry."[2]		Sophists could be described both as teachers and philosophers, having traveled about in Greece teaching their students various life skills, particularly rhetoric and public speaking. These were useful qualities of the time, during which persuasive ability had a large influence on one's political power and economic wealth. Athens became the center of the sophists' activity, due to the city's freedom of speech for non slave citizens and wealth of resources. There were numerous differences among Sophist teachings, and they lectured on subjects that were as diverse as semantics and rhetoric, to ontology, epistemology. Sophists taught their beliefs for a considerable price.		What set sophists apart from other teachers in ancient Greece was their considerable wealth of knowledge and dialect in regards to rhetoric. These men held small sessions, if not one on one conversations to teach others. Sophists often had a bad reputation because they never claimed to teach virtue and were eventually let go of by later and more famous philosophers such as Aristotle and Socrates who monumentally were against the sophists.[citation needed]		Sophists were popular partly due to the nature and culture surrounding ancient Greece. Athens was a famously democratic state among those not enslaved: every free person was able to have their ideas heard in the ekklesia, an assembly made up of citizens who helped make governmental decisions. Citizens were judged on their attendance and their participation in the ekklesia and often sought out Sophists to teach them basics of rhetoric in order for their ideas to be heard and considered. The rhetorical teachings from the sophists became valued by many Athenian citizens until the fallout between the sophists and the philosophers a little later on.[citation needed]		Sophists became popular following the development of thought and society in Athens, in the fifth century B.C. They offered practical education with teachings that included speculation on the nature of the universe as well as the art of life and politics. They believed that law was an agreement between people and that justice is nonexistent. Among the Sophists, Protagoras, Gorgias, Prodicus, Hippias, Thrasymachus, Callicles, Lycophron, Antiphon, and Cratylus are the best known.[citation needed]		Early Sophists were well respected but they soon became unpopular and were subject to much opposition and controversy due to their high fees and their radical challenges to convention. The only citizens who had the money to learn from the Sophists came from the aristocratic class, meaning that many citizens were unable to learn from them. Sophists were thought to teach these aristocrats how to manipulate the public by catering to popular opinion, rather than being concerned with the truth.[3]		Before the writing of Plato, the word "sophist" could be used as either a respectful or contemptuous title, much like the word "intellectual" can be used today. It was in Plato’s dialogue, Sophist, that the first record of an attempt to answer the question “What is a Sophist?” is made. Plato described Sophists as paid hunters after the young and wealthy, as merchants of knowledge, as athletes in a contest of words, and purgers of souls. From Plato's assessment of Sophists it could be concluded that Sophists do not offer true knowledge, but only an opinion of things. Plato describes them as shadows of the true early Sophists and wrote, “...the art of contradiction making, descended from an insincere kind of conceited mimicry, of the semblance-making breed, derived from image making, distinguished as portion, not divine but human, of production, that presents, a shadow play of words—such are the blood and the lineage which can, with perfect truth, be assigned to the authentic Sophist”. Plato sought to separate the Sophist from the Philosopher. Where a Sophist was a person who makes his living through deception, a philosopher was a lover of wisdom who sought truth. To give the Philosophers greater credence, the Sophists had to receive a negative connotation.[4]		Most sophists claimed to teach arête (“excellence” or “virtue”) in the management and administration of not only one’s affairs, but the city’s as well. Before the fifth century B.C., it was believed that aristocratic birth qualified a person for arête and politics. However, Protagoras, who is regarded as the first Sophist, explained that arête is the result of training rather than birth.[citation needed]		Protagoras was one of the best-known and most successful teachers. He taught his students the necessary skills and knowledge for a successful life, particularly in politics, rather than philosophy. He trained his pupils to argue from both points of view because he believed that truth could not be limited to just one side of the argument. Protagoras wrote about a variety of subjects and some fragments of his work survived. He is the author of the famous saying, “Man is the measure of all things,” which is the opening sentence of a work called Truth.[5]		Gorgias is another well-known Sophist. Gorgias’ writings showcase his ability of making ridiculous and unpopular positions appear stronger. Gorgias authored a lost work known as On the Non-Existent, which centers on the argument that nothing exists. In it, he attempts to persuade his readers that thought and existence are different.[6]		In comparison, Socrates accepted no fee, instead professed a self-effacing posture, which he exemplified by Socratic questioning (i.e. the Socratic method, although Diogenes Laertius wrote that Protagoras the sophist invented the "Socratic" method[7][8]). His attitude towards the Sophists was by no means oppositional; in one dialogue Socrates even stated that the Sophists were better educators than he was,[9] which he validated by sending one of his students to study under a sophist.		Only portions of the Sophists’ writings have survived and they are mainly known from Plato, a philosopher who helped lay the foundations of Western philosophy and science. Plato studied philosophy under the guidance from Socrates. Plato discusses his view on the Sophists’ thought, although his attitude is generally hostile. Due to his opposition, he is largely responsible for the modern view of the sophist as a stingy instructor who deceives. He depicts Socrates as refuting some sophists in several Dialogues. These texts depict the sophists in an unflattering light, and it is unclear how accurate or fair Plato's representation of them may be; however, Protagoras and Prodicus are portrayed in a largely positive light in Protagoras (dialogue). Another contemporary, the comic playwright Aristophanes, criticizes the sophists as hairsplitting wordsmiths. Aristophanes made no distinction between sophists and philosophers as Socrates did, and believed both would argue any position for the right fee. In the comedic play The Clouds by Aristophanes, Strepsiades seeks the help of Socrates (a parody of the actual philosopher) in an effort to avoid paying his debts. In the play, Socrates promises to teach Strepsiades' son to argue his way out of paying his debts.[citation needed]		Some scholars, such as Ugo Zilioli[10] argue that the sophists held a relativistic view on cognition and knowledge. However, this may involve the Greek word "doxa," which means "culturally shared belief" rather than "individual opinion." Their philosophy contains criticism of religion, law, and ethics.		In some cases, such as Gorgias, some of his works survived, allowing the author to be judged on his own terms. In most cases, however, knowledge of sophist thought comes from fragmentary quotations that lack context. Many of these quotations come from Aristotle, who seems to have held the sophists in slight regard.[citation needed]		From the late 1st century AD the Second Sophistic, a philosophical and rhetorical movement, was the chief expression of intellectual life. The term "Second Sophistic" comes from Philostratos, who rejecting the term "New Sophistic" traced the beginnings of the movement to the orator Aeschines in the 4th century BC. But its earliest representative was really Nicetas of Smyrna, in the late 1st century AD. Unlike the original Sophistic movement of the 5th century BC, the Second Sophistic was little concerned with politics. But it was, to a large degree, to meet the everyday needs and respond to the practical problems of Greco-Roman society. It came to dominate higher education and left its mark on many forms of literature.[citation needed]		Despite the opposition from philosophers Socrates, Plato and Aristotle, it is clear that Sophists had a vast influence on a number of spheres, including the growth of knowledge and on ethical political theory. Their teachings, although controversial, had a huge influence on thought in the fifth century B.C.[citation needed] The Sophists turned away from the theoretical natural science to the more sensible[to whom?] examination of human affairs and the betterment and success of human life. They argued that divine deities could no longer be the explanation of human action.		Owing largely to the influence of Plato and Aristotle, philosophy came to be regarded as distinct from sophistry, the latter being regarded as specious and rhetorical, a practical discipline. Thus, by the time of the Roman Empire, a sophist was simply a teacher of rhetoric and a popular public speaker. For instance, Libanius, Himerius, Aelius Aristides, and Fronto were sophists in this sense.[citation needed]		The sophists' rhetorical techniques were extremely useful for any young nobleman looking for public office. The societal roles the Sophists filled had important ramifications for the Athenian political system at large. The historical context provides evidence for their considerable influence, as Athens became more and more democratic during the period in which the Sophists were most active.[11]		The Sophists certainly were not directly responsible for Athenian democracy, but their cultural and psychological contributions played an important role in its growth. They contributed to the new democracy in part by espousing expertise in public deliberation, since this was the foundation of decision-making, which allowed and perhaps required a tolerance of the beliefs of others. This liberal attitude would naturally have precipitated into the Athenian assembly as Sophists acquired increasingly high-powered clients.[12] Continuous rhetorical training gave the citizens of Athens "the ability to create accounts of communal possibilities through persuasive speech".[13] This was extremely important for the democracy, as it gave disparate and sometimes superficially unattractive views a chance to be heard in the Athenian assembly.		In addition, Sophists had great impact on the early development of law, as the sophists were the first lawyers in the world. Their status as lawyers was a result of their extremely developed argumentation skills.[14]		The Sophists were notorious for claiming to teach virtue/excellence and for accepting fees for teaching. The influence of this stance on education in general, and medical education in particular, has been described by Seamus Mac Suibhne.[15] An ongoing debate is centered on the interpretation between the sophists who charged for their services and Socrates who did not.[16]		In modern usage, sophism, sophist and sophistry are redefined and used disparagingly. A sophism is a specious argument for displaying ingenuity in reasoning or for deceiving someone.[17] A sophist is a person who reasons with clever but fallacious and deceptive arguments. [18][19]		
The United States school-to-prison link or school-to-prison pipeline is a metaphor used to describe the increasing patterns of contact students have with the juvenile and adult criminal justice systems as a result of the recent practices implemented by educational institutions, zero tolerance policies, and the use of police in schools.[1] The metaphor is currently a hot topic of debate in discussions surrounding educational disciplinary policies as media coverage of youth violence and mass incarceration has grown during the early 21st century.[1][2][3]		The current sociopolitical climate, relating to mass incarceration, existent in the United States serves as a critical component in increasing the contact the incarceration system has with the United States education system, as patterns of criminalization translate into the school context.[1] Specific practices implemented in United States schools over the past ten years to reduce violence in schools, including zero tolerance policies and an increase in School Resource Officers have created the environment for criminalization of youth in schools. This results from patterns of discipline in schools mirroring law enforcement models.		The disciplinary policies and practices that create an environment for the United States school-to-prison link to occur disproportionately affect Latino and Black students which is later reflected in the rates of incarceration. Between 1999 and 2007, the percentage of black students being suspended has increased by twelve percent, while the percentage of white students being suspended has declined since the implementation of zero tolerance policies.[4] Relating this statistic to patterns of overall incarceration in the U.S., from 1980 to 2008, the number of people incarcerated in America quadrupled from roughly 500,000 to 2.3 million people.[5] The graphic to the right shows the uniqueness of this practice in comparison to other countries across the globe, with the United States incarcerating a larger portion of its population than any other country in 2008. The United States holds 25% of the world’s prisoners, but only has 5% of the world’s population.[6] To put this rate in perspective, it should be noted that the United States homicide rate is 4.3x that of the UK ( 3.9 vs. 0.9 per 100,000[7][better source needed]), and that the United States also incarcerates at a close ratio to the UK, with 4.7x ( 693 vs 146 per 100,000[8]). Hence, the United States' higher incarceration rate may well represent a wealthier nation's natural reaction to higher crime rates. Of course, this implies the real issue maybe the motivation to crime itself versus incarceration’s overuse. Of the total incarcerated population in the United States, 61% are Black or Latino.[5]						For the half-century prior to 1975 the incarceration rate in the U.S. was fairly constant at roughly 0.1 percent of the population, as indicated in the accompanying figure.		Enough of the changes listed here as possible drivers of the "school-to-prison pipeline" occurred during the last quarter of the twentieth century and may have been large enough to explain this increase. Any changes since 2000 that might contribute to this phenomenon are either too minor to have such a macro effect or were too recent to be reflected in these numbers yet.		Exclusionary disciplinary policies, specifically zero tolerance policies, that remove students from the school environment increase the probability of a youth coming into contact with the incarceration system. Approximately 3.3 million suspensions and over 100,000 expulsions occur each year. This number has nearly doubled since 1974, with rates escalating in the mid 1990s as zero tolerance policies began to be widely adopted. Rising rates of the use of expulsion and suspension are not connected to higher rates of misbehaviors.[1] Zero tolerance policies are discussed in more detail later in the article, in the Current policies maintaining the link section.		Research is increasingly beginning to examine the connections between school failure and later contact with the criminal justice system for minorities[9] Once a child drops out, they are eight times more likely to be incarcerated than youth who graduate from high school.[10] Studies have found that 68% of all males in state and federal prison do not have a high school diploma.[11] Suspensions and expulsions have been shown to increase a young person's probability of dropping out and becoming involved with the criminal justice system. However it is unclear if the factors determining risk of dropping out are not wholly or partially the same as the factors determining the risk of incarceration as an individual likely to enter the criminal justice system is also likely to encounter difficulties within the education system.		School disciplinary policies disproportionately affect Black and Latino youth in the education system,[clarification needed][citation needed] a practice known as the discipline gap. This discipline gap is also connected to the achievement gap. The U.S. Department of Education Office for Civil Rights issued a brief in 2014 outlining the current disparities. Black students are suspended and expelled at a rate three times greater than white students. The Advancement Project found that "In the 2006-2007 school year, there was no state in which African-American students were not suspended more often than white students".[12] On average, 5% of white students are suspended, compared to 16% of black students. Black students represent 16% of student enrollment, and represent 27% of students referred to law enforcement and 31% of students subjected to a school-related arrest. Combined, 70% of students involved in "In-School arrests or referred to law enforcement are Black or Latino."[5][11][13] The majority of these arrests are under zero tolerance policies.		Disparities were found in the implementation of zero tolerance policies (ZTPs) in relation to minor offenses. In 2010, in North Carolina black students were punished for the same minor offenses, specifically cell phone, dress code, disruptive behavior and display of affection by more than 15 percent for each category of offense than white students. "The Council of State Governments Report found that black students were more likely to be disciplined for less serious “discretionary” offenses, and that when other factors were controlled for, higher percentages of White students were disciplined on more serious nondiscretionary grounds, such as possessing drugs or carrying a weapon".[14]		A 2009 study reported that the racial disparity in rates of school suspensions could not be explained solely by racial differences in rates of delinquent behavior, and that this disparity in turn was "strongly associated with similar levels of disproportion in juvenile court referrals."[15] Similarly, a 2010 study found that black students were more likely to be referred to the office than students of other races, and that this disparity could be partly, but not completely, explained by student behavior and school-level factors.[16] In contrast, a 2014 study found that although black students were more likely to be suspended, this disparity "was completely accounted for by a measure of the prior problem behavior of the student," and concluded that "the use of suspensions by teachers and administrators may not have been as racially biased as some scholars have argued."[17]		Schools with a higher percentage of black students are more likely to implement zero tolerance policies and to use extremely punitive discipline, supporting the racial threat hypothesis.[18]		Zero tolerance policies are school disciplinary polices that set predetermined consequences or punishments for specific offenses. By definition zero tolernce policies, as any policy that is "unreasonable rule or policy that is the same for everyone but has an unfair effect on people who share a particular attribute" are necessarily discriminatory.[19][20][21]. The zero tolerance approach was first introduced in the 1980s to reduce drug use in schools. The use of zero tolerance policies spread more widely in the 1990s. To reduce gun violence, the Gun Free Schools Act of 1994 (GFSA) required that federal funding "must 1) have policies to expel for a calendar year any student who brings a firearm to school or to school zone, and 2) report that student to local law enforcement, thereby blurring any distinction between disciplinary infractions at school and the law" .[1] During the 1996-1997 school year, 94% of schools had zero tolerance policies for fire arms, 87% for alcohol, and 79% for violence.[22]		Over the past decade, zero tolerance policies have expanded to predetermined punishments for a wide degree of rule violations. Zero-tolerance policies do not distinguish between serious and non-serious offenses. All students who commit a given offense receive the same treatment.[23] Behaviors punished by zero tolerance policies are most often non-serious offense and are punished on the same terms as a student would be for bringing a gun or drugs to school. In 2006, 95% of out-of-school suspensions were for nonviolent, minor disruptions such as tardiness.[24] In 2006-2007, "out-of-school suspensions for non-serious, non-violent offenses accounted for 37.2% of suspensions in Maryland, whereas only 6.7% of suspensions were issued for dangerous behaviors".[12] In Chicago, the widespread adoption of zero-tolerance policies in 1994 resulted in a 51% increase in student suspensions for the next four years and a 3,000% increase in expulsions.[25]		The most direct way these policies increase the probability of a youth coming into contact with the incarceration system is through their exclusionary methods. Suspension, expulsion, and an increased risk of dropping out all contribute to a youth's increased chances of becoming involved with the incarceration system. Suspension removes students from the structure and supervision provided through schooling, providing opportunities for youth to engage in criminal activities while not in the school environment. Other factors may include "increased exposure to peers involved in antisocial behavior, as well as effects on school performance and completion and student attitudes toward antisocial behavior".[26] Suspension can lead to feelings of alienation from the school setting that can lead to students to feel rejected, increasing chances of relationships with antisocial peers. Relationships with peers have strong impacts on student behavior, demonstrated through differential association theory. Students are more than twice as likely to be arrested during months in which they are forcibly removed from school.[27] Students who have been suspended are three times more likely to drop out by the 10th grade than students who have never been suspended. Dropping out makes that student three times more likely to be incarcerated.[13]		Zero tolerance policies increase the number of School Resource Officers (SRO) in schools, which increases the contact a student has with the criminal justice system. Students may be referred by teachers or other administrators but most often zero tolerance policies are directly enforced by police or school resource officers.[1] The practice of increasing the number of police in schools contributes to patterns of criminalization.[28] This increase in SROs has led to contemporary school discipline beginning to mirror approaches used in legal and law enforcement. Zero tolerance policies increase the use of profiling, a very common practice used in law enforcement. This practice is able to identify students who may engage in misbehavior, but the use of profiling is unreliable in ensuring school safety, as this practice over identifies students from minority populations. There were no students involved in the 1990s shootings who were Black or Latino and the 1990s school shootings were the main basis for the increase in presence of police in schools.[29]		A Justice Policy Institute report (2011) found a 38% increase in the number of SROs between 1997 and 2007 as a result of the growing implementation of zero tolerance policies.[10] In 1999, 54% of students surveyed reported seeing a security guard or police officer in their school, by 2005, this number increased to 68%. The education system has seen a huge increase in the number of students referred to law enforcement. In one city in Georgia, when police officers were introduced into the schools, "school-based referrals to juvenile court in the county increased 600% over a three year period". There was no increase in the number of serious offenses or safety violations during this three-year period.[30] In 2012, forty-one states required schools to report students to law enforcement for various misbehaviors on school grounds.[12] This practice increases the use of law enforcement professionals in handling student behavior and decreases the use of in-classroom (non-exclusionary) management of behaviors.		In 2014, the United Nations Human Rights Committee (HRC) expressed concern with increasing criminalization of students in response to school disciplinary problems, and recommended that the US government "promote the use of alternatives to the application of criminal law" to address such issues. The HRC also noted its concern with the use of corporal punishment in schools in the US.[31] In the second Universal Periodic Review of the United States' human-rights record, the government avowed taking "effective measures to help ensure non-discrimination in school discipline policies and practices".[32]		Restorative justice approaches provide the space for students, teachers, families, schools, and communities to "resolve conflict, promote academic achievement, and address school safety".[12] The use of restorative justice in schools began in the early 1990s with initiatives in Australia. Restorative justice models are used globally and have recently been introduced to school disciplinary policies in the United States as an alternative approach to current punitive models, such as zero tolerance.[12]		A substantial body of research claims that incarceration rates are primarily a function of media editorial policies, largely unrelated to the actual crime rate: Beginning especially in the 1970s, the mainstream commercial media in the U.S. increased coverage of the police blotter, while reducing coverage of investigative journalism. This had at least two advantages for the commercial media organizations:		Advertising rates are set based on the audience. Because "if it bleeds, it leads," the media were able to accomplish this change without losing audience.		Beyond this, the growth of private prisons increased the pool of major advertisers who could be offended by honest reporting on incarcerations and the school-to-prison pipeline: It makes financial sense to report on this only to the extent that such reporting is needed to maintain an audience.[33][34]		
A college town or university town is a community (often a separate town or city, but in some cases a town/city neighborhood or a district) that is dominated by its university population. The university may be large, or there may be several smaller institutions such as liberal arts colleges clustered, or the residential population may be small, but college towns in all cases are so dubbed because the presence of the educational institution(s) pervades economic and social life. Many local residents may be employed by the university—which may be the largest employer in the community—many businesses cater primarily to the university, and the student population may outnumber the local population.						In Europe, a university town is generally characterised by having an ancient university. The economy of the city is closely related with the university activity and highly supported by the entire university structure, which may include university hospitals and clinics, printing houses, libraries, laboratories, business incubators, student rooms, dining halls, students' unions, student societies, and academic festivities. Moreover, the history of the city is often intertwined with that of the university. Many European university towns have not been merely important places of science and education, but also centres of political, cultural and social influence throughout the centuries.		Besides a highly educated and largely transient population, a stereotypical college town often has many people in non-traditional lifestyles and subcultures and with a high tolerance for unconventionality in general, and has a very active musical or cultural scene. The majority of the population is usually politically liberal. Many have become centres of technological research and innovative startups.		As in the case of a company town, the large and transient university population may come into conflict with other townspeople. Students may come from outside the area, and perhaps subscribe to a different—sometimes radically different—culture. Most students are young people, whose living habits may be different from older people.		Economically, the high spending power of the university and of its students in aggregate may inflate the cost of living above that of the region. It is common for university employees to commute from surrounding areas, finding the cost of living in town too expensive.		Studentification, in which a growing student population move in large numbers to traditionally non-student neighborhoods, may be perceived as a form of invasion or gentrification. It may be due to university enrollment expanding beyond the capacity of on-campus housing, inadequate zoning enforcement, and/or student culture. Neighborhood associations may work to limit conversion of family homes to student rentals, while some local residents may oppose the construction of large on-campus dormitories or expansion of fraternity and sorority houses, forcing a growing enrollment to seek housing in town. Moreover, a single-family home can be converted into several smaller rental units, or shared by a number of students whose combined resources exceed those of a typical single-family rental—a strong incentive for absentee landlords to cater to students.		In the US, educational institutions are often exempted from local taxes, so in the absence of a system for "Payments In Lieu Of Taxes" (PILOT), the university population will disproportionately burden parts of the local public infrastructure, such as roads or law enforcement. Some analysts argue that students relieve the burden on other parts of the local public infrastructure, such as local primary and secondary schools, by far the most costly line item in most North American city and town budgets, by providing tax revenues through local sales tax and property tax paid by landlords. When a university expands its facilities, the potential loss of property tax revenue is thus a concern, in addition to local desire to preserve open space or historic neighborhoods.		As a result, local people may resent the university and its students. The students, in turn, may criticize the local residents' taking jobs at the university provided by student tuition and fees, and accepting the tax revenues (e.g. local sales tax, property tax on rented properties) that students generate, but resenting students' lifestyles. Some students refer to other inhabitants as "townies", a term with somewhat derogatory connotations.		This "town and gown" dichotomy notwithstanding, students and the outside community typically find a peaceful (even friendly) coexistence, with the town receiving significant economic and cultural benefits from the university, and the students often adapting to the culture of the town.		While noise, traffic, and other quality of life issues have not been resolved, some advocates of New Urbanism have led the development of neighborhoods in college towns by specifically capitalizing on their proximity to university life. For instance, some universities have developed properties to allow faculty and staff members to walk to work, reducing demand for limited on-campus parking; Duke University's Trinity Heights development is a key example. In many cases, developers have built communities where access to the university (even if not directly adjacent) is promoted as an advantage.		Student housing is also an important component of college towns. In the United States most state universities have 50 percent or more of their enrolled students living off-campus. This trend, which began in the 1960s, originally meant the conversion of near campus single-family homes to student housing, creating "student ghettos."		Colleges and other developers began building purpose-built off-campus student housing areas in the 1970s in more college towns. Beginning around 2000 in the United States, nationwide real estate investment trusts (REIT) and publicly traded corporations began developing student housing complexes.		Another notable development since the 1990s is the surge in popularity of retirees relocating to college towns. Retirees are attracted to these locations because of cultural and educational opportunities, college athletic events, good medical facilities (often at teaching hospitals affiliated with medical schools), a low cost of living, and often a pedestrian- or public transit-friendly development pattern. Several development companies now specialize in constructing retirement communities in college towns. In some cases the communities have developed formal relationships with the local institution.		The demand for housing from students, faculty, staff, and retirees has kept college town home prices stable during the housing market downturn that began in 2005.[1]		
		Education in Canada is for the most part provided publicly, funded and overseen by federal, provincial, and local governments.[16] Education is within provincial jurisdiction and the curriculum is overseen by the province.[17] Education in Canada is generally divided into primary education, followed by secondary education and post-secondary. Within the provinces under the ministry of education, there are district school boards administering the educational programs.[18]		Education is compulsory up to the age of 16 in every province in Canada, except for Manitoba, Ontario and New Brunswick, where the compulsory age is 18, or as soon as a high school diploma has been achieved. In some provinces early leaving exemptions can be granted under certain circumstances at 14. Canada generally has 190 (180 in Quebec) school days in the year, officially starting from September (after Labour Day) to the end of June (usually the last Friday of the month, except in Quebec when it is just before June 24 – the provincial holiday). In British Columbia secondary schools, there are 172 school days during a school year. (2013-2014).[19] In Alberta, high school students get an additional four weeks off to accommodate for exam break; two weeks in January, and two in June. Classes typically end on the 15th of those two months.						Elementary, secondary, and post-secondary education in Canada is a provincial responsibility and there are many variations between the provinces. Some educational fields are supported at various levels by federal departments. For example, the Department of National Defence includes the Royal Military College of Canada, while the Department of Indian and Northern Affairs Canada is responsible for the education of First Nations.[20][21] Vocational training can be subsidized by the Learning branch of Human Resources and Skills Development Canada (a federal department).[22][23][24]		About one out of ten Canadians does not have a high school diploma – one in seven has a university degree – the adult population that is without a high school diploma is a combination of both immigrant and Canadian-born. In many places, publicly funded high school courses are offered to the adult population. The ratio of high school graduates versus non diploma-holders is changing rapidly, partly due to changes in the labour market that require people to have a high school diploma and, in many cases, a university degree. Nonetheless, more than 51% of Canadians have a college degree, the highest rate in the world by far.[25] The majority of schools, at 67%, are co-educational.		Canada spends about 5.4% of its GDP on education.[12] The country invests heavily in tertiary education (more than 20 000 USD per student).[26] Recent reports suggest that from 2006 the tuition fees of Canadian universities have increased by 40 percent.[27] Since the adoption of section 23 of the Constitution Act, 1982, education in both English and French has been available in most places across Canada (if the population of children speaking the minority language justifies it), although French Second Language education/French Immersion is available to anglophone students across Canada.		According to an announcement of Canadian Minister of Citizenship and Immigration, Canada is introducing a new, fast-track system to let foreign students and graduates with Canadian work experience become permanent eligible residents in Canada.[28]		Most schools have introduced one or more initiatives such as programs in Native studies, antiracism, Aboriginal cultures and crafts; visits by elders and other community members; and content in areas like indigenous languages, Aboriginal spirituality, indigenous knowledge of nature, and tours to indigenous heritage sites.[29] Although these classes are offered, most appear to be limited by the area or region in which students reside. "The curriculum is designed to elicit development and quality of people's cognition through the guiding of accommodations of individuals to their natural environment and their changing social order"[30]		Some scholars view academics as a form of "soft power" helping to educate and to create positive attitudes,[31] although there is criticism that educators are merely telling students what to think, instead of how to think for themselves, and using up a large proportion of classroom time in the process.[32][33] Efforts to keep students happy and socially conscious often come at the expense of academic achievement. Social promotion policies, grade inflation, failure to objectively track student progress, lack of corrective feedback for students, and fashionable teaching methods that slow student progress to a crawl all mean that high schools and colleges have had to lower their academic standards in order to give the average student any chance to succeed.[34][35]		Furthermore, "subjects that typically get assessed (i.e., language arts, mathematics, and science) assume greater importance than non-assessed subjects (i.e., music, visual arts, and physical education) or facets of the curriculum (i.e., reading and writing versus speaking and listening)."[36]		The Constitution of Canada provides constitutional protections for some types of publicly funded religious-based and language-based school systems.		The Constitution Act, 1867 contains a guarantee for publicly funded religious-based separate schools, provided the separate schools were established by law prior to the province joining Confederation. Court cases have established that this provision did not apply to Nova Scotia, New Brunswick, Manitoba, British Columbia, and Prince Edward Island, since those provinces did not provide a legal guarantee for separate schools prior to Confederation. The provision did originally apply to Ontario, Quebec, Saskatchewan, Alberta, and Newfoundland and Labrador, since these provinces did have pre-existing separate schools. This constitutional provision was repealed in Quebec by a constitutional amendment in 1997, and for Newfoundland and Labrador in 1998. The constitutional provision continues to apply to Ontario, Saskatchewan and Alberta. There is a similar federal statutory provision which applies to the Northwest Territories.		Section 23 of the Canadian Charter of Rights and Freedoms guarantees the right of citizens who were educated in the minority language in a particular province to have their children educated in the minority language in publicly funded schools. In practice, this guarantee means that there are publicly funded English schools in Quebec, and publicly funded French schools in the other provinces and the territories.		Quebec students must attend a French school up until the end of high school unless one of their parents qualifies as a rights-holder under s.23 of the Charter. In Ontario, French language schools automatically admit students recognized under section 23 of the Canadian Charter of Rights and Freedoms and may admit non-francophone students through the board's admissions committee consisting of the school principal, a school superintendent and a teacher.		Most education programs in Canada begin in kindergarten (age five) or grade one (age six) and go to grade twelve (age 17 or 18), except in Quebec, where students finish a year earlier. After completion of a secondary school diploma, students may go on to post-secondary studies.		Normally, for each type of publicly funded school (such as Public English or Public French), the province is divided into districts (or divisions). For each district, board members (trustees) are elected only by its supporters within the district (voters receive a ballot for just one of the boards in their area). Normally, all publicly funded schools are under the authority of their local district school board. These school boards would follow a common curriculum set up by the province the board resides in. Only Alberta allows public charter schools, which are independent of any district board. Instead, they each have their own board, which reports directly to the province.		Primary education and secondary education combined are sometimes referred to as K-12 (Kindergarten through Grade 12). Secondary schooling, known as high school, collegiate institute, école secondaire or secondary school, consists of different grades depending on the province in which one resides. Furthermore, grade structure may vary within a province or even within a school division and may or may not include middle school or junior high school.		Kindergarten (or its equivalent) is available for children in all provinces in the year they turn five (except Ontario and Quebec, where it begins a year earlier), but the names of these programs, provincial funding, and the number of hours provided varies widely. For example, the Department of Education in Nova Scotia refers to Kindergarten as Grade Primary.[37]		Ontario offers two years of optional kindergarten (junior kindergarten for four-year-olds and senior kindergarten for five-year-olds). At French schools in Ontario, these programs are called Maternelle and CPE Centre de la Petite Enfance.[38] In 2010, Ontario increased both years to full-day programs, while BC's single year of kindergarten became full-day in 2012. Quebec offers heavily subsidized preschool programs and introduced an early kindergarten program for children from low-income families in 2013. Students in the Prairie provinces are not required by statute to attend kindergarten. As a result, kindergarten often is not available in smaller towns.		Dependent on the province the age of mandatory entry to the education system is at 4–7 years. Starting at grade one, at age six or seven, there is universal publicly funded access up to grade twelve (age seventeen to eighteen), except in Quebec, where secondary school ends one year earlier. Children are required to attend school until the age of sixteen (eighteen in Manitoba, Ontario, and New Brunswick). In Quebec, the typical high school term ends after Secondary V/Grade eleven (age sixteen to seventeen); following this, students who wish to pursue their studies to the university level have to attend college (see Education in Quebec). Quebec is currently the only province where Grade 12 is part of postsecondary, though Grade 11 was also the end of secondary education in Newfoundland and Labrador prior to the introduction of grade 12 in 1983.		Ontario had a "Grade 13" known as Ontario Academic Credit (OAC) year, but this was abolished in 2003 by the provincial government to cut costs. As a result, the curriculum has been compacted, and the more difficult subjects, such as mathematics, are comparatively harder than before. However, the system is now approximately equivalent to what has been the case outside of Quebec and Ontario for many years.		Students may continue to attend high school until the ages of 19 to 21 (the cut-off age for high school varies between provinces). Those 19 and over may attend adult school. Students of high school age who have received long-term suspensions or have been expelled, or are otherwise unable or unwilling to attend conventional schools may be offered alternative learning options to complete their secondary education, such as drop-in programs, night school, or distance/online classes.		An increasing number of international students are attending pre-university courses at Canadian high schools.		Post-secondary education in Canada is also the responsibility of the individual provinces and territories. Those governments provide the majority of funding to their public post-secondary institutions, with the remainder of funding coming from tuition fees, the federal government, and research grants. Compared to other countries in the past, Canada has had the highest tertiary school enrollment as a percentage of their graduating population.[40]		Nearly all post-secondary institutions in Canada have the authority to grant academic credentials (i.e., diplomas or degrees). Generally speaking, universities grant degrees (e.g., bachelor's, master's or doctorate degrees) while colleges, which typically offer vocationally oriented programs, grant diplomas and certificates. However, some colleges offer applied arts degrees that lead to or are equivalent to degrees from a university. Private career colleges are overseen by legislative acts for each province. For example, in British Columbia training providers will be registered and accredited with the (PCTIA) Private Career Training Institutions Agency regulated under the Private Career Training Institutions Act (SBC 2003) [41] Each province with their own correlating agency. Unlike the United States, there is no "accreditation body" that oversees the universities in Canada. Universities in Canada have degree-granting authority via an Act or Ministerial Consent from the Ministry of Education of the particular province.		Post-secondary education in Quebec begins with college following graduation from Grade 11 (or Secondary V). Students complete a two- or three-year general program leading to admission to a university, or a professional program leading directly into the labour force. In most cases, bachelor's degree programs in Quebec are three years instead of the usual four; however, in many cases, students attending a university in Quebec that did not graduate from college must complete an additional year of coursework. When Ontario had five years of high school, a three-year bachelor's degree was common, but these degrees are being phased out in favour of the four-year degree.		The main variation between the provinces, with respect to the universities, is the amount of funding they receive and the amount of tuition and other fees they charge.		The Royal Military College of Canada (RMC), is the military academy of the Canadian Forces and is a full degree-granting university. RMC is the only federal institution with degree-granting powers.		About 5.6% of students are in private schools.[42] A minority of these are elite private schools, which are attended by only a small fraction of students, but do have a great deal of prestige and prominence. A far larger portion of private schools are religious based institutions. Private schools are also used to study outside the country. For example, Canadian College Italy has an Ontario curriculum, but the school is located in Italy.		Private schools have historically been less common on the Canadian Prairies and were often forbidden under municipal and provincial statutes enacted to provide equality of education to students regardless of family income. This is especially true in Alberta, where successive Social Credit (or populist conservative) governments denounced the concept of private education as the main cause of denial of opportunity to the children of the working poor.		In the past, private universities in Canada maintained a religious history or foundation. However, since 1999, the Province of New Brunswick passed the Degree Granting Act[43] allowing private universities to operate in the Province.[44][45] The University of Fredericton is the newest university to receive designation in New Brunswick.		Trinity Western University, in Langley British Columbia, was founded in 1962 as a junior college and received full accreditation in 1985. In 2002, British Columbia's Quest University became the first privately funded liberal arts university without a denominational affiliation (although it is not the first private liberal arts university). Many provinces, including Ontario and Alberta, have passed legislation allowing private degree-granting institutions (not necessarily universities) to operate there.		Many Canadians remain polarized on the issue of permitting private universities into the Canadian market. On the one hand, Canada's top universities find it difficult to compete with the private American powerhouses because of funding, but on the other hand, the fact that the price of private universities tends to exclude those who cannot pay that much for their education could prevent a significant portion of Canada's population from being able to attend these schools.		In addition to the issue of access, some Canadians find issue with protections instituted within the Charter of Rights and Freedoms as ruled by the Supreme Court of Canada in 2001 and consistent with federal and provincial law that (private) faith-based universities in Canada based on the long established principles of freedom of conscience and religion can exempt itself from more recent human rights legislation when they insist in their “community covenant” code signed by staff, faculty and students that they act in accordance with the faith of the school. The covenant may require restraint from those acts considered in contradiction with the tenets of their faith such as homosexual relationships, sex outside marriage or more broadly abstain from consuming alcohol on campus or viewing pornography.[46] However, private-Christian based schools do not preclude homosexual or lesbian students from attending.[47] Some faith-based universities have been known to fire staff and faculty which refused to adhere or whose actions were in opposition with the tenets of the faith, although in some provinces, their dismissals have been successfully challenged in court based on the circumstances.[48]		Each province deals differently with private religious schools. In Ontario the Catholic system continues to be fully publicly funded while other faiths are not. Ontario has several private Jewish, Muslim, and Christian schools all funded through tuition fees. Since the Catholic schools system is entrenched in the constitution, the Supreme Court has ruled that this system is constitutional. However, the United Nations Human Rights Committee has ruled that Ontario's system is discriminatory, suggesting that Ontario either fund no faith-based schools, or all of them.[49] In 2002 the government of Mike Harris introduced a controversial program to partially fund all private schools, but this was criticized for undermining the public education system and the program was eliminated after the Liberals won the 2003 provincial election.		In other provinces privately operated religious schools are funded. In British Columbia the government pays independent schools that meet rigorous provincial standards up to 50% of the per-student operating cost of public schools. The province has a number of Sikh, Hindu, Christian, and Muslim schools. Alberta also has a network of charter schools, which are fully funded schools offering distinct approaches to education within the public school system. Alberta charter schools are not private and the province does not grant charters to religious schools. These schools have to follow the provincial curriculum and meet all standards, but are given considerable freedom in other areas. In all other provinces private religious schools receive some funding, but not as much as the public system.		An example of how schools can be divided by religion, Toronto has two English boards; Toronto Catholic District School Board and Toronto District School Board, and two French boards; Conseil scolaire de district catholique Centre-Sud and Conseil scolaire Viamonde.		As the education system in Canada is managed by the varying provincial governments in Canada, the way the educational stages are grouped and named may differ from each region, or even between districts and individual schools. The ages are the age of the students when they end the school year in June.		English schools in Quebec have the same grade system as French schools, but with English names. For example, "elementary school" is not called "école primaire" in an English school, but has the same grading system.		The following table shows how grades are organized in various provinces. Often, there will be exceptions within each province, both with terminology for groups, and which grades apply to each group.		Notes:		[53][54]		
A professional degree, formerly known in the US as a first professional degree, is a degree that prepares someone to work in a particular profession, often meeting the academic requirements for licensure or accreditation.[1][2][3][4] Professional degrees may be either graduate or undergraduate entry, depending on the profession concerned and the country, and may be classified as bachelor's, master's or doctoral degrees. For a variety of reasons, professional degrees may bear the name of a different level of qualification from their classification in qualifications frameworks, e.g. some UK professional degrees are named bachelor's but are at master's level, while some Australian and Canadian professional degrees have the name "doctor" but are classified as master's or bachelor's degrees.[5][6][7]						The first doctorates were awarded in the mid twelfth century to recognise teachers (doctors) in mediaeval universities, either in civil law at the University of Bologna[8] or in theology at the University of Paris.[9] These were followed shortly afterwards by doctorates in canon law, and then in the thirteenth century by doctorates in medicine, grammar, logic and philosophy. These mediaeval doctorates remained, however, essentially teaching qualifications, with their major importance being the ius ubique docendi – the right to teach anywhere.[8]		The first university medical school to be established in the United Kingdom was at the University of Edinburgh in 1726, followed in 1744 by the University of Glasgow. In 1817 Glasgow became the first British university to offer a separate degree in surgery, the Master of Surgery. However, other Scottish universities – St Andrews and the two universities in Aberdeen – also offered medical degrees, often in absentia and without examination, despite not having medical schools.[10] In England, the two universities (Oxford and Cambridge) were only sporadically interested in medical teaching, which was mainly carried out in the London hospitals.[11] It was not until the establishment of the University of London in 1836, however, that students at the hospital medical schools could earn degrees. Following the passing of the Medical Act 1858 and the establishment of the General Medical Council, Scottish graduates gained the right to practice in England and degrees in both medicine and surgery became the norm, standardising eventually on the double Bachelor of Medicine, Bachelor of Surgery degree.		The first university in England to offer training in theology for those intending to become priests in the Church of England was the University of Durham in 1833, following the lead of colleges such as St Bees Theological College and St David's College, Lampeter. The Licence in Theology could be taken as either a one year graduate course, following on from a BA, or a three year undergraduate course.[12] Shortly after, in 1837, Durham also became the first British university to teach engineering, although the course closed after a few years.		Anglican theological colleges partnered with local universities to offer professional degrees in theology and ministry during the twentieth century. Since 2014, however, the Common Award degrees, validated by Durham, have offered a more unified training across the theological colleges. Some colleges continue to offer other degrees in addition to the Common Awards, such as the Cambridge Bachelor of Theology at the Cambridge Theological Federation		Legal studies in England were mainly confined to the Inns of Court until the late nineteenth century. The only undergraduate course was at Cambridge and concentrated on Roman civil law rather than English common law; in terms of employment that the bishops accepted it as equivalent to a BA for ordination was more useful than the legal training it provided, and it was generally seen as an easy option for those who couldn't cope with the mathematics on the BA course.[13] Cambridge reformed its course in 1858, and London established an undergraduate course a few years later. However, it has only been since the 1960s that law schools have taken on a leading role in training lawyers and truly established professional degrees.[14]		In the latter part of the twentieth century, many chartered bodies introduced educational requirements for their chartered professional statuses, most notably the Engineering Council requirements for Chartered Engineer. This led to the accreditation of degrees by the relevant professional bodies and, in the case of engineering, to the Washington Accord – an international agreement between engineering regulatory bodies to recognise professional degrees accredited in each country – signed originally in 1989 by the UK, US, Australia, Canada, Ireland and New Zealand, and since expanded to include many other countries.[15] In the twenty-first century, the standard professional degree for many science and engineering fields was raised from bachelor's to master's level, including for qualification as a Chartered Physicist (from 2001), Chartered Scientist (from 2004) and Chartered Engineer (from 2012).[16][17][18]		The M.B. or Bachelor of Medicine was the first medical degree to be granted in the United States and Canada. The first medical schools that granted the MB degree were at the University of Pennsylvania and Columbia University. Columbia University was the first American university to grant the M.D. degree in 1770, although, as in England, this followed the M.B. (which was the qualifying degree) and required completion of a thesis.[19][20][21] Professional societies started licensing doctors from the 1760s, and in the early nineteenth century started setting up their own medical schools, known as proprietary medical colleges, the first being the medical college of the Medical Society of the County of New York, which opened March 12, 1807. These eliminated the general education and long lecture terms of the university schools, making them much more popular. Without effective regulation, abuses arose, and national conventions in 1846 and 1847 led to the establishment of the American Medical Association. This new body set the first nationwide standards for M.D. degrees, requiring that students had a liberal education in arts and sciences as part of their degree, that they had served an apprenticeship before starting the course, and that the course lasted three years.[22]		The M.D. was thus the first entry-level professional degree to be awarded as a purely trade school 'doctor'-degree in the United States, before the first European-style doctorate, the Ph.D., was awarded by an American Institution in 1861,[23] although the M.D. was not established as a post-baccalaureate degree until much later.[24] The President of Yale, Arthur Twining Hadley, stated in the early 20th century that: "However convenient it might be to insist on the possession of a bachelor's degree by all pupils in the schools of law or medicine, I feel that it would be a violation of our duty to these professions to hedge ourselves about by any such artificial limitations."[25] This changed (for medicine) after Abraham Flexner's damning report into the state of medical education in 1910: by 1930 almost all medical schools required a previous liberal arts degree before starting the M.D. course.[22]		Law degrees were introduced in the US by the College of William and Mary in 1792, with its "Batchelor of Law" (sic) degree. This was followed by the "Graduate of Law" at the University of Virginia in 1829, which became the first American LL.B. in 1840. The J.D. was introduced by the University of Chicago in 1902, with the same curriculum as the LL.B. but requiring a previous B.A. or B.S. for entry. The J.D. spread, but encountered opposition, and Harvard, which imposed graduate entry as a requirement for its LL.B. course in 1909, and Yale used the name for their post-LL.B. degree, elsewhere called the LL.M. By the 1930s, when most law schools had shifted to graduate entry, the standard degree was once again the LL.B. The second shift to the J.D., again without a change of curriculum, came in the 1960s, with all American Bar Association-accredited professional degrees adopting the nomenclature by 1971.[26]		In the late twentieth and early twenty-first century, other professions, particularly in clinical fields, transitioned their professional degrees to doctorates, following the example of the M.D. and J.D. In the 1990s there was also some debate in the architectural community about renaming the professional degree in architecture a "doctorate".[27] The spread of professional doctorates raised concerns about the standards of the new degrees, particularly in cases such as Physical Therapy, where the standard set by the American Physical Therapy Association for the doctorate is the same as that for the master's degree. Critics have claimed that these degrees should not be called doctorates, pointing out that a Ph.D. takes an average of twelve years from the start of college, compared to five and a half to six and a half years for professional doctorates, while defenders of the new professional doctorates have said the point of comparison should be the M.D. and J.D., not the Ph.D.[28]		Among the professional degrees in the United States, one particular form was the graduate-entry first-professional degree, often denominated as a doctorate. The U.S. Department of Education defines these as: "A first-professional degree was an award that required completion of a program that met all of the following criteria: (1) completion of the academic requirements to begin practice in the profession; (2) at least 2 years of college work prior to entering the program; and (3) a total of at least 6 academic years of college work to complete the degree program, including prior required college work plus the length of the professional program itself."[29] The use of the term "first-professional" was discontinued by the Department of Education as of 2010-11, when new post-baccalaureate award categories were introduced.[30] Prior to this, first-professional degrees were awarded in the following ten fields:[29]		Since 2011, the classification "doctor's degree - professional practice" has been used for "[a] doctor's degree that is conferred upon completion of a program providing the knowledge and skills for the recognition, credential, or license required for professional practice." As with the "first professional degree", this classification also requires that the total time in higher education is at least six years, although the requirement for at least two years of college-level study prior to entering the program was removed.[29] The Department of Education does not define which fields professional doctorates may be awarded in, unlike with the "first professional degree". Besides professional doctorates, other professional degrees can exist that use the title of bachelor or master, e.g. B.Arch. and M.Arch. in architecture.[31] In particular, first professional degrees in theology, which did not use the title of doctor, were reclassified as master's degrees in 2011 - including the B.D.[30]		A distinction is drawn in the US between professional doctorates and "doctor's degree - research/scholarship", with the latter being "[a] Ph.D. or other doctor's degree that requires advanced work beyond the master's level, including the preparation and defense of a dissertation based on original research, or the planning and execution of an original project demonstrating substantial artistic or scholarly achievement."[29] Internationally, US professional doctorates (which, unlike research doctorates, are not defined as requiring work beyond the master's level) are not generally considered to be doctoral level qualifications.[32][33][34][35][36][37] The classification of "Doctor's degree - other" also exists for doctorates that do not meet the definition of either professional doctorates or research doctorates.[29]		Some professional fields offer degrees beyond the professional doctorate or other degree required for qualification, sometimes termed post-professional degrees. Higher professional degrees may also be offered in fields that do not have specific academic requirements for entry, such as Fine Arts. These degrees may be at master's or doctorate levels.[38][39][40]		Professional degrees in the UK are accredited by professional, statutory and regulatory bodies, which work with the Quality Assurance Agency on defining benchmark statements for their subjects.[41] Specific benchmark statements have also been produced for professional qualifications in Scotland.[42]		Many professional degrees span teaching at bachelor's and master's level, leading to a master's level award. This includes older degrees that retain the names of bachelor's degrees for historic reasons, e.g. the Bachelor of Medicine, Bachelor of Surgery (MBBS, MBBCh, etc.), Bachelor of Dental Surgery (BDS) and Bachelor of Veterinary Science (BVS), and newer integrated master's degrees such as the Master of Engineering (MEng) or Master of Pharmacy (MPharm).[43][44] In some subjects, qualification can be via separate bachelor's and master's degrees, e.g. a Bachelor of Engineering (BEng) followed by a Master of Science (MSc) in Engineering,[44] or a Bachelor of Arts (BA) or Bachelor of Science (BSc) in Architecture followed by a year of professional experience, then a two-year Master of Architecture (MArch).[45] In some subjects the normal professional degree is a bachelor's degree, e.g. the Bachelor of Laws (LLB) or BA in Law (for both solicitors and barristers)[46] or a BSc in Surveying.[47] Some professional bodies also offer different levels of professional recognition, e.g. a master's degree is needed for Chartered Engineers or Chartered Scientists but a bachelor's degree for Incorporated Engineers and a bachelor's or foundation degree for Registered Scientists.[44][48]		It is common for professional qualification in the UK to require professional experience in addition to academic qualification. For Architecture, the standard route has a year of experience between the bachelor's and master's stages and a further year after the master's before the final examination;[45] becoming a Chartered Engineer requires post-degree Initial Professional Development that typically takes four to six years;[49] becoming a General Practitioner requires five years of study beyond the MBBS, while qualifying as a Consultant takes seven to nine more years.[50]		In addition to initial professional degrees, some professional master's degrees and most professional doctorates, e.g. the Master of Business Administration (MBA), Doctor of Education (EdD) and Doctor of Engineering (EngD), are offered for those already established in professions. It should be noted that UK professional doctorates are research degrees at the same level as PhDs, normally including teaching at doctoral level but still assessed by a doctoral research thesis or equivalent.[43][51]		Some professional degrees are designed specifically for trainees or members within a particular organisation, rather than being available via general enrolment. Examples of these include the Church of England's Common Awards with Durham University and the Association of Chartered Certified Accountants' BSc in Applied Accounting with Oxford Brookes University.[52][53]		In medicine, individual countries specify rules for recognising foreign qualifications; in the US, for example, this is carried out by the Educational Commission for Foreign Medical Graduates (ECFMG) and in the UK by the General Medical Council (GMC).[54][55] The Australian Medical Council, US ECFMG, UK GMC, Medical Council of Canada, Danish Health and Medicines Authority and Korean Institute of Medical Education and Evaluation jointly sponsor the World Directory of Medical Schools.[56] At least one state in the US, Wisconsin, permits foreign graduates to use the title "MD" if licensed to practice in the US.[57]		In engineering, the Washington Accord (1989) recognised that the academic training (i.e. professional degrees) for full professional status (Professional Engineer, Chartered Engineer, etc.) is equivalent in the signatory countries.[15] Similarly the Sydney Accord (2001) recognises similar academic training between signatories for Engineering Technologists, Incorporated Engineers, etc. and the Dublin Accord (2002) for Engineering Technicians.[58][59] For computing and information technology, the Seoul Accord (2008) recognises similar academic training on accredited courses for computing and information technology professionals in the signatory countries.[60]		
Students		Unions		Government of France		The volatile period of civil unrest in France during May 1968 was punctuated by demonstrations and massive general strikes as well as the occupation of universities and factories across France. At the height of its fervor, it brought the entire economy of France to a virtual halt.[1] The protests reached such a point that political leaders feared civil war or revolution; the national government itself momentarily ceased to function after President Charles de Gaulle secretly left France for a few hours. The protests spurred an artistic movement, with songs, imaginative graffiti, posters, and slogans.[2][3]		“May 68” had an impact on French society that resounded for decades afterward. It is considered to this day as a cultural, social and moral turning point in the history of the country. As Alain Geismar—one of the leaders of the time—later pointed out, the movement succeeded “as a social revolution, not as a political one”.[4]		The unrest began with a series of student occupation protests against capitalism, consumerism, American imperialism and traditional institutions, values and order. It then spread to factories with strikes involving 11 million workers, more than 22% of the total population of France at the time, for two continuous weeks.[1] The movement was characterized by its spontaneous and de-centralized wildcat disposition; this created contrast and sometimes even conflict between itself and the establishment, trade unions and workers' parties.[1] It was the largest general strike ever attempted in France, and the first nationwide wildcat general strike.[1]		The student occupations and wildcat general strikes initiated across France were met with forceful confrontation by university administrators and police. The de Gaulle administration's attempts to quell those strikes by police action only inflamed the situation further, leading to street battles with the police in the Latin Quarter, followed by the spread of general strikes and occupations throughout France. De Gaulle went to a French military base in Germany, and after returning dissolved the National Assembly, and called for new parliamentary elections for 23 June 1968. Violence evaporated almost as quickly as it arose. Workers went back to their jobs, and when the elections were finally held in June, the Gaullist party emerged even stronger than before.						In February 1968, the French Communists and French Socialists formed an electoral alliance. Communists had long supported Socialist candidates in elections, but in the "February Declaration" the two parties agreed to attempt to form a joint government to replace President Charles de Gaulle and his Gaullist Party.[5] On 22 March far-left groups, a small number of prominent poets and musicians, and 150 students occupied an administration building at Paris University at Nanterre and held a meeting in the university council room dealing with class discrimination in French society and the political bureaucracy that controlled the university's funding. The university's administration called the police, who surrounded the university. After the publication of their wishes, the students left the building without any trouble. After this first record some leaders of what was named the "Movement of 22 March" were called together by the disciplinary committee of the university.		Following months of conflicts between students and authorities at the Paris University at Nanterre, the administration shut down the university on May 2, 1968.[6] Students at the Sorbonne University in Paris met on May 3 to protest against the closure and the threatened expulsion of several students at Nanterre.[7] On Monday, 6 May, the national student union, the Union Nationale des Étudiants de France (UNEF)—still the largest student union in France today—and the union of university teachers called a march to protest against the police invasion of Sorbonne. More than 20,000 students, teachers and supporters marched towards the Sorbonne, still sealed off by the police, who charged, wielding their batons, as soon as the marchers approached. While the crowd dispersed, some began to create barricades out of whatever was at hand, while others threw paving stones, forcing the police to retreat for a time. The police then responded with tear gas and charged the crowd again. Hundreds more students were arrested.		High school student unions spoke in support of the riots on 6 May. The next day, they joined the students, teachers and increasing numbers of young workers who gathered at the Arc de Triomphe to demand that:		Negotiations broke down, and students returned to their campuses after a false report that the government had agreed to reopen them, only to discover the police still occupying the schools. This led to a near revolutionary fervor among the students.		On Friday, 10 May, another huge crowd congregated on the Rive Gauche. When the Compagnies Républicaines de Sécurité again blocked them from crossing the river, the crowd again threw up barricades, which the police then attacked at 2:15 in the morning after negotiations once again floundered. The confrontation, which produced hundreds of arrests and injuries, lasted until dawn of the following day. The events were broadcast on radio as they occurred and the aftermath was shown on television the following day. Allegations were made that the police had participated, through agents provocateurs, in the riots, by burning cars and throwing Molotov cocktails.[8]		The government's heavy-handed reaction brought on a wave of sympathy for the strikers. Many of the nation's more mainstream singers and poets joined after the heavy-handed police brutality came to light. American artists also began voicing support of the strikers. The major left union federations, the Confédération Générale du Travail (CGT) and the Force Ouvrière (CGT-FO), called a one-day general strike and demonstration for Monday, 13 May.		Well over a million people marched through Paris on that day; the police stayed largely out of sight. Prime Minister Georges Pompidou personally announced the release of the prisoners and the reopening of the Sorbonne. However, the surge of strikes did not recede. Instead, the protesters became even more active.		When the Sorbonne reopened, students occupied it and declared it an autonomous "people's university". Public opinion at first supported the students, but quickly turned against them after their leaders, invited to appear on national television, "behaved like irresponsible utopianists who wanted to destroy the 'consumer society.'"[9] Nonetheless, in the weeks that followed, approximately 401 popular action committees were set up in Paris and elsewhere to take up grievances against the government and French society, including the Sorbonne Occupation Committee. The students Daniel Cohn-Bendit and Alain Krivine had emerged as prominent figures at that time.		In the following days, workers began occupying factories, starting with a sit-down strike at the Sud Aviation plant near the city of Nantes on 14 May, then another strike at a Renault parts plant near Rouen, which spread to the Renault manufacturing complexes at Flins in the Seine Valley and the Paris suburb of Boulogne-Billancourt. Workers had occupied roughly fifty factories by 16 May, and 200,000 were on strike by 17 May. That figure snowballed to two million workers on strike the following day and then ten million, or roughly two-thirds of the French workforce, on strike the following week.		These strikes were not led by the union movement; on the contrary, the CGT tried to contain this spontaneous outbreak of militancy by channeling it into a struggle for higher wages and other economic demands. Workers put forward a broader, more political and more radical agenda, demanding the ousting of the government and President de Gaulle and attempting, in some cases, to run their factories. When the trade union leadership negotiated a 35% increase in the minimum wage, a 7% wage increase for other workers, and half normal pay for the time on strike with the major employers' associations, the workers occupying their factories refused to return to work and jeered their union leaders.[citation needed] In fact, in the May '68 movement there was a lot of "anti-unionist euphoria,"[10] against the mainstream unions, the CGT, FO and CFDT, that were more willing to compromise with the powers that be than enact the will of the base.[1]		On 25 May and 26 May, the Grenelle agreements were conducted at the Ministry of Social Affairs. They provided for an increase of the minimum wage by 25% and of average salaries by 10%. These offers were rejected, and the strike went on. The working class and top intellectuals were joining in solidarity for a major change in workers' rights.		On 27 May, the meeting of the UNEF, the most outstanding of the events of May 1968, proceeded and gathered 30,000 to 50,000 people in the Stade Sebastien Charlety. The meeting was extremely militant with speakers demanding the government be overthrown and elections held.		The Socialists saw an opportunity to act as a compromise between de Gaulle and the Communists. On 28 May, François Mitterrand of the Federation of the Democratic and Socialist Left declared that "there is no more state" and stated that he was ready to form a new government. He had received a surprisingly high 45% of the vote in the 1965 presidential election. On 29 May, Pierre Mendès France also stated that he was ready to form a new government; unlike Mitterrand he was willing to include the Communists. Although the Socialists did not have the Communists' ability to form large street demonstrations, they had more than 20% of the country's support.[9][5]		On the morning of 29 May, de Gaulle postponed the meeting of the Council of Ministers scheduled for that day and secretly removed his personal papers from Élysée Palace. He told his son-in-law Alain de Boissieu, "I do not want to give them a chance to attack the Elysée. It would be regrettable if blood were shed in my personal defense. I have decided to leave: nobody attacks an empty palace." De Gaulle refused Pompidou's request that he dissolve the National Assembly as he believed that their party, the Gaullists, would lose the resulting election. At 11:00 a.m., he told Pompidou, "I am the past; you are the future; I embrace you."[9]		The government announced that de Gaulle was going to his country home in Colombey-les-Deux-Églises before returning the next day, and rumors spread that he would prepare his resignation speech there. The presidential helicopter did not arrive in Colombey, however, and de Gaulle had told no one in the government where he was going. For more than six hours the world did not know where the French president was.[12] The canceling of the ministerial meeting, and the president's mysterious disappearance, stunned the French[9] including Pompidou, who shouted, "He has fled the country!"[11]		The national government had effectively ceased to function. Édouard Balladur later wrote that as prime minister, Pompidou "by himself was the whole government" as most officials were "an incoherent group of confabulators" who believed that revolution would soon occur. A friend of the prime minister offered him a weapon, saying, "You will need it"; Pompidou advised him to go home. One official reportedly began burning documents, while another asked an aide how far they could flee by automobile should revolutionaries seize fuel supplies. Withdrawing money from banks became difficult, gasoline for private automobiles was unavailable, and some people tried to obtain private planes or fake national identity cards.[9]		Pompidou unsuccessfully requested that military radar be used to follow de Gaulle's two helicopters, but soon learned that he had gone to the headquarters of the French military in Germany, in Baden-Baden, to meet General Jacques Massu. Massu persuaded the discouraged de Gaulle to return to France; now knowing that he had the military's support, de Gaulle rescheduled the meeting of the Council of Ministers for the next day, 30 May,[9] and returned to Colombey by 6:00 p.m.[12] His wife Yvonne gave the family jewels to their son and daughter-in-law—who stayed in Baden for a few more days—for safekeeping, however, indicating that the de Gaulles still considered Germany a possible refuge. Massu kept as a state secret de Gaulle's loss of confidence until others disclosed it in 1982; until then most observers believed that his disappearance was intended to remind the French people of what they might lose. Although the disappearance was real and not intended as motivation, it indeed had such an effect on France.[9]		On 30 May, 400,000 to 500,000 protesters (many more than the 50,000 the police were expecting) led by the CGT marched through Paris, chanting: "Adieu, de Gaulle!" ("Farewell, de Gaulle!"). Maurice Grimaud, head of the Paris police, played a key role in avoiding revolution by both speaking to and spying on the revolutionaries, and by carefully avoiding the use of force. While Communist leaders later denied that they had planned an armed uprising, and extreme militants only comprised 2% of the populace, they had overestimated de Gaulle's strength as shown by his escape to Germany.[9] (One scholar, otherwise skeptical of the French Communists' willingness to maintain democracy after forming a government, has claimed that the "moderate, nonviolent and essentially antirevolutionary" Communists opposed revolution because they sincerely believed that the party must come to power through legal elections, not armed conflict that might provoke harsh repression from political opponents.)[5]		The movement was largely centered around the Paris metropolitan area, and not elsewhere. Had the rebellion occupied key public buildings in Paris, the government would have had to use force to retake them. The resulting casualties could have incited a revolution, with the military moving from the provinces to retake Paris as in 1871. Minister of Defence Pierre Messmer and Chief of the Defence Staff Michel Fourquet prepared for such an action, and Pompidou had ordered tanks to Issy-les-Moulineaux.[9] While the military was free of revolutionary sentiment, using an army mostly of conscripts the same age as the revolutionaries would have been very dangerous for the government.[5][12] A survey taken immediately after the crisis found that 20% of Frenchmen would have supported a revolution, 23% would have opposed it, and 57% would have avoided physical participation in the conflict. 33% would have fought a military intervention, while only 5% would have supported it and a majority of the country would have avoided any action.[9]		At 2:30 p.m. on 30 May, Pompidou persuaded de Gaulle to dissolve the National Assembly and call a new election by threatening to resign. At 4:30 p.m., de Gaulle broadcast his own refusal to resign. He announced an election, scheduled for 23 June, and ordered workers to return to work, threatening to institute a state of emergency if they did not. The government had leaked to the media that the army was outside Paris. Immediately after the speech, about 800,000 supporters marched through the Champs-Elysées waving the national flag; the Gaullists had planned the rally for several days, which attracted a crowd of diverse ages, occupations, and politics. The Communists agreed to the election, and the threat of revolution was over.[13][9][12]		From that point, the revolutionary feeling of the students and workers faded away. Workers gradually returned to work or were ousted from their plants by the police. The national student union called off street demonstrations. The government banned a number of leftist organizations. The police retook the Sorbonne on 16 June. Contrary to de Gaulle's fears, his party won the greatest victory in French parliamentary history in the legislative election held in June, taking 353 of 486 seats versus the Communists' 34 and the Socialists' 57.[9] The February Declaration and its promise to include Communists in government likely hurt the Socialists in the election. Their opponents cited the example of the Czechoslovak National Front government of 1945, which led to a Communist takeover of the country in 1948. Socialist voters were divided; in a February 1968 survey a majority had favored allying with the Communists, but 44% believed that Communists would attempt to seize power once in government. (30% of Communist voters agreed.)[5]		On Bastille Day, there were resurgent street demonstrations in the Latin Quarter, led by socialist students, leftists and communists wearing red arm-bands and anarchists wearing black arm-bands. The Paris police and the Compagnies Républicaines de Sécurité harshly responded starting around 10 pm and continuing through the night, on the streets, in police vans, at police stations, and in hospitals where many wounded were taken. There was, as a result, much bloodshed among students and tourists there for the evening's festivities. No charges were filed against police or demonstrators, but the governments of Britain and West Germany filed formal protests, including for the indecent assault of two English schoolgirls by police in a police station.		Despite the size of de Gaulle's triumph, it was not a personal one. The post-crisis survey showed that a majority of the country saw de Gaulle as too old, too self-centered, too authoritarian, too conservative, and too anti-American. As the April 1969 referendum would show, the country was ready for "Gaullism without de Gaulle".[9]		A few examples:[14]		May 1968 is an important reference point in French politics, representing for some the possibility of liberation and for others the dangers of anarchy.[4] For some, May 1968 meant the end of traditional collective action and the beginning of a new era to be dominated mainly by the so-called new social movements.[26]		Someone who took part in or supported this period of unrest is referred to as soixante-huitard - a term, derived from the French for "68", which has also entered the English language.		
Liceo classico (classical lyceum) is the oldest, public secondary school type in Italy. The educational curriculum lasts five years, and students are generally about 14 to 19 years of age. Due its rigorous curriculum and numerous notable alumni, it is often considered the most prestigious secondary school students can attend throughout Italy.		Until 1969, this was the only secondary education track that allowed a student access to any kind of Italian university (including humanities and jurisprudence). It is known as a social scientific and humanistic school, one of the few European secondary school types where the study of ancient languages (Latin and ancient Greek) and their literature are compulsory.						A liceo classico offers a wide selection of subjects, but the central subjects are those related to literature. Several hours are also dedicated to the study of history and philosophy.		The liceo classico's distinctive subjects are history, Latin and ancient Greek. In Italy, Latin is taught in other kinds of schools as well, like liceo scientifico, Liceo delle Scienze Umane and few others with linguistic specializations. However, ancient Greek is taught only in the liceo classico.		Another peculiarity of the liceo classico is how the years of course are called: in all the other Italian five-year secondary schools, the years are referred to with increasing numbers from 1 to 5. In liceo classico the first two years are called ginnasio; the name comes from the Greek gymnasion (training ground). The first year is called "4th year of ginnasio", and the second year is referred to as "5th year of ginnasio" because, until the reform of 1962, this course of study started just after a three-year middle school ("scuola media inferiore"). By 1963, the first three years were suppressed and integrated in the 'unified secondary school', where Latin was mandatory as a subject to access the high schools until 1975. The remaining three years of liceo classico are referred as "1st, 2nd and 3rd year of liceo". However, nowadays this habit is waning, even though the names of the different years are still colloquially used.		This naming system comes from the Gentile reform of the fascist regime, named after Giovanni Gentile, an Italian philosopher and politician, who had planned an eight-year school career (five years of ginnasio and three of liceo) that could be accessed by passing a test after the fifth year of elementary school. There was also another test between the ginnasio and the liceo. Several reforms changed the Italian school system in about 1940 and 1960; the first three years of ginnasio were separated and became an independent kind of school. In 1968, the compulsory test which had to be taken at the end of the ginnasio to enter the liceo was abolished, so the liceo classico got the structure it has today.		In 2010, the Gelmini reform changed the traditional Italian school system, so now students follow this specific pattern of courses that covers a large range of disciplines:		However, nowadays it is common to find licei offering (together with this programme of studies) courses in music theory and history of music or an in-depth course in science or maths, for one or two hours a week every year.		At the end, students must pass the Esame di Stato (until 1999 denominated Esame di maturità) to obtain their certificate.		
The baccalauréat (French pronunciation: ​[bakaloʁea]), often known in France colloquially as bac, is an academic qualification which French students take at the end of high school. It was introduced by Napoleon I in 1808. It is the main diploma required to pursue university studies. There is also the European Baccalaureate which students take at the end of the European School education. It confirms a rounded secondary education and gives access to a wide range of university education. It differs from British A levels and Scottish Highers, but is similar to the North American high school diploma, in that it is earned comprehensively and cannot be obtained in single subjects.						Much like British A levels or European Matura, the baccalauréat allows French and international students to obtain a standardised qualification, typically at the age of 18. It qualifies holders to work in certain areas, go on to tertiary education, or acquire some other professional qualification or training.		Although it is not legally required, the vast majority of students in their final year of secondary school take the exam. Unlike some US high school diplomas, it is an exam not for lycée completion but university entrance.		The word bac is also used to refer to one of the end-of-year exams that students must pass to get their baccalauréat diploma: le bac de philo, for example, is the philosophy exam, which all students must take, regardless of their field of study.		Within France, there are three main types of baccalauréat:		For entrance to regular universities within France, however, there are some restrictions as to the type of baccalauréat that can be presented. In some cases, it may be possible to enter a French university without the bac by taking a special exam, the diploma for entrance to higher education.		Though most students take the bac at the end of secondary school, it is also possible to enter as a candidat libre (literally, "free candidate") without affiliation to a school. Students who did not take the bac upon completion of secondary school (or did not manage to pass it) and would like to attend university, or feel that the bac would help them accomplish professional aspirations, may exercise this option. The exam is no different from the one administered to secondary-school students, except that free candidates are tested in Physical Education, whereas students' Physical Education grade is calculated based on evaluation throughout the year		The students who sit for the baccalauréat général choose one of three streams (termed séries) in the penultimate lycée year. Each stream results in a specialization and carries different weights (coefficients) associated with each subject. Another terminology is sometimes used, which existed before 1994, and further divided the different séries. Until this date, it was possible to sit for a bac C or D (which is now S), B (now ES), or A1, A2, A3 (now L). People who passed the baccalauréat before this reform still use this terminology when they mention their diploma. However, the streams for the baccalauréat général are now as follows:		The baccalauréat permits students to choose to sit for exams in over forty world languages or French regional languages (such as Alsatian, Catalan or Norman).		The S stream prepares students for work in scientific fields such as medicine, engineering and the natural sciences. Natural sciences students must specialise in either Mathematics, Physics & Chemistry, Computer science or Earth & Life Sciences.		bAdded to general subject above, except for computer science. cOnly points above 10 out of 20 (50%) are taken into consideration. Multiplied by two for first subject (except Latin & Greek, where the multiplier is 3) and by one for the second subject. dTwo-subject maximum.		Students of the Baccalauréat économique et social prepare for careers in the social sciences, in Philosophy (and other human sciences) in management and business administration, and in economics. The subject Economics & Social Sciences is the most heavily weighed and is only offered in this stream. History & Geography and Mathematics are also important subjects in ES.		(Same school curricula as L stream)		(Same school curricula as L stream)		bThe exam is 5-hours long for students in this specialization. An additional topic with a duration of 1 hour is distributed at the beginning of the test to all students who choose this specialization. cAdded to general subject above. dOnly points above 10 out of 20 (50%) are taken into consideration. Multiplied by two for first subject (except Latin & Greek, where the multiplier is three) and by one for the second subject. eTwo-subject maximum.		fThe highest coefficient is applied to students who choose this specialization.		gThe test of specialty economy is included in the compulsory economy test, it is the same thing for mathematics. The test of English specialty is not included in the compulsory English test.		Students in the L stream prepare for careers in the humanities such as education, linguistics, and public service. They also have interests in the arts. The most important subjects in the literary stream are Philosophy and French language & literature and other languages, usually English, German and Spanish.		(Same school curricula as ES stream)		(Same school curricula as ES stream)		bOnly points above 10 out of 20 (50%) are taken into consideration. Multiplied by two for first subject (except Latin & Greek, where the multiplier is three) and by one for the second subject. cTwo-subject maximum.		The majority of the baccalauréat examination takes place in a week in June. For lycée students, this is the end of the last year, terminale.		Most examinations are given in essay-form. The student is given a substantial block of time (depending on the exam, from two to five hours) to complete a multiple-page, well-argued paper. The number of pages filled-out varies from exam to exam but is usually substantial considering all answers have to be written down, explained and justified. Mathematics and science exams are problem sets but some science questions also require an essay-type answer. Foreign language exams often include a short translation section as well. In the S stream, the Mathematics and the Earth & Life Sciences tests sometimes contain some multiple-choice exams (questionnaire à choix multiples).		All students also have to work on a research project called the travaux personnels encadrés or TPE. These are generally conducted in groups of 2, 3 or 4 and focus on a subject determined by the students under supervision of a faculty member.		When taken in mainland France, the baccalauréat material is the same for all students in a given stream. Secrecy surrounding the material is very tight and the envelopes containing the exams are unsealed by a high-ranking school officer (usually a principal or vice-principal) in front of the examinees only a few minutes prior to the start of the examination. The procedure is the same for each subject, in each stream. Students usually have an identification number and an assigned seat. The number is written on all exam material and the name is hidden by folding and sealing the upper right corner of the examination sheet(s). In this fashion, anonymity is respected. The correcting staff is usually a member of the teaching staff in the same district or, at a larger scale, in the same académie. To avoid conflicts of interests, a teacher who has lectured to a student or group of students cannot grade their exam. Also, to ensure greater objectivity on the part of the examiners, the test is anonymous. The grader sees only an exam paper with a serial number, with all personally identifying material stripped away and forbidden from appearing, thus curbing any favoritism based upon sex, religion, national origin, or ethnicity.		Unlike the English GCSEs, Scottish Standard Grades or the American SAT, the French baccalauréat is not a completely standardised test. Since most answers — even for biology questions — are given in essay form, the grades may vary from grader to grader especially in subjects like philosophy and French literature.		Students generally take the French language and literature exam at the end of première, due to the fact that this subject is not taught in terminale (where it is replaced with a philosophy course). It also has an oral examination component, along with the written part. The oral exam covers works studied throughout première.		Each baccalauréat stream has its own set of subjects that each carry a different weight (coefficient). This allows some subjects to be more important than others. For example, in the ES stream, Economics & Social Science carry more weight than the Natural Sciences; so, the former is more important than the latter. Students usually study more for exams that carry heavier weights since the grade they obtain in these exams have a bigger impact on their mean grade. Whether or not one passes the bac and/or receives eventual honours are determined in the calculation of this mean.		The general baccalauréat offers several additional variants. The best known subset is the "option internationale du baccalauréat", the OIB. This is sometimes confusingly translated as the "French international baccalaureat". However it is unrelated to the International Baccalaureate (IB).		The OIB adds further subjects to the French national exam. Students choose one of the L, ES or S streams. It differs as students take a two-year syllabus in literature, history and geography in a foreign language. This syllabus and the way it is examined is modelled on the national exam of the target nation. For instance, the British Section (administered by the University of Cambridge) models the programmes on A-levels in English, History and Geography. It is therefore necessary to be fully bilingual to complete this qualification. To date there are 15 different sections supporting 14 different languages. The list is as follows: American (U.S.A), Arabic, British, Chinese, Danish, Dutch, German, Italian, Japanese, Norwegian, Polish, Portuguese, Russian, Spanish and Swedish.[1]		At the end of the "Terminale", OIB students have extra exams in Literature and History/Geography. These exams have a high weight in the final mark of the bac and they do not give extra points to the OIB students. Overall, these students work much more (up to an additional 10 hours per week of classes with a significant amount of required reading and homework attached as well) than the other general baccalauréat students and many of them tend to go to foreign universities. University admissions tutors often consider reducing the entrance requirements for students taking the OIB compared with those taking the standard French Baccalaureate, to reflect the additional demands of the OIB.[2]		Since the students that attend these schools make up a fairly small demographic, they tend to be spread over a far larger area than would traditionally be expected of a normal lycée or secondary school. As a consequence, many of these students must commute long distances, with one-hour trips each way being fairly common. Add these long commutes to the longer days and increased workload that come with the OIB, and the result is that this system is highly demanding of students, and it is not uncommon for those who cannot handle the workload to transfer to schools teaching the standard French baccalaureate. This is one of the main reasons why many consider the OIB qualification to be highly challenging, and that it is not only a sign of academic prowess but also one of tenacity and hard work.[citation needed]		To test their foreign or regional language students can choose among these different languages (all languages listed are not necessarily taught in all schools): English, German, Arabic, Armenian, Cambodian, Chinese, Danish, Spanish, Finnish, Modern Greek, Hebrew, Italian, Japanese, Dutch, Norwegian, Persian, Polish, Portuguese, Russian, Turkish, Vietnamese; regional languages: Basque, Breton, Catalan, Corsican, Auvergnat, Gascon, Languedoc, Limousin, Niçard, Provençal, Vivaro-Alpine, regional languages of Alsace regional languages of Moselle), Tahitian, Albanian, Amharic, Melanesian languages, Bambara, Berber, Bulgarian, Cambodian, Korean, Croatian, Hausa, Hindi, Hungarian, Indonesian, Malaysian, Laotian, Lithuanian, Macedonian, Malagasy, Persian, Fulani, Romanian, Serbian, Slovak, Slovenian, Swedish, Swahili, Tamil, Czech.[3]		The pass mark is 10 out of 20. The 2014 success rate for the baccalauréat in mainland France was 87.9%.[4]		For the baccalauréat, three levels of honours are given:		Honours are prestigious but not crucial, as admissions to the classes préparatoires (or preparatory classes, which prepare students for the grande école exams) are decided months before the exam.		French educators seldom use the entire grading scale. The same applies when marking the baccalauréat. Therefore, students are very unlikely to get a 20 out of 20 or more (it is actually possible to get more than 20, thanks to options). It is also very rare to see scores lower than 5.		Grade inflation has become a concern. Between 2005 and 2016 the proportion of students receiving an honour in the general bac doubled.[5]		European section is an option in French high schools in order to teach a subject through a European language other than French. It also gives pupils the opportunity of having more hours in the language studied. It is also an opportunity to learn more about the culture of the country of which you're speaking the language. That is to say, if you learn History in Spanish, you will mainly study the History of Spain and that of Central and South America. Teachers present their lessons in English, German, Italian or Spanish.		At the end of their high school, students can receive a "European section" mention on their baccalaureat. In order to have this mention, they need to get at least a 12/20 mark at their language exam and 10/20 mark at an additional oral exam on the subject in the language.		For example, if you chose to study History in Spanish as an additional subject, you will have to take your Spanish exam like the rest of your classmates (that do not have History in Spanish) and get at least a 12/20 mark, and you will also have to pass an oral exam discussing history in Spanish language, and you have to get at least a 10/20 mark at this exam.		If a student averages between 8 and 10, he or she is permitted to sit for the épreuve de rattrapage (also called the second groupe), a supplemental oral exam given in two subjects of the student's choice. If the student does well enough in these exams to raise the overall weighted grade to a 10, then he or she receives his or her baccalauréat. If the student does poorly in the orals and receives below a 10, he or she may choose to repeat the final year of lycée (terminale).		The student cannot choose to re-sit the entire examination in September, as the September exams may only be taken by those who have not been able to take the June exams for serious reasons (such as illness).		There are a small number of schools which prepare students for the baccalauréat in the United States. Otherwise, it is possible to prepare for the baccalauréat with the CNED, a French public institution under the oversight of the department of education dedicated to providing distance learning material. It can, of course, only be taken after completion of the necessary coursework, which is entirely in French. Upon receiving the baccalauréat, students wishing to pursue post-secondary studies in the US generally will submit their lycée/high school transcripts to a college or university office of undergraduate admissions. If it is decided that the coursework, along with American standardized test scores, application essays, and letters of recommendation, merits admission, students holding the baccalauréat will be admitted to the undergraduate program to which they have applied.		
A business school is a university-level institution that confers degrees in business administration or management. Such a school can also be known as school of management, school of business administration, or, colloquially, b-school or biz school. A business school teaches topics such as accounting, administration, strategy, economics, entrepreneurship, finance, human resource management, management science, management information systems, international business, logistics, marketing, organizational psychology, organizational behavior, public relations, research methods and real estate among others.						There are several forms of business schools, including a school of business, business administration, and management.		Common degrees are as follows.		Some business schools structure their teaching around the use of case studies (i.e. the case method). Case studies have been used in Graduate and Undergraduate business education for nearly one hundred years. Business cases are historical descriptions of actual business situations. Typically, information is presented about a business firm's products, markets, competition, financial structure, sales volumes, management, employees and other factors influencing the firm's success. The length of a business case study may range from two or three pages to 30 pages, or more.		Business schools often obtain case studies published by the Harvard Business School, INSEAD, the Ross School of Business at the University of Michigan, the Richard Ivey School of Business at The University of Western Ontario, the Darden School at the University of Virginia, IESE, other academic institutions, or case clearing houses (such as The Case Centre). Harvard's most popular case studies include Lincoln Electric Co.[42] and Google, Inc.[43]		Students are expected to scrutinize the case study and prepare to discuss strategies and tactics that the firm should employ in the future. Three different methods have been used in business case teaching:		When Harvard Business School started operating in 1908, the faculty realized that there were no textbooks suitable for a graduate program in business.[45] Their first solution to this problem involved interviewing leading practitioners of business and writing detailed accounts of what these managers were doing, based partly on the case method already in use at Harvard Law School. Of course, the professors could not present these cases as practices to be emulated, because there were no criteria available for determining what would succeed and what would not succeed. So the professors instructed their students to read the cases and to come to class prepared to discuss the cases and to offer recommendations for appropriate courses of action. The basic outlines of this method still operate in business-school curricula as of 2016[update].		In contrast to the case method some schools use a skills-based approach in teaching business. This approach emphasizes quantitative methods, in particular operations research, management information systems, statistics, organizational behavior, modeling and simulation, and decision science. The leading institution in this method is the Tepper School of Business at Carnegie Mellon University. The goal is to provide students a set of tools that will prepare them to tackle and solve problems.		Another important approach used in business school is the use of business games that are used in different disciplines such as business, economics, management, etc. Some colleges are blending many of these approaches throughout their degree programs, and even blending the method of delivery for each of these approaches. A study from by Inside Higher Ed and the Babson Survey Research Group[46] shows that there is still disagreement as to the effectiveness of the approaches but the reach and accessibility is proving to be more and more appealing. Liberal arts colleges in the United States like New England College,[47] Wesleyan University,[48] and Bryn Mawr College are now offering complete online degrees in many business curriculae despite the controversy that surrounds the learning method.		There are also several business schools which still rely on the lecture method to give students a basic business education. Lectures are generally given from the professor's point of view, and rarely require interaction from the students unless notetaking is required. Lecture as a method of teaching in business schools has been criticized by experts for reducing the incentive and individualism in the learning experience.[49]		In addition to teaching students, many business schools run Executive Education programs. These may be either open programs or company-specific programs. Executives may also acquire an MBA title in an Executive MBA program within university of business or from top ranked business schools. Many business schools seek close co-operation with business.[50]		There are three main accreditation agencies for business schools in the United States. ACBSP, AACSB, and the IACBE. In Europe, the EQUIS accreditation system is run by the EFMD.		Each year, well-known business publications such as The Economist,[51] Eduniversal,[52] U.S. News & World Report,[53][54] Fortune, Financial Times,[55] Business Week[56] and The Wall Street Journal[57] publish rankings of selected MBA programs and business schools that, while controversial in their methodology,[58] nevertheless can directly influence the prestige of schools that achieve high scores. Academic research is also considered to be an important feature and popular way to gauge the prestige of business schools.[59][60][61]		
In the history of Europe, the Middle Ages or Medieval Period lasted from the 5th to the 15th century. It began with the fall of the Western Roman Empire and merged into the Renaissance and the Age of Discovery. The Middle Ages is the middle period of the three traditional divisions of Western history: classical antiquity, the medieval period, and the modern period. The medieval period is itself subdivided into the Early, High, and Late Middle Ages.		Population decline, counterurbanisation, invasion, and movement of peoples, which had begun in Late Antiquity, continued in the Early Middle Ages. The large-scale movements of the Migration Period, including various Germanic peoples, formed new kingdoms in what remained of the Western Roman Empire. In the seventh century, North Africa and the Middle East—once part of the Byzantine Empire—came under the rule of the Umayyad Caliphate, an Islamic empire, after conquest by Muhammad's successors. Although there were substantial changes in society and political structures, the break with classical antiquity was not complete. The still-sizeable Byzantine Empire survived in the east and remained a major power. The empire's law code, the Corpus Juris Civilis or "Code of Justinian", was rediscovered in Northern Italy in 1070 and became widely admired later in the Middle Ages. In the West, most kingdoms incorporated the few extant Roman institutions. Monasteries were founded as campaigns to Christianise pagan Europe continued. The Franks, under the Carolingian dynasty, briefly established the Carolingian Empire during the later 8th and early 9th century. It covered much of Western Europe but later succumbed to the pressures of internal civil wars combined with external invasions—Vikings from the north, Hungarians from the east, and Saracens from the south.		During the High Middle Ages, which began after 1000, the population of Europe increased greatly as technological and agricultural innovations allowed trade to flourish and the Medieval Warm Period climate change allowed crop yields to increase. Manorialism, the organisation of peasants into villages that owed rent and labour services to the nobles, and feudalism, the political structure whereby knights and lower-status nobles owed military service to their overlords in return for the right to rent from lands and manors, were two of the ways society was organised in the High Middle Ages. The Crusades, first preached in 1095, were military attempts by Western European Christians to regain control of the Holy Land from Muslims. Kings became the heads of centralised nation states, reducing crime and violence but making the ideal of a unified Christendom more distant. Intellectual life was marked by scholasticism, a philosophy that emphasised joining faith to reason, and by the founding of universities. The theology of Thomas Aquinas, the paintings of Giotto, the poetry of Dante and Chaucer, the travels of Marco Polo, and the Gothic architecture of cathedrals such as Chartres are among the outstanding achievements toward the end of this period and into the Late Middle Ages.		The Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350, the Black Death killed about a third of Europeans. Controversy, heresy, and the Western Schism within the Catholic Church paralleled the interstate conflict, civil strife, and peasant revolts that occurred in the kingdoms. Cultural and technological developments transformed European society, concluding the Late Middle Ages and beginning the early modern period.						The Middle Ages is one of the three major periods in the most enduring scheme for analysing European history: classical civilisation, or Antiquity; the Middle Ages; and the Modern Period.[1]		Medieval writers divided history into periods such as the "Six Ages" or the "Four Empires", and considered their time to be the last before the end of the world.[2] When referring to their own times, they spoke of them as being "modern".[3] In the 1330s, the humanist and poet Petrarch referred to pre-Christian times as antiqua (or "ancient") and to the Christian period as nova (or "new").[4] Leonardo Bruni was the first historian to use tripartite periodisation in his History of the Florentine People (1442).[5] Bruni and later historians argued that Italy had recovered since Petrarch's time, and therefore added a third period to Petrarch's two. The "Middle Ages" first appears in Latin in 1469 as media tempestas or "middle season".[6] In early usage, there were many variants, including medium aevum, or "middle age", first recorded in 1604,[7] and media saecula, or "middle ages", first recorded in 1625.[8] The alternative term "medieval" (or occasionally "mediaeval"[9] or "mediæval")[10] derives from medium aevum.[9] Tripartite periodisation became standard after the German 17th-century historian Christoph Cellarius divided history into three periods: Ancient, Medieval, and Modern.[8]		The most commonly given starting point for the Middle Ages is 476,[11] first used by Bruni.[5][A] For Europe as a whole, 1500 is often considered to be the end of the Middle Ages,[13] but there is no universally agreed upon end date. Depending on the context, events such as Christopher Columbus's first voyage to the Americas in 1492, the conquest of Constantinople by the Turks in 1453, or the Protestant Reformation in 1517 are sometimes used.[14] English historians often use the Battle of Bosworth Field in 1485 to mark the end of the period.[15] For Spain, dates commonly used are the death of King Ferdinand II in 1516, the death of Queen Isabella I of Castile in 1504, or the conquest of Granada in 1492.[16] Historians from Romance-speaking countries tend to divide the Middle Ages into two parts: an earlier "High" and later "Low" period. English-speaking historians, following their German counterparts, generally subdivide the Middle Ages into three intervals: "Early", "High", and "Late".[1] In the 19th century, the entire Middle Ages were often referred to as the "Dark Ages",[17][B] but with the adoption of these subdivisions, use of this term was restricted to the Early Middle Ages, at least among historians.[2]		The Roman Empire reached its greatest territorial extent during the second century AD; the following two centuries witnessed the slow decline of Roman control over its outlying territories.[19] Economic issues, including inflation, and external pressure on the frontiers combined to create the Crisis of the Third Century, with emperors coming to the throne only to be rapidly replaced by new usurpers.[20] Military expenses increased steadily during the third century, mainly in response to the war with the Sasanian Empire, which revived in the middle of the third century.[21] The army doubled in size, and cavalry and smaller units replaced the Roman legion as the main tactical unit.[22] The need for revenue led to increased taxes and a decline in numbers of the curial, or landowning, class, and decreasing numbers of them willing to shoulder the burdens of holding office in their native towns.[21] More bureaucrats were needed in the central administration to deal with the needs of the army, which led to complaints from civilians that there were more tax-collectors in the empire than tax-payers.[22]		The Emperor Diocletian (r. 284–305) split the empire into separately administered eastern and western halves in 286; the empire was not considered divided by its inhabitants or rulers, as legal and administrative promulgations in one division were considered valid in the other.[23][C] In 330, after a period of civil war, Constantine the Great (r. 306–337) refounded the city of Byzantium as the newly renamed eastern capital, Constantinople.[24] Diocletian's reforms strengthened the governmental bureaucracy, reformed taxation, and strengthened the army, which bought the empire time but did not resolve the problems it was facing: excessive taxation, a declining birthrate, and pressures on its frontiers, among others.[25] Civil war between rival emperors became common in the middle of the 4th century, diverting soldiers from the empire's frontier forces and allowing invaders to encroach.[26] For much of the 4th century, Roman society stabilised in a new form that differed from the earlier classical period, with a widening gulf between the rich and poor, and a decline in the vitality of the smaller towns.[27] Another change was the Christianisation, or conversion of the empire to Christianity, a gradual process that lasted from the 2nd to the 5th centuries.[28][29]		In 376, the Goths, fleeing from the Huns, received permission from Emperor Valens (r. 364–378) to settle in the Roman province of Thracia in the Balkans. The settlement did not go smoothly, and when Roman officials mishandled the situation, the Goths began to raid and plunder.[D] Valens, attempting to put down the disorder, was killed fighting the Goths at the Battle of Adrianople on 9 August 378.[31] As well as the threat from such tribal confederacies from the north, internal divisions within the empire, especially within the Christian Church, caused problems.[32] In 400, the Visigoths invaded the Western Roman Empire and, although briefly forced back from Italy, in 410 sacked the city of Rome.[33] In 406 the Alans, Vandals, and Suevi crossed into Gaul; over the next three years they spread across Gaul and in 409 crossed the Pyrenees Mountains into modern-day Spain.[34] The Migration Period began, when various peoples, initially largely Germanic peoples, moved across Europe. The Franks, Alemanni, and the Burgundians all ended up in northern Gaul while the Angles, Saxons, and Jutes settled in Britain,[35] and the Vandals went on to cross the strait of Gibraltar after which they conquered the province of Africa.[36] In the 430s the Huns began invading the empire; their king Attila (r. 434–453) led invasions into the Balkans in 442 and 447, Gaul in 451, and Italy in 452.[37] The Hunnic threat remained until Attila's death in 453, when the Hunnic confederation he led fell apart.[38] These invasions by the tribes completely changed the political and demographic nature of what had been the Western Roman Empire.[35]		By the end of the 5th century the western section of the empire was divided into smaller political units, ruled by the tribes that had invaded in the early part of the century.[39] The deposition of the last emperor of the west, Romulus Augustulus, in 476 has traditionally marked the end of the Western Roman Empire.[12][E] By 493 the Italian peninsula was conquered by the Ostrogoths.[40] The Eastern Roman Empire, often referred to as the Byzantine Empire after the fall of its western counterpart, had little ability to assert control over the lost western territories. The Byzantine emperors maintained a claim over the territory, but while none of the new kings in the west dared to elevate himself to the position of emperor of the west, Byzantine control of most of the Western Empire could not be sustained; the reconquest of the Mediterranean periphery and the Italian Peninsula (Gothic War) in the reign of Justinian (r. 527–565) was the sole, and temporary, exception.[41]		The political structure of Western Europe changed with the end of the united Roman Empire. Although the movements of peoples during this period are usually described as "invasions", they were not just military expeditions but migrations of entire peoples into the empire. Such movements were aided by the refusal of the Western Roman elites to support the army or pay the taxes that would have allowed the military to suppress the migration.[42] The emperors of the 5th century were often controlled by military strongmen such as Stilicho (d. 408), Aetius (d. 454), Aspar (d. 471), Ricimer (d. 472), or Gundobad (d. 516), who were partly or fully of non-Roman background. When the line of Western emperors ceased, many of the kings who replaced them were from the same background. Intermarriage between the new kings and the Roman elites was common.[43] This led to a fusion of Roman culture with the customs of the invading tribes, including the popular assemblies that allowed free male tribal members more say in political matters than was common in the Roman state.[44] Material artefacts left by the Romans and the invaders are often similar, and tribal items were often modelled on Roman objects.[45] Much of the scholarly and written culture of the new kingdoms was also based on Roman intellectual traditions.[46] An important difference was the gradual loss of tax revenue by the new polities. Many of the new political entities no longer supported their armies through taxes, instead relying on granting them land or rents. This meant there was less need for large tax revenues and so the taxation systems decayed.[47] Warfare was common between and within the kingdoms. Slavery declined as the supply weakened, and society became more rural.[48][F]		Between the 5th and 8th centuries, new peoples and individuals filled the political void left by Roman centralised government.[46] The Ostrogoths, a Gothic tribe, settled in Roman Italy in the late fifth century under Theoderic the Great (d. 526) and set up a kingdom marked by its co-operation between the Italians and the Ostrogoths, at least until the last years of Theodoric's reign.[50] The Burgundians settled in Gaul, and after an earlier realm was destroyed by the Huns in 436 formed a new kingdom in the 440s. Between today's Geneva and Lyon, it grew to become the realm of Burgundy in the late 5th and early 6th centuries.[51] Elsewhere in Gaul, the Franks and Celtic Britons set up small polities. Francia was centred in northern Gaul, and the first king of whom much is known is Childeric I (d. 481). His grave was discovered in 1653 and is remarkable for its grave goods, which included weapons and a large quantity of gold.[52]		Under Childeric's son Clovis I (r. 509–511), the founder of the Merovingian dynasty, the Frankish kingdom expanded and converted to Christianity. The Britons, related to the natives of Britannia – modern-day Great Britain – settled in what is now Brittany.[53][G] Other monarchies were established by the Visigothic Kingdom in the Iberian Peninsula, the Suebi in northwestern Iberia, and the Vandal Kingdom in North Africa.[51] In the sixth century, the Lombards settled in Northern Italy, replacing the Ostrogothic kingdom with a grouping of duchies that occasionally selected a king to rule over them all. By the late sixth century, this arrangement had been replaced by a permanent monarchy, the Kingdom of the Lombards.[54]		The invasions brought new ethnic groups to Europe, although some regions received a larger influx of new peoples than others. In Gaul for instance, the invaders settled much more extensively in the north-east than in the south-west. Slavs settled in Central and Eastern Europe and the Balkan Peninsula. The settlement of peoples was accompanied by changes in languages. The Latin of the Western Roman Empire was gradually replaced by languages based on, but distinct from, Latin, collectively known as Romance languages. These changes from Latin to the new languages took many centuries. Greek remained the language of the Byzantine Empire, but the migrations of the Slavs added Slavic languages to Eastern Europe.[55]		As Western Europe witnessed the formation of new kingdoms, the Eastern Roman Empire remained intact and experienced an economic revival that lasted into the early 7th century. There were fewer invasions of the eastern section of the empire; most occurred in the Balkans. Peace with the Sasanian Empire, the traditional enemy of Rome, lasted throughout most of the 5th century. The Eastern Empire was marked by closer relations between the political state and Christian Church, with doctrinal matters assuming an importance in Eastern politics that they did not have in Western Europe. Legal developments included the codification of Roman law; the first effort—the Codex Theodosianus—was completed in 438.[57] Under Emperor Justinian (r. 527–565), another compilation took place—the Corpus Juris Civilis.[58] Justinian also oversaw the construction of the Hagia Sophia in Constantinople and the reconquest of North Africa from the Vandals and Italy from the Ostrogoths,[59] under Belisarius (d. 565).[60] The conquest of Italy was not complete, as a deadly outbreak of plague in 542 led to the rest of Justinian's reign concentrating on defensive measures rather than further conquests.[59]		At the Emperor's death, the Byzantines had control of most of Italy, North Africa, and a small foothold in southern Spain. Justinian's reconquests have been criticised by historians for overextending his realm and setting the stage for the early Muslim conquests, but many of the difficulties faced by Justinian's successors were due not just to over-taxation to pay for his wars but to the essentially civilian nature of the empire, which made raising troops difficult.[61]		In the Eastern Empire the slow infiltration of the Balkans by the Slavs added a further difficulty for Justinian's successors. It began gradually, but by the late 540s Slavic tribes were in Thrace and Illyrium, and had defeated an imperial army near Adrianople in 551. In the 560s the Avars began to expand from their base on the north bank of the Danube; by the end of the 6th century they were the dominant power in Central Europe and routinely able to force the Eastern emperors to pay tribute. They remained a strong power until 796.[62]		An additional problem to face the empire came as a result of the involvement of Emperor Maurice (r. 582–602) in Persian politics when he intervened in a succession dispute. This led to a period of peace, but when Maurice was overthrown, the Persians invaded and during the reign of Emperor Heraclius (r. 610–641) controlled large chunks of the empire, including Egypt, Syria, and Anatolia until Heraclius' successful counterattack. In 628 the empire secured a peace treaty and recovered all of its lost territories.[63]		In Western Europe, some of the older Roman elite families died out while others became more involved with ecclesiastical than secular affairs. Values attached to Latin scholarship and education mostly disappeared, and while literacy remained important, it became a practical skill rather than a sign of elite status. In the 4th century, Jerome (d. 420) dreamed that God rebuked him for spending more time reading Cicero than the Bible. By the 6th century, Gregory of Tours (d. 594) had a similar dream, but instead of being chastised for reading Cicero, he was chastised for learning shorthand.[64] By the late 6th century, the principal means of religious instruction in the Church had become music and art rather than the book.[65] Most intellectual efforts went towards imitating classical scholarship, but some original works were created, along with now-lost oral compositions. The writings of Sidonius Apollinaris (d. 489), Cassiodorus (d. c. 585), and Boethius (d. c. 525) were typical of the age.[66]		Changes also took place among laymen, as aristocratic culture focused on great feasts held in halls rather than on literary pursuits. Clothing for the elites was richly embellished with jewels and gold. Lords and kings supported entourages of fighters who formed the backbone of the military forces.[H] Family ties within the elites were important, as were the virtues of loyalty, courage, and honour. These ties led to the prevalence of the feud in aristocratic society, examples of which included those related by Gregory of Tours that took place in Merovingian Gaul. Most feuds seem to have ended quickly with the payment of some sort of compensation.[69] Women took part in aristocratic society mainly in their roles as wives and mothers of men, with the role of mother of a ruler being especially prominent in Merovingian Gaul. In Anglo-Saxon society the lack of many child rulers meant a lesser role for women as queen mothers, but this was compensated for by the increased role played by abbesses of monasteries. Only in Italy does it appear that women were always considered under the protection and control of a male relative.[70]		Peasant society is much less documented than the nobility. Most of the surviving information available to historians comes from archaeology; few detailed written records documenting peasant life remain from before the 9th century. Most of the descriptions of the lower classes come from either law codes or writers from the upper classes.[71] Landholding patterns in the West were not uniform; some areas had greatly fragmented landholding patterns, but in other areas large contiguous blocks of land were the norm. These differences allowed for a wide variety of peasant societies, some dominated by aristocratic landholders and others having a great deal of autonomy.[72] Land settlement also varied greatly. Some peasants lived in large settlements that numbered as many as 700 inhabitants. Others lived in small groups of a few families and still others lived on isolated farms spread over the countryside. There were also areas where the pattern was a mix of two or more of those systems.[73] Unlike in the late Roman period, there was no sharp break between the legal status of the free peasant and the aristocrat, and it was possible for a free peasant's family to rise into the aristocracy over several generations through military service to a powerful lord.[74]		Roman city life and culture changed greatly in the early Middle Ages. Although Italian cities remained inhabited, they contracted significantly in size. Rome, for instance, shrank from a population of hundreds of thousands to around 30,000 by the end of the 6th century. Roman temples were converted into Christian churches and city walls remained in use.[75] In Northern Europe, cities also shrank, while civic monuments and other public buildings were raided for building materials. The establishment of new kingdoms often meant some growth for the towns chosen as capitals.[76] Although there had been Jewish communities in many Roman cities, the Jews suffered periods of persecution after the conversion of the empire to Christianity. Officially they were tolerated, if subject to conversion efforts, and at times were even encouraged to settle in new areas.[77]		Religious beliefs in the Eastern Empire and Iran were in flux during the late sixth and early seventh centuries. Judaism was an active proselytising faith, and at least one Arab political leader converted to it.[I] Christianity had active missions competing with the Persians' Zoroastrianism in seeking converts, especially among residents of the Arabian Peninsula. All these strands came together with the emergence of Islam in Arabia during the lifetime of Muhammad (d. 632).[79] After his death, Islamic forces conquered much of the Eastern Empire and Persia, starting with Syria in 634–635 and reaching Egypt in 640–641, Persia between 637 and 642, North Africa in the later seventh century, and the Iberian Peninsula in 711.[80] By 714, Islamic forces controlled much of the peninsula in a region they called Al-Andalus.[81]		The Islamic conquests reached their peak in the mid-eighth century. The defeat of Muslim forces at the Battle of Tours in 732 led to the reconquest of southern France by the Franks, but the main reason for the halt of Islamic growth in Europe was the overthrow of the Umayyad Caliphate and its replacement by the Abbasid Caliphate. The Abbasids moved their capital to Baghdad and were more concerned with the Middle East than Europe, losing control of sections of the Muslim lands. Umayyad descendants took over the Iberian Peninsula, the Aghlabids controlled North Africa, and the Tulunids became rulers of Egypt.[82] By the middle of the 8th century, new trading patterns were emerging in the Mediterranean; trade between the Franks and the Arabs replaced the old Roman economy. Franks traded timber, furs, swords and slaves in return for silks and other fabrics, spices, and precious metals from the Arabs.[83]		The migrations and invasions of the 4th and 5th centuries disrupted trade networks around the Mediterranean. African goods stopped being imported into Europe, first disappearing from the interior and by the 7th century found only in a few cities such as Rome or Naples. By the end of the 7th century, under the impact of the Muslim conquests, African products were no longer found in Western Europe. The replacement of goods from long-range trade with local products was a trend throughout the old Roman lands that happened in the Early Middle Ages. This was especially marked in the lands that did not lie on the Mediterranean, such as northern Gaul or Britain. Non-local goods appearing in the archaeological record are usually luxury goods. In the northern parts of Europe, not only were the trade networks local, but the goods carried were simple, with little pottery or other complex products. Around the Mediterranean, pottery remained prevalent and appears to have been traded over medium-range networks, not just produced locally.[84]		The various Germanic states in the west all had coinages that imitated existing Roman and Byzantine forms. Gold continued to be minted until the end of the 7th century, when it was replaced by silver coins. The basic Frankish silver coin was the denarius or denier, while the Anglo-Saxon version was called a penny. From these areas, the denier or penny spread throughout Europe during the centuries from 700 to 1000. Copper or bronze coins were not struck, nor were gold except in Southern Europe. No silver coins denominated in multiple units were minted.[85]		Christianity was a major unifying factor between Eastern and Western Europe before the Arab conquests, but the conquest of North Africa sundered maritime connections between those areas. Increasingly the Byzantine Church differed in language, practices, and liturgy from the Western Church. The Eastern Church used Greek instead of the Western Latin. Theological and political differences emerged, and by the early and middle 8th century issues such as iconoclasm, clerical marriage, and state control of the Church had widened to the extent that the cultural and religious differences were greater than the similarities.[86] The formal break, known as the East–West Schism, came in 1054, when the papacy and the patriarchy of Constantinople clashed over papal supremacy and excommunicated each other, which led to the division of Christianity into two Churches—the Western branch became the Roman Catholic Church and the Eastern branch the Eastern Orthodox Church.[87]		The ecclesiastical structure of the Roman Empire survived the movements and invasions in the west mostly intact, but the papacy was little regarded, and few of the Western bishops looked to the bishop of Rome for religious or political leadership. Many of the popes prior to 750 were more concerned with Byzantine affairs and Eastern theological controversies. The register, or archived copies of the letters, of Pope Gregory the Great (pope 590–604) survived, and of those more than 850 letters, the vast majority were concerned with affairs in Italy or Constantinople. The only part of Western Europe where the papacy had influence was Britain, where Gregory had sent the Gregorian mission in 597 to convert the Anglo-Saxons to Christianity.[88] Irish missionaries were most active in Western Europe between the 5th and the 7th centuries, going first to England and Scotland and then on to the continent. Under such monks as Columba (d. 597) and Columbanus (d. 615), they founded monasteries, taught in Latin and Greek, and authored secular and religious works.[89]		The Early Middle Ages witnessed the rise of monasticism in the West. The shape of European monasticism was determined by traditions and ideas that originated with the Desert Fathers of Egypt and Syria. Most European monasteries were of the type that focuses on community experience of the spiritual life, called cenobitism, which was pioneered by Pachomius (d. 348) in the 4th century. Monastic ideals spread from Egypt to Western Europe in the 5th and 6th centuries through hagiographical literature such as the Life of Anthony.[90] Benedict of Nursia (d. 547) wrote the Benedictine Rule for Western monasticism during the 6th century, detailing the administrative and spiritual responsibilities of a community of monks led by an abbot.[91] Monks and monasteries had a deep effect on the religious and political life of the Early Middle Ages, in various cases acting as land trusts for powerful families, centres of propaganda and royal support in newly conquered regions, and bases for missions and proselytisation.[92] They were the main and sometimes only outposts of education and literacy in a region. Many of the surviving manuscripts of the Latin classics were copied in monasteries in the Early Middle Ages.[93] Monks were also the authors of new works, including history, theology, and other subjects, written by authors such as Bede (d. 735), a native of northern England who wrote in the late 7th and early 8th centuries.[94]		The Frankish kingdom in northern Gaul split into kingdoms called Austrasia, Neustria, and Burgundy during the 6th and 7th centuries, all of them ruled by the Merovingian dynasty, who were descended from Clovis. The 7th century was a tumultuous period of wars between Austrasia and Neustria.[95] Such warfare was exploited by Pippin (d. 640), the Mayor of the Palace for Austrasia who became the power behind the Austrasian throne. Later members of his family inherited the office, acting as advisers and regents. One of his descendants, Charles Martel (d. 741), won the Battle of Poitiers in 732, halting the advance of Muslim armies across the Pyrenees.[96][J] Great Britain was divided into small states dominated by the kingdoms of Northumbria, Mercia, Wessex, and East Anglia, which were descended from the Anglo-Saxon invaders. Smaller kingdoms in present-day Wales and Scotland were still under the control of the native Britons and Picts.[98] Ireland was divided into even smaller political units, usually known as tribal kingdoms, under the control of kings. There were perhaps as many as 150 local kings in Ireland, of varying importance.[99]		The Carolingian dynasty, as the successors to Charles Martel are known, officially took control of the kingdoms of Austrasia and Neustria in a coup of 753 led by Pippin III (r. 752–768). A contemporary chronicle claims that Pippin sought, and gained, authority for this coup from Pope Stephen II (pope 752–757). Pippin's takeover was reinforced with propaganda that portrayed the Merovingians as inept or cruel rulers, exalted the accomplishments of Charles Martel, and circulated stories of the family's great piety. At the time of his death in 768, Pippin left his kingdom in the hands of his two sons, Charles (r. 768–814) and Carloman (r. 768–771). When Carloman died of natural causes, Charles blocked the succession of Carloman's young son and installed himself as the king of the united Austrasia and Neustria. Charles, more often known as Charles the Great or Charlemagne, embarked upon a programme of systematic expansion in 774 that unified a large portion of Europe, eventually controlling modern-day France, northern Italy, and Saxony. In the wars that lasted beyond 800, he rewarded allies with war booty and command over parcels of land.[100] In 774, Charlemagne conquered the Lombards, which freed the papacy from the fear of Lombard conquest and marked the beginnings of the Papal States.[101][K]		The coronation of Charlemagne as emperor on Christmas Day 800 is regarded as a turning point in medieval history, marking a return of the Western Roman Empire, since the new emperor ruled over much of the area previously controlled by the Western emperors.[104] It also marks a change in Charlemagne's relationship with the Byzantine Empire, as the assumption of the imperial title by the Carolingians asserted their equivalence to the Byzantine state.[105] There were several differences between the newly established Carolingian Empire and both the older Western Roman Empire and the concurrent Byzantine Empire. The Frankish lands were rural in character, with only a few small cities. Most of the people were peasants settled on small farms. Little trade existed and much of that was with the British Isles and Scandinavia, in contrast to the older Roman Empire with its trading networks centred on the Mediterranean.[104] The empire was administered by an itinerant court that travelled with the emperor, as well as approximately 300 imperial officials called counts, who administered the counties the empire had been divided into. Clergy and local bishops served as officials, as well as the imperial officials called missi dominici, who served as roving inspectors and troubleshooters.[106]		Charlemagne's court in Aachen was the centre of the cultural revival sometimes referred to as the "Carolingian Renaissance". Literacy increased, as did development in the arts, architecture and jurisprudence, as well as liturgical and scriptural studies. The English monk Alcuin (d. 804) was invited to Aachen and brought the education available in the monasteries of Northumbria. Charlemagne's chancery—or writing office—made use of a new script today known as Carolingian minuscule,[L] allowing a common writing style that advanced communication across much of Europe. Charlemagne sponsored changes in church liturgy, imposing the Roman form of church service on his domains, as well as the Gregorian chant in liturgical music for the churches. An important activity for scholars during this period was the copying, correcting, and dissemination of basic works on religious and secular topics, with the aim of encouraging learning. New works on religious topics and schoolbooks were also produced.[108] Grammarians of the period modified the Latin language, changing it from the Classical Latin of the Roman Empire into a more flexible form to fit the needs of the Church and government. By the reign of Charlemagne, the language had so diverged from the classical that it was later called Medieval Latin.[109]		Charlemagne planned to continue the Frankish tradition of dividing his kingdom between all his heirs, but was unable to do so as only one son, Louis the Pious (r. 814–840), was still alive by 813. Just before Charlemagne died in 814, he crowned Louis as his successor. Louis's reign of 26 years was marked by numerous divisions of the empire among his sons and, after 829, civil wars between various alliances of father and sons over the control of various parts of the empire. Eventually, Louis recognised his eldest son Lothair I (d. 855) as emperor and gave him Italy. Louis divided the rest of the empire between Lothair and Charles the Bald (d. 877), his youngest son. Lothair took East Francia, comprising both banks of the Rhine and eastwards, leaving Charles West Francia with the empire to the west of the Rhineland and the Alps. Louis the German (d. 876), the middle child, who had been rebellious to the last, was allowed to keep Bavaria under the suzerainty of his elder brother. The division was disputed. Pepin II of Aquitaine (d. after 864), the emperor's grandson, rebelled in a contest for Aquitaine, while Louis the German tried to annex all of East Francia. Louis the Pious died in 840, with the empire still in chaos.[110]		A three-year civil war followed his death. By the Treaty of Verdun (843), a kingdom between the Rhine and Rhone rivers was created for Lothair to go with his lands in Italy, and his imperial title was recognised. Louis the German was in control of Bavaria and the eastern lands in modern-day Germany. Charles the Bald received the western Frankish lands, comprising most of modern-day France.[110] Charlemagne's grandsons and great-grandsons divided their kingdoms between their descendants, eventually causing all internal cohesion to be lost.[111][M] In 987 the Carolingian dynasty was replaced in the western lands, with the crowning of Hugh Capet (r. 987–996) as king.[N][O] In the eastern lands the dynasty had died out earlier, in 911, with the death of Louis the Child,[114] and the selection of the unrelated Conrad I (r. 911–918) as king.[115]		The breakup of the Carolingian Empire was accompanied by invasions, migrations, and raids by external foes. The Atlantic and northern shores were harassed by the Vikings, who also raided the British Isles and settled there as well as in Iceland. In 911, the Viking chieftain Rollo (d. c. 931) received permission from the Frankish King Charles the Simple (r. 898–922) to settle in what became Normandy.[116][P] The eastern parts of the Frankish kingdoms, especially Germany and Italy, were under continual Magyar assault until the invader's defeat at the Battle of Lechfeld in 955.[118] The breakup of the Abbasid dynasty meant that the Islamic world fragmented into smaller political states, some of which began expanding into Italy and Sicily, as well as over the Pyrenees into the southern parts of the Frankish kingdoms.[119]		Efforts by local kings to fight the invaders led to the formation of new political entities. In Anglo-Saxon England, King Alfred the Great (r. 871–899) came to an agreement with the Viking invaders in the late 9th century, resulting in Danish settlements in Northumbria, Mercia, and parts of East Anglia.[120] By the middle of the 10th century, Alfred's successors had conquered Northumbria, and restored English control over most of the southern part of Great Britain.[121] In northern Britain, Kenneth MacAlpin (d. c. 860) united the Picts and the Scots into the Kingdom of Alba.[122] In the early 10th century, the Ottonian dynasty had established itself in Germany, and was engaged in driving back the Magyars. Its efforts culminated in the coronation in 962 of Otto I (r. 936–973) as Holy Roman Emperor.[123] In 972, he secured recognition of his title by the Byzantine Empire, which he sealed with the marriage of his son Otto II (r. 967–983) to Theophanu (d. 991), daughter of an earlier Byzantine Emperor Romanos II (r. 959–963).[124] By the late 10th century Italy had been drawn into the Ottonian sphere after a period of instability;[125] Otto III (r. 996–1002) spent much of his later reign in the kingdom.[126] The western Frankish kingdom was more fragmented, and although kings remained nominally in charge, much of the political power devolved to the local lords.[127]		Missionary efforts to Scandinavia during the 9th and 10th centuries helped strengthen the growth of kingdoms such as Sweden, Denmark, and Norway, which gained power and territory. Some kings converted to Christianity, although not all by 1000. Scandinavians also expanded and colonised throughout Europe. Besides the settlements in Ireland, England, and Normandy, further settlement took place in what became Russia and in Iceland. Swedish traders and raiders ranged down the rivers of the Russian steppe, and even attempted to seize Constantinople in 860 and 907.[128] Christian Spain, initially driven into a small section of the peninsula in the north, expanded slowly south during the 9th and 10th centuries, establishing the kingdoms of Asturias and León.[129]		In Eastern Europe, Byzantium revived its fortunes under Emperor Basil I (r. 867–886) and his successors Leo VI (r. 886–912) and Constantine VII (r. 913–959), members of the Macedonian dynasty. Commerce revived and the emperors oversaw the extension of a uniform administration to all the provinces. The military was reorganised, which allowed the emperors John I (r. 969–976) and Basil II (r. 976–1025) to expand the frontiers of the empire on all fronts. The imperial court was the centre of a revival of classical learning, a process known as the Macedonian Renaissance. Writers such as John Geometres (fl. early 10th century) composed new hymns, poems, and other works.[130] Missionary efforts by both Eastern and Western clergy resulted in the conversion of the Moravians, Bulgars, Bohemians, Poles, Magyars, and Slavic inhabitants of the Kievan Rus'. These conversions contributed to the founding of political states in the lands of those peoples—the states of Moravia, Bulgaria, Bohemia, Poland, Hungary, and the Kievan Rus'.[131] Bulgaria, which was founded around 680, at its height reached from Budapest to the Black Sea and from the Dnieper River in modern Ukraine to the Adriatic Sea.[132] By 1018, the last Bulgarian nobles had surrendered to the Byzantine Empire.[133]		Few large stone buildings were constructed between the Constantinian basilicas of the 4th century and the 8th century, although many smaller ones were built during the 6th and 7th centuries. By the beginning of the 8th century, the Carolingian Empire revived the basilica form of architecture.[135] One feature of the basilica is the use of a transept,[136] or the "arms" of a cross-shaped building that are perpendicular to the long nave.[137] Other new features of religious architecture include the crossing tower and a monumental entrance to the church, usually at the west end of the building.[138]		Carolingian art was produced for a small group of figures around the court, and the monasteries and churches they supported. It was dominated by efforts to regain the dignity and classicism of imperial Roman and Byzantine art, but was also influenced by the Insular art of the British Isles. Insular art integrated the energy of Irish Celtic and Anglo-Saxon Germanic styles of ornament with Mediterranean forms such as the book, and established many characteristics of art for the rest of the medieval period. Surviving religious works from the Early Middle Ages are mostly illuminated manuscripts and carved ivories, originally made for metalwork that has since been melted down.[139][140] Objects in precious metals were the most prestigious form of art, but almost all are lost except for a few crosses such as the Cross of Lothair, several reliquaries, and finds such as the Anglo-Saxon burial at Sutton Hoo and the hoards of Gourdon from Merovingian France, Guarrazar from Visigothic Spain and Nagyszentmiklós near Byzantine territory. There are survivals from the large brooches in fibula or penannular form that were a key piece of personal adornment for elites, including the Irish Tara Brooch.[141] Highly decorated books were mostly Gospel Books and these have survived in larger numbers, including the Insular Book of Kells, the Book of Lindisfarne, and the imperial Codex Aureus of St. Emmeram, which is one of the few to retain its "treasure binding" of gold encrusted with jewels.[142] Charlemagne's court seems to have been responsible for the acceptance of figurative monumental sculpture in Christian art,[143] and by the end of the period near life-sized figures such as the Gero Cross were common in important churches.[144]		During the later Roman Empire, the principal military developments were attempts to create an effective cavalry force as well as the continued development of highly specialised types of troops. The creation of heavily armoured cataphract-type soldiers as cavalry was an important feature of the 5th-century Roman military. The various invading tribes had differing emphasis on types of soldiers—ranging from the primarily infantry Anglo-Saxon invaders of Britain to the Vandals and Visigoths, who had a high proportion of cavalry in their armies.[145] During the early invasion period, the stirrup had not been introduced into warfare, which limited the usefulness of cavalry as shock troops because it was not possible to put the full force of the horse and rider behind blows struck by the rider.[146] The greatest change in military affairs during the invasion period was the adoption of the Hunnic composite bow in place of the earlier, and weaker, Scythian composite bow.[147] Another development was the increasing use of longswords[148] and the progressive replacement of scale armour by mail armour and lamellar armour.[149]		The importance of infantry and light cavalry began to decline during the early Carolingian period, with a growing dominance of elite heavy cavalry. The use of militia-type levies of the free population declined over the Carolingian period.[150] Although much of the Carolingian armies were mounted, a large proportion during the early period appear to have been mounted infantry, rather than true cavalry.[151] One exception was Anglo-Saxon England, where the armies were still composed of regional levies, known as the fyrd, which were led by the local elites.[152] In military technology, one of the main changes was the return of the crossbow, which had been known in Roman times and reappeared as a military weapon during the last part of the Early Middle Ages.[153] Another change was the introduction of the stirrup, which increased the effectiveness of cavalry as shock troops. A technological advance that had implications beyond the military was the horseshoe, which allowed horses to be used in rocky terrain.[154]		The High Middle Ages was a period of tremendous expansion of population. The estimated population of Europe grew from 35 to 80 million between 1000 and 1347, although the exact causes remain unclear: improved agricultural techniques, the decline of slaveholding, a more clement climate and the lack of invasion have all been suggested.[157][158] As much as 90 per cent of the European population remained rural peasants. Many were no longer settled in isolated farms but had gathered into small communities, usually known as manors or villages.[158] These peasants were often subject to noble overlords and owed them rents and other services, in a system known as manorialism. There remained a few free peasants throughout this period and beyond,[159] with more of them in the regions of Southern Europe than in the north. The practice of assarting, or bringing new lands into production by offering incentives to the peasants who settled them, also contributed to the expansion of population.[160]		Other sections of society included the nobility, clergy, and townsmen. Nobles, both the titled nobility and simple knights, exploited the manors and the peasants, although they did not own lands outright but were granted rights to the income from a manor or other lands by an overlord through the system of feudalism. During the 11th and 12th centuries, these lands, or fiefs, came to be considered hereditary, and in most areas they were no longer divisible between all the heirs as had been the case in the early medieval period. Instead, most fiefs and lands went to the eldest son.[161][Q] The dominance of the nobility was built upon its control of the land, its military service as heavy cavalry, control of castles, and various immunities from taxes or other impositions.[R] Castles, initially in wood but later in stone, began to be constructed in the 9th and 10th centuries in response to the disorder of the time, and provided protection from invaders as well as allowing lords defence from rivals. Control of castles allowed the nobles to defy kings or other overlords.[163] Nobles were stratified; kings and the highest-ranking nobility controlled large numbers of commoners and large tracts of land, as well as other nobles. Beneath them, lesser nobles had authority over smaller areas of land and fewer people. Knights were the lowest level of nobility; they controlled but did not own land, and had to serve other nobles.[164][S]		The clergy was divided into two types: the secular clergy, who lived out in the world, and the regular clergy, who lived under a religious rule and were usually monks.[166] Throughout the period monks remained a very small proportion of the population, usually less than one per cent.[167] Most of the regular clergy were drawn from the nobility, the same social class that served as the recruiting ground for the upper levels of the secular clergy. The local parish priests were often drawn from the peasant class.[168] Townsmen were in a somewhat unusual position, as they did not fit into the traditional three-fold division of society into nobles, clergy, and peasants. During the 12th and 13th centuries, the ranks of the townsmen expanded greatly as existing towns grew and new population centres were founded.[169] But throughout the Middle Ages the population of the towns probably never exceeded 10 per cent of the total population.[170]		Jews also spread across Europe during the period. Communities were established in Germany and England in the 11th and 12th centuries, but Spanish Jews, long settled in Spain under the Muslims, came under Christian rule and increasing pressure to convert to Christianity.[77] Most Jews were confined to the cities, as they were not allowed to own land or be peasants.[171][T] Besides the Jews, there were other non-Christians on the edges of Europe—pagan Slavs in Eastern Europe and Muslims in Southern Europe.[172]		Women in the Middle Ages were officially required to be subordinate to some male, whether their father, husband, or other kinsman. Widows, who were often allowed much control over their own lives, were still restricted legally. Women's work generally consisted of household or other domestically inclined tasks. Peasant women were usually responsible for taking care of the household, child-care, as well as gardening and animal husbandry near the house. They could supplement the household income by spinning or brewing at home. At harvest-time, they were also expected to help with field-work.[173] Townswomen, like peasant women, were responsible for the household, and could also engage in trade. What trades were open to women varied by country and period.[174] Noblewomen were responsible for running a household, and could occasionally be expected to handle estates in the absence of male relatives, but they were usually restricted from participation in military or government affairs. The only role open to women in the Church was that of nuns, as they were unable to become priests.[173]		In central and northern Italy and in Flanders, the rise of towns that were to a degree self-governing stimulated economic growth and created an environment for new types of trade associations. Commercial cities on the shores of the Baltic entered into agreements known as the Hanseatic League, and the Italian Maritime republics such as Venice, Genoa, and Pisa expanded their trade throughout the Mediterranean.[U] Great trading fairs were established and flourished in northern France during the period, allowing Italian and German merchants to trade with each other as well as local merchants.[176] In the late 13th century new land and sea routes to the Far East were pioneered, famously described in The Travels of Marco Polo written by one of the traders, Marco Polo (d. 1324).[177] Besides new trading opportunities, agricultural and technological improvements enabled an increase in crop yields, which in turn allowed the trade networks to expand.[178] Rising trade brought new methods of dealing with money, and gold coinage was again minted in Europe, first in Italy and later in France and other countries. New forms of commercial contracts emerged, allowing risk to be shared among merchants. Accounting methods improved, partly through the use of double-entry bookkeeping; letters of credit also appeared, allowing easy transmission of money.[179]		The High Middle Ages was the formative period in the history of the modern Western state. Kings in France, England, and Spain consolidated their power, and set up lasting governing institutions.[180] New kingdoms such as Hungary and Poland, after their conversion to Christianity, became Central European powers.[181] The Magyars settled Hungary around 900 under King Árpád (d. c. 907) after a series of invasions in the 9th century.[182] The papacy, long attached to an ideology of independence from secular kings, first asserted its claim to temporal authority over the entire Christian world; the Papal Monarchy reached its apogee in the early 13th century under the pontificate of Innocent III (pope 1198–1216).[183] Northern Crusades and the advance of Christian kingdoms and military orders into previously pagan regions in the Baltic and Finnic north-east brought the forced assimilation of numerous native peoples into European culture.[184]		During the early High Middle Ages, Germany was ruled by the Ottonian dynasty, which struggled to control the powerful dukes ruling over territorial duchies tracing back to the Migration period. In 1024, they were replaced by the Salian dynasty, who famously clashed with the papacy under Emperor Henry IV (r. 1084–1105) over Church appointments as part of the Investiture Controversy.[185] His successors continued to struggle against the papacy as well as the German nobility. A period of instability followed the death of Emperor Henry V (r. 1111–25), who died without heirs, until Frederick I Barbarossa (r. 1155–90) took the imperial throne.[186] Although he ruled effectively, the basic problems remained, and his successors continued to struggle into the 13th century.[187] Barbarossa's grandson Frederick II (r. 1220–1250), who was also heir to the throne of Sicily through his mother, clashed repeatedly with the papacy. His court was famous for its scholars and he was often accused of heresy.[188] He and his successors faced many difficulties, including the invasion of the Mongols into Europe in the mid-13th century. Mongols first shattered the Kievan Rus' principalities and then invaded Eastern Europe in 1241, 1259, and 1287.[189]		Under the Capetian dynasty the French monarchy slowly began to expand its authority over the nobility, growing out of the Île-de-France to exert control over more of the country in the 11th and 12th centuries.[190] They faced a powerful rival in the Dukes of Normandy, who in 1066 under William the Conqueror (duke 1035–1087), conquered England (r. 1066–87) and created a cross-channel empire that lasted, in various forms, throughout the rest of the Middle Ages.[191][192] Normans also settled in Sicily and southern Italy, when Robert Guiscard (d. 1085) landed there in 1059 and established a duchy that later became the Kingdom of Sicily.[193] Under the Angevin dynasty of Henry II (r. 1154–89) and his son Richard I (r. 1189–99), the kings of England ruled over England and large areas of France,[194][V] brought to the family by Henry II's marriage to Eleanor of Aquitaine (d. 1204), heiress to much of southern France.[196][W] Richard's younger brother John (r. 1199–1216) lost Normandy and the rest of the northern French possessions in 1204 to the French King Philip II Augustus (r. 1180–1223). This led to dissension among the English nobility, while John's financial exactions to pay for his unsuccessful attempts to regain Normandy led in 1215 to Magna Carta, a charter that confirmed the rights and privileges of free men in England. Under Henry III (r. 1216–72), John's son, further concessions were made to the nobility, and royal power was diminished.[197] The French monarchy continued to make gains against the nobility during the late 12th and 13th centuries, bringing more territories within the kingdom under the king's personal rule and centralising the royal administration.[198] Under Louis IX (r. 1226–70), royal prestige rose to new heights as Louis served as a mediator for most of Europe.[199][X]		In Iberia, the Christian states, which had been confined to the north-western part of the peninsula, began to push back against the Islamic states in the south, a period known as the Reconquista.[201] By about 1150, the Christian north had coalesced into the five major kingdoms of León, Castile, Aragon, Navarre, and Portugal.[202] Southern Iberia remained under control of Islamic states, initially under the Caliphate of Córdoba, which broke up in 1031 into a shifting number of petty states known as taifas,[201] who fought with the Christians until the Almohad Caliphate re-established centralised rule over Southern Iberia in the 1170s.[203] Christian forces advanced again in the early 13th century, culminating in the capture of Seville in 1248.[204]		In the 11th century, the Seljuk Turks took over much of the Middle East, occupying Persia during the 1040s, Armenia in the 1060s, and Jerusalem in 1070. In 1071, the Turkish army defeated the Byzantine army at the Battle of Manzikert and captured the Byzantine Emperor Romanus IV (r. 1068–71). The Turks were then free to invade Asia Minor, which dealt a dangerous blow to the Byzantine Empire by seizing a large part of its population and its economic heartland. Although the Byzantines regrouped and recovered somewhat, they never fully regained Asia Minor and were often on the defensive. The Turks also had difficulties, losing control of Jerusalem to the Fatimids of Egypt and suffering from a series of internal civil wars.[206] The Byzantines also faced a revived Bulgaria, which in the late 12th and 13th centuries spread throughout the Balkans.[207]		The crusades were intended to seize Jerusalem from Muslim control. The First Crusade was proclaimed by Pope Urban II (pope 1088–99) at the Council of Clermont in 1095 in response to a request from the Byzantine Emperor Alexios I Komnenos (r. 1081–1118) for aid against further Muslim advances. Urban promised indulgence to anyone who took part. Tens of thousands of people from all levels of society mobilised across Europe and captured Jerusalem in 1099.[208] One feature of the crusades was the pogroms against local Jews that often took place as the crusaders left their countries for the East. These were especially brutal during the First Crusade,[77] when the Jewish communities in Cologne, Mainz, and Worms were destroyed, and other communities in cities between the rivers Seine and Rhine suffered destruction.[209] Another outgrowth of the crusades was the foundation of a new type of monastic order, the military orders of the Templars and Hospitallers, which fused monastic life with military service.[210]		The crusaders consolidated their conquests into crusader states. During the 12th and 13th centuries, there were a series of conflicts between those states and the surrounding Islamic states. Appeals from those states to the papacy led to further crusades,[208] such as the Third Crusade, called to try to regain Jerusalem, which had been captured by Saladin (d. 1193) in 1187.[211][Y] In 1203, the Fourth Crusade was diverted from the Holy Land to Constantinople, and captured the city in 1204, setting up a Latin Empire of Constantinople[213] and greatly weakening the Byzantine Empire. The Byzantines recaptured the city in 1261, but never regained their former strength.[214] By 1291 all the crusader states had been captured or forced from the mainland, although a titular Kingdom of Jerusalem survived on the island of Cyprus for several years afterwards.[215]		Popes called for crusades to take place elsewhere besides the Holy Land: in Spain, southern France, and along the Baltic.[208] The Spanish crusades became fused with the Reconquista of Spain from the Muslims. Although the Templars and Hospitallers took part in the Spanish crusades, similar Spanish military religious orders were founded, most of which had become part of the two main orders of Calatrava and Santiago by the beginning of the 12th century.[216] Northern Europe also remained outside Christian influence until the 11th century or later, and became a crusading venue as part of the Northern Crusades of the 12th to 14th centuries. These crusades also spawned a military order, the Order of the Sword Brothers. Another order, the Teutonic Knights, although founded in the crusader states, focused much of its activity in the Baltic after 1225, and in 1309 moved its headquarters to Marienburg in Prussia.[217]		During the 11th century, developments in philosophy and theology led to increased intellectual activity. There was debate between the realists and the nominalists over the concept of "universals". Philosophical discourse was stimulated by the rediscovery of Aristotle and his emphasis on empiricism and rationalism. Scholars such as Peter Abelard (d. 1142) and Peter Lombard (d. 1164) introduced Aristotelian logic into theology. In the late 11th and early 12th centuries cathedral schools spread throughout Western Europe, signalling the shift of learning from monasteries to cathedrals and towns.[218] Cathedral schools were in turn replaced by the universities established in major European cities.[219] Philosophy and theology fused in scholasticism, an attempt by 12th- and 13th-century scholars to reconcile authoritative texts, most notably Aristotle and the Bible. This movement tried to employ a systemic approach to truth and reason[220] and culminated in the thought of Thomas Aquinas (d. 1274), who wrote the Summa Theologica, or Summary of Theology.[221]		Chivalry and the ethos of courtly love developed in royal and noble courts. This culture was expressed in the vernacular languages rather than Latin, and comprised poems, stories, legends, and popular songs spread by troubadours, or wandering minstrels. Often the stories were written down in the chansons de geste, or "songs of great deeds", such as The Song of Roland or The Song of Hildebrand.[222] Secular and religious histories were also produced.[223] Geoffrey of Monmouth (d. c. 1155) composed his Historia Regum Britanniae, a collection of stories and legends about Arthur.[224] Other works were more clearly history, such as Otto von Freising's (d. 1158) Gesta Friderici Imperatoris detailing the deeds of Emperor Frederick Barbarossa, or William of Malmesbury's (d. c. 1143) Gesta Regum on the kings of England.[223]		Legal studies advanced during the 12th century. Both secular law and canon law, or ecclesiastical law, were studied in the High Middle Ages. Secular law, or Roman law, was advanced greatly by the discovery of the Corpus Juris Civilis in the 11th century, and by 1100 Roman law was being taught at Bologna. This led to the recording and standardisation of legal codes throughout Western Europe. Canon law was also studied, and around 1140 a monk named Gratian (fl. 12th century), a teacher at Bologna, wrote what became the standard text of canon law—the Decretum.[225]		Among the results of the Greek and Islamic influence on this period in European history was the replacement of Roman numerals with the decimal positional number system and the invention of algebra, which allowed more advanced mathematics. Astronomy advanced following the translation of Ptolemy's Almagest from Greek into Latin in the late 12th century. Medicine was also studied, especially in southern Italy, where Islamic medicine influenced the school at Salerno.[226]		In the 12th and 13th centuries, Europe produced economic growth and innovations in methods of production. Major technological advances included the invention of the windmill, the first mechanical clocks, the manufacture of distilled spirits, and the use of the astrolabe.[228] Concave spectacles were invented around 1286 by an unknown Italian artisan, probably working in or near Pisa.[229]		The development of a three-field rotation system for planting crops[158][Z] increased the usage of land from one half in use each year under the old two-field system to two-thirds under the new system, with a consequent increase in production.[230] The development of the heavy plough allowed heavier soils to be farmed more efficiently, aided by the spread of the horse collar, which led to the use of draught horses in place of oxen. Horses are faster than oxen and require less pasture, factors that aided the implementation of the three-field system.[231]		The construction of cathedrals and castles advanced building technology, leading to the development of large stone buildings. Ancillary structures included new town halls, houses, bridges, and tithe barns.[232] Shipbuilding improved with the use of the rib and plank method rather than the old Roman system of mortise and tenon. Other improvements to ships included the use of lateen sails and the stern-post rudder, both of which increased the speed at which ships could be sailed.[233]		In military affairs, the use of infantry with specialised roles increased. Along with the still-dominant heavy cavalry, armies often included mounted and infantry crossbowmen, as well as sappers and engineers.[234] Crossbows, which had been known in Late Antiquity, increased in use partly because of the increase in siege warfare in the 10th and 11th centuries.[153][AA] The increasing use of crossbows during the 12th and 13th centuries led to the use of closed-face helmets, heavy body armour, as well as horse armour.[236] Gunpowder was known in Europe by the mid-13th century with a recorded use in European warfare by the English against the Scots in 1304, although it was merely used as an explosive and not as a weapon. Cannon were being used for sieges in the 1320s, and hand-held guns were in use by the 1360s.[237]		In the 10th century the establishment of churches and monasteries led to the development of stone architecture that elaborated vernacular Roman forms, from which the term "Romanesque" is derived. Where available, Roman brick and stone buildings were recycled for their materials. From the tentative beginnings known as the First Romanesque, the style flourished and spread across Europe in a remarkably homogeneous form. Just before 1000 there was a great wave of building stone churches all over Europe.[238] Romanesque buildings have massive stone walls, openings topped by semi-circular arches, small windows, and, particularly in France, arched stone vaults.[239] The large portal with coloured sculpture in high relief became a central feature of façades, especially in France, and the capitals of columns were often carved with narrative scenes of imaginative monsters and animals.[240] According to art historian C. R. Dodwell, "virtually all the churches in the West were decorated with wall-paintings", of which few survive.[241] Simultaneous with the development in church architecture, the distinctive European form of the castle was developed, and became crucial to politics and warfare.[242]		Romanesque art, especially metalwork, was at its most sophisticated in Mosan art, in which distinct artistic personalities including Nicholas of Verdun (d. 1205) become apparent, and an almost classical style is seen in works such as a font at Liège,[243] contrasting with the writhing animals of the exactly contemporary Gloucester Candlestick. Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.[244]		From the early 12th century, French builders developed the Gothic style, marked by the use of rib vaults, pointed arches, flying buttresses, and large stained glass windows. It was used mainly in churches and cathedrals, and continued in use until the 16th century in much of Europe. Classic examples of Gothic architecture include Chartres Cathedral and Reims Cathedral in France as well as Salisbury Cathedral in England.[245] Stained glass became a crucial element in the design of churches, which continued to use extensive wall-paintings, now almost all lost.[246]		During this period the practice of manuscript illumination gradually passed from monasteries to lay workshops, so that according to Janetta Benton "by 1300 most monks bought their books in shops",[247] and the book of hours developed as a form of devotional book for lay-people. Metalwork continued to be the most prestigious form of art, with Limoges enamel a popular and relatively affordable option for objects such as reliquaries and crosses.[248] In Italy the innovations of Cimabue and Duccio, followed by the Trecento master Giotto (d. 1337), greatly increased the sophistication and status of panel painting and fresco.[249] Increasing prosperity during the 12th century resulted in greater production of secular art; many carved ivory objects such as gaming-pieces, combs, and small religious figures have survived.[250]		Monastic reform became an important issue during the 11th century, as elites began to worry that monks were not adhering to the rules binding them to a strictly religious life. Cluny Abbey, founded in the Mâcon region of France in 909, was established as part of the Cluniac Reforms, a larger movement of monastic reform in response to this fear.[252] Cluny quickly established a reputation for austerity and rigour. It sought to maintain a high quality of spiritual life by placing itself under the protection of the papacy and by electing its own abbot without interference from laymen, thus maintaining economic and political independence from local lords.[253]		Monastic reform inspired change in the secular Church. The ideals that it was based upon were brought to the papacy by Pope Leo IX (pope 1049–1054), and provided the ideology of the clerical independence that led to the Investiture Controversy in the late 11th century. This involved Pope Gregory VII (pope 1073–85) and Emperor Henry IV, who initially clashed over episcopal appointments, a dispute that turned into a battle over the ideas of investiture, clerical marriage, and simony. The emperor saw the protection of the Church as one of his responsibilities as well as wanting to preserve the right to appoint his own choices as bishops within his lands, but the papacy insisted on the Church's independence from secular lords. These issues remained unresolved after the compromise of 1122 known as the Concordat of Worms. The dispute represents a significant stage in the creation of a papal monarchy separate from and equal to lay authorities. It also had the permanent consequence of empowering German princes at the expense of the German emperors.[252]		The High Middle Ages was a period of great religious movements. Besides the Crusades and monastic reforms, people sought to participate in new forms of religious life. New monastic orders were founded, including the Carthusians and the Cistercians. The latter especially expanded rapidly in their early years under the guidance of Bernard of Clairvaux (d. 1153). These new orders were formed in response to the feeling of the laity that Benedictine monasticism no longer met the needs of the laymen, who along with those wishing to enter the religious life wanted a return to the simpler hermetical monasticism of early Christianity, or to live an Apostolic life.[210] Religious pilgrimages were also encouraged. Old pilgrimage sites such as Rome, Jerusalem, and Compostela received increasing numbers of visitors, and new sites such as Monte Gargano and Bari rose to prominence.[254]		In the 13th century mendicant orders—the Franciscans and the Dominicans—who swore vows of poverty and earned their living by begging, were approved by the papacy.[255] Religious groups such as the Waldensians and the Humiliati also attempted to return to the life of early Christianity in the middle 12th and early 13th centuries, but they were condemned as heretical by the papacy. Others joined the Cathars, another heretical movement condemned by the papacy. In 1209, a crusade was preached against the Cathars, the Albigensian Crusade, which in combination with the medieval Inquisition, eliminated them.[256]		The first years of the 14th century were marked by famines, culminating in the Great Famine of 1315–17.[257] The causes of the Great Famine included the slow transition from the Medieval Warm Period to the Little Ice Age, which left the population vulnerable when bad weather caused crop failures.[258] The years 1313–14 and 1317–21 were excessively rainy throughout Europe, resulting in widespread crop failures.[259] The climate change—which resulted in a declining average annual temperature for Europe during the 14th century—was accompanied by an economic downturn.[260]		These troubles were followed in 1347 by the Black Death, a pandemic that spread throughout Europe during the following three years.[261][AB] The death toll was probably about 35 million people in Europe, about one-third of the population. Towns were especially hard-hit because of their crowded conditions.[AC] Large areas of land were left sparsely inhabited, and in some places fields were left unworked. Wages rose as landlords sought to entice the reduced number of available workers to their fields. Further problems were lower rents and lower demand for food, both of which cut into agricultural income. Urban workers also felt that they had a right to greater earnings, and popular uprisings broke out across Europe.[264] Among the uprisings were the jacquerie in France, the Peasants' Revolt in England, and revolts in the cities of Florence in Italy and Ghent and Bruges in Flanders. The trauma of the plague led to an increased piety throughout Europe, manifested by the foundation of new charities, the self-mortification of the flagellants, and the scapegoating of Jews.[265] Conditions were further unsettled by the return of the plague throughout the rest of the 14th century; it continued to strike Europe periodically during the rest of the Middle Ages.[261]		Society throughout Europe was disturbed by the dislocations caused by the Black Death. Lands that had been marginally productive were abandoned, as the survivors were able to acquire more fertile areas.[266] Although serfdom declined in Western Europe it became more common in Eastern Europe, as landlords imposed it on those of their tenants who had previously been free.[267] Most peasants in Western Europe managed to change the work they had previously owed to their landlords into cash rents.[268] The percentage of serfs amongst the peasantry declined from a high of 90 to closer to 50 per cent by the end of the period.[165] Landlords also became more conscious of common interests with other landholders, and they joined together to extort privileges from their governments. Partly at the urging of landlords, governments attempted to legislate a return to the economic conditions that existed before the Black Death.[268] Non-clergy became increasingly literate, and urban populations began to imitate the nobility's interest in chivalry.[269]		Jewish communities were expelled from England in 1290 and from France in 1306. Although some were allowed back into France, most were not, and many Jews emigrated eastwards, settling in Poland and Hungary.[270] The Jews were expelled from Spain in 1492, and dispersed to Turkey, France, Italy, and Holland.[77] The rise of banking in Italy during the 13th century continued throughout the 14th century, fuelled partly by the increasing warfare of the period and the needs of the papacy to move money between kingdoms. Many banking firms loaned money to royalty, at great risk, as some were bankrupted when kings defaulted on their loans.[271][AD]		Strong, royalty-based nation states rose throughout Europe in the Late Middle Ages, particularly in England, France, and the Christian kingdoms of the Iberian Peninsula: Aragon, Castile, and Portugal. The long conflicts of the period strengthened royal control over their kingdoms and were extremely hard on the peasantry. Kings profited from warfare that extended royal legislation and increased the lands they directly controlled.[272] Paying for the wars required that methods of taxation become more effective and efficient, and the rate of taxation often increased.[273] The requirement to obtain the consent of taxpayers allowed representative bodies such as the English Parliament and the French Estates General to gain power and authority.[274]		Throughout the 14th century, French kings sought to expand their influence at the expense of the territorial holdings of the nobility.[275] They ran into difficulties when attempting to confiscate the holdings of the English kings in southern France, leading to the Hundred Years' War,[276] waged from 1337 to 1453.[277] Early in the war the English under Edward III (r. 1327–77) and his son Edward, the Black Prince (d. 1376),[AE] won the battles of Crécy and Poitiers, captured the city of Calais, and won control of much of France.[AF] The resulting stresses almost caused the disintegration of the French kingdom during the early years of the war.[280] In the early 15th century, France again came close to dissolving, but in the late 1420s the military successes of Joan of Arc (d. 1431) led to the victory of the French and the capture of the last English possessions in southern France in 1453.[281] The price was high, as the population of France at the end of the Wars was likely half what it had been at the start of the conflict. Conversely, the Wars had a positive effect on English national identity, doing much to fuse the various local identities into a national English ideal. The conflict with France also helped create a national culture in England separate from French culture, which had previously been the dominant influence.[282] The dominance of the English longbow began during early stages of the Hundred Years' War,[283] and cannon appeared on the battlefield at Crécy in 1346.[237]		In modern-day Germany, the Holy Roman Empire continued to rule, but the elective nature of the imperial crown meant there was no enduring dynasty around which a strong state could form.[284] Further east, the kingdoms of Poland, Hungary, and Bohemia grew powerful.[285] In Iberia, the Christian kingdoms continued to gain land from the Muslim kingdoms of the peninsula;[286] Portugal concentrated on expanding overseas during the 15th century, while the other kingdoms were riven by difficulties over royal succession and other concerns.[287][288] After losing the Hundred Years' War, England went on to suffer a long civil war known as the Wars of the Roses, which lasted into the 1490s[288] and only ended when Henry Tudor (r. 1485–1509 as Henry VII) became king and consolidated power with his victory over Richard III (r. 1483–85) at Bosworth in 1485.[289] In Scandinavia, Margaret I of Denmark (r. in Denmark 1387–1412) consolidated Norway, Denmark, and Sweden in the Union of Kalmar, which continued until 1523. The major power around the Baltic Sea was the Hanseatic League, a commercial confederation of city states that traded from Western Europe to Russia.[290] Scotland emerged from English domination under Robert the Bruce (r. 1306–29), who secured papal recognition of his kingship in 1328.[291]		Although the Palaeologi emperors recaptured Constantinople from the Western Europeans in 1261, they were never able to regain control of much of the former imperial lands. They usually controlled only a small section of the Balkan Peninsula near Constantinople, the city itself, and some coastal lands on the Black Sea and around the Aegean Sea. The former Byzantine lands in the Balkans were divided between the new Kingdom of Serbia, the Second Bulgarian Empire and the city-state of Venice. The power of the Byzantine emperors was threatened by a new Turkish tribe, the Ottomans, who established themselves in Anatolia in the 13th century and steadily expanded throughout the 14th century. The Ottomans expanded into Europe, reducing Bulgaria to a vassal state by 1366 and taking over Serbia after its defeat at the Battle of Kosovo in 1389. Western Europeans rallied to the plight of the Christians in the Balkans and declared a new crusade in 1396; a great army was sent to the Balkans, where it was defeated at the Battle of Nicopolis.[292] Constantinople was finally captured by the Ottomans in 1453.[293]		During the tumultuous 14th century, disputes within the leadership of the Church led to the Avignon Papacy of 1305–78,[294] also called the "Babylonian Captivity of the Papacy" (a reference to the Babylonian captivity of the Jews),[295] and then to the Great Schism, lasting from 1378 to 1418, when there were two and later three rival popes, each supported by several states.[296] Ecclesiastical officials convened at the Council of Constance in 1414, and in the following year the council deposed one of the rival popes, leaving only two claimants. Further depositions followed, and in November 1417 the council elected Martin V (pope 1417–31) as pope.[297]		Besides the schism, the Western Church was riven by theological controversies, some of which turned into heresies. John Wycliffe (d. 1384), an English theologian, was condemned as a heretic in 1415 for teaching that the laity should have access to the text of the Bible as well as for holding views on the Eucharist that were contrary to Church doctrine.[298] Wycliffe's teachings influenced two of the major heretical movements of the later Middle Ages: Lollardy in England and Hussitism in Bohemia.[299] The Bohemian movement initiated with the teaching of Jan Hus, who was burned at the stake in 1415 after being condemned as a heretic by the Council of Constance. The Hussite Church, although the target of a crusade, survived beyond the Middle Ages.[300] Other heresies were manufactured, such as the accusations against the Knights Templar that resulted in their suppression in 1312 and the division of their great wealth between the French King Philip IV (r. 1285–1314) and the Hospitallers.[301]		The papacy further refined the practice in the Mass in the Late Middle Ages, holding that the clergy alone was allowed to partake of the wine in the Eucharist. This further distanced the secular laity from the clergy. The laity continued the practices of pilgrimages, veneration of relics, and belief in the power of the Devil. Mystics such as Meister Eckhart (d. 1327) and Thomas à Kempis (d. 1471) wrote works that taught the laity to focus on their inner spiritual life, which laid the groundwork for the Protestant Reformation. Besides mysticism, belief in witches and witchcraft became widespread, and by the late 15th century the Church had begun to lend credence to populist fears of witchcraft with its condemnation of witches in 1484 and the publication in 1486 of the Malleus Maleficarum, the most popular handbook for witch-hunters.[302]		During the Later Middle Ages, theologians such as John Duns Scotus (d. 1308)[AG] and William of Ockham (d. c. 1348),[220] led a reaction against scholasticism, objecting to the application of reason to faith. Their efforts undermined the prevailing Platonic idea of "universals". Ockham's insistence that reason operates independently of faith allowed science to be separated from theology and philosophy.[303] Legal studies were marked by the steady advance of Roman law into areas of jurisprudence previously governed by customary law. The lone exception to this trend was in England, where the common law remained pre-eminent. Other countries codified their laws; legal codes were promulgated in Castile, Poland, and Lithuania.[304]		Education remained mostly focused on the training of future clergy. The basic learning of the letters and numbers remained the province of the family or a village priest, but the secondary subjects of the trivium—grammar, rhetoric, logic—were studied in cathedral schools or in schools provided by cities. Commercial secondary schools spread, and some Italian towns had more than one such enterprise. Universities also spread throughout Europe in the 14th and 15th centuries. Lay literacy rates rose, but were still low; one estimate gave a literacy rate of ten per cent of males and one per cent of females in 1500.[305]		The publication of vernacular literature increased, with Dante (d. 1321), Petrarch (d. 1374) and Giovanni Boccaccio (d. 1375) in 14th-century Italy, Geoffrey Chaucer (d. 1400) and William Langland (d. c. 1386) in England, and François Villon (d. 1464) and Christine de Pizan (d. c. 1430) in France. Much literature remained religious in character, and although a great deal of it continued to be written in Latin, a new demand developed for saints' lives and other devotional tracts in the vernacular languages.[304] This was fed by the growth of the Devotio Moderna movement, most prominently in the formation of the Brethren of the Common Life, but also in the works of German mystics such as Meister Eckhart and Johannes Tauler (d. 1361).[306] Theatre also developed in the guise of miracle plays put on by the Church.[304] At the end of the period, the development of the printing press in about 1450 led to the establishment of publishing houses throughout Europe by 1500.[307]		In the early 15th century, the countries of the Iberian peninsula began to sponsor exploration beyond the boundaries of Europe. Prince Henry the Navigator of Portugal (d. 1460) sent expeditions that discovered the Canary Islands, the Azores, and Cape Verde during his lifetime. After his death, exploration continued; Bartolomeu Dias (d. 1500) went around the Cape of Good Hope in 1486 and Vasco da Gama (d. 1524) sailed around Africa to India in 1498.[308] The combined Spanish monarchies of Castile and Aragon sponsored the voyage of exploration by Christopher Columbus (d. 1506) in 1492 that discovered the Americas.[309] The English crown under Henry VII sponsored the voyage of John Cabot (d. 1498) in 1497, which landed on Cape Breton Island.[310]				One of the major developments in the military sphere during the Late Middle Ages was the increased use of infantry and light cavalry.[311] The English also employed longbowmen, but other countries were unable to create similar forces with the same success.[312] Armour continued to advance, spurred by the increasing power of crossbows, and plate armour was developed to protect soldiers from crossbows as well as the hand-held guns that were developed.[313] Pole arms reached new prominence with the development of the Flemish and Swiss infantry armed with pikes and other long spears.[314]		In agriculture, the increased usage of sheep with long-fibred wool allowed a stronger thread to be spun. In addition, the spinning wheel replaced the traditional distaff for spinning wool, tripling production.[315][AH] A less technological refinement that still greatly affected daily life was the use of buttons as closures for garments, which allowed for better fitting without having to lace clothing on the wearer.[317] Windmills were refined with the creation of the tower mill, allowing the upper part of the windmill to be spun around to face the direction from which the wind was blowing.[318] The blast furnace appeared around 1350 in Sweden, increasing the quantity of iron produced and improving its quality.[319] The first patent law in 1447 in Venice protected the rights of inventors to their inventions.[320]		The Late Middle Ages in Europe as a whole correspond to the Trecento and Early Renaissance cultural periods in Italy. Northern Europe and Spain continued to use Gothic styles, which became increasingly elaborate in the 15th century, until almost the end of the period. International Gothic was a courtly style that reached much of Europe in the decades around 1400, producing masterpieces such as the Très Riches Heures du Duc de Berry.[321] All over Europe secular art continued to increase in quantity and quality, and in the 15th century the mercantile classes of Italy and Flanders became important patrons, commissioning small portraits of themselves in oils as well as a growing range of luxury items such as jewellery, ivory caskets, cassone chests, and maiolica pottery. These objects also included the Hispano-Moresque ware produced by mostly Mudéjar potters in Spain. Although royalty owned huge collections of plate, little survives except for the Royal Gold Cup.[322] Italian silk manufacture developed, so that Western churches and elites no longer needed to rely on imports from Byzantium or the Islamic world. In France and Flanders tapestry weaving of sets like The Lady and the Unicorn became a major luxury industry.[323]		The large external sculptural schemes of Early Gothic churches gave way to more sculpture inside the building, as tombs became more elaborate and other features such as pulpits were sometimes lavishly carved, as in the Pulpit by Giovanni Pisano in Sant'Andrea. Painted or carved wooden relief altarpieces became common, especially as churches created many side-chapels. Early Netherlandish painting by artists such as Jan van Eyck (d. 1441) and Rogier van der Weyden (d. 1464) rivalled that of Italy, as did northern illuminated manuscripts, which in the 15th century began to be collected on a large scale by secular elites, who also commissioned secular books, especially histories. From about 1450 printed books rapidly became popular, though still expensive. There were around 30,000 different editions of incunabula, or works printed before 1500,[324] by which time illuminated manuscripts were commissioned only by royalty and a few others. Very small woodcuts, nearly all religious, were affordable even by peasants in parts of Northern Europe from the middle of the 15th century. More expensive engravings supplied a wealthier market with a variety of images.[325]		The medieval period is frequently caricatured as a "time of ignorance and superstition" that placed "the word of religious authorities over personal experience and rational activity."[326] This is a legacy from both the Renaissance and Enlightenment, when scholars favourably contrasted their intellectual cultures with those of the medieval period. Renaissance scholars saw the Middle Ages as a period of decline from the high culture and civilisation of the Classical world; Enlightenment scholars saw reason as superior to faith, and thus viewed the Middle Ages as a time of ignorance and superstition.[14]		Others argue that reason was generally held in high regard during the Middle Ages. Science historian Edward Grant writes, "If revolutionary rational thoughts were expressed [in the 18th century], they were only made possible because of the long medieval tradition that established the use of reason as one of the most important of human activities".[327] Also, contrary to common belief, David Lindberg writes, "the late medieval scholar rarely experienced the coercive power of the Church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led".[328]		The caricature of the period is also reflected in some more specific notions. One misconception, first propagated in the 19th century[329] and still very common, is that all people in the Middle Ages believed that the Earth was flat.[329] This is untrue, as lecturers in the medieval universities commonly argued that evidence showed the Earth was a sphere.[330] Lindberg and Ronald Numbers, another scholar of the period, state that there "was scarcely a Christian scholar of the Middle Ages who did not acknowledge [Earth's] sphericity and even know its approximate circumference".[331] Other misconceptions such as "the Church prohibited autopsies and dissections during the Middle Ages", "the rise of Christianity killed off ancient science", or "the medieval Christian Church suppressed the growth of natural philosophy", are all cited by Numbers as examples of widely popular myths that still pass as historical truth, although they are not supported by current historical research.[332]				
Student rights are those rights, such as civil, constitutional, contractual and consumer rights, which regulate student rights and freedoms and allow students to make use of their educational investment. These include such things as the right to free speech and association, to due process, equality, autonomy, safety and privacy, and accountability in contracts and advertising, which regulate the treatment of students by teachers and administrators. There is very little scholarship about student rights throughout the world. In general most countries have some kind of student rights (or rights that apply in the educational setting) enshrined in their laws and proceduralized by their court precedents. Some countries, like Romania, in the European Union, have comprehensive student bills of rights, which outline both rights and how they are to be proceduralized. Most countries, however, like the United States and Canada, do not have a cohesive bill of rights and students must use the courts to determine how rights precedents in one area apply in their own jurisdictions.						Canada, like the United States, has a number of laws and court precedents which regulate higher education and provide student rights. The Canadian Encyclopedia, which details issues of Canadian life and governance, states that in Canada "Basically two sorts of rights apply to students: substantive rights – the actual rights that students should enjoy – and procedural rights – methods by which students claim their rights. This article is concerned with students in public institutions, although those in private schools can claim rights under the common law and provincial education Acts."[1]		Canada does not yet have a national student Bill of Rights or comparable document. If and when one is put in place in Canada it is likely that this document will be called a Charter of Student Rights and Freedoms. The Canadian Charter of Rights and Freedoms is equivalent to the National Bill of Rights in the United States. The Canadian national student union or government is the Canadian Federation of Students and it has not put forth any such bill.		In the AlBaho Case, a French criminal court found three senior academics at the École Supérieure de Physique et de Chimie Industrielles de la Ville de Paris (ICPSE) guilty of email espionage. This was the first incident where academic staff were found guilty of a criminal act as a result of a complaint made by a student – and where those staff members had the full support of their institution.		In the US, students have many rights accorded by bills or laws (e.g. the Civil Rights Act and Higher Education Act) and executive presidential orders. These have been proceduralized by the courts to varying degrees. The US does not, however, have a national Student Bill of Rights and students rely on institutions to voluntarily provide this information. While some colleges are posting their own student bills, there is no legal requirement that they do so and no requirement that they post all legal rights.[2]		Decision making should not be arbitrary or capricious / random and, thus, interfere with fairness.[3][4][5][6][7] While this case concerned a private school, Healy v. Larsson (1974) found that what applied to private intuitions applied also to public.[8]		Institutions are required, contractually, to follow their own rules.[3][9][10][11][12] Institutional documents may also be considered binding implied-n-fact contracts. Goodman v. President and Trustees of Bowdoin College (2001) ruled that institutional documents are still contractual regardless if they have a disclaimer.		Students are protected from deviation from information advertised in bulletins or circulars.[13][14]		Students are protected from deviation from information advertised in regulations.[13][14]		Students are protected from deviation from information advertised in course catalogues.[13][14][15]		Students are protected from deviation from information advertised in student codes.[16][17]		Students are protected from deviation from information advertised in handbooks.[16][18]		Healy v. Larsson (1974) found that a student who completed degree requirements prescribed by an academic advisor was entitled to a degree on the basis that this was an implied contract.		Mississippi Medical Center v. Hughes (2000) determined that students have an implied right to a continuous contract during a period of continuous enrollment suggesting that students have the right to graduate so long as they fulfill the requirements as they were originally communicated.[16] Degree requirement changes are unacceptable.[16][19] Bruner v. Petersen (1997) found also that contractual protections do not apply in the event that a student, who has failed to meet requirements, is readmitted into a program.[16] The student may be required to meet additional requirements which support their success. This may also help avoid issues of discrimination.		Brody v. Finch University of Health Sciences Chicago Med. School (1998) determined that students have the right to notice of degree requirement changes.[16]		Verbal contracts are also binding.[20][21] The North Carolina Court of Appeals in Long v. University of North Carolina at Wilmington (1995) found, however, that verbal agreements must be made in an official capacity in order to be binding (Bowden, 2007). Dezick v. Umpqua Community College (1979) found a student was compensated because classes offered orally by the dean were not provided.		Verbal contracts are binding.[20][21][22] They must be made in an official capacity, however, to be binding.[8] Dezick v. Umpqua Community College (1979) found a student was compensated because classes offered orally by the dean were not provided. Healy v. Larsson (1974) found that a student who completed degree requirements prescribed by an academic advisor was entitled to a degree on the basis that this was an implied contract. An advisor should, thus, be considered an official source of information.		Mississippi Medical Center v. Hughes (2000) determined that students have an implied right to a continuous contract during a period of continuous enrollment suggesting that students have the right to graduate so long as they fulfill the requirements as they were originally communicated.[23] Degree requirement changes are unacceptable.[19][24] Bruner v. Petersen (1997) found also that contractual protections do not apply in the event that a student, who has failed to meet requirements, is readmitted into a program.[23] The student may be required to meet additional requirements which support their success. This may also help avoid issues of discrimination.		Brody v. Finch University of Health Sciences Chicago Med. School (1998) determined that students have the right to notice of degree requirement changes (Kaplan & Lee, 2011[23]). If a student, for instance, is absent for a semester and is not continuously enrolled they need to know if degree requirements have changed.		Decision making should not be arbitrary or capricious / random and, thus, interfere with fairness.[3][4][6][24][25] This is a form of discrimination. While this case concerned a private school, Healy v. Larsson (1974) found that what applied to private intuitions applied also to public.[8]		The 2008 Higher Education Opportunity Act (HOEA, 2008)[26] requires that institutions disclose institutional statistics on the Department of Education (DOE) website to allow students to make more informed educational decisions. Information required on the DOE website includes: tuition, fees, net price of attendance, tuition plans, and statistics including sex, ability, ethnic and transfer student ratios as well as ACT/SAT scores, degrees offered, enrolled, and awarded. Institutions are also required to disclose transfer credit policies and articulation agreements.		The 1990 Americans With Disabilities Act (ADA) and Section 504 of the 1973 Rehabilitation Act prohibits ability discrimination in academic recruitment. This includes ability discrimination in recruitment. Individuals designated with a disability by a medical professional, legally recognized with a disability[19][24][27] and deemed otherwise qualified are entitled to equal treatment and reasonable accommodations.[28][29] The Supreme Court defined Otherwise qualified as an individual who can perform the required tasks in spite of rather than except for their disability.[30][31]		Title IX of the 1972 Higher Education Act Amendments[32] protect all sexes from pre-admission inquiries with regard to pregnancy, parental status, family or marital status. It can be seen that this act also protects against such inquiry regarding inter-sexed, transsexual, transgender or androgynous individuals.		The 1990 Americans With Disabilities Act (ADA)[33] and Section 504 of the 1973 Rehabilitation Act.[34] This includes ability discrimination in admissions. Individuals designated with a disability by a medical professional, legally recognized with a disability[19][24][27] and deemed otherwise qualified are entitled to equal treatment and reasonable accommodations in both educational and employment related activities.[28][29] The Supreme Court defined Otherwise qualified as an individual who can perform the required tasks in spite of rather than except for their disability.[30][31]		Individuals may not be discriminated against on the basis of their color in either undergraduate or graduate school admissions.[35][36]		Protection from discrimination in admissions[30][37] entails that students receive accommodations required to prove they are otherwise qualified, protection from unfair testing practices, testing accommodations for speech, manual and hearing disabilities and access to alternative testing offered in accessible facilities. Alternative testing must also be offered as frequently as are standard tests.[38] Where no alternative testing exists, institutions, however, are not responsible for accommodations.[38][39]		Educational tests which are biased in favor of one gender, may not be relied upon as the sole source of information decision making.[24][40]		Students' equality entails that individuals not be treated differently by individuals or systematically by an institution. Thus, testing policies which systematically discriminate, are unlawful according to the constitution. United States v. Fordice (1992) prohibited the use of ACT scores in Mississippi admissions, for instance, because the gap between ACT scores of white and black student was greater than the GPA gap which was not considered at all.[24]		When a school has engaged in racial discrimination in the past they are required by law to take race conscious affirmative action to correct it.[24][41][42][43]		White students are protected from racial discrimination at historic minority institutions.[24][44][45] Racial equality calls for the equal treatment of all individuals; it does not permit, however, lower admissions test requirements[35][46] or subjective judgments for racial minorities when there are objective standards in place for all applicants.[35][47]		There may be no segregation in the admissions process including subjective interviews[23][41][42][48][49] when there are objective standards in place for all applicants.[35][47]		Students are protected from the use of lower admissions test scores.[24][46]		Students are protected from the use of quotas which set aside seats for certain demographics.[35][41][42][49][50][51]		Students are protected from deviation from information advertised in registration materials.[24][52] This may be a binding implied-in-fact contract. Goodman v. President and Trustees of Bowdoin College (2001) ruled that institutional documents are still contractual regardless if they have a disclaimer.		Institutions must be careful with readmissions after students have failed to complete necessary program requirements. Readmission raises questions as to why individuals were removed from the program in the first place and whether future applicants may be admitted under like conditions. Discrimination may be alleged regarding both the initial removal and also in the case that other students are not readmitted under like circumstances. Kaplan & Lee and Lee (2011)[23] recommend that institutions, if they wish to avoid breach of contract and discrimination accusations, have an explicit readmission policy even if that policy denies readmission. If students take a voluntary leave of absence, institutions must have a valid reason to refuse readmission.[24][53]		Students are protected from deviation from information advertised in class syllabi.[54][55][56] This may be a binding implied-n-fact contract. Goodman v. President and Trustees of Bowdoin College (2001) ruled that institutional documents are still contractual regardless if they have a disclaimer.		Students are entitled to receive instruction on advertised course content.[57][58] Institutions have the right to require coverage of designated course material by teachers[59][60][61][62] and faculty and students are generally protected if they adhere to syllabus guidelines.[54][55]		Students may expect teaching in conformity with the course level advertised.[11][15] Andre v. Pace University (1994) awarded damages on the grounds of negligent misrepresentation and breach of contract.[3]		Teachers must give reasonable attention to all stated course subjects.[63]		Students may have all advertised content covered in sufficient depth.[59][64]		Scallet v. Rosenblum (1996) found that "tight control over the curriculum was necessary to ensure uniformity across class sections".[65]		Students may be graded fairly and in accordance with criteria set forth by the course syllabuses and may be protected from the addition of new grading criteria.[54][56] Institutions have the responsibility of preserving quality in grade representations and comparability between classes and prevent grade inflation.[24][56] Teachers have the right, under the first amendment, to communicate their opinions regarding student grades,[59][66] but institutions are required to meet students implied contract rights to fair grading practices. Departments may change grades issued by teachers which are not in line with grading policies or are unfair or unreasonable.[66][67]		Students have the right to learn.[59][68][69][70][71] Teachers do not have free rein in the classroom. They must act within departmental requirements which ensure students' right to learn and must be considered effective.[59][72] Sweezy v. New Hampshire (1957)[70] found that teachers have the right to lecture. They do not have academic freedom under the law.[71] Any academic freedom rules are put in place by the school.		Students may expect protection from the misuse of time;[73] teachers may not waste students' time or use the class as a captive audience for views or lessons not related to the course.[56][73] Riggin v. Bd. of Trustees of Ball St. Univ. found that instructors may not "wast[e] the time of the students who have come there and paid money for a different purpose."		Students can expect effective teaching even if it requires departmental involvement in teaching and curriculum development.[74][75] Kozol (2005)[76] observed that the curriculum development may not be beneficial for all students since some students come from disadvantaged backgrounds where not every student has equitable opportunities to succeed in school. If there is departmental involvement in the students' learning then the departments need to acknowledge that students are different when they belong to a minority group. Ogbu (2004)[77] argued that for an effective teaching to take place, departments need to understand students at a group level as well as at an individual level because even students within the same minority groups are different. Given that students have the right to effective teaching, department involvement needs to understand cultural diversity and cultural differences before a curriculum development is considered.		Teachers have the right to regulated expression[59][64] but may not use their first amendment privileges punitively or discriminatorily[24][78] or in a way which prevents students from learning by ridiculing, proselytizing, harassment or use of unfair grading practices.[24][79]		The 1990 Americans With Disabilities Act[33] and Section 504 of the 1973 Rehabilitation Act[34] prohibit disability based discrimination in the classroom. Act This includes ability discrimination in learning[19][23][27] and deemed otherwise qualified are entitled to equal treatment and reasonable accommodations in both educational and employment related activities.[28][29] The Supreme Court defined 'Otherwise Qualified' as an individual who can perform the required tasks in spite of rather than except for their disability.[30][31]		Disabled students are entitled to equal access to classrooms facilities required to achieve a degree.[24][33][38][80][81]		Students Equality entails that individuals not be treated differently by individuals or systematically by an institution. Thus, testing policies which systematically discriminate, are unlawful according to the constitution. United States v. Fordice (1992) prohibited the use of ACT scores in Mississippi admissions, for instance, because the gap between ACT scores of white and black student was greater than the GPA gap which was not considered at all.[35]		Institutions have an obligation to provide equal opportunities in athletics, bands and clubs. This includes equal accommodation of interests and abilities for both sexes, provision of equipment and facility scheduling for such activities as games and practices, travel allowance and dorm room facilities. It includes also equal quality facilities including locker rooms, medical services, tutoring services, coaching and publicity.[82] To ensure that sufficient opportunities are made available for women, institutions are responsible for complying with Title IX in one of three ways. They must provide athletic opportunities proportionate to enrollment, prove that they are continually expanding opportunities for the underrepresented sex or accommodate the interests and abilities of the underrepresented sex.[83]		The 2008 Higher Education Opportunity Act[26] also requires the disclosure of athletics information including male and female undergraduate enrollment, number of teams and team statistics including the number of players, team operating expenses, recruitment, coach salaries, aid to teams and athletes and team revenue (HEOA, 2008). This information is required to ensure equality standards are met.		Good v. Associated Students Univ. of Washington (1975) found students have the right to have visitors and solicitors in their residence hall rooms.		Students are entitled to housing of equal quality and cost and to equal housing policies.[82]		Until the nineteen nineties gender segregation was permissible so long as institutional rationale for doing so was narrowly defined and justifiable.[24][84] This precedent was officially reversed, however, after the Supreme Court in United States v. Commonwealth of Virginia (1992) found that a woman mistakenly admitted to a men's military college was entitled to remain enrolled.[35][80]		Students with disabilities are also entitled to equal quality dormitories with living accommodations (Section 504 Rehabilitation Act, 1973; Kaplan & Lee, 2011.[24][85] All accommodations are currently free to the student even if the student has the financial means to pay for them.[35][86]		Students are entitled to equal treatment in housing regardless of age unless there is a narrowly defined goal which requires unequal treatment and policy is neutrally applied.[24][87][88][89] Prostrollo v. University of South Dakota (1974), for instance, found that the institution may require all single freshmen and sophomores to live on campus.[24] They did not discriminate between age groups.[35][88]		Piazzola v. Watkins (1971) established that students are not required to waive search and seizure rights as a condition of dormitory residence.[90] Random door sweeps are impermissible.[91][92]		Institutions may enter rooms in times of emergency, if they have proof of illegal activity or a threat to the educational environment.[93][94] Both these terms must be clearly stipulated in advance. Otherwise institutions must ask for permission to enter.[23][95][96] When dorms rooms are legally searched for narrowly defined reasons or officials are legally permitted to enter student rooms, students are not protected from property damage incurred in the search process[24][97] or action taken when evidence is in plain sight.[35][98]		Evidence found in student dorm rooms by institutional employees cannot be used in a court of law and institutions cannot allow police to conduct a search and seizure with out warrant.[99][100][101] Students may not be punished for refusing a warrantless search from institutional authorities or police officers.[23][102] When students freely allow institutional officials to enter institutions can hold students accountable for evidence in plain sight.[35][98]		Griswald v. Connecticut (1965) found that the third, fourth, and fifteen amendments together constitute an inalienable right to privacy. Students are extended the same privacy rights extended to the community at large.[35][98][103]		The 1971 Family Rights and Privacy Act[104] and the 2008 Higher Education Opportunity Act[105] protect student information. Students have the right to access their records, dispute record keeping and limited control over the release of documents to third parties.		FRPA and the HOEA require students sign a release before their student records will be provided to third parties (e.g.: to parents and employers tec.). This legislation does allow schools, however, to release information without student approval for the purpose of institutional audit, evaluation, or study, student aid consideration, institutional accreditation, compliance with legal subpoenas or juvenile justice system officers[104] or in order to comply with laws requiring identification of sex offenders on campus.[26] Institutions may also disclose information to student guardians if the student is declared a dependant for tax purposes (FERPA).		Under FERPA, schools may publish directory information, including the students name, address, phone number, date of birth, place of birth, awards, attendance dates or student ID number, unless students ask the school not to disclose it. The institution must inform students they are entitled to these rights.		Individuals may use pseudonyms online and are not required to identify themselves (Kaplan & Lee, 2011).[23][106] Drug testing Random National Collegiate Athletic Association (NCAA) urine testing is legal to protect athlete health, fair competition and opportunities to educate about drug abuse in sports.[107] Officials are allowed to watch athletes urinate.[108] This overturned an earlier ruling which prohibited urination watching.		The 2008 Higher Education Opportunity Act[26] requires that institutions disclose institutional statistics on the Department of Education (DOE) website to allow students to make more informed educational decisions. Information required on the DOE website includes: tuition, fees, net price of attendance, tuition plans, and statistics including sex, ability, ethnic and transfer student ratios as well as ACT/SAT scores, degrees offered, enrolled, and awarded. Institutions are also required to disclose transfer credit policies and articulation agreements.		The 2008 HOEA[26] also requires institutions of higher education provide financial aid information disclosures, which essentially advertise the financial aid program, pre eligibility disclosures pertaining to the individual student, information differentiating federally insured or subsidized and private loans, preferred lender agreements, institutional rational for the establishment of preferred lender agreements and notice that schools are required to process any loan chosen by students.		According to the 2008 HOEA, financial aid information disclosures must include the average financial aid awarded per person, cost of tuition, fees, room, board, books, supplies and transport.[26]		According to the 2008 HOEA, financial aid information disclosures must include the amount of aid not requiring repayment, eligible loans, loan terms, net required repayment.[26]		Pre-eligibility disclosures must include notice of repayment, lender details, the principle amount, fees, interest rate, interest details, limits of borrowing, cumulative balance, estimated payment, frequency, repayment start date, minimum and maximum payments and details regarding deferment, forgiveness, consolidation and penalties.[26]		Institutions are also required to utilize standard financial terminology and standard dissemination of financial aid information, forms, procedures, data security and searchable financial aid databases to ensure that students can easily understand their contractual rights and obligations. Forms must be clear, succinct, easily readable and disability accessible.		The HOEA (2008) requires third party student loan lenders to disclose information concerning alternative federal loans, fixed and variable rates, limit adjustments, co-borrower requirements, maximum loans, rate, principle amount, interest accrual, total estimated repayment requirement, maximum monthly payment and deferral options.		The HOEA (2008) requires institutions of higher education to engage in financial aid eligibility awareness campaigning to make students aware of student aid and the realities of accepting it.		Van Stry v. State (1984) found institutions may not use student fees to support organizations outside the university.[90] Teachers, likewise, have the right to refuse to pay union fees when they are allocated to objectionable political purposes.[90][109] This implies that students have a right to know what activities they are being allocated towards.		The 2008 Higher Education Opportunity Act requires the disclosure of athletics information including male and female undergraduate enrollment, number of teams and team statistics including the number of players, team operating expenses, recruitment, coach salaries, aid to teams and athletes and team revenue (HEOA, 2008). This information is required to ensure equality standards are met. This ensures that institutions are abiding by Title IX of the 1972 Higher Education Act Amendments which limits sexual discrimination and requires institutions to offer equal sport, club and opportunities.		Rosenberger v. Rector and Visitors of the University of Virginia (1995) found student fees must be allocated in a viewpoint neutral way. They cannot be based on religious, political or personal views (Henderickson; Good v. Associated Students University of Washington) and they cannot be levied as a punishment.[90][110] This suggests that students have a right to policy justification so that they know they are viewpoint neutral.		Students may expect protection from the misuse of time;[73] teachers may not waste students' time or use the class as a captive audience for views or lessons not related to the course.[56][73] Riggin v. Bd. of Trustees of Ball St. Univ. found that instructors may not "wast[e] the time of the students who have come there and paid money for a different purpose." This assumes that students are entitled to know course objectives and content.		Students may be graded fairly and in accordance with criteria set forth by the course syllabuses and may be protected from the addition of new grading criteria.[54][56] Institutions have the responsibility of preserving quality in grade representations and comparability between classes and prevent grade inflation.[24][56] This assumes that students have the right to a syllabus to ensure fair grading.		The 1990 Americans With Disabilities Act[111] and Section 504 of the 1973 Rehabilitation Act protect students against discrimination based on ability.[24][31][34][39][90][112] This includes ability discrimination in discipline and dismissal. Individuals shall be designated with a disability by a medical professional, legally recognized with a disability.[19][24][27]		Matthews v. Elderidge (1976) found when there is the possibility that one’s interests will be deprived through procedural error, the value of additional safe guards and governmental interests, including monetary expenses, should be weighed.[3] Foster v. Board of Trustees of Butler County Community College (1991) found that students are not entitled to due process rights when appealing rejected admissions applications.[24] They are not yet students.		Due process is required when actions have the potential to resulting a property or monetary loss or loss of income or future income etc. This includes degree revocation[3][113] or dismissal. Students have a property interest in remaining at the institution and have protection form undue removal.[24][114]		Students also have a liberty right to protect themselves from defamation of character or a threat to their reputation. Federal district courts have, therefore, found that due process is required in cases involving charges of plagiarism, cheating[90][115] and falsification of research data.[3][113]		In disciplinary measures students are entitled to the provision of a definite charge.[11][90][116][117]		Students are entitled to a prompt notice of charges, e.g., ten days before the hearing.[90][118][119]		In cases involving expulsion or dismissal students are entitled to right to "expert" judgment with a judge who is empowered to expel.[90][118][119]		Students may inspect documents considered by institutional officials in disciplinary hearings.[90][118][119]		Students may stand as a witness and tell their story during disciplinary hearings.[118][119][120]		Students may record disciplinary hearings to ensure they are conducted in a legal fashion.[90][118][119]		Students can expect rulings in disciplinary hearings to be based solely on evidence presented at the hearing.[118][119][121] Students are also entitled to a hearing before a person or committee not involved in the dispute.[11]		Students may expect to receive a written account of findings from disciplinary hearings showing how decisions are in line with evidence.[90][118][119]		Board of Curators of the University of Missouri et al. v. Horowitz (1978) found that fairness means that decisions, a) may not be arbitrary or capricious, b) must provide equal treatment with regard to sex, religion or personal appearance etc. and c) must be determined in a careful and deliberate manner.		Hearings must be conducted before suspension or discipline unless there is a proven threat to danger, damage of property or academic disruption.[122]		Texas Lightsey v. King (1983) determined that due process requires that the outcomes of investigation be taken seriously. A student cannot, for instance, be dismissed for cheating after a hearing has found him not guilty.[3]		The American Bar Association (ABA) found that the need for a fair and just hearing also precludes the use of zero tolerance policies which ignore the circumstances surrounding an action.[3] An individual who commits a crime because they believe they are in danger may not be held accountable in the same way as an individual who conducts the same crime for self-interest.		Students accused of criminal acts including drug possession,[3][123] plagiarism, cheating[90][115] and falsification of research data or fraud, may have greater due process rights.		Students accused of criminal acts may cross-examine witnesses,[3][124] counsel.[3][125]		Students accused of criminal acts may have an open trial to ensure that it is conducted fairly,[3][124] counsel.[3][125]		In non-criminal hearings in the educational setting, schools may use a lesser standard evidence but where criminal matters are concerned they must have clear and convincing evidence.[3][124]		may be present.[3][125]		Students accused of criminal acts should have access to a higher appeals process.[126]		The Student & Administration Equality Act is proposed legislation in the North Carolina General Assembly (House Bill 843) would allow any student or student organization that is charged with a violation of conduct at a North Carolina state university the right to be represented by an attorney at any stage of the disciplinary process regarding the charge of misconduct.		Students are protected from unwarranted search and seizure.[24][102] The fourth and fourteenth amendments protect from search and seizure without a warrant. They enshrine the individuals right to be “secure in their persons, houses, papers and effects.” Warrants must include person, place and specific items eligible for search and or seizure. Search and seizure rights do not apply to automobiles.		Individuals are protected from arrest by undeputized campus police[23][127] and illegal search and seizure if arrest is made.		Students are protected from entrapment by campus police as individuals are protected outside the educational environment.[35][128]		Piazzola v. Watkins (1971) established that students are not required to waive search and seizure rights as a condition of dormitory residence.[90] Random dorm sweeps are impermissible.[129]		Institutions may enter rooms in times of emergency, if they have proof of illegal activity or a threat to the educational environment.[93][94] Both these terms must be clearly stipulated in advance. Otherwise institutions must ask for permission to enter.[24][95][96] When dorms rooms are legally searched for narrowly defined reasons or officials are legally permitted to enter student rooms, students are not protected from property damage incurred in the search process[35][97] or action taken when evidence is in plain sight.[35][98]		Evidence found in student dorm rooms by institutional employees cannot be used in a court of law and institutions cannot allow police to conduct a search and seizure with out warrant.[93][100][101] Students may not be punished for refusing a warrantless search from institutional authorities or police officers.[35][102] When students freely allow institutional officials to enter institutions can hold students accountable for evidence in plain sight.[35][98]		A number of state courts have also found that institutions have a responsibility to prevent or make efforts to limit injury on campus from dangerous property and criminal conditions[24][130][131][132] so long as injury is both foreseeable and preventable.		Knoll v. Board of Regents of the University of Nebraska (1999) found that institutions are responsible for ensuring the safety of facilities which are either under institutional jurisdiction or oversight. Institutions are, thus, responsible for institutionally owned dormitories and fraternities whether on campus or off campus and also for fraternities which may not be owned by the institution but are regulated by the institution. By taking on a regulatory role the institution also takes on this liability. Another state court found, that when students are not lawfully permitted to be on institutional property or in institutional buildings after hours, for instance, the institution is not responsible.[23][133] Where institutions willfully take responsibility for something like a fraternity or require students to abide by their rules they also take on the liability.		Students should be safe from for seeable crime especially in light of past reports of crime, if loitering or dangerous conditions have been made etc.[131][132] Institutions are required to take safety precautions including the monitoring of unauthorized personnel in dormitories, taking action against unauthorized personnel when they pose a threat to safety and ensuring adequate security measures are in place.		Students deserve protection from other students over whom the institution has oversight including voluntarily assumed jurisdiction e.g.: clubs, sororities, fraternities, teams.[131][134] This, for instance, includes protection from foreseeable or preventable fraternity hazing even if fraternities are not located on institutional property. The institution also has a responsibility to inform itself of safety risks existent in institutionally regulated programs (White, 2007). State courts have found that institutions are not responsible, however, for screening ex-convicts before admission,[24][135] 1987).		Students have the right to constitutional freedoms and protections in higher education. Prior to the 1960s institutions of higher education did not have to respect students constitutional rights but could act as a parent in the interest of the student (Nancy Thomas, 1991). In 1960 Shelton v. Turner found "the vigilant protection of constitutional freedoms is nowhere more vital than in the community of American schools" and in 1961 Dixon v. Alabam found that students were not required to give up, as a condition of admission, their constitutional rights and protections.[3][136]		Students retain their first amendment rights in institutions of higher education.[137] Papish v. Board of Curators of the Univ. of Missouri (1973) and Joyner v. Whiting (1973) found students may engage in speech that do not interfere with the rights of others or of the operation of the school.[138] Because schools are places of education they may regulate speech by time, manner and place as long as they provide free speech zones for students[90][139] as long as they are not used to limit expression.[90][140]		The first amendment protects religious, indecent speech and profane hand gestures including the middle finger.[3][90][141][142][143][144][145][146] Texas v. Johnson (1989) found that “[i]f there is a bedrock principle underlying the first amendment, it is that the government may not prohibit the expression of an idea simply because society finds the idea itself offensive or disagreeable. The first amendment does not recognize exceptions for bigotry, racism, and religious intolerance or ideas or matters some may deem trivial, vulgar or profane.”		Clothing, armbands, newspaper editorials and demonstrations have all been found legal forms of free speech.[147][148]		The first amendment covers internet communications.[23][149][150][151] On forums designated by the institution as public forums or commonly used as public forums, students may express themselves without content regulation or removal.[23][150] Online Policy Group v. Diebold, Inc., 2004 Regulation may take place to prevent illegal activities.[35][149]		Students are protected from discrimination based on sex in any program or activity receiving federal funding except military, fraternity, sorority organizations.[24][83][152][153]		Sexual harassment is considered a form of sex discrimination under Title IV of the 1964 Civil Rights Act[35][154][155][156] and applies to all federal programs and activities. Sexual harassment has been prohibited in educational settings[24][157][158] and applies also to both opposite and same sex harassment by students.[159][160]		Institutions have an obligation to provide equal opportunities in athletics, bands and clubs. This includes equal accommodation of interests and abilities for both sexes, provision of equipment and facility scheduling for such activities as games and practices, travel allowance and dorm room facilities. It includes also equal quality facilities including locker rooms, medical services, tutoring services, coaching and publicity.[82] To ensure that sufficient opportunities are made available for women, institutions are responsible for complying with Title IX in one of three ways. They must provide athletic opportunities proportionate to enrollment, prove that they are continually expanding opportunities for the underrepresented sex or accommodate the interests and abilities of the underrepresented sex.[83]		The 2008 Higher Education Opportunity Act also requires the disclosure of athletics information including male and female undergraduate enrollment, number of teams and team statistics including the number of players, team operating expenses, recruitment, coach salaries, aid to teams and athletes and team revenue.[105] This information is required to ensure equality standards are met.		The 1990 Americans With Disabilities Act[33] and Section 504 of the 1973 Rehabilitation Act[34] prohibits ability discrimination in higher education.[24][31][39][90][112] This includes ability discrimination in facility use. Individuals designated with a disability by a medical professional, legally recognized with a disability[19][24][27] and deemed otherwise qualified are entitled to equal treatment and reasonable accommodations in both educational and employment related activities.[28][29] The Supreme Court defined Otherwise qualified as an individual who can perform the required tasks in spite of rather than except for their disability.[30][31]		The 1972 Equal Educational Opportunity Act protects students equal rights to educational opportunity regardless of race and the 1965 Lyndon B. Johnson Executive Order 11246 and the 1964 Civil Rights Act require equal access to employment opportunities regardless of race.[35][153][161][162]		Students are protected from racial segregation which compromises access to quality education.[23][35][163][164][165]		All federal employers or federal contractors are required to take affirmative action[166] to help counteract the effects of historical discrimination. They must create goals, timetables, action plans, budgets and reporting systems to ensure that marginalized populations are given equal employment opportunities. Regulations must also be posted in conspicuous places easily available to all staff and potential employees.[167]		Diversity is defined in much broader terms than race. Grutter v. Bollinger (2003)[50] found a “broad range of qualities and experiences that may be considered valuable contributions” and “a wide variety of characteristics besides race and ethnicity.” Members of the majority are also protected from reverse discrimination.[41][50][51][168] Race neutral affirmative action policies must make exceptions on an individual basis and may not discriminate based on race or color.[41][50][51][168]		Individuals have the right to equal treatment regardless of national origin in institutions of higher education (HEA, 1965) so long as they are citizens or resident aliens of the United States.[35][169] The 1986 Immigration and Reform Control Act also prohibits discrimination based on citizenship. Institutions have the right to discriminate based on national origin so long as objectives are both narrowly defined and neutrally applied.[35][170] It is, thus, permissible to require non-resident aliens who are legally present in the United States to have health insurance for instance.		Age discrimination in federally funded programs is prohibited by the 1975 Age Discrimination Act.[171] This act builds on the 1967 Age Discrimination in Employment Act.[89][172][173][174] It provides protection from unequal treatment between people of different ages from any explicit or implied distinctions which effect the benefits of participation.		Gay Activists Alliance v. Board of Regents of University of Oklahoma (1981) found student groups are entitled to equal and unbiased recognition. Recognition includes the unbiased allocation of facility and equipment resources except when there is proof that a student group does not maintain reasonable housekeeping or poses a threat of danger, disruption or criminal action.[120][175]		Healey v. James (1972)[176] found students have the right to self-determination. “Students—who, by reason of the 26th Amendment, become eligible to vote when 18 years of age—are adults who are members of the college or university community. Their interests and concerns are often quite different from those of the faculty. They often have values, views, and ideologies that are at war with the ones which the college has traditionally espoused or indoctrinated.[176] Bradshaw v. Rawlings (1979) found that "adult students now demand and receive expanded rights of privacy in their college life".[177]		Carr v. St. Johns University (1962) and Healey v. Larsson (1971, 1974) established that students and institutions of higher education formed a contractual relationship. Institutions of higher education are responsible to ensure that contracts, including those implied and verbal, are fair,[3][4] in good faith[24][178] and not unconscionable.[35][179]		Students are protected from deviation from information advertised in the following documents: registration materials, manuals,[24][52] course catalogues,[15][180] bulletins, circulars, regulations,[20] Ross v. Creighton University class syllabi,[54][55][56] student codes,[17][35] and handbooks.[18][35] These documents may be binding implied-n-fact contracts. Goodman v. President and Trustees of Bowdoin College (2001) ruled that institutional documents are still contractual regardless if they have a disclaimer. This decision found that "even though the college had reserved the right to change the student handbook unilaterally and without notice, this reservation of rights did not defeat the contractual nature of the student handbook."		Ross v. Creighton University found that verbal contracts are binding.[21][181] The North Carolina Court of Appeals in Long v. University of North Carolina at Wilmington (1995) found, however, that verbal agreements must be made in an official capacity in order to be binding.[8] Dezick v. Umpqua Community College (1979) found a student was compensated because classes offered orally by the dean were not provided. Healy v. Larsson (1974) found that a student who completed degree requirements prescribed by an academic advisor was entitled to a degree on the basis that this was an implied contract. An advisor should, thus, be considered an official source of information.		John F. Kennedy's 1962 Consumer Bill of Rights, which is not a legal document, asserts that consumers have the right to consumer safety, information preventing fraud, deceit and informed choice, to choose from multiple alternative options and the right to complaint, to be heard and addressed. A number of these principles are enshrined in the law of higher education.		Johnson v. Schmitz (2000) found in a federal district court that a PhD committee established for the sole purpose of advising the student had an obligation to advise the student in his best interest.[24] This is a limited fiduciary right.		Bradshaw v. Rawlings (1979) reiterated that where a special relationship is established, courts may impose a duty upon an institution or individual to ensure the care of others. Duty is defined here “as an obligation to which the law will give recognition in order to require one person to conform to a particular standard of conduct with respect to another person.” Institutions have a duty of care to ensure the safety of students while respecting their personal autonomy. Mullins v. Pine Manor found that "[t]he fact that a college need not police the morals of its resident students... does not entitle it to abandon any effort to ensure their physical safety”.[182]		Dixon v. Alabama (1961) determined that when students' constitutional rights are not upheld, students are eligible to sue for damages in a court of law for monetary or material damages.[24][35][38][89][183][184] Individuals may also file complaints regarding discrimination with the federal Office of Civil Rights (OCR).[35][171][185]		A number of state courts have also found that institutions have a responsibility to prevent or make efforts to limit injury on campus from dangerous property and criminal conditions[24][130][131][132] so long as injury is both foreseeable and preventable.		Knoll v. Board of Regents of the University of Nebraska (1999) found that institutions are responsible for ensuring the safety of facilities which are either under institutional jurisdiction or oversight. Institutions are, thus, responsible for institutionally owned dormitories and fraternities whether on campus or off campus and also for fraternities which may not be owned by the institution but are regulated by the institution. By taking on a regulatory role the institution also takes on this liability. Another state court found, that when students are not lawfully permitted to be on institutional property or in institutional buildings after hours, for instance, the institution is not responsible.[35][133]		Students should be safe from for seeable crime especially in light of past reports of crime, loitering or dangerous conditions.[131][132] Institutions are required to take safety precautions including the monitoring of unauthorized personnel in dormitories, taking action against unauthorized personnel when they pose a threat to safety and ensuring adequate security measures are in place.		Students deserve protection from other students over whom the institution has oversight including voluntarily assumed jurisdiction e.g.: clubs, sororities, fraternities, teams.[131][134] This, for instance, includes protection from foreseeable or preventable fraternity hazing even if fraternities are not located on institutional property. The institution also has a responsibility to inform itself of safety risks existent in institutionally regulated programs.[131] State courts have found that institutions are not responsible, however, for screening exconvicts before admission.[35][186]		Students are protected from discrimination based on sex in any program or activity receiving federal funding except military, fraternity, sorority organizations. There are protections for both public and private employment.[24][35][83][152][153] All employment opportunities must be merit based.[82][187]		All sexes have the right to equal pay for equal work performed in the workplace in institutions of higher education. This would include student employment.[82][187] This may suggest that transgender people are also entitled to equal pay in the workplace.		Women do not have to go on mandatory pregnancy leave before birth, and the right to doctor prescribed leave during pregnancy.[188]		Sexual harassment is prohibited in both educational and workplace settings[24][157][158] and applies also to both opposite and same sex harassment by employees.[158][160][189]		The 1997 Department of Education and Office of Civil Rights Sexual Harassment Guidelines find also that institutions are liable for incidences wherein the institution was aware or "should have been aware" of sexual harassment and took no immediate action.[190][191] The majority of federal court cases involving educational institutions prohibit the maintenance of conditions which allow harassment by other students to continue.[35][154][192][193]		Ability discrimination in federally funded and private programs and activities is prohibited under the 1990 Americans With Disabilities Act (ADA) and Section 504 of the 1973 Rehabilitation Act.[31][35][39][90][112] Individuals designated with a disability by a medical professional, legally recognized with a disability[19][27][35] and deemed otherwise qualified are entitled to equal treatment and reasonable accommodations.[28][29] The Supreme Court defined Otherwise qualified as an individual who can perform the required tasks in spite of rather than except for their disability.[30][31]		The 1990 Americans With Disabilities Act[33] and Section 504 of the 1973 Rehabilitation Act.[34] This includes ability discrimination in recruitment. Individuals designated with a disability by a medical professional, legally recognized with a disability.[19][24][27]		The 1990 Americans With Disabilities Act[33] and Section 504 of the 1973 Rehabilitation Act[34] in discipline and dismissal.[31][35][39][90][112]		Age discrimination in federally funded programs is prohibited by the 1975 Age Discrimination Act.[171] This act builds on the 1967 Age Discrimination in Employment Act.[172][194] It provides protection from unequal treatment between people of different ages from any explicit or implied distinctions which effect the benefits of participation.		Executive Order 11246[167] expanded upon the 1953 Dwight D. Eisenhower Executive Order 10479,[195] which established an anti-discrimination committee to oversee governmental contracting. The 1967 Lyndon B. Johnson Executive Order 11375[196] also requires all facets of federal employment or federally contracted employment be regulated based on merit – this includes institutions of higher education.		Individuals have the right to equal treatment regardless of national origin in employment settings[166][197] so long as they are citizens or resident aliens of the United States.[24][169] The 1986 Immigration and Reform Control Act also prohibits discrimination based on citizenship.		Romania is the country with the greatest student rights legislation currently in place. In 2011 the National Alliance of Student Organizations in Romania, which is also part of the European Student Union, worked with the Romanian National Government to bring into law the Romanian National Student Code of Rights and Responsibilities. This document provides Romanian students with roughly a hundred theoretical and procedural rights necessary to ensure theoretical rights are fulfilled. This document includes the following rights:[198]		Students in both Europe and North America began calling for the expansion of civil rights and student rights during the Vietnam War era. They established legal rights by forming student unions and lobbying for institutional policies (thus, changing the cultural treatment of students), lobbying for legislative change on state and national levels and circulating petitions for the creation of national student rights bills. In America, for instance, students won the right to retain their civil rights in institutions of higher education.[200] In Europe, this movement has been explosive. Students have banded together and formed unions in individual institutions, at the state and national levels and eventually at the continental level as the European Student Union.[201] They have been instrumental in lobbying for rights in individual countries and in the EU in general. In 2011, for instance, Romania put forth an extensive national student bill of rights providing Romanian students with a hundred rights assembled in a clear and easy to access document.[201] Europe has also set forth legislation stipulating the rights of EU students studying in other EU countries.		European students have used the tactics established during the global labour movement of the eighteen and nineteen hundreds to gain labour or work place standards. They have unionized, stated their demands both verbally and in writing (sometimes in the form of a proposed student bill of rights), publicized their message and gone on strike.[202] During the labor movement, workers in the United States, for example, won the right to a 40-hour work week, to a minimum wage, to equal pay for equal work, to be paid on time, to contract rights, for safety standards, a complaint filing process etc.[203] Students have, likewise, demanded that these regulations as well as civil, constitutional, contract and consumer rights, which regulate other industries, be applied to higher education.		The European student movement and the United States movement differ in a number of ways. These differences may be a factor in determining why European Students have been more successful in obtaining legally recognized student rights, from the right to access free education to the right to move and study freely from one EU country to the next, to the right to exercise their national legal rights in institutions of higher education.		Differences between European and United States student movements		The European Student Union ESU mandate requires the ESU to determine the demands of students and to convey them to legislators. The United States Student Association USSA also has a mandate to amplify the student voice in legal decision making but it does not stipulate how it will determine the student voice or ensure that it is representative of the students themselves. The ESU focuses on gathering input from students across the nation, creates a student bill of rights enabling students to critique it, proposes legislation to achieve these rights at both the state and continental level and then creates information resources so students know their rights.[204] The USSA, determines its objectives through the USSA membership. USSA does not seem to conduct research across the nation or to state student objectives on their website so students can express a desire to add or delete from this list. If the USSA does conduct research they do not show this on their website, do not have a search function on the website and do not publish this information for students.		The ESU clearly states student demands through the nation and through the EU. They have compiled these demands into a student bill of rights, referred to as the 2008 Student Rights Charter. This document is not legally binding but it is a clear representation of all student demands. It helps students, institutions and governments understand what students are demanding[201] and also helps student unions, in individual institutions, lobby for rights which help change the culture and treatment of students on a local level. The ESU has democratically created a proposed student bill of rights they want accepted in legislation at a national and continental level. These demands include: access to higher education, to student involvement in institutional governance, extracurricular support and curricular quality standards. Each right has been broken down into more detailed demands required to achieve these rights. While student associations in America are pushing for this, there has been no centralized effort through the national student association.		USSA Legislative initiatives have included student debt forgiveness, enabling illegal students to attend college, allocating more governmental money toward institutions and students but again these objectives seem to be created by USSA members without national research on the student voice. There is no way to search their website to determine if they conduct research to gather input form students across the nation.		The European student movement and the United States movement also differ on a local institutional level. In Europe most institutional student organizations are referred to as student unions which suggests that they are engaged in lobbying for student rights. In America these are referred to as Student Governments or Student Associations and the focus is more on learning the democratic process. The problem is, however, that most student governments only have about 20-25% representation in the Academic Senate or institutional decision making body and far less experience in democratic processes than other institutional representatives. Student governments focus on teaching students how to be leaders and participate in democracy where as unions focus more on determining the student voice and achieving student rights through lobbying.		
A school uniform is a uniform worn by students primarily for a school or otherwise educational institution. They are common in primary and secondary schools in various countries. Although often used interchangeably, there is an important distinction between dress codes and school uniforms: according to scholars such as Nathan Joseph, clothing can only be considered a uniform when it "(a) serves as a group emblem, (b) certifies an institution's legitimacy by revealing individual’s relative positions and (c) suppresses individuality."[1] An example of a uniform would be requiring white button-downs and ties for boys and pleated skirts for girls, with both wearing blazers. A uniform can even be as simple as requiring collared shirts, or restricting colour choices and limiting items students are allowed to wear. A dress code, on the other hand, is much less restrictive, and focuses "on promoting modesty and discouraging anti-social fashion statements," according to Marian Wilde.[2] Examples of a dress code would be not allowing ripped clothing, no logos or limiting the amount of skin that can be shown.						It is difficult to trace the origins of the uniform as there is no comprehensive written history, but rather a variety of known influences. School uniforms are believed to be a practice which dates to the 16th century in the United Kingdom. It is believed that the Christ Hospital School in London in 1552 was the first school to use a school uniform.[3] The earliest documented proof of institutionalised use of a standard academic dress dates back to 1222 when the then Archbishop of Canterbury ordered the wearing of the cappa clausa.[4] This monastic and academic practice evolved into collegiate uniforms in England, particularly in charity schools where uniform dress was often provided for poor children. Universities, primary schools and secondary schools used uniforms as a marker of class and status.[5] Although school uniforms can often be considered conservative and old-fashioned, uniforms in recent years have changed as societal dress codes have changed.[6]		In the United States, a movement toward using uniforms in state schools began when Bill Clinton addressed it in the 1996 State of the Union, saying: "If it means that teenagers will stop killing each other over designer jackets, then our public schools should be able to require their students to wear uniforms."[7] As of 1998 approximately 25% of all U.S. public elementary, middle and junior high schools had adopted a uniform policy or were considering a policy, and two thirds were implemented between 1995 and 1997.[8]		There are an abundance of theories and empirical studies looking at school uniforms, making statements about their effectiveness. These theories and studies elaborate on the benefits and also the shortcomings of uniform policies. The issue of nature vs. nurture comes into play, as uniforms affect the perceptions of masculinity and femininity, complicate the issue of gender classification and also subdue the sexuality of girls. With uniforms also comes a variety of controversies, pros, cons and major legal implications.[citation needed]		There are two main empirical findings that are most often cited in the political rhetoric surrounding the uniform debate. One of these, the case study of the Long Beach Unified School District, is most often cited in support of school uniforms and their effectiveness whereas Effects of Student Uniforms on Attendance, Behavior Problems, Substance Use, and Academic Achievement is the most frequently cited research in opposition to the implementation of school uniform policies.		The case study of the Long Beach Unified School District was the study of the first large, urban school in the United States to implement a uniform policy. In 1994, mandatory school uniforms were implemented for the districts elementary and middle schools as a strategy to address the students' behaviour issues.[9] The district simultaneously implemented a longitudinal study to research the effects of the uniforms on student behavior. The study attributed favourable student behavioral changes and a significant drop in school discipline issues[9] to the mandatory uniform policy. This case study attributed the following noticeable outcomes to the use of uniforms throughout the district:		Other research found that uniforms were not an effective deterrent to decrease truancy, did not decrease behavior problems, decrease substance use, or increase student achievement.		A study suggested that "instead of directly affecting specific outcomes, uniforms act as a catalyst for change and provide a highly visible opportunity for additional programs" within schools. In fact, Brunsma et al., 1998 considered that this was the case with the Long Beach Unified School District case study as several additional reform efforts were implemented simultaneously with the mandatory uniform policy.[10]		Brunsma stated that despite the inconclusiveness of the effects of uniforms, they became more common because "this is an issue of children's rights, of social control, and one related to increasing racial, class and gender inequalities in our schools."[11]		As uniforms have become more normalised, there have also been an increasing number of lawsuits brought against school districts. According to David Brunsma, one in four public elementary schools and one in eight public middle and high schools in the USA have policies dictating what a student wears to school.[12] The school code within states’ constitutions typically asserts that it allows the board of school directors to make reasonable rules and regulations as they see fit in managing the school’s affairs. As of 2008, there are currently 23 states that allow school districts to mandate school uniforms.[13] The constitutional objections usually brought upon school districts tend to fall into one of the following two categories: (1) a violation of the students’ First Amendment right to free expression (2) a violation of parents' right to raise their children without government interference. Although up until this point, The Supreme Court has not ruled on a case involving school uniforms directly, in the 1968 decision Tinker v. Des Moines Independent Community School District, the Court ruled that upon entering school, students do not shed their constitutional rights to freedom of speech.[14]		Internationally, there are differing views of school uniforms. In the Australian state of Queensland, Ombudsman Fred Albietz ruled in 1998 that state schools may not require uniforms.[15] In the Philippines, the Department of Education abolished the requirement of school uniforms in public schools[when defined as?].[16] In England and Wales, technically a state school may not permanently exclude students for "breaching school uniform policy", under a policy promulgated by the Department for Children, Schools and Families but students not wearing the correct uniform are asked to go home and change. In Scotland, some local councils (that have responsibility for delivering state education) do not insist on students wearing a uniform as a precondition to attending and taking part in curricular activities.[17] Turkey abolished mandatory uniforms in 2010.[18]		In the Canady v. Bossier Parish School Board lawsuit in 2000, a Louisiana district court ruled in favour of the school board because it did not see how the free speech rights of the students were being violated due to the school board's uniform policy. Even though the plaintiff appealed the decision, the Fifth Circuit Court also ruled in favour of the school board after implementing a four-step system that is still used today. Firstly, a school board has to have the right to set up a policy. Secondly, the policy must be determined to support a fundamental interest of the board as a whole. Thirdly, the guidelines cannot have been set for the purpose of censorship. Finally, the limits on student expression cannot be greater than the interest of the board. As long as these four policies are in place, then no constitutional violation can be claimed.[19]		In the Forney Independent School District of Forney, Texas in 2001, the school board decided to implement a school uniform policy allowing the students to wear a polo shirt, oxford shirt or blouse in four possible colours, and blue or khaki pants or shirts, a skirt or jumper. While there was some flexibility with shoes, certain types were prohibited along with any sort of baggy clothes. The parents of the Littlefield family requested that their son be exempt from the policy, but were denied. In response, the Littlefields filed a lawsuit against the school district, under the pretenses that this uniform mandate infringed on their rights as parents to control how they brought up their children and their education. They even went as far as to cite an infringement on religious freedom, claiming that opting out of the uniforms on the grounds of religion allowed the school to rank the validity of certain religions. Before trial, the District Court dismissed the case, so the family appealed. Ultimately, the Fifth Circuit Court ruled that the students' rights were not being violated even though the claims presented were valid. They ruled that school rules derived from the education would override the parents' right to control their children's upbringing in this specific situation. As far as the religious freedom violation accusations, the court ruled that the policy did not have a religious goal, and thus did not infringe on religious freedom rights.[20]		In 2003, Liberty High School, a school of the Clark County School District in Henderson, Nevada, implemented a uniform policy of khakis and red, white or blue polo shirts. A junior by the name of Kimberly Jacobs was suspended a total of five times because she wore a religious shirt to school and got cited for uniform violations. Her family sued the Clark County School District under the claims that her First Amendment rights were being infringed upon and that the uniform policy was causing students to be deprived of due process. The plaintiff's requests were for injunctive relief, the expunging of suspensions from Jacob's school record and awarding of damages. The injunction was granted to the family meaning that the school could no longer discipline her for breaking the uniform policy. At this ruling, the school district appealed. The next court ruled on the side of the school district as it determined that the uniform policy was in fact neutral and constitutional, and it dismissed the claims of the plaintiff.[21]		In 2011, a Nevada public elementary school of the Washoe County School District decided to add the school's motto, Tomorrow's Leaders embroidered in small letters on the shirt. In response, Mary and John Frudden, parents of a student sued the school district on the basis of it violating the 1st Amendment. The court ultimately dismissed the case filed by the Fruddens over the uniforms. However, the family appealed, and two years later, a three-judge panel of the 9th U.S. Circuit Court of Appeals heard the case. The court ruled to reverse the previous decision of dismissing the case, and also questioned the apparent policy for students that were part of a nationally recognised group such as Boy Scouts and Girl Scouts who were able to wear the uniforms in place of the school ones on regular meeting days. The 9th circuit panel ruled that the school had not provided enough evidence for why it instituted this policy, and that the family was never given a chance to argue.[22]		There are several positive and negative social implications of uniforms on both the students wearing them and society as a whole.		One of the criticisms of uniforms is that it imposes standards of masculinity and femininity from a young age. Uniforms are considered a form of discipline that schools use to control student behavior and often promote conventional gendered dress.[23][24] Boys often are required to wear trousers, belts, and closed-toe shoes and have their shirts tucked in at all times. They are also often required to have their hair cut short. Some critics allege that this uniform is associated with the dress of a professional business man, which, they claim, gives boys at a young age the impression that masculinity is gained through business success.[25] For girls, some uniforms promote femininity by requiring girls to wear skirts. Skirts are seen by some critics as a symbol of femininity because they restrict movement and force certain ways of sitting and playing.[24]		Uniforms often start to increase in popularity around middle school in the United States, when students begin going through puberty. Uniforms can be seen as a way to restrict the sexualization of girls (rules on hems of skirts, no shoulders). Uniforms take the focus away from sexuality and focus it on academics in a school setting for girls.[26]		Miniskirts have been very popular in Japan, where they became part of school uniforms, and they came to be worn within the Kogal culture.[27][28]		In some cultures, the topic of school uniforms has sparked a multitude of controversies and debates over the years. Debates concerning the constitutionality and economic feasibility of uniforms also contribute to the controversy.		In the United States, the implementation of school uniforms began following ten years of research indicating the effectiveness of private schools. Some state-school reformers cited this research to support policies linked to private and Catholic school success. However, within the Catholic school literature, school uniforms have never been acknowledged as a primary factor in producing a Catholic school effect.[29][30] Some public-school administrators began implementing uniform policies to improve the overall school environment and academic achievement of the students. This is based on the assumption that uniforms are the direct cause of behavioral and academic outcome changes.[30]		Another area of controversy regarding school uniform and dress code policies revolve around the issue of gender. Nowadays, more teenagers are more frequently "dressing to articulate, or confound gender identity and sexual orientation", which brings about "responses from school officials that ranged from indifferences to applause to bans".[31] In 2009, there were multiple conflicts across the United States arising from disparities between the students' perception of their own gender, and the school administrators' perception of the students' gender identity. Instances include the following:[32]		Although not all schools in the United States are required to wear school uniforms, the United States is slowly adapting the use of school uniforms. "Almost one in five US public schools required students to wear uniforms during the 2011-2012 school year, up from one in eight in 2003-2004."[33] The ideology of school uniform is that it will create a safer environment for students and help with equality. In some areas uniforms have become essential due to the poverty level that the schools reside in. "Mandatory uniform policies in public schools are found more commonly in high-poverty areas."[34]		Stephanie Northen of The Guardian wrote that school uniforms are less controversial in the United Kingdom compared to the United States and are usually not opposed on free speech grounds.[11]		Advocates of uniforms have proposed multiple reasons supporting their implementation and claiming their success in schools. A variety of these claims have no research supporting them. Some of these pros include the following: Advocates believe that uniforms affect student safety by:[10]		For example, in the first year of the mandatory uniform policy in Long Beach, California, officials reported that fighting in schools decreased by more than 50%, assault and battery by 34%, sex offenses by 74%, and robbery by 66%.[35] Advocates also believe that uniforms increase student learning and positive attitudes toward school through:		Wearing uniforms leads to decreased behavior problems by increasing attendance rates, lowering suspension rates, and decreasing substance use among the student body. Proponents also attribute positive psychological outcomes like increased self-esteem, increased spirit, and reinforced feelings of oneness among students to wearing uniforms. Additional proponent arguments include that school uniforms:[37]		Currently pros of school uniforms center around how uniforms impact schools' environments. Proponents have found a significant positive impact on school climate, safety, and students’ self-perception from the implementation of uniforms.		The opposing side of uniforms have claimed their ineffectiveness using a variety of justifications, a variety of which have research supporting them. Some of the cons to school uniforms include the following legal, financial, and questionable effectiveness concerns:[10] The primary concern with school uniforms or strict dress codes is that it limits the ability of students to express themselves. Clothing is viewed as a mean of expression – making all students wear the same clothes or limit them to what they can wear can disrupt their sense of identity. One of the main controversies can lie within Dress Code Policies vs. Freedom of Speech.[38] This establishes that students cannot wear the latest trends, mid-drift, or clothes that the school finds that interrupts the learning environment. However, students can wear clothing artifacts that express their religion. "Both the Constitution and most state laws protect students' rights to wear religious attire inool [sic] school, such as the wearing of a turban, yarmulke, or head scarf."[38]		Another negative aspect of school uniforms is that it can be sexist. Boys and girls are not disciplined the same when it comes to dress codes. "Transgender students have been sent home for wearing clothing different from what’s expected of their legal sex, while others have been excluded from yearbooks."[39] Some schools are not advocates of females and females dressing of the opposite sex. Research on how school uniforms and school dress codes influence the student can be inconclusive, but many people oppose to school uniforms and strict dress code policies. "In the U.S., over half of public schools have a dress code, which frequently outline gender-specific policies."[39]		According to Marian Wilde,[42] additional opponent arguments include that school uniforms:		
The University of Bologna (Italian: Università di Bologna, UNIBO), founded in 1088, is the oldest university in continuous operation[2], as well as one of the leading academic institutions in Italy and Europe[3].		It was the first place of study to use the term universitas for the corporations of students and masters, which came to define the institution located in Bologna, Italy.[4] The University's crest carries the motto Alma mater studiorum and the date A.D. 1088, and it has about 85,500 students in its 11 schools.[5] It has campuses in Ravenna, Forlì, Cesena and Rimini and a branch center abroad in Buenos Aires.[6] It also has a school of excellence named Collegio Superiore di Bologna. An associate publisher of the University of Bologna is Bononia University Press S.p.A. (BUP).						The date of its founding is uncertain, but believed by most accounts to have been 1088.[4] The university received a charter from Frederick I Barbarossa in 1158, but in the 19th century, a committee of historians led by Giosuè Carducci traced the founding of the University back to 1088, which would make it the oldest continuously-operating university in the world.[7][8][9]		The University arose around mutual aid societies of foreign students called "nations" (as they were grouped by nationality) for protection against city laws which imposed collective punishment on foreigners for the crimes and debts of their countrymen. These students then hired scholars from the city to teach them. In time the various "nations" decided to form a larger association, or universitas—thus, the university. The university grew to have a strong position of collective bargaining with the city, since by then it derived significant revenue through visiting foreign students, who would depart if they were not well treated. The foreign students in Bologna received greater rights, and collective punishment was ended. There was also collective bargaining with the scholars who served as professors at the university. By the initiation or threat of a student strike, the students could enforce their demands as to the content of courses and the pay professors would receive. University professors were hired, fired, and had their pay determined by an elected council of two representatives from every student "nation" which governed the institution, with the most important decisions requiring a majority vote from all the students to ratify. The professors could also be fined if they failed to finish classes on time, or complete course material by the end of the semester. A student committee, the "Denouncers of Professors", kept tabs on them and reported any misbehavior. Professors themselves were not powerless, however, forming a College of Teachers, and securing the rights to set examination fees and degree requirements. Eventually, the city ended this arrangement, paying professors from tax revenues and making it a chartered public university.[10]		The university is historically notable for its teaching of canon and civil law; indeed, it was set up in large part with the aim of studying the Digest,[11] a central text in Roman law, which had been rediscovered in Italy in 1070, and the university was central in the development of medieval Roman law.[12] Until modern times, the only degree granted at that university was the doctorate.		Higher education processes are being harmonised across the European Community. Nowadays the University offers 101 different "Laurea" or "Laurea breve" first-level degrees (three years of courses), followed by 108 "Laurea specialistica" or "Laurea magistrale" second-level degrees (two years). However, other 11 courses have maintained preceding rules of "Laurea specialistica a ciclo unico" or "Laurea magistrale a ciclo unico", with only one cycle of study of five years, except for medicine and dentistry which requires six years of courses. After the "Laurea" one may attain 1st level Master (one-year diploma, similar to a Postgraduate diploma). After second-level degrees are attained, one may proceed to 2nd level Master, specialisation schools (residency), or doctorates of research (PhD).		The 11 Schools (which replace the preexisting 23 faculties) are:		The University is structured in 33 departments[13] (there were 66 until 2012), organized by homogeneous research domains that integrate activities related to one or more Faculty. A new department of Latin History was added in 2015.		The 33 departments are:		In the early 1950s, some students of the University of Bologna were among the founders of the review "il Mulino". On 25 April 1951 the first issue of the review was published in Bologna. In a short time, "il Mulino" became one of the most interesting reference points in Italy for the political and cultural debate, and established important editorial relationships in Italy and abroad. Editorial activities evolved along with the review. In 1954, the il Mulino publishing house (Società editrice il Mulino) was founded, which today represents one of the most relevant Italian publishers. In addition to this were initiated research projects (focusing mostly on the educational institutions and the political system in Italy), that eventually led, in 1964, to the establishment of the Istituto Carlo Cattaneo.		See: Serafino Mazzetti, Repertorio di tutti I professori antichi e moderni della famosa Università...di Bologna (Bologna 1848).  		The 2017 QS World University Rankings ranked the University of Bologna 182nd in the world[18], with as many as 21 subjects in the top 100 of their respective areas. In the 2016-17 THE World University Rankings the University of Bologna was ranked in the world's top 250 universities.[19]		In 2017, the Italian newspaper La Repubblica, in collaboration with CENSIS, has awarded the University of Bologna the first place in its academic ranking of Italian universities with more than 40,000 students.[20] It ranks first of universities in Southern Europe and retains a reputation as the most prestigious Italian university.[21]		Coordinates: 44°29′38″N 11°20′34″E﻿ / ﻿44.49389°N 11.34278°E﻿ / 44.49389; 11.34278		
An album is a collection of audio recordings issued as a single item on CD, record, audio tape or another medium. Albums of recorded music were developed in the early 20th century, first as books of individual 78rpm records, then from 1948 as vinyl LP records played at  33 1⁄3 rpm. Vinyl LPs are still issued, though in the 21st-century album sales have mostly focused on compact disc (CD) and MP3 formats. The audio cassette was a format used from the late 1970s through to the 1990s alongside vinyl.		An album may be recorded in a recording studio (fixed or mobile), in a concert venue, at home, in the field, or a mix of places. The time frame for completely recording an album varies between a few hours to several years. This process usually requires several takes with different parts recorded separately, and then brought or "mixed" together. Recordings that are done in one take without overdubbing are termed "live", even when done in a studio. Studios are built to absorb sound, eliminating reverberation, so as to assist in mixing different takes; other locations, such as concert venues and some "live rooms", allow for reverberation, which creates a "live" sound.[1] The majority of studio recordings contain an abundance of editing, sound effects, voice adjustments, etc. With modern recording technology, musicians can be recorded in separate rooms or at separate times while listening to the other parts using headphones; with each part recorded as a separate track.		Album covers and liner notes are used, and sometimes additional information is provided, such as analysis of the recording, and lyrics or librettos.[2][3] Historically, the term "album" was applied to a collection of various items housed in a book format. In musical usage the word was used for collections of short pieces of printed music from the early nineteenth century.[4] Later, collections of related 78rpm records were bundled in book-like albums[5] (one side of a 78 rpm record could hold only about 3.5 minutes of sound). When long-playing records were introduced, a collection of pieces on a single record was called an album; the word was extended to other recording media such as compact disc, MiniDisc, Compact audio cassette, and digital albums as they were introduced.[6]						The LP record (long play), or  33 1⁄3 rpm microgroove vinyl record, is a gramophone record format introduced by Columbia Records in 1948.[7] It was adopted by the record industry as a standard format for the "album". Apart from relatively minor refinements and the important later addition of stereophonic sound capability, it has remained the standard format for vinyl albums. The term "album" had been carried forward from the early nineteenth century when it had been used for collections of short pieces of music.[4] Later, collections of related 78rpm records were bundled in book-like albums.[5] When long-playing records were introduced, a collection of pieces on a single record was called an album; the word was extended to other recording media such as compact disc, MiniDisc, Compact audio cassette, and digital albums, as they were introduced.[6] As part of a trend of shifting sales in the music industry, some commenters have declared that the early 21st century experienced the death of the album.		While an album may contain as many or as few tracks as required, the criteria for the UK Albums Chart is that a recording counts as an "album" if it either has more than four tracks or lasts more than 25 minutes.[8] Sometimes shorter albums are referred to as "mini-albums" or EPs.[9] Albums such as Tubular Bells, Amarok, Hergest Ridge by Mike Oldfield, and Yes's Close to the Edge, include fewer than four tracks. There are no formal rules against artists such as Pinhead Gunpowder referring to their own releases under thirty minutes as "albums".		If an album becomes too long to fit onto a single vinyl record or CD, it may be released as a double album where two vinyl LPs or compact discs are packaged together in a single case, or a triple album containing three LPs or compact discs. Recording artists who have an extensive back catalogue may re-release several CDs in one single box with a unified design, often containing one or more albums (in this scenario, these releases can sometimes be referred to as a "two (or three)-fer"), or a compilation of previously unreleased recordings. These are known as box sets. Some musical artists have also released more than three compact discs or LP records of new recordings at once, in the form of boxed sets, although in that case the work is still usually considered to be an album.		Material (music or sounds) is stored on an album in sections termed tracks, normally 11 or 12 tracks. A music track (often simply referred to as a track) is an individual song or instrumental recording. The term is particularly associated with popular music where separate tracks are known as album tracks; the term is also used for other formats such as EPs and singles. When vinyl records were the primary medium for audio recordings a track could be identified visually from the grooves and many album covers or sleeves included numbers for the tracks on each side. On a compact disc the track number is indexed so that a player can jump straight to the start of any track. On digital music stores such as iTunes the term song is often used interchangeably with track regardless of whether there is any vocal content.		A bonus track (also known as a bonus cut or bonus) is a piece of music which has been included as an extra. This may be done as a marketing promotion, or for other reasons. It is not uncommon to include singles as bonus tracks on re-issues of old albums, where those tracks weren't originally included. Online music stores allow buyers to create their own albums by selecting songs themselves; bonus tracks may be included if a customer buys a whole album rather than just one or two songs from the artist. The song is not necessarily free nor is it available as a stand-alone download, adding also to the incentive to buy the complete album. In contrast to hidden tracks, bonus tracks are included on track listings and usually do not have a gap of silence between other album tracks.		The contents of the album are usually recorded in a studio or live in concert, though may be recorded in other locations, such as at home (as with JJ Cale's Okie,[10][11] Beck's Odelay,[12] David Gray's White Ladder,[13] and others),[14][15][16] in the field - as with early Blues recordings,[17] in prison,[18] or with a mobile recording unit such as the Rolling Stones Mobile Studio.[19][20]		Although recording in a studio can be done using large multi-track systems with many overdubs and different takes of the same instrument, some recordings may be done "live" in order to reproduce the feel and energy of a live performance.[citation needed] Basic parts such as drums and rhythm guitar may be recorded live, with overdubs such as solos and vocals recorded later.[citation needed] Studio recordings may be mixed and mastered at different facilities.[citation needed]		Vinyl LP records have two sides, each comprising one-half of the album. If a pop or rock album contained tracks released separately as commercial singles, they were conventionally placed in particular positions on the album.[6] A common configuration was to have the album led off by the second and third singles, followed by a ballad. The first single would lead off side 2.[citation needed] In the past many singles (such as the Beatles' "Hey Jude" and Bob Dylan's "Positively 4th Street") did not appear on albums, but others (such as the Beatles' "Come Together" and Dylan's "Like a Rolling Stone") formed part of an album released concurrently. Today, many commercial albums of music tracks feature one or more singles, which are released separately to radio, TV or the Internet as a way of promoting the album.[21] Albums have also been issued that are compilations of older tracks not originally released together, such as singles not originally found on albums, b-sides of singles, or unfinished "demo" recordings.[6]		Album sets of the past were sequenced for record changers. In the case of a two-record set, for example, sides 1 and 4 would be stamped on one record, and sides 2 and 3 on the other. The user would stack the two records onto the spindle of an automatic record changer, with side 1 on the bottom and side 2 (on the other record) on top. Side 1 would automatically drop onto the turntable and be played. When finished, the tone arm's position would trigger a mechanism which moved the arm out of the way, dropped the record with side 2, and played it. When both records had been played, the user would pick up the stack, turn it over, and put them back on the spindle—sides 3 and 4 would then play in sequence.[6] Record changers were used for many years of the LP era, but eventually fell out of use.		8-track tape (formally Stereo 8: commonly known as the eight-track cartridge, eight-track tape, or simply eight-track) is a magnetic tape sound recording technology popular in the United States[22] from the mid-1960s to the late 1970s when the Compact Cassette format took over.[23][24] The format is regarded as an obsolete technology, and was relatively unknown outside the United States, the United Kingdom, Canada and Australia.[25][26]		Stereo 8 was created in 1964 by a consortium led by Bill Lear of Lear Jet Corporation, along with Ampex, Ford Motor Company, General Motors, Motorola, and RCA Victor Records (RCA). It was a further development of the similar Stereo-Pak four-track cartridge created by Earl "Madman" Muntz. A later quadraphonic version of the format was announced by RCA in April 1970 and first known as Quad-8, then later changed to just Q8.		The Compact Cassette was a popular medium for distributing pre-recorded music in the late 1970s through to the 1990s. The very first "Compact Cassette" was introduced by Philips in August 1963 in the form of a prototype.[27] Compact Cassettes became especially popular during the 1980s after the advent of the Sony Walkman, which allowed the person to control what they listened to.[27][28] The Walkman was convenient because of its size, the device could fit in most pockets and often came equipped with a clip for belts or pants.[27] Compact cassettes also saw the creation of mixtapes, which are tapes containing a compilation of songs created by any average listener of music.[29] The songs on a mixtape generally relate to one another in some way, whether it be a conceptual theme or an overall sound.[29] The compact cassette used double-sided magnetic tape to distribute music for commercial sale.[27][30] The music is recorded on both the "A" and "B" side of the tape, with cassette being "turned" to play the other side of the album.[27] Compact Cassettes were also a popular way for musicians to record "Demos" or "Demo Tapes" of their music to distribute to various record labels, in the hopes of acquiring a recording contract.[31] The sales of Compact Cassettes eventually began to decline in the 1990s, after the release and distribution Compact Discs. After the introduction of Compact discs, the term "Mixtape" began to apply to any personal compilation of songs on any given format.[29] Recently there has been a revival of Compact Cassettes by independent record labels and DIY musicians who prefer the format because of its difficulty to share over the internet.[32]		The compact disc format replaced both the vinyl record and the cassette as the standard for the commercial mass-market distribution of physical music albums.[33] After the introduction of music downloading and MP3 players such as the iPod, US album sales dropped 54.6% from 2001 to 2009.[34] The CD is a digital data storage device which permits digital recording technology to be used to record and play-back the recorded music.[30][33]		Most recently, the MP3 audio format has matured, revolutionizing the concept of digital storage. Early MP3 albums were basically CD-rips created by early CD-ripping software, and sometimes real-time rips from cassettes and vinyl.		The so-called "MP3 album" isn't necessarily just in MP3 file format, in which higher quality formats such as FLAC and WAV can be used on storage mediums that MP3 albums reside on, such as CD-R-ROMs, hard drives, flash memory (e.g. thumbdrives, MP3 players, SD cards), etc.[citation needed]		Most albums are studio albums - that is, they are recorded in a recording studio with equipment meant to give those overseeing the recording as much control as possible over the sound of the album. They minimize external noises and reverberations and have highly sensitive microphones and sound mixing equipment. In some studios, each member of a band records their part in separate rooms (or even at separate times, while listening to the other parts of the track with headphones to keep the timing right).				An album may be recorded in a recording studio (fixed or mobile), in a concert venue, at home, in the field, or a mix of places. The recording process may occur within a few hours or may take several years to complete, usually in several takes with different parts recorded separately, and then brought or "mixed" together. Recordings that are done in one take without overdubbing are termed "live", even when done in a studio. Studios are built to absorb sound, eliminating reverberation, so as to assist in mixing different takes; other locations, such as concert venues and some "live rooms", allow for reverberation, which creates a "live" sound.[1]		Concert or stage performances are recorded using remote recording techniques. Live albums may be recorded at a single concert, or combine recordings made at multiple concerts. They may include applause and other noise from the audience, comments by the performers between pieces, improvisation, and so on. They may use multitrack recording direct from the stage sound system (rather than microphones placed among the audience), and can employ additional manipulation and effects during post-production to enhance the quality of the recording. Comedy albums, in particular, are most often recorded live because the audience reaction is part of the cue that the comedian is succeeding.		The best-selling live album worldwide is Garth Brooks' Double Live, having sold in excess of 21 million copies as of November 2006.[35] In Rolling Stone's 500 Greatest Albums of All Time 18 albums were live albums.[citation needed]				A solo album, in popular music, is an album recorded by a current or former member of a rock group which is released under that artist's name only, even though some or all other band members may be involved. The solo album appeared as early as the late 1940s. A 1947 Billboard magazine article heralded "Margaret Whiting huddling with Capitol execs over her first solo album on which she will be backed by Frank De Vol".[36] There is no formal definition setting forth the amount of participation a band member can solicit from other members of his band, and still have the album referred to as a solo album. One reviewer wrote that Ringo Starr's third venture, Ringo, "[t]echnically... wasn't a solo album because all four Beatles appeared on it".[37] Three of the four members of the Beatles released solo albums while the group was officially still together.		A performer may record a solo album for a number of reasons. A solo performer working with other members will typically have full creative control of the band, be able to hire and fire accompanists, and get the majority of the proceeds. The performer may be able to produce songs that differ widely from the sound of the band with which the performer has been associated, or that the group as a whole chose not to include in its own albums. Graham Nash, of The Hollies described his experience in developing a solo album as follows: "The thing that I go through that results in a solo album is an interesting process of collecting songs that can't be done, for whatever reason, by a lot of people".[38] A solo album may also represent the departure of the performer from the group.				A tribute or cover album is a collection of cover versions of songs or instrumental compositions. Its concept may involve various artists covering the songs of a single artist, genre or period, a single artist covering the songs of various artists or a single artist, genre or period, or any variation of an album of cover songs which is marketed as a "tribute".		
Student Human Rights Ordinance (Korean: 학생인권조례) is an ordinance in operation in some cities and provinces, South Korea, first began in 2010 Gyeonggi and expanded to 2011 Gwangju, 2012 Seoul, and 2013 Jeonbuk (North Jeolla). The primary objective of this ordinance is to guarantee human rights of student and youth in South Korea.[1]		
"Freshman 15" is an expression commonly used in the United States and Canada that refers to an amount (somewhat arbitrarily set at 15 pounds, and originally just 10[1]) of weight gained during a student's first year at college. In Australia and New Zealand it is sometimes referred to as First Year Fatties,[2] Fresher Spread,[3] or Fresher Five,[4][5] the latter referring to a five-kilogram gain.		The purported causes of this weight gain are increased alcohol intake and the consumption of fat and carbohydrate-rich cafeteria-style food and fast food in university dormitories. Many other causes include malnutrition, stress, and decreased levels of exercise. All of these factors can affect each person in a different way. Studies confirm many of these causes. Colleges and universities have recently been cracking down on this common problem and are trying to educate people on how to prevent it. This problem has grown so much that students are focusing on how to stop the freshman 15 before it happens.[6]		Despite how commonly the Freshman 15 is asserted, an Ohio State University study showed that the average college student gains only two to three pounds in their first year. Additionally, it showed that college students did not gain any more weight than non-college students of the same age, and that the only factor that increased weight gain was heavy drinking.[7]						College meal plans are designed to give students a wide variety of options. The most generic meal plans include a set amount of meals per day, so many per week, or so many per semester. In addition, plans may include extra money that can be spent on snacks or other meals. Students can eat several meals a day or less than three meals a day. The meal plan was designed to benefit the student but it can be abused.[8]		The dining halls at colleges try to make dining at school convenient and comfortable. Dining halls can provide a wide variety and bountiful options of food. They can also provide a place where students can endlessly indulge in high calorie foods such as pizza, fried food, and ice cream. When exposed to these fast food restaurants, students are generally more likely to choose them over healthier options, which leads to weight gain, especially if fast food restaurants are more prevalent on campus than other restaurants.[9] A study done on 60 students at Cornell University showed that 20% of the weight gained by the test subjects was due to the fact students were eating at all-you-can-eat dining halls.[8]		College dining halls appeal to some students and repulse others, and this is especially problematic in the first year. A study published in the Journal of Adolescent Health determined that “regular family meals provide an opportunity for the role modeling of healthy eating patterns and social interactions among family members, and may thus help to reinforce healthy eating patterns and prevent disordered eating behaviors.”[10] Thus, parents determine when, where, what and how their children eat. Away from home, often for the first time, students have no parental monitoring of their eating habits and have to discover, or rediscover, what good eating patterns are.		In parental-supervised eating, teenagers typically ingest the proper amount of calories. The average 18-year-old-male is 68 to 70 in (170 to 180 cm) tall and weighs between 160 and 170 lb (73 and 77 kg). The average 18-year-old-female is 64 in (160 cm) tall and weighs between 125 and 130 lb (57 and 59 kg).[11] According to a calorie counter[12] used at the Baylor College of Medicine, an average 18-year-old-male who is rarely active needs to consume approximately 2676 calories per day to maintain his weight.[13] Similarly, an average 18-year-old female who is rarely active needs to consume approximately 1940 calories per day to maintain her weight.		In parental-supervised diets, students also usually ingest the proper proportion of foods from the different dietary groups; once removed from the parental dinner table, many college students do not eat enough fruits, vegetables, and dairy products.[14] This is because when students go off to college, they face an independence that they usually have not experienced before. Many have to learn how to go out and feed themselves instead of having their parents cook for them.[15] Research has shown that over 60 percent of college students commonly ingest sugary and fatty foods like chocolate and potato chips over fruits and vegetables.[16] Presently, sugar accounts for approximately 20 percent of an American’s diet, which equates to about 90 pounds of sugar per person per year.[17] This explains why a study, conducted by Stephanie Goodwin of Virginia Polytechnic Institute, states that three out of four students don’t eat at least five servings of fruits and vegetables daily, denying students key vitamins C and E, as well as fiber.		Malnutrition can be caused by a number of things including inadequate or unbalanced diet, problems with digestion or absorption, or certain medical conditions.[18] Hunger is a main cause of malnutrition because if it is not satisfied then malnutrition is sure to follow. People suffer from hunger because of a lack of food and the nutrients which accompany food in the short term. If hunger proceeds for an extended period of time there is a good chance that it will lead to malnutrition. Malnutrition can affect people of every age. Though infants, children, and adolescents suffer more from malnutrition because of their need for critical nutrients for their normal development, older people may have problems because of aging or illness. People of college age have issues with malnutrition as well, though it may not be as severe as with the younger kids or the elderly. In people in their undergraduate years of study at a four-year university, malnutrition can occur due to negligence of eating and even their diet.[19]		College students must deal with many different changes in living conditions when it comes to dining.		In addition, some college students consume a lot of alcoholic beverages. The vitamins and minerals consumed from alcohol and from food consumed with alcohol have a good chance of being unabsorbed. People who drink large amounts of alcohol have a good chance of becoming malnourished or losing an unhealthy amount of weight because of the absorption blocking qualities of alcohol.[19]		The body has a certain number of calories that it needs to consume in order to maintain its weight. This is determined through height, weight, age, and several other factors, which differs from person to person. When a person takes in more or fewer calories than that set limit, weight is either gained or lost. Alcohol provides a large amount of calories in a small quantity of liquid, which tends to lead to unwanted extra calories.[20]		When drinking alcohol on a regular basis, certain vitamin and mineral deficiencies can follow. Examples of these deficiencies are as follows:		These deficiencies can lead to weight issues caused by malnutrition. When consuming alcohol, these vitamins and minerals must be replaced. Often this is how certain cravings arise.[20]		According to the National Institute on Alcohol Abuse and Alcoholism's research, people who tend to drink the largest amount of alcohol have the poorest eating habits compared to those who do not consume much alcohol at all. Those who do not drink a large quantity of alcohol seem to have the best quality diets. In this study researchers compared the Healthy Eating scores of 3,000 participants in the National Health and Nutrition Examination Survey with their overall consumption of alcohol. They used frequency, quantity, and average daily volume to measure the alcohol consumption.		The researchers found that as the alcohol quantity increased, the Health Index scores declined. As the frequency of alcohol consumed increased, the Healthy Eating scored declined. Diet quality was the poorest among those who consumed the largest quantity of alcohol. Care packages filled with unhealthy treats, sent usually by parents, was found to be the leading cause of weight gain. Those who drank less alcohol in an infrequent time frame had the best health index scores overall.[21]		It is not unusual for college students, especially freshmen, to experience abnormal levels of stress. This is more prevalent for freshmen because they are still transitioning from high school. College students can hold jobs while taking classes and may feel they have no time for studying, while freshmen might be stressed just trying to adjust to the college work load. There are hundreds of reasons for why college students get stressed, but, whatever the reason, it also can lead to weight gain. This is because when the body is stressed, it releases hormones such as adrenaline or more importantly cortisol. Cortisol has been tested to slow down the body's metabolism. Other studies have shown that when people are stressed, they have cravings for foods that are high in calories such as sweet, salty, and processed foods. Not only do people crave bad food when they are nervous or stressed, but they eat large quantities of it through continuous snacking even though they might not be hungry.[22] Therefore, an increase in weight can be seen in freshmen students even though they are eating normally.		A study done by Jatturong R. Wichianson and colleagues at the University of Southern California showed a direct relationship between eating late at night (Night eating syndrome) and stress levels with college students. They used a standardized test to measure both the levels of NES and perceived stress each student had. The results showed that students that had higher levels of stress were more likely to have NES due to the inability to adapt. This study shows that students who were not able to deal with stress appropriately were more likely to use late night eating to solve their issues.[23]		Nicole L. Mihalopoulos and colleagues developed a study at a private university in the Northeastern United States. Their goal was to determine if college students did truly gain weight in their freshmen year. Test subjects were made up of male and female freshmen college students who lived on campus. They took an online survey to answer questions about their eating patterns, social behaviors, as well as weight. The purpose of this was to discover if the individuals showed signs of body image issues or eating disorders.		125 freshmen were eligible for testing and the average age was 18.4. The results showed that about half of the test subjects gained weight. The men gained an average 3.7 lbs and women gained an average 1.7 lbs their freshmen year. These results disproved their hypothesis that the women would have a larger weight gain than the men, but this stays consistent with other studies done on the hypothesis. Even though only 5% of the test subjects showed a weight gain of 15 lbs or greater the authors of this study concluded that the freshmen year in college has potential for weight gain and can even lead to obesity later on in life.[24]		
The Rouge Forum is an organization of educational activists, which focuses on issues of equality, democracy, and social justice.						The Rouge Forum emerged from a series of political controversies within the National Council for the Social Studies (NCSS) during the 1990s. In particular, two events at the 1994 annual meeting of NCSS in Phoenix galvanized a small group of activists who later founded the organization. First, a staff person from the Central Committee for Conscientious Objectors (CCCO) was arrested for leafleting at the NCSS conference; and secondly, the governing body of NCSS rejected a resolution condemning California Proposition 187 and calling for a boycott of California as a site for future meetings of the NCSS. These events fueled a level of political activism the NCSS had rarely experienced and identified the need for organized action in support of free speech and anti-racist pedagogy in the field of social studies education in general and within NCSS in particular.[1]		The Rouge Forum was formally organized and held its first meeting in Detroit at Wayne State University in 1998. Continued activism within NCSS remained a major topic of discussion at this meeting, however, the organization's concerns were broadened by the participation of teachers and teachers educators working in the areas of literacy and special education. Rouge Forum members have worked closely with, and played leadership roles within the inclusive school and whole language literacy movements. For several years in early 2000s, The Rouge Forum, The Whole Schooling Consortium, and the Whole Language Umbrella of the National Council of Teachers of English held joint meetings.		A key principle underlying the actions of the Rouge Forum is that schools and educators play a critical role in the creation of a more democratic, egalitarian society (or a society that increases in inequality and authoritarianism). The Rouge Forum is perhaps the only school-based group in North America that has connected imperialism, war, and the regulation of schooling.[2]		The Rouge Forum has been active in efforts to resist curriculum standardization and high-stakes testing in schools, particularly as a result of the No Child Left Behind Act. Rouge Forum members have also joined, and assumed leadership in, community coalitions organized against the wars in Iraq and Afghanistan, usually coalitions involving labor, leftists, grassroots collectives, and religious groups aimed at ending the war, and are frequently involved in school-based organizing, and counter-military recruitment as well.		The Rouge Forum has published a newspaper/zine since 1999. The Rouge Forum News appeared twice a year in both print and online editions from 1999-2004. In 2009, The Rouge Forum News returned to a regular publication schedule as a digital zine.		The Rouge Forum holds meetings on a regular basis at both local and national levels. The national conferences have been held on a more or less annual basis; all meetings are action-oriented and the national conferences usually include workshops for teachers and students; panel discussions; community-building and cultural events; as well as academic presentations. Many prominent voices for democracy and critical pedagogy have participated in Rouge Forum meetings.		Past Rouge Forum conferences:		Adam Renner Education for Social Justice Lecture		Every year the Rouge Forum honours the life and work of Adam Renner (August 18, 1970 – December 19, 2010) by inviting a critical scholar, educator, or activist to deliver the Adam Renner Education for Social Justice Lecture.		Renner was a teacher, scholar, musician, revolutionary activist, and martial artist. He received his BA in Mathematics from Thomas More College. While teaching mathematics at Seton High School in Cincinnati, OH, he completed his MEd at Northern Kentucky University. In 2002, Adam received his PhD in Cultural Studies at the University of Tennessee, Knoxville and subsequently worked as a professor at Bellarmine University in Louisville, KY.		His scholarship focused on service learning, social difference, social justice, and pedagogy and he published in numerous journals including Educational Studies, EcoJustice Review, High School Journal, Journal of Curriculum Theorizing, Rethinking Schools among others.		Along with his life partner, Gina Stiens, he created a service partnership with schools and social service organizations in Jamaica and taught an ongoing course, “Education for Liberation or Domination: A Critical Encounter in Jamaica.” He was a key leader and organizer in the Rouge Forum serving as Community Coordinator and as editor of the Rouge Forum News.		In the fall of 2010, Adam left his professorship at Bellarmine University and returned to the school classroom, as a math teacher at June Jordan School for Social Equity in San Francisco.		In an article for Substance News, published just weeks before he died, Renner wrote,		“For me and my K-12 classroom, for instance, I have been searching for the intersection of liberation, curriculum, and student experience (comprised of individual traumas, structural oppression, nine years of mis-schooling, varying levels of confidence and skill, etc.). How can I shape the revolutionary subjects necessary to help tip the inflexion point toward the necessary qualitative changes? When we teach math, social studies, language arts, and science, can we credibly do so in a way that is separate from the growing militarization of our schools and society, gang and drug infestations in our communities, rampant unemployment, a school to prisons pipeline, the assurance of our students’ ignorance through standardization and a teach-to-the-test mafia-like pressure on teachers? So, if we shouldn’t teach our classes that way, can we organize in such a way that militates against such explorations?[3]		Adam Renner Education for Social Justice Memorial Lecturers include:		Books		Journals		Blogs		Newspapers		
A zero-tolerance policy in schools is a strict enforcement of regulations and bans against undesirable behaviors or possession of items. Public criticism against such policies have arisen due to their enforcement and the resulting (sometimes devastating) consequences when the behavior or possession was done in ignorance, by accident, or under extenuating circumstances. For example, a ban against guns resulting in a Rhode Island boy with a gun charm on a key chain being suspended. In schools, common zero-tolerance policies concern possession or use of illicit drugs or weapons. Students, and sometimes staff, parents, and other visitors, who possess a banned item for any reason are always (if the policy is followed) to be punished.		In the United States and Canada, zero-tolerance policies have been adopted in various schools and other education venues. Zero-tolerance policies in the United States became widespread in 1994, after federal legislation required states to expel any student who brought a firearm to school for one year, or lose all federal funding.[1]		These policies are promoted as preventing drug abuse and violence in schools. Critics say zero tolerance in schools have resulted in punishments which have been criticised as egregiously unfair against students and teachers, especially in schools with poorly written policies. Consequently, critics describe these policies as zero-logic policies because they treat juveniles the way that adults would be treated[2] — or more harshly, given that children are seldom granted full permission to speak up in their own defence to adults with authority over them. Many people have been critical of zero tolerance policies, claiming that they are overly draconian, provide little if any benefit to anyone, contribute to overcrowding of the criminal justice system, and/or disproportionately target blacks and Latinos.[3]						There is no credible evidence that zero tolerance reduces violence or drug abuse by students.[4] Furthermore, school suspension and expulsion result in a number of negative outcomes for both schools and students.[4]		The American Bar Association has found that the evidence indicates that minority children are the most likely to suffer the negative consequences of zero tolerance policies. Analysis of the suspension rate of students show that black females and other racial minorities are suspended at a greater rate.[5]		The American Psychological Association concluded that the available evidence does not support the use of zero tolerance policies as defined and implemented, that there is a clear need to modify such policies, and that the policies create a number of unintended negative consequences,[6][7] including making schools "less safe".[8]		In 2014, a study of school discipline figures was conducted. It was found that suspensions and expulsions as a result of zero tolerance policies have not reduced school disruptions. The study's author stated that "zero tolerance approaches to school discipline are not the best way to create a safe climate for learning".[9]		Another study says that zero tolerance policies are viewed as a quick fix solution for student problems.[10][11][12] While this seems like a simple action-reaction type of situation, it often leaves out the mitigating circumstances that are often the important details in student incidents. Even civilian judges consider mitigating circumstances before passing judgement or sentencing. If zero tolerance policies were applied in adult courtroom scenarios, they would be fundamentally unjust and unconstitutional due to neglecting the laws involving due process, along with cruel and unusual punishments. According to the U.S. Department Of Education, about 1 in 5 middle school and high school students will be suspended.[13]		The label of zero tolerance began with the Gun-Free Schools Act of 1994, when Congress authorized public-school funding subject to the adoption of zero-tolerance policies.[14] Similar policies of intolerance coupled with expulsions for less serious behaviors than bringing a weapon to school had long been a part of private, and particularly religious, schools. The use of zero-tolerance policies in secular, public schools increased dramatically after the Columbine High School massacre, with principals declaring that safety concerns made them want zero tolerance for weapons. These have led to a large number of disproportionate responses to minor, or technical transgressions, many of which have attracted the attention of the international media. These cases include students being suspended or expelled for such offenses as possession of ibuprofen or Midol (both legal, non-prescription drugs commonly used to treat menstrual cramps and headaches) with permission of the students' parents, keeping pocketknives (small utility knife) in cars, and carrying sharp tools outside of a woodshop classroom (where they are often required materials). In Seal v. Morgan, a student was expelled for having a knife in his car on school property, despite his protestations that he was unaware of the knife's presence.[15] In some jurisdictions, zero-tolerance policies have come into conflict with freedom of religion rules already in place allowing students to carry, for example, kirpans.		In the "kids for cash" scandal, judge Mark Ciavarella, who promoted a platform of zero tolerance, received kickbacks for constructing a private prison that housed juvenile offenders, and then proceeded to fill the prison by sentencing children to extended stays in juvenile detention for offenses as minimal as mocking a principal on Myspace, scuffles in hallways, trespassing in a vacant building, and shoplifting DVDs from Wal-Mart. Critics of zero-tolerance policies argue that harsh punishments for minor offenses are normalized. The documentary Kids for Cash interviews experts on adolescent behavior, who argue that the zero tolerance model has become a dominant approach to policing juvenile offenses after the Columbine shooting.[16][17]		In New York City, Carmen Fariña, head of the New York City Department of Education, restricted school suspension by principals in 2015.[18] The Los Angeles Unified school board, responsible for educating 700,000 students, voted in 2013 to ban suspensions for "willful defiance", which had mostly been used against students from racial minorities.[19][20] A year later, the same school district decided to decriminalise school discipline so that minor offences would be referred to school staff rather than prosecuted -- the previous approach had resulted in black students being six times more likely to be arrested or given a ticket than white students.[21] The district saw suspensions drop by 53%, and graduation rates rise by 12%.[20]		Media attention has proven embarrassing to school officials, and the embarrassment has resulted in changes to state laws as well as to local school policies. One school board member gave this reason for changes his district made to their rigid policy: "We are doing this because we got egg on our face."[22]		Proponents of punishment- and exclusion-based philosophy of school discipline policies claim that such policies are required to create an appropriate environment for learning.[9][34] This rests on the assumption that strong enforcement can act as a psychological deterrent to other potentially disruptive students.[9]		The policy assumption is that inflexibility is a deterrent because, no matter how or why the rule was broken, the fact that the rule was broken is the basis for the imposition of the penalty. This is intended as a behavior modification strategy: since those at risk know that it may operate unfairly, they may be induced to take even unreasonable steps to avoid breaking the rule. This is a standard policy in rule- and law-based systems around the world on "offenses" as minor as traffic violations to major health and safety legislation for the protection of employees and the environment.[35]		Disciplinarian parents view zero-tolerance policies as a tool to fight corruption.[36] Under this argument, if subjective judgment is not allowed, most attempts by the authority person to encourage bribes or other favors in exchange for leniency are clearly visible.		Critics of zero-tolerance policies in schools say they are part of a school-to-prison pipeline [37] that over-polices children with behavioural problems, treating their problems as criminal justice issues rather than educational and behavioural problems. Students that may previously have been given short school suspensions before the implementation of policies are now sent to juvenile courts.[38]		Critics of zero-tolerance policies frequently refer to cases where minor offenses have resulted in severe punishments. Typical examples include the honor-roll student being expelled from school under a "no weapons" policy while in possession of nail clippers,[39] or for possessing "drugs" like cough drops and dental mouthwash or "weapons" like rubber bands.[1]		A related criticism is that zero-tolerance policies make schools feel like a jail or a prison. Furthermore zero-tolerance policies have been struck down by U.S. courts[40] and by departments of education.[41]		Another criticism is that the zero-tolerance policies have actually caused schools to turn a blind eye to bullying, resulting in them refusing to solve individual cases in an attempt to make their image look better. The zero-tolerance policy also punishes both the attacker and the defender in a fight, even when the attacker was the one who started the fight unprovoked.		A particularly dismaying hypothesis about zero tolerance policies is that they may actually discourage some people from reporting criminal and illegal behavior, for fear of losing relationships, and for many other reasons. That is, ironically, zero tolerance policies may be ineffective in the very purpose for which they were originally designed.[42]		As schools develop responses to online bullying, schools that have overly harsh approaches to zero tolerance policies may increasingly police speech of students in their own time, that would normally be protected by free speech laws.[43]		The American Bar Association opposes "zero tolerance policies that mandate either expulsion or referral of students to juvenile or criminal court, without regard to the circumstances or nature of the offense or the students [sic] history."[44]		Critics of zero tolerance policies also argue that the large numbers of students who are suspended and expelled from school experience negative effects which can prohibit them from finishing high school. Students who experience suspension, expulsion and arrests pay higher psychological and social costs: such as depression, suicidal thoughts, academic failure, and run the risk of being incarcerated as adults. In a study by Forrest et. al., (2000), psychologists identified that a third of youths in juvenile detention centers were diagnosed with depression shortly after being incarcerated.[45] In addition to being diagnosed with depression many youths found themselves having suicidal thoughts (Gnau et. al. 1997).[46]		Research found that Black, Latino, and White adults with low educational attainment risked a higher propensity of being incarcerated in their lifetime (Pettit & Western 2010).[47] In the same study they found that the incarceration rate in 2008 rose to 37% since the 1980’s. This showed that incarceration rates of people with low levels of education were continuing to rise and that students were not completing their high school requirements.		According to kidsdata.org for the year 2012, 21,638 students were suspended and 592 students were expelled from San Diego County schools. A total of 10.1% of students did not complete their high school diploma.[48]		Despite a decrease in juvenile arrest, suspensions, expulsions, and drop out rates, many still argue that these disciplinary policies have helped contribute to students not completing their high school curriculum. Schools are struggling to keep students within the walls of the educational system rather than the walls of a juvenile detention center.		The Escondido school knives case is a collection of school suspensions, possible expulsions, and protests surrounding knives that were found in the cars of two students at San Pasqual High School in Escondido, California.		Brandon Cappaletti, an 18-year-old student at the school, had been on a fishing trip in early January, 2016. He says he had used the knives to cut line and would have used them to prepare fish. After the fishing trip, he left the knives in his truck. Another 16-year-old student had a pocket knife in his glove compartment.[49]		On January 27, police dogs were sniffing vehicles in the school's parking lot. Each student had Advil in the truck. In each case, the dogs alerted on the Advil. Cappaletti was called out of class to unlock the truck. The police found the Advil and the knives. In the case of the 16-year-old, the alert also which led to the police finding the pocket knife.[49]		Both boys were arrested. Cappaletti was later released to his mother. The 16-year-old's case was referred to the California's juvenile diversion program. Both boys have been charged with a misdemeanor for having a knife on school property with a blade longer than two-and-a-half inches. Both were also immediately suspended from classes.[49] A hearing on Feb. 25 will decide whether or not they will be expelled.		Cappaletti has already joined the U.S. Marines Corps. If convicted of the misdemeanor, he might not be able to remain. His mother Amy Cappelletti said that “He’s the most patriotic student. He never gets into trouble. These weren’t Crocodile Dundee knives.”[50] At a School Board meeting on February 9, 2016, hundreds of people asked the Board not to expel the two students. Tony Coreley, the high school's football coach said that “There are rules and laws that the district has to follow, but this (situation) is unfortunate.”[50]		School District spokeswoman Karyl O’Brien said that the situation was unfortunate, but under California's zero-tolerance policy, there were rules the school had to follow.		On February 11, 2016, the Superintendent of the Escondido Union School District stated that the students would not be expelled, and that they were expected to be allowed to return to school the following week.[51] As of February 13, the misdemeanor charges remain unresolved.[52]		Laval University Press (French translation), 2004: 155-84.		
Linear algebra is the branch of mathematics concerning vector spaces and linear mappings between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.		The set of points with coordinates that satisfy a linear equation forms a hyperplane in an n-dimensional space. The conditions under which a set of n hyperplanes intersect in a single point is an important focus of study in linear algebra. Such an investigation is initially motivated by a system of linear equations containing several unknowns. Such equations are naturally represented using the formalism of matrices and vectors.[1][2][3]		Linear algebra is central to both pure and applied mathematics. For instance, abstract algebra arises by relaxing the axioms of a vector space, leading to a number of generalizations. Functional analysis studies the infinite-dimensional version of the theory of vector spaces. Combined with calculus, linear algebra facilitates the solution of linear systems of differential equations.		Techniques from linear algebra are also used in analytic geometry, engineering, physics, natural sciences, computer science, computer animation, advanced facial recognition algorithms and the social sciences (particularly in economics). Because linear algebra is such a well-developed theory, nonlinear mathematical models are sometimes approximated by linear models.						The study of linear algebra first emerged from the study of determinants, which were used to solve systems of linear equations. Determinants were used by Leibniz in 1693, and subsequently, Gabriel Cramer devised Cramer's Rule for solving linear systems in 1750. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.[4]		The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for "womb". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".[4]		In 1882, Hüseyin Tevfik Pasha wrote the book titled "Linear Algebra".[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.[4]		The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.		Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s.[7] Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do "matrix algebra, formerly reserved for college" in the 1960s.[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum.[9] In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based "matrix orientation" as opposed to a theoretical orientation.[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular value decomposition (SVD), as 'so many other disciplines use it'.[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination.[13][14]		The main structures of linear algebra are vector spaces. A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms. Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The operations of addition and multiplication in a vector space must satisfy the following axioms.[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.		The first four axioms are those of V being an abelian group under vector addition. Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.		Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map		that is compatible with addition and scalar multiplication:		for any vectors u,v ∈ V and a scalar a ∈ F.		Additionally for any vectors u, v ∈ V and scalars a, b ∈ F:		When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.		Linear transformations have geometric significance. For example, 2 × 2 real matrices denote standard planar mappings that preserve the origin.		Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ..., vk:		where a1, a2, ..., ak are scalars. The set of all linear combinations of vectors v1, v2, ..., vk is called their span, which forms a subspace.		A linear combination of any system of vectors with all zero coefficients is the zero vector of V. If this is the only way to express the zero vector as a linear combination of v1, v2, ..., vk then these vectors are linearly independent. Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V. Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis.[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.		Any two bases of a vector space V have the same cardinality, which is called the dimension of V. The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of V has finite number of elements, V is called a finite-dimensional vector space. If V is finite-dimensional and U is a subspace of V, then dim U ≤ dim V. If U1 and U2 are subspaces of V, then		One often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism.		A particular basis {v1, v2, ..., vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ..., an) is the linear combination		The condition that v1, v2, ..., vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ..., vn assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to v). In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn. Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn. Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V → W may be encoded by an m × n matrix A with entries in the field F, called the matrix of T with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.		There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V. While Rn has a standard basis {e1, e2, ..., en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V).		One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20]). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.		In general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation T is to find "characteristic lines" that are invariant sets under T. If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector. The scalar λ such that Tv = λv is called a characteristic value or eigenvalue of T.		To find an eigenvector or an eigenvalue, we note that		where I is the identity matrix. For there to be nontrivial solutions to that equation, det(T − λ I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ..., vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues λ1, λ2, ..., λn, and if v = a1v1 + ... + an vn, then,		Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).		Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a map		that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]		Note that in R, it is symmetric.		We can define the length of a vector v in V by		and we can prove the Cauchy–Schwarz inequality:		In particular, the quantity		and so we can call this quantity the cosine of the angle between the two vectors.		Two vectors are orthogonal if ⟨ u , v ⟩ = 0 {\displaystyle \langle u,v\rangle =0} . An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly nice to deal with, since if v = a1 v1 + ... + an vn, then a i = ⟨ v , v i ⟩ {\displaystyle a_{i}=\langle v,v_{i}\rangle } .		The inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying		If T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.		Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.		Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:		The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.		In the example, x is eliminated from L2 by adding (3/2)L1 to L2. x is then eliminated from L3 by adding L1 to L3. Formally:		The result is:		Now y is eliminated from L3 by adding −4L2 to L3:		The result is:		This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.		The last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen that		Then, z can be substituted into L2, which can then be solved to obtain		Next, z and y can be substituted into L1, which can be solved to obtain		The system is solved.		We can, in general, write any system of linear equations as a matrix equation:		The solution of this system is characterized as follows: first, we find a particular solution x0 of this equation using Gaussian elimination. Then, we compute the solutions of Ax = 0; that is, we find the null space N of A. The solution set of this equation is given by x 0 + N = { x 0 + n : n ∈ N } {\displaystyle x_{0}+N=\{x_{0}+n:n\in N\}} . If the number of variables is equal to the number of equations, then we can characterize when the system has a unique solution: since N is trivial if and only if det A ≠ 0, the equation has a unique solution if and only if det A ≠ 0.[23]		The least squares method is used to determine the best fit line for a set of data.[24] This line will minimize the sum of the squares of the residuals.		Fourier series are a representation of a function f: [−π, π] → R as a trigonometric series:		This series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.		The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the "same" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product		The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n ≥ 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient ak, we take the inner product with hk:		and by orthonormality, ⟨ f , h k ⟩ = a k {\displaystyle \langle f,h_{k}\rangle =a_{k}} ; that is,		Quantum mechanics is highly inspired by notions in linear algebra. In quantum mechanics, the physical state of a particle is represented by a vector, and observables (such as momentum, energy, and angular momentum) are represented by linear operators on the underlying vector space. More concretely, the wave function of a particle describes its physical state and lies in the vector space L2 (the functions φ: R3 → C such that ∫ − ∞ ∞ ∫ − ∞ ∞ ∫ − ∞ ∞ | ϕ | 2 d x d y d z {\displaystyle \int _{-\infty }^{\infty }\int _{-\infty }^{\infty }\int _{-\infty }^{\infty }|\phi |^{2}dxdydz} is finite), and it evolves according to the Schrödinger equation. Energy is represented as the operator H = − ℏ 2 2 m ∇ 2 + V ( x , y , z ) {\displaystyle H=-{\frac {\hbar ^{2}}{2m}}\nabla ^{2}+V(x,y,z)} , where V is the potential energy. H is also known as the Hamiltonian operator. The eigenvalues of H represents the possible energies that can be observed. Given a particle in some state φ, we can expand φ into a linear combination of eigenstates of H. The component of H in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).		Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two dimensional plane E. When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.		Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]		where a, b and c are not all zero. Then,		or		where x = (x, y, 1) is the 3 × 1 set of homogeneous coordinates associated with the point (x, y).[26]		Homogeneous coordinates identify the plane E with the z = 1 plane in three dimensional space. The x−y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1).		The linear equation, λ, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point αx1 + βx2 is also on the line, for any real α and β.		Now consider the equations of the two lines λ1 and λ2,		which forms a system of linear equations. The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,		or using homogeneous coordinates,		The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates, the solutions are multiples of the following solution:[26]		if the rows of B are linearly independent (i.e., λ1 and λ2 represent distinct lines). Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns.[27] Notice that this yields a point in the z = 1 plane only when the 2 × 2 submatrix associated with x3 has a non-zero determinant.		It is interesting to consider the case of three lines, λ1, λ2 and λ3, which yield the matrix equation,		which in homogeneous form yields,		Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E. For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.		Another way to approach linear algebra is to consider linear functions on the two dimensional real plane E=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in E and consider the linear function λ: E→R, given by		or		This transformation has the important property that if Ay=d, then		This shows that the sum of vectors in E map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation.[25] For this case, where the image space is a real number the map is called a linear functional.[27]		Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj. It is now possible to see that		Thus, the columns of the matrix A are the image of the basis vectors of E in R.		This is true for any pair of vectors used to define coordinates in E. Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E. This means a vector x has coordinates (α,β), such that x=αv+βw. Then, we have the linear functional		where Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form as		This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E. Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is to find the real numbers α, β, so that x=αv+βw, that is		To solve this equation for α, β, we compute the linear coordinate functionals σ and τ for the basis v, w, which are given by,[26]		The functionals σ and τ compute the components of x along the basis vectors v and w, respectively, that is,		which can be written in matrix form as		These coordinate functionals have the properties,		These equations can be assembled into the single matrix equation,		Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.[25][27]		The set of points in the plane E that map to the same image in R under the linear functional λ define a line in E. This line is the image of the inverse map, λ−1: R→E. This inverse image is the set of the points x=(x, y) that solve the equation,		Notice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c.		In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation		Solve for y and obtain the inverse image as the set of points,		For convenience the free parameter x has been relabeled t.		The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,		Notice that if h is a solution to this homogeneous equation, then t h is also a solution.		The set of points of a linear functional that map to zero define the kernel of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.[25][27]		Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.		In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V∗ consisting of linear maps f: V → F where F is the field of scalars. Multilinear maps T: Vn → F can be described via tensor products of elements of V∗.		If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V × V → V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).		Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.		Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.		Algebraic geometry considers the solutions of systems of polynomial equations.		There are several related topics in the field of computer programming that utilizes much of the techniques and theorems linear algebra encompasses and refers to.		
Socialism is a range of economic and social systems characterised by social ownership and democratic control of the means of production,[10] as well as the political theories, and movements associated with them.[11] Social ownership may refer to forms of public, collective, or cooperative ownership, or to citizen ownership of equity.[12] There are many varieties of socialism and there is no single definition encapsulating all of them.[13] Social ownership is the common element shared by its various forms.[5][14][15]		Socialist economic systems can be divided into non-market and market forms.[16] Non-market socialism involves the substitution of factor markets and money, with engineering and technical criteria, based on calculation performed in-kind, thereby producing an economic mechanism that functions according to different economic laws from those of capitalism. Non-market socialism aims to circumvent the inefficiencies and crises traditionally associated with capital accumulation and the profit system.[25] By contrast, market socialism retains the use of monetary prices, factor markets, and, in some cases, the profit motive, with respect to the operation of socially owned enterprises and the allocation of capital goods between them. Profits generated by these firms would be controlled directly by the workforce of each firm, or accrue to society at large in the form of a social dividend.[26][27][28] The socialist calculation debate discusses the feasibility and methods of resource allocation for a socialist system.		The socialist political movement includes a set of political philosophies that originated in the revolutionary movements of the mid-to-late 1700s, and of concern for the social problems that were associated with capitalism.[13] In addition to the debate over markets and planning, the varieties of socialism differ in their form of social ownership, how management is to be organised within productive institutions, and the role of the state in constructing socialism.[2][13] Core dichotomies include reformism versus revolutionary socialism, and state socialism versus libertarian socialism. Socialist politics has been both centralist and decentralised; internationalist and nationalist in orientation; organised through political parties and opposed to party politics; at times overlapping with trade unions and at other times independent of, and critical of, unions; and present in both industrialised and developing countries.[29] While all tendencies of socialism consider themselves democratic, the term "democratic socialism" is often used to highlight its advocates' high value for democratic processes in the economy and democratic political systems,[30] usually to draw contrast to tendencies they may perceive to be undemocratic in their approach. "Democratic socialism" is frequently used to draw contrast to the political system of the Soviet Union, which critics argue operated in an authoritarian fashion.[31][32][33]		By the late 19th century, after the work of Karl Marx and his collaborator Friedrich Engels, as technological development outstripped the economic dynamics of capitalism,[34] "socialism" had come to signify opposition to capitalism, and advocacy for a post-capitalist system based on some form of social ownership of the means of production.[35][36] By the 1920s, social democracy and communism had become the two dominant political tendencies within the international socialist movement.[37] By this time, Socialism emerged as "the most influential secular movement of the twentieth century, worldwide. It is a political ideology (or world view), a wide and divided political movement"[38] and while the emergence of the Soviet Union as the world's first nominally socialist state led to socialism's widespread association with the Soviet economic model, many economists and intellectuals argued that in practice the model functioned as a form of state capitalism,[39][40][41] or a non-planned administrative or command economy.[42][43] Socialist parties and ideas remain a political force with varying degrees of power and influence in all continents, heading national governments in many countries around the world. Today, some socialists have also adopted the causes of other social movements, such as environmentalism, feminism and liberalism.[44]						The origin of the term socialism may be traced back and attributed to a number of originators, in addition to significant historical shifts in the usage and scope of the word.		For Andrew Vincent, "The word ‘socialism’ finds its root in the Latin sociare, which means to combine or to share. The related, more technical term in Roman and then medieval law was societas. This latter word could mean companionship and fellowship as well as the more legalistic idea of a consensual contract between freemen."[45]		The term "socialism" was created by Henri de Saint-Simon, one of the founders of what would later be labelled "utopian socialism". Simon coined "socialism" as a contrast to the liberal doctrine of "individualism", which stressed that people act or should act as if they are in isolation from one another.[46] The original "utopian" socialists condemned liberal individualism for failing to address social concerns during the industrial revolution, including poverty, social oppression, and gross inequalities in wealth; viewing liberal individualism as degenerating society into supporting selfish egoism that harmed community life through promoting a society based on competition.[46] They presented socialism as an alternative to liberal individualism based on the shared ownership of resources, although their proposals for socialism differed significantly. Saint-Simon proposed economic planning, scientific administration, and the application of modern scientific advancements to the organization of society; by contrast, Robert Owen proposed the organization of production and ownership in cooperatives.[46][47]		The term socialism is attributed to Pierre Leroux,[48] and to Marie Roch Louis Reybaud in France; and in Britain to Robert Owen in 1827, father of the cooperative movement.[49][50]		The modern definition and usage of "socialism" settled by the 1860s, becoming the predominant term among the group of words "co-operative", "mutualist" and "associationist", which had previously been used as synonyms. The term "communism" also fell out of use during this period, despite earlier distinctions between socialism and communism from the 1840s.[51] An early distinction between socialism and communism was that the former aimed to only socialise production while the latter aimed to socialise both production and consumption (in the form of free access to final goods).[52] By 1888, however, Marxists employed the term "socialism" in place of "communism", which had come to be considered an old-fashion synonym for socialism. It wasn't until 1917 after the Bolshevik revolution that "socialism" came to refer to a distinct stage between capitalism and communism, introduced by Vladimir Lenin as a means to defend the Bolshevik seizure of power against traditional Marxist criticisms that Russia's productive forces were not sufficiently developed for socialist revolution.[53]		A distinction between "communist" and "socialist" as descriptors of political ideologies arose in 1918 after the Russian Social-Democratic Labour Party renamed itself to the All-Russian Communist Party, where "communist" came to specifically mean socialists who supported the politics and theories of Leninism, Bolshevism and later Marxism-Leninism;[54] although communist parties continued to describe themselves as socialists dedicated to socialism.[55]		The words socialism and communism eventually accorded with the adherents' and opponents' cultural attitude towards religion. In Christian Europe, communism was believed to be the atheist way of life. In Protestant England, the word communism was too culturally and aurally close to the Roman Catholic communion rite; hence, English atheists denoted themselves socialists.[56] Friedrich Engels argued that in 1848, at the time when the Communist Manifesto was published, "socialism was respectable on the continent, while communism was not." The Owenites in England and the Fourierists in France were considered "respectable" socialists, while working-class movements that "proclaimed the necessity of total social change" denoted themselves communists. This latter branch of socialism produced the communist work of Étienne Cabet in France and Wilhelm Weitling in Germany.[57] The British moral philosopher John Stuart Mill also came to advocate a form of economic socialism within a liberal context. In later editions of his Principles of Political Economy (1848), Mill would argue that "as far as economic theory was concerned, there is nothing in principle in economic theory that precludes an economic order based on socialist policies."[58][59] While democrats looked to the Revolutions of 1848 as a democratic revolution, which in the long run ensured liberty, equality, and fraternity, Marxists denounced 1848 as a betrayal of working-class ideals by a bourgeoisie indifferent to the legitimate demands of the proletariat.[60]		Socialist models and ideas espousing common or public ownership have existed since antiquity. It has been claimed, though controversially, that there were elements of socialist thought in the politics of classical Greek philosophers Plato[61] and Aristotle.[62] Mazdak, a Persian communal proto-socialist,[63] instituted communal possessions and advocated the public good. Abū Dharr al-Ghifārī, a Companion of Prophet Muhammad, is credited by many as a principal antecedent of Islamic socialism.[64][65][66][67][68] In the period right after the French Revolution, activists and theorists like François-Noël Babeuf, Étienne-Gabriel Morelly, Philippe Buonarroti, and Auguste Blanqui influenced the early French labour and socialist movements.[69] In Britain, Thomas Paine proposed a detailed plan to tax property owners to pay for the needs of the poor in Agrarian Justice[70] while Charles Hall wrote The Effects of Civilization on the People in European States, denouncing capitalism's effects on the poor of his time[71] which influenced the utopian schemes of Thomas Spence.[72]		The first "self-conscious socialist movements developed in the 1820s and 1830s. The Owenites, Saint-Simonians and Fourierists provided a series of coherent analyses and interpretations of society. They also, especially in the case of the Owenites, overlapped with a number of other working-class movements like the Chartists in the United Kingdom."[73] The Chartists gathered significant numbers around the People's Charter of 1838, which demanded the extension of suffrage to all male adults. Leaders in the movement also called for a more equitable distribution of income and better living conditions for the working classes. "The very first trade unions and consumers’ cooperative societies also emerged in the hinterland of the Chartist movement, as a way of bolstering the fight for these demands."[74] A later important socialist thinker in France was Pierre-Joseph Proudhon who proposed his philosophy of mutualism in which "everyone had an equal claim, either alone or as part of a small cooperative, to possess and use land and other resources as needed to make a living".[75] There were also currents inspired by dissident Christianity of Christian socialism "often in Britain and then usually coming out of left liberal politics and a romantic anti-industrialism"[69] which produced theorists such as Edward Bellamy, Frederick Denison Maurice, and Charles Kingsley.[76]		The first advocates of socialism favoured social levelling in order to create a meritocratic or technocratic society based on individual talent. Count Henri de Saint-Simon is regarded as the first individual to coin the term socialism.[77] Saint-Simon was fascinated by the enormous potential of science and technology and advocated a socialist society that would eliminate the disorderly aspects of capitalism and would be based on equal opportunities.[78][unreliable source?] He advocated the creation of a society in which each person was ranked according to his or her capacities and rewarded according to his or her work.[77] The key focus of Saint-Simon's socialism was on administrative efficiency and industrialism, and a belief that science was the key to progress.[79] This was accompanied by a desire to implement a rationally organised economy based on planning and geared towards large-scale scientific and material progress,[77] and thus embodied a desire for a more directed or planned economy. Other early socialist thinkers, such as Thomas Hodgkin and Charles Hall, based their ideas on David Ricardo's economic theories. They reasoned that the equilibrium value of commodities approximated prices charged by the producer when those commodities were in elastic supply, and that these producer prices corresponded to the embodied labour – the cost of the labour (essentially the wages paid) that was required to produce the commodities. The Ricardian socialists viewed profit, interest and rent as deductions from this exchange-value.[citation needed]		West European social critics, including Robert Owen, Charles Fourier, Pierre-Joseph Proudhon, Louis Blanc, Charles Hall, and Saint-Simon, were the first modern socialists who criticised the excessive poverty and inequality of the Industrial Revolution. They advocated reform, with some such as Robert Owen advocating the transformation of society to small communities without private property. Robert Owen's contribution to modern socialism was his understanding that actions and characteristics of individuals were largely determined by the social environment they were raised in and exposed to.[79] On the other hand, Charles Fourier advocated phalansteres which were communities that respected individual desires (including sexual preferences), affinities and creativity and saw that work has to be made enjoyable for people.[80] The ideas of Owen and Fourier were tried in practice in numerous intentional communities around Europe and the American continent in the mid-19th century.		The Paris Commune was a government that briefly ruled Paris from 18 March (more formally, from 28 March) to 28 May 1871. The Commune was the result of an uprising in Paris after France was defeated in the Franco-Prussian War. The Commune elections held on 26 March elected a Commune council of 92 members, one member for each 20,000 residents.[81] Despite internal differences, the Council began to organise the public services essential for a city of two million residents. It also reached a consensus on certain policies that tended towards a progressive, secular, and highly-democratic social democracy.		Because the Commune was only able to meet on fewer than 60 days in all, only a few decrees were actually implemented. These included the separation of church and state, the remission of rents owed for the entire period of the siege (during which, payment had been suspended), the abolition of night work in the hundreds of Paris bakeries, the granting of pensions to the unmarried companions and children of National Guards killed on active service; the free return, by the city pawnshops, of all workmen's tools and household items valued up to 20 francs, pledged during the siege.[82] The Commune was concerned that skilled workers had been forced to pawn their tools during the war; the postponement of commercial debt obligations, and the abolition of interest on the debts; and the right of employees to take over and run an enterprise if it were deserted by its owner; the Commune, nonetheless, recognised the previous owner's right to compensation.[82]		The International Workingmen's Association (IWA), also known as the First International, was founded in London in 1864. The International Workingmen's Association united diverse revolutionary currents including French followers of Proudhon,[83] Blanquists, Philadelphes, English trade unionists, socialists and social democrats. The IWA held a preliminary conference in 1865 and had its first congress at Geneva in 1866. Due to the wide variety of philosophies present in the First International, there was conflict from the start. The first objections to Marx came from the mutualists who opposed communism and statism. However, shortly after Mikhail Bakunin and his followers (called collectivists while in the International) joined in 1868, the First International became polarised into two camps headed by Marx and Bakunin respectively.[84] The clearest differences between the groups emerged over their proposed strategies for achieving their visions of socialism. The First International became the first major international forum for the promulgation of socialist ideas.		The followers of Bakunin were called collectivist anarchists and sought to collectivise ownership of the means of production while retaining payment proportional to the amount and kind of labor of each individual. Like Proudhonists, they asserted the right of each individual to the product of his labor and to be remunerated for their particular contribution to production. By contrast, anarcho-communists sought collective ownership of both the means and the products of labor. Errico Malatesta put it: "...instead of running the risk of making a confusion in trying to distinguish what you and I each do, let us all work and put everything in common. In this way each will give to society all that his strength permits until enough is produced for every one; and each will take all that he needs, limiting his needs only in those things of which there is not yet plenty for every one."[85] Anarchist communism as a coherent, modern economic-political philosophy was first formulated in the Italian section of the First International by Carlo Cafiero, Emilio Covelli, Errico Malatesta, Andrea Costa and other ex-Mazzinian republicans.[86] Out of respect for Mikhail Bakunin, they did not make their differences with collectivist anarchism explicit until after Bakunin's death.[87]		Syndicalism emerged in France inspired in part by the ideas of Pierre-Joseph Proudhon and later by Fernand Pelloutier and Georges Sorel.[88] It developed at the end of the 19th century "out of the French trade-union movement – syndicat is the French word for trade union. It was a significant force in Italy and Spain in the early 20th century until it was crushed by the fascist regimes in those countries. In the United States, syndicalism appeared in the guise of the Industrial Workers of the World, or "Wobblies," founded in 1905."[88] Syndicalism is an economic system where industries are organised into confederations (syndicates);[89] the economy is managed by negotiation between specialists and worker representatives of each field, comprising multiple non-competitive categorised units.[90] Thus, syndicalism is a form of communism and economic corporatism, and also refers to the political movement and tactics used to bring about this type of system. An influential anarchist movement based on syndicalist ideas is anarcho-syndicalism.[91] The International Workers Association is an international anarcho-syndicalist federation of various labor unions from different countries.		The Fabian Society' is a British socialist organisation which was established with the purpose of advancing the principles of socialism via gradualist and reformist means.[92] The society laid many of the foundations of the Labour Party and subsequently affected the policies of states emerging from the decolonisation of the British Empire, most notably India and Singapore. Originally, the Fabian Society was committed to the establishment of a socialist economy, alongside a commitment to British imperialism as a progressive and modernising force.[93] Today, the society functions primarily as a think tank and is one of 15 socialist societies affiliated with the Labour Party. Similar societies exist in Australia (the Australian Fabian Society), in Canada (the Douglas-Coldwell Foundation and the now disbanded League for Social Reconstruction) and in New Zealand.		Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public".[94] It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century. Inspired by medieval guilds, theorists such as Samuel G. Hobson and G.D.H. Cole advocated the public ownership of industries and their organisation into guilds, each of which would be under the democratic control of its trade union. Guild socialists were less inclined than Fabians to invest power in a state.[88] At some point "like the American Knights of Labor, guild socialism wanted to abolish the wage system".		As the ideas of Marx and Engels took on flesh, particularly in central Europe, socialists sought to unite in an international organisation. In 1889, on the centennial of the French Revolution of 1789, the Second International was founded, with 384 delegates from 20 countries representing about 300 labour and socialist organisations.[95] It was termed the "Socialist International" and Engels was elected honorary president at the third congress in 1893. Anarchists were ejected and not allowed in, mainly due to pressure from Marxists.[96] It has been argued that, at some point, the Second International turned "into a battleground over the issue of libertarian versus authoritarian socialism. Not only did they effectively present themselves as champions of minority rights; they also provoked the German Marxists into demonstrating a dictatorial intolerance which was a factor in preventing the British labor movement from following the Marxist direction indicated by such leaders as H. M. Hyndman".[96]		Reformism arose as an alternative to revolution. Eduard Bernstein was a leading social democrat in Germany who proposed the concept of evolutionary socialism. Revolutionary socialists quickly targeted reformism: Rosa Luxemburg condemned Bernstein's Evolutionary Socialism in her 1900 essay Reform or Revolution?. Revolutionary socialism encompasses multiple social and political movements that may define "revolution" differently from one another. The Social Democratic Party (SPD) in Germany became the largest and most powerful socialist party in Europe, despite working illegally until the anti-socialist laws were dropped in 1890. In the 1893 elections, it gained 1,787,000 votes, a quarter of the total votes cast, according to Engels. In 1895, the year of his death, Engels emphasised the Communist Manifesto's emphasis on winning, as a first step, the "battle of democracy".[97]		In 1904, Australians elected the first Australian Labor Party prime minister: Chris Watson, who became the first democratically elected social democrat. In 1909 the first Kibbutz was established in Palestine[98] by Russian Jewish Immigrants. The Kibbutz Movement would then expand through the 20th century following a doctrine of Zionist socialism.[99] The British Labour Party first won seats in the House of Commons in 1902. The International Socialist Commission (ISC, also known as Berne International) was formed in February 1919 at a meeting in Bern by parties that wanted to resurrect the Second International.[100]		By 1917, the patriotism of World War I changed into political radicalism in most of Europe, the United States, and Australia. Other socialist parties from around the world who were beginning to gain importance in their national politics in the early 20th century included the Italian Socialist Party, the French Section of the Workers' International, the Spanish Socialist Workers' Party, the Swedish Social Democratic Party, the Russian Social Democratic Labour Party, the Socialist Party of America in the United States, the Argentinian Socialist Party and the Chilean Partido Obrero Socialista.		In February 1917, revolution exploded in Russia. Workers, soldiers and peasants established soviets (councils), the monarchy fell, and a provisional government convoked pending the election of a constituent assembly. In April of that year, Vladimir Lenin, leader of the Majority (or in Russian: "Bolshevik") faction of socialists in Russia and known for his profound and controversial expansions of Marxism, was allowed to cross Germany to return to his country from exile in Switzerland.		Lenin had published essays on his analysis of imperialism, the monopoly and globalisation phase of capitalism as predicted by Marx, as well as analyses on the social conditions of his contemporary time. He observed that as capitalism had further developed in Europe and America, the workers remained unable to gain class consciousness so long as they were too busy working and concerning with how to make ends meet. He therefore proposed that the social revolution would require the leadership of a vanguard party of class-conscious revolutionaries from the educated and politically active part of the population.[101]		Upon arriving in Petrograd, he declared that the revolution in Russia was not over but had only begun, and that the next step was for the workers' soviets to take full state authority. He issued a thesis outlining the Bolshevik's party programme, including rejection of any legitimacy in the provisional government and advocacy for state power to be given to the peasant and working class through the soviets. The Bolsheviks became the most influential force in the soviets, and on 7 November, the capitol of the provisional government was stormed by Bolshevik Red Guards in what afterwards known as the "Great October Socialist Revolution". The rule of the provisional government was ended and the Russian Socialist Federative Soviet Republic – the world's first constitutionally socialist state – was established. On 25 January 1918, at the Petrograd Soviet, Lenin declared "Long live the world socialist revolution!"[102] He proposed an immediate armistice on all fronts, and transferred the land of the landed proprietors, the crown and the monasteries to the peasant committees without compensation.[103]		On 26 January 1918, the day after assuming executive power, Lenin wrote Draft Regulations on Workers' Control, which granted workers control of businesses with more than five workers and office employees, and access to all books, documents and stocks, and whose decisions were to be "binding upon the owners of the enterprises".[104] Governing through the elected soviets, and in alliance with the peasant-based Left Socialist-Revolutionaries, the Bolshevik government began nationalising banks and industry, and disavowed the national debts of the deposed Romanov royal régime. It sued for peace, withdrawing from World War I, and convoked a Constituent Assembly in which the peasant Socialist-Revolutionary Party (SR) won a majority.[105]		The Constituent Assembly elected Socialist-Revolutionary leader Victor Chernov President of a Russian republic, but rejected the Bolshevik proposal that it endorse the Soviet decrees on land, peace and workers' control, and acknowledge the power of the Soviets of Workers', Soldiers' and Peasants' Deputies. The next day, the Bolsheviks declared that the assembly was elected on outdated party lists,[106] and the All-Russian Central Executive Committee of the Soviets dissolved it.[107][108] In March 1919 world communist parties formed Comintern (also known as the Third International) at a meeting in Moscow.[109]		Parties which did not want to be a part of the resurrected Second International (ISC) or Comintern formed the International Working Union of Socialist Parties (IWUSP, also known as Vienna International/Vienna Union/Two-and-a-Half International) on 27 February 1921 at a conference in Vienna.[110] The ISC and the IWUSP joined to form the Labour and Socialist International (LSI) in May 1923 at a meeting in Hamburg[111] Left wing groups which did not agree to the centralisation and abandonment of the soviets by the Bolshevik Party led Left-wing uprisings against the Bolsheviks; such groups included Socialist Revolutionaries,[112] Left Socialist Revolutionaries, Mensheviks, and anarchists.[113]		Within this left wing discontent the most large scale events were the worker's Kronstadt rebellion[114][115][116] and the anarchist led Revolutionary Insurrectionary Army of Ukraine uprising which controlled an area known as the Free Territory.[117][118][119]		The Bolshevik Russian Revolution of January 1918 engendered Communist parties worldwide, and their concomitant revolutions of 1917–23. Few Communists doubted that the Russian success of socialism depended on successful, working-class socialist revolutions in developed capitalist countries.[120][121] In 1919, Lenin and Trotsky organised the world's Communist parties into a new international association of workers – the Communist International, (Comintern), also called the Third International.		The Russian Revolution also influenced uprisings in other countries around this time. The German Revolution of 1918–1919 resulted in the replacing Germany's imperial government with a republic. The revolutionary period lasted from November 1918 until the formal establishment of the Weimar Republic in August 1919, and included an episode known as the Bavarian Soviet Republic[122][123][124][125] and the Spartacist uprising. In Italy, the events known as the Biennio Rosso[126][127] was characterised by mass strikes, worker manifestations and self-management experiments through land and factories occupations. In Turin and Milan, workers councils were formed and many factory occupations took place led by anarcho-syndicalists organised around the Unione Sindacale Italiana.[128]		By 1920, the Red Army, under its commander Trotsky, had largely defeated the royalist White Armies. In 1921, War Communism was ended and, under the New Economic Policy (NEP), private ownership was allowed for small and medium peasant enterprises. While industry remained largely state-controlled, Lenin acknowledged that the NEP was a necessary capitalist measure for a country unripe for socialism. Profiteering returned in the form of "NEP men" and rich peasants (Kulaks) gained power in the countryside.[129] Nevertheless, the role of Trotsky in this episode has been questioned by other socialists, including ex-Trotskyists. In the United States, Dwight Macdonald broke with Trotsky and left the Trotskyist Socialist Workers Party, by raising the question of the Kronstadt rebellion, which Trotsky as leader of the Soviet Red Army and the other Bolsheviks had brutally repressed. He then moved towards democratic socialism[130] and anarchism.[131]		A similar critique of Trotsky's role on the events around the Kronstadt rebellion was raised by the American anarchist Emma Goldman. In her essay "Trotsky Protests Too Much" she says "I admit, the dictatorship under Stalin's rule has become monstrous. That does not, however, lessen the guilt of Leon Trotsky as one of the actors in the revolutionary drama of which Kronstadt was one of the bloodiest scenes."[132]		In 1922, the fourth congress of the Communist International took up the policy of the United Front, urging Communists to work with rank and file Social Democrats while remaining critical of their leaders, whom they criticised for betraying the working class by supporting the war efforts of their respective capitalist classes. For their part, the social democrats pointed to the dislocation caused by revolution, and later, the growing authoritarianism of the Communist Parties. When the Communist Party of Great Britain applied to affiliate to the Labour Party in 1920, it was turned down.		In 1923, on seeing the Soviet State's growing coercive power, a dying Lenin said Russia had reverted to "a bourgeois tsarist machine... barely varnished with socialism."[133] After Lenin's death in January 1924, the Communist Party of the Soviet Union – then increasingly under the control of Joseph Stalin – rejected the theory that socialism could not be built solely in the Soviet Union, in favour of the concept of Socialism in One Country. Despite the marginalised Left Opposition's demand for the restoration of Soviet democracy, Stalin developed a bureaucratic, authoritarian government, that was condemned by democratic socialists, anarchists and Trotskyists for undermining the initial socialist ideals of the Bolshevik Russian Revolution.[134][135][self-published source?][unreliable source?]		In 1924, the Mongolian People's Republic was established and was ruled by the Mongolian People's Party. The Russian Revolution and the appearance of the Soviet State motivated a worldwide current of national Communist parties which ended having varying levels of political and social influence. Among these there appeared the Communist Party of France, the Communist Party USA, the Italian Communist Party, the Chinese Communist Party, the Mexican Communist Party, the Brazilian Communist Party, the Chilean Communist Party and the Communist Party of Indonesia.		In Spain in 1936, the national anarcho-syndicalist trade union Confederación Nacional del Trabajo (CNT) initially refused to join a popular front electoral alliance, and abstention by CNT supporters led to a right-wing election victory. But in 1936, the CNT changed its policy and anarchist votes helped bring the popular front back to power. Months later, the former ruling class responded with an attempted coup, sparking the Spanish Civil War (1936–1939).[136]		In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land.[137][138] The events known as the Spanish Revolution was a workers' social revolution that began during the outbreak of the Spanish Civil War in 1936 and resulted in the widespread implementation of anarchist and more broadly libertarian socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia, and parts of the Levante.		Much of Spain's economy was put under worker control; in anarchist strongholds like Catalonia, the figure was as high as 75%, but lower in areas with heavy Communist Party of Spain influence, as the Soviet-allied party actively resisted attempts at collectivisation enactment. Factories were run through worker committees, agrarian areas became collectivised and run as libertarian communes. Anarchist historian Sam Dolgoff estimated that about eight million people participated directly or indirectly in the Spanish Revolution.[139]		Trotsky's Fourth International was established in France in 1938 when Trotskyists argued that the Comintern or Third International had become irretrievably "lost to Stalinism" and thus incapable of leading the international working class to political power.[140] The rise of Nazism and the start of World War II led to the dissolution of the LSI in 1940. After the War, the Socialist International was formed in Frankfurt in July 1951 as a successor to the LSI.[141]		After World War II, social democratic governments introduced social reform and wealth redistribution via state welfare and taxation. Social Democratic parties dominated post-war politics in countries such as France, Italy, Czechoslovakia, Belgium and Norway. At one point, France claimed to be the world's most state-controlled capitalist country. The nationalised public utilities included Charbonnages de France (CDF), Electricité de France (EDF), Gaz de France (GDF), Air France, Banque de France, and Régie Nationale des Usines Renault.[142]		In 1945, the British Labour Party, led by Clement Attlee, was elected to office based on a radical socialist programme. The UK Labour Government nationalised major public utilities such as mines, gas, coal, electricity, rail, iron, steel, and the Bank of England. British Petroleum was officially nationalised in 1951.[143] Anthony Crosland said that in 1956, 25% of British industry was nationalised, and that public employees, including those in nationalised industries, constituted a similar proportion of the country's total employed population.[144] The Labour Governments of 1964–1970 and 1974–1979 intervened further.[145] It re-nationalised steel (1967, British Steel) after the Conservatives had denationalised it, and nationalised car production (1976, British Leyland).[146] The National Health Service provided taxpayer-funded health care to everyone, free at the point of service.[147] Working-class housing was provided in council housing estates, and university education became available via a school grant system.[148]		The Nordic model is the economic and social models of the Nordic countries (Denmark, Iceland, Norway, Sweden and Finland). During most of the post-war era, Sweden was governed by the Swedish Social Democratic Party largely in cooperation with trade unions and industry.[149] In Sweden, the Social Democratic Party held power from 1936 to 1976, 1982 to 1991, 1994 to 2006, and 2014 to present.		From 1945 to 1962, the Norwegian Labour Party held an absolute majority in the parliament led by Einar Gerhardsen who was Prime Minister with 17 years in office. This particular adaptation of the mixed market economy is characterised by more generous welfare states (relative to other developed countries), which are aimed specifically at enhancing individual autonomy, ensuring the universal provision of basic human rights and stabilising the economy. It is distinguished from other welfare states with similar goals by its emphasis on maximising labour force participation, promoting gender equality, egalitarian and extensive benefit levels, large magnitude of redistribution, and expansionary fiscal policy.[150]		The USSR played a decisive role in the Allied victory in World War II.[151][152] After the War, the USSR became a recognised superpower,[153] The Soviet era saw some of the most significant technological achievements of the 20th century, including the world's first spacecraft, and the first astronaut. The Soviet economy was the modern world's first centrally planned economy. It was based on a system of state ownership of industry managed through Gosplan (the State Planning Commission), Gosbank (the State Bank) and the Gossnab (State Commission for Materials and Equipment Supply).		Economic planning was conducted through a series of Five-Year Plans. The emphasis was on fast development of heavy industry and the nation became one of the world's top manufacturers of a large number of basic and heavy industrial products, but it lagged in light industrial production and consumer durables.[citation needed]		The Eastern Bloc was the group of former communist states of Central and Eastern Europe, generally the Soviet Union and the countries of the Warsaw Pact[154][155][156] which included the People's Republic of Poland, the German Democratic Republic, the People's Republic of Hungary, the People's Republic of Bulgaria, the Czechoslovak Socialist Republic, the Socialist Republic of Romania, the People's Socialist Republic of Albania and the Socialist Federal Republic of Yugoslavia. The Hungarian Revolution of 1956 was a spontaneous nationwide revolt against the government of the People's Republic of Hungary and its Soviet-imposed policies, lasting from 23 October until 10 November 1956. Soviet leader Nikita Khrushchev's denunciation of the excesses of Stalin's regime during the Twentieth Party Congress of the Communist Party of the Soviet Union on 1956,[157] as well as the revolt in Hungary,[158][159][160][161] produced ideological fractures and disagreements within the communist and socialist parties of Western Europe.		In the postwar years, socialism became increasingly influential throughout the so-called Third World. Embracing a new Third World Socialism, countries in Africa, Asia, and Latin America often nationalised industries held by foreign owners. The Chinese Kuomintang Party, the previous ruling party in Taiwan, was referred to as having a socialist ideology since Kuomintang's revolutionary ideology in the 1920s incorporated unique Chinese Socialism as part of its ideology.[162][163] The Soviet Union trained Kuomintang revolutionaries in the Moscow Sun Yat-sen University. Movie theatres in the Soviet Union showed newsreels and clips of Chiang, at Moscow Sun Yat-sen University Portraits of Chiang were hung on the walls, and in the Soviet May Day Parades that year, Chiang's portrait was to be carried along with the portraits of Karl Marx, Lenin, Stalin and other socialist leaders.[164]		The Chinese Revolution was the second stage in the Chinese Civil War which ended in the establishment of the People's Republic of China led by the Chinese Communist Party. The term "Third World" was coined by French demographer Alfred Sauvy in 1952, on the model of the Third Estate, which, according to the Abbé Sieyès, represented everything, but was nothing: "...because at the end this ignored, exploited, scorned Third World like the Third Estate, wants to become something too" (Sauvy).		The emergence of this new political entity, in the frame of the Cold War, was complex and painful. Several tentatives were made to organise newly independent states in order to oppose a common front towards both the US's and the USSR's influence on them, with the consequences of the Sino-Soviet split already at works. Thus, the Non-Aligned Movement constituted itself, around the main figures of Prime Minister Jawaharlal Nehru of India, President Sukarno of Indonesia, leader Josip Broz Tito of Yugoslavia, and Gamal Abdel Nasser of Egypt who successfully opposed the French and British imperial powers during the 1956 Suez crisis. After the 1954 Geneva Conference which ended the French war against Ho Chi Minh in Vietnam, the 1955 Bandung Conference gathered Nasser, Nehru, Tito, Sukarno, and Zhou Enlai, Premier of the People's Republic of China.		As many African countries gained independence during the 1960s, some of them rejected capitalism in favour of a more afrocentric economic model. The main architects of African Socialism were Julius Nyerere of Tanzania, Léopold Senghor of Senegal, Kwame Nkrumah of Ghana and Sékou Touré of Guinea.[165]		The Cuban Revolution (1953–1959) was an armed revolt conducted by Fidel Castro's 26th of July Movement and its allies against the government of Cuban President Fulgencio Batista. The revolution began in July 1953, and finally ousted Batista on 1 January 1959, replacing his government with Castro's revolutionary state. Castro's government later reformed along communist lines, becoming the Communist Party of Cuba in October 1965.[166]		The New Left was a term used mainly in the United Kingdom and United States in reference to activists, educators, agitators and others in the 1960s and 1970s who sought to implement a broad range of reforms on issues such as gay rights, abortion, gender roles and drugs[167] in contrast to earlier leftist or Marxist movements that had taken a more vanguardist approach to social justice and focused mostly on labour unionisation and questions of social class.[168][169][170] The New Left rejected involvement with the labour movement and Marxism's historical theory of class struggle.[171]		In the U.S., the "New Left" was associated with the Hippie movement and anti-war college campus protest movements, as well as the black liberation movements such as the Black Panther Party.[172] While initially formed in opposition to the "Old Left" Democratic party, groups composing the New Left gradually became central players in the Democratic coalition.[167]		The protests of 1968 represented a worldwide escalation of social conflicts, predominantly characterised by popular rebellions against military, capitalist, and bureaucratic elites, who responded with an escalation of political repression. These protests marked a turning point for the Civil Rights Movement in the United States, which produced revolutionary movements like the Black Panther Party; the prominent civil rights leader Martin Luther King Jr. organised the "Poor People's Campaign" to address issues of economic justice,[173] while personally showing sympathy with democratic socialism.[174] In reaction to the Tet Offensive, protests also sparked a broad movement in opposition to the Vietnam War all over the United States and even into London, Paris, Berlin and Rome. In 1968 in Carrara, Italy the International of Anarchist Federations was founded during an international anarchist conference held there by the three existing European federations of France, the Italian and the Iberian Anarchist Federation as well as the Bulgarian federation in French exile.		Mass socialist or communist movements grew not only in the United States but also in most European countries. The most spectacular manifestation of this were the May 1968 protests in France, in which students linked up with strikes of up to ten million workers, and for a few days the movement seemed capable of overthrowing the government.[citation needed]		In many other capitalist countries, struggles against dictatorships, state repression, and colonisation were also marked by protests in 1968, such as the beginning of the Troubles in Northern Ireland, the Tlatelolco massacre in Mexico City, and the escalation of guerrilla warfare against the military dictatorship in Brazil. Countries governed by communist parties had protests against bureaucratic and military elites. In Eastern Europe there were widespread protests that escalated particularly in the Prague Spring in Czechoslovakia. In response, USSR occupied Czechoslovakia. The occupation was denounced by the Italian and French[175] Communist parties, and the Communist Party of Finland. Few western European political leaders defended the occupation, among them the Portuguese communist secretary-general Álvaro Cunhal.[176] along with the Luxembourg party[175] and conservative factions of the Greek party.[175]		In the Chinese Cultural Revolution, a social-political youth movement mobilised against "bourgeois" elements which were seen to be infiltrating the government and society at large, aiming to restore capitalism. This movement motivated Maoism-inspired movements around the world in the context of the Sino-Soviet split.[citation needed]		In Indonesia, a right wing military regime led by Suharto killed between 500,000 and one million people, mainly to crush the growing influence of the Communist Party of Indonesia and other leftist sectors, with support from the United States government, which provided kill lists containing thousands of names of suspected high-ranking Communists.[177][178][179][180][181]		In Latin America in the 1960s, a socialist tendency within the catholic church appeared which was called Liberation theology[183][184] which motivated even the Colombian priest Camilo Torres to enter the ELN guerrilla. In Chile, Salvador Allende, a physician and candidate for the Socialist Party of Chile, was elected president through democratic elections in 1970. In 1973, his government was ousted by the U.S.-backed military dictatorship of Augusto Pinochet, which lasted until the late 1980s.[185]		In Italy, Autonomia Operaia was a leftist movement particularly active from 1976 to 1978. It took an important role in the autonomist movement in the 1970s, aside earlier organisations such as Potere Operaio, created after May 1968, and Lotta Continua.[186] This experience prompted the contemporary socialist radical movement autonomism.[187]		The Nicaraguan Revolution encompassed the rising opposition to the Somoza dictatorship in the 1960s and 1970s, the campaign led by the Sandinista National Liberation Front (FSLN) to violently oust the dictatorship in 1978–79, the subsequent efforts of the FSLN to govern Nicaragua from 1979 until 1990[188] and the socialist measures which included widescale agrarian reform[189][190] and educational programs.[191] The People's Revolutionary Government was proclaimed on 13 March 1979 in Grenada which was overthrown by armed forces of the United States in 1983. The Salvadoran Civil War (1979–1992) was a conflict between the military-led government of El Salvador and the Farabundo Martí National Liberation Front (FMLN), a coalition or 'umbrella organisation' of five socialist guerrilla groups. A coup on 15 October 1979 led to the killings of anti-coup protesters by the government as well as anti-disorder protesters by the guerillas, and is widely seen as the tipping point towards the civil war.[192]		In 1982, the newly elected French socialist government of François Mitterrand made nationalisations in a few key industries, including banks and insurance companies.[193] Eurocommunism was a trend in the 1970s and 1980s in various Western European communist parties to develop a theory and practice of social transformation that was more relevant for a Western European country and less aligned to the influence or control of the Communist Party of the Soviet Union. Outside Western Europe, it is sometimes called Neocommunism.[194] Some Communist parties with strong popular support, notably the Italian Communist Party (PCI) and the Communist Party of Spain (PCE) adopted Eurocommunism most enthusiastically, and the Communist Party of Finland was dominated by Eurocommunists. The French Communist Party (PCF) and many smaller parties strongly opposed Eurocommunism and stayed aligned with the Communist Party of the Soviet Union until the end of the USSR.		In the late 1970s and in the 1980s, the Socialist International had extensive contacts and discussion with the two powers of the Cold War, the United States and the Soviet Union, about East-West relations and arms control. Since then, the SI has admitted as member parties the Nicaraguan FSLN, the left-wing Puerto Rican Independence Party, as well as former Communist parties such as the Democratic Party of the Left of Italy and the Front for the Liberation of Mozambique (FRELIMO). The Socialist International aided social democratic parties in re-establishing themselves when dictatorship gave way to democracy in Portugal (1974) and Spain (1975). Until its 1976 Geneva Congress, the SI had few members outside Europe and no formal involvement with Latin America.[195]		After Mao's death in 1976 and the arrest of the faction known as the Gang of Four, who were blamed for the excesses of the Cultural Revolution, Deng Xiaoping took power and led the People's Republic of China to significant economic reforms. The Communist Party of China loosened governmental control over citizens' personal lives and the communes were disbanded in favour of private land leases. Thus, China's transition from a planned economy to a mixed economy named as "socialism with Chinese characteristics"[196] which maintained state ownership rights over land, state or cooperative ownership of much of the heavy industrial and manufacturing sectors and state influence in the banking and financial sectors. China adopted its current constitution on 4 December 1982. President Jiang Zemin and Premier Zhu Rongji led the nation in the 1990s. Under their administration, China's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%.[197][198] At the Sixth National Congress of the Communist Party of Vietnam in December 1986, reformist politicians replaced the "old guard" government with new leadership.[199][200] The reformers were led by 71-year-old Nguyen Van Linh, who became the party's new general secretary.[199][200] Linh and the reformers implemented a series of free-market reforms – known as Đổi Mới ("Renovation") – which carefully managed the transition from a planned economy to a "socialist-oriented market economy".[201][202] Mikhail Gorbachev wished to move the USSR towards of Nordic-style social democracy, calling it "a socialist beacon for all mankind."[203][204] Prior to its dissolution in 1991, the USSR had the second largest economy in the world after the United States.[205] With the collapse of the Soviet Union, the economic integration of the Soviet republics was dissolved, and overall industrial activity declined substantially.[206] A lasting legacy remains in the physical infrastructure created during decades of combined industrial production practices, and widespread environmental destruction.[207]		Many social democratic parties, particularly after the Cold War, adopted neoliberal market policies including privatisation, deregulation and financialisation. They abandoned their pursuit of moderate socialism in favour of market liberalism. By the 1980s, with the rise of conservative neoliberal politicians such as Ronald Reagan in the United States, Margaret Thatcher in Britain, Brian Mulroney in Canada and Augusto Pinochet in Chile, the Western welfare state was attacked from within, but state support for the corporate sector was maintained.[208] Monetarists and neoliberals attacked social welfare systems as impediments to private entrepreneurship. In the UK, Labour Party leader Neil Kinnock made a public attack against the entryist group Militant at the 1985 Labour Party conference. The Labour Party ruled that Militant was ineligible for affiliation with the Labour Party, and the party gradually expelled Militant supporters. The Kinnock leadership had refused to support the 1984–1985 miner's strike over pit closures, a decision that the party's left wing and the National Union of Mineworkers blamed for the strike's eventual defeat. In 1989, at Stockholm, the 18th Congress of the Socialist International adopted a new Declaration of Principles, saying:		Democratic socialism is an international movement for freedom, social justice, and solidarity. Its goal is to achieve a peaceful world where these basic values can be enhanced and where each individual can live a meaningful life with the full development of his or her personality and talents, and with the guarantee of human and civil rights in a democratic framework of society.[209]		In the 1990s, the British Labour Party, under Tony Blair, enacted policies based on the free market economy to deliver public services via the Private finance initiative. Influential in these policies was the idea of a "third Way" which called for a re-evalutation of welfare state policies.[210] In 1995, the Labour Party re-defined its stance on socialism by re-wording Clause IV of its constitution, effectively rejecting socialism by removing all references to public, direct worker or municipal ownership of the means of production. The Labour Party stated: "The Labour Party is a democratic socialist party. It believes that, by the strength of our common endeavour we achieve more than we achieve alone, so as to create, for each of us, the means to realise our true potential, and, for all of us, a community in which power, wealth, and opportunity are in the hands of the many, not the few."[211]		African socialism has been and continues to be a major ideology around the continent. Julius Nyerere was inspired by Fabian socialist ideals.[212] He was a firm believer in rural Africans and their traditions and ujamaa, a system of collectivisation that according to Nyerere was present before European imperialism. Essentially he believed Africans were already socialists. Other African socialists include Jomo Kenyatta, Kenneth Kaunda, Nelson Mandela and Kwame Nkrumah. Fela Kuti was inspired by socialism and called for a democratic African republic. In South Africa the African National Congress (ANC) abandoned its partial socialist allegiances after taking power, and followed a standard neoliberal route. From 2005 through to 2007, the country was wracked by many thousands of protests from poor communities. One of these gave rise to a mass movement of shack dwellers, Abahlali baseMjondolo that, despite major police suppression, continues to work for popular people's planning and against the creation of a market economy in land and housing.		In Asia, states with socialist economies – such as the People's Republic of China, North Korea, Laos, and Vietnam – have largely moved away from centralised economic planning in the 21st century, placing a greater emphasis on markets. Forms include the Chinese socialist market economy and the Vietnamese socialist-oriented market economy. They utilise state-owned corporate management models as opposed to modelling socialist enterprise on traditional management styles employed by government agencies. In China living standards continued to improve rapidly despite the late-2000s recession, but centralised political control remained tight.[213] Brian Reynolds Myers in his book The Cleanest Race, and later supported by other academics,[214][215] dismisses the idea that Juche is North Korea's leading ideology, regarding its public exaltation as designed to deceive foreigners and that it exists to be praised and not actually read,[216] pointing out that North Korea's constitution of 2009 omits all mention of communism.[215]		Though the authority of the state remained unchallenged under Đổi Mới, the government of Vietnam encourages private ownership of farms and factories, economic deregulation and foreign investment, while maintaining control over strategic industries.[202] The Vietnamese economy subsequently achieved strong growth in agricultural and industrial production, construction, exports and foreign investment. However, these reforms have also caused a rise in income inequality and gender disparities.[217][218]		Elsewhere in Asia, some elected socialist parties and communist parties remain prominent, particularly in India and Nepal. The Communist Party of Nepal[which?] in particular calls for multi-party democracy, social equality, and economic prosperity.[219] In Singapore, a majority of the GDP is still generated from the state sector comprising government-linked companies.[220] In Japan, there has been a resurgent interest in the Japanese Communist Party among workers and youth.[221][222] In Malaysia, the Socialist Party of Malaysia got its first Member of Parliament, Dr. Jeyakumar Devaraj, after the 2008 general election. In 2010, there were 270 kibbutzim in Israel. Their factories and farms account for 9% of Israel's industrial output, worth US$8 billion, and 40% of its agricultural output, worth over $1.7 billion.[223] Some Kibbutzim had also developed substantial high-tech and military industries. For example, in 2010, Kibbutz Sasa, containing some 200 members, generated $850 million in annual revenue from its military-plastics industry.[224]		The United Nations World Happiness Report 2013 shows that the happiest nations are concentrated in northern Europe, where the Nordic model of social democracy is employed, with Denmark topping the list. This is at times attributed to the success of the Nordic model in the region. The Nordic countries ranked highest on the metrics of real GDP per capita, healthy life expectancy, having someone to count on, perceived freedom to make life choices, generosity and freedom from corruption.[225] The objectives of the Party of European Socialists, the European Parliament's socialist and social-democratic bloc, are now "to pursue international aims in respect of the principles on which the European Union is based, namely principles of freedom, equality, solidarity, democracy, respect of Human Rights and Fundamental Freedoms, and respect for the Rule of Law." As a result, today, the rallying cry of the French Revolution – "Egalité, Liberté, Fraternité" – is promoted as essential socialist values.[226] To the left of the PES at the European level is the Party of the European Left, (PEL; also commonly abbreviated "European Left") which is a political party at the European level and an association of democratic socialist, socialist[227] and communist[227] political parties in the European Union and other European countries. It was formed in January 2004 for the purposes of running in the 2004 European Parliament elections. PEL was founded on 8–9 May 2004 in Rome.[228] Elected MEPs from member parties of the European Left sit in the European United Left–Nordic Green Left (GUE/NGL) group in the European parliament.		The socialist Left Party in Germany grew in popularity[229] due to dissatisfaction with the increasingly neoliberal policies of the SPD, becoming the fourth biggest party in parliament in the general election on 27 September 2009.[230] Communist candidate Dimitris Christofias won a crucial presidential runoff in Cyprus, defeating his conservative rival with a majority of 53%.[231] In Ireland, in the 2009 European election, Joe Higgins of the Socialist Party took one of three seats in the capital Dublin European constituency.		In Denmark, the Socialist People's Party (SF or Socialist Party for short) more than doubled its parliamentary representation to 23 seats from 11, making it the fourth largest party.[232] In 2011, the socialist parties of Social Democrats, Socialist People's Party and the Danish Social Liberal Party formed government, after a slight victory over the liberal parties. They were led by Helle Thorning-Schmidt, and had the Red-Green Alliance as a supporting party.		In Norway, the Red-Green Coalition consists of the Labour Party (Ap), the Socialist Left Party (SV), and the Centre Party (Sp), and governed the country as a majority government from the 2005 general election until 2013.		In the Greek legislative election of January 2015, the Coalition of the Radical Left (SYRIZA), led by Alexis Tsipras, won a legislative election for the first time while the Communist Party of Greece won 15 seats in parliament. SYRIZA has been characterised as an anti-establishment party,[233] whose success has sent "shock-waves across the EU".[234]		In the UK, the National Union of Rail, Maritime and Transport Workers put forward a slate of candidates in the 2009 European Parliament elections under the banner of No to EU – Yes to Democracy, a broad left-wing alter-globalisation coalition involving socialist groups such as the Socialist Party, aiming to offer an alternative to the "anti-foreigner" and pro-business policies of the UK Independence Party.[235][236][237] In the following May 2010 UK general election, the Trade Unionist and Socialist Coalition, launched in January 2010[238] and backed by Bob Crow, the leader of the National Union of Rail, Maritime and Transport Workers union (RMT), other union leaders and the Socialist Party among other socialist groups, stood against Labour in 40 constituencies.[239][240] The Trade Unionist and Socialist Coalition plans to contest the 2011 elections, having gained the endorsement of the RMT June 2010 conference.[241] Left Unity was also founded in 2013 after the film director Ken Loach appealed for a new party of the left to replace the Labour Party, which he claimed had failed to oppose austerity and had shifted towards neoliberalism.[242][243][244][245] In 2015, following a defeat at the 2015 UK general election, Jeremy Corbyn, a self-described socialist[246] took over from Ed Miliband as leader of the Labour Party.		In France, the Revolutionary Communist League (LCR) candidate in the 2007 presidential election, Olivier Besancenot, received 1,498,581 votes, 4.08%, double that of the Communist candidate.[247] The LCR abolished itself in 2009 to initiate a broad anti-capitalist party, the New Anticapitalist Party, whose stated aim is to "build a new socialist, democratic perspective for the twenty-first century".[248]		On 25 May 2014 in Spain the left wing party Podemos entered candidates for the 2014 European parliamentary elections, some of which were unemployed. In a surprise result, it polled 7.98% of the vote and thus was awarded five seats out of 54.[249][250] while the older United Left was the third largest overall force obtaining 10.03% and 5 seats, 4 more than the previous elections.[251]		The current government of Portugal was established on 26 November 2015 as a Socialist Party (PS) minority government led by prime minister António Costa. Costa succeeded in securing support for a Socialist minority government by the Left Bloc (B.E.), the Portuguese Communist Party (PCP) and the Ecologist Party "The Greens" (PEV).[252]		All around Europe and in some places of Latin America there exists a social center and squatting movement mainly inspired by autonomist and anarchist ideas.[253][254]		According to a 2013 article in The Guardian, "Contrary to popular belief, Americans don't have an innate allergy to socialism. Milwaukee has had several socialist mayors (Frank Zeidler, Emil Seidel and Daniel Hoan), and there is currently an independent socialist in the US Senate, Bernie Sanders of Vermont."[255] Sanders, once mayor of Vermont's largest city, Burlington, has described himself as a democratic socialist[256][257] and has praised Scandinavian-style social democracy.[258][259]		Anti-capitalism, anarchism and the anti-globalisation movement rose to prominence through events such as protests against the World Trade Organization Ministerial Conference of 1999 in Seattle. Socialist-inspired groups played an important role in these movements, which nevertheless embraced much broader layers of the population and were championed by figures such as Noam Chomsky. In Canada, the Co-operative Commonwealth Federation (CCF), the precursor to the social democratic New Democratic Party (NDP), had significant success in provincial politics. In 1944, the Saskatchewan CCF formed the first socialist government in North America. At the federal level, the NDP was the Official Opposition, from 2011 through 2015.[260]		For the Encyclopedia Britannica "the attempt by Salvador Allende to unite Marxists and other reformers in a socialist reconstruction of Chile is most representative of the direction that Latin American socialists have taken since the late 20th century. ... Several socialist (or socialist-leaning) leaders have followed Allende's example in winning election to office in Latin American countries."[75] Venezuelan President Hugo Chávez, Nicaraguan President Daniel Ortega, Bolivian President Evo Morales, and Ecuadorian president Rafael Correa refer to their political programmes as socialist. Chávez has adopted the term socialism of the 21st century. After winning re-election in December 2006, Chávez said, "Now more than ever, I am obliged to move Venezuela's path towards socialism."[261] Hugo Chávez was also reelected in October 2012 for his third six-year term as President, but he died in March 2013 from cancer. After Chávez's death on 5 March 2013, vice-president from Chavez's party Nicolás Maduro assumed the powers and responsibilities of the President. A special election was held on 14 April of the same year to elect a new President, which Maduro won by a tight margin as the candidate of the United Socialist Party of Venezuela; he was formally inaugurated on 19 April.[262] "Pink tide" is a term being used in contemporary 21st-century political analysis in the media and elsewhere to describe the perception that Leftist ideology in general, and Left-wing politics in particular, are increasingly influential in Latin America.[263][264][265]		Foro de São Paulo is a conference of leftist political parties and other organisations from Latin America and the Caribbean. It was launched by the Workers' Party (Portuguese: Partido dos Trabalhadores – PT) of Brazil in 1990 in the city of São Paulo. The Forum of São Paulo was constituted in 1990 when the Brazilian Workers' Party approached other parties and social movements of Latin America and the Caribbean with the objective of debating the new international scenario after the fall of the Berlin Wall and the consequences of the implementation of what were taken as neoliberal policies adopted at the time by contemporary right-leaning governments in the region, the stated main objective of the conference being to argue for alternatives to neoliberalism.[266] Among its member include current socialist and social-democratic parties currently in government in the region such as Bolivia's Movement for Socialism, Brazil's Workers Party, the Communist Party of Cuba, the Ecuadorian PAIS Alliance, the Venezuelan United Socialist Party of Venezuela, the Socialist Party of Chile, the Uruguayan Broad Front, the Nicaraguan Sandinista National Liberation Front and the salvadorean Farabundo Martí National Liberation Front.		The Progressive Alliance is a political international founded on 22 May 2013 by political parties, the majority of whom are current or former members of the Socialist International. The organisation states the aim of becoming the global network of "the progressive", democratic, social-democratic, socialist and labour movement".[267][268]		Early socialist thought took influences from a diverse range of philosophies such as civic republicanism, Enlightenment rationalism, romanticism, forms of materialism, Christianity (both Catholic and Protestant), natural law and natural rights theory, utilitarianism and liberal political economy.[269] Another philosophical basis for a lot of early socialism was the emergence of positivism during the European Enlightenment. Positivism held that both the natural and social worlds could be understood through scientific knowledge and be analyzed using scientific methods. This core outlook influenced early social scientists and different types of socialists ranging from anarchists like Peter Kropotkin to technocrats like Saint Simon.[270]		The fundamental objective of socialism is to attain an advanced level of material production and therefore greater productivity, efficiency and rationality as compared to capitalism and all previous systems, under the view that an expansion of human productive capability is the basis for the extension of freedom and equality in society.[271] Many forms of socialist theory hold that human behaviour is largely shaped by the social environment. In particular, socialism holds that social mores, values, cultural traits and economic practices are social creations and not the result of an immutable natural law.[272][273] The object of their critique is thus not human avarice or human consciousness, but the material conditions and man-made social systems (i.e.: the economic structure of society) that gives rise to observed social problems and inefficiencies. Bertrand Russell, often considered to be the father of analytic philosophy, identified as a socialist. Bertrand Russell opposed the class struggle aspects of Marxism, viewing socialism solely as an adjustment of economic relations to accommodate modern machine production to benefit all of humanity through the progressive reduction of necessary work time.[274]		Socialists view creativity as an essential aspect of human nature, and define freedom as a state of being where individuals are able to express their creativity unhindered by constraints of both material scarcity and coercive social institutions.[275] The socialist concept of individuality is thus intertwined with the concept of individual creative expression. Karl Marx believed that expansion of the productive forces and technology was the basis for the expansion of human freedom, and that socialism, being a system that is consistent with modern developments in technology, would enable the flourishing of "free individualities" through the progressive reduction of necessary labour time. The reduction of necessary labour time to a minimum would grant individuals the opportunity to pursue the development of their true individuality and creativity.[276]		Socialists argue that the accumulation of capital generates waste through externalities that require costly corrective regulatory measures. They also point out that this process generates wasteful industries and practices that exist only to generate sufficient demand for products to be sold at a profit (such as high-pressure advertisement); thereby creating rather than satisfying economic demand.[277][278]		Socialists argue that capitalism consists of irrational activity, such as the purchasing of commodities only to sell at a later time when their price appreciates, rather than for consumption, even if the commodity cannot be sold at a profit to individuals in need; therefore, a crucial criticism often made by socialists is that making money, or accumulation of capital, does not correspond to the satisfaction of demand (the production of use-values).[279] The fundamental criterion for economic activity in capitalism is the accumulation of capital for reinvestment in production; this spurs the development of new, non-productive industries that don't produce use-value and only exist to keep the accumulation process afloat (otherwise the system goes into crisis), such as the spread of the financial industry, contributing to the formation of economic bubbles.[280]		Socialists view private property relations as limiting the potential of productive forces in the economy. According to socialists, private property becomes obsolete when it concentrates into centralised, socialised institutions based on private appropriation of revenue (but based on cooperative work and internal planning in allocation of inputs) until the role of the capitalist becomes redundant.[281] With no need for capital accumulation and a class of owners, private property in the means of production is perceived as being an outdated form of economic organization that should be replaced by a free association of individuals based on public or common ownership of these socialised assets.[282][283] Private ownership imposes constraints on planning, leading to uncoordinated economic decisions that result in business fluctuations, unemployment and a tremendous waste of material resources during crisis of overproduction.[284]		Excessive disparities in income distribution lead to social instability and require costly corrective measures in the form of redistributive taxation, which incurs heavy administrative costs while weakening the incentive to work, inviting dishonesty and increasing the likelihood of tax evasion while (the corrective measures) reduce the overall efficiency of the market economy.[285] These corrective policies limit the incentive system of the market by providing things such as minimum wages, unemployment insurance, taxing profits and reducing the reserve army of labor, resulting in reduced incentives for capitalists to invest in more production. In essence, social welfare policies cripple capitalism and its incentive system and are thus unsustainable in the long-run.[286] Marxists argue that the establishment of a socialist mode of production is the only way to overcome these deficiencies. Socialists and specifically Marxian socialists, argue that the inherent conflict of interests between the working class and capital prevent optimal use of available human resources and leads to contradictory interest groups (labor and business) striving to influence the state to intervene in the economy in their favor at the expense of overall economic efficiency.		Early socialists (Utopian socialists and Ricardian socialists) criticised capitalism for concentrating power and wealth within a small segment of society.[287] In addition, they complained that capitalism does not utilise available technology and resources to their maximum potential in the interests of the public.[283]		At a certain stage of development, the material productive forces of society come into conflict with the existing relations of production or – this merely expresses the same thing in legal terms – with the property relations within the framework of which they have operated hitherto. Then begins an era of social revolution. The changes in the economic foundation lead sooner or later to the transformation of the whole immense superstructure. – Karl Marx, Critique of the Gotha Program[288]		Karl Marx and Friedrich Engels argued that socialism would emerge from historical necessity as capitalism rendered itself obsolete and unsustainable from increasing internal contradictions emerging from the development of the productive forces and technology. It was these advances in the productive forces combined with the old social relations of production of capitalism that would generate contradictions, leading to working-class consciousness.[289]		Marx and Engels held the view that the consciousness of those who earn a wage or salary (the working class in the broadest Marxist sense) would be moulded by their conditions of wage slavery, leading to a tendency to seek their freedom or emancipation by overthrowing ownership of the means of production by capitalists, and consequently, overthrowing the state that upheld this economic order. For Marx and Engels, conditions determine consciousness and ending the role of the capitalist class leads eventually to a classless society in which the state would wither away. The Marxist conception of socialism is that of a specific historical phase that will displace capitalism and precede communism. The major characteristics of socialism (particularly as conceived by Marx and Engels after the Paris Commune of 1871) are that the proletariat will control the means of production through a workers' state erected by the workers in their interests. Economic activity would still be organised through the use of incentive systems and social classes would still exist, but to a lesser and diminishing extent than under capitalism.		For orthodox Marxists, socialism is the lower stage of communism based on the principle of "from each according to his ability, to each according to his contribution" while upper stage communism is based on the principle of "from each according to his ability, to each according to his need"; the upper stage becoming possible only after the socialist stage further develops economic efficiency and the automation of production has led to a superabundance of goods and services.[290][291] Marx argued that the material productive forces (in industry and commerce) brought into existence by capitalism predicated a cooperative society since production had become a mass social, collective activity of the working class to create commodities but with private ownership (the relations of production or property relations). This conflict between collective effort in large factories and private ownership would bring about a conscious desire in the working class to establish collective ownership commensurate with the collective efforts their daily experience.[288]		Socialists have taken different perspectives on the state and the role it should play in revolutionary struggles, in constructing socialism, and within an established socialist economy.		In the 19th century the philosophy of state socialism was first explicitly expounded by the German political philosopher Ferdinand Lassalle. In contrast to Karl Marx's perspective of the state, Lassalle rejected the concept of the state as a class-based power structure whose main function was to preserve existing class structures. Thus Lassalle also rejected the Marxist view that the state was destined to "wither away". Lassalle considered the state to be an entity independent of class allegiances and an instrument of justice that would therefore be essential for achieving socialism.[292]		Preceding the Bolshevik-led revolution in Russia, many socialists including reformists, orthodox Marxist currents such as council communism, anarchists and libertarian socialists criticised the idea of using the state to conduct central planning and own the means of production as a way to establish socialism. Following the victory of Leninism in Russia, the idea of "state socialism" spread rapidly throughout the socialist movement, and eventually "state socialism" came to be identified with the Soviet economic model.[293]		Joseph Schumpeter rejected the association of socialism (and social ownership) with state ownership over the means of production, because the state as it exists in its current form is a product of capitalist society and cannot be transplanted to a different institutional framework. Schumpeter argued that there would be different institutions within socialism than those that exist within modern capitalism, just as feudalism had its own distinct and unique institutional forms. The state, along with concepts like property and taxation, were concepts exclusive to commercial society (capitalism) and attempting to place them within the context of a future socialist society would amount to a distortion of these concepts by using them out of context.[294]		Utopian socialism is a term used to define the first currents of modern socialist thought as exemplified by the work of Henri de Saint-Simon, Charles Fourier, and Robert Owen, which inspired Karl Marx and other early socialists.[295] However, visions of imaginary ideal societies, which competed with revolutionary social-democratic movements, were viewed as not being grounded in the material conditions of society and as reactionary.[296] Although it is technically possible for any set of ideas or any person living at any time in history to be a utopian socialist, the term is most often applied to those socialists who lived in the first quarter of the 19th century who were ascribed the label "utopian" by later socialists as a negative term, in order to imply naivete and dismiss their ideas as fanciful or unrealistic.[79]		Religious sects whose members live communally, such as the Hutterites, for example, are not usually called "utopian socialists", although their way of living is a prime example. They have been categorised as religious socialists by some. Likewise, modern intentional communities based on socialist ideas could also be categorised as "utopian socialist".		For Marxists, the development of capitalism in western Europe provided a material basis for the possibility of bringing about socialism because, according to the Communist Manifesto, "What the bourgeoisie produces above all is its own grave diggers",[297] namely the working class, which must become conscious of the historical objectives set it by society.		Revolutionary socialists believe that a social revolution is necessary to effect structural changes to the socioeconomic structure of society. Among revolutionary socialists there are differences in strategy, theory, and the definition of "revolution". Orthodox Marxists and Left Communists take an impossibilist stance, believing that revolution should be spontaneous as a result of contradictions in society due to technological changes in the productive forces. Lenin theorised that under capitalism the workers cannot achieve class consciousness beyond organising into unions and making demands of the capitalists. Therefore, Leninists advocate that it is historically necessary for a vanguard of class-conscious revolutionaries to take a central role in coordinating the social revolution to overthrow the capitalist state and, eventually, the institution of the state altogether.[298] "Revolution" is not necessarily defined by revolutionary socialists as violent insurrection,[299] but as a complete dismantling and rapid transformation of all areas of class society led by the majority of the masses: the working class.		Reformism is generally associated with social democracy and gradualist democratic socialism. Reformism is the belief that socialists should stand in parliamentary elections within capitalist society and, if elected, utilise the machinery of government to pass political and social reforms for the purposes of ameliorating the instabilities and inequities of capitalism.		Socialist economics starts from the premise that "individuals do not live or work in isolation but live in cooperation with one another. Furthermore, everything that people produce is in some sense a social product, and everyone who contributes to the production of a good is entitled to a share in it. Society as a whole, therefore, should own or at least control property for the benefit of all its members."[88]		The original conception of socialism was an economic system whereby production was organised in a way to directly produce goods and services for their utility (or use-value in classical and Marxian economics): the direct allocation of resources in terms of physical units as opposed to financial calculation and the economic laws of capitalism (see: Law of value), often entailing the end of capitalistic economic categories such as rent, interest, profit and money.[300] In a fully developed socialist economy, production and balancing factor inputs with outputs becomes a technical process to be undertaken by engineers.[301]		Market socialism refers to an array of different economic theories and systems that utilise the market mechanism to organise production and to allocate factor inputs among socially owned enterprises, with the economic surplus (profits) accruing to society in a social dividend as opposed to private capital owners.[302] Variations of market socialism include Libertarian proposals such as mutualism, based on classical economics, and neoclassical economic models such as the Lange Model. However, some economists such as Joseph Stiglitz, Mancur Olson and others not specifically advancing anti-socialists positions have shown that prevailing economic models upon which such democratic or market socialism models might be based have logical flaws or unworkable presuppositions.[303][304]		The ownership of the means of production can be based on direct ownership by the users of the productive property through worker cooperative; or commonly owned by all of society with management and control delegated to those who operate/use the means of production; or public ownership by a state apparatus. Public ownership may refer to the creation of state-owned enterprises, nationalisation, municipalisation or autonomous collective institutions. Some socialists feel that in a socialist economy, at least the 'commanding heights' of the economy must be publicly owned.[305] However, economic liberals and right libertarians view private ownership of the means of production and the market exchange as natural entities or moral rights which are central to their conceptions of freedom and liberty, and view the economic dynamics of capitalism as immutable and absolute. Therefore, they perceive public ownership of the means of production, cooperatives and economic planning as infringements upon liberty.[306][307]		Management and control over the activities of enterprises are based on self-management and self-governance, with equal power-relations in the workplace to maximise occupational autonomy. A socialist form of organisation would eliminate controlling hierarchies so that only a hierarchy based on technical knowledge in the workplace remains. Every member would have decision-making power in the firm and would be able to participate in establishing its overall policy objectives. The policies/goals would be carried out by the technical specialists that form the coordinating hierarchy of the firm, who would establish plans or directives for the work community to accomplish these goals.[308]		The role and use of money in a hypothetical socialist economy is a contested issue. According to the Austrian school economist Ludwig von Mises, an economic system that does not use money, financial calculation and market pricing will be unable to effectively value capital goods and coordinate production, and therefore these types of socialism are impossible because they lack the necessary information to perform economic calculation in the first place.[309][310] Socialists including Karl Marx, Robert Owen, Pierre-Joseph Proudhon and John Stuart Mill advocated various forms of labour vouchers or labour-credits, which like money would be used to acquire articles of consumption, but unlike money, they are unable to become capital and would not be used to allocate resources within the production process. Bolshevik revolutionary Leon Trotsky argued that money could not be arbitrarily abolished following a socialist revolution. Money had to exhaust its "historic mission", meaning it would have to be used until its function became redundant, eventually being transformed into bookkeeping receipts for statisticians, and only in the more distant future would money not be required for even that role.[311]		The economic anarchy of capitalist society as it exists today is, in my opinion, the real source of the evil... I am convinced there is only one way to eliminate these grave evils, namely through the establishment of a socialist economy, accompanied by an educational system which would be oriented toward social goals. In such an economy, the means of production are owned by society itself and are utilised in a planned fashion. A planned economy, which adjusts production to the needs of the community, would distribute the work to be done among all those able to work and would guarantee a livelihood to every man, woman, and child. The education of the individual, in addition to promoting his own innate abilities, would attempt to develop in him a sense of responsibility for his fellow men in place of the glorification of power and success in our present society.		A planned economy is a type of economy consisting of a mixture of public ownership of the means of production and the coordination of production and distribution through economic planning. There are two major types of planning: decentralised-planning and centralised-planning. Enrico Barone provided a comprehensive theoretical framework for a planned socialist economy. In his model, assuming perfect computation techniques, simultaneous equations relating inputs and outputs to ratios of equivalence would provide appropriate valuations in order to balance supply and demand.[313]		The most prominent example of a planned economy was the economic system of the Soviet Union, and as such, the centralised-planned economic model is usually associated with the Communist states of the 20th century, where it was combined with a single-party political system. In a centrally planned economy, decisions regarding the quantity of goods and services to be produced are planned in advance by a planning agency. (See also: Analysis of Soviet-type economic planning). The economic systems of the Soviet Union and the Eastern Bloc are further classified as command economies, which are defined as systems where economic coordination is undertaken by commands, directives and production targets.[314] Studies by economists of various political persuasions on the actual functioning of the Soviet economy indicate that it was not actually a planned economy. Instead of conscious planning, the Soviet economy was based on a process whereby the plan was modified by localised agents and the original plans went largely unfulfilled. Planning agencies, ministries and enterprises all adapted and bargained with each other during the formulation of the plan as opposed to following a plan passed down from a higher authority, leading some economists to suggest that planning did not actually take place within the Soviet economy and that a better description would be an "administered" or "managed" economy.[315]		Although central planning was largely supported by Marxist–Leninists, some factions within the Soviet Union before the rise of Stalinism held positions contrary to central planning. Leon Trotsky rejected central planning in favour of decentralised planning. He argued that central planners, regardless of their intellectual capacity, would be unable to coordinate effectively all economic activity within an economy because they operated without the input and tacit knowledge embodied by the participation of the millions of people in the economy. As a result, central planners would be unable to respond to local economic conditions.[316] State socialism is unfeasible in this view because information cannot be aggregated by a central body and effectively used to formulate a plan for an entire economy, because doing so would result in distorted or absent price signals.[317]		A self-managed, decentralised economy is based on autonomous self-regulating economic units and a decentralised mechanism of resource allocation and decision-making. This model has found support in notable classical and neoclassical economists including Alfred Marshall, John Stuart Mill and Jaroslav Vanek. There are numerous variations of self-management, including labour-managed firms and worker-managed firms. The goals of self-management are to eliminate exploitation and reduce alienation.[318] Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public".[319] It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century.[319] It was strongly associated with G. D. H. Cole and influenced by the ideas of William Morris.		One such system is the cooperative economy, a largely free market economy in which workers manage the firms and democratically determine remuneration levels and labour divisions. Productive resources would be legally owned by the cooperative and rented to the workers, who would enjoy usufruct rights.[320] Another form of decentralised planning is the use of cybernetics, or the use of computers to manage the allocation of economic inputs. The socialist-run government of Salvador Allende in Chile experimented with Project Cybersyn, a real-time information bridge between the government, state enterprises and consumers.[321] Another, more recent, variant is participatory economics, wherein the economy is planned by decentralised councils of workers and consumers. Workers would be remunerated solely according to effort and sacrifice, so that those engaged in dangerous, uncomfortable, and strenuous work would receive the highest incomes and could thereby work less.[322] A contemporary model for a self-managed, non-market socialism is Pat Devine's model of negotiated coordination. Negotiated coordination is based upon social ownership by those affected by the use of the assets involved, with decisions made by those at the most localised level of production.[323]		Michel Bauwens identifies the emergence of the open software movement and peer-to-peer production as a new, alternative mode of production to the capitalist economy and centrally planned economy that is based on collaborative self-management, common ownership of resources, and the production of use-values through the free cooperation of producers who have access to distributed capital.[324]		Anarchist communism is a theory of anarchism which advocates the abolition of the state, private property, and capitalism in favour of common ownership of the means of production.[325][326] Anarcho-syndicalism was practiced in Catalonia and other places in the Spanish Revolution during the Spanish Civil War. Sam Dolgoff estimated that about eight million people participated directly or at least indirectly in the Spanish Revolution.[327]		The economy of the former Socialist Federal Republic of Yugoslavia established a system based on market-based allocation, social ownership of the means of production and self-management within firms. This system substituted Yugoslavia's Soviet-type central planning with a decentralised, self-managed system after reforms in 1953.[328]		The Marxian economist Richard D. Wolff argues that "re-organising production so that workers become collectively self-directed at their work-sites" not only moves society beyond both capitalism and state socialism of the last century, but would also mark another milestone in human history, similar to earlier transitions out of slavery and feudalism.[329] As an example, Wolff claims that Mondragon is "a stunningly successful alternative to the capitalist organisation of production."[330]		State socialism can be used to classify any variety of socialist philosophies that advocates the ownership of the means of production by the state apparatus, either as a transitional stage between capitalism and socialism, or as an end-goal in itself. Typically it refers to a form of technocratic management, whereby technical specialists administer or manage economic enterprises on behalf of society (and the public interest) instead of workers' councils or workplace democracy.		A state-directed economy may refer to a type of mixed economy consisting of public ownership over large industries, as promoted by various Social democratic political parties during the 20th century. This ideology influenced the policies of the British Labour Party during Clement Attlee's administration. In the biography of the 1945 UK Labour Party Prime Minister Clement Attlee, Francis Beckett states: "the government... wanted what would become known as a mixed economy".[331]		Nationalisation in the UK was achieved through compulsory purchase of the industry (i.e. with compensation). British Aerospace was a combination of major aircraft companies British Aircraft Corporation, Hawker Siddeley and others. British Shipbuilders was a combination of the major shipbuilding companies including Cammell Laird, Govan Shipbuilders, Swan Hunter, and Yarrow Shipbuilders; the nationalisation of the coal mines in 1947 created a coal board charged with running the coal industry commercially so as to be able to meet the interest payable on the bonds which the former mine owners' shares had been converted into.[332][333]		Market socialism consists of publicly owned or cooperatively owned enterprises operating in a market economy. It is a system that utilises the market and monetary prices for the allocation and accounting of the means of production, thereby retaining the process of capital accumulation. The profit generated would be used to directly remunerate employees, collectively sustain the enterprise or finance public institutions.[334] In state-oriented forms of market socialism, in which state enterprises attempt to maximise profit, the profits can be used to fund government programs and services through a social dividend, eliminating or greatly diminishing the need for various forms of taxation that exist in capitalist systems. The neoclassical economist Léon Walras believed that a socialist economy based on state ownership of land and natural resources would provide a means of public finance to make income taxes unnecessary.[335] Yugoslavia implemented a market socialist economy based on cooperatives and worker self-management.		Mutualism is an economic theory and anarchist school of thought that advocates a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labour in the free market.[336] Integral to the scheme was the establishment of a mutual-credit bank that would lend to producers at a minimal interest rate, just high enough to cover administration.[337] Mutualism is based on a labour theory of value that holds that when labour or its product is sold, in exchange, it ought to receive goods or services embodying "the amount of labour necessary to produce an article of exactly similar and equal utility".[338]		The current economic system in China is formally referred to as a socialist market economy with Chinese characteristics. It combines a large state sector that comprises the 'commanding heights' of the economy, which are guaranteed their public ownership status by law,[339] with a private sector mainly engaged in commodity production and light industry responsible from anywhere between 33%[340] (People's Daily Online 2005) to over 70% of GDP generated in 2005.[341] Although there has been a rapid expansion of private-sector activity since the 1980s, privatisation of state assets was virtually halted and were partially reversed in 2005.[342] The current Chinese economy consists of 150 corporatised state-owned enterprises that report directly to China's central government.[343] By 2008, these state-owned corporations had become increasingly dynamic and generated large increases in revenue for the state,[344][345] resulting in a state-sector led recovery during the 2009 financial crises while accounting for most of China's economic growth.[346] However, the Chinese economic model is widely cited as a contemporary form of state capitalism, the major difference between Western capitalism and the Chinese model being the degree of state-ownership of shares in publicly listed corporations.		The Socialist Republic of Vietnam has adopted a similar model after the Doi Moi economic renovation, but slightly differs from the Chinese model in that the Vietnamese government retains firm control over the state sector and strategic industries, but allows for private-sector activity in commodity production.[347]		The major socialist political movements are described below. Independent socialist theorists, utopian socialist authors, and academic supporters of socialism may not be represented in these movements. Some political groups have called themselves socialist while holding views that some consider antithetical to socialism. The term socialist has also been used by some politicians on the political right as an epithet against certain individuals who do not consider themselves to be socialists, and against policies that are not considered socialist by their proponents.		There are many variations of socialism and as such there is no single definition encapsulating all of socialism. However, there have been common elements identified by scholars.[348] Angelo S. Rappoport in his Dictionary of Socialism (1924) analysed forty definitions of socialism to conclude that common elements of socialism include: general criticisms of the social effects of private ownership and control of capital – as being the cause of poverty, low wages, unemployment, economic and social inequality, and a lack of economic security; a general view that the solution to these problems is a form of collective control over the means of production, distribution and exchange (the degree and means of control vary amongst socialist movements); agreement that the outcome of this collective control should be a society based upon social justice, including social equality, economic protection of people, and should provide a more satisfying life for most people.[349] Bhikhu Parekh in The Concepts of Socialism (1975) identifies four core principles of socialism and particularly socialist society: sociality, social responsibility, cooperation, and planning.[350] Michael Freeden in his study Ideologies and Political Theory (1996) states that all socialists share five themes: the first is that socialism posits that society is more than a mere collection of individuals; second, that it considers human welfare a desirable objective; third, that it considers humans by nature to be active and productive; fourth, it holds the belief of human equality; and fifth, that history is progressive and will create positive change on the condition that humans work to achieve such change.[350]		Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions,[351][352][353][354] but that several authors have defined as more specific institutions based on non-hierarchical free associations.[355][356][357][358] Anarchism holds the state to be undesirable, unnecessary, or harmful.[359][360] While anti-statism is central, some argue[361] that anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including, but not limited to, the state system.[355][362][363][364][365][366][367] Mutualists advocate market socialism, collectivist anarchists workers cooperatives and salaries based on the amount of time contributed to production, anarcho-communists advocate a direct transition from capitalism to libertarian communism and a gift economy and anarcho-syndicalists worker's direct action and the general strike.		Modern democratic socialism is a broad political movement that seeks to promote the ideals of socialism within the context of a democratic system. Some Democratic socialists support social democracy as a temporary measure to reform the current system, while others reject reformism in favour of more revolutionary methods. Modern social democracy emphasises a program of gradual legislative modification of capitalism in order to make it more equitable and humane, while the theoretical end goal of building a socialist society is either completely forgotten or redefined in a pro-capitalist way. The two movements are widely similar both in terminology and in ideology, although there are a few key differences.		The major difference between social democracy and democratic socialism is the object of their politics: contemporary social democrats support a welfare state and unemployment insurance as a means to "humanise" capitalism, whereas democratic socialists seek to replace capitalism with a socialist economic system, arguing that any attempt to "humanise" capitalism through regulations and welfare policies would distort the market and create economic contradictions.[368]		Democratic socialism generally refers to any political movement that seeks to establish an economy based on economic democracy by and for the working class. Democratic socialism is difficult to define, and groups of scholars have radically different definitions for the term. Some definitions simply refer to all forms of socialism that follow an electoral, reformist or evolutionary path to socialism, rather than a revolutionary one.[369]		You can't talk about ending the slums without first saying profit must be taken out of slums. You're really tampering and getting on dangerous ground because you are messing with folk then. You are messing with captains of industry. Now this means that we are treading in difficult water, because it really means that we are saying that something is wrong with capitalism. There must be a better distribution of wealth, and maybe America must move toward a democratic socialism.		Blanquism refers to a conception of revolution generally attributed to Louis Auguste Blanqui which holds that socialist revolution should be carried out by a relatively small group of highly organised and secretive conspirators.[373] Having seized power, the revolutionaries would then use the power of the state to introduce socialism. It is considered a particular sort of 'putschism' – that is, the view that political revolution should take the form of a putsch or coup d'état.[374] Rosa Luxemburg and Eduard Bernstein[375] have criticised Lenin that his conception of revolution was elitist and essentially 'Blanquist'.[376] Marxism–Leninism is a political ideology combining Marxism (the scientific socialist concepts theorised by Karl Marx and Friedrich Engels) and Leninism (Vladimir Lenin's theoretical expansions of Marxism which include anti-imperialism, democratic centralism, and party-building principles).[377] Marxism–Leninism was the official ideology of the Communist Party of the Soviet Union and of the Communist International (1919–43) and later it became the main guiding ideology for Trotskyists, Maoists, and Stalinists.		Libertarian socialism (sometimes called social anarchism,[380][381] left-libertarianism[382][383] and socialist libertarianism[384]) is a group of anti-authoritarian[385] political philosophies inside the socialist movement that rejects socialism as centralised state ownership and control of the economy[386] including criticism of wage labour relationships within the workplace,[387] as well as the state itself.[388] It emphasises workers' self-management of the workplace[388] and decentralised structures of political organization,[389] asserting that a society based on freedom and equality can be achieved through abolishing authoritarian institutions that control certain means of production and subordinate the majority to an owning class or political and economic elite.[390] Libertarian socialists generally place their hopes in decentralised means of direct democracy and federal or confederal associations such as libertarian municipalism, citizens' assemblies, trade unions, and workers' councils.[391][392] Relatedly, anarcho-syndicalist Gaston Leval explained: "We therefore foresee a Society in which all activities will be coordinated, a structure that has, at the same time, sufficient flexibility to permit the greatest possible autonomy for social life, or for the life of each enterprise, and enough cohesiveness to prevent all disorder...In a well-organized society, all of these things must be systematically accomplished by means of parallel federations, vertically united at the highest levels, constituting one vast organism in which all economic functions will be performed in solidarity with all others and that will permanently preserve the necessary cohesion." All of this is generally done within a general call for libertarian[393] and voluntary human relationships[394] through the identification, criticism, and practical dismantling of illegitimate authority in all aspects of human life.[399] As such, libertarian socialism, within the larger socialist movement, seeks to distinguish itself both from Leninism/Bolshevism and from social democracy.[400]		Past and present political philosophies and movements commonly described as libertarian socialist include anarchism (especially anarchist communism, anarchist collectivism, anarcho-syndicalism,[401] and mutualism[402]) as well as autonomism, communalism, participism, revolutionary syndicalism, and libertarian Marxist philosophies such as council communism and Luxemburgism;[403] as well as some versions of "utopian socialism"[404] and individualist anarchism.[405][406][407]		Christian socialism is a broad concept involving an intertwining of the Christian religion with the politics and economic theories of socialism.		Islamic socialism is a term coined by various Muslim leaders to describe a more spiritual form of socialism. Muslim socialists believe that the teachings of the Qur'an and Muhammad are compatible with principles of equality and public ownership drawing inspiration from the early Medina welfare state established by Muhammad. Muslim Socialists are more conservative than their western contemporaries and find their roots in Anti-imperialism, anti-colonialism and Arab nationalism. Islamic Socialist leaders believe in Democracy and deriving legitimacy from public mandate as opposed to religious texts.		Social democracy is a political ideology which "is derived from a socialist tradition of political thought. Many social democrats refer to themselves as socialists or democratic socialists, and some use these terms interchangeably. Others have opined that there are clear differences between the three terms, and preferred to describe their own political beliefs by using the term ‘social democracy’ only."[408] There are two main directions, either to establish democratic socialism, or to build a welfare state within the framework of the capitalist system. The first variant has officially its goal by establishing democratic socialism through reformist and gradualist methods.[409] In the second variant Social democracy becomes a policy regime involving a welfare state, collective bargaining schemes, support for publicly financed public services, and a Capitalist-based economy like a mixed economy. It is often used in this manner to refer to the social models and economic policies prominent in Western and Northern Europe during the later half of the 20th century.[410][411] It has been described by Jerry Mander as "hybrid" economics, an active collaboration of capitalist and socialist visions, and, while such systems aren't perfect, they tend to provide high standards of living.[412] Numerous studies and surveys indicate that people tend to live happier lives in social democratic societies rather than neoliberal ones.[413][414][415][416]		Social democrats supporting the first variant, advocate for a peaceful, evolutionary transition of the economy to socialism through progressive social reform of capitalism.[417][418] It asserts that the only acceptable constitutional form of government is representative democracy under the rule of law.[419] It promotes extending democratic decision-making beyond political democracy to include economic democracy to guarantee employees and other economic stakeholders sufficient rights of co-determination.[419] It supports a mixed economy that opposes the excesses of capitalism such as inequality, poverty, and oppression of various groups, while rejecting both a totally free market or a fully planned economy.[420] Common social democratic policies include advocacy of universal social rights to attain universally accessible public services such as education, health care, workers' compensation, and other services, including child care and care for the elderly.[421] Social democracy is connected with the trade union labour movement and supports collective bargaining rights for workers.[422] Most social democratic parties are affiliated with the Socialist International.[409]		Liberal socialism is a socialist political philosophy that includes liberal principles within it.[423] Liberal socialism does not have the goal of abolishing capitalism with a socialist economy;[424] instead, it supports a mixed economy that includes both public and private property in capital goods.[425][426] Although liberal socialism unequivocally favors a mixed market economy, it identifies legalistic and artificial monopolies to be the fault of capitalism[427] and opposes an entirely unregulated economy.[428] It considers both liberty and equality to be compatible and mutually dependent on each other.[423] Principles that can be described as "liberal socialist" have been based upon or developed by the following philosophers: John Stuart Mill, Eduard Bernstein, John Dewey, Carlo Rosselli, Norberto Bobbio, and Chantal Mouffe.[429] Other important liberal socialist figures include Guido Calogero, Piero Gobetti, Leonard Trelawny Hobhouse, John Maynard Keynes, and R. H. Tawney.[428] Liberal socialism has been particularly prominent in British and Italian politics.[428]		Socialist feminism is a branch of feminism that focuses upon both the public and private spheres of a woman's life and argues that liberation can only be achieved by working to end both the economic and cultural sources of women's oppression.[430] Marxist feminism's foundation is laid by Friedrich Engels in his analysis of gender oppression in The Origin of the Family, Private Property, and the State (1884). August Bebel's Woman under Socialism (1879), the "single work dealing with sexuality most widely read by rank-and-file members of the Social Democratic Party of Germany (SPD)",.[431] In the late nineteenth and early twentieth centuries, both Clara Zetkin and Eleanor Marx were against the demonisation of men and supported a proletariat revolution that would overcome as many male–female inequalities as possible.[432] As their movement already had the most radical demands in women's equality, most Marxist leaders, including Clara Zetkin[433][434] and Alexandra Kollontai,[435][436] counterposed Marxism against liberal feminism, rather than trying to combine them. Anarcha-feminism began with late 19th and early 20th century authors and theorists such as anarchist feminists Emma Goldman and Voltairine de Cleyre[437] In the Spanish Civil War, an anarcha-feminist group, Mujeres Libres ("Free Women") linked to the Federación Anarquista Ibérica, organised to defend both anarchist and feminist ideas.[438] In 1972, the Chicago Women's Liberation Union published "Socialist Feminism: A Strategy for the Women's Movement," which is believed to be the first to use the term "socialist feminism," in publication.[439]		Many socialists were early advocates for LGBT rights. For early socialist Charles Fourier, true freedom could only occur without suppressing passions; the suppression of passions is not only destructive to the individual, but to society as a whole. Writing before the advent of the term 'homosexuality', Fourier recognised that both men and women have a wide range of sexual needs and preferences which may change throughout their lives, including same-sex sexuality and androgénité. He argued that all sexual expressions should be enjoyed as long as people are not abused, and that "affirming one's difference" can actually enhance social integration.[440] In Oscar Wilde's The Soul of Man Under Socialism, he passionately advocates for an egalitarian society where wealth is shared by all, while warning of the dangers of social systems that crush individuality. Wilde's libertarian socialist politics were shared by other figures who actively campaigned for homosexual emancipation in the late 19th century such as Edward Carpenter.[441] The Intermediate Sex: A Study of Some Transitional Types of Men and Women was a book from 1908 and an early work arguing for gay liberation written by Edward Carpenter[442] who was also an influential personality in the foundation of the Fabian Society and the Labour Party. After the Russian Revolution under the leadership of Vladimir Lenin and Leon Trotsky, the Soviet Union abolished previous laws against homosexuality.[443] Harry Hay was an early leader in the American LGBT rights movement as well as a member of the Communist Party USA. He is known for his roles in helping to found several gay organisations, including the Mattachine Society, the first sustained gay rights group in the United States which in its early days had a strong marxist influence. The Encyclopedia of Homosexuality reports that "As Marxists the founders of the group believed that the injustice and oppression which they suffered stemmed from relationships deeply embedded in the structure of American society".[444] Also emerging from a number of events, such as the May 1968 insurrection in France, the anti-Vietnam war movement in the US and the Stonewall riots of 1969, militant Gay Liberation organisations began to spring up around the world. Many saw their roots in left radicalism more than in the established homophile groups of the time,[445] The Gay Liberation Front took an anti-capitalist stance and attacked the nuclear family and traditional gender roles.[446]		Eco-socialism, green socialism or socialist ecology is a political position merging aspects of Marxism, socialism, and/or libertarian socialism with that of green politics, ecology and alter-globalisation. Eco-socialists generally believe that the expansion of the capitalist system is the cause of social exclusion, poverty, war and environmental degradation through globalisation and imperialism, under the supervision of repressive states and transnational structures.[447] Contrary to the depiction of Karl Marx by some environmentalists,[448] social ecologists[449] and fellow socialists[450] as a productivist who favoured the domination of nature, eco-socialists have revisited Marx's writings and believe that he "was a main originator of the ecological world-view".[451] Eco-socialist authors, like John Bellamy Foster[452] and Paul Burkett,[453] point to Marx's discussion of a "metabolic rift" between man and nature, his statement that "private ownership of the globe by single individuals will appear quite absurd as private ownership of one man by another" and his observation that a society must "hand it [the planet] down to succeeding generations in an improved condition".[454] The English socialist William Morris is largely credited with developing key principles of what was later called eco-socialism.[455] During the 1880s and 1890s, Morris promoted his eco-socialist ideas within the Social Democratic Federation and Socialist League.[456] Green anarchism, or ecoanarchism, is a school of thought within anarchism which puts a particular emphasis on environmental issues. An important early influence was the thought of the American anarchist Henry David Thoreau and his book Walden[457] and Élisée Reclus.[458][459]		In the late 19th century there emerged anarcho-naturism as the fusion of anarchism and naturist philosophies within individualist anarchist circles in France, Spain, Cuba[460] and Portugal.[461] Social ecology is closely related to the work and ideas of Murray Bookchin and influenced by anarchist Peter Kropotkin. Bookchin's first book, Our Synthetic Environment, was published under the pseudonym Lewis Herber in 1962, a few months before Rachel Carson's Silent Spring.[462] His groundbreaking essay "Ecology and Revolutionary Thought" introduced ecology as a concept in radical politics.[463] In the 1970s, Barry Commoner, suggesting a left-wing response to the Limits to Growth model that predicted catastrophic resource depletion and spurred environmentalism, postulated that capitalist technologies were chiefly responsible for environmental degradation, as opposed to population pressures.[464] The 1990s saw the socialist feminists Mary Mellor[465] and Ariel Salleh[466] address environmental issues within an eco-socialist paradigm. With the rising profile of the anti-globalisation movement in the Global South, an "environmentalism of the poor", combining ecological awareness and social justice, has also become prominent.[467] David Pepper also released his important work, Ecosocialism: From Deep Ecology to Social Justice, in 1994, which critiques the current approach of many within Green politics, particularly deep ecologists.[468] Currently, many Green Parties around the world, such as the Dutch Green Left Party (GroenLinks), contain strong eco-socialist elements. Radical Red-green alliances have been formed in many countries by eco-socialists, radical Greens and other radical left groups. In Denmark, the Red-Green Alliance was formed as a coalition of numerous radical parties. Within the European Parliament, a number of far-left parties from Northern Europe have organised themselves into the Nordic Green Left Alliance.		Syndicalism is a social movement that operates through industrial trade unions and rejects state socialism and the use of establishment politics to establish or promote socialism. They reject using state power to construct a socialist society, favouring strategies such as the general strike. Syndicalists advocate a socialist economy based on federated unions or syndicates of workers who own and manage the means of production. Some Marxist currents advocate Syndicalism, such as DeLeonism. Anarcho-syndicalism is a theory of anarchism which views syndicalism as a method for workers in capitalist society to gain control of an economy and, with that control, influence broader society. The Spanish Revolution, largely orchestrated by the anarcho-syndicalist trade union CNT during the Spanish Civil War offers an historical example.[469] The International Workers' Association is an international federation of anarcho-syndicalist labor unions and initiatives.		Upon what point are orthodox political economy and socialism in absolute conflict? Political economy has held and holds that the economic laws governing the production and distribution of wealth which it has established are natural laws ... not in the sense that they are laws naturally determined by the condition of the social organism (which would be correct), but that they are absolute laws, that is to say that they apply to humanity at all times and in all places, and consequently, that they are immutable in their principal points, though they may be subject to modification in details. Scientific socialism holds, the contrary, that the laws established by classical political economy, since the time of Adam Smith, are laws peculiar to the present period in the history of civilized humanity, and that they are, consequently, laws essentially relative to the period of their analysis and discovery.		
New South Wales (abbreviated as NSW) is a state on the east coast of Australia. It borders Queensland to the north, Victoria to the south, and South Australia to the west. Its coast borders the Tasman Sea to the east. The Australian Capital Territory is an enclave within the state. New South Wales' state capital is Sydney, which is also Australia's most populous city. In March 2014[update], the estimated population of New South Wales was 7.5 million,[9] making it Australia's most populous state. Just under two-thirds of the state's population, 4.67 million, live in the Greater Sydney area.[10] Inhabitants of New South Wales are referred to as New South Welshmen.[1][2]		The Colony of New South Wales was founded as a penal colony in 1788. It originally comprised more than half of the Australian mainland with its western boundary set at 129th meridian east in 1825. The colony also included the island territories of New Zealand, Van Diemen's Land, Lord Howe Island, and Norfolk Island. During the 19th century, most of the colony's area was detached to form separate British colonies that eventually became New Zealand and the various states and territories of Australia. However, the Swan River Colony has never been administered as part of New South Wales.		Lord Howe Island remains part of New South Wales, while Norfolk Island has become a federal territory, as have the areas now known as the Australian Capital Territory and the Jervis Bay Territory.						The prior inhabitants of New South Wales were the Aboriginal tribes who arrived in Australia about 40,000 to 60,000 years ago. Before European settlement there were an estimated 250,000 Aboriginal people in the region.[11]		The Wodi Wodi people are the original custodians of the Illawarra region of South Sydney.[12] Speaking a variant of the Dharawal language, the Wodi Wodi people lived across a large stretch of land which was roughly surrounded by what is now known as Campbelltown, Shoalhaven River and Moss Vale.[12]		The Bundjalung people are the original custodians of parts of the northern coastal areas.		The European discovery of New South Wales was made by Captain James Cook during his 1770 survey along the unmapped eastern coast of the Dutch-named continent of New Holland, now Australia. In his original journal(s) covering the survey, in triplicate to satisfy Admiralty Orders, Cook first named the land "New Wales", named after Wales. However, in the copy held by the Admiralty, he "revised the wording" to "New South Wales".[13]		The first British settlement was made by what is known in Australian history as the First Fleet; this was led by Captain Arthur Phillip, who assumed the role of governor of the settlement on arrival in 1788 until 1792.[14][15]		After years of chaos and anarchy after the overthrow of Governor William Bligh, a new governor, Lieutenant-Colonel (later Major-General) Lachlan Macquarie, was sent from Britain to reform the settlement in 1809.[16] During his time as governor, Macquarie commissioned the construction of roads, wharves, churches and public buildings, sent explorers out from Sydney and employed a planner to design the street layout of Sydney. Macquarie's legacy is still evident today.		During the 19th century, large areas were successively separated to form the British colonies of Tasmania (proclaimed as a separate colony named Van Diemen's Land in 1825), South Australia (1836), Victoria (1851) and Queensland (1859). Responsible government was granted to the New South Wales colony in 1855. Following the Treaty of Waitangi, William Hobson declared British sovereignty over New Zealand in 1840. In 1841 it was separated from the Colony of New South Wales to form the new Colony of New Zealand.		Charles Darwin visited Australia in January 1836 and in The Voyage of the Beagle (chapter 19 of the 11th edition) records his hesitations about and fascination with New South Wales, including his speculations about the geological origin and formation of the great valleys, the aboriginal population, the situation of the convicts, and the future prospects of the country.		At the end of the 19th century, the movement toward federation between the Australian colonies gathered momentum. Conventions and forums involving colony leaders were held on a regular basis. Proponents of New South Wales as a free trade state were in dispute with the other leading colony Victoria, which had a protectionist economy. At this time customs posts were common on borders, even on the Murray River.		Travelling from NSW to Victoria in those days was very difficult. Supporters of federation included the NSW premier Sir Henry Parkes whose 1889 Tenterfield Speech (given in Tenterfield) was pivotal in gathering support for NSW involvement. Edmund Barton, later to become Australia's first Prime Minister, was another strong advocate for federation and a meeting held in Corowa in 1893 drafted an initial constitution.		In 1898 popular referenda on the proposed federation were held in NSW, Victoria, South Australia and Tasmania. All votes resulted in a majority in favour, but the NSW government under Premier George Reid (popularly known as "yes–no Reid" because of his constant changes of opinion on the issue) had set a requirement for a higher "yes" vote than just a simple majority which was not met.		In 1899 further referenda were held in the same states as well as Queensland (but not Western Australia). All resulted in yes votes with majorities increased from the previous year. NSW met the conditions its government had set for a yes vote. As a compromise to the question on where the capital was to be located, an agreement was made that the site was to be within NSW but not closer than 100 miles (161 km) from Sydney, while the provisional capital would be Melbourne. Eventually the area that now forms the Australian Capital Territory was ceded by NSW when Canberra was selected.		In the years after World War I, the high prices enjoyed during the war fell with the resumption of international trade. Farmers became increasingly discontented with the fixed prices paid by the compulsory marketing authorities set up as a wartime measure by the Hughes government. In 1919 the farmers formed the Country Party, led at national level by Earle Page, a doctor from Grafton, and at state level by Michael Bruxner, a small farmer from Tenterfield.		The Great Depression, which began in 1929, ushered in a period of political and class conflict in New South Wales. The mass unemployment and collapse of commodity prices brought ruin to both city workers and to farmers. The beneficiary of the resultant discontent was not the Communist Party, which remained small and weak, but Jack Lang's Labor populism. Lang's second government was elected in November 1930 on a policy of repudiating New South Wales' debt to British bondholders and using the money instead to help the unemployed through public works. This was denounced as illegal by conservatives, and also by James Scullin's federal Labor government. The result was that Lang's supporters in the federal Caucus brought down Scullin's government, causing a second bitter split in the Labor Party. In May 1932 the Governor, Sir Philip Game dismissed his government. The subsequent election was won by the conservative opposition.		By the outbreak of World War II in 1939, the differences between New South Wales and the other states that had emerged in the 19th century had faded as a result of federation and economic development behind a wall of protective tariffs.[citation needed] New South Wales continued to outstrip Victoria as the centre of industry, and increasingly of finance and trade as well.[citation needed] Labor returned to office under the moderate leadership of William McKell in 1941 and remained in power for 24 years. World War II saw another surge in industrial development to meet the needs of a war economy, and also the elimination of unemployment.		Labor stayed in power until 1965. Towards the end of its term in power it announced a plan for the construction of an opera/arts facility on Bennelong Point. The design competition was won by Jørn Utzon. Controversy over the cost of what would eventually become the Sydney Opera House became a political issue and was a factor in the eventual defeat of Labor in 1965 by the conservative Liberal Party led by Sir Robert Askin. Sir Robert remains a controversial figure with supporters claiming him to be reformist especially in terms of reshaping the NSW economy. Others though, regard the Askin era as synonymous with corruption with Askin the head of a network involving NSW police and SP bookmaking (Goot).		In the late 1960s a secessionist movement in the New England region of the state led to a referendum on the issue. The new state would have consisted of much of northern NSW including Newcastle. The referendum was narrowly defeated and, as of 2010[update], there are no active or organised campaigns for new states in NSW.		Askin's resignation in 1975 was followed by a number of short lived premierships by Liberal Party leaders. When a general election came in 1976 the ALP under Neville Wran were returned to power. Wran was able to transform this narrow one seat victory into landslide wins (known as Wranslide) in 1978 and 1981.[citation needed]		After winning a comfortable though reduced majority in 1984, Wran resigned as premier and left parliament. His replacement Barrie Unsworth struggled to emerge from Wran's shadow and lost a 1988 election against a resurgent Liberal Party led by Nick Greiner. Unsworth was replaced as ALP leader by Bob Carr. Initially Greiner was a popular leader instigating reform such as the creation of the Independent Commission Against Corruption (ICAC). Greiner called a snap election in 1991 which the Liberals were expected to win. However the ALP polled extremely well and the Liberals lost their majority and needed the support of independents to retain power.		Greiner was accused (by ICAC) of corrupt actions involving an allegation that a government position was offered to tempt an independent (who had defected from the Liberals) to resign his seat so that the Liberal party could regain it and shore up its numbers. Greiner resigned but was later cleared of corruption. His replacement as Liberal leader and Premier was John Fahey whose government secured Sydney the right to host the 2000 Summer Olympics. In the 1995 election, Fahey's government lost narrowly and the ALP under Bob Carr returned to power.		Like Wran before him Carr was able to turn a narrow majority into landslide wins at the next two elections (1999 and 2003). During this era, NSW hosted the 2000 Sydney Olympics which were internationally regarded as very successful, and helped boost Carr's popularity. Carr surprised most people by resigning from office in 2005. He was replaced by Morris Iemma, who remained Premier after being re-elected in the March 2007 state election, until he was replaced by Nathan Rees in September 2008.[17] Rees was subsequently replaced by Kristina Keneally in December 2009.[18] Keneally's government was defeated at the 2011 state election and Barry O'Farrell became Premier on 28 March. On 17 April 2014 O'Farrell stood down as Premier after misleading an ICAC investigation concerning a gift of a bottle of wine. The Liberal Party then elected Treasurer Mike Baird as party leader and Premier. Baird resigned as Premier on 23 January 2017, and was replaced by Gladys Berejiklian.		Executive authority is vested in the Governor of New South Wales, who represents and is appointed by Elizabeth II, Queen of Australia. The current Governor is David Hurley. The Governor commissions as Premier the leader of the parliamentary political party that can command a simple majority of votes in the Legislative Assembly. The Premier then recommends the appointment of other Members of the two Houses to the Ministry, under the principle of responsible or Westminster government. It should be noted, however, that as in other Westminster systems, there is no constitutional requirement in NSW for the Government to be formed from the Parliament—merely convention. The Premier is Gladys Berejiklian of the Liberal Party.[18]		The form of the Government of New South Wales is prescribed in its Constitution, dating from 1856 and currently the Constitution Act 1902 (NSW).[19] Since 1901 New South Wales has been a state of the Commonwealth of Australia, and the Australian Constitution regulates its relationship with the Commonwealth.		In 2006, the Constitution Amendment Pledge of Loyalty Act 2006 No 6,[20] was enacted to amend the NSW Constitution Act 1902 to require Members of the New South Wales Parliament and its Ministers to take a pledge of loyalty to Australia and to the people of New South Wales instead of swearing allegiance to Elizabeth II her heirs and successors, and to revise the oaths taken by Executive Councillors. The Pledge of Loyalty Act was officially assented to by the Queen on 3 April 2006.		Under the Australian Constitution, New South Wales ceded certain legislative and judicial powers to the Commonwealth, but retained independence in all other areas. The New South Wales Constitution says: "The Legislature shall, subject to the provisions of the Commonwealth of Australia Constitution Act, have power to make laws for the peace, welfare, and good government of New South Wales in all cases whatsoever".[21]		The first "responsible" self-government of New South Wales was formed on 6 June 1856 with Sir Stuart Alexander Donaldson appointed by Governor Sir William Denison as its first Colonial Secretary which in those days accounted also as the Premier.[22] The State Parliament is composed of the Sovereign and two houses: the Legislative Assembly (lower house), and the Legislative Council (upper house). Elections are held every four years on the fourth Saturday of March, the most recent being on 28 March 2015. At each election one member is elected to the Legislative Assembly from each of 93 electoral districts and half of the 42 members of the Legislative Council are elected by a statewide electorate.		New South Wales is divided into 128 local government areas. There is also the Unincorporated Far West Region which is not part of any local government area, in the sparsely inhabited Far West, and Lord Howe Island, which is also unincorporated but self-governed by the Lord Howe Island Board.		New South Wales is policed by the New South Wales Police Force, a statutory authority. Established in 1862, the NSW Police Force investigates Summary and Indictable offences throughout the State of New South Wales. The state has two fire services: the volunteer based New South Wales Rural Fire Service, which is responsible for the majority of the state, and the Fire and Rescue NSW, a government agency responsible for protecting urban areas. There is some overlap due to suburbanisation. Ambulance services are provided through the Ambulance Service of New South Wales. Rescue services (i.e. vertical, road crash, confinement) are a joint effort by all emergency services, with Ambulance Rescue, Police Rescue Squad and Fire Rescue Units contributing. Volunteer rescue organisations include the Australian Volunteer Coast Guard, State Emergency Service (SES), Surf Life Saving New South Wales and Volunteer Rescue Association (VRA).		The estimated population of New South Wales at the end of December 2016 was 7,797,800 people.[23]		The principal ancestries of New South Wales's residents (as surveyed in 2011) are:[24]		62.9% of NSW's population is based in Sydney.[25]		Passage through New South Wales is vital for cross-continent transport. Rail and road traffic from Brisbane (Queensland) to Perth (Western Australia), or to Melbourne (Victoria) must pass through New South Wales.		The majority of railways in New South Wales are currently operated by the state government. Some lines began as branch-lines of railways starting in other states. For instance, Balranald near the Victorian border was connected by a rail line coming up from Victoria and into New South Wales. Another line beginning in Adelaide crossed over the border and stopped at Broken Hill.		Railways management are conducted by Sydney Trains and NSW TrainLink[27] which maintain rolling stock. Sydney Trains operates trains within Sydney while NSW TrainLink operates outside Sydney, intercity, country and interstate services.		Major roads are the concern of both federal and state governments. The latter maintains these through the Department of Roads and Maritime Services, formerly the Roads and Traffic Authority, and before that, the Department of Main Roads (DMR).		The main roads in New South Wales are		Other roads are usually the concern of the RTA and/or the local government authority.		Kingsford Smith Airport (commonly Sydney Airport, and locally referred to as Mascot Airport or just 'Mascot'), located in the southern Sydney suburb of Mascot is the major airport for not just the state but the whole nation. It is a hub for Australia's national airline Qantas.		Other airlines serving regional New South Wales include:[28]		The state government through Sydney Ferries operates ferries within Sydney Harbour and the Parramatta River. It also has a ferry service within Newcastle.[32] All other ferry services are privately operated.[33]		Spirit of Tasmania ran a commercial ferry service between Sydney and Devonport, Tasmania. This service was terminated in 2006.[34]		Private boat services operated between South Australia, Victoria and New South Wales along the Murray and Darling Rivers but these only exist now as the occasional tourist paddle-wheeler service.[35]		The NSW school system comprises a kindergarten to year 12 system with primary schooling up to year 6 and secondary schooling between years 7 and 12. Schooling is compulsory until age 17.[36]		Primary and secondary schools include government and non-government schools. Government schools are further classified as comprehensive and selective schools. Non-government schools include Catholic schools, other denominational schools, and non-denominational independent schools.		Typically, a primary school provides education from kindergarten level to year 6. A secondary school, usually called a "high school", provides education from years 7 to 12. Secondary colleges are secondary schools which only cater for years 11 and 12.		The government classifies the 13 years of primary and secondary schooling into six stages, beginning with early stage 1 (Kindergarten) and ending with stage 6 (years 11 and 12).		The School Certificate was awarded by the Board of Studies to students at the end of Year 10. The Board of Studies administered five external tests in English-literacy, Mathematics, Science, Australian History, Geography, Civics and Citizenship. The tests were designed to grade a student on their ability. The results of this test were categorised into bands 1 through to 6 with band 1 as the lowest and band 6 as the highest.[37] Adrian Piccoli, the NSW Education Minister confirmed that School Certificate tests would not continue after 2011.[38]		The Higher School Certificate (HSC) is the usual Year 12 leaving certificate in NSW. Most students complete the HSC prior to entering the workforce or going on to study at university or TAFE (although the HSC itself can be completed at TAFE). The HSC must be completed for a student to get an Australian Tertiary Admission Rank (formerly Universities Admission Index), which determines the student's rank against fellow students who completed the Higher School Certificate.		Eleven universities primarily operate in New South Wales. Sydney is home to Australia's first university, the University of Sydney founded in 1850. Other universities include the University of New South Wales, Macquarie University, the University of Technology, Sydney and Western Sydney University. The Australian Catholic University has two of its six campuses in Sydney, and the private University of Notre Dame Australia also operates a secondary campus in the city.		Outside Sydney, the leading universities are the University of Newcastle and the University of Wollongong. Armidale is home to the University of New England, and Charles Sturt University and Southern Cross University have campuses spread across cities in the state's south-west and north coast respectively.		The public universities are state government agencies, however they are largely regulated by the federal government, which also administers their public funding. Admission to NSW universities is arranged together with universities in the Australian Capital Territory by another government agency, the Universities Admission Centre.		Primarily vocational training is provided up the level of advanced diplomas is provided by the state government's ten Technical and Further Education (TAFE) institutes. These institutes run courses in more than130 campuses throughout the state.		New South Wales is bordered on the north by Queensland, on the west by South Australia, on the south by Victoria and on the east by the Tasman Sea. The Australian Capital Territory and the Jervis Bay Territory form a separately administered entity that is bordered entirely by New South Wales. The state can be divided geographically into four areas. New South Wales' three largest cities, Sydney, Newcastle and Wollongong, lie near the centre of a narrow coastal strip extending from cool temperate areas on the far south coast to subtropical areas near the Queensland border.		The Illawarra region is centred on the city of Wollongong, with the Shoalhaven, Eurobodalla and the Sapphire Coast to the south. The Central Coast lies between Sydney and Newcastle, with the Mid North Coast and Northern Rivers regions reaching northwards to the Queensland border. Tourism is important to the economies of coastal towns such as Coffs Harbour, Lismore, Nowra and Port Macquarie, but the region also produces seafood, beef, dairy, fruit, sugar cane and timber.		The Great Dividing Range extends from Victoria in the south through New South Wales to Queensland, parallel to the narrow coastal plain. This area includes the Snowy Mountains, the Northern, Central and Southern Tablelands, the Southern Highlands and the South West Slopes. Whilst not particularly steep, many peaks of the range rise above 1,000 metres (3,281 ft), with the highest Mount Kosciuszko at 2,229 m (7,313 ft). Skiing in Australia began in this region at Kiandra around 1861. The relatively short ski season underwrites the tourist industry in the Snowy Mountains. Agriculture, particularly the wool industry, is important throughout the highlands. Major centres include Armidale, Bathurst, Bowral, Goulburn, Inverell, Orange, Queanbeyan and Tamworth.		There are numerous forests in New South Wales, with such tree species as Red Gum Eucalyptus and Crow Ash (Flindersia australis), being represented.[39] Forest floors have a diverse set of understory shrubs and fungi. One of the widespread fungi is Witch's Butter (Tremella mesenterica).[40]		The western slopes and plains fill a significant portion of the state's area and have a much sparser population than areas nearer the coast. Agriculture is central to the economy of the western slopes, particularly the Riverina region and Murrumbidgee Irrigation Area in the state's south-west. Regional cities such as Albury, Dubbo, Griffith and Wagga Wagga and towns such as Deniliquin, Leeton and Parkes exist primarily to service these agricultural regions. The western slopes descend slowly to the western plains that comprise almost two-thirds of the state and are largely arid or semi-arid. The mining town of Broken Hill is the largest centre in this area.[41]		One possible definition of the centre for New South Wales is located 33 kilometres (21 mi) west-north-west of Tottenham.[42]		The major part of New South Wales, west of the Great Dividing Range, has an arid to semi arid climate. Rainfall averages from 150 millimetres (5.9 in) to 500 millimetres (20 in) a year throughout most of this region. Summer temperatures can be very hot, while winter nights can be quite cold in this region. Rainfall varies throughout the state. The far north-west receives the least, less than 180 mm (7 in) annually, while the east receives between 700 to 1,400 mm (28 to 55 in) of rain.[43]		The climate along the flat, coastal plain east of the range varies from oceanic in the south to humid subtropical in the northern half of the state, right above Wollongong. Rainfall is highest in this area; however, it still varies from around 800 millimetres (31 in) to as high as 3,000 millimetres (120 in) in the wettest areas, for example Dorrigo. Along the southern coast, rainfall is heaviest in winter due to cold fronts which move across southern Australia. While in the far north, around Lismore, rain is heaviest in summer from tropical systems and occasionally even cyclones.[43]		The climate in the southern half of the state is generally warm to hot in summer and cool in the winter. The seasons are more defined in the southern half of the state, especially as one moves inland towards South West Slopes, Central West and the Riverina region. The climate in the northeast region of the state, or the North Coast, bordering Queensland, is hot and humid in the summer and mild in winter. The Northern Tablelands, which are also on the north coast, have relatively mild summers and cold winters, due to their high elevation on the Great Dividing Range.		Peaks along the Great Dividing Range vary from 500 metres (1,640 ft) to over 2,000 metres (6,562 ft) above sea level. Temperatures can be cool to cold in winter with frequent frosts and snowfall, and are rarely hot in summer due to the elevation. Lithgow has a climate typical of the range, as do the regional cities of Orange, Cooma, Oberon and Armidale. Such places fall within the subtropical highland (Cwb) variety. Rainfall is moderate in this area, ranging from 600 to 800 mm (24 to 31 in).		Snowfall is common in the higher parts of the range, sometimes occurring as far north as the Queensland border. On the highest peaks of the Snowy Mountains, the climate can be subpolar oceanic and even alpine on the higher peaks with very cold temperatures and heavy snow. The Blue Mountains, Southern Tablelands and Central Tablelands, which are situated on the Great Dividing Range, have mild to warm summers and cold winters, although not as severe as those in the Snowy Mountains.[43]		The highest maximum temperature recorded was 49.7 °C (121 °F) at Menindee in the west of the state on 10 January 1939. The lowest minimum temperature was −23 °C (−9 °F) at Charlotte Pass in the Snowy Mountains on 29 June 1994. This is also the lowest temperature recorded in the whole of Australia excluding the Antarctic Territory.[44]		Since the 1970s, New South Wales has undergone an increasingly rapid economic and social transformation.[citation needed] Old industries such as steel and shipbuilding have largely disappeared; although agriculture remains important, its share of the state's income is smaller than ever before.[citation needed]		New industries such as information technology and financial services are largely centred in Sydney and have risen to take their place, with many companies having their Australian headquarters in Sydney CBD.[citation needed] In addition, the Macquarie Park area of Sydney has attracted the Australian headquarters of many information technology firms.		Coal and related products are the state's biggest export. Its value to the state's economy is over A$5 billion, accounting for about 19% of all exports from NSW.[46]		Tourism has also become important, with Sydney as its centre, also stimulating growth on the North Coast, around Coffs Harbour and Byron Bay.[citation needed] Tourism is worth over $25.1 billion to the New South Wales economy and employs 7.1% of the workforce.[47] In 2007, then-Premier of New South Wales Morris Iemma established Events New South Wales to "market Sydney and NSW as a leading global events destination". In July 2011 Events NSW merged with three key state authorities including Tourism NSW to establish Destination NSW (DNSW).[48]		New South Wales had a Gross State Product in 2010–11 (equivalent to Gross Domestic Product) of $419.9 billion which equalled $57,828 per capita.[49]		On 9 October 2007 NSW announced plans to build a 1,000 MW bank of wind powered turbines. The output of these is anticipated to be able to power up to 400,000 homes. The cost of this project will be $1.8 billion for 500 turbines.[50] On 28 August 2008 the New South Wales cabinet voted to privatise electricity retail, causing 1,500 electrical workers to strike after a large anti-privatisation campaign.[51]		The NSW business community is represented by the NSW Business Chamber which has 30,000 members.		Agriculture is spread throughout the eastern two-thirds of New South Wales. Cattle, sheep and pigs are the predominant types of livestock produced in NSW and they have been present since their importation during the earliest days of European settlement. Economically the state is the most important state in Australia, with about one-third of the country's sheep, one-fifth of its cattle, and one-third of its small number of pigs. New South Wales produces a large share of Australia's hay, fruit, legumes, lucerne, maize, nuts, wool, wheat, oats, oilseeds (about 51%), poultry, rice (about 99%),[52] vegetables, fishing including oyster farming, and forestry including wood chips.[53] Bananas and sugar are grown chiefly in the Clarence, Richmond and Tweed River areas.		Wools are produced on the Northern Tablelands as well as prime lambs and beef cattle. The cotton industry is centred in the Namoi Valley in northwestern New South Wales. On the central slopes there are many orchards, with the principal fruits grown being apples, cherries and pears.		About 40,200 hectares of vineyards lie across the eastern region of the state, with excellent wines produced in the Hunter Valley, with the Riverina being the largest wine producer in New South Wales.[54] Australia’s largest and most valuable Thoroughbred horse breeding area is centred on Scone in the Hunter Valley.[55] The Hunter Valley is the home of the world-famous Coolmore,[56] Darley and Kia-Ora Thoroughbred horse studs.		About half of Australia's timber production is in New South Wales. Large areas of the state are now being replanted with eucalyptus forests.		Under the Water Management Act 2000, updated riparian water rights were given to those within NSW with livestock. This change was named "The Domestic Stock Right" which gives "an owner or occupier of a landholding is entitled to take water from a river, estuary or lake which fronts their land or from an aquifer which is underlying their land for domestic consumption and stock watering without the need for an access licence."[57]		New South Wales has more than 780 national parks and reserves covering more than 8% of the state.[58] These parks range from rainforests, waterfalls, rugged bush to marine wonderlands and outback deserts, including World Heritage sites.[59]		The Royal National Park on the southern outskirts of Sydney became Australia's first National Park when proclaimed on 26 April 1879. Originally named The National Park until 1955, this park was the second National Park to be established in the world after Yellowstone National Park in the U.S. Kosciuszko National Park is the largest park in state encompassing New South Wales' alpine region.[60]		The National Parks Association was formed in 1957 to create a system of national parks all over New South Wales which led to the formation of the National Parks and Wildlife Service in 1967.[61] This government agency is responsible for developing and maintaining the parks and reserve system, and conserving natural and cultural heritage, in the state of New South Wales. These parks preserve special habitats, plants and wildlife, such as the Wollemi National Park where the Wollemi Pine grows and areas sacred to Australian Aboriginals such as Mutawintji National Park in western New South Wales.		Throughout Australian history, NSW sporting teams have been very successful in both winning domestic competitions and providing players to the Australian national teams.		The largest sporting competition in the state is the National Rugby League, which expanded from the New South Wales Rugby League and Australian Rugby Leagues whose headquarters are in Sydney. The state is represented by The 'Blues' in the traditional State of Origin series. Sydney is the spiritual home of Australian rugby league and to 9 of the 16 NRL teams: (Sydney Roosters, South Sydney Rabbitohs, Parramatta Eels, Cronulla-Sutherland Sharks, Wests Tigers, Penrith Panthers, Canterbury Bulldogs and Manly-Warringah Sea Eagles), as well as being the northern home of the St George Illawarra Dragons, which is half-based in Wollongong. A tenth team, the Newcastle Knights is located in Newcastle. The City vs Country Origin match is also taken to various regional cities around the state.		The state is represented by four teams in soccer's A-League: Sydney FC (the inaugural champions in 2005–06), the Western Sydney Wanderers, the Central Coast Mariners, based at Gosford and the Newcastle United Jets (2007–08 A League Champions). Australian rules football has historically not been strong in New South Wales outside the Riverina region. However, the Sydney Swans relocated from South Melbourne in 1982 and their presence and success since the late 1990s has raised the profile of Australian rules football, especially after their AFL premiership in 2005. A second NSW AFL club, the Greater Western Sydney Giants, entered the competition in 2012. Other teams in national competitions include basketball's Sydney Kings, Sydney Uni Flames, rugby union's NSW Waratahs and netball's Sydney Swifts.		Sydney was the host of the 2000 Summer Olympics and the 1938 British Empire Games. The Olympic Stadium, now known as ANZ Stadium is the scene of the annual NRL Grand Final. It also regularly hosts State of Origin matches and rugby union internationals, and hosted the final of the 2003 Rugby World Cup and the football World Cup qualifier between Australia and Uruguay.		The main summer sport is cricket and the SCG hosts the 'New Year' cricket Test match from 2–6 January each year, and is also one of the sites for the finals of the One Day International series. The NSW Blues play in the Ford Ranger Cup and Sheffield Shield cricket competitions. The annual Sydney to Hobart Yacht Race begins in Sydney Harbour on Boxing Day. The climax of Australia's touring car racing series is the Bathurst 1000, held near the city of Bathurst.		The popular equine sports of campdrafting and polocrosse were developed in New South Wales and competitions are now held across Australia. Polocrosse is now played in many overseas countries.		Major professional teams include:		As Australia's most populous state, New South Wales is home to a number of cultural institutions of importance to the nation. In music, New South Wales is home to the Sydney Symphony Orchestra, Australia's busiest and largest orchestra. Australia's largest opera company, Opera Australia, is headquartered in Sydney. Both of these organisations perform a subscription series at the Sydney Opera House. Other major musical bodies include the Australian Chamber Orchestra. Sydney is host to the Australian Ballet for its Sydney season (the ballet is headquartered in Melbourne). Apart from the Sydney Opera House, major musical performance venues include the City Recital Hall and the Sydney Town Hall.		New South Wales is home to several major museums and art galleries, including the Australian Museum, the Powerhouse Museum, the Museum of Sydney, the Art Gallery of New South Wales and the Museum of Contemporary Art.		Sydney is home to five Arts teaching organisations, which have all produced world-famous students: The National Art School, The College of Fine Arts, the National Institute of Dramatic Art (NIDA), the Australian Film, Television & Radio School and the Conservatorium of Music (now part of the University of Sydney).		New South Wales is the setting and shooting location of many Australian films, including Mad Max 2, which was shot near the mining town of Broken Hill. The state has also attracted international productions, both as a setting, such as in Mission: Impossible 2, and as a stand-in for other locations, as seen in The Matrix franchise, The Great Gatsby and Unbroken.[62][63] 20th Century Fox operates Fox Studios Australia in Sydney. Screen NSW, which controls the state film industry, generates approximately $100 million into the New South Wales economy each year.[64]				
Student activism is work by students to cause political, environmental, economic, or social change. Although often focused on schools, curriculum, and educational funding, student groups have influenced greater political events.[1]		Modern student activist movements vary widely in subject, size, and success, with all kinds of students in all kinds of educational settings participating, including public and private school students; elementary, middle, senior, undergraduate, and graduate students; and all races, socio-economic backgrounds, and political perspectives.[2] Some student protests focus on the internal affairs of a specific institution; others focus on broader issues such as a war or dictatorship. Likewise, some student protests focus on an institution's impact on the world, such as a disinvestment campaign, while others may focus on a regional or national policy's impact on the institution, such as a campaign against government education policy. Although student activism is commonly associated with left-wing politics, right-wing student movements are not uncommon; for example, large student movements fought on both sides of the apartheid struggle in South Africa.[3]		Student activism at the university level is nearly as old as the university itself. Students in Paris and Bologna staged collective actions as early as the 13th century, chiefly over town and gown issues.[4] Student protests over broader political issues also have a long pedigree. In Joseon Dynasty Korea, 150 Sungkyunkwan students staged an unprecedented remonstration against the king in 1519 over the Kimyo purge.[5]						In Argentina, as elsewhere in Latin America, the tradition of student activism dates back to at least the 19th century, but it was not until after 1900 that it became a major political force.[6] in 1918 student activism triggered a general modernization of the universities especially tending towards democratization, called the University Revolution (Spanish: revolución universitaria).[7] The events started in Córdoba and were accompanied by similar uprisings across Latin America.[6]		Australian Students have a long history of being active in political debates. This is particularly true in the newer universities that have been established in suburban areas.[8]		For much of the 20th century, the major campus organizing group across Australia was the Australian Union of Students, which was founded in 1937 as the Union of Australian University Students.[9] The AUS folded in 1984.[10] It was replaced by the National Union of Students in 1987.[11]		Student politics of Bangladesh is reactive, confrontational and violent. Student organizations act as the armament of the political parties they are part of. So every now and then there are affrays and commotions. Over the years, political clashes and factional feuds in the educational institutes killed many, seriously hampering academic atmosphere. To check those hitches, universities have no options but go to lengthy and unexpected closures. So classes are not completed on time and there are session jams.		The student wings of ruling parties dominate the campuses and residential halls through crime and violence to enjoy various unauthorized facilities. They control the residential halls to manage seats in favor of their party members and loyal pupils. They eat and buy for free from the restaurants and shops nearby. They extort and grab tenders to earn illicit money. They take money from the freshmen candidates and put pressures on teachers to get an acceptance for them. They take money from the job seekers and put pressures on university administrations to appoint them.[12]		In Canada, New Left student organizations from the late 1950s and 1960s became mainly two: SUPA (Student Union for Peace Action) and CYC (Company of Young Canadians). SUPA grew out of the CUCND (Combined Universities Campaign for Nuclear Disarmament) in December 1964, at a University of Saskatchewan conference.[13] While CUCND had focused on protest marches, SUPA sought to change Canadian society as a whole.[14] The scope expanded to grass-roots politics in disadvantaged communities and 'consciousness raising' to radicalize and raise awareness of the 'generation gap' experienced by Canadian youth. SUPA was a decentralized organization, rooted in local university campuses. SUPA however disintegrated in late 1967 over debates concerning the role of working class and 'Old Left'.[15] Members moved to the CYC or became active leaders in CUS (Canadian Union of Students), leading the CUS to assume the mantle of New Left student agitation.		In 1968, SDU (Students for a Democratic University) was formed at McGill and Simon Fraser Universities. SFU SDU, originally former SUPA members and New Democratic Youth, absorbed members from the campus Liberal Club and Young Socialists. SDU was prominent in an Administration occupation in 1968, and a student strike in 1969.[16] After the failure of the student strike, SDU broke up. Some members joined the IWW and Yippies (Youth International Party). Other members helped form the Vancouver Liberation Front in 1970. The FLQ (Quebec Liberation Front) was considered a terrorist organization, causing the use of the War Measures Act after 95 bombings in the October Crisis. This was the only peacetime use of the War Measures Act.[17]		Anti-Bullying Day (a.k.a. Pink Shirt Day) was created by high school students David Shepherd, and Travis Price of Berwick, Nova Scotia,[18] and is now celebrated annually across Canada.		In 2012, the Quebec Student Movement arose due to an increase of tuition of 75%; that took students out of class and into the streets because that increase did not allow students to comfortably extend their education, because of fear of debt or not having money at all. Following elections that year, premier Jean Charest promised to repeal anti-assembly laws and cancel the tuition hike.[19]		Since the 1970s, PIRGs (Public Interest Research Groups) have been created as a result of Student Union referendums across Canada in individual provinces. Like their American counterparts, Canadian PIRGs are student directed, run, and funded.[20] Most operate on a consensus decision making model. Despite efforts at collaboration, Canadian PIRGs are independent of each other.		From 2011 to 2013, Chile was rocked by a series of student-led nationwide protests across Chile, demanding a new framework for education in the country, including more direct state participation in secondary education and an end to the existence of profit in higher education. Currently in Chile, only 45% of high school students study in traditional public schools and most universities are also private. No new public universities have been built since the end of the Chilean transition to democracy in 1990, even though the number of university students has swelled. Beyond the specific demands regarding education, the protests reflected a "deep discontent" among some parts of society with Chile's high level of inequality.[21] Protests have included massive non-violent marches, but also a considerable amount of violence on the part of a side of protestors as well as riot police.		The first clear government response to the protests was a proposal for a new education fund[22] and a cabinet shuffle which replaced Minister of Education Joaquín Lavín[23] and was seen as not fundamentally addressing student movement concerns. Other government proposals were also rejected.		Since the defeat of the Qing Dynasty during the First (1839–1842) and Second Opium Wars (1856–1860), student activism has played a significant role in the modern Chinese history.[24] Fueled mostly by Chinese nationalism, Chinese student activism strongly believes that young people are responsible for China's future.[24] This strong nationalistic belief has been able to manifest in several forms such as Democracy, anti-Americanism and Communism.[24]		One of the most important acts of student activism in Chinese history is the 1919 May Fourth Movement that saw over 3,000 students of Peking University and other schools gathered together in front of Tiananmen and holding a demonstration. It is regarded as an essential step of the democratic revolution in China, and it had also give birth to Chinese Communism. Anti-Americanism movements led by the students during the Chinese Civil War were also instrumental in discrediting the KMT government and bring the Communist victory in China.[24] In 1989, the democracy movement led by the students at the Tiananmen Square protests ended in a brutal government crackdown which would later be called a massacre.		During communist rule, students in Eastern Europe were the force behind several of the best-known instances of protest. The chain of events leading to the 1956 Hungarian Revolution was started by peaceful student demonstrations in the streets of Budapest, later attracting workers and other Hungarians. In Czechoslovakia, one of the most known faces of the protests following the Soviet-led invasion that ended the Prague Spring was Jan Palach, a student who committed suicide by setting fire to himself on January 16, 1969. The act triggered a major protest against the occupation.[25]		Student-dominated youth movements have also played a central role in the "color revolutions" seen in post-communist societies in recent years. The first example of this was the Serbian Otpor! ("Resistance!" in Serbian), formed in October 1998 as a response to repressive university and media laws that were introduced that year. In the presidential campaign in September 2000, the organisation engineered the "Gotov je" ("He's finished") campaign that galvanized Serbian discontent with Slobodan Milošević, ultimately resulting in his defeat.[26]		Otpor has inspired other youth movements in Eastern Europe, such as Kmara in Georgia, which played an important role in the Rose Revolution, and Pora in Ukraine, which was key in organising the demonstrations that led to the Orange Revolution.[27] Like Otpor, these organisations have consequently practiced non-violent resistance and used ridiculing humor in opposing authoritarian leaders. Similar movements include KelKel in Kyrgyzstan, Zubr in Belarus and MJAFT! in Albania.		Opponents of the "color revolutions" have accused the Soros Foundations and/or the United States government of supporting and even planning the revolutions in order to serve western interests.[28] Supporters of the revolutions have argued that these allegations are greatly exaggerated, and that the revolutions were positive events, morally justified, whether or not Western support had an influence on the events.		In France, student activists have been influential in shaping public debate. In May 1968 the University of Paris at Nanterre was closed due to problems between the students and the administration.[29] In protest of the closure and the expulsion of Nanterre students, students of the Sorbonne in Paris began their own demonstration.[30] The situation escalated into a nationwide insurrection.		The events in Paris were followed by student protests throughout the world. The German student movement participated in major demonstrations against proposed emergency legislation. In many countries, the student protests caused authorities to respond with violence. In Spain, student demonstrations against Franco's dictatorship led to clashes with police. A student demonstration in Mexico City ended in a storm of bullets on the night of October 2, 1968, an event known as the Tlatelolco massacre. Even in Pakistan, students took to the streets to protest changes in education policy, and on November 7 two college students died after police opened fire on a demonstration.[31] The global reverberations from the French uprising of 1968 continued into 1969 and even into the 1970s.[32]		In 1815 in Jena (Germany) the "Urburschenschaft" was founded. That was a Studentenverbindung that was concentrated on national and democratic ideas. In 1817, inspired by liberal and patriotic ideas of a united Germany, student organisations gathered for the Wartburg festival at Wartburg Castle, at Eisenach in Thuringia, on the occasion of which reactionary books were burnt.		In 1819 the student Karl Ludwig Sand murdered the writer August von Kotzebue, who had scoffed at liberal student organisations.		In May 1832 the Hambacher Fest was celebrated at Hambach Castle near Neustadt an der Weinstraße with about 30 000 participants, amongst them many students. Together with the Frankfurter Wachensturm in 1833 planned to free students held in prison at Frankfurt and Georg Büchner's revolutionary pamphlet Der Hessische Landbote that were events that led to the revolutions in the German states in 1848.		In the 1960s, the worldwide upswing in student and youth radicalism manifested itself through the German student movement and organisations such as the German Socialist Student Union. The movement in Germany shared many concerns of similar groups elsewhere, such as the democratisation of society and opposing the Vietnam War, but also stressed more nationally specific issues such as coming to terms with the legacy of the Nazi regime and opposing the German Emergency Acts.		Hong Kong Student activist group Scholarism began an occupation of the Hong Kong government headquarters on 30 August 2012. The goal of the protest was, expressly, to force the government to retract its plans to introduce Moral and National Education as a compulsory subject.[33] On 1 September, an open concert was held as part of the protest, with an attendance of 40,000.[34] At last, the government de facto striked down the Moral and National Education.		Student organizations made important roles during the Umbrella Movement. Standing Committee of the National People's Congress (NPCSC) made decisions on the Hong Kong political reform on 31 August 2014, which the Nominating Committee would tightly control the nomination of the Chief Executive candidate, candidates outside the Pro-Beijing camp would not have opportunities to be nominated. The Hong Kong Federation of Students and Scholarism led a strike against the NPCSC's decision beginning on 22 September 2014, and started protesting outside the government headquarters on 26 September 2014.[35] On 28 September, the Occupy Central with Love and Peace movement announced that the beginning of their civil disobedience campaign.[36] Students and other members of the public demonstrated outside government headquarters, and some began to occupy several major city intersections.[37]		In 16 January 2017 a large group of students (nearly more than 20 lakhs) protested in state of Tamil Nadu and Puducherry for the ban on Jallikattu. The first protest took place in Alanganallur madurai district where few were involved and have been arrested in pay for that a mass huge protest started at Marina beach Chennai, after that Puducherry, Trichy, Salem, Coimbatore, Erode, Tirunelveli, Dharmapuri, Krishnagiri, Vellore, and almost all district of Tamil Nadu participated in the protest. The ban was made by Supreme court of India in 2014 when PETA filed a petition against Jallikattu as a cruelty to animals.		On 20 Jan temporary ordinance has been passed on lifting the ban on Jallikattu.		It was the only Students protest in India done very peacefully. Food, water, mobile restrooms were all arranged by the students and were provided free to more than 20 lakhs student. Most of the Students stayed in the protest place. Many families were also joined the protest. No harassment of women has been filed during the protest as the women also stayed in the protest.Everyone was wondering how such a remarkable protest take place in Tamil Nadu by tamilians. No political parties were allowed in the protest. And from that many tamilians across the various countries showed their support. Not only tamilians everyone everybody started supporting this in spite of religion their language.		Indonesia has hosted "some of the most important acts of student resistance in the world's history".[38] university student groups have repeatedly been the first groups to stage street demonstrations calling for governmental change at key points in the nation's history, and other organizations from across the political spectrum have sought to align themselves with student groups. In 1928, the Youth Pledge (Sumpah Pemuda) helped to give voice to anti-colonial sentiments.		During the political turmoil of the 1960s, right-wing student groups staged demonstrations calling for then-President Sukarno to eliminate alleged Communists from his government, and later demanding that he resign.[39] Sukarno did step down in 1967, and was replaced by Army general Suharto.[40]		Student groups also played a key role in Suharto's 1998 fall by initiating large demonstrations that gave voice to widespread popular discontent with the president in the aftermath of the May 1998 riots.[41] High school and university students in Jakarta, Yogyakarta, Medan, and elsewhere were some of the first groups willing to speak out publicly against the military government. Student groups were a key part of the political scene during this period. Upon taking office after Suharto stepped down, B. J. Habibie made numerous mostly unsuccessful overtures to placate the student groups that had brought down his predecessor. When that failed, he sent a combined force of police and gangsters to evict protesters occupying a government building by force.[42] The ensuing carnage left two students dead and 181 injured.[42]		In Iran, students have been at the forefront of protests both against the pre-1979 secular monarchy and, in recent years, against the theocratic islamic republic. Both religious and more moderate students played a major part in Ruhollah Khomeini's opposition network against the Shah Mohammad Reza Pahlavi.[43] In January 1978 the army dispersed demonstrating students and religious leaders, killing several students and sparking a series of widespread protests that ultimately led to the Iranian Revolution the following year. On November 4, 1979, militant Iranian students calling themselves the Muslim Students Following the Line of the Imam seized the U.S. embassy in Tehran holding 52 embassy employees hostage for a 444 days (see Iran hostage crisis).		Recent years have seen several incidents when liberal students have clashed with the Iranian government, most notably the Iranian student riots of July 1999. Several people were killed in a week of violent confrontations that started with a police raid on a university dormitory, a response to demonstrations by a group of students of Tehran University against the closure of a reformist newspaper. Akbar Mohammadi was given a death sentence, later reduced to 15 years in prison, for his role in the protests. In 2006, he died at Evin prison after a hunger strike protesting the refusal to allow him to seek medical treatment for injuries suffered as a result of torture.[44]		At the end of 2002, students held mass demonstrations protesting the death sentence of reformist lecturer Hashem Aghajari for alleged blasphemy. In June 2003, several thousand students took to the streets of Tehran in anti-government protests sparked by government plans to privatise some universities.[45]		In the May 2005 Iranian presidential election, Iran's largest student organization, The Office to Consolidate Unity, advocated a voting boycott.[46] After the election of President Mahmoud Ahmadinejad, student protests against the government has continued. In May 2006, up to 40 police officers were injured in clashes with demonstrating students in Tehran.[47] At the same time, the Iranian government has called for student action in line with its own political agenda. In 2006, President Ahmadinejad urged students to organize campaigns to demand that liberal and secular university teachers be removed.[48]		In 2009, after the disputed presidential election, a series of student protests broke out, which became known as the Iranian Green Movement. The violent measures used by the Iranian government to suppress these protests have been the subject of widespread international condemnation.[49]		In Israel the students were amongst the leading figures in the 2011 Israeli social justice protests that grew out of the Cottage cheese boycott.[50]		Japanese student movement began during the Taishō Democracy, and grew in activity after World War II. They were mostly carried out by activist students. One such event was the Anpo opposition movement, which occurred in 1960, in opposition to the Anpo treaty.[51] In the subsequent student uprising in 1968, leftist activists barricaded themselves in universities, resulting in armed conflict with the Japanese police force.[52] Some wider causes were supported including opposition to the Vietnam War and apartheid, and for the acceptance of the hippie lifestyle.		Since the amendment of Section 15 of the Universities and University Colleges Act 1971 (UUCA) in 1975, students were barred from being members of, and expressing support or opposition to, any political parties or "any organization, body or group of persons which the Minister, after consultation with the Board, has specified in writing to the Vice-Chancellor to be unsuitable to the interests and well-being of the students or the University." However, in October 2011, the Court of Appeal ruled that the relevant provision in Section 15 UUCA was unconstitutional due to Article 10 of the Federal Constitution pertaining to freedom of expression.[53]		Since the act prohibiting students from expressing "support, sympathy or opposition" to any political party was enacted in 1971, Malaysian students have repeatedly demanded that the ban on political involvement be rescinded. The majority of students are not interested in politics because they are afraid that the universities will take action against them. The U.U.C.A. (also known by its Malaysian acronym AUKU) not however been entirely successful in eliminating student activism and political engagement.[54]		In Kuala Lumpur on 14 April 2012, student activists camped out at Independence Square and marched against a government loan program that they said charged students high interest rates and left them with debt.[55]		The largest student movement in Malaysia is the Solidariti Mahasiswa Malaysia (SMM)(Student Solidarity of Malaysia). SMM is a coalition group that represents numerous student organizations.[56] Currently, SMM is actively campaigning against the UUCA and a free education at primary, secondary and tertiary level.		During the protests of 1968, Mexican government killed an estimated 30 to 300 students and civilian protesters. This killing is known as in the Tlatelolco massacre. killing of an estimated 30 to 300 students and civilians by military and police on October 2, 1968, in the Plaza de las Tres Culturas in the Tlatelolco section of Mexico City. The events are considered part of the Mexican Dirty War, when the government used its forces to suppress political opposition. The massacre occurred 10 days before the opening of the 1968 Summer Olympics in Mexico City.[57]		More recent student movements include Yo Soy 132 in 2012. Yo Soy 132 was a social movement composed for the most part of Mexican university students from private and public universities, residents of Mexico, claiming supporters from about 50 cities around the world.[58] It began as opposition to the Institutional Revolutionary Party (PRI) candidate Enrique Peña Nieto and the Mexican media's allegedly biased coverage of the 2012 general election.[59] The name Yo Soy 132, Spanish for "I Am 132", originated in an expression of solidarity with the original 131 protest's initiators. The phrase drew inspiration from the Occupy movement and the Spanish 15-M movement.[60][61][62] The protest movement was self-proclaimed as the "Mexican spring" (an allusion to the Arab Spring) by its first spokespersons,[63] and called the "Mexican occupy movement" in the international press.[64]		Following the 2014 Iguala mass kidnapping, students responded nationally in protest from marches to destruction of property. Through social media, hashtags such as #TodosSomosAyotzinapa spread and prompted global student response.[65]		Student political activism has existed in U.K since the 1880s with the formation of the student representative councils, precursors of union organisations designed to present students interests. These later evolved into unions, many of which became part of the National Union of Students formed in 1921. However, the NUS was designed to be specifically outside of "political and religious interests", reducing its importance as a centre for student activism. During the 1930s students began to become more politically involved with the formation of many socialist societies at universities, ranging from social democratic to Marxist–Leninist and Trotskyite, even leading to Brian Simon, a communist, becoming head of the NUS.[66]		However, it was not until the 1960s that student activism became important in British universities. Here, like many other countries, the Vietnam war and issues of racism became a focus for many other local frustrations, such as fees and student representation. In 1962, the first student protest against the Vietnam War was held, with CND. However, student activism did not begin on a large scale until the mid-1960s. In 1965, a student protest of 250 students was held outside Edinburgh's American embassy and the beginning of protests against the Vietnam war in Grovesnor square. It also saw the first major teach-in in Britain in 1965, where students debated the Vietnam War and alternative non-violent means of protest at the London School of Economics, sponsored by the Oxford Union.[67]		In 1966 the Radical Student Alliance and Vietnam Solidarity Campaign were formed, both of which became centres for the protest movement. However, the first student sit-in was held at the London School of Economics in 1967 by their Student's Union over the suspension of two students. Its success and a national student rally of 100,000 held in the same year is usually considered to mark the start of the movement. Up until the mid-1970s student activities were held including a protest of up to 80,000 strong in Grosvenor Square, anti-racist protests and occupations in Newcastle, the breaking down of riot control gates and forced closure of the London School of Economics, and Jack Straw becoming the head of the NUS for the RSA. However, many protests were over more local issues, such as student representation in college governance,[68] better accommodation, lower fees or even canteen prices.		Student protests erupted again in 2010 during the Premiership of David Cameron over the issue of tuition fees, higher education funding cuts and withdrawal of the Education Maintenance Allowance.[69]		In the United States, student activism is often understood as a form of youth activism that is specifically oriented toward change in the American educational system. Student activism in the United States dates to the beginning of public education, if not before. Some of the first well documented, directed activism occurred on the campuses of black institutions like Fisk and Howard in the 1920s. At Fisk, student's concerns surrounding disciplinary rules designed to undermine black identity coalesced into demands for the resignation of President Fayette Avery McKenzie. Spurred by alum W.E.B. Du Bois' 1924 commencement speech, the students ignored the 10p.m. curfew to protest, and staged subsequent walkouts. After a committee formed to investigate the protests ruled unfavorably on Mckenzie's abilities and handling of the unrest, he resigned on April 16, 1925. Events at Fisk had wide repercussions, as black students elsewhere began to question the repressive status quo of the postwar black university.[70]		The next wave of activism was spurred by Depression-era realities of the 1930s. The American Youth Congress was a student-led organization in Washington, DC, which lobbied the US Congress against war and racial discrimination and for youth programs. It was heavily supported by First Lady Eleanor Roosevelt.[71]		The counterculture era of the 1960s and early 1970s saw several waves of student activists gaining increasing political prominence in American society. Students formed social movements that moved them from resistance to liberation.[72] An early important national student group was the Student's Peace Union, established in 1959.[73] Another highlight of this period was Students for a Democratic Society (SDS) launched in Ann Arbor, Michigan, was a student-led organization that focused on schools as a social agent that simultaneously oppresses and potentially uplifts society. SDS eventually spun off the Weather Underground. Another successful group was Ann Arbor Youth Liberation, which featured students calling for an end to state-led education. Also notable were the Student Nonviolent Coordinating Committee and the Atlanta Student Movement, predominantly African American groups that fought against racism and for integration of public schools across the US.		The longest student strike in American history started on November 6, 1968 and lasted until March 21, 1969 at San Francisco State College to raise awareness of third world student access to higher education.[74][75][better source needed]		The largest student strike in American history took place in May and June 1970, in response to the Kent State shootings and the American invasion of Cambodia. Over four million students participated in this action.[76]		American society saw an increase in student activism again in the 1990s. The popular education reform movement has led to a resurgence of populist student activism against standardized testing and teaching,[77] as well as more complex issues including military/industrial/prison complex and the influence of the military and corporations in education[78] There is also increased emphasis on ensuring that changes that are made are sustainable, by pushing for better education funding and policy or leadership changes that engage students as decision-makers in schools. Notably, universities participated in the Disinvestment from South Africa movement, University of California, Berkeley being the first institution to disinvest completely from companies implicated in and profiting from the Apartheid movement following student organizing and activism.		Major contemporary campaigns include work for funding of public schools, against increased tuitions at colleges or the use of sweatshop labor in manufacturing school apparel (e.g. United students against sweatshops), for increased student voice throughout education planning, delivery, and policy-making (e.g. The Roosevelt Institution), and to raise national and local awareness of the humanitarian consequences of the Darfur Conflict.[79] There is also increasing activism around the issue of global warming. Antiwar activism has also increased leading to the creation of the Campus Antiwar Network and the refounding of SDS in 2006.		Following the national growth of the Black Lives Matter Movement, and more intensely since the 2016 election of U.S. President Donald Trump, student activism has been on the rise. Alt-Right Breitbart senior editor Milo Yiannopoulos' tour Dangerous Faggot sparked protest at University of California, Davis, where he was scheduled to speak alongside "Pharma Bro" Martin Shkreli and University of California, Berkeley, all shutting his talks down before they started through large-scale protest. [80]		
The Hong Kong Federation of Students (Chinese: 香港專上學生聯會, HKFS) is a student organisation founded in May 1958 by the student unions of four higher education institutions in Hong Kong. The inaugural committee had seven members representing the four schools. The purpose of the HKFS is to promote student movements and to enhance the student body's engagement in society. Since the 1990s, the federation has taken an interest in daily events in Hong Kong, and no longer restricts itself to the areas of education and politics. The HKFS council (代表會) is convened by representatives of the university student unions. The representatives are elected by the university students. A standing committee is appointed by the Council.						In 1971, the Senkaku Islands dispute arose. The administration of the Senkaku/Diaoyutai Islands was transferred from the United States to Japan. On 14 February 1971, Hong Kong students established the Hong Kong Action Committee in Defence of the Diaoyutai Islands (香港保衛釣魚台行動委員會).[citation needed] The Action Committee held demonstrations in front of the Japanese consulate in Hong Kong. Twenty-one people were arrested, seven of whom were university students. On 17 April, the Hong Kong University Students' Union held a peaceful demonstration involving about 1000 students. On 7 July, the HKFS held a demonstration on a larger scale. In 1971, the HKFS was an illegal organisation and some students were arrested by the Royal Hong Kong Police. On 13 May 1975, the federation held its last protest over the issue.		During 1975 and 1976, the standing committee of the HKFS voiced its support for the Cultural Revolution in mainland China. The committee criticised Mak Chung Man, who led students to protest against the communists and said he was "against all the Chinese". Students resented this statement and the issue became the main topic of debate during the HKFS elections of 1976.		In April 1977, the Hong Kong University Students' Union suggested the removal of the words "anti-right wing" from the action guide of the HKFS but the standing committee refused to vote. All delegates from the Hong Kong University Students' Union withdrew from the HKFS in protest.		In April 1979, the HKFS commemorated the May Fourth Movement. The event was poorly attended.		During the 1980s, the HKFS began to support democracy in Taiwan and mainland China. In 1981, the Hong Kong Standard revealed that the HKFS had been placed on a "Red List" in a classified Standing Committee on Pressure Groups (SCOPG) report for being "pro-communist". In March 1983, the HKFS reported the Shue Yan College to the Hong Kong Independent Commission against Corruption but no prosecution was launched. After 1984, the HKFS changed from supporting communism to fully supporting democratic development.		In February 1989, about 4000 students boycotted their classes to protest against the policy of the Hong Kong Education Department. During the Tiananmen Square protests of 1989, the HKFS took part in China-wide demonstrations and strikes. On 20 May, when the tropical cyclone signal number 8 was hoisted, thousands of students took part in a massive demonstration. After the 4 June massacre, all of the HKFS represented university students stopped attending classes. In 1991, there were protests (said by police to be illegal) to support the dissident, Wang Dan.		In 2003, 2004 and 2005, the HKFS took an active part in the 1 July marches.		In 2014, the HKFS, led by Alex Chow and Lester Shum, was a participating organisation in the Umbrella Movement. The movement demanded genuine democracy in future chief executive elections.[1] Admiralty, Causeway Bay and Mong Kok were occupied by suffragists for two months.		Some democratic activists criticised the HKFS for failing to lead the movement. In early 2015, five of the member organisations held disaffiliation referenda. Four passed, reducing the number of member organisations from eight to four. The results are as follows:[2]		Amidst this push for localism in Hong Kong, the HKFS was, for the first time, absent from the Victoria Park candlelight vigil commemorating the 1989 Tiananmen Massacre.		The Hong Kong Federation of Students is formed by the student unions of four institutions:		Former members were:		Chio Ka Fai		Gary Fong (CUSU)		Victor Wong (BUSU)		Lai Wai-kin (PUSU)		Dennis Yip (HKUSTSU)		Nathan Law Kwun-chung (LUSU)		Ding Ka-kat (CityUSU)		Chan Kok-hin (SYUSU)		Wong Ka-fai (Deputy)		Ding Ka-Kat		Sunny Cheung (BUSU)		Flora Wong (PUSU)		Sunny Leung Hiu-yeung (CityUSU)		Shek Tsz-kin (HKUSTSU)		Katherine Ko (LUSU)		Shui Ling Tjhan (SYUSU)		Kwok Chui-ying		Andy Lam (HKUSTSU)		Alice Cheung (LUSU)		Liu Chun-sing (SYUSU)				
Anti-racism includes beliefs, actions, movements, and policies adopted or developed to oppose racism.		By its nature, anti-racism tends to promote the view that racism in a particular society is both pernicious and socially pervasive, and that particular changes in political, economic, and social life are required to eliminate it.						The European discovery of the Americas by Christopher Columbus did not occur until 1492. However, two Papal bulls announced several decades before that event were designed to help ward off increasing Muslim invasions into Europe, which they believed would have an effect on the New World.		When Islam presented a serious military threat to Italy and Central Europe during the mid-15th century around the time of the fall of Constantinople, Pope Nicholas V tried to unite Christendom against them but failed. He then granted Portugal the right to subdue and even enslave Muslims whether white or any other race, pagans and other non-Christians in the papal bull Dum Diversas (1452). While this bull preceded the Atlantic slave trade by several decades, slavery and the slave trade were part of African societies and tribes which supplied the Arab world with slaves long before the arrival of the Europeans.		Increasingly, the Italian merchants from the wealthiest states in Italy, especially Genoa and Venice joined in the lucrative trade and some members sported exotic lackeys and few domestic or workshop slaves whereas before slavery was forbidden in Christendom and only formerly in Muslim Spain and Sicily and their buffer border marches were seen and legally allowed. Racial views of Superiority started developing and became more acute about these slaves, social views imported from the Court of Granada where they were highly stratified and classified.		The following year saw the Fall of Constantinople to Muslim conquerors of the ever-growing Ottoman Empire which left the pope as the undoubted contested leader of Christendom when the Orthodox Church leadership became under submission. Several decades later, European explorers and missionaries spread Christianity to the Americas, Asia, Africa and Oceania. Pope Alexander VI had awarded colonial rights over most of the newly discovered lands by the Iberian Kingdoms of Castile and Portugal. Under their patronato system, however, Royal authorities, not the Vatican, controlled as in Europe all clerical appointments in the new colonies. Thus, the 1455 Papal bull Romanus Pontifex granted the Portuguese all lands behind Cape Bojador "allowing to reduce pagans and other enemies of Christ to perpetual servitude."[citation needed] Later, the 1481 Papal bull Aeterni regis granted all lands south of the Canary Islands to the Portuguese Empire, while in May 1493 the Aragonese-born Pope Alexander VI decreed in the Bull Inter caetera that all lands west of a meridian only 100 leagues west of the Cape Verde Islands should belong to the Spanish Empire while new lands discovered east of that line would belong to Portugal. These arrangements were later confirmed in the 1494 Treaty of Tordesillas.		The European origins of racism spread to the Americas alongside the Europeans, but establishment views were questioned when applied to indigenous peoples. After the discovery of the New World many of the clergy sent to the New World, educated in the new Humane values of the Renaissance blooming but still new in Europe and not ratified by the Vatican, began to criticize Spain and their own Church's treatment and views of indigenous peoples and slaves.		In December 1511, Antonio de Montesinos, a Dominican friar, was the first man to openly rebuke the Spanish authorities and administrators of Hispaniola for their "cruelty and tyranny" in dealing with the American natives and those forced to labor as slaves. King Ferdinand enacted the Laws of Burgos and Valladolid in response. However enforcement was lax, and the New Laws of 1542 have to be made to take a stronger line. Because some people like Fray Bartolomé de las Casas questioned not only the Crown but the Papacy at the Valladolid Controversy whether the Indians were truly men who deserved baptism, Pope Paul III in the papal bull Veritas Ipsa or Sublimis Deus (1537) confirmed that the Indians and other races were deserving men, so long as they became baptized.[1][2] Afterward, their Christian conversion effort gained momentum along social rights, while leaving the same status recognition unanswered for Africans of Black Race, and legal social racism prevailed towards the Indians or Asians. However, by then the last schism of the Reformation had taken place in Europe in those few decades along political lines, and the different views on the Value of human lives of different races were not corrected in the lands of Northern Europe, which would join the Colonial race at the end of the century and over the next, as the Portuguese and Spanish Empires waned. It would take another century, with the influence of the French Empire at its height, and its consequent Enlightenment developed at the highest circles of its Court, to return these previously inconclusive issues to the forefront of the political discourse championed by many intellectual men since Rousseau. These issues gradually permeated to the lower social levels, where they were a reality lived by men and women of different races from the European racial majority.		Prior to the American Revolution, a small group of Quakers including John Woolman and Anthony Benezet successfully persuaded their fellow members of the Religious Society of Friends to free their slaves, divest from the slave trade, and create unified Quaker policies against slavery. This afforded their tiny religious denomination some moral authority to help begin the Abolitionist movement on both sides of the Atlantic. Woolman died of smallpox in England in 1775, shortly after crossing the Atlantic to bring his anti-slavery message to the Quakers of the British Isles.		During and after the American Revolution, Quaker ministrations and preachings against slavery began to spread beyond their movement. In 1783, 300 Quakers, chiefly from the London area, presented the British Parliament with their signatures on the first petition against the slave trade. In 1785, Englishman Thomas Clarkson, enrolled at Cambridge, and in the course of writing an essay in Latin (Anne liceat invitos in servitutem dare (Is it lawful to enslave the unconsenting?), read the works of Benezet, and began a lifelong effort to outlaw the slave trade in England. In 1787, sympathizers formed the Committee for the Abolition of the Slave Trade, a small non-denominational group that could lobby more successfully by incorporating Anglicans, who, unlike the Quakers, could lawfully sit in Parliament. The twelve founding members included nine Quakers, and three pioneering Anglicans: Granville Sharp, Thomas Clarkson, and William Wilberforce – all evangelical Christians.		Later successes in opposing racism were won by the abolitionist movement, both in England and the United States. Though many Abolitionists did not regard blacks or mulattos as equal to whites, they did in general believe in freedom and often even equality of treatment for all people. A few, like John Brown, went further. Brown was willing to die on behalf of, as he said, "millions in this slave country whose rights are disregarded by wicked, cruel, and unjust enactments ..." Many black Abolitionists, such as Frederick Douglass, explicitly argued for the humanity of blacks and mulattoes, and for the equality of all people.		Prior to and during the American Civil War, racial egalitarianism in the North became much stronger and more generally disseminated. The success of black troops in the Union Army had a dramatic impact on Northern sentiment. The Emancipation Proclamation was a notable example of this shift in political attitudes, although it notably did not completely extinguish legal slavery in several states. After the war, the Reconstruction government passed the Fourteenth Amendment and Fifteenth Amendments to the Constitution to guarantee the rights of blacks and mulattoes. Many ex-slaves had access to education for the first time. Blacks and mulattoes were also allowed to vote, which meant that African-Americans were elected to Congress in numbers not equaled until the Voting Rights Act and the Warren Court helped re-enfranchise black Americans.[citation needed]		Due to resistance in the South, however, and a general collapse of idealism in the North, Reconstruction ended, and gave way to the nadir of American race relations. The period from about 1890 to 1920 saw the re-establishment of Jim Crow laws. President Woodrow Wilson, who regarded Reconstruction as a disaster, segregated the federal government.[3] The Ku Klux Klan grew to its greatest peak of popularity and strength. D. W. Griffith's The Birth of a Nation was a movie sensation.		In 1911 the First Universal Races Congress met in London, at which distinguished speakers from many countries for four days discussed race problems and ways to improve interracial relations.[4]		Friedrich Tiedemann was one of the first people scientifically to contest racism. In 1836, using craniometric and brain measurements (taken by him from Europeans and black people from different parts of the world), he refuted the belief of many contemporary naturalists and anatomists that black people have smaller brains and are thus intellectually inferior to white people, saying it was scientifically unfounded and based merely on the prejudiced opinions of travelers and explorers.[5] The evolutionary biologist Charles Darwin wrote in 1871 that ‘[i]t may be doubted whether any character can be named which is distinctive of a race and is constant’ and that ‘[a]lthough the existing races of man differ in many respects, as in colour, hair, shape of skull, proportions of the body, &c., yet if their whole structure be taken into consideration they are found to resemble each other closely in a multitude of points.’[6]		At the start of the 20th Century, the work of anthropologists trying to end the paradigms of cultural evolutionism and social Darwinism within social sciences—anthropologists like Franz Boas, Marcel Mauss, Bronisław Malinowski, Pierre Clastres and Claude Lévi-Strauss—began the initiative to the end of racism in human sciences and establish cultural relativism as the new dominant paradigm.		Japan first proposed articles dedicated to the elimination of racial discrimination to be added to the rules of the League of Nations. This was the first proposal concerning the international elimination of racial discrimination in the world.[citation needed]		Although the proposal received a majority (11 out of 16) of votes, the chairman, U.S. President Woodrow Wilson, overturned it saying that important issues should be unanimously approved. Billy Hughes[7] and Joseph Cook vigorously opposed it as it undermined the White Australia policy.[citation needed]		Opposition to racism revived in the 1920s and 1930s. At that time, anthropologists such as Franz Boas, Ruth Benedict, Margaret Mead, and Ashley Montagu argued for the equality of humans across races and cultures. Eleanor Roosevelt was a very visible advocate for minority rights during this period. Anti-capitalist organizations like the Industrial Workers of the World, which gained popularity during 1905–1926, were explicitly egalitarian.		Beginning with the Harlem Renaissance and continuing into the 1960s, many African-American writers argued forcefully against racism.		During the Civil Rights Movement, Jim Crow laws were repealed in the South and blacks finally re-won the right to vote in Southern states. Dr. Martin Luther King Jr. was an influential force, and his "I Have a Dream" speech is an exemplary condensation of his egalitarian ideology.		Egalitarianism has been a catalyst for feminism, anti-war, and anti-imperialist movements. Henry David Thoreau's opposition to the Mexican–American War, for example, was based in part on his fear that the U.S. was using the war as an excuse to expand American slavery into new territories. Thoreau's response was chronicled in his famous essay "Civil Disobedience", which in turn helped ignite Gandhi's successful campaign against the British in India.[citation needed] Gandhi's example in turn inspired the American civil rights movement.		As James Loewen notes in Lies My Teacher Told Me: "Throughout the world, from Africa to Northern Ireland, movements of oppressed people continue to use tactics and words borrowed from our abolitionist and civil rights movements." In East Germany, revolutionary Iran, Tiananmen Square, and South Africa, images, words, and tactics developed by human rights supporters have been used regularly and repeatedly.		Many of these uses have been controversial. For example, the pro-life movement often draws connections between its goals and the goals of abolitionism. In Zimbabwe, Robert Mugabe has used anti-racist rhetoric to promote a land distribution scheme whereby privately held land is confiscated from white Rhodesians and distributed to blacks, which has resulted in widespread starvation (see Land reform in Zimbabwe).[8][9][10]		The phrase "Anti-racist is a code word for anti-white", coined by high-profile white nationalist Robert Whitaker, is commonly associated with the topic of white genocide, a white nationalist conspiracy theory that mass immigration, integration, miscegenation, low fertility rates and abortion are being promoted in predominantly white countries to deliberately turn them minority-white and hence cause white people to become extinct through forced assimilation.[11][12][13][14][15][16][17][18][19] The phrase has been spotted on billboards near Birmingham, Alabama[20] and in Harrison, Arkansas.[21]		International		Europe		North America		Other		
Folk etymology or reanalysis – sometimes called pseudo-etymology, popular etymology, or analogical reformation – is a change in a word or phrase resulting from the replacement of an unfamiliar form by a more familiar one.[1][2][3] The form or the meaning of an archaic, foreign, or otherwise unfamiliar word is reanalyzed as resembling more familiar words or morphemes. Rebracketing is a form of folk etymology in which a word is broken down or "bracketed" into a new set of supposed elements. Back-formation, creating a new word by removing or changing parts of an existing word, is often based on folk etymology.		The term folk etymology is a loan translation from German Volksetymologie, coined by Ernst Förstemann in 1852.[4]		Folk etymology is a productive process in historical linguistics and language change. Reanalysis of a word's history or original form can affect its spelling, pronunciation, or meaning. This is frequently seen in relation to loanwords or words that have become archaic or obsolete.		Examples of words created or changed through folk etymology include the English dialectal form sparrowgrass, originally from Greek ἀσπάραγος ("asparagus") remade by analogy to the more familiar words sparrow and grass,[5] or the word burger, originally from Hamburg + -er ("thing connected with"), but understood as ham + burger.[6]						The technical term "folk etymology" refers to a change in the form of a word caused by erroneous popular beliefs about its etymology. The English word is a translation of the German term Volksetymologie, coined by Ernst Förstemann. Förstemann noted that in addition to scientific etymology based on careful study in philology, there exist scholarly but often unsystematic accounts, as well as popular accounts for the history of linguistic forms.[4] Until academic linguists developed comparative philology and described the laws underlying sound changes, the derivation of words was a matter mostly of guess-work. Speculation about the original form of words in turn feeds back into the development of the word and thus becomes a part of a new etymology.[7]		Believing a word to have a certain origin, people begin to pronounce, spell, or otherwise use the word in a manner appropriate to that perceived origin. This popular etymologizing has had a powerful influence on the forms which words take. Examples in English include crayfish or crawfish, which are not historically related to fish but come from Middle English crevis, cognate with French écrevisse. Likewise chaise lounge, from the original French chaise longue ("long chair"), has come to be associated with the word lounge.[8]		Rebracketing is a process of language change in which parts of a word that appear to be meaningful (such as *ham in hamburger) are mistaken as elements of the word's etymology (in this case, the word ham). Rebracketing functions by reanalyzing the constituent parts of a word. For example, the Old French word orenge ("orange tree") comes from Arabic النرنج ‎ an nāranj ("the orange tree"), with the initial n of nāranj understood as part of the article.[9]		In back-formation a new word is created, often by removing elements thought to be affixes. For example, Italian pronuncia ("pronunciation; accent") is derived from the verb pronunciare ("to pronounce; to utter") and English edit derives from editor.[10] Some cases of back-formation are based on folk etymology.[6]		In linguistic change caused by folk etymology, the form of a word changes so that it better matches its popular rationalisation. Typically this happens either to unanalyzable foreign words or to compounds where the word underlying one part of the compound becomes obsolete.		There are many examples of words borrowed from foreign languages, and subsequently changed by folk etymology.		The spelling of many borrowed words reflects folk etymology. For example, andiron borrowed from Old French was variously spelled aundyre or aundiren in Middle English, but was altered by association with iron.[11] Other Old French loans altered in a similar manner include belfry (from berfrei) by association with bell, female (from femelle) by male, and penthouse (from apentis) by house.[citation needed] The variant spelling of licorice as liquorice comes from the supposition that it has something to do with liquid.[12] Anglo-Norman licoris (influenced by licor "liquor") and Late Latin liquirītia were respelled for similar reasons, though the ultimate origin of all three is Greek γλυκύρριζα (glycyrrhiza) "sweet root".[13]		Reanalysis of loan words can affect their spelling, pronunciation, or meaning. The word cockroach, for example, was borrowed from Spanish cucaracha but was assimilated to the existing English words cock and roach.[14] Jerusalem artichoke, from Italian girasole, is a kind of sunflower; it is not related to artichokes and does not come from Jerusalem.[15] The phrase forlorn hope originally meant "storming party, body of skirmishers"[16] from Dutch verloren hoop "lost troop". But confusion with English hope has given the term an additional meaning of "hopeless venture".[17]		Sometimes imaginative stories are created to account for the link between a borrowed word and its popularly assumed sources. The names of the serviceberry, service tree, and related plants, for instance, come from the Latin name sorbus. The plants were called syrfe in Old English, which eventually became service.[18] Fanciful stories suggest that the name comes from the fact that the trees bloom in spring, a time when circuit-riding preachers resume church services or when funeral services are carried out for people who died during the winter.[19]		A seemingly plausible but no less speculative etymology accounts for the form of Welsh rarebit, a dish made of cheese and toasted bread. The earliest known reference to the dish in 1725 called it Welsh rabbit.[20] The origin of that name is unknown, but presumably humorous, since the dish contains no rabbit. In 1785 Francis Grose suggested in A Classical Dictionary of the Vulgar Tongue that the dish is "a Welch rare bit",[21] though the word rarebit was not common prior to Grose's dictionary. Both versions of the name are in current use; individuals sometimes express strong opinions concerning which version is correct.[22]		When a word or other form becomes obsolete, words or phrases containing the obsolete portion may be reanalyzed and changed.		Some compound words from Old English were reanalyzed in Middle or Modern English when one of the constituent words fell out of use. Examples include bridegroom from Old English brydguma "bride-man". The word gome "man" from Old English guma fell out of use during the sixteenth century and the compound was eventually reanalyzed with the Modern English word groom "male servant".[23] A similar reanalysis caused sandblind, from Old English sāmblind "half-blind" with a once-common prefix sām- "semi-", to be respelled as though it is related to sand. The word island derives from Old English igland. The modern spelling with the letter s is the result of comparison with the synonym isle from Old French and ultimately Latin insula, though the Old French and Old English words are not historically related.[24] In a similar way, the spelling of wormwood was likely affected by comparison with wood.[25][26]:449		The phrase curry favour, meaning to flatter, comes from Middle English curry favel, "groom a chestnut horse". This was an allusion to a fourteenth century French morality poem, Roman de Fauvel, about a chestnut-colored horse who corrupts men through duplicity. The phrase was reanalyzed in early Modern English by comparison to favour as early as 1510.[27]		Words need not completely disappear before their compounds are reanalyzed. The word shamefaced was originally shamefast. The original meaning of fast "fixed in place" still exists but mainly in frozen expressions such as stuck fast, hold fast, and play fast and loose.[citation needed] The songbird wheatear or white-ear is a back-formation from Middle English whit-ers "white arse", referring to the prominent white rump found in most species.[28] Although both white and arse are common in Modern English, the folk etymology may be euphemism.[29]		Reanalysis of archaic or obsolete forms can lead to changes in meaning as well. The original meaning of hangnail referred to a corn on the foot.[30] The word comes from Old English ang- + nægel ("anguished nail" or "compressed spike"), but the spelling and pronunciation were affected by folk etymology in the seventeenth century or earlier.[31] Thereafter, the word came to be used for a tag of skin or torn cuticle near a fingernail or toenail.[30]		Several words in Medieval Latin were subject to folk etymology. For example, the word widerdonum meaning "reward" was borrowed from Old High German widarlōn "repayment of a loan". The l → d alteration is due to confusion with Latin donum "gift".[32][26]:157 Similarly, the word baceler or bacheler (related to modern English bachelor) referred to a junior knight. It is attested from the eleventh century, though its ultimate origin is uncertain. By the late Middle Ages its meaning was extended to the holder of a university degree inferior to master or doctor. This was later re-spelled baccalaureus, probably reflecting a false derivation from bacca laurea "laurel berry", alluding to the possible laurel crown of a poet or conqueror.[33][26]:17–18		In the fourteenth or fifteenth century French scholars began to spell the verb savoir ("to know") as sçavoir on the false belief it was derived from Latin scire "to know". In fact it comes from sapere "to be wise".[34]		The Italian word liocorno "unicorn" derives from 13th century lunicorno (lo "the" + unicorno "unicorn"). Folk etymology based on lione "lion" altered the spelling and pronunciation. Dialectal liofante "elephant" was likewise altered from elefante by association with lione.[26]:486		The Dutch word for "hammock" is hangmat. It was borrowed from Spanish hamaca (ultimately from Arawak amàca) and altered by comparison with hangen and mat, "hanging mat". German Hängematte shares this folk etymology.[35]		The Finnish compound word for "jealous" mustasukkainen literally means "black-socked" (musta "black" and sukka "sock"). However, the word is a case of a misunderstood loan translation from Swedish svartsjuk "black-sick". The Finnish word sukka fit with a close phonological equivalent to the Swedish sjuk [36][Finnish-language verification needed]		Islambol, a folk etymology meaning "full of Islam", is one of the names of Istanbul used after the Ottoman conquest of 1453.[37][Turkish-language verification needed]		An example from Persian is the word shatranj (chess), which is derived from the Sanskrit chaturanga (2nd century BCE), and after losing the "u" to syncope, becomes chatrang in Middle Persian (6th century CE). Today it is sometimes factorized as shat (hundred) + ranj (worry / mood), or "a hundred worries".[citation needed]		
Study skills, academic skills, or study strategies are approaches applied to learning. They are generally critical to success in school,[1] considered essential for acquiring good grades, and useful for learning throughout one's life.		Study skills are an array of skills which tackle the process of organizing and taking in new information, retaining information, or dealing with assessments. They include mnemonics, which aid the retention of lists of information; effective reading; concentration techniques;[2] and efficient notetaking.[3]		While often left up to the student and their support network, study skills are increasingly taught in high school and at the university level. A number of books and websites are available, from works on specific techniques such as Tony Buzan's books on mind-mapping to general guides to successfully study such as those by Stella Cottrell and Understanding Examination Techniques and Effective study Strategies by Respicius Rwehumbiza.		More broadly, any skill which boosts a person's ability to study, retain and recall information which assists in and passing exams can be termed a study skill, and this could include time management and motivational techniques.		Study skills are discrete techniques that can be learned, usually in a short time, and applied to all or most fields of study. They must therefore be distinguished from strategies that are specific to a particular field of study (e.g. music or technology), and from abilities inherent in the student, such as aspects of intelligence or learning styles.						The term study skills is used for general approaches to learning, skills for specific courses of study. There are many theoretical works on the subject, including a vast number of popular books and websites. Manuals for students have been published since the 1940s[citation needed].		In the 1950s and 1960s, college instructors in the fields of psychology and the study of education used research, theory, and experience with their own students in writing manuals.[4][5] Marvin Cohn based the advice for parents in his 1978 book Helping Your Teen-Age Student on his experience as a researcher and head of a university reading clinic that tutored teenagers and young adults.[6] In 1986, when Dr. Gary Gruber’s Essential Guide to Test Taking for Kids was first published, the author had written 22 books on taking standardized tests. A work in two volumes, one for upper elementary grades and the other for middle school, the Guide has methods for taking tests and completing schoolwork.[7][8]		Memorization is the process of committing something to memory. The act of memorization is often a deliberate mental process undertaken in order to store in memory for later recall items such as experiences, names, appointments, addresses, telephone numbers, lists, stories, poems, pictures, maps, diagrams, facts, music or other visual, auditory, or tactical information. Memorization may also refer to the process of storing particular data into the memory of a device. One of the most basic approaches to learning any information is simply to repeat it by rote. Typically this will include reading over notes or a textbook, and re-writing notes.		The weakness with rote learning is that it implies a passive reading or listening style. Educators such as John Dewey have argued that students need to learn critical thinking – questioning and weighing up evidence as they learn. This can be done during lectures or when reading books.		One method used to focus on key information when studying from books is the PQRST method.[9] This method prioritizes the information in a way that relates directly to how they will be asked to use that information in an exam. PQRST is an acronym for Preview, Question, Read, Summary, Test.[10]		There are a variety of studies from different colleges nationwide that show peer-communication can help increase better study habits tremendously. One study shows that an average of 73% score increase was recorded by those who were enrolled in the classes surveyed.[citation needed]		Flashcards are visual cues on cards. These have numerous uses in teaching and learning, but can be used for revision. Students often make their own flashcards, or more detailed index cards – cards designed for filing, often A5 size, on which short summaries are written. Being discrete and separate, they have the advantage of allowing students to re-order them, pick a selection to read over, or choose randomly for self-testing.		Summary methods vary depending on the topic, but most involve condensing the large amount of information from a course or book into shorter notes. Often, these notes are then condensed further into key facts.		Organized summaries: Such as outlines showing keywords and definitions and relations, usually in a tree structure.		Spider diagrams: Using spider diagrams or mind maps can be an effective way of linking concepts together. They can be useful for planning essays and essay responses in exams. These tools can give a visual summary of a topic that preserves its logical structure, with lines used to show how different parts link together.		Some learners are thought to have a visual learning style, and will benefit greatly from taking information from their studies which are often heavily verbal, and using visual techniques to help encode and retain it in memory.		Some memory techniques make use of visual memory, for example the method of loci, a system of visualising key information in real physical locations e.g. around a room.		Diagrams are often underrated tools. They can be used to bring all the information together and provide practice reorganizing what has been learned in order to produce something practical and useful. They can also aid the recall of information learned very quickly, particularly if the student made the diagram while studying the information. Pictures can then be transferred to flashcards that are very effective last-minute revision tools rather than rereading any written material.		A mnemonic is a method of organizing and memorizing information. Some use a simple phrase or fact as a trigger for a longer list of information. For example, the cardinal points of the compass can be recalled in the correct order with the phrase "Never Eat Shredded Wheat". Starting with North, the first letter of each word relates to a compass point in clockwise order round a compass.		The Black-Red-Green method (developed through the Royal Literary Fund) helps the student to ensure that every aspect of the question posed has been considered, both in exams and essays.[11] The student underlines relevant parts of the question using three separate colors (or some equivalent). BLAck denotes 'BLAtant instructions', i.e. something that clearly must be done; a directive or obvious instruction. REd is a REference Point or REquired input of some kind, usually to do with definitions, terms, cited authors, theory, etc. (either explicitly referred to or strongly implied). GREen denotes GREmlins, which are subtle signals one might easily miss, or a ‘GREEN Light’ that gives a hint on how to proceed, or where to place the emphasis in answers [1]. Another popular method whilst studying is to P.E.E; Point, evidence and explain, reason being, this helps the student break down exam questions allowing them to maximize their marks/grade during the exam. Many Schools will encourage practicing the P.E. Eing method prior to an exam.		Spacing, also called distributed learning by some; helps individuals remember at least as much if not more information for a longer period of time than using only one study skill. Using spacing in addition to other study methods can improve retention and performance on tests.[12] Spacing is especially useful for retaining and recalling new material.[12] The theory of spacing is that instead of cramming all studying into one long study session an individual would split that single session to a few shorter sessions that are hours, if not days apart. Studying will not last longer than it would have originally and one is not working harder but this tool gives the user the ability to remember and recall things for a longer time period. The science behind this; according to Jost’s Law from 1897 “If two associations are of equal strength but of different age, a new repetition has a greater value for the older one”.[13] This means that if a person were to study two things once, at different times, the one studied most recently will be easier to recall.		Often, improvements to the effectiveness of study may be achieved through changes to things unrelated to the study material itself, such as time-management, boosting motivation and avoiding procrastination, and in improvements to sleep and diet.		Time management in study sessions aims to ensure that activities that achieve the greatest benefit are given the greatest focus. A traffic lights system is a simple way of identifying the importance of information, highlighting or underlining information in colours:		This reminds students to start with the things which will provide the quickest benefit, while 'red' topics are only dealt with if time allows. The concept is similar to the ABC analysis, commonly used by workers to help prioritize. Also, some websites (such as FlashNotes) can be used for additional study materials and may help improve time management and increase motivation.		In addition to time management, sleep is an important lifestyle change that could effect your studying. Sleeping less means that you have more time to study. This is a true fact, but just because you are “studying,” does not necessarily mean that your brain is processing everything that you are trying to learn or memorize. It is a proven fact that sleeping more can help you study better, because your brain can process more facts when it has had the rest it needs every night.[15]		In addition to time management and sleep, emotional state of mind matters when a student is studying, according to Benedict Carey in his book “The Surprising Truth About How We Learn and Why It Happens” when speaking about studying versus test taking “we perform better on tests when in the same state of mind as when we studied”[16] If an individual is calm or nervous in class; replicating that emotion can assist in studying. With replicating the emotion an individual is more likely to recall more information if they are in the same state of mind when in class. This also goes the other direction; if one is upset but normally calm in class it’s much better to wait until they are feeling calmer to study. At the time of the test or class they will remember more.		Studying can also be more effective if one changes their environment while studying. For example: the first time studying the material, one can study in a bedroom, the second time one can study outside, and the final time one can study in a coffee shop. The thinking behind this is that as when an individual changes their environment the brain associates different aspects of the learning and gives a stronger hold and additional brain pathways with which to access the information. In this context environment can mean many things; from location, to sounds, to smells, to other stimuli including foods. When discussing environment in regards to its affect on studying and retention Carey says “a simple change in venue improved retrieval strength (memory) by 40 percent.”[17] Another change in the environment can be background music; if people study with music playing and they are able to play the same music during test time they will recall more of the information they studied.[18] According to Carey “background music weaves itself subconsciously into the fabric of stored memory.”[19] This “distraction” in the background helps to create more vivid memories with the studied material.[19]		
Labor rights or workers' rights are a group of legal rights and claimed human rights having to do with labor relations between workers and their employers, usually obtained under labor and employment law. In general, these rights' debates have to do with negotiating workers' pay, benefits, and safe working conditions. One of the most central of these rights is the right to unionize. Unions take advantage of collective bargaining and industrial action to increase their members' wages and otherwise change their working situation. Labor rights can also take in the form of worker's control and worker's self management in which workers have a democratic voice in decision and policy making. The labor movement initially focused on this "right to unionize", but attention has shifted elsewhere.		Critics of the labor rights movement claim that regulation promoted by labor rights activists may limit opportunities for work. In the United States, critics objected to unions establishing closed shops, situations where employers could only hire union members. The Taft–Hartley Act banned the closed shop but allowed the less restrictive union shop. Taft–Hartley also allowed states to pass right-to-work laws, which require an open shop where a worker's employment is not affected by his or her union membership. Labor counters that the open shop leads to a free rider problem.						Throughout history, workers claiming some sort of right have attempted to pursue their interests. During the Middle Ages, the Peasants' Revolt in England expressed demand for better wages and working conditions. One of the leaders of the revolt, John Ball famously argued that people were born equal saying, "When Adam delved and Eve span, who was then the gentleman?" Laborers often appealed to traditional rights. For instance, English peasants fought against the enclosure movement, which took traditionally communal lands and made them private.		In England 1833, a law was passed saying that any child under the age of 9 could not work, children age 9-13 could only work 8 hours a day, and children aged 14–18 could only work 12 hours a day.		Labor rights are a relatively new addition to the modern corpus of human rights. The modern concept of labor rights dates to the 19th century after the creation of labor unions following the industrialization processes. Karl Marx stands out as one of the earliest and most prominent advocates for workers rights. His philosophy and economic theory focused on labor issues and advocates his economic system of socialism, a society which would be ruled by the workers. Many of the social movements for the rights of the workers were associated with groups influenced by Marx such as the socialists and communists. More moderate democratic socialists and social democrats supported worker's interests as well. More recent workers rights advocacy has focused on the particular role, exploitation, and needs of women workers, and of increasingly mobile global flows of casual, service, or guest workers.		The International Labour Organization was formed in 1919 as part of the League of Nations to protect worker's rights. The ILO later became incorporated into the United Nations. The UN itself backed workers rights by incorporating several into two articles of the United Nations Declaration of Human Rights, which is the basis of the International Covenant on Economic, Social and Cultural Rights (article 6-8). These read:		The ILO and several other groups have sought international labor standards to create legal rights for workers across the world. Recent movements have also been made to encourage countries to promote labor rights at the international level through fair trade.[1]		Identified by the International Labour Organisation (ILO) in the ‘Declaration of the Fundamental Principles and Rights at Work’,[2] core labor standards are “widely recognized to be of particular importance”.[3] They are universally applicable, regardless of whether the relevant conventions have been ratified, the level of development of a country or cultural values.[4] These standards are composed of qualitative, not quantitative standards and don’t establish a particular level of working conditions, wages or health and safety standards.[2] They are not intended to undermine the comparative advantage that developing countries may hold. Core labor standards are important human rights and are recognized in widely ratified international human rights instruments including the Convention on the Rights of the Child (CROC), the most widely ratified human rights treaty with 193 parties, and the ICCPR with 160 parties.[5]		The core labor standards are:		Very few ILO member countries have ratified all of these conventions due to domestic constraints yet as these rights are also recognised in the UDHR, and form a part of customary international law they are committed to respect these rights. For a discussion on the incorporation of these core labor rights into the mechanisms of the World Trade Organization, see The Recognition of Labour Standards within the World Trade Organisation. There are many other issues outside of this core, in the UK employee rights includes the right to employment particulars, an itemised pay statement, a disciplinary process at which they have the right to be accompanied, daily breaks, rest breaks, paid holidays and more.[11]		Aside from the right to organize, labor movements have campaigned on various other issues that may be said to relate to labor rights.		Many labor movement campaigns have to do with limiting hours in the work place. 19th century labor movements campaigned for an Eight-hour day. Worker advocacy groups have also sought to limit work hours, making a working week of 40 hours or less standard in many countries. A 35-hour workweek was established in France in 2000, although this standard has been considerably weakened since then. Workers may agree with employers to work for longer, but the extra hours are payable overtime. In the European Union the working week is limited to a maximum of 48 hours including overtime (see also Working Time Directive).		Labor rights advocates have also worked to combat child labor. They see child labor as exploitative, cruel, and often economically damaging. Child labor opponents often argue that working children are deprived of an education. In 1948 and then again in 1989, the United Nations declared that children have a right to social protection.[12] In 2007, Massachusetts updated their child labor laws that required all minors to have work permits.[13]		Labor rights advocates have worked to improve workplace conditions which meet established standards. During the Progressive Era, the United States began workplace reforms, which received publicity boosts from Upton Sinclair's The Jungle and events such as the 1911 Triangle Shirtwaist Factory fire. Labor advocates and other groups often criticize production facilities with poor working conditions as sweatshops and occupational health hazards, and campaign for better labor practices and recognition of workers rights throughout the world.		Recent initiatives in the field of sustainability have included a focus on social sustainability, which includes promoting workers' rights and safe working conditions, prevention of human trafficking, and elimination of illegal child labor from the sustainably sourced products and services.[14] Organizations such as the U.S. Department of Labor and Department of State have released studies on products that have been identified as using child labor and industries using or funded by human trafficking. Labor rights are defined internationally by sources such as the Norwegian Agency for Public Management and eGovernment (DIFI) and the International Finance Corporation performance standards.[14]		The labor movement pushes for guaranteed minimum wage laws, and there are continuing negotiations about increases to the minimum wage. However, opponents see minimum wage laws as limiting employment opportunities for unskilled and entry level workers.		Illegal immigrants cannot complain to the authorities about underpayment and mistreatment as they would be deported; and their willingness to work for low rates may depress rates of pay for others. Similarly, legal migrant workers are sometimes abused. For instance, migrants have faced a number of alleged abuses in the United Arab Emirates (including Dubai). Human Rights Watch lists several problems including "nonpayment of wages, extended working hours without overtime compensation, unsafe working environments resulting in death and injury, squalid living conditions in labor camps, and withholding of passports and travel documents by employers."[15] Despite laws against the practice, employers confiscate migrant workers' passports. Without their passports, workers cannot switch jobs or return home.[1] These workers have little recourse for labor abuses, but conditions have been improving.[16] Labor and social welfare minister Ali bin Abdullah al-Kaabi has undertaken a number of reforms to help improve labor practices in his country.[15]		The right to equal treatment, regardless of gender, origin and appearance, religion, sexual orientation, is also seen by many as a worker's right. Discrimination in the work place is illegal in many countries, but some see the wage gap between genders and other groups as a persistent problem.		
Coordinates: 4°N 72°W﻿ / ﻿4°N 72°W﻿ / 4; -72		Colombia (/kəˈlʌmbiə/ kə-LUM-biə or /kəˈlɒmbiə/ kə-LOM-biə; Spanish: [koˈlombja] ( listen)), officially the Republic of Colombia (Spanish:  República de Colombia (help·info)),[Note 1] is a sovereign state largely situated in the northwest of South America, with territories in Central America. Colombia shares a border to the northwest with Panama, to the east with Venezuela and Brazil and to the south with Ecuador and Peru.[11] It shares its maritime limits with Costa Rica, Nicaragua, Honduras, Jamaica, Haiti and the Dominican Republic.[12] It is a unitary, constitutional republic comprising thirty-two departments. The territory of what is now Colombia was originally inhabited by indigenous peoples, with as most advanced the Muisca, Quimbaya and the Tairona.		The Spanish set foot on Colombian soil for the first time in 1499 and in the first half of the 16th century initiated a period of conquest and colonization, ultimately creating the New Kingdom of Granada, with as capital Santafé de Bogotá. Independence from Spain was acquired in 1819, but by 1830 the "Gran Colombia" Federation was dissolved. What is now Colombia and Panama emerged as the Republic of New Granada. The new nation experimented with federalism as the Granadine Confederation (1858), and then the United States of Colombia (1863), before the Republic of Colombia was finally declared in 1886. Panama seceded in 1903. Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict, which escalated in the 1990s but then decreased from 2005 onward.[13] Colombia is one of the most ethnically and linguistically diverse countries in the world, and thereby possesses a rich cultural heritage. The urban centres are mostly located in the highlands of the Andes mountains.		Colombian territory also encompasses Amazon rainforest, tropical grassland and both Caribbean and Pacific coastlines. Ecologically, it is one of the world's 17 megadiverse countries, and the most densely biodiverse of these per square kilometer.[14] Colombia is a middle power and a regional actor with the fourth-largest economy in Latin America,[4] is part of the CIVETS group of six leading emerging markets and is a member of the UN, the WTO, the OAS, the Pacific Alliance, and other international organizations.[15] Colombia has a diversified economy with macroeconomic stability and favorable growth prospects in the long run.[16][17]						The name "Colombia" is derived from the last name of Christopher Columbus (Italian: Cristoforo Colombo, Spanish: Cristóbal Colón). It was conceived by the Venezuelan revolutionary Francisco de Miranda as a reference to all the New World, but especially to those portions under Spanish and Portuguese rule. The name was later adopted by the Republic of Colombia of 1819, formed from the territories of the old Viceroyalty of New Granada (modern-day Colombia, Panama, Venezuela, Ecuador, and northwest Brazil).[18]		When Venezuela, Ecuador and Cundinamarca came to exist as independent states, the former Department of Cundinamarca adopted the name "Republic of New Granada". New Granada officially changed its name in 1858 to the Granadine Confederation. In 1863 the name was again changed, this time to United States of Colombia, before finally adopting its present name – the Republic of Colombia – in 1886.[18]		To refer to this country, the Colombian government uses the terms Colombia and República de Colombia.		Owing to its location, the present territory of Colombia was a corridor of early human migration from Mesoamerica and the Caribbean to the Andes and Amazon basin. The oldest archaeological finds are from the Pubenza and El Totumo sites in the Magdalena Valley 100 kilometres (62 mi) southwest of Bogotá.[19] These sites date from the Paleoindian period (18,000–8000 BCE). At Puerto Hormiga and other sites, traces from the Archaic Period (~8000–2000 BCE) have been found. Vestiges indicate that there was also early occupation in the regions of El Abra and Tequendama in Cundinamarca. The oldest pottery discovered in the Americas, found at San Jacinto, dates to 5000–4000 BCE.[20]		Indigenous people inhabited the territory that is now Colombia by 12,500 BCE. Nomadic hunter-gatherer tribes at the El Abra, Tibitó and Tequendama sites near present-day Bogotá traded with one another and with other cultures from the Magdalena River Valley.[21] Between 5000 and 1000 BCE, hunter-gatherer tribes transitioned to agrarian societies; fixed settlements were established, and pottery appeared. Beginning in the 1st millennium BCE, groups of Amerindians including the Muisca, Quimbaya, and Tairona developed the political system of cacicazgos with a pyramidal structure of power headed by caciques. The Muisca inhabited mainly the area of what is now the Departments of Boyacá and Cundinamarca high plateau (Altiplano Cundiboyacense) where they formed the Muisca Confederation. They farmed maize, potato, quinoa and cotton, and traded gold, emeralds, blankets, ceramic handicrafts, coca and especially rock salt with neighboring nations. The Tairona inhabited northern Colombia in the isolated mountain range of Sierra Nevada de Santa Marta.[22] The Quimbaya inhabited regions of the Cauca River Valley between the Western and Central Ranges of the Colombian Andes.[23] Most of the Amerindians practiced agriculture and the social structure of each indigenous community was different. Some groups of indigenous people such as the Caribs lived in a state of permanent war, but others had less bellicose attitudes.[24] The Incas expanded their empire onto the southwest part of the country.[25]		Alonso de Ojeda (who had sailed with Columbus) reached the Guajira Peninsula in 1499.[26][27] Spanish explorers, led by Rodrigo de Bastidas, made the first exploration of the Caribbean coast in 1500.[28] Christopher Columbus navigated near the Caribbean in 1502.[29] In 1508, Vasco Núñez de Balboa accompanied an expedition to the territory through the region of Gulf of Urabá and they founded the town of Santa María la Antigua del Darién in 1510, the first stable settlement on the continent. [Note 2][30]		Santa Marta was founded in 1525,[31] and Cartagena in 1533.[32] Spanish conquistador Gonzalo Jiménez de Quesada led an expedition to the interior in April 1536, and christened the districts through which he passed "New Kingdom of Granada". In August 1538, he founded provisionally its capital near the Muisca cacicazgo of Bacatá, and named it "Santa Fe". The name soon acquired a suffix and was called Santa Fe de Bogotá.[33][34] Two other notable journeys by early conquistadors to the interior took place in the same period. Sebastián de Belalcázar, conqueror of Quito, traveled north and founded Cali, in 1536, and Popayán, in 1537;[35] from 1536 to 1539, German conquistador Nikolaus Federmann crossed the Llanos Orientales and went over the Cordillera Oriental in a search for El Dorado, the "city of gold".[36][37] The legend and the gold would play a pivotal role in luring the Spanish and other Europeans to New Granada during the 16th and 17th centuries.[38]		The conquistadors made frequent alliances with the enemies of different indigenous communities. Indigenous allies were crucial to conquest, as well as to creating and maintaining empire.[39] Indigenous peoples in New Granada experienced a decline in population due to conquest as well as Eurasian diseases, such as smallpox, to which they had no immunity.[40][41] With the risk that the land was deserted, the Spanish Crown sold properties to all persons interested in colonise territories creating large farms and possession of mines.[42][43][44]		In the 16th century, the nautical science in Spain reached a great development thanks to numerous scientific figures of the Casa de Contratación and nautical science was an essential pillar of the Iberian expansion.[45]		In 1542, the region of New Granada, along with all other Spanish possessions in South America, became part of the Viceroyalty of Peru, with its capital at Lima.[46] In 1547, New Granada became the Captaincy-General of New Granada within the viceroyalty.		In 1549, the Royal Audiencia was created by a royal decree, and New Granada was ruled by the Royal Audience of Santa Fe de Bogotá, which at that time comprised the provinces of Santa Marta, Rio de San Juan, Popayán, Guayana and Cartagena.[47] But important decisions were taken from the colony to Spain by the Council of the Indies.[48][49]		In the 16th century, Europeans began to bring slaves from Africa. Spain was the only European power that could not establish factories in Africa to purchase slaves and therefore the Spanish empire relied on the asiento system, awarding merchants (mostly from Portugal, France, England and the Dutch Empire) the license to trade enslaved people to their overseas territories.[51][52] Also there were people who defended the human rights and freedoms of oppressed peoples.[Note 3][Note 4] The indigenous peoples could not be enslaved because they were legally subjects of the Spanish Crown [57] and to protect the indigenous peoples, several forms of land ownership and regulation were established: resguardos, encomiendas and haciendas.[42][43][44]		In 1717 the Viceroyalty of New Granada was originally created, and then it was temporarily removed, to finally be reestablished in 1739. The Viceroyalty had Santa Fé de Bogotá as its capital. This Viceroyalty included some other provinces of northwestern South America which had previously been under the jurisdiction of the Viceroyalties of New Spain or Peru and correspond mainly to today's Venezuela, Ecuador and Panama. So, Bogotá became one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City, though it remained somewhat backward compared to those two cities in several economic and logistical ways.[58][59]		After Great Britain declared war on Spain in 1739, Cartagena quickly became the British forces' top target but an upset Spanish victory during the War of Jenkins' Ear, a war with Great Britain for economic control of the Caribbean, cemented Spanish dominance in the Caribbean until the Seven Years' War.[50][60]		The 18th-century priest, botanist and mathematician José Celestino Mutis was delegated by Viceroy Antonio Caballero y Góngora to conduct an inventory of the nature of the New Granada. Started in 1783, this became known as the Royal Botanical Expedition to New Granada which classified plants, wildlife and founded the first astronomical observatory in the city of Santa Fe de Bogotá.[61] In July 1801 the Prussian scientist Alexander von Humboldt reached Santa Fe de Bogotá where he met with Mutis. In addition, historical figures in the process of independence in New Granada emerged from the expedition as the astronomer Francisco José de Caldas, the scientist Francisco Antonio Zea, the zoologist Jorge Tadeo Lozano and the painter Salvador Rizo.[62][63]		Since the beginning of the periods of conquest and colonization, there were several rebel movements against Spanish rule, but most were either crushed or remained too weak to change the overall situation. The last one that sought outright independence from Spain sprang up around 1810, following the independence of St. Domingue (present-day Haiti) in 1804, which provided some support to an eventual leader of this rebellion: Simón Bolívar. Francisco de Paula Santander also would play a decisive role.[64][65][66]		A movement was initiated by Antonio Nariño, who opposed Spanish centralism and led the opposition against the Viceroyalty.[67] Cartagena became independent in November 1811.[68] In 1811 the United Provinces of New Granada were proclaimed, headed by Camilo Torres Tenorio.[69][70] The emergence of two distinct ideological currents among the patriots (federalism and centralism) gave rise to a period of instability.[71] Shortly after the Napoleonic Wars ended, Ferdinand VII, recently restored to the throne in Spain, unexpectedly decided to send military forces to retake most of northern South America. The viceroyalty was restored under the command of Juan Sámano, whose regime punished those who participated in the patriotic movements, ignoring the political nuances of the juntas.[72] The retribution stoked renewed rebellion, which, combined with a weakened Spain, made possible a successful rebellion led by the Venezuelan-born Simón Bolívar, who finally proclaimed independence in 1819.[73][74] The pro-Spanish resistance was defeated in 1822 in the present territory of Colombia and in 1823 in Venezuela.[75][76][77]		The territory of the Viceroyalty of New Granada became the Republic of Colombia, organized as a union of the current territories of Colombia, Panama, Ecuador, Venezuela, parts of Guyana and Brazil and north of Marañón River.[78] The Congress of Cúcuta in 1821 adopted a constitution for the new Republic.[79][80] Simón Bolívar became the first President of Colombia, and Francisco de Paula Santander was made Vice President.[81] However, the new republic was unstable and three countries emerged from the collapse of Gran Colombia in 1830 (New Granada, Ecuador and Venezuela).[82][83]		Colombia was the first constitutional government in South America,[84] and the Liberal and Conservative parties, founded in 1848 and 1849 respectively, are two of the oldest surviving political parties in the Americas.[85] Slavery was abolished in the country in 1851.[86][87]		Internal political and territorial divisions led to the dissolution of Gran Colombia in 1830.[82][83] The so-called "Department of Cundinamarca" adopted the name "New Granada", which it kept until 1858 when it became the "Confederación Granadina" (Granadine Confederation). After a two-year civil war in 1863, the "United States of Colombia" was created, lasting until 1886, when the country finally became known as the Republic of Colombia.[84][88] Internal divisions remained between the bipartisan political forces, occasionally igniting very bloody civil wars, the most significant being the Thousand Days' War (1899–1902).[89]		The United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the separation of the Department of Panama in 1903 and the establishment of it as a nation.[90] The United States paid Colombia $25,000,000 in 1921, seven years after completion of the canal, for redress of President Roosevelt's role in the creation of Panama, and Colombia recognized Panama under the terms of the Thomson–Urrutia Treaty.[91] Colombia and Peru went to war because of territory disputes far in the Amazon basin. The war ended with a peace deal brokered by the League of Nations. The League finally awarded the disputed area to Colombia in June 1934. [92]		Soon after, Colombia achieved some degree of political stability, which was interrupted by a bloody conflict that took place between the late 1940s and the early 1950s, a period known as La Violencia ("The Violence"). Its cause was mainly mounting tensions between the two leading political parties, which subsequently ignited after the assassination of the Liberal presidential candidate Jorge Eliécer Gaitán on 9 April 1948.[93][94] The ensuing riots in Bogotá, known as El Bogotazo, spread throughout the country and claimed the lives of at least 180,000 Colombians.[95]		Colombia entered the Korean War when Laureano Gómez was elected president. It was the only Latin American country to join the war in a direct military role as an ally of the United States. Particularly important was the resistance of the Colombian troops at Old Baldy.[96]		The violence between the two political parties decreased first when Gustavo Rojas deposed the President of Colombia in a coup d'état and negotiated with the guerrillas, and then under the military junta of General Gabriel París.[97][98]		After Rojas' deposition, the Colombian Conservative Party and Colombian Liberal Party agreed to create the "National Front", a coalition which would jointly govern the country. Under the deal, the presidency would alternate between conservatives and liberals every 4 years for 16 years; the two parties would have parity in all other elective offices.[99] The National Front ended "La Violencia", and National Front administrations attempted to institute far-reaching social and economic reforms in cooperation with the Alliance for Progress.[100][101] Despite the progress in certain sectors, many social and political problems continued, and guerrilla groups were formally created such as the FARC, the ELN and the M-19 to fight the government and political apparatus.[102]		Since the 1960s, the country has suffered from an asymmetric low-intensity armed conflict between the government forces, left-wing guerrilla groups and right-wing paramilitaries.[103] The conflict escalated in the 1990s,[104] mainly in remote rural areas.[105] Since the beginning of the armed conflict, human rights defenders have fought for the respect for human rights, despite staggering opposition.[Note 5][Note 6] Several guerrillas' organizations decided to demobilize after peace negotiations in 1989–1994.[13]		The United States has been heavily involved in the conflict since its beginnings, when in the early 1960s the U.S. government encouraged the Colombian military to attack leftist militias in rural Colombia. This was part of the U.S. fight against communism.[103][13][109]		On 4 July 1991, a new Constitution was promulgated. The changes generated by the new constitution are viewed as positive by Colombian society.[110][111]		The administration of President Álvaro Uribe (2002–10), adopted the democratic security policy which included an integrated counter-terrorism and counter-insurgency campaign.[112] The Government economic plan also promoted confidence in investors.[113]		As part of a controversial peace process the AUC (right-wing paramilitaries) as a formal organization had ceased to function.[114] In February 2008, millions of Colombians demonstrated against FARC and other outlawed groups.[115]		After peace negotiations in Cuba, the Colombian government of President Juan Manuel Santos and guerrilla of FARC-EP announced consensus on a 6-point plan towards peace.[116] The first peace accord was submitted to voters in a national referendum and was rejected with 50.2% voting against it and 49.8% voting in favor, on a 37.4% turnout.[117][118] Afterward, the Colombian government and the FARC signed a revised peace deal in November 2016,[119] which the Colombian congress approved.[120]		The Government began a process of assistance, attention and comprehensive reparation for victims of conflict.[121][122] In 2016, President Santos was awarded the Nobel Peace Prize.[123]		Colombia shows modest progress in the struggle to defend human rights, as expressed by HRW.[124] In terms of international relations, Colombia and Venezuela have agreed to restore diplomatic relations.[125] Colombia with a very clean electricity generation matrix reaffirms its support for the Paris Climate Agreement.[126]		The geography of Colombia is characterized by its six main natural regions that present their own unique characteristics, from the Andes mountain range region shared with Ecuador and Venezuela; the Pacific coastal region shared with Panama and Ecuador; the Caribbean coastal region shared with Venezuela and Panama; the Llanos (plains) shared with Venezuela; the Amazon Rainforest region shared with Venezuela, Brazil, Peru and Ecuador; to the insular area, comprising islands in both the Atlantic and Pacific oceans.[127]		Colombia is bordered to the northwest by Panama; to the east by Venezuela and Brazil; to the south by Ecuador and Peru;[11] it established its maritime boundaries with neighboring countries through seven agreements on the Caribbean Sea and three on the Pacific Ocean.[12] It lies between latitudes 12°N and 4°S, and longitudes 67° and 79°W.		Part of the Ring of Fire, a region of the world subject to earthquakes and volcanic eruptions,[128] In the interior of Colombia the Andes are the prevailing geographical feature. Most of Colombia's population centers are located in these interior highlands. Beyond the Colombian Massif (in the south-western departments of Cauca and Nariño) these are divided into three branches known as cordilleras (mountain ranges): the Cordillera Occidental, running adjacent to the Pacific coast and including the city of Cali; the Cordillera Central, running between the Cauca and Magdalena River valleys (to the west and east respectively) and including the cities of Medellín, Manizales, Pereira and Armenia; and the Cordillera Oriental, extending north east to the Guajira Peninsula and including Bogotá, Bucaramanga and Cúcuta.[127][129][130]		Peaks in the Cordillera Occidental exceed 4,700 m (15,420 ft), and in the Cordillera Central and Cordillera Oriental they reach 5,000 m (16,404 ft). At 2,600 m (8,530 ft), Bogotá is the highest city of its size in the world.[127]		East of the Andes lies the savanna of the Llanos, part of the Orinoco River basin, and, in the far south east, the jungle of the Amazon rainforest. Together these lowlands comprise over half Colombia's territory, but they contain less than 6% of the population. To the north the Caribbean coast, home to 21.9% of the population and the location of the major port cities of Barranquilla and Cartagena, generally consists of low-lying plains, but it also contains the Sierra Nevada de Santa Marta mountain range, which includes the country's tallest peaks (Pico Cristóbal Colón and Pico Simón Bolívar), and the La Guajira Desert. By contrast the narrow and discontinuous Pacific coastal lowlands, backed by the Serranía de Baudó mountains, are sparsely populated and covered in dense vegetation. The principal Pacific port is Buenaventura.[127][129][130]		The main rivers of Colombia are Magdalena, Cauca, Guaviare, Atrato, Meta, Putumayo and Caquetá. Colombia has four main drainage systems: the Pacific drain, the Caribbean drain, the Orinoco Basin and the Amazon Basin. The Orinoco and Amazon Rivers mark limits with Colombia to Venezuela and Peru respectively.[131]		Protected areas and the "National Park System" cover an area of about 14,268,224 hectares (142,682.24 km2) and account for 12.77% of the Colombian territory.[132] Compared to neighboring countries, rates of deforestation in Colombia are still relatively low.[133] Colombia is the sixth country in the world by magnitude of total renewable freshwater supply, and still has large reserves of freshwater.[134]		The climate of Colombia is characterized for being tropical presenting variations within six natural regions and depending on the altitude, temperature, humidity, winds and rainfall.[135] The diversity of climate zones in Colombia is characterized for having tropical rainforests, savannas, steppes, deserts and mountain climate.		Mountain climate is one of the unique features of the Andes and other high altitude reliefs where climate is determined by elevation. Below 1,000 meters (3,281 ft) in elevation is the warm altitudinal zone, where temperatures are above 24 °C (75.2 °F). About 82.5% of the country's total area lies in the warm altitudinal zone. The temperate climate altitudinal zone located between 1,001 and 2,000 meters (3,284 and 6,562 ft)) is characterized for presenting an average temperature ranging between 17 and 24 °C (62.6 and 75.2 °F). The cold climate is present between 2,001 and 3,000 meters (6,565 and 9,843 ft) and the temperatures vary between 12 and 17 °C (53.6 and 62.6 °F). Beyond the cold land lie the alpine conditions of the forested zone and then the treeless grasslands of the páramos. Above 4,000 meters (13,123 ft), where temperatures are below freezing, the climate is glacial, a zone of permanent snow and ice.[135]		Ice cap climate in the Nevado del Ruiz		Alpine tundra climate in the Sumapaz Paramo		Oceanic climate in Tota Lake		Cold desert climate near Villa de Leyva		Tropical wet climate in the tepuis of the Serranía de Chiribiquete		Mediterranean climate in Boyacá Department		Tropical rainforest climate in the Amazon Rainforest		Tropical savanna climate in Los Llanos		Hot desert climate in the Guajira Peninsula		Tropical wet and dry climate in San Andrés y Providencia		Warm and wet climate in Caño Cristales		Mountain climate in the Cordillera Occidental		Colombia is one of the megadiverse countries in biodiversity,[137] ranking first in bird species.[138] As for plants, the country has between 40,000 and 45,000 plant species, equivalent to 10 or 20% of total global species, this is even more remarkable given that Colombia is considered a country of intermediate size.[139] Colombia is the second most biodiverse country in the world, lagging only after Brazil which is approximately 7 times bigger.[14]		Colombia is the country in the planet more characterized by a high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemisms (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth live in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world’s mammals species, 14% of the amphibian species and 18% of the bird species of the world.[140]		Colombia has about 2,000 species of marine fish and is the second most diverse country in freshwater fish. Colombia is the country with more endemic species of butterflies, number 1 in terms of orchid species and approximately 7,000 species of beetles. Colombia is second in the number of amphibian species and is the third most diverse country in reptiles and palms. There are about 1,900 species of mollusks and according to estimates there are about 300,000 species of invertebrates in the country. In Colombia there are 32 terrestrial biomes and 314 types of ecosystems.[141][142]		The government of Colombia takes place within the framework of a presidential participatory democratic republic as established in the Constitution of 1991.[111] In accordance with the principle of separation of powers, government is divided into three branches: the executive branch, the legislative branch and the judicial branch.[143]		As the head of the executive branch, the President of Colombia serves as both head of state and head of government, followed by the Vice President and the Council of Ministers. The president is elected by popular vote to serve four-year term (In 2015, Colombia’s Congress approved the repeal of a 2004 constitutional amendment that eliminated the one-term limit for presidents).[144] At the provincial level executive power is vested in department governors, municipal mayors and local administrators for smaller administrative subdivisions, such as corregimientos or comunas.[145] All regional elections are held one year and five months after the presidential election.[146][147]		The legislative branch of government is represented nationally by the Congress, a bicameral institution comprising a 166-seat Chamber of Representatives and a 102-seat Senate.[148][149] The Senate is elected nationally and the Chamber of Representatives is elected in electoral districts.[150] Members of both houses are elected to serve four-year terms two months before the president, also by popular vote.[151]		The judicial branch is headed by four high courts,[152] consisting of the Supreme Court which deals with penal and civil matters, the Council of State, which has special responsibility for administrative law and also provides legal advice to the executive, the Constitutional Court, responsible for assuring the integrity of the Colombian constitution, and the Superior Council of Judicature, responsible for auditing the judicial branch.[153] Colombia operates a system of civil law, which since 2005 has been applied through an adversarial system.		Despite a number of controversies, the democratic security policy has ensured that former President Uribe remained popular among Colombian people, with his approval rating peaking at 76%, according to a poll in 2009.[154] However, having served two terms, he was constitutionally barred from seeking re-election in 2010.[155] In the run-off elections on 20 June 2010 the former Minister of defense Juan Manuel Santos won with 69% of the vote against the second most popular candidate, Antanas Mockus. A second round was required since no candidate received over the 50% winning threshold of votes.[156] Santos won nearly 51% of the vote in second-round elections on 15 June 2014, beating right-wing rival Óscar Iván Zuluaga, who won 45%. His term as Colombia's president runs for four years beginning 7 August 2014.[157]		The foreign affairs of Colombia are headed by the President, as head of state, and managed by the Minister of Foreign Affairs.[158] Colombia has diplomatic missions in all continents.[159]		Colombia was one of the 4 founding members of the Pacific Alliance, which is a political, economic and co-operative integration mechanism that promotes the free circulation of goods, services, capital and persons between the members, as well as a common stock exchange and joint embassies in several countries.[160] Colombia is also a member of the United Nations, the World Trade Organization, the Organization of American States, the Organization of Ibero-American States, the Union of South American Nations and the Andean Community of Nations.[161][162][163][164] Colombia is a global partner of NATO.[165] Colombia is currently in the accession process with the OECD.[166]		The executive branch of government is responsible for managing the defense of Colombia, with the President commander-in-chief of the armed forces. The Ministry of Defence exercises day-to-day control of the military and the Colombian National Police. Colombia has 455,461 active military personnel.[167] And in 2016 3.4% of the country's GDP went towards military expenditure, placing it 24th in the world. Colombia's armed forces are the largest in Latin America, and it is the second largest spender on its military after Brazil.[168][169]		The Colombian military is divided into three branches: the National Army of Colombia; the Colombian Air Force; and the Colombian Navy. The National Police functions as a gendarmerie, operating independently from the military as the law enforcement agency for the entire country. Each of these operates with their own intelligence apparatus separate from the National Intelligence Directorate (DNI, in Spanish).[170]		The National Army is formed by divisions, brigades, special brigades and special units;[171] the Colombian Navy by the Naval Infantry, the Naval Force of the Caribbean, the Naval Force of the Pacific, the Naval Force of the South, the Naval Force of the East, Colombia Coast Guards, Naval Aviation and the Specific Command of San Andres y Providencia;[172] and the Air Force by 15 air units.[173] The National Police has a presence in all municipalities.		Colombia is divided into 32 departments and one capital district, which is treated as a department (Bogotá also serves as the capital of the department of Cundinamarca). Departments are subdivided into municipalities, each of which is assigned a municipal seat, and municipalities are in turn subdivided into corregimientos in rural areas and into comunas in urban areas. Each department has a local government with a governor and assembly directly elected to four-year terms, and each municipality is headed by a mayor and council. There is a popularly elected local administrative board in each of the corregimientos or comunas.[174][175][176][177]		In addition to the capital four other cities have been designated districts (in effect special municipalities), on the basis of special distinguishing features. These are Barranquilla, Cartagena, Santa Marta and Buenaventura. Some departments have local administrative subdivisions, where towns have a large concentration of population and municipalities are near each other (for example in Antioquia and Cundinamarca). Where departments have a low population (for example Amazonas, Vaupés and Vichada), special administrative divisions are employed, such as "department corregimientos", which are a hybrid of a municipality and a corregimiento.[174][175]		Click on a department on the map below to go to its article.		Historically an agrarian economy, Colombia urbanised rapidly in the 20th century, by the end of which just 15.8% of the workforce were employed in agriculture, generating just 6.8% of GDP; 19.6% of the workforce were employed in industry and 64.6% in services, responsible for 34.0% and 59.2% of GDP respectively.[178][179] The country's economic production is dominated by its strong domestic demand. Consumption expenditure by households is the largest component of GDP.[180][16][181]		Colombia's market economy grew steadily in the latter part of the 20th century, with gross domestic product (GDP) increasing at an average rate of over 4% per year between 1970 and 1998. The country suffered a recession in 1999 (the first full year of negative growth since the Great Depression), and the recovery from that recession was long and painful. However, in recent years growth has been impressive, reaching 6.9% in 2007, one of the highest rates of growth in Latin America.[182] According to International Monetary Fund estimates, in 2012 Colombia's GDP (PPP) was US$500 billion (28th in the world and third in South America).		Total government expenditures account for 28.7 percent of the domestic economy. Public debt equals 41 percent of gross domestic product. A strong fiscal climate was reaffirmed by a boost in bond ratings.[183][184][185] Annual inflation closed 2016 at 5.75% YoY (vs. 6.77% YoY in 2015).[186] The average national unemployment rate in 2016 was 9.2%,[187] although the informality is the biggest problem facing the labour market (the income of formal workers climbed 24.8% in 5 years while labor incomes of informal workers rose only 9%).[188] Colombia has Free trade Zone (FTZ),[189] such as Zona Franca del Pacifico, located in the Valle del Cauca, one of the most striking areas for foreign investment.[190]		The financial sector has grown favorably due to good liquidity in the economy, the growth of credit and the positive performance of the Colombian economy.[17][191][192] The Colombian Stock Exchange through the Latin American Integrated Market (MILA) offers a regional market to trade equities.[193][194] Colombia is now one of only three economies with a perfect score on the strength of legal rights index, according to the World Bank.[195]		The electricity production in Colombia comes mainly from renewable energy sources. 69.97% is obtained from the hydroelectric generation.[197] Colombia's commitment to renewable energy was recognized in the 2014 Global Green Economy Index (GGEI), ranking among the top 10 nations in the world in terms of greening efficiency sectors.[198]		Colombia is rich in natural resources, and its main exports include mineral fuels, oils, distillation products, fruit and other agricultural products, sugars and sugar confectionery, food products, plastics, precious stones, metals, forest products, chemical goods, pharmaceuticals, vehicles, electronic products, electrical equipments, perfumery and cosmetics, machinery, manufactured articles, textile and fabrics, clothing and footwear, glass and glassware, furniture, prefabricated buildings, military products, home and office material, construction equipment, software, among others.[199] Principal trading partners are the United States, China, the European Union and some Latin American countries.[200][201]		Non-traditional exports have boosted the growth of Colombian foreign sales as well as the diversification of destinations of export thanks to new free trade agreements.[202]		In 2016, the National Administrative Department of Statistics (DANE) reported that 28.0% of the population were living below the poverty line, of which 8.5% in "extreme poverty".[5] The Government has also been developing a process of financial inclusion within the country's most vulnerable population.[203]		Recent economic growth has led to a considerable increase of new millionaires, including the new entrepreneurs, Colombians with a net worth exceeding US $1 billion.[204][205]		The contribution of Travel & Tourism to GDP was USD5,880.3bn (2.0% of total GDP) in 2016. Tourism generated 556,135 jobs (2.5% of total employment) in 2016.[206] Foreign tourist visits were predicted to have risen from 0.6 million in 2007 to 2.98 million in 2015.[207][208]		Colombia has more than 3,950 research groups in science and technology.[209] iNNpulsa, a government body that promotes entrepreneurship and innovation in the country, provides grants to startups, in addition to other services it and institutions like Apps.co provide. Co-working spaces have arisen to serve as communities for startups large and small.[210][211] Organizations such as the Corporation for Biological Research (CIB) for the support of young people interested in scientific work has been successfully developed in Colombia.[212] The International Center for Tropical Agriculture based in Colombia investigates the increasing challenge of global warming and food security.[213]		Important inventions related to the medicine have been made in Colombia, such as the first external artificial pacemaker with internal electrodes, invented by the electronics engineer Jorge Reynolds Pombo, invention of great importance for those who suffer from heart failure. Also invented in Colombia were the microkeratome and keratomileusis technique, which form the fundamental basis of what now is known as LASIK (one of the most important techniques for the correction of refractive errors of vision) and the Hakim valve for the treatment of Hydrocephalus, among others.[214] Colombia has begun to innovate in military technology for its army and other armies of the world; especially in the design and creation of personal ballistic protection products, military hardware, military robots, bombs, simulators and radar.[215][216][217]		Some leading Colombian scientists are Joseph M. Tohme, researcher recognized for his work on the genetic diversity of food, Manuel Elkin Patarroyo who is known for his groundbreaking work on synthetic vaccines for malaria, Francisco Lopera who discovered the "Paisa Mutation" or a type of early-onset Alzheimer's,[218] Rodolfo Llinás known for his study of the intrinsic neurons properties and the theory of a syndrome that had changed the way of understanding the functioning of the brain, Jairo Quiroga Puello recognized for his studies on the characterization of synthetic substances which can be used to fight fungus, tumors, tuberculosis and even some viruses and Ángela Restrepo who established accurate diagnoses and treatments to combat the effects of a disease caused by the Paracoccidioides brasiliensis, among other scientists.[219][220][221]		Transportation in Colombia is regulated within the functions of the Ministry of Transport [222] and entities such as the National Roads Institute (INVÍAS) responsible for the Highways in Colombia,[223] the Aerocivil, responsible for civil aviation and airports,[224] the National Infrastructure Agency, in charge of concessions through public–private partnerships, for the design, construction, maintenance, operation, and administration of the transport infrastructure,[225] the General Maritime Directorate (Dimar) has the responsibility of coordinating maritime traffic control along with the Colombian Navy,[226] among others and under the supervision of the Superintendency of Ports and Transport.[227] The road network in Colombia has a length of about 215,000 kms of which 23,000 are paved.[228] Rail transportation in Colombia is dedicated almost entirely to freight shipments and the railway network has a length of 1,700 Kms of potentially active rails.[228] Colombia has 3,960 kilometers of gas pipelines, 4,900 kilometers of oil pipelines, and 2,990 kilometers of refined-products pipelines.[228]		The target of Colombia’s government is to build 7,000 km of roads for the 2016–2020 period and reduce travel times by 30 per cent and transport costs by 20 per cent. A toll road concession programme will comprise 40 projects, and is part of a larger strategic goal to invest nearly $50bn in transport infrastructure, including: railway systems; making the Magdalena river navigable again; improving port facilities; as well as an expansion of Bogotá’s airport.[229]		With an estimated 49 million people in 2017, Colombia is the third-most populous country in Latin America, after Brazil and Mexico.[3] It is also home to the third-largest number of Spanish speakers in the world after Mexico and the United States.[230] At the beginning of the 20th century, Colombia's population was approximately 4 million.[231] Since the early 1970s Colombia has experienced steady declines in its fertility, mortality, and population growth rates. The population growth rate for 2015 is estimated to be 0.9%.[232] The total fertility rate was 1.9 births per woman in 2015.[233] About 26.8% of the population were 15 years old or younger, 65.7% were between 15 and 64 years old, and 7.4% were over 65 years old. The proportion of older persons in the total population has begun to increase substantially.[234] Colombia is projected to have a population of 50.2 million by 2020 and 55.3 million by 2050.[235]		The population is concentrated in the Andean highlands and along the Caribbean coast, also the population densities are generally higher in the Andean region. The nine eastern lowland departments, comprising about 54% of Colombia's area, have less than 6% of the population.[129][130] Traditionally a rural society, movement to urban areas was very heavy in the mid-20th century, and Colombia is now one of the most urbanized countries in Latin America. The urban population increased from 31% of the total in 1938 to nearly 60% in 1973, and by 2014 the figure stood at 76%.[236][237] The population of Bogotá alone has increased from just over 300,000 in 1938 to approximately 8 million today.[238] In total seventy-two cities now have populations of 100,000 or more (2015).[239] As of 2012[update] Colombia has the world's largest populations of internally displaced persons (IDPs), estimated to be up to 4.9 million people.[240]		The life expectancy is 74.8 years in 2015 and infant mortality is 13.6 per thousand in 2015.[241][242] In 2015, 94.58% of adults and 98.66% of youth are literate and the government spends about 4.49% of its GDP in education.[243]		Colombia is ranked third in the world in the Happy Planet Index.		More than 99.2% of Colombians speak Spanish, also called Castilian; 65 Amerindian languages, two Creole languages, the Romani language and Colombian Sign Language are also spoken in the country. English has official status in the archipelago of San Andrés, Providencia and Santa Catalina.[8][244][245][246]		Including Spanish, a total of 101 languages are listed for Colombia in the Ethnologue database. The specific number of spoken languages varies slightly since some authors consider as different languages what others consider to be varieties or dialects of the same language. Best estimates recorded 71 languages that are spoken in-country today—most of which belong to the Chibchan, Tucanoan, Bora–Witoto, Guajiboan, Arawakan, Cariban, Barbacoan, and Saliban language families. There are currently about 850,000 speakers of native languages.[247][248]		Colombia is ethnically diverse, its people descending from the original native inhabitants, Spanish colonists, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East, all contributing to a diverse cultural heritage.[249] The demographic distribution reflects a pattern that is influenced by colonial history.[250] Whites tend to live mainly in urban centers, like Bogotá, Medellín or Cali, and the burgeoning highland cities. The populations of the major cities also include mestizos. Mestizo campesinos (people living in rural areas) also live in the Andean highlands where some Spanish conquerors mixed with the women of Amerindian chiefdoms. Mestizos include artisans and small tradesmen that have played a major part in the urban expansion of recent decades.[251]		The 2005 census reported that the "non-ethnic population", consisting of whites and mestizos (those of mixed white European and Amerindian ancestry), constituted 86% of the national population. 10.6% is of African ancestry. Indigenous Amerindians comprise 3.4% of the population. 0.01% of the population are Roma. An extraofficial estimate considers that the 49% of the Colombian population is Mestizo or of mixed European and Amerindian ancestry, and that approximately 37% is White, mainly of Spanish lineage, but there is also a large population of Middle East descent; in some sectors of society there is a considerable input of Italian and German ancestry.[2]		Many of the Indigenous peoples experienced a reduction in population during the Spanish rule [252] and many others were absorbed into the mestizo population, but the remainder currently represents over eighty distinct cultures. Reserves (resguardos) established for indigenous peoples occupy 30,571,640 hectares (305,716.4 km2) (27% of the country's total) and are inhabited by more than 800,000 people.[253] Some of the largest indigenous groups are the Wayuu,[254] the Paez, the Pastos, the Emberá and the Zenú.[255] The departments of La Guajira, Cauca, Nariño, Córdoba and Sucre have the largest indigenous populations.[1]		The Organización Nacional Indígena de Colombia (ONIC), founded at the first National Indigenous Congress in 1982, is an organization representing the indigenous peoples of Colombia. In 1991, Colombia signed and ratified the current international law concerning indigenous peoples, Indigenous and Tribal Peoples Convention, 1989.[256]		Black Africans were brought as slaves, mostly to the coastal lowlands, beginning early in the 16th century and continuing into the 19th century. Large Afro-Colombian communities are found today on the Caribbean and Pacific coasts. The population of the department of Chocó, running along the northern portion of Colombia's Pacific coast, is over 80% black.[257] British and Jamaicans migrated mainly to the islands of San Andres and Providencia. A number of other Europeans and North Americans migrated to the country in the late 19th and early 20th centuries, including people from the former USSR during and after the Second World War.[258][259]		Many immigrant communities have settled on the Caribbean coast, in particular recent immigrants from the Middle East. Barranquilla (the largest city of the Colombian Caribbean) and other Caribbean cities have the largest populations of Lebanese, Palestinian, and other Arabs.[260][261] There are also important communities of Chinese, Japanese, Romanis and Jews.[249] There is a major migration trend of Venezuelans, due to the political and economic situation in Venezuela.[262][263]		The National Administrative Department of Statistics (DANE) does not collect religious statistics, and accurate reports are difficult to obtain. However, based on various studies and a survey, about 90% of the population adheres to Christianity, the majority of which (70.9%) are Roman Catholic, while a significant minority (16.7%) adhere to Protestantism (primarily Evangelicalism). Some 4.7% of the population is atheist or agnostic, while 3.5% claim to believe in God but do not follow a specific religion. 1.8% of Colombians adhere to Jehovah's Witnesses and Adventism and less than 1% adhere to other religions, such as Islam, Judaism, Buddhism, Mormonism, Hinduism, Indigenous religions, Hare Krishna movement, Rastafari movement, Orthodox Catholic Church, and spiritual studies. The remaining people either did not respond or replied that they did not know. In addition to the above statistics, 35.9% of Colombians reported that they did not practice their faith actively.[264][265][266]		While Colombia remains a mostly Roman Catholic country by baptism numbers, the 1991 Colombian constitution guarantees freedom of religion and all religious faiths and churches are equally free before the law.[267]		Colombia is a highly urbanized country. The largest cities in the country are Bogotá, with an estimated 8 million inhabitants, Medellín, with an estimated 2.5 million inhabitants, Cali, with an estimated 2.4 million inhabitants, and Barranquilla, with an estimated 1.2 million inhabitants. Cartagena highlights in number of inhabitants and the city of Bucaramanga is relevant in terms of metropolitan area population.[239]		Colombia lies at the crossroads of Latin America and the broader American continent, and as such has been hit by a wide range of cultural influences. Native American, Spanish and other European, African, American, Caribbean, and Middle Eastern influences, as well as other Latin American cultural influences, are all present in Colombia's modern culture. Urban migration, industrialization, globalization, and other political, social and economic changes have also left an impression.		Many national symbols, both objects and themes, have arisen from Colombia's diverse cultural traditions and aim to represent what Colombia, and the Colombian people, have in common. Cultural expressions in Colombia are promoted by the government through the Ministry of Culture.		Colombian literature dates back to pre-Columbian era; a notable example of the period is the epic poem known as the Legend of Yurupary.[269] In Spanish colonial times, notable writers include Juan de Castellanos (Elegías de varones ilustres de Indias), Hernando Domínguez Camargo and his epic poem to San Ignacio de Loyola, Pedro Simón, Juan Rodríguez Freyle (El Carnero),[270] Lucas Fernández de Piedrahita, and the nun Francisca Josefa de Castillo, representative of mysticism.		Post-independence literature linked to Romanticism highlighted Antonio Nariño, José Fernández Madrid, Camilo Torres Tenorio and Francisco Antonio Zea.[271][272] In the second half of the nineteenth century and early twentieth century the literary genre known as costumbrismo became popular; great writers of this period were Tomás Carrasquilla, Jorge Isaacs and Rafael Pombo (the latter of whom wrote notable works of children's literature).[273][274] Within that period, authors such as José Asunción Silva, José Eustasio Rivera, León de Greiff, Porfirio Barba-Jacob and José María Vargas Vila developed the modernist movement.[275][276][277] In 1872, Colombia established the Colombian Academy of Language, the first Spanish language academy in the Americas.[278] Candelario Obeso wrote the groundbreaking Cantos Populares de mi Tierra (1877), the first book of poetry by an Afro-Colombian author.[279][280]		Between 1939 and 1940 seven books of poetry were published under the name Stone and Sky in the city of Bogotá that significantly impacted the country; they were edited by the poet Jorge Rojas.[281] In the following decade, Gonzalo Arango founded the movement of "nothingness" in response to the violence of the time;[282] he was influenced by nihilism, existentialism, and the thought of another great Colombian writer: Fernando González Ochoa.[283] During the boom in Latin American literature, successful writers emerged, led by Nobel laureate Gabriel García Márquez and his magnum opus, One Hundred Years of Solitude, Eduardo Caballero Calderón, Manuel Mejía Vallejo, and Álvaro Mutis, a writer who was awarded the Cervantes Prize and the Prince of Asturias Award for Letters.[284][285] Other leading contemporary authors are Fernando Vallejo, William Ospina (Rómulo Gallegos Prize) and Germán Castro Caycedo.		Colombian art has over 3,000 years of history. Colombian artists have captured the country's changing political and cultural backdrop using a range of styles and mediums. There is archeological evidence of ceramics being produced earlier in Colombia than anywhere else in the Americas, dating as early as 3,000 BCE.[286][287]		The earliest examples of gold craftsmanship have been attributed to the Tumaco people [288] of the Pacific coast and date to around 325 BCE. Roughly between 200 BCE and 800 CE, the San Agustín culture, masters of stonecutting, entered its “classical period". They erected raised ceremonial centres, sarcophagi, and large stone monoliths depicting anthropomorphic and zoomorphhic forms out of stone.[287][289]		Colombian art has followed the trends of the time, so during the 16th to 18th centuries, Spanish Catholicism had a huge influence on Colombian art, and the popular baroque style was replaced with rococo when the Bourbons ascended to the Spanish crown.[290][291] More recently, Colombian artists Pedro Nel Gómez and Santiago Martínez Delgado started the Colombian Murial Movement in the 1940s, featuring the neoclassical features of Art Deco.[286][287][292][293]		Since the 1950s, the Colombian art started to have a distinctive point of view, reinventing traditional elements under the concepts of the 20th century. Examples of this are the Greiff portraits by Ignacio Gómez Jaramillo, showing what the Colombian art could do with the new techniques applied to typical Colombian themes. Carlos Correa, with his paradigmatic “Naturaleza muerta en silencio” (silent dead nature), combines geometrical abstraction and cubism. Alejandro Obregón is often considered as the father of modern Colombian painting, and one of the most influential artist in this period, due to his originality, the painting of Colombian landscapes with symbolic and expressionist use of animals, (specially the Andean condor).[287][294][295] Fernando Botero, Omar Rayo, Enrique Grau, Édgar Negret, David Manzur, Rodrigo Arenas Betancourt and Oscar Murillo are some of the Colombian artists featured at the international level.[286][296] [297][298]		The Colombian sculpture from the sixteenth to 18th centuries was mostly devoted to religious depictions of ecclesiastic art, strongly influenced by the Spanish schools of sacred sculpture. During the early period of the Colombian republic, the national artists were focused in the production of sculptural portraits of politicians and public figures, in a plain neoclassicist trend.[299] During the 20th century, the Colombian sculpture began to develop a bold and innovative work with the aim of reaching a better understanding of national sensitivity.[287][300]		Colombian photography was marked by the arrival of the daguerreotype. Jean-Baptiste Louis Gros was who brought the daguerreotype process to Colombia in 1841. The Piloto public library has Latin America’s largest archive of negatives, containing 1.7 million antique photographs covering Colombia 1848 until 2005.[301][302]		The Colombian press has promoted the work of the cartoonists. In recent decades, fanzines, internet and independent publishers have been fundamental to the growth of the comic in Colombia.[303][304][305]		Throughout the times, there have been a variety of architectural styles, from those of indigenous peoples to contemporary ones, passing through colonial (military and religious), Republican, transition and modern styles.[306]		Ancient habitation areas, longhouses, crop terraces, roads as the Inca road system, cemeteries, hypogeums and necropolises are all part of the architectural heritage of indigenous peoples.[307] Some prominent indigenous structures are the preceramic and ceramic archaeological site of Tequendama,[308] Tierradentro (a park that contains the largest concentration of pre-Columbian monumental shaft tombs with side chambers),[309] the largest collection of religious monuments and megalithic sculptures in South America, located in San Agustín, Huila.[289][310] Lost city (an archaeological site with a series of terraces carved into the mountainside, a net of tiled roads and several circular plazas) and also stand out the large villages mainly built with stone, wood, cane and mud.[311]		Architecture during the period of conquest and colonization is mainly derived of adapting European styles to local conditions, and Spanish influence, especially Andalusian and Extremaduran, can be easily seen.[312] When Europeans founded cities two things were making simultaneously: the dimensioning of geometrical space (town square, street), and the location of a tangible point of orientation.[313] The construction of forts was common throughout the Caribbean and in some cities of the interior, because of the dangers that represented the hostile indigenous groups and the pirates who roamed the seas.[314] Churches, chapels, schools, and hospitals belonging to religious orders cause a great urban impact.[315] Baroque architecture is used in military buildings and public spaces.[316] Marcelino Arroyo, Francisco José de Caldas and Domingo de Petrés were great representatives of neo-classical architecture.[315]		The National Capitol is a great representative of romanticism.[317] Wood is extensively used in doors, windows, railings and ceilings during the colonization of Antioquia. The Caribbean architecture acquires a strong Arabic influence.[318] The Teatro Colón in Bogotá is a lavish example of architecture from the 19th century.[319] The quintas houses with innovations in the volumetric conception are some of the best examples of the Republican architecture; the Republican action in the city focused on the design of three types of spaces: parks with forests, small urban parks and avenues and the Gothic style was most commonly used for the design of churches.[320]		Deco style, modern neoclassicism, eclecticism folklorist and art deco ornamental resources significantly influenced the architecture of Colombia, especially during the transition period.[321] Modernism contributed with new construction technologies and new materials (steel, reinforced concrete, glass and synthetic materials) and the topology architecture and lightened slabs system also have a great influence.[322] The most influential architects of the modern movement were Rogelio Salmona and Fernando Martínez Sanabria.[323]		The contemporary architecture of Colombia is designed to give greater importance to the materials, this architecture takes into account the specific natural and artificial geographies and is also an architecture that appeals to the senses.[324] The conservation of the architectural and urban heritage of Colombia has been promoted in recent years.[325]		Colombian music blends European-influenced guitar and song structure with large gaita flutes and percussion instruments from the indigenous population, while its percussion structure and dance forms come from Africa. Colombia has a diverse and dynamic musical environment.[326] Musicians, composers, music producers and singers from Colombia are recognized internationally such as Shakira, Juanes, Carlos Vives and others.[327]		Guillermo Uribe Holguín, an important cultural figure in the National Symphony Orchestra of Colombia, Luis Antonio Calvo and Blas Emilio Atehortúa are some of the greatest exponents of the art music.[328] The Bogotá Philharmonic Orchestra is one of the most active orchestras in Colombia.[329]		Caribbean music has many vibrant rhythms, such as cumbia (it is played by the maracas, the drums, the gaitas and guacharaca), porro (it is a monotonous but joyful rhythm), mapalé (with its fast rhythm and constant clapping) and the "vallenato", which originated in the northern part of the Caribbean coast (the rhythm is mainly played by the caja, the guacharaca, and accordion).[330][331][332][333][334]		The music from the Pacific coast, such as the currulao is characterized by its strong use of drums (instruments such as the native marimba, the conunos, the bass drum, the side drum and the cuatro guasas or tubular rattle). An important rhythm of the south region of the Pacific coast is the contradanza (it is used in dance shows, as a result of the striking colours of the costumes).[330][335][336] Marimba music, traditional chants and dances from the Colombia South Pacific region are on UNESCO's Representative List of the Intangible Cultural Heritage of Humanity.[337][338][339]		Important musical rhythms of the Andean Region are the danza (dance of Andean folklore arising from the transformation of the European contredance), the bambuco (it is played with guitar, tiple[340] and mandolin, the rhythm is danced by couples), the pasillo (a rhythm inspired by the Austrian waltz and the Colombian "danza", the lyrics have been composed by well-known poets), the guabina (the tiple, the bandola and the requinto are the basic instruments), the sanjuanero (it originated in Tolima and Huila Departments, the rhythm is joyful and fast).[341][342][343][344][345] Apart from these traditional rhythms, salsa music has spread throughout the country, and the city of Cali is considered by many salsa singers to be 'The New Salsa Capital of the World'.[330][346][347]		The instruments that distinguish the music of the Eastern Plains are the harp, the cuatro (a type of four-stringed guitar) and maracas. Important rhythms of this region are the joropo (a fast rhythm and there is also tapping as a result of its flamenco ancestry) and the galeron (it is heard a lot while cowboys are working).[330][348][349][350]		The music of the Amazon region is strongly influenced by the indigenous religious practices. Some of the musical instruments used are the manguaré (a musical instrument of ceremonial type, consisting of a pair of large cylindrical drums), the quena (melodic instrument), the rondador, the congas, bells, and different types of flutes.[351][352][353]		The music of the Archipelago of San Andrés, Providencia and Santa Catalina is usually accompanied by a mandolin, a tub-bass, a jawbone, a guitar and maracas. Some popular archipelago rhythms are the Schottische, the Calypso, the Polka and the Mento.[354][355]		Theater was introduced in Colombia during the Spanish colonization in 1550 through zarzuela companies. Colombian theater is supported by the Ministry of Culture and a number of private and state owned organizations. The Ibero-American Theater Festival of Bogotá is the cultural event of the highest importance in Colombia and one of the biggest theater festivals in the world.[356] Other important theater events are: The Festival of Puppet The Fanfare (Medellín), The Manizales Theater Festival, The Caribbean Theatre Festival (Santa Marta) and The Art Festival of Popular Culture "Cultural Invasion" (Bogotá).[357][358][359]		Although the Colombian cinema is young as an industry, more recently the film industry was growing with support from the Film Act passed in 2003.[361] Many film festivals take place in Colombia, but the two most important are the Cartagena Film Festival, which is the oldest film festival in Latin America, and the Bogotá Film Festival.[360][362][363]		Some important national circulation newspapers are El Tiempo and El Espectador. Television in Colombia has two privately owned TV networks and three state-owned TV networks with national coverage, as well as six regional TV networks and dozens of local TV stations. Private channels, RCN and Caracol are the highest-rated. The regional channels and regional newspapers cover a department or more and its content is made in these particular areas.[364][365][366]		Colombia has three major national radio networks: Radiodifusora Nacional de Colombia, a state-run national radio; Caracol Radio and RCN Radio, privately owned networks with hundreds of affiliates. There are other national networks, including Cadena Super, Todelar, and Colmundo. Many hundreds of radio stations are registered with the Ministry of Information Technology and Communications.[367]		Colombia's varied cuisine is influenced by its diverse fauna and flora as well as the cultural traditions of the ethnic groups. Colombian dishes and ingredients vary widely by region. Some of the most common ingredients are: cereals such as rice and maize; tubers such as potato and cassava; assorted legumes; meats, including beef, chicken, pork and goat; fish; and seafood.[368][369] Colombia cuisine also features a variety of tropical fruits such as cape gooseberry, feijoa, arazá, dragon fruit, mangostino, granadilla, papaya, guava, mora (blackberry), lulo, soursop and passionfruit.[370] Colombia is one of the world's largest consumers of fruit juices.[371]		Among the most representative appetizers and soups are patacones (fried green plantains), sancocho de gallina (chicken soup with root vegetables) and ajiaco (potato and corn soup). Representative snacks and breads are pandebono, arepas (corn cakes), aborrajados (fried sweet plantains with cheese), torta de choclo, empanadas and almojábanas. Representative main courses are bandeja paisa, lechona tolimense, mamona, tamales and fish dishes (such as arroz de lisa), especially in coastal regions where kibbeh, suero, costeño cheese and carimañolas are also eaten. Representative side dishes are papas chorreadas (potatoes with cheese), remolachas rellenas con huevo duro (beets stuffed with hard-boiled egg) and arroz con coco (coconut rice).[370][368] Organic food is a current trend in big cities, although in general across the country the fruits and veggies are very natural and fresh.[372][373]		Representative desserts are buñuelos, natillas, Maria Luisa cake, bocadillo made of guayaba (guava jelly), cocadas (coconut balls), casquitos de guayaba (candied guava peels), torta de natas, obleas, flan de mango, roscón, milhoja, manjar blanco, dulce de feijoa, dulce de papayuela, torta de mojicón, and esponjado de curuba. Typical sauces (salsas) are hogao (tomato and onion sauce) and Colombian-style ají.[370][368]		Some representative beverages are coffee (Tinto), champús, cholado, lulada, avena colombiana, sugarcane juice, aguapanela, aguardiente, hot chocolate and fresh fruit juices (often made with water or milk).[370][368]		Tejo is Colombia’s national sport and is a team sport that involves launching projectiles to hit a target.[374] But of all sports in Colombia, football is the most popular. Colombia was the champion of the 2001 Copa América, in which they set a new record of being undefeated, conceding no goals and winning each match. Interestingly, Colombia has been awarded “mover of the year” twice.[375]		Colombia is a mecca for roller skaters. The national team is a perennial powerhouse at the World Roller Speed Skating Championships.[376] Colombia has traditionally been very good in cycling and a large number of Colombian cyclists have triumphed in major competitions of cycling.[377]		In baseball, another sport rooted in the Caribbean Coast, Colombia was world amateur champion in 1947 and 1965. Baseball is popular in the Caribbean, mainly in the cities Cartagena, Barranquilla and Santa Marta. Of those cities have come good players like: Orlando Cabrera, Édgar Rentería[378] who was champion of the World Series in 1997 and 2010, and others who have played in Major League Baseball.[379]		Boxing is one of the sports that more world champions has produced for Colombia.[380][381] Motorsports also occupies an important place in the sporting preferences of Colombians; Juan Pablo Montoya is a race car driver known for winning 7 Formula One events. Colombia also has excelled in sports such as BMX, judo, shooting sport, taekwondo, wrestling, high diving and athletics, also has a long tradition in weightlifting and bowling.[382][383][384]		The overall life expectancy in Colombia at birth is 74.8 years (71.2 years for males and 78.4 years for females).[241] Health standards in Colombia have improved very much since the 1980s, healthcare reforms have led to the massive improvements in the healthcare systems of the country. Although this new system has widened population coverage by the social and health security system from 21% (pre-1993) to 96% in 2012,[386] health disparities persist.		Through health tourism, many people from over the world travel from their places of residence to other countries in search of medical treatment and the attractions in the countries visited. Colombia is projected as one of Latin America’s main destinations in terms of health tourism due to the quality of its health care professionals, a good number of institutions devoted to health, and an immense inventory of natural and architectural sites. Cities such as Bogotá, Cali, Medellín and Bucaramanga are the most visited in cardiology procedures, neurology, dental treatments, stem cell therapy, ENT, ophthalmology and joint replacements among others for the medical services of high quality.[387]		A study conducted by América Economía magazine ranked 21 Colombian health care institutions among the top 44 in Latin America, amounting to 48 percent of the total.[385]		The educational experience of many Colombian children begins with attendance at a preschool academy until age five (Educación preescolar). Basic education (Educación básica) is compulsory by law.[388] It has two stages: Primary basic education (Educación básica primaria) which goes from first to fifth grade – children from six to ten years old, and Secondary basic education (Educación básica secundaria), which goes from sixth to ninth grade. Basic education is followed by Middle vocational education (Educación media vocacional) that comprises the tenth and eleventh grades. It may have different vocational training modalities or specialties (academic, technical, business, and so on.) according to the curriculum adopted by each school.[389]		After the successful completion of all the basic and middle education years, a high-school diploma is awarded. The high-school graduate is known as a bachiller, because secondary basic school and middle education are traditionally considered together as a unit called bachillerato (sixth to eleventh grade). Students in their final year of middle education take the ICFES test (now renamed Saber 11) in order to gain access to higher education (Educación superior). This higher education includes undergraduate professional studies, technical, technological and intermediate professional education, and post-graduate studies. Technical professional institutions of Higher Education are also opened to students holder of a qualification in Arts and Business. This qualification is usually awarded by the SENA after a two years curriculum.[390]		Bachilleres (high-school graduates) may enter into a professional undergraduate career program offered by a university; these programs last up to five years (or less for technical, technological and intermediate professional education, and post-graduate studies), even as much to six to seven years for some careers, such as medicine. In Colombia, there is not an institution such as college; students go directly into a career program at a university or any other educational institution to obtain a professional, technical or technological title. Once graduated from the university, people are granted a (professional, technical or technological) diploma and licensed (if required) to practice the career they have chosen. For some professional career programs, students are required to take the Saber-Pro test, in their final year of undergraduate academic education.[389]		Public spending on education as a proportion of gross domestic product in 2015 was 4.49%. This represented 15.05% of total government expenditure. The primary and secondary gross enrolment ratios stood at 113.56% and 98.09% respectively. School-life expectancy was 14.42 years. A total of 94.58% of the population aged 15 and older were recorded as literate, including 98.66% of those aged 15–24.[243]		General information		Government		Culture		Geography		 Amazonas  Antioquia  Arauca  Atlántico  Bolívar  Boyacá		 Caldas  Caquetá  Casanare  Cauca  Cesar  Chocó		 Córdoba  Cundinamarca  Guainía  Guaviare  Huila  La Guajira		 Magdalena  Meta  Nariño  N. Santander  Putumayo  Quindío		 Risaralda  San Andrés  Santander  Sucre  Tolima  Valle del Cauca		 Vaupés  Vichada Capital district:  Bogotá				
Compulsory education refers to a period of education that is required of all people and is imposed by law. Depending on the country, this education may take place at a registered school (schooling) or at home (homeschooling).		International Covenant on Economic, Social and Cultural Rights requires, within a reasonable number of years, the principle of compulsory education free of charge for all.[1]						Compulsory education was not unheard of in ancient times. However instances are generally tied to royal, religious or military organization—substantially different from modern notions of compulsory education.		Plato's The Republic (c. 424–c. 348 BCE) is credited with having popularized the concept of compulsory education in Western intellectual thought. Plato's rationale was straight-forward. The ideal city would require ideal individuals, and ideal individuals would require an ideal education. Popularization of Plato's ideas began with the wider Renaissance and the translation of Plato's works by Marsilio Ficino (1433–1499), culminating in the Enlightenment. The Enlightenment philosopher Jean-Jacques Rousseau, known for his own work on education, Emile, or On Education had said To get a good idea of public education, read Plato’s Republic. It is not a political treatise, as those who merely judge books by their title think, but it is the finest, most beautiful work on education ever written.		In Sparta boys between the age 6 and 7 left their homes and were sent to military school. School courses were harsh and have been described as a "brutal training period". Between the age of 18 and 20, Spartan males had to pass a test that consisted of fitness, military ability, and leadership skills. A student's failure meant a forfeiture of citizenship (perioidos) and political rights. Passing was a rite of passage to manhood and citizenry, in which he would continue to serve in the military and train as a soldier until the age of 60, when the soldier could retire to live with his family.[2]		Every parent in Judea since ancient times was required to teach their children at least informally. Over the centuries, as cities, towns and villages developed, a class of teachers called Rabbis evolved. According to the Talmud (tractate Bava Bathra 21a), which praises the sage Joshua ben Gamla with the institution of formal Jewish education in the 1st century AD, Ben Gamla instituted schools in every town and made formal education compulsory from age 6-7.[3]		The Aztec Triple Alliance, which ruled from 1428 to 1521 in what is now central Mexico, is considered to be the first state to implement a system of universal compulsory education.[4][5]		The Protestant Reformation prompted the establishment of compulsory education for boys and girls, first in regions that are now part of Germany, and later in Europe and in the United States.		Martin Luther's seminal text An die Ratsherren aller Städte deutschen Landes (To the Councillors of all Towns in German Countries,1524) called for establishing compulsory schooling so that all parishioners would be able to read the Bible by themselves.[6] The Protestant South-West of the Holy Roman Empire soon followed suit. In 1559, the German Duchy Württemberg established a compulsory education system for boys.[7] In 1592, the German Duchy Palatinate-Zweibrücken became the first territory in the world with compulsory education for girls and boys,[8] followed in 1598 by Strasbourg, then a free city of the Holy Roman Empire and now part of France.		In Scotland, the School Establishment Act of 1616 commanded every parish to establish a school for everyone paid for by parishioners. The Parliament of Scotland confirmed this with the Education Act of 1633 and created a local land-based tax to provide the required funding. The required majority support of parishioners, however, provided a tax evasion loophole which heralded the Education Act of 1646. The turmoil of the age meant that in 1661 there was a temporary reversion to the less compulsory 1633 position. However, in 1696 a new Act re-established the compulsory provision of a school in every parish with a system of fines, sequestration, and direct government implementation as a means of enforcement where required.		In the United States, following Luther and other Reformers, the Separatist Congregationalists who founded Plymouth Colony in 1620, obliged parents to teach their children how to read and write.[9] The Massachusetts School Laws, three legislative acts enacted in the Massachusetts Bay Colony in 1642, 1647, and 1648, are commonly regarded as the first steps toward compulsory education in the United States. The 1647 law, in particular, required every town having more than 50 families to hire a teacher, and every town of more than 100 families to establish a school.[10] The Puritan zeal for learning was reflected in the early and rapid rise of educational institutions; e.g., Harvard College was founded as early as 1636.[11]		Prussia implemented a modern compulsory education system in 1763. It was introduced by the Generallandschulreglement (General School Regulation), a decree of Frederick the Great in 1763-5.[12] The Generallandschulreglement, authored by Johann Julius Hecker, asked for all young citizens, girls and boys, to be educated from age 5 to age 13-14 and to be provided with a basic outlook on (Christian) religion, singing, reading and writing based on a regulated, state-provided curriculum of text books. The teachers, often former soldiers, were asked to cultivate silk worms to make a living besides contributions from the local citizens and municipalities.[13][14]		In Austria, Hungary and the Lands of the Bohemian Crown (Czech lands), mandatory primary education was introduced by Empress Maria Theresa in 1774.[12]		Compulsory school attendance based on the Prussian model gradually spread to other countries. It was quickly adopted by the governments in Denmark-Norway and Sweden, and also in Finland, Estonia and Latvia within the Russian Empire, but it was rejected in Russia itself.[15][16]		The United Kingdom was slow to introduce compulsory education due to the upper class defending its educational privileges.[16] In England and Wales, the Elementary Education Act of 1870 paved the way for compulsory education by establishing school boards to set up schools in any places that did not have adequate provision. Attendance was made compulsory until age 10 in 1880. The Education Act of 1996 made it an obligation on parents to require children to have a full-time education from age 5 to 16. However, attendance at school itself is not compulsory: Section 7 of the Act allows for home education.		France was equally slow to introduce compulsory education, this time due to conflicts between the secular state and the Catholic Church,[16] and as a result between anti-clerical and Catholic political parties. The first set of Jules Ferry Laws, passed in 1881, made primary education free for girls and boys; communes and departments had the shared responsibility to fund it. In 1882, the second set of Jules Ferry Laws made education compulsory for girls and boys until the age of 13. In 1936, the upper age limit was raised to 14.[17] In 1959, it was further extended to 16.[18]		In 1852, Massachusetts was the first U.S. state to pass a contemporary universal public education law. In particular, the Massachusetts General Court required every town to create and operate a grammar school. Fines were imposed on parents who did not send their children to school, and the government took the power to take children away from their parents and apprentice them to others if government officials decided that the parents were "unfit to have the children educated properly".[19]		The spread of compulsory attendance in the Massachusetts tradition throughout the U.S., especially for Native Americans, has been credited to General Richard Henry Pratt.[20] Pratt used techniques developed on Native Americans in a prisoner of war camp in Fort Marion, Augustine, Florida, to force demographic minorities across America into government schools.[20] His prototype was the Carlisle Indian Industrial School in Pennsylvania.		In 1918, Mississippi was the last state to enact a compulsory attendance law.[21]		In 1922 an attempt was made by the voters of Oregon to enact the Oregon Compulsory Education Act, which would require all children between the ages of 8 and 16 to attend State School. Only leaving exceptions for mentally or physically unfit children, exceeding a certain living distance from a state school, or having written consent from a county superintendent to receive private instruction.[22] The law was passed by popular vote but was later ruled unconstitutional by the United States Supreme Court in Pierce v. Society of Sisters, determining that "a child is not a mere creature of the state". This case settled the dispute about whether or not private schools had the right to do business and educate within the United States.		In Japan, compulsory education was established shortly after the Meiji Restoration in 1868. Initially, it was strongly influenced by the Prussian education system. After World War II, it was rebuilt to a large extent, and the new education model is largely influenced by the American model.[23]		The following table indicates at what ages compulsory education starts and ends, country by country. The most common age for starting compulsory education is 6, although this varies between 3 and 8.[24][25]		Due to population growth and the proliferation of compulsory education, UNESCO calculated in 2006 that over the subsequent 30 years, more people would receive formal education than in all prior human history.[47]		
An adult learner (North America) or mature learner (UK) (sometimes also called adult student, returning adult, and adult returner) is a person who is 25 years and up who is involved in forms of learning. Adult learners fall in a specific criteria of being experienced, and do not have the high school diploma. Many of the adult learners go back to school to finish of a degree, or earn a new one.[1]		Malcolm Knowles's work distinguished adult learners as distinct from adolescent and child learners in his principle of andragogy.[2] He established 5 assumptions about the adult learner. This included self-concept, adult learner experience, readiness to learn, orientation to learning, and motivation to learning. [1]						In the US, adult learners fall into the category of nontraditional students, whom the National Center for Education Statistics defines as meeting at least one of the following seven criteria:		It should be noted that not all non-traditional students are adult learners, as the term refers to the brain development of the person, but adult learners are considered non-traditional students. This can be due to the wide range of cultural, job, and educational backgrounds.[2]		In the UK, a student is normally classified as a mature student if he or she is an (undergraduate) student who is at least 25+ years old at the start of his or her course, or in the Irish case on the first of January of the year of entry, and usually having been away from school for at least two years.[1] The normal entry requirements for school-leavers wishing to start an undergraduate degree are often not applied to mature students.[1][citation needed]		Adult students are frequently referred to as nontraditional students in higher education. Adult students are contrasted with traditional students, who are typically aged 18-22, attend full-time, live on campus, do not work, and have few, if any, family responsibilities.[3]. In 2008, 36 percent of postsecondary students were age 25 or older and 47 percent were independent students.[4]		As opposed to a child or adolescent, adult learners typically have more life experience and their brains are more fully developed. When confronted with new knowledge or an experience, adult learners construe new meaning based on their life experiences and their more developed brains process these experiences differently than someone with a less developed brain (children and adolescents).[5]		
Bullying in academia is workplace bullying of scholars and staff in academia, especially places of higher education such as colleges and universities. It is believed to be common, although has not received as much attention from researchers as bullying in some other contexts.[1]						Bullying is the longstanding violence, physical or psychological, conducted by an individual or group and directed against an individual who is not able to defend himself in the actual situation, with a conscious desire to hurt, threaten, or frighten that individual or put him under stress.[2]		Workplace bullying ranges into the following categories.[3]		Several aspects of academia lend themselves to the practice and discourage its reporting and mitigation. Its leadership is usually drawn from the ranks of faculty, most of whom have not received the management training that could enable an effective response to such situations.[4] The perpetrators may possess tenure — a high-status and protected position – or the victims may belong to the increasing number of adjunct professors, who are often part-time employees.[4]		Academic mobbing is arguably the most prominent type of bullying in academia. Academic victims of bullying may also be particularly conflict-averse.[4]		The generally decentralized nature of academic institutions can make it difficult for victims to seek recourse, and appeals to outside authority have been described as "the kiss of death."[5][6][7] Therefore, academics who are subject to bullying in workplace are often cautious about reporting any problems. Social media has recently been used to expose or allege bullying in academia anonymously.[8] Bullying research credits an organizational rift in two interdependent and adversarial systems that comprise a larger structure of nearly all colleges and universities worldwide: faculty and administration. While both systems distribute employee power across standardized bureaucracies, administrations favor an ascription-oriented business model with a standardized criteria determining employee rank.		Faculty depend on greater open-ended and improvised standards that determine rank and job retention. The leveraged intradepartmental peer reviews (although often at a later time, these three reviews are believed to be leveraged by the fact the peers determine promotions of one another at later times) of faculty for annual reappointment of tenure-track, tenure, and post-tenure review is believed to offer "unregulated gray area" that nurture the origin of bullying cases in academia. Although tenure and post-tenure review lead to interdepartmental evaluation, and all three culminate in an administrative decision, bullying is commonly a function of administrative input before or during the early stages of intradepartmental review.		Kenneth Westhues' study of mobbing in academia found that vulnerability was increased by personal differences such as being a foreigner or of a different sex; by working in a post-modern field such as music or literature; financial pressure; or having an aggressive superior.[9] Other factors included envy, heresy and campus politics.[9]		The bullying in this workplace has been described as somewhat more subtle than usual.[6] Its recipients may be the target of unwanted physical contact, violence, obscene or loud language during meetings, be disparaged among their colleagues in venues they are not aware of, and face difficulties when seeking promotion.[6][10] It may also be manifested by undue demands for compliance with regulations.[11]		A 2008 study of the topic, conducted on the basis of a survey at a Canadian university, concluded that the practice had several unproductive costs, including increased employee turnover.[12]		In 2008 the United Kingdom's University and College Union released the results of a survey taken among its 9,700 members.[13] 51% of respondents said they had never been bullied, 16.7% that they had occasionally experienced it, and 6.7% that they were "always" or "often" subjected to bullying.[13] The results varied by member institutions, with respondents from the University of East London reporting the highest incidence.[13]		The Times Higher Education commissioned a survey in 2005 and received 843 responses.[10] Over 40% reported they had been bullied, with 33% reporting "unwanted physical contact" and 10% reporting physical violence; about 75% reported they were aware that co-workers had been bullied.[10] The incidence rate found in this survey was higher than that usually found via internal polling (12 to 24 percent).[10]		Author C. K. Gunsalus describes the problem as "low incidence, high severity," analogous to research misconduct.[5] She identifies the aggressors' misuse of the concepts of academic freedom and collegiality as a commonly used strategy.[5]		In a 2005 British study, around 35% of medical students reported having been bullied. Around one in four of the 1,000 students questioned said they had been bullied by a doctor, while one in six had been bullied by a nurse. Manifestations of bullying included:[14]		Hollis, L. P. (2015). Bully university? The cost of workplace bullying and employee disengagement in American higher education. Sage Open, 5(2), 2158244015589997.		Books		Academic papers		
The Free Speech Movement (FSM) was a student protest which took place during the 1964–65 academic year on the campus of the University of California, Berkeley under the informal leadership of students Mario Savio,[1] Jack Weinberg, Michael Rossman, George Barton, Brian Turner, Bettina Aptheker, Steve Weissman, Michael Teal, Art Goldberg, Jackie Goldberg, and others. In protests unprecedented in scope, students insisted that the university administration lift the ban of on-campus political activities and acknowledge the students' right to free speech and academic freedom. However, former FSM activist Sol Stern acknowledged in later years that the group's name was misleading and was not a true representation of the FSM's real agenda.[2]						In 1958, activist students organized SLATE, a campus political party meaning a "slate" of candidates running on the same level – a same "slate." The students created SLATE to promote the right of student groups to support off-campus issues.[3] In the fall of 1964, student activists, some of whom had traveled with the Freedom Riders and worked to register African American voters in Mississippi in the Freedom Summer project, set up information tables on campus and were soliciting donations for causes connected to the Civil Rights Movement. According to existing rules at the time, fundraising for political parties was limited exclusively to the Democratic and Republican school clubs. There was also a mandatory "loyalty oath" required of faculty, which had led to dismissals and ongoing controversy over academic freedom. Sol Stern, a former radical who took part in the Free Speech Movement,[4] stated in a 2014 City Journal article that the group viewed the United States to be both racist and imperialistic and that the main intent after lifting Berkeley's loyalty oath was to build on the legacy of C Wright Mills and weaken the Cold War consensus by promoting the ideas of the Cuban Revolution.[2]		On September 14, 1964, Dean Katherine Towle announced that existing University regulations prohibiting advocacy of political causes or candidates, outside political speakers, recruitment of members, and fundraising by student organizations at the intersection of Bancroft and Telegraph Avenues would be "strictly enforced."[5] (This strip was until then thought to be city property, not campus property.)		On October 1, 1964, former graduate student Jack Weinberg was sitting at the CORE table. He refused to show his identification to the campus police and was arrested. There was a spontaneous movement of students to surround the police car in which he was to be transported. The police car remained there for 32 hours, all while Weinberg was inside it. At one point, there may have been 3,000 students around the car. The car was used as a speaker's podium and a continuous public discussion was held which continued until the charges against Weinberg were dropped.[5]		On December 2, between 1,500 and 4,000 students went into Sproul Hall as a last resort in order to re-open negotiations with the administration on the subject of restrictions on political speech and action on campus.[5] Among other grievances was the fact that four of their leaders were being singled out for punishment. The demonstration was orderly; students studied, watched movies, and sang folk songs. Joan Baez was there to lead in the singing, as well as lend moral support. "Freedom classes" were held by teaching assistants on one floor, and a special Channukah service took place in the main lobby. On the steps of Sproul Hall, Mario Savio[1] gave a famous speech:		... But we're a bunch of raw materials that don't mean to be — have any process upon us. Don't mean to be made into any product! Don't mean — Don't mean to end up being bought by some clients of the University, be they the government, be they industry, be they organized labor, be they anyone! We're human beings! ... There's a time when the operation of the machine becomes so odious — makes you so sick at heart — that you can't take part. You can't even passively take part. And you've got to put your bodies upon the gears and upon the wheels, upon the levers, upon all the apparatus, and you've got to make it stop. And you've got to indicate to the people who run it, to the people who own it, that unless you're free, the machine will be prevented from working at all.[6]		At midnight, Alameda County deputy district attorney Edwin Meese III telephoned Governor Edmund Brown, Sr, asking for authority to proceed with a mass arrest. Shortly after 2 a.m. on December 4, 1964, police cordoned off the building, and at 3:30 a.m. began the arrest. Close to 800 students were arrested,[5] most of which were transported by bus to Santa Rita Jail in Dublin, about 25 miles away. They were released on their own recognizance after a few hours behind bars. About a month later, the university brought charges against the students who organized the sit-in, resulting in an even larger student protest that all but shut down the university.		After much disturbance, the University officials slowly backed down. By January 3, 1965, the new acting chancellor, Martin Meyerson (who had replaced the resigned Edward Strong), established provisional rules for political activity on the Berkeley campus. He designated the Sproul Hall steps an open discussion area during certain hours of the day and permitting tables. This applied to the entire student political spectrum, not just the liberal elements that drove the Free Speech Movement.[7]		Most outsiders, however, identified the Free Speech Movement as a movement of the Left. Students and others opposed to U.S. foreign policy did indeed increase their visibility on campus following the FSM's initial victory. In the spring of 1965, the FSM was followed by the Vietnam Day Committee,[5] a major starting point for the anti-Vietnam war movement.		The Free Speech Movement had long-lasting effects at the Berkeley campus and was a pivotal moment for the civil liberties movement in the 1960s. It was seen as the beginning of the famous student activism that existed on the campus in the 1960s, and continues to a lesser degree today. There was a substantial voter backlash against the individuals involved in the Free Speech Movement. Ronald Reagan won an unexpected victory in the fall of 1966 and was elected Governor.[8] He then directed the UC Board of Regents to dismiss UC President Clark Kerr because of the perception that he had been too soft on the protesters. The FBI had kept a secret file on Kerr.		Reagan had gained political traction by campaigning on a platform to "clean up the mess in Berkeley".[8] In the minds of those involved in the backlash, a wide variety of protests, concerned citizens, and activists were lumped together. Furthermore, television news and documentary filmmaking had made it possible to photograph and broadcast moving images of protest activity. Much of this media is available today as part of the permanent collection of the Bancroft Library at Berkeley, including iconic photographs of the protest activity by student Ron Enfield (then chief photographer for the Berkeley campus newspaper, the Daily Cal).[9] A reproduction of what may be considered the most recognizable and iconic photograph of the movement, a shot of suit-clad students carrying the Free Speech banner through the University's Sather Gate in Fall of 1964, now stands at the entrance to the college's Free Speech Movement Cafe.[9]		Earlier protests against the House Committee on Un-American Activities meeting in San Francisco in 1960 had included an iconic scene as protesters were literally washed down the steps inside the Rotunda of San Francisco City Hall with fire hoses. The anti-Communist film Operation Abolition[10][11][12][13] depicted this scene and became an organizing tool for the protesters.		The 20th anniversary reunion of the FSM was held during the first week of October, 1984, to considerable media attention. A rally in Sproul Plaza featured FSM veterans Mario Savio, who ended a long self-imposed silence, Jack Weinberg, and Jackie Goldberg. The week continued with a series of panels open to the public on the movement and its impact.[14] The 30th anniversary reunion, held during the first weekend of December 1994, was also a public event, with another Sproul Plaza rally featuring Savio, Weinberg, Goldberg, panels on the FSM, and current free speech issues.[15] In April 2001, UC's Bancroft Library held a symposium celebrating the opening of the Free Speech Movement Digital Archive. Although not a formal FSM reunion, many FSM leaders were on the panels and other participants were in the audience.[16] The 40th anniversary reunion, the first after Savio's death in 1996, was held in October 2004. It featured columnist Molly Ivins giving the annual Mario Savio Memorial Lecture, followed later in the week by the customary rally in Sproul Plaza and panels on civil liberties issues.[17] A Sunday meeting was a more private event, primarily a gathering for the veterans of the movement, in remembrance of Savio and of a close FSM ally, professor Reginald Zelnik, who had died in an accident in May.[18]		Today, Sproul Hall and the surrounding Sproul Plaza are active locations for protests and marches, as well as the ordinary daily tables with free literature from anyone of any political orientation who wishes to appear. A wide variety of groups of all political, religious and social persuasions set up tables at Sproul Plaza. The Sproul steps, now officially known as the "Mario Savio Steps", may be reserved by anyone for a speech or rally.[5] An on-campus restaurant commemorating the event, the Mario Savio Free Speech Movement Cafe, resides in a portion of the Moffitt Undergraduate Library.		The Free Speech Monument, commemorating the movement, was created in 1991 by artist Mark Brest van Kempen. It is located, appropriately, in Sproul Plaza. The monument consists of a six-inch hole in the ground filled with soil and a granite ring surrounding it. The granite ring bears the inscription, "This soil and the air space extending above it shall not be a part of any nation and shall not be subject to any entity's jurisdiction." The monument makes no explicit reference to the movement, but it evokes notions of free speech and its implications through its rhetoric.[19]		Sol Stern, a former radical who took part in the Free Speech Movement,[20] stated in a 2014 City Journal article that the group viewed the United States to be both racist and imperialistic and that the main intent, of Stern's own group (Root and Branch magazine), after lifting Berkeley's loyalty oath was to build on the legacy of C. Wright Mills and weaken the Cold War consensus by promoting the ideas of the Cuban Revolution.[21]		
A bachelor's degree (from Middle Latin baccalaureus) or baccalaureate (from Modern Latin baccalaureatus) is an undergraduate academic degree awarded by colleges and universities upon completion of a course of study lasting three to seven years (depending on institution and academic discipline). In some institutions and educational systems, some bachelor's degrees can only be taken as graduate or postgraduate degrees after a first degree has been completed. In countries with qualifications frameworks, bachelor's degrees are normally one of the major levels in the framework (sometimes two levels where non-honours and honours bachelor's degrees are considered separately), although some qualifications titled bachelor's degrees may be at other levels (e.g. MBBS) and some qualifications with non-bachelor's titles may be classified as bachelor's degrees (e.g. the Scottish MA and Canadian MD).		The term bachelor in the 12th century referred to a knight bachelor, who was too young or poor to gather vassals under his own banner. By the end of the 13th century, it was also used by junior members of guilds or universities. By folk etymology or wordplay, the word baccalaureus came to be associated with bacca lauri ("laurel berry") in reference to laurels being awarded for academic success or honours.[1]		Under the British system, and those influenced by it, undergraduate academic degrees are differentiated as either non-honours degrees (known variously as pass degrees, ordinary degrees or general degrees) or honours degrees, the latter sometimes denoted by the addition of "(Hons)" after the degree abbreviation.[2] An honours degree generally requires a higher academic standard than a pass degree, and in some systems an additional year of study beyond the non-honours bachelor's. In some countries, e.g. Australia, there is a "postgraduate" bachelor's honours degree, which may be taken as a consecutive academic degree, continuing on from the completion of a bachelor's degree program in the same field, or (in a similar manner to the UK system) as part of an integrated honours program.						In most African countries, the university systems follow the model of their former colonizing power. For example, the Nigerian university system is similar to the British system, while the Ivorian system is akin to the French.		Bachelor's degrees in Algerian universities are called "الليسانس" in Arabic or la licence in French; the degree normally takes three years to complete and is a part of the LMD ("license", "master", "doctorate") reform, students can enroll in a bachelor's degree program in different fields of study after having obtained their baccalauréat (the national secondary education test). The degree is typically identical to the program of France's universities, as specified in the LMD reform. Bachelor's degree programs cover most of the fields in Algerian universities, except some fields, such as Medicine and Pharmaceutical Science.		Bachelor's degrees at the University of Botswana normally take four years. The system draws on both British and American models. Degrees are classified as First Class, Second Class Division One (2:1), Second Class Division Two (2:2) and Third as in English degrees, but without being described as honours. The main degrees are named by British tradition (Arts, Science, Law, etc.), but in recent years there have been a number of degrees named after specific subjects, such as Bachelor of Library and Information.		In Morocco, a bachelor's degree is referred to as al-ʾijāzah (Arabic, French: licence). The course of study takes three years, which are further divided into two cycles. The first cycle comprises the first, or propaedeutic, year. After successfully completing their first two years, students can pursue either theoretical specialization (études fondamentales) or professional specialization (études professionnelles). The second cycle is one year long, after whose completion students receive the licence d'études fondamentales or the licence professionnelle.[3] This academic degree system was introduced in September 2003.[4]		University admission is extremely competitive, with attendant advantages and disadvantages. Nonetheless, it takes four to five years to complete a bachelor's degree. In cases of poor performance, the time limit is double the standard amount of time. For example, one may not study for more than 10 years for a five-year course. Students are normally asked to leave if they must take longer. Nigerian universities offer B.Sc., B.Tech. (usually from Universities of Technology), B.Arch. (six years), and other specialized undergraduate degrees, such as B.Eng. Science undergraduate degrees may require six months or a semester dedicated to SIWES (Students Industrial Work Experience Scheme) but it is usually mandatory for all engineering degrees. A semester for project work/thesis is required, not excluding course work, during the bachelor thesis in the final year. The classifications of degrees: first-class, second-class (upper and lower), third-class (with honours; i.e., B.Sc. (Hons)) and a pass (no honours). First- and second-class graduates are immediately eligible for advanced postgraduate degrees (i.e., M.Sc. and Ph.D.), but other classes may be required for an additional postgraduate diploma before such eligibility.[5]		Furthermore, all graduating students are obliged to do the National Youth Service Corps (NYSC) requirement, which usually takes one year, after which they are eligible to pursue higher degrees. The NYSC is a paramilitary service that involves students' being posted to different parts of the country to serve in various capacities. Principal objectives of the NYSC are to forge national cohesion, encourage students to apply their obtained knowledge to solving problems of rural Nigeria, and others. The NYSC was established by law after the Nigerian Civil War.[6]		Polytechnical schools (polytechnics) in Nigeria are not considered universities. They are mandated to educate technicians of high calibre; they offer the OND (ordinary national diploma) and the HND (higher national diploma). The polytechnics focus very strongly on practical technical training. The B.Sc. and HND are compared in engineering circles but there are significant differences in training philosophies.		Honours degrees in Nigeria are differentiated only on the basis of performance. Honours degrees include the first-class degree, second-class degrees (upper and lower) and the third-class degree, but not the pass. All university students must do an independent research project which applies the knowledge obtained during the previous years of study.		The project work must be submitted in the semester before graduation and usually takes a significant number of points. Further course work is not precluded during the project work, but the courses are fewer and are at an advanced level. Project work is orally defended before the faculty and before peers. In the sciences and engineering a demonstration of the project is usually required. The exceptions are theoretical work, for which a media project is required.		In South Africa, an honours degree is an additional postgraduate qualification in the same area as the undergraduate major, and requires at least one further year of study as well as a research report.		In Tunisia, a bachelor's degree is referred to as al-ʾijāzah in Arabic, or la license in French; the degree takes three years to complete and is a part of the LMD (license, master, doctorat) reform, students can enroll in a bachelor's degree program in different fields of study after having obtained their baccalauréat (the national secondary education test). The degree is typically identical to the program of France's universities, as specified in the LMD reform. Most universities in Tunisia offer the 3-year bachelor's degree, except some fields, which are Medicine, Pharmacy, Engineering, Architecture and Bachelor of Science in Business Administration, solely offered by Tunis Business School and lasts 4 years.[7]		In Kenya, university education is highly valued and supported by the government,[8] affluent individuals as well as corporate entities who demonstrate this by providing loans and scholarships to students who perform exceptionally well in their Kenya Certificate of Secondary Education (KCSE) examination. A bachelor's degree is awarded to students who successfully complete a three to seven-year course depending on the area of study. For most degree programs, a research project and an internship period after which a report is written by the student is a must before the student is allowed to graduate. In 2012, a number of select colleges were upgraded to university status in a bid to increase the intake of students into the degree program.[9]		In Bangladesh, universities and colleges award three- and four-year degrees (three-year degrees courses are called pass courses and four-year degree courses are called honours courses) in science and business (B.Sc., B.B.S., B.B.A., four-year and three months[clarification needed], etc.) and three- and four-year degrees in arts (B.A., B.S.S., etc.). Engineering universities provide four-year degree programs for bachelor's degree courses of study (B.Sc. in Eng and B.Sc.) . Medical colleges have five-year degree programmes (MBBS, BDS). In law education there is a two-year LL.B. degree after completing three years in a B.A. program for a total of five years of study. There is also a four-year LL.B. honours degree. The Bachelor of Architecture (B.Arch.) and Bachelor of Pharmacy (B.pharm. degree are professional degrees awarded to students who complete a five-year course of study in the field at some universities. All of these programs begin after achieving the Higher Secondary Certificate (HSC—in total 12 years of school education).		Since the undergraduate education system in China is modeled after its American counterpart, all the degrees are adapted from those of the United States excepting the release of the degree certificate. Once a student has fulfilled his/her course requirements, a graduate certificate will be given. In order to get the degree, a student must finish and pass the dissertation stage; only then will he or she be awarded a degree credentialed by the Ministry of Education of the People's Republic of China. Four years of education is the standard length, although some private small colleges not credentialed by the Ministry of Education do offer three-year programs. Normally, about 90% of graduates are able to obtain a degree; however, no degree is awarded with excellency or honor. It is also referred to as a "Xueshi" (學士).		The colonial link and the establishment of the University of the South Pacific in 1968 allowed the education system to follow suit from the qualification system of the Commonwealth. University of the South Pacific is the only university in the Oceania region to be internationally recognized outside Australia and New Zealand with its bachelor's and other awards program. It is also the highest ranked in the university ranking in the island region and also ranked above some Australian universities like the University of Canberra, University of Sunshine Coast and New Zealand universities like Lincoln University and Waikato Institute of Technology.[10]		Bachelor's degrees in engineering are four-year degree programmes while medical colleges are five-year degree programmes. Bachelor's degrees (BE, graduate in engineering, BArch, BTech, BSc) that also begin after secondary school year twelve (also called +2). The Bachelor of Technology (commonly abbreviated as BTech) is an undergraduate academic degree conferred after completion of a three or four-year programme of studies at an accredited university or accredited university-level institution. In India, the Bachelor of Technology degree is a professional engineering degree awarded after completion of four-years of extensive/vast engineering study and research.		In India, BTech. is otherwise called as BE. Some universities offer it as BTech and some as BE. However, the name of degree does not make any difference viz, as the curriculum of AICTE/UGC is standard all across. Mostly all autonomous government organisation confer a BTech degree and private institutes which are affiliated to regional universities confer BE degree. The Bachelor of Architecture (BArch) degree programme is of five years' duration while still people could pursue civil engineering which has a duration of four years its is under BTech as it known in India. The Bachelor of Science in Agriculture, BSc or is a four-year full-time degree. There are also some integrated programmes. The techno-legal degree like BTech with LLB is a six-year full-time degree course in Engineering and Law. In the general curriculum, there are three and four year programmes, with Honours track being in the four year category. A bachelor's degree (BA, BCom, BSc, BBA etc.) is awarded by the respective university to which the college is affiliated which is of three years. BCom is most commonly pursued degree in India. The duration of the course is three to four years and minimum eligibility is 10+2 from any stream. BCA (Bachelor of Computer Applications) is a degree of 3 years in India, as are journalism degrees. These in India are known by various names like BJ (Bachelor of Journalism), BCJ (Bachelor of Communication and Journalism), BMM (Bachelor of Mass Media), BJMC (Bachelor of Journalism and Mass Communication), BAJMC (Bachelor of Arts in Journalism and Mass Communication), BAMC (Bachelor of Arts in Mass Communication). Employability prospects vary by the reputation of the institute and course. A majority of BBA colleges in India offer this bachelor's degree programme in the form of a three-year course. However, there are four-year part-time courses as well. A student is eligible to study BBA in India only if s/he has passed the 10+2 level examination or higher secondary examination from a recognised board or council in the country. A BBA degree can be portrayed as the gateway to the global business sector. This authentic business management course includes subjects like the following:		Integrated Bachelor of Computer Application (BCA) can be pursued in India. Bachelor of Computer Applications is a three-year under-graduate degree course awarded in India in the field of Computer Applications. Some students use online or distance education programs to earn this degree.		The course aims at realising the following student objectives:		After completion of this course, students may move on to higher studies, earning degrees such as:		Other students move directly to industry, working as programmers, networking professionals, graphics designers, and related positions.		Some of the institutes also provide the graduate diploma courses. A graduate diploma is basically the same thing as a graduate certificate. This terminology is more common in England, Australia, Canada, Scotland, Wales, etc., whereas "certificate" is more common in the US.		In Iran, provide four years of education leading to a B.Sc.		In Indonesia, most of the current bachelor's degrees are domain-specific degrees. Therefore, there are probably more than 20 bachelor's degrees. For instance, S.Psi for Sarjana Psikologi (literally translated as "Bachelor of Psychology/B.Psy., B.A."), S.T. for Sarjana Teknik (literally translated as "Bachelor of Engineering"), S.Si. for Sarjana Sains (literally translated as "Bachelor of Science"), S.Farm for Sarjana Farmasi (literally translated as "Bachelor of Pharmacy"), S.E for Sarjana Ekonomi (literally translated as "Bachelor of Economy"), S.Kom. for Sarjana Ilmu Komputer (literally translated as "Bachelor of Computer Science"), or S.Sos. for Sarjana Ilmu Sosial (literally translated as "Bachelor of Social Sciences"). In the past, the Indonesian academic system adopted the old European/western degrees, such as the Ir (inginieur) for an engineering degree and doctor's degree (doktorandus) for a degree in either social or natural sciences.		Since the undergraduate education system in Jordan is modeled after its American counterpart, all the degrees are adapted from those of the United States excepting the release of the degree certificate. Once a student has fulfilled his/her course requirements, a graduate certificate will be given. In order to get the degree, a student must finish and pass the dissertation stage; only then will he or she be awarded a degree credentialed by the Ministry of Higher Education of the Hashemite Kingdom of Jordan. Four years of education is the standard length.		In Nepal, the bachelor's degree was initially a three-year program for courses like Bachelor of Business Studies (B.B.S.), Bachelor of Sciences (B.Sc)., Bachelor of Education (B.Ed.), Bachelor of Arts (B.A.) from Tribhuvan University, Pokhara University, Purbanchal University and other new regional university equivalent but now it is mostly a four-year program for new courses like Bachelor of Business Administration (B.B.A.), Bachelor of Business Information System (B.B.I.S.), Bachelor of Information Management (B.I.M.), Bachelor of Engineering (B.E.), Bachelor of Science in Computer Studies and Information Technology (B.Sc).C.S.I.T. Some bachelor's programs are still three years long, such as the Bachelor of Arts (B.A) and Bachelor of Education (B.Ed). It is completed after 10+2 level (High School). Bachelor of Business Administration (B.B.A), Bachelor of Information Management (B.I.M.), Bachelor of Business Information Systems (B.B.I.S.), Bachelor of Engineering, and Bachelor of Science in Computer Science and Information Technology (B.Sc.C.S.I.T.) are a few popular bachelor's degree programs. B.Sc. and B.B.Sc. have recently turned into four year programs from three year programs. In Nepal, Tribhuvan University as an oldest and biggest University based on number of student and academic department, Kathmandu University, Purbanchal University, Pokhara University, Nepal Sanskrit University and other new regional universities are operating currently. M.B.A. and B.B.A. from all universities are examined under the system of Percentage and G.P.A, and traditional university courses are accessed on division base like pass division, second division, first division and distinction. In Nepal, there is no top up, honours and exchange or related tie up degree courses authorised and practiced by Nepalese Government and other educational Institution but these day, Affiliation from foreign universities, online and distance mode is popular in modern working youth population. M.B.A., B.B.A., B. Pharm., B. Sc. Nursing, Bachelor of Nursing (B.N.), B. E. has a trending professional demand in Nepalese market.		Institutes of higher learning in Malaysia provide three or four years of education leading to a B.Sc. Hons Degree. The standards of categorization is almost consistent among Malaysian Universities. Candidates who excel in their academic results will be awarded a First Class Bachelor Hons Degree (usually 3.67 CGPA and above), followed by Class Second Upper (usually between 3.00-3.66 CGPA), Class Second Lower (usually 2.50-2.99 CGPA), Class Three (usually 2.00-2.49 CGPA) and General Degree (Without Honours), for usually 1.99 and below CGPA candidates.		In Pakistan, arts, commerce and science colleges provide four-year bachelor's degrees (BA, BSc, BBA, BCom etc.). Generally these programs are of four years duration as elsewhere in the world and begin after completing higher secondary school education by receiving a Higher Secondary School Certificate (HSSC) acknowledging one's twelve years of study by the respective board. After successful completion of these programs, a bachelor's degree is awarded by the respective university. Some colleges are affiliated with a university (mostly a public sector university) and teach a part-time degree equals to fourteen years of education such as a two-year BA, BCom etc. A student may enroll in a two years BA, BCom as well as a four-year BA as an external candidate (external candidates are enrolled for examination and study program on self basis[clarification needed] or through private tuition providers). Main universities offering these two programs are University of the Punjab and University of Karachi where more than 50,000 students appear in BA and BCom exam as external candidates.		Engineering and medical colleges provide four and five-year degree programs respectively for bachelor's degrees (BE/BS/BSc Eng and MBBS) that also begin after higher secondary school year 12. The Bachelor of Architecture (BArch) degree program is of five years' duration.		In the Philippines, where the term "course" is commonly used to refer to a bachelor's degree major, course of study or program, several undergraduate categories exist—the two most common degrees awarded being Bachelor of Science (B.Sc.) and Bachelor of Arts (B.A. or A.B.). Specializations ("majors") in economics, business administration, social work, agriculture, nursing, accountancy, architecture and engineering are offered as B.S. degrees in most colleges and universities. The latter three specializations require five years of schooling, in contrast to the standard of four years. Other common degrees are Bachelor in Education (B.Ed.) and Bachelor of Laws (LL.B., a professional degree). Being patterned after the United States, all universities and colleges offer graduation with honors—cum laude, magna cum laude, and summa cum laude.		Universities, colleges, and institutions of higher learning provide the bachelor's degree, called 'haksa' (Korean: 학사). For example, a university student who majored in literature and graduates obtains a B.A., called 'munhaksa' (Korean: 문학사). Even if he or she does not go to an institution of higher learning, a person can get a bachelor's degree through the Bachelor's Degree Examination for Self-Education.		Recognised institutes of higher learning only are authorised to award degrees in Sri Lanka. Three years full-time bachelor's degree without an area of specialization is known as a general degree. A degree with a specialization (in accounting, chemistry, plant biotechnology, zoology, physics, engineering, IT, law, etc.) is known as a special degree and requires four years of study and more entrance qualifications. A degree in medicine, an M.B.B.Sc., requires a minimum of six years.		In Australia, a "bachelor degree"[11] is normally a three to four-year program, leading to a qualification at level 7 of the Australian Qualifications Framework.[12] Entry to a number of professions, such as law practice and teaching, require a bachelor's degree (a 'professional' degree). Other degrees, such as Bachelor of Arts don't necessarily elicit entry into a profession, though many organisations require a bachelor's degree for employment.		A one-year postgraduate bachelor honours degree at can be achieved as a consecutive stand-alone course following a bachelor's degree in the same field, or as an additional year as part of a bachelor's degree program.[12] The honours course is normally only open to those who do well in their bachelor's degree program and involves study at a more advanced level than that bachelor's degree.[13] Both the bachelor and bachelor honours degrees are aligned with level 6 of the EQF, the same as British and Irish bachelor's degrees with and without honours, and other Bologna Process first cycle qualifications.[14]		Some bachelor's degrees (e.g. engineering and environmental science) include an integrated honours degree as part of a four-year program. Honours is generally for students who want to take up a research track for postgraduate studies, and increasingly for those who want an extra edge in the job market. Marking scales for Honours differ; generally, First Class Honours (85–100%) denotes an excellent standard of achievement; Second Class Division 1 (75-84%) a high standard; Second Class Division 2 (65–74%) a good standard; Third Class (50–64%) satisfactory standard; a final mark below 50% is a fail of the course.		Bachelor honours degrees include a major Independent research component, allowing students to develop skills that will enable them to proceed to further study or to work in research roles in industry.[15] First-class or second-class (upper division) honours are generally required for entry into doctoral programs (e.g. PhDs, etc.); an alternative route to doctoral study is via a "masters degree".[16][17]		In New Zealand, only recognised institutions—usually universities and polytechnics—have degree-awarding powers.		Most bachelor's degrees are three years full-time, but certain degrees, such as the Bachelor of Laws and the Bachelor of Engineering degrees, require four years of study. A Bachelor of Medicine degree requires a minimum of six years.		Where students opt to study two bachelor's degrees simultaneously—referred to as a "conjoint degree" or "double degree"—an extra year of study is added. The number of years of study required is determined based on the degree with the greatest number of years. For example, a B.Com. degree requires three years of full-time study, but a double B.Com.–LL.B. degree will require five years of full-time study because the LL.B. degree is four years long. Exceptional students may choose to complete a degree in a shorter amount of time by taking on extra courses, usually with the help of summer school. Students who complete a double degree program will have two separate bachelor's degrees at the end of their studies.		Consistently high-performing students may also be invited to complete the 'honours' program. This usually requires an extra year of study with an extra honours dissertation. An honors award is credited with "Hons." (e.g., Bachelor of Laws (Hons.)). Some degrees also offer a Post Graduate Diploma, which often consists of the same workload, but with added flexibility. PGDip does not usually require a dissertation. However, the student may complete one if desired. A diploma award is credited with 'PGDip' and the name of the degree (for example, 'PGDipArts' or 'PGDipScience'.		Usually the region presents bachelor's, Master's, doctoral, and postdoctoral degrees.		Education in Canada is governed independently by each province and territory, however a common framework for degrees was agreed by the. Council of Ministers of Education, Canada in 2007. This adopted descriptors for bachelor's, master's and doctoral degrees that were deliberately similar to those defined by the Bologna Process.[18]		Under the framework, four general forms of bachelor's degree are defined: general programs providing a broad education and preparing graduates for graduate-entry professional programs or employment generally; in-depth academic programs in a specific subject that prepare graduates for postgraduate study in the field or employment generally; applied programs that concentrate on a mastery of practice rather than knowledge; and professional programs, often (but not exclusively) graduate-entry, that prepare graduates to practice as professionals in a specific field. It should be noted that this last category included graduate-entry degrees titled as if they were doctorates, such as MD, JD and DDS degrees – despite their names, these are considered bachelor's degrees.[18]		Bachelor's degrees may take either three or four years to complete and are awarded by colleges and universities. In many universities and colleges bachelor´s degrees are differentiated either as bachelor´s or as honours bachelor´s degrees. The term "Honours" is an academic distinction, which indicates that students must achieve their bachelor's degree with a sufficiently high overall grade point average; in addition, some programs may require more education than non-honours programs. The honours degrees are sometimes designated with the abbreviation in brackets of '(Hon(s))'.		In Quebec, students have to go through a minimum of two years of college before entering, for example, a three-year Bachelor of Science (B.Sc.) or a four-year Bachelor of Engineering (B.Eng.) program. As a consequence, there is no de jure "honors degree" (although some universities market some of their programs as being de facto honors degrees in their English-language materials[citation needed]), but there are some specializations called "concentrations" in French, which are mostly taken as optional courses.		In the province of Ontario, the vast majority of bachelor's degrees offered by Ontario universities are academic in nature. On the other hand, Ontario provincial legislation requires bachelor's degrees offered by Ontario colleges to be applied and vocationally-focused[19]		Bachelor's degrees in the United States are typically designed to be completed in four years of full-time study, although some programs (such as engineering or architecture)[20] usually take five, and some universities and colleges allow ambitious students (usually with the help of summer school, who are taking many classes each semester, and/or who have existing credit from high school Advanced Placement or International Baccalaureate course exams) to complete them in as little as three years. Some US colleges and universities have a separate academic track known as an "honors" or "scholars" program, generally offered to the top percentile of students (based on GPA), that offers more challenging courses or more individually directed seminars or research projects in lieu of the standard core curriculum. Those students are awarded the same bachelor's degree as students completing the standard curriculum but with the notation in cursu honorum on the transcript and the diploma. Usually, the above Latin honors are separate from the notation for this honors course, but a student in the honors course generally must maintain grades worthy of at least the cum laude notation anyway.[21] Hence, a graduate might receive a diploma Artium Baccalaureatum rite or Artium Baccalaureatum summa cum laude in the regular course or Artium Baccalaureatum summa cum laude in cursu honorum in the honors course.		If the student has completed the requirements for an honors degree only in a particular discipline (e.g., English language and literature), the degree is designated accordingly (e.g., B.A. with Honors in English). In this case, the degree candidate will complete the normal curriculum for all subjects except the selected discipline ("English," in the preceding example). The requirements in either case usually require completion of particular honors seminars, independent research at a level higher than usually required (often with greater personal supervision by faculty than usual), and a written honors thesis in the major subject.		Many universities and colleges in the United States award bachelor's degrees with Latin honors, usually (in ascending order) cum laude ("with honor/praise"), magna cum laude ("with great honor/praise"), summa cum laude ("with highest honor/praise"), and the occasionally seen maxima cum laude ("with maximal honor/praise"). Requirements for such notations of honors generally include minimum grade point averages (GPA), with the highest average required for the summa distinction (or maxima, when that distinction is present). In the case of some schools, such as Bates College, Colby College, Middlebury College, Guilford College, Franklin College Switzerland, and larger universities like the University of Virginia, Princeton University, North Carolina State University, University of Massachusetts Amherst, a senior thesis for degrees in the humanities or laboratory research for natural science (and sometimes social science) degrees is also required. Five notable exceptions are Reed College, Massachusetts Institute of Technology, The Evergreen State College, Sarah Lawrence College, and Bennington College, which do not have deans' lists, Latin honors recognitions, or undergraduate honors programs or subjects.		Bachelor's degrees may take an average of five years (from four to five years) to complete depending on the course load and the program and they are awarded by colleges and universities. Medicine is from 6 to 7 years. Each college has its own curriculum and requirements with an emphasis of their choice, governed independently by each state of the republic. After finishing all the subjects the student require a final work, which means the completion of particular honors seminars, research and development or a written thesis in a particular field. Mexico's regulations established as an obligation in order to receive their license and title the fulfillment of a "Social Service" to the nation (usually for those who finished their studies in a public institution) as a remuneration to society in the form of social actions, the benefits, as students, were received during training. This requirement takes about six months to one year depending on the type of degree. Bachelor's degree should not be falsely related with its Spanish cognate "Bachiller", which designate a prerequisite for matriculate in a career or bachelor studies. The official name for a bachelor's degree in Mexico is "Licenciado" and such studies are referred as "Licenciatura".		Bachelor's degrees should not be confused with Engineering Degrees, where an Ingeniería is prefixed to the name and requieres additional courses for certification as an Engineer.		In Brazil, a bachelor's degree takes from three years to six years to complete depending on the course load and the program. A bachelor's degree is the title sought by Brazilians in order to be a professional in a certain area of human knowledge. Master's and doctoral degrees are additional degrees for those seeking an academic career or a specific understanding of a field.		Even without a formal adhesion to the Bologna system, a Brazilian "bachelor's" would correspond to a European "first cycle." A Brazilian "bachelor's" takes three to six years[22] for completion, as well as usually a written monograph or concluding project, in the same way that a European bachelor's can be finished in three to four years, after which time Europeans may embark on a one- to two-year 2nd cycle program usually called a "Master's", according to the Bologna Process.		Depending on programs and personal choices, Europeans can achieve a master's degree in as little as four years (a three-year bachelor's and a one-year Master's) and as long as six years (a four-year bachelor's, a two-year Master's) of higher education. In Brazil it would be possible to have a specialization "lato-sensu" degree—which differs from a Brazilian "stricto-sensu" master's degree—in as little as three years (two years for a "tecnólogo"[23] degree and an additional year for a specialization) or as long as eight years (six years for professional degrees, plus two years for a master's "stricto-sensu" degree—typical in medicine or engineering).		In Colombia, secondary school has two milestones, in 9th and 11th grades. After completing the first 4 years of secondary school (6th, 7th, 8th and 9th grades), a student is considered to have completed the basic secondary school while after completed the last two years (10th and 11th grades) is considered to have completed "bachillerato" or high school diploma.		This degree can be only academic (the most common) or:		After graduating from high-school, hopeful students must present a nationwide exam that determines their eligibility to apply for their desired program, depending on the score the student achieves on the exam. In Colombia, the system of academic degrees is similar to the US model. After completing their "bachillerato" (high school), students can take one of three options. The first one is called a "Profesional" (professional career), which is similar to a bachelor's degree requiring from four to six years of study according to the chosen program, However, strictly-career-related subjects are taken from the very beginning unlike US where focused career-related subjects usually are part of the curriculum from the third year. The other option is called a "Técnico" (technician); this degree consists of only two and a half years of study and prepares the student for technical or mechanical labors. Finally, the third option is called a "Tecnólogo" (equivalent to an associate degree), and consist of 3 years of study. A technical school gives to the student, after a program of two years, an under graduate degree in areas like software development, networks and IT, accountancy, nursing and other areas of health services, mechanics, electricity and technic-like areas.		Universities offer graduate degrees in ICFES endorsed programs like medicine, engineering, laws, accountancy, business management and other professional areas. A typical undergraduate program usually takes 10 or 11 semesters and some (i.e. medicine) require an additional period of service or practice to apply for the degree. A student who has obtained an undergraduate degree can opt to continue studying a career after completing their undergraduate degree by continuing onto Master's and Doctorate degrees. They can also choose to a specialization in certain fields of study by doing an extra year.		ICFES is the national authority for the education quality. A complete list of under graduate and graduate programs approved by ICFES can be found here: http://snies.mineducacion.gov.co/consultasnies/programa/buscar.jsp?control=0.09832581685767972		In Guyana, the universities offer Bachelor programs in different streams like Bachelor of Atrs (B.A), Bachelor of Science in Nursing, Design and Arts, Liberal Arts, Psychology, Doctor of Medicine (MD) and other health science programs. These programs are delivered by University of Guyana, Texila American University, Green Heart Medical University, Lesley university and many more offers these bachelor programs.		In these countries, there are two titles that should not be confused:		Bachelor's degrees exist in almost every country in Europe. However, these degrees were only recently introduced in some Continental European countries, where bachelor's degrees were unknown before the Bologna process. Undergraduate programs in Europe overall lead to the following most widely accepted degrees:		The rest of the programmes typically lead to Bachelor of Engineering degree (B.Eng.), Bachelor of Business Administration degree (B.B.A.), or other variants. Also, associate degrees are rising in popularity on the undergraduate level in Europe.		On a per-country, per-discipline and sometimes even per-institute basis, the duration of an undergraduate degree program is typically three or four years, but can range anywhere from three to six years. This is an important factor in the student's decision-making process.		The historical situation in Austria was very similar to that in Germany, with the traditional first degrees being the Magister and the Diplom, which are master's-level qualifications. From 2004, bachelor's degrees have been reintroduced as part of the Bologna Process reforms. These can be studied at universities, leading to a bachelor's degree (BA or BSc) after three or four years, and at Fachhochschulen (universities of applied science), leading to a Bachelor (FH) after three years.[24]		Education in Belgium is run by the language communities, with separate higher education systems being administered by the Flemish Community and the French Community. Both systems have been reformed to align with the Bologna Process, the Flemish Community from 2003 and the French Community from 2004. In the Flemish Community, bachelor's degrees may be either academic or professional. These degrees last three years, and may be followed in both cases by a more advanced Bachelor-na-bachelor diploma, lasting one year (c.f. the Australian bachelor honours degree). All of these qualifications are at level 6 on the EQF, to which the Flemish Qualification Framework was referenced in June 2011. In the French Community, universities award grade de bachelier (3 years) as the equivalent of bachelor's degrees. Outside of universities, professional programs may be type long (long type) or type court (short type), both of which are offered at Hautes Ecoles and Ecoles Supérieures des Arts. The type long takes in a grade de bachelier (type long) (3 years), which is followed by a master degree (1 or 2 years), while the type court has a grade de bachelier professionnalisant (type court) (3 years), which may be followed by a bachelier de spécialisation (1 year). All bachelier degrees (including the bachelier de spécialisation) are equivalent to level 6 of the EQF, but have not been formally referenced.[25]		Most universities and colleges in Croatia today offer a three-year bachelor program, which can be followed up typically with a two-year master's (graduate) program.		Academies that specialize in the arts, e. g. the Academy of Fine Arts in Zagreb, have four-year bachelor's programs followed by a one-year master's.		Historically, the baccalareus was the undergraduate degree awarded to students who graduated from the course of trivium (grammar, dialectic and rhetoric) at a faculty of liberal arts (either at the Charles University or at the University of Olomouc). It was a necessary prerequisite to continue either with the faculty of liberal arts (quadrivium leading to a master's degree and further to a doctoral degree) or to study at one of the other three historical faculties—law, medicine or theology.		A bachelor's degree, abbreviated Bc.A., in the field of fine arts, and Bc. (Bakalář in Czech) in other fields is awarded for accredited undergraduate programs at universities and colleges.		The vast majority of undergraduate programmes offered in the Czech Republic have a standard duration of three years.		In the Czech tertiary education system, most universities and colleges today offer a three-year bachelor program, which can be followed up typically with a two-year master's (graduate) program. Some specializations, such as doctors of medicine and veterinary doctors, hold exceptions from the general system in that the only option is a six-year master's program with no bachelor stage (graduate with title doctor). This is due mainly to the difficulty of meaningfully splitting up the education for these specialisations.		The bachelor's degree was re-introduced at universities in Denmark in 1993, after the original degree (baccalaureus) was abandoned in 1775. The bachelor's degree is awarded after three or four years of study at a university and follows a scheme quite similar to the British one. Two bachelor's degrees are given at the university level today:		However, both in the business and the academic world in Denmark, the bachelor's degree is still considered to be "the first half" of a master's (candidatus). It is often not considered a degree in its own right .[citation needed], despite the politicians' best attempts to make it more accepted.		The bachelor's degree has also been used since the late 1990s in a number of areas like nursing and teaching. Usually referred to as a "Professional Bachelor" (Danish: professionsbachelor), these degrees usually require 3 to 4½ years of combined theoretical and practical study at a so-called "(professional) university college" (Danish: professionshøjskole). These professional bachelor's degrees do grant access to some university Master's program. These professional bachelor's degrees are considered to be a full education.		Bachelor's degrees in the Faroe Islands are much the same as in Denmark.		The traditional bachelor's degree is the equivalent of the French Maîtrise four-year degree. Since the new European system of 2004 LMD Bologna process was founded, it has become standard to recognize a bachelor's degree over three years with a licence degree, a master's degree over five years, and a doctorate over eight years.		Some private institutions are however literally naming their degrees Bachelor's, Master's and Executive, such as the Bordeaux MBA/Collège International de Bordeaux. Not all of them are yet accredited by the French State, but offer similar course subjects, structures and methods to those found in Anglo-Saxon institutions.		Historically, Bachelor's degrees, called "Bakkalaureus", originally existed in Germany since the late Middle Ages. They were abolished up until 1820 as part of educational reforms at this time. The Abitur degree—the final degree received in school after a specialized 'college phase' of two years—replaced it, and universities only awarded graduate degrees.		The Magister degree, a graduate degree, was awarded after five years of study. In 1899, a second graduate degree, the Diplom, was introduced when the Technische Hochschulen (TH) received university status. Since the introduction of the universities of applied sciences, a shortened version of the latter, referred to as Diplom (FH) and designed to take three to four years, was introduced between 1969 and 1972.		However, to comply with the European Bologna process, in 1998 a new educational law reintroduced the bachelor's degree (first degree after three years of study) in Germany. Today, these degrees can be called either "Bakkalaureus" or "Bachelor" (in accordance with federal law), but the English term is more common. According to the Bologna modell, the Bachelor is followed by the post-graduate master's degree of another two years. The traditional degrees of Diplom and Magister were mostly abolished in 2010, the Diplom still remains in a few subjects and universities and has been reintroduced as alternative degree in some places.		The traditional degrees have been re-mapped to the new European Credit Transfer and Accumulation System (ECTS) point system to make them comparable to the new bachelor's degree. Traditional and Bologna process degrees are ranked as follows in Germany:		The old four-, five-, or six-year laurea system was discontinued in the early 2000s as per the Bologna process, with some exceptions such as law school or medical school. The bachelor's degree, called "Laurea", takes three years to complete (note that Italian students graduate from high school at age 19) and grants access to graduate degrees (known as "Laurea Magistrale"). In order to graduate, students must earn 180 credits (ECTS) and write a thesis for which students have to elaborate on an argument under the supervision of a professor (generally from three to eight ECTS). Graduation marks go from 66 to 110. According to each faculty internal ruling, a lode may be awarded to candidates with a 110/110 mark for recognition of the excellence of the final project.		In 2003, the German-style education system was changed to conform to the ECTS because of the Bologna process. The existing academic degree granted with a diploma was transformed into a baccalaureus (bachelor's degree). The universities usually award a bachelor's degree after three years (following which, a master's degree will be two years long) or four years (following which, a master's degree will be one year long).		In the Netherlands, the Bachelor of Arts and Master of Arts degrees were introduced in 2002. Until that time, a single program that led to the doctorandus degree was in effect, which comprised the same course load as the bachelor's and Master's programs put together. (The doctorandus title was in use for almost all fields of study; other titles were used for legal studies (meester) and engineering (ingenieur).) Those who had already started the doctorandus program could, upon completing it, opt for the doctorandus degree (before their name, abbreviated to 'drs.'), or simply use the master's degree (behind their name) in accordance with the new standard. Since these graduates do not have a separate bachelor's degree (which is in fact—in retrospect—incorporated into the program), the master's degree is their first academic degree.		In 2003/2004, the Dutch degree system was changed because of the Bologna process. Former degrees included:		ingenieur		While the titles ing., bc., ir., mr., drs. and dr. are used before one's own name, the degrees B, M or D are mentioned after one's name. It is still allowed to use the traditional titles.		Whether a bachelor's degree is granted by a hogeschool or university is highly relevant since these parallel systems of higher education have traditionally served somewhat different purposes, with the vocational colleges mainly concentrating on skills and practical training. A B.A. or B.Sc. from a university grants 'immediate' entry into a master's program. Moreover, this is usually considered a formality to allow students to switch to foreign universities master's programs. Meanwhile, those having completed a HBO from a vocational college, which represented the highest possible level of vocational education available, can only continue to a "master's" on completion of a challenging year of additional study, which in itself can serve as a type of selection process, with the prospective M.Sc. students being required to cover a great deal of ground in a single year.		Recently, HBO (vocational) master's degrees have been introduced in the Netherlands. Graduates thereof may use neither the extension "of Arts" (M.A.) nor "of Science" (M.Sc.). They may use an M followed by the field of specialization (e.g., M.Des).		This year of study to "convert" from the vocational to academic (WO-wetenschappelijk onderwijs, literally "scientific education") is also known as a "bridge" or "premasters" year. Note that despite the use of the terminology "university of applied science" the higher vocational colleges are not considered to be "universities" within the Netherlands.		Important aspects of Dutch bachelor's degree courses (and others) relative to some of those offered abroad include:		In February, 2011, the Dutch State Secretary of Education decided to adhere to the recommendations written in a report by the Veerman Commission. In the near future, the distinction between academic and higher vocational degrees will disappear.		In Poland, the licentiate degree corresponds to the bachelor's degree in Anglophone countries. In Polish, it is called licencjat. To obtain the licencjat degree, one must complete three years of study. There is also a similar degree called engineer (Inżynier) which differs from the licencjat in that it is awarded by technical universities and the program usually lasts for 3.5 years. After that, the student can continue education for 2 or 1.5 years, respectively, to obtain the Polish magisterium degree, which corresponds to a master's degree.		Presently, the Portuguese equivalent of a bachelor's degree is the licenciatura, awarded after three years of study (four in some few cases) at an accredited university or polytechnical institution. It is an undergraduate first study cycle program which is required to advance into further studies such as master's degree programs.		Before the Bologna process (2006/2007), the bacharelato (bachelor's degree) existed in the Portuguese higher education system. It required three years of study, being roughly equivalent to the present licenciatura. At that time, the licenciatura referred to a licentiate's degree (equivalent to the present master's degree), which required usually five years of study. A licenciatura could also be obtained by performing two years of study after obtaining a bacharelato.		Today, the former and current licenciatura degrees are referred in Portugal, respectively, as pre-Bologna and post-Bologna licenciaturas.		The specialist's degree (Russian: специалист), (Ukrainian: спецiалiст) was the first academic distinction in the Soviet Union, awarded to students upon completion of five-year studies at the university level. The degree can be compared both to the bachelor's and master's degree. In the early 1990s, Bakalavr (Бакалавр, "bachelor") degrees were introduced in all the countries of the Commonwealth of Independent States except Turkmenistan. After the bakalavr degree (usually four years), one can earn a master's degree (another one or two years) while preserving the old five-year specialist scheme.		In Spain, due to the ongoing transition to a model compliant with the Bologna agreement, exact equivalents to the typical Anglo-Saxon bachelor's degree and master's degree are being implemented progressively. Currently, there is an undergraduate bachelor's degree called "Título de Grado" or simply "Grado" (its duration generally being four years), a postgraduate master's degree called "Título de Máster" or "Máster" (between one and two years), and a doctor's degree called "Título de Doctor" or "Doctorado". The "Título de Grado" is now the prerequisite to access to a Master study. The "Título de Máster" is now the prerequisite to access to doctoral studies, and its duration and the kind of institutions that can teach these programs are regulated in the framework of the European Higher Education Area.		Up until 2009/2010, the system was split into three categories of degrees. There were the so-called first-cycle degrees: "Diplomado" or "Ingeniero Técnico", with nominal durations varying between three and four years; there were also second-cycle degrees: "Licenciado" or "Ingeniero" with nominal durations varying between four and six years; and finally the third-cycle degrees: "Doctor." The official first-cycle degrees are comparable in terms of duration, scope, and educational outcomes to an Anglo-Saxon bachelor's degree. Meanwhile, the second-cycle degrees are comparable in terms of duration, scope, and educational outcomes to an Anglo-Saxon bachelor's + Master's degrees combination if compared with the Anglo-Saxon system. In this traditional system the access to doctoral studies was granted only to the holders of "Licenciado", "Ingeniero" or "Arquitecto" (second-cycle) degrees, and the "Master" or "Magister" titles were unregulated (so, there coexisted so-called "Master" programs with different durations, from some months to two years, backed by universities or centers without any official recognition) and only the reputation of the program/institution could back them.		The Swedish equivalent of a bachelor's degree is called kandidatexamen. It is earned after three years of studies, of which at least a year and a half in the major subject. A thesis of at least 15 ECTS credits must be included in the degree. Previously, there was a Bachelor of Law degree (juris kandidat) which required 4.5 years of study, but this degree now has a new name, juristexamen (and is now a master's degree called "Master of Laws").		Like Austria and Germany, Switzerland did not have a tradition of bachelor's and master's degrees. In 2003, after the application of the Bologna process, bachelor's and graduate master's degrees replaced the old degrees. As of 1 December 2005 the Rectors' Conference of the Swiss Universities granted holders of a lizentiat or diploma the right to use the corresponding master title. As of 2006[update], certificates of equivalence are issued by the university that issued the original degree. Currently three to four years of study are required to be awarded a bachelor's degree. A master's degree will require another two to three years of coursework and a thesis.		The bachelor's degree is the standard undergraduate degree in the United Kingdom, with the most common degrees being the bachelor of arts (BA) and bachelor of science (BSc). Most bachelor's degree courses (apart from the very rare postgraduate awards, and those in medicine, dentistry and veterinary science) lead to honours degrees, with ordinary degrees generally only being awarded to those who do not meet the required pass mark for an honours degree. With the exception of the postgraduate bachelor's degrees and bachelor's degrees in medicine, dentistry and veterinary science, UK bachelor's degrees (whether honours or non-honours) are first cycle (end of cycle) qualifications under the Bologna Process. Postgraduate bachelor's degrees and bachelor's degrees in medicine, dentistry and veterinary science are second cycle (end of cycle) qualifications. Some bachelor's degrees in medicine, dentistry and veterinary science offer intercalated degrees en route to the final qualification.[30][31][32]		Bachelor's degrees should not be confused with baccalaureate qualifications, which derive their name from the same root. In the UK, baccalaureate qualifications, e.g. International Baccalaureate, Welsh Baccalaureate, English Baccalaureate, are gained at secondary schools rather than being degree-level qualifications.		Until the 19th century, a bachelor's degree represented the first degree in a particular faculty, with Arts representing undergraduate study, thus the Bachelor of Civil Law (BCL) at Oxford and the Bachelor of Laws (LLB) at Cambridge, for example, were postgraduate degrees. Vestiges of this system still remain in the ancient universities, with Oxford and Cambridge awarding BAs for undergraduate degrees in both arts and sciences (although both award undergraduate BTh degrees through associated theological colleges, and Oxford awards BFA degrees in addition to the BA) and defining other bachelor's degrees (e.g. BPhil, BCL) as postgraduate awards equivalent to master's degrees,[33][34] although many postgraduate bachelor's degrees have now been replaced by equivalent master's degrees (e.g. LLM for the LLB at Cambridge and MSc for the BSc at Oxford).[35][36] The same historical usage of indicating an undergraduate degree by it being in the faculty of arts rather than being a bachelor's degree gives rise to the Oxbridge MA and the Scottish MA).		Common bachelor's degrees and abbreviations:		In England, Wales and Northern Ireland, bachelor's degrees normally take three years of study to complete, although courses may take four years where they include a year abroad or a placement year. Degrees may have titles related to their broad subject area or faculty, such as BA or BSc, or may be more subject specific, e.g. BEng or LLB. The majority of bachelor's degrees are now honours degrees, although this has not always been the case historically.		Although first degree courses are usually three years (360 credits), direct second year entry is sometimes possible for students transferring from other courses or who have completed foundation degrees, via accreditation of prior learning or more formal credit transfer arrangements. Some universities compress the three-year course into two years by teaching for a full calendar year (180 credits) rather than a standard academic year (120 credits), thus maintaining the full 360-credit size of the course.[37][38]		In addition to bachelor's degrees, some institutions offer integrated master's degrees as first degrees in some subjects (particularly in STEM fields). These integrate teaching at bachelor's and master's level on a four-year (five-year if with industrial experience) course, which often shares the first two years with the equivalent bachelor's course.		The normal academic standard for bachelor's degrees in England, Wales and Northern Ireland is the honours degree. These are normally classified in one of four classes of honours, depending upon the marks gained in examinations and other assessments: first class, upper second class (2:1), lower second class (2:2), or third class; some institutions have announced that they intend to replace this system of classifying honours degrees with an American-style Grade Point Average.[39] An ordinary (or unclassified) degree, which only requires passes worth 300 credits rather than the 360 of the honours degree, may be awarded if a student has completed the full honours degree course but has not obtained sufficient passes to earn a degree. Completion of just the first two years of the course can lead to a Diploma of Higher Education and completion of only the first year to a Certificate of Higher Education.		On the Framework for Higher Education Qualifications, standard undergraduate bachelor's degrees with and without honours are at level 6, although the courses include learning across levels 4 to 6. Honours degrees normally require 360 credits with a minimum of 90 at level 6, while ordinary degrees need 300 credits with s minimum of 60 at level 6. Bachelor's degrees in medicine, dentistry and veterinary science are at level 7, with learning spanning levels 4 to 7, and are not normally credit rated. The Diploma of Higher Education is a level 5 (second year of bachelor's degree) qualification and requires 240 credits, a minimum of 90 at level 5; The Certificate of Higher Education is a level 4 (first year of bachelor's degree) qualification and requires 120 credits, a minimum of 90 at level 4.[40]		Other qualifications at level 6 of the Framework for Higher Education Qualifications or the Regulated Qualifications Framework, such as graduate diplomas and certificates, some BTEC Advanced Professional awards, diplomas and certificates, and the graduateship of the City & Guilds of London Institute are at the same level as bachelor's degrees, although not necessarily representing the same credit volume.[41]		At Scottish universities, bachelor's degrees (and the equivalent Scottish MA awarded by some institutions) are normally honours degrees, taking four years of study (or five with a year abroad or in industry), but may also be ordinary degrees (also known as pass, general or designated degrees) requiring three year of study. Honours degrees may be awarded as BA (Hons) or MA (Hons) in the arts and social sciences, or BSc (Hons) for sciences, or have more specific titles such as BEng. As in the rest of the UK, integrated master's degrees, taking five years in Scotland, are also offered as first degrees alongside bachelor's degrees.[42]		An honours degree may be directly linked to professional or vocational qualifications, particularly in fields such as engineering, surveying and architecture. These courses tend to have highly specified curricula, leaving students without many options for broader study. Others, following a more traditional route, start off with a broad range of studies across the faculty that has admitted the student or, via modular study, across the whole university. Students on these courses specialise later in their degree programmes.[42] Typically degree grades are based only on the final two years of study, after a specialisation has been chosen, so broader study courses taken in the first two years do not affect the final degree grade.[43]		Honours degrees are subdivided into classes in the same way as the rest of the UK, depending on the overall grade achieved. These are, from highest to lowest; first class, upper second class (2:1), lower second class (2:2), and third class.		Ordinary degrees are awarded to students who have completed three years at university studying a variety of related subjects.[44] These may be taken over a broad range of subjects or (as with honours degrees) with a specialisation in a particular subject (in the latter case, they are sometimes known s designated degrees). As ordinary degrees in Scotland constitute a distinct course of study, rather than a grade below honours degrees, they can be graded (from lowest to highest) as "pass", "merit" or "distinction".[45][46] As in the rest of the UK, Certificates and Diplomas of Higher Education may be earned by those completing one and two years of a bachelor's degree course respectively.[42]		The first two years, sometimes three, of both an ordinary degree and an honours degree are identical, but candidates for the ordinary degree study in less depth in their final year and often over a wider variety of subjects, and do not usually complete a dissertation. A Scottish ordinary degree is thus different from ordinary degrees in the rest of the UK in comprising a distinct course of study from the honours degree. In keeping with the Scottish "broad education" philosophy, ordinary degrees (and more rarely honours ones) may mix different disciplines such as sciences and humanities taught in different faculties and in some cases even different universities.[47]		Bachelor's degrees with honours are at level 10 of the Scottish Credit and Qualifications Framework (SCQF) and require 480 credits with a minimum of 90 at level 10 and 90 at level 9. Ordinary degrees are at level 9 and require 360 credits with a minimum of 90 at level 9.[48] Both honours degrees and ordinary degrees qualify as first cycle (end of cycle) qualifications in the Bologna Process. Bachelor's degrees in medicine, dentistry and veterinary science are at level 11 of the SCQF and are second cycle (end of cycle) qualifications in the Bologna Process.[49]		Bachelor's degrees exist in almost every city in Turkey. Mostly preferred universities of Turkey are Middle East Technical University, Boğaziçi University, Yeditepe University, Sabanci University, Koc University, Hacettepe University, Ankara University, Istanbul Technical University, Istanbul University, Yildiz Technical University, Bilkent University, Koç University, by B.A. students. They all grants Bachelor of Arts or Bachelor of Science degrees upon completion of eight-semester programs offered by its faculties and the School of Foreign Languages. Also double-major is available in those universities. Some universities offer the opportunity for ordinary degree students to transfer to an honours degree course in the same subject if an acceptable standard is reached after the first or second year of study. It is called in Turkish "Önlisans Mezunu."		While some of the public and private universities are offering 30% English in their programs, there are also many universities which offer 100% English language in the programs such as Middle East Technical University, Sabanci University, Boğaziçi University, Yeditepe University.		Many other specialized bachelor's degrees are offered as well. Some are in very specialized areas, like the five-year B.I.Des. or B.Sc.I.Des. degree in industrial design.[50] Others are offered only at a limited number of universities, such as the Walsh School of Foreign Service at Georgetown University's Bachelor of Science in Foreign Service (B.Sc.F.S.). The University of Delaware offers a Bachelor of Applied Arts and Science (B.A.A.Sc.) degree, a degree which often indicates an interdisciplinary course of study for many majors within its School of Arts and Science.[51] Stanford University's Bachelor of Arts and Science degree is for students who are receiving one degree but who have completed two arts and sciences majors, one of which would ordinarily lead to the B.A. and one of which would ordinarily lead to the B.Sc.		At many institutions one can only complete a two-degree program if the bachelor's degrees to be earned are of different types (e.g., one could earn a B.A. in philosophy and a B.Sc.C.Eng. in chemical engineering simultaneously, but a person studying philosophy and English would receive only a single B.A. with the two majors). Rules on this vary considerably, however.		The Bachelor of Science in Agriculture [B.Sc. (Ag) or B.Sc. (Hons.) Agriculture] offers a broad training in the sciences. The focus of this four-year applied degree is on the development of analytical, quantitative, computing and communication skills. Students learn how to apply the knowledge and principles of science to the understanding and management of the production, processing and marketing of agricultural products, and to the management and conservation of our natural resources. All students undertake rural field trips and approved professional experience within agricultural or horticultural enterprises, natural resource management, agribusiness industries, or commercial or government organisations active in the field.		The Bachelor of Architecture (B.Arch.) degree is a professional degree awarded to students who complete the five-year course of study in the field at some universities. Many universities offer a B.Sc. or B.A. (majoring in Architecture) after the first three or four years, and then a post-graduate diploma, B.Arch. or M.Arch. for the following two to four years.		The Bachelor of Design (B.Des., or S.Des. in Indonesia) is awarded to those who complete the four- or four-and-a-half-year course of study in design, usually majoring in a specific field of design, such as interior design or graphic design.		The Bachelor of Arts degrees (B.A., A.B.; also known as Artium Baccalaureus) along with the Bachelor of Science degrees are the most common undergraduate degrees given. Originally, in the universities of Oxford, Cambridge and Dublin, all undergraduate degrees were in the faculty of arts, hence the name of the degree. The Bachelor of Applied Arts and Sciences (B.A.A.Sc.) is an undergraduate degree that bridges academic and work-life experiences.		There are various undergraduate degrees in information technology incorporating programming, database design, software engineering, networks and information systems. These programs prepare graduates for further postgraduate research degrees or for employment in a variety of roles in the information technology industry. The program focus may be on the technical or theoretical aspects of the subject matter, depending on which course is taken.		In countries following British tradition, (the University of Malta is an exception) medical students pursue an undergraduate medical education and receive bachelor's degrees in medicine and surgery (M.B.B.Chir., M.B.B.S., B.M.B.S., B.M., M.B.Ch.B., etc.). This was historically taken at the universities of Oxford, Cambridge, and Dublin after the initial B.A. degree, and in Oxford, Cambridge, and Dublin the B.A. is still awarded for the initial three years of medical study, with the B.M.B.Ch., M.B.B.Chir., or M.B.B.Ch.B.A.O., respectively, being awarded for the subsequent clinical stage of training. Some British universities give a bachelor's degree in science, or medical science, midway through the medical course, and most allow students to intercalate a year of more specialized study for an intercalated Bachelor of Science (B.Sc.), Bachelor of Medical Science (B.Med.Sc.), or Bachelor of Medical Biology (B.Med.Biol.) degree with honors. Although notionally M.B. and B.S. are two degrees, they must be taken together. In some Irish universities, a third degree, Bachelor of Arts in Obstetrics (B.A.O.), is added. However, this third degree is an anachronism from the 19th century and is not registerable with the Irish Medical Council. In the UK, these qualifications, while retaining the title of bachelor's degrees, are master's degree level qualifications.[53]		It should be noted that use of the courtesy title of doctor is attached to the profession of doctor or physician and is granted by registration, not by earning the qualifying degree. Trainee doctors in the UK are allowed to use the title once they begin their training and receive provisional registration.[54][55][56]		Th Canadian MD degree is, despite its name, classified as a bachelor's degree.[18]		Dentistry is offered both as an undergraduate and a postgraduate course. In countries following the British model, the first degree in dentistry is the Bachelor of Dental Surgery, which is a master's degree level qualification in the UK.[53] In some parts of the world, the doctorate of dental surgery (DDS) is the usual undergraduate program. Postgraduate courses such as the Bachelor of Dentistry (B.Dent.)—awarded exclusively by the University of Sydney in Australia—requires a previous bachelor's degree.		The Bachelor of Midwifery degree is a professional degree awarded to students who have complete a three- to five-year (depending on the country) course of study in midwifery. Common abbreviations include B.Sc.Mid, B.Mid, B.H.Sc.Mid.		Physiotherapy is offered both as an undergraduate and a graduate course of study. Studies leading to the Bachelor of Physiotherapy (B.P.T.) degree usually constitute the undergraduate program. In the graduate program, courses leading to a degree such as the Master of Physiotherapy degree are offered.		In the Canadian province of Quebec, French universities offer both undergraduate and graduate courses leading to the obtention of a Bachelor of Science degree with a major in physiotherapy and a Master of Science degree specialized in physiotherapy. McGill University, the Université de Montréal, and the Université de Sherbrooke are among the post-secondary institutions that offer such programs.		Optometry is a four-year or five-year course. Although students graduate with a B.Sc. after three years of study, passing a further supervised preregistration year is required to become a fully qualified optometrist. The National Institute of Ophthalmic Sciences is among the post-secondary institutions that offer such programs. It is the academic arm of The Tun Hussein Onn National Eye Hospital and the only eye hospital based institution in Malaysia.		The Bachelor of Nursing degree is a three- to five-year undergraduate degree that prepares students for a career in nursing. Often the degree is required to gain "registered nurse", or equivalent, status—subject to completion of exams in the area of residence. Sometimes, though, the degree is offered only to nurses who are already registered. Alternate titles include Bachelor of Science in Nursing and Bachelor of Nursing Science, with abbreviations B.Sc.N., B.N.Sc.		The Bachelor of Veterinary Science program is generally a five-year course of study that is required for becoming a veterinarian. It is also known as the Bachelor of Veterinary Medicine and Surgery at some universities (B.V.M.S.). In the UK, this is a master's degree level qualification that retains the title of bachelor's for historical reasons.[53] In the United States, no bachelor's degree of veterinary science is given, only the Doctor of Veterinary Medicine (D.V.M.) degree is.		The Bachelor of Pharmacy (B.Pharm.) degree is a common undergraduate degree for the practice of pharmacy. In the United States, Canada, and France, however, all colleges of pharmacy have now phased out the degree in favor of the Pharm.D., or doctor of pharmacy, degree or the Ph.D., doctor of philosophy, degree in pharmacy. Some universities, such as the University of Mississippi, award a Bachelor of Science in pharmaceutical sciences (B.Sc.P.Sc.) degree as a part of the seven-year Pharm.D. program after the completion of the first four years. However, the B.ScP.Sc. degree does not qualify the recipient for the practice of pharmacy, for which it is required that students earn a Pharm.D. degree.		Public health is usually studied at the master's degree level. The Bachelor of Science in Public Health (B.Sc.P.H.) degree is a four-year undergraduate degree that prepares students for careers in the public, private, or nonprofit sectors in areas such as public health, environmental health, health administration, epidemiology, or health policy and planning.		The Bachelor of Kinesiology degree (B.K. or B.Sc.K.)is a specialized degree in the field of human movement and kinetics. Some schools still offer it under the aegis of a School of Physical Education (B.P.Ed. or B.H.P.Ed.), although "kinesiology" or "human kinetics" is currently the more popularly accepted term for the discipline.		Bachelor of Science in Nutrition and Dietetics (B.S.N.D.), Bachelor of Food Science and Nutrition (B.F.S.N.) Specific areas of study include clinical nutrition, food technology, hospitality and services management, research, community worker, health care management, educator, sports science, agricultural sciences, private practice and other allied health fields. The degree is awarded following four to six years of collegiate study in America (average five years), from three to four in Europe and Australia. In America (especially Latin America) Nutrition per se is separated from Dietetics, where the latter is equivalent to a technical degree.		The Bachelor of Aviation (B.Av.) is awarded to students who complete a four-year course of study in the field of aviation.		The Bachelor of Divinity, Bachelor of Theology, Bachelor of Religious Studies, Bachelor of Biblical Studies, and Bachelor of Religious Education (B.D., B.Th., B.R.S., B.B.S., and B.R.E.) degrees are awarded on completion of a program of study of divinity or related disciplines, such as theology, religious studies, or religious education.		Traditionally the B.D. was in fact a graduate degree rather than a first degree, and typically emphasised academic theology, biblical languages etc. This has become a less common arrangement, but should be noted since, for example, a B.D. takes precedence over a Ph.D. in Cambridge University's order of seniority.		While the theological bachelor's degree is generally conferred upon completion of a four-year program, it is also conferred in some specialized three-year programs. From there, the next level of advancement is generally the Master of Divinity (M.Div.), Master of Theology (Th.M.), Master of Religious Studies, or Master of Religious Education (M.R.E.) degree. In the United States the "main line" Protestant clergy typically take a four-year bachelor's degree in whatever field they choose, then earn the M.Div. (Master of Divinity) degree in an additional three years as part of preparation for ordination.		The Bachelor of Fine Arts (B.F.A.) degree is a specialized degree awarded for courses of study in the fine and/or performing arts, frequently by an arts school or conservatory, although it is equally available at a significant number of traditional colleges and universities. In contrast to the B.A. or B.S., which are generally considered to be academic degrees, the B.F.A. is usually referred to as a professional degree, whose recipients have generally received four years of study and training in their major field as compared to the two years of study in the major field usually found in most traditional non-Commonwealth Bachelor of Arts or Bachelor of Science programs.		In the United States, the Bachelor of Fine Arts degree differs from a Bachelor of Arts degree in that the majority of the program consists of a practical studio component, as contrasted with lecture and discussion classes. A typical B.F.A. program in the United States consists of two-thirds study in the arts, with one-third in more general liberal arts studies. For a B.A. in Art, the ratio might be reversed.		The Bachelor of Film and Television (B.F.T.V.) degree is an undergraduate degree for the study of film and/or television production including areas of cinematography, directing, scriptwriting, sound, animation, and typography.		The Bachelor of Integrated Studies (B.I.S.) is an interdisciplinary bachelor's degree offered by several universities in the United States and Canada. It allows students to design a customized and specific course of study to best suit their educational and professional objectives. Generally, this degree is sponsored by two or more departments within the university. Schools which confer the B.I.S. degree include the University of Manitoba, Pittsburg State University, University of South Carolina Upstate, Weber State University, Ferris State University, Arizona State University, University of Minnesota, Miami University (Ohio), the University of Virginia, the University of New Brunswick, and Tallinn University of Technology amongst others.		The Bachelor of Journalism (B.A.J. or B.Sc.J.) degree is a professional degree awarded to students who have studied journalism at a four-year accredited university. Not all universities, however, grant this degree. In the United States, schools tend to offer the B.A. or B.S. with a major in journalism instead. The world's oldest school of journalism at the University of Missouri offers a B.J. degree, not to be confused with the bachelor's degree in jurisprudence at Oxford University. In South Africa, Rhodes University has the oldest school of journalism in Africa and allows students to take a fourth-year specialisation to raise their B.A. to B.A.J. status, equivalent to a B.A. (Hons).x		The Bachelor of Landscape Architecture (B.L.Arch.) degree is awarded to students who complete the five- (in some countries four-) year course of study in the field.		The Bachelor of Liberal Arts, Bachelor of General Studies, Bachelor of Liberal Studies, Bachelor of Science in general studies, or Bachelor of Applied Studies (B.L.A., B.G.S., B.L.S., B.Sc.G.S., B.A.S.) degree is sometimes awarded to students who major in the liberal arts, in general, or in interdisciplinary studies. The Bachelor of Professional Studies is awarded to students who major in professional career studies.		The Bachelor of Library Science or Bachelor of Library and Information Science (B.L.Sc., B.L.I.Sc.) degree is sometimes awarded to students who major in library science, although Master's of library science degrees are more common.		The Bachelor of Music (B.Mus.) degree is a professional or academic undergraduate degree in music at most conservatories in the US and the UK. It is also commonly awarded at schools of music in large private or public universities. Areas of study typically include music performance, music education, music therapy, music composition, academic fields (music history/musicology, music theory, ethnomusicology), and may include jazz, commercial music, recording technology, sacred music/music ministry, or music business. Small liberal arts colleges and universities without schools of music often award only B.A. in music, with different sets of requirements. (see also: BFA)		The Bachelor of Mortuary Science (B.M.S.) is a professional undergraduate degree, awarded by the Cincinnati College of Mortuary Science of Cincinnati, Ohio and Southern Illinois University Carbondale. It was introduced in 1986 and it is awarded to students that complete 120 semester hours of course work and receive passing scores on the National Board Exam administered by The International Conference of Funeral Service Examining Boards.[57]		The Bachelor of Philosophy (B.Phil. or Ph.B.) degree is either an undergraduate or graduate degree. Generally, it entails independent research or a thesis/capstone project.		The Bachelor of Arts or Science in Psychology (B.A.Psy., B.Sc.Psy., B.Psych., or Psy.B.) degree is a degree awarded to students who have completed a course of study in the field of psychology. Courses typically last five years, but may last as long as six. In Nepal, there are three- and four-year courses available for higher-level students. See Psychologist#Licensing and regulation, Training and licensing of clinical psychologists.		The Bachelor of Education degree (B.Ed.) is a four-year undergraduate professional degree offered by many American colleges and universities for those preparing to be licensed as teachers. Variants include the B.Ed., B.A.Ed, B.A.T. (Bachelor of Arts for Teaching), and B.S.T. degrees. Preparation for the M.S. in education, this degree is most often received by those interested in early childhood, elementary level, and special education, or by those planning to be school administrators. Secondary level teachers often major in their subject area instead (i.e., history, chemistry, or mathematics), with a minor in education. Some states require elementary teachers to choose a subject major as well, and minor in education.		In Canada, the bachelor of education is a two-year professional degree in which students will specialise in either elementary or secondary education, and that is taken after the completion of a three or four year bachelor's degree with a major in a teachable subject, such as English, French, Mathematics, Biology, Chemistry, or a social science. Some universities also offer concurrent, five year programs with student completing both a bachelor's degree in arts or science as well as their B.Ed. The possession of a B.Ed. and a second bachelor's degree is required to teach in most public anglophone and francophone schools in Canada. The B.Ed. prepares teachers for completion of either M.A. (master's of arts) programs in education, M.Ed. (masters of education) programs, or post graduate certificates in education.		The Bachelor of Science and/with Education degree (B.Sc.Ed.) is a degree awarded to students who complete the four- to five-year course of study in the field of science (major and minor in General Biology, Chemistry, Physics, and Mathematics) and education. Although notionally B.Sc. and B.Ed. are two degrees, they must be taken together. The graduates will work as science (physics, chemistry, biology) teachers in high schools, as lecturers in pre university colleges and matriculation centers and can progress to postgraduate programs (M.Sc. and Ph.D.) in various areas in science or education.		The Bachelor of Science in Forestry (B.Sc.F.) is a degree awarded to students who complete the four-year course of study in the field of forestry.		The Bachelor of Science degrees (B.Sc., Sc.B.) along with the Bachelor of Arts degrees are the most common undergraduate degrees given. The Bachelor of Applied Arts and Sciences (B.A.A.Sc.) is an undergraduate degree that bridges academic and work-life experiences.		The Bachelor of Science in Law degree (B.Sc.L.) is a special-purpose degree that allows someone who has had some prior studies but has not achieved a bachelor's degree to resume his or her education and take up the study of law with the goal of eventually receiving the juris doctor degree.		The Bachelor of Social Science (B.S.Sc.) is a three- or four-year undergraduate British degree that enables students to specialize in the area of social science. Compared to the Bachelor of Arts, which allows students to study a vast range of disciplines, the Bachelor of Social Science enables students to develop more central and specialized knowledge of the social sciences. Many universities place the Bachelor of Social Science between the Bachelor of Arts and Bachelor of Science undergraduate degrees.		The Bachelor of Social Work (B.S.W.) degree is a four-year undergraduate degree. Usually the first two years consist of liberal arts courses and the last two years focus on social work classes in human development, policy/law, research, and practice. Programs accredited by the Council on Social Work Education require B.S.W. students to complete a minimum of 400 field education or internship hours. Accredited B.S.W. programs often allow students who are interested in obtaining a Master of Social Work degree to complete the degree in a shorter amount of time or waive courses. In Latin America this is a four to five year degree that can replace liberal arts subjects into health sciences, resulting in social work as a type of community psychology and socioeconomic studies, focused in hospitals, prisons or pedagogy, among others.		The Bachelor of Technology degree (B.Tech) is a three- or four-year undergraduate degree. Generally, the program is comparable to a Bachelor of Science degree program, which is additionally supplemented by either occupational placements (supervised practical or internships) or practice-based classroom courses.		The Bachelor of Laws (LL.B.) is the principal academic degree in law in most common law countries other than the United States, and anglophone Canada, where it has been superseded by the juris doctor (J.D.) degree.		The Bachelor of Talmudic Law degree (B.T.L.) or a First Talmudic Degree (F.T.D.) is the degree awarded in most Yeshivas around the United States.		The Bachelor of Tourism Studies (B.T.S.) degree is awarded to those who complete the four- or five-year course of study in tourism, laws regarding tourism, planning and development, marketing, economics, sociology, anthropology, arts and world history (dependent on the country in which one takes the course), ticketing, hospitality, computer applications, and much more. The course would have an interdisciplinary approach with a vast range of units so the tourismologist professional would be able to identify necessary actions toward a sustainable touristic environment focus on local community uniqueness, values and traditions. As tourism is a growing industry, in India there is a lot of opportunity for those who complete this course of study. It is available in select universities of India.		The Bachelor of Mathematics or Bachelor of Mathematical Sciences degree (B.Math. and B.Math.Sc.) is given at the conclusion of a four-year honors program or a three-year general program. Several universities, mostly in Canada and Australia, award such degrees. The usual degree for mathematics in all other countries is the B.Sc.		The Bachelor of Urban and Regional Planning or Bachelor of Urban Planning or just Bachelor of Planning degree (B.U.R.P., B.U.P., or B.Plan) is a degree offered at some institutions as a four or five-year[58] professional undergraduate degree in urban planning. Programs vary in their focus on studio work and may or may not involve professional practice.		The Bachelor of Public Affairs and Policy Management degree (B.P.A.P.Mgt.) is a specialized four-year honors degree dedicated to the study of public policy within an interdisciplinary framework. The degree was created as a direct response to the changing nature of civic society and the growing need for university graduates who can work effectively in the new policy environment.		The Bachelor of Innovation is a four-year degree in a range of different fields.[59][60] The major fields, in engineering business, arts, science or education, are similar to their associated B.A. or B.Sc. degrees. The general education elements are restructured to provide a common core of innovation, entrepreneurship and team skills.[61] The degree was created as a direct response to the increasing pace of innovation in today's society and the need for graduates that understanding effective teaming, as well as the innovation process.		
An athlete (American and British English) or sportsman or sportswoman (British English) is a person who is good at a sport and competes in one or more sports that involve physical strength, speed or endurance. The term's application to those who participate in other activities, such as horse riding or driving, is somewhat controversial.		Athletes may be professionals or amateurs.[1] Most professional athletes have particularly well-developed physiques obtained by extensive physical training and strict exercise accompanied by a strict dietary regimen.						The word "athlete" is a romanization of the Greek: άθλητὴς, athlētēs, one who participates in a contest; from ἄθλος, áthlos, or ἄθλον, áthlon, a contest or feat. The primary definition of "sportsman" according to Webster's Third Unabridged Dictionary (1960) is, "a person who is active in sports: as (a): one who engages in the sports of the field and especially in hunting or fishing." Athletes involved in isotonic exercises have an increased mean left ventricular end-diastolic volume and are less likely to be depressed.[2][3] Due to their strenuous physical activities, athletes are far more likely than the general population to visit massage salons and pay for services from massotherapists and masseurs.[4] Athletes whose sport espouses endurance more than strength usually have a lower calorie intake than other athletes.[5]		An "all-around athlete" is a person who competes in multiple sports at a high level. Examples of people who played numerous sports professionally include Jim Thorpe, Lionel Conacher, Deion Sanders, Danny Ainge and Babe Zaharias. Others include Ricky Williams, Bo Jackson, and Damon Allen, each of whom was drafted both by Major League Baseball and by professional gridiron football leagues such as the NFL and the CFL. Japanese athletes such as Kazushi Sakuraba, Kazuyuki Fujita, Masakatsu Funaki and Naoya Ogawa have successfully competed in both professional wrestling and mixed martial arts.		The title of "World's Greatest Athlete" traditionally belongs to the world's top competitor in the decathlon (males) and heptathlon (females) in track and field. The decathlon consists of 10 events: 100 meters, long jump, shot put, high jump, 400 meters, 110 m hurdles, discus, pole vault, javelin, and 1500 m. The heptathlon consists of seven events: the 100 m hurdles, high jump, shot put, 200 meters, long jump, javelin, and 800 meters. These competitions require an athlete to possess the whole spectrum of athletic ability in order to be successful including speed, strength, coordination, jumping ability, and endurance.		Although the title "World's Greatest Athlete" seems a natural fit for these two events, its traditional association with the decathlon/heptathlon officially began with Jim Thorpe. During the 1912 Olympics in Stockholm, Sweden, Thorpe won the gold medal in the Decathlon (among others). Thorpe notably also competed professionally in soccer, baseball, American Football, and basketball; and competed collegiately in track and field, soccer, baseball, lacrosse, and did ballroom dancing. King Gustav V of Sweden, while awarding Thorpe the decathlon gold said: "You, sir, are the greatest athlete in the world." That title has become associated with the decathlon event ever since.		
Nepal (/nəˈpɔːl/ ( listen);[9] Nepali: नेपाल  Nepāl [neˈpal]), officially the Federal Democratic Republic of Nepal (Nepali: सङ्घीय लोकतान्त्रिक गणतन्त्र नेपाल Sanghiya Loktāntrik Ganatantra Nepāl),[10] is a landlocked central Himalayan country in South Asia. Nepal is divided into 7 states and 75 districts and 744 local units including 4 metropolises, 13 sub-metropolises, 246 municipal councils and 481 villages.[11] It has a population of 26.4 million and is the 93rd largest country by area.[2][12] Bordering China in the north and India in the south, east, and west, it is the largest sovereign Himalayan state. Nepal does not border Bangladesh, which is located within only 27 km (17 mi) of its southeastern tip. Neither does it border Bhutan due to the Indian state of Sikkim being located in between. Nepal has a diverse geography, including fertile plains,[13] subalpine forested hills, and eight of the world's ten tallest mountains, including Mount Everest, the highest point on Earth. Kathmandu is the nation's capital and largest city. It is a multiethnic nation with Nepali as the official language.		The territory of Nepal has a recorded history since the Neolithic age. The name "Nepal" is first recorded in texts from the Vedic Age, the era which founded Hinduism, the predominant religion of the country. In the middle of the first millennium BCE, Gautama Buddha, the founder of Buddhism, was born in southern Nepal. Parts of northern Nepal were intertwined with the culture of Tibet. The Kathmandu Valley in central Nepal became known as Nepal proper because of its complex urban civilisation. It was the seat of the prosperous Newar confederacy known as Nepal Mandala. The Himalayan branch of the ancient Silk Road was dominated by the valley's traders. The cosmopolitan region developed distinct traditional art and architecture. By the 18th century, the Gorkha Kingdom achieved the unification of Nepal. The Shah dynasty established the Kingdom of Nepal and later formed an alliance with the British Empire, under its Rana dynasty of premiers. The country was never colonised but served as a buffer state between Imperial China and Colonial India.[14][15][16] In the 20th century, Nepal ended its isolation and forged strong ties with regional powers. Parliamentary democracy was introduced in 1951, but was twice suspended by Nepalese monarchs in 1960 and 2005. The Nepalese Civil War resulted in the proclamation of a republic in 2008, ending the reign of the world's last Hindu monarchy.[17]		Modern Nepal is a federal secular parliamentary republic. It has seven states. Nepal is a developing nation, ranking 144th on the Human Development Index (HDI) in 2016. The country struggles with the transition from a monarchy to a republic. It also suffers from high levels of hunger and poverty. Despite these challenges, Nepal is making steady progress, with the government declaring its commitment to elevate the nation from least developed country status by 2022.[18][19] Nepal also has a vast potential to generate hydropower for export.		Nepal's foreign relations expanded after the Anglo-Nepal Treaty of 1923, which was recognised by the League of Nations. After a Soviet veto in 1949, Nepal was admitted to the United Nations in 1955. Friendship treaties were signed with the Dominion of India in 1950 and the People's Republic of China in 1960.[20][21] Nepal hosts the permanent secretariat of the South Asian Association for Regional Cooperation (SAARC), of which it is a founding member. Nepal is also a member of the Non Aligned Movement and the Bay of Bengal Initiative. The military of Nepal is the fifth largest in South Asia and is notable for its Gurkha history, particularly during the world wars, and has been a significant contributor to United Nations peacekeeping operations.						Local legends have that a Hindu sage named "Ne" established himself in the valley of Kathmandu in prehistoric times and that the word "Nepal" came into existence as the place was protected ("pala" in Pali) by the sage "Nemi". It is mentioned in Vedic texts that this region was called Nepal centuries ago. According to the Skanda Purana, a rishi called "Nemi" used to live in the Himalayas.[22] In the Pashupati Purana, he is mentioned as a saint and a protector.[23] He is said to have practised meditation at the Bagmati and Kesavati rivers[24] and to have taught there.[25]		The name of the country is also identical in origin to the name of the Newar people. The terms "Nepāl", "Newār", "Newāl" and "Nepār" are phonetically different forms of the same word, and instances of the various forms appear in texts in different times in history. Nepal is the learned Sanskrit form and Newar is the colloquial Prakrit form.[26] A Sanskrit inscription dated 512 CE found in Tistung, a valley to the west of Kathmandu, contains the phrase "greetings to the Nepals" indicating that the term "Nepal" was used to refer to both the country and the people.[27][28]		It has been suggested that "Nepal" may be a Sanskritization of "Newar", or "Newar" may be a later form of "Nepal".[29] According to another explanation, the words "Newar" and "Newari" are vulgarisms arising from the mutation of P to V, and L to R.[30]		Neolithic tools found in the Kathmandu Valley indicate that people have been living in the Himalayan region for at least eleven thousand years.[31] The oldest population layer is believed to be represented by the Kusunda people.[32] Which, according to Hogdson in 1847, were the earliest inhabitants and probably of proto-Australoid origin.[33]		Nepal is first mentioned in the late Vedic Atharvaveda Pariśiṣṭa as a place exporting blankets and in the post-Vedic Atharvashirsha Upanishad.[34] In Samudragupta's Allahabad Pillar it is mentioned as a bordering country. The Skanda Purana has a separate chapter known as "Nepal Mahatmya" that explains in more details about the beauty and power of Nepal.[35] Nepal is also mentioned in Hindu texts such as the Narayana Puja.[34]		Tibeto-Burman-speaking people probably lived in Nepal 2500 years ago.[36] However, there is no archaeologic evidence of the Gopal Bansa or Kirati rulers, only mention by the later Licchavi and Malla eras.[37] The first inhabitants of Nepal were properly of Dravidian origin whose history predates the onset of the Bronze Age in South Asia (around 3300 BCE), before the coming of other ethnic groups like the Tibeto-Burmans and Indo-Aryans from across the border.[31]		Around 500 BCE, small kingdoms and confederations of clans arose in the southern regions of Nepal. From one of these, the Shakya polity, arose a prince who later renounced his status to lead an ascetic life, founded Buddhism, and came to be known as Gautama Buddha (traditionally dated 563–483 BCE).		By 250 BCE, the southern regions came under the influence of the Maurya Empire of North India and parts of Nepal later on became a nominal vassal state under the Gupta Empire in the fourth century CE. Beginning in the third century CE, the Licchavi Kingdom governed the Kathmandu Valley and the region surrounding central Nepal.		There is a quite detailed description of the kingdom of Nepal in the account of the renowned Chinese Buddhist pilgrim monk Xuanzang, dating from c. 645 CE.[38][39] Stone inscriptions in the Kathmandu Valley are important sources for the history of Nepal.		The Licchavi dynasty went into decline in the late eighth century, probably due to the Tibetan Empire, and was followed by a Newar or Thakuri era, from 879 CE (Nepal Sambat 1), although the extent of their control over the present-day country is uncertain.[40] In the eleventh century it seems to have included the Pokhara area. By the late eleventh century, southern Nepal came under the influence of the Chalukya dynasty of South India. Under the Chalukyas, Nepal's religious establishment changed as the kings patronised Hinduism instead of the Buddhism prevailing at that time.		In the early 12th century, leaders emerged in far western Nepal whose names ended with the Sanskrit suffix malla ("wrestler"). These kings consolidated their power and ruled over the next 200 years, until the kingdom splintered into two dozen petty states. Another Malla dynasty beginning with Jayasthiti emerged in the Kathmandu valley in the late 14th century, and much of central Nepal again came under a unified rule. In 1482 the realm was divided into three kingdoms: Kathmandu, Patan, and Bhaktapur.		In the mid-18th century, Prithvi Narayan Shah, a Gorkha king, set out to put together what would become present-day Nepal. He embarked on his mission by securing the neutrality of the bordering mountain kingdoms. After several bloody battles and sieges, notably the Battle of Kirtipur, he managed to conquer the Kathmandu Valley in 1769. A detailed account of Prithvi Narayan Shah's victory was written by Father Giuseppe, an eyewitness to the war.[41]		The Gorkha dominion reached its height when the North Indian territories of the Kumaon and Garhwal Kingdoms in the west to Sikkim in the east came under Nepal rule. At its maximum extent, Greater Nepal extended from the Teesta River in the east, to Kangra, Himachal Pradesh, across the Sutlej in the west as well as further south into the Terai plains and north of the Himalayas than at present. A dispute with Tibet over the control of mountain passes and inner Tingri valleys of Tibet forced the Qing Emperor of China to start the Sino-Nepali War compelling the Nepali to retreat and pay heavy reparations to Peking.		Rivalry between Kingdom of Nepal and the East India Company over the annexation of minor states bordering Nepal eventually led to the Anglo-Nepali War (1815–16). At first the British underestimated the Nepali and were soundly defeated until committing more military resources than they had anticipated needing. They were greatly impressed by the valour and competence of their adversaries. Thus began the reputation of Gurkhas as fierce and ruthless soldiers. The war ended in the Sugauli Treaty, under which Nepal ceded recently captured portions of Sikkim and lands in Terai as well as the right to recruit soldiers. Madhesis, having supported the East India Company during the war, had their lands gifted to Nepali.[42]		Factionalism inside the royal family led to a period of instability. In 1846 a plot was discovered revealing that the reigning queen had planned to overthrow Jung Bahadur Kunwar, a fast-rising military leader. This led to the Kot massacre; armed clashes between military personnel and administrators loyal to the queen led to the execution of several hundred princes and chieftains around the country. Jung Bahadur Kunwar emerged victorious and founded the Rana dynasty, later known as Jung Bahadur Rana. The king was made a titular figure, and the post of Prime Minister was made powerful and hereditary. The Ranas were staunchly pro-British and assisted them during the Indian Rebellion of 1857 (and later in both World Wars). Some parts of the Terai region populated with non-Nepali peoples were gifted to Nepal by the British as a friendly gesture because of her military help to sustain British control in India during the rebellion. In 1923, the United Kingdom and Nepal formally signed an agreement of friendship that superseded the Sugauli Treaty of 1816.[42]		Slavery was abolished in Nepal in 1924.[43] Nevertheless, debt bondage even involving debtors' children has been a persistent social problem in the Terai. Rana rule was marked by tyranny, debauchery, economic exploitation and religious persecution.[44][45]		In the late 1940s, newly emerging pro-democracy movements and political parties in Nepal were critical of the Rana autocracy. Meanwhile, with the invasion of Tibet by China in the 1950s, India sought to counterbalance the perceived military threat from its northern neighbour by taking pre-emptive steps to assert more influence in Nepal. India sponsored both King Tribhuvan (ruled 1911–55) as Nepal's new ruler in 1951 and a new government, mostly comprising the Nepali Congress, thus terminating Rana hegemony in the kingdom.[42]		After years of power wrangling between the king and the government, King Mahendra (ruled 1955–72) scrapped the democratic experiment in 1959, and a "partyless" Panchayat system was made to govern Nepal until 1989, when the "Jan Andolan" (People's Movement) forced King Birendra (ruled 1972–2001) to accept constitutional reforms and to establish a multiparty parliament that took seat in May 1991.[46] In 1991–92, Bhutan expelled roughly 100,000 Bhutanese citizens of Nepali descent, most of whom have been living in seven refugee camps in eastern Nepal ever since.[47]		In 1996, the Communist Party of Nepal started a bid to replace the royal parliamentary system with a people's republic by violent means. This led to the long Nepali Civil War and more than 12,000 deaths.		On 1 June 2001, there was a massacre in the royal palace. King Birendra, Queen Aishwarya and seven other members of the royal family were killed. The alleged perpetrator was Crown Prince Dipendra, who committed suicide (he died three days later) shortly thereafter. This outburst was alleged to have been Dipendra's response to his parents' refusal to accept his choice of wife. Nevertheless, there is speculation and doubts among Nepali citizens about who was responsible.		Following the carnage, King Birendra's brother Gyanendra inherited the throne. On 1 February 2005, King Gyanendra dismissed the entire government and assumed full executive powers to quash the violent Maoist movement,[46] but this initiative was unsuccessful because a stalemate had developed in which the Maoists were firmly entrenched in large expanses of countryside but could not yet dislodge the military from numerous towns and the largest cities. In September 2005, the Maoists declared a three-month unilateral ceasefire to negotiate.		In response to the 2006 democracy movement, King Gyanendra agreed to relinquish sovereign power to the people. On 24 April 2006 the dissolved House of Representatives was reinstated. Using its newly acquired sovereign authority, on 18 May 2006 the House of Representatives unanimously voted to curtail the power of the king and declared Nepal a secular state, ending its time-honoured official status as a Hindu Kingdom. On 28 December 2007, a bill was passed in parliament to amend Article 159 of the constitution – replacing "Provisions regarding the King" by "Provisions of the Head of the State" – declaring Nepal a federal republic, and thereby abolishing the monarchy.[48] The bill came into force on 28 May 2008.[49]		The Unified Communist Party of Nepal (Maoist) won the largest number of seats in the Constituent Assembly election held on 10 April 2008, and formed a coalition government which included most of the parties in the CA. Although acts of violence occurred during the pre-electoral period, election observers noted that the elections themselves were markedly peaceful and "well-carried out".[50]		The newly elected Assembly met in Kathmandu on 28 May 2008, and, after a polling of 564 constituent Assembly members, 560 voted to form a new government,[49] with the monarchist Rastriya Prajatantra Party, which had four members in the assembly, registering a dissenting note. At that point, it was declared that Nepal had become a secular and inclusive democratic republic,[51][52] with the government announcing a three-day public holiday from 28–30 May. The king was thereafter given 15 days to vacate Narayanhity Palace so it could reopen as a public museum.[53]		Nonetheless, political tensions and consequent power-sharing battles have continued in Nepal. In May 2009, the Maoist-led government was toppled and another coalition government with all major political parties barring the Maoists was formed.[54] Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist–Leninist) was made the Prime Minister of the coalition government.[55] In February 2011 the Madhav Kumar Nepal Government was toppled and Jhala Nath Khanal of the Communist Party of Nepal (Unified Marxist–Leninist) was made the Prime Minister.[56] In August 2011 the Jhala Nath Khanal Government was toppled and Baburam Bhattarai of the Communist Party of Nepal (Maoist) was made the Prime Minister.[57]		The political parties were unable to draft a constitution in the stipulated time.[58] This led to dissolution of the Constituent Assembly to pave way for new elections to strive for a new political mandate. In opposition to the theory of separation of powers, then Chief Justice Khil Raj Regmi was made the chairman of the caretaker government. Under Regmi, the nation saw peaceful elections for the constituent assembly. The major forces in the earlier constituent assembly (namely CPN Maoists and Madhesi parties) dropped to distant 3rd and even below.[59][60]		In February 2014, after consensus was reached between the two major parties in the constituent assembly, Sushil Koirala was sworn in as the new prime minister of Nepal.[61][62]		In 20 September 2015, a new constitution, the "Constitution of Nepal 2015" (Nepali: नेपालको संविधान २०७२) was announced by President Ram Baran Yadav in the constituent assembly. The constituent assembly was transformed into a legislative parliament by the then-chairman of that assembly. The new constitution of Nepal has changed Nepal practically into a federal democratic republic by making 7 unnamed states.		On 25 April 2015, a magnitude 7.8 earthquake struck Nepal.[63] Two weeks later, on 12 May, another earthquake with a magnitude of 7.3 hit Nepal, which left more than 8,500 people dead and about 21,000, injured.[64]		In October 2015, Bidhya Devi Bhandari was nominated as the first female president.[65]		Nepal is of roughly trapezoidal shape, 800 kilometres (497 mi) long and 200 kilometres (124 mi) wide, with an area of 147,181 km2 (56,827 sq mi). See List of territories by size for the comparative size of Nepal. It lies between latitudes 26° and 31°N, and longitudes 80° and 89°E.		Nepal is commonly divided into three physiographic areas:Himal, Pahad and Terai. These ecological belts run east–west and are vertically intersected by Nepal's major, north to south flowing river systems.		The southern lowland plains or Terai bordering India are part of the northern rim of the Indo-Gangetic Plain.Terai is a low land region containing some hill ranges. They were formed and are fed by three major Himalayan rivers: the Kosi, the Narayani, and the Karnali as well as smaller rivers rising below the permanent snowline. This region has a subtropical to tropical climate. The outermost range of foothills called Sivalik Hills or Churia Range cresting at 700 to 1,000 metres (2,297 to 3,281 ft) marks the limit of the Gangetic Plain, however broad, low valleys called Inner Tarai Valleys (Bhitri Tarai Uptyaka) lie north of these foothills in several places.		Pahad is a mountain region which does not generally contain snow. The mountains vary from 800 to 4,000 metres (2,625 to 13,123 ft) in altitude with progression from subtropical climates below 1,200 metres (3,937 ft) to alpine climates above 3,600 metres (11,811 ft). The Lower Himalayan Range reaching 1,500 to 3,000 metres (4,921 to 9,843 ft) is the southern limit of this region, with subtropical river valleys and "hills" alternating to the north of this range. Population density is high in valleys but notably less above 2,000 metres (6,562 ft) and very low above 2,500 metres (8,202 ft) where snow occasionally falls in winter.		Himal is mountain region containing snow and situated in the Great Himalayan Range, makes up the northern part of Nepal. It contains the highest elevations in the world including 8,848 metres (29,029 ft) height Mount Everest (Sagarmāthā in Nepali) on the border with China. Seven other of the world's "eight-thousanders" are in Nepal or on its border with China: Lhotse, Makalu, Cho Oyu, Kangchenjunga, Dhaulagiri, Annapurna and Manaslu.		Nepal has five climatic zones, broadly corresponding to the altitudes. The tropical and subtropical zones lie below 1,200 metres (3,937 ft), the temperate zone 1,200 to 2,400 metres (3,937 to 7,874 ft), the cold zone 2,400 to 3,600 metres (7,874 to 11,811 ft), the subarctic zone 3,600 to 4,400 metres (11,811 to 14,436 ft), and the Arctic zone above 4,400 metres (14,436 ft).		Nepal experiences five seasons: summer, monsoon, autumn, winter and spring. The Himalaya blocks cold winds from Central Asia in the winter and forms the northern limit of the monsoon wind patterns. In a land once thickly forested, deforestation is a major problem in all regions, with resulting erosion and degradation of ecosystems.		Nepal is popular for mountaineering, having some of the highest and most challenging mountains in the world, including Mount Everest. Technically, the south-east ridge on the Nepali side of the mountain is easier to climb; so, most climbers prefer to trek to Everest through Nepal.		The highest mountains in Nepal are given here:[66]		The collision between the Indian subcontinent and Eurasia, which started in Paleogene time and continues today, produced the Himalaya and the Tibetan Plateau. Nepal lies completely within this collision zone, occupying the central sector of the Himalayan arc, nearly one third of the 2,400 km (1,500 mi)-long Himalayas.[67][68][69][70][71][72]		The Indian plate continues to move north relative to Asia at the rate of approximately 50 mm (2.0 in) per year.[73] This is approximately twice the speed at which human fingernails grow, which is very fast given the size of the blocks of Earth's crust involved.[original research?] As the strong Indian continental crust subducts beneath the relatively weak Tibetan crust, it pushes up the Himalayan Mountains. This collision zone has accommodated huge amounts of crustal shortening as the rock sequences slide one over another.		Based on a study published in 2014, of the Main Frontal Thrust, on average a great earthquake occurs every 750 ± 140 and 870 ± 350 years in the east Nepal region.[74] A study from 2015 found a 700-year delay between earthquakes in the region. The study also suggests, that because of tectonic stress transfer, the earthquake from 1934 in Nepal and the 2015 earthquake are connected – following a historic earthquake pattern.[75]		Erosion of the Himalayas is a very important source of sediment, which flows via several great rivers: the Indus, Ganges, and Brahmaputra River systems to the Indian Ocean.[76]		The dramatic differences in elevation found in Nepal result in a variety of biomes, from tropical savannas along the Indian border, to subtropical broadleaf and coniferous forests in the Hill Region, to temperate broadleaf and coniferous forests on the slopes of the Himalaya, to montane grasslands and shrublands and rock and ice at the highest elevations.		At the lowest elevations is the Terai-Duar savanna and grasslands ecoregion. These form a mosaic with the Himalayan subtropical broadleaf forests, which occur from 500 to 1,000 metres (1,600 to 3,300 ft) and include the Inner Terai Valleys. Himalayan subtropical pine forests occur between 1,000 and 2,000 metres (3,300 and 6,600 ft).		Above these elevations, the biogeography of Nepal is generally divided from east to west by the Gandaki River. Ecoregions to the east tend to receive more precipitation and to be more species-rich. Those to the west are drier with fewer species.		From 1,500 to 3,000 metres (4,900 to 9,800 ft), are temperate broadleaf forests: the eastern and western Himalayan broadleaf forests. From 3,000 to 4,000 metres (9,800 to 13,100 ft) are the eastern and western Himalayan subalpine conifer forests. To 5,500 metres (18,000 ft) are the eastern and western Himalayan alpine shrub and meadows.		View of Khartuwa village from Thakuri village of Sitalpati, Shankhuwasabha, eastern Nepal.		NASA Landsat-7 Image of Nepal. Nepal shares its boundaries with India and China		The Annapurna range of the Himalayas.		Phoksundo Lake		Kali Gandaki Gorge is one of the deepest gorges on earth.		Marshyangdi Valley – There are many such valleys in the Himalaya created by glacier flows.		Mount Everest, the highest peak on earth, lies on the Nepal-China border		Wind erosion in Kalopani		A field in Terai		Phulchowki Hill		Hills view of Ghorahi, Dang		View of mountains		Nepal has seen rapid political changes during the last two decades. Up until 1990, Nepal was a monarchy under executive control of the King. Faced with a communist movement against absolute monarchy, King Birendra, in 1990, agreed to a large-scale political reform by creating a parliamentary monarchy with the king as the head of state and a prime minister as the head of the government.		Nepal's legislature was bicameral, consisting of a House of Representatives called the Pratinidhi Sabha and a National Council called the Rastriya Sabha. The House of Representatives consisted of 205 members directly elected by the people. The National Council had 60 members: ten nominated by the king, 35 elected by the House of Representatives, and the remaining 15 elected by an electoral college made up of chairs of villages and towns. The legislature had a five-year term but was dissolvable by the king before its term could end. All Nepali citizens 18 years and older became eligible to vote.		The executive comprised the King and the Council of Ministers (the cabinet). The leader of the coalition or party securing the maximum seats in an election was appointed as the Prime Minister. The Cabinet was appointed by the king on the recommendation of the Prime Minister. Governments in Nepal tended to be highly unstable, falling either through internal collapse or parliamentary dissolution by the monarch, on the recommendation of the prime minister, according to the constitution; no government has survived for more than two years since 1991.		The movement in April 2006 brought about a change in the nation's governance: an interim constitution was promulgated, with the King giving up power, and an interim House of Representatives was formed with Maoist members after the new government held peace talks with the Maoist rebels. The number of parliamentary seats was also increased to 330. In April 2007, the Communist Party of Nepal (Maoist) joined the interim government of Nepal.		In December 2007, the interim parliament passed a bill making Nepal a federal republic, with a president as head of state. Elections for the constitutional assembly were held on 10 April 2008; the Maoist party led the results but did not achieve a simple majority of seats.[77] The new parliament adopted the 2007 bill at its first meeting by an overwhelming majority, and King Gyanendra was given 15 days to leave the Royal Palace in central Kathmandu. He left on 11 June.[78]		On 26 June 2008, the prime minister Girija Prasad Koirala, who had served as Acting Head of State since January 2007, announced that he would resign on the election of the country's first president by the Constituent Assembly. The first round of voting, on 19 July 2008, saw Parmanand Jha win election as Nepali vice-president, but neither of the contenders for president received the required 298 votes and a second round was held two days later. Ram Baran Yadav of the Nepali Congress party defeated Maoist-backed Ram Raja Prasad Singh with 308 of the 590 votes cast.[79] Koirala submitted his resignation to the new president after Yadav's swearing-in ceremony on 23 July 2008.		On 15 August 2008, Maoist leader Prachanda (Pushpa Kamal Dahal) was elected Prime Minister of Nepal, the first since the country's transition from a monarchy to a republic. On 4 May 2009, Dahal resigned over on-going conflicts with regard to the sacking of the Army chief. Since Dahal's resignation, the country has been in a serious political deadlock with one of the big issues being the proposed integration of the former Maoist combatants, also known as the People's Liberation Army, into the national security forces.[80] After Dahal, Jhala Nath Khanal of CPN (UML) was elected the Prime Minister. Khanal was forced to step down as he could not succeed in carrying forward the Peace Process and the constitution writing. On August 2011, Maoist Babu Ram Bhattarai became third Prime Minister after the election of constituent assembly.[81] On 24 May 2012, Nepals's Deputy PM Krishna Sitaula resigned.[82] On 27 May 2012, the country's Constituent Assembly failed to meet the deadline for writing a new constitution for the country. Prime Minister Baburam Bhattarai announced that new elections will be held on 22 November 2012. "We have no other option but to go back to the people and elect a new assembly to write the constitution," he said in a nationally televised speech. One of the main obstacles has been disagreement over whether the states which will be created will be based on ethnicity.[83]		Nepal is one of the few countries in Asia to abolish the death penalty.[84] Nepal is the only Asian country where the possibility of same-sex marriage has been proposed in the high court and in the legislature although same-sex marriage currently does not exist in Nepal (see also LGBT rights in Nepal and Same-sex marriage in Nepal). The decision was based on a seven-person government committee study, and enacted through Supreme Court's ruling November 2008. The ruling granted full rights for LGBT individuals, including the right to marry[85] and now can get citizenship as a third gender rather than male or female as authorised by Nepal's Supreme Court in 2007.[86]		Nepal is governed according to the Constitution of Nepal, which came into effect on 20 September 2015, replacing the Interim Constitution of 2007. The Constitution was drafted by the Second Constituent Assembly following the failure of the First Constituent Assembly to produce a constitution in its mandated period. The constitution is the fundamental law of Nepal. It defines Nepal as having multi-ethnic, multi-lingual, multi-religious, multi-cultural characteristics with common aspirations of people living in diverse geographical regions, and being committed to and united by a bond of allegiance to national independence, territorial integrity, national interest and prosperity of Nepal. All the Nepali people collectively constitute the nation.		The Constitution of Nepal has defined three organs of the government.[87]		The form of governance of Nepal shall be a multi-party, competitive, federal democratic republican parliamentary system based on plurality.		The executive power of Nepal shall rest with the Council of Ministers in accordance with the Constitution and law.The President shall appoint the parliamentary party leader of the political party with the majority in the House of Representatives as a Prime Minister, and a Council of Ministers shall be formed in his/her chairmanship		The executive power of the State shall, pursuant to the Constitution and laws, be vested in the Council of Ministers of the State. Provided that the executive power of the State shall be exercised by the State Head in case of absence of the State Executive in a State of Emergency or enforcement of Federal rule. Every state shall have a State Head as the representative of the Federal government. The President shall appoint a State Head for every state. The State Head shall exercise the rights and duties as specified in the constitution or laws. The State Head shall appoint the leader of the parliamentary party with majority in the State Assembly as the Chief Minister and the State Council of Ministers shall be formed under the chairpersonship of the Chief Minister.		There shall be a Legislature, called Federal Parliament, consisting of two Houses, namely the House of Representatives and the National Assembly.		Except when dissolved earlier, the term of House of Representatives shall be five years. The House of Representatives shall consist of 275 members as follows:		The National Assembly shall be a permanent house. The tenure of members of National Assembly shall be six years. The National Assembly shall consist of two 59 members as follows:		There shall be a unicameral legislature in a state which shall be called the State Assembly. Every State Assembly shall consist of the following number of members:		Powers relating to justice in Nepal shall be exercised by courts and other judicial institutions in accordance with the provisions of this Constitution, other laws and recognised principles of justice. There shall be the following courts in Nepal:		Nepal has close ties with both of its neighbors, India and China. In accordance with a long-standing treaty, Indian and Nepali citizens may travel to each other's countries without a passport or visa. Nepali citizens may work in India without legal restriction. The Indian Army maintains seven Gorkha regiments consisting of Gorkha troops recruited mostly from Nepal.		However, in the years since the Government of Nepal has been communised and dominated by socialists, and India's government has been controlled by more right-wing parties, India has been remilitarising the "porous" Indo-Nepali border to stifle the flow of Islamist groups.[88]		Nepal established relations with the People's Republic of China on 1 August 1955, and relations since have been based on the Five Principles of Peaceful Coexistence. Nepal has aided China in the aftermath of the 2008 Sichuan earthquake, and China has provided economic assistance for Nepali infrastructure. Both countries have cooperated to host the 2008 Summer Olympics summit of Mt. Everest.[89] Nepal has assisted in curbing anti-China protests from the Tibetan diaspora.[90]		Nepal's military consists of the Nepali Army, which includes the Nepali Army Air Service. The Nepali Police Force is the civilian police and the Armed Police Force Nepal[91] is the paramilitary force. Service is voluntary and the minimum age for enlistment is 18 years. Nepal spends $99.2 million (2004) on its military—1.5% of its GDP. Much of the equipment and arms are imported from India. Consequently, the US provided M16s, M4s and other Colt weapons to combat communist (Maoist) insurgents. The standard-issue battle rifle of the Nepali army is the Colt M16.[92]		In the new regulations by Nepali Army, female soldiers have been barred from participating in combat situations and fighting in the frontlines of war. However, they are allowed to be a part of the army in sections like intelligence, headquarters, signals and operations.[93]		As of 20 September 2015, Nepal is divided into 7 states and 75 districts. It has 744 local units. There are 4 metropolises, 13 sub-metropolises, 246 municipal councils and 481 village councils for official works. The constitution grants 22 absolute powers to the local units while they share 15 more powers with the central and state governments.[11]		State No. 1		State No. 2		State No. 3		State No. 4		State No. 5		State No. 6		State No. 7		* – denotes the districts that are proposed to be re-allocated to State No. 4 and State No. 6		Nepal's gross domestic product (GDP) for 2012 was estimated at over $17.921 billion (adjusted to nominal GDP).[6] In 2010, agriculture accounted for 36.1%, services comprised 48.5%, and industry 15.4% of Nepal's GDP.[94] While agriculture and industry are contracting, the contribution by the service sector is increasing.[94][95]		Agriculture employs 76% of the workforce, services 18% and manufacturing and craft-based industry 6%. Agricultural produce – mostly grown in the Terai region bordering India – includes tea, rice, corn, wheat, sugarcane, root crops, milk, and water buffalo meat. Industry mainly involves the processing of agricultural produce, including jute, sugarcane, tobacco, and grain. Its workforce of about 10 million suffers from a severe shortage of skilled labour.		Nepal's economic growth continues to be adversely affected by the political uncertainty. Nevertheless, real GDP growth was estimated to increase to almost 5 percent for 2011–2012. This is an improvement from the 3.5 percent GDP growth in 2010–2011 and would be the second-highest growth rate in the post-conflict era.[96] Sources of growth include agriculture, construction, financial and other services. The contribution of growth by consumption fuelled by remittances has declined since 2010/2011. While remittance growth slowed to 11 percent (in Nepali Rupee terms) in 2010/2011, it has since increased to 37 percent. Remittances are estimated to be equivalent to 25–30 percent of GDP. Inflation has been reduced to a three-year low of 7 percent.[96]		The proportion of poor people has declined substantially since 2003. The percentage of people living below the international poverty line (people earning less than US$1.25 per day) has halved in seven years.[96] At this measure of poverty the percentage of poor people declined from 53.1% in 2003/2004 to 24.8% in 2010/2011.[96] With a higher poverty line of US$2 per-capita per day, poverty declined by one-quarter to 57.3%.[96] However, the income distribution remains grossly uneven.[97]		In a recent survey, Nepal has performed extremely well in reducing poverty along with Rwanda and Bangladesh as the percentage of poor dropped to 44.2 percent of the population in 2011 from 64.7 percent in 2006—4.1 percentage points per year, which means that Nepal has made improvement in sectors like nutrition, child mortality, electricity, improved flooring and assets. If the progress of reducing poverty continues at this rate, then it is predicted that Nepal will halve the current poverty rate and eradicate it within the next 20 years.[98][99]		The spectacular landscape and diverse, exotic cultures of Nepal represent considerable potential for tourism, but growth in the industry has been stifled by political instability and poor infrastructure. Despite these problems, in 2012 the number of international tourists visiting Nepal was 598,204, a 10% increase on the previous year.[100] The tourism sector contributed nearly 3% of national GDP in 2012 and is the second-biggest foreign income earner after remittances.[101]		The rate of unemployment and underemployment approaches half of the working-age population. Thus many Nepali citizens move to other countries in search of work. Destinations include India, Qatar, the United States, Thailand, the United Kingdom, Saudi Arabia, Japan, Brunei Darussalam, Australia, and Canada.[102][103] Nepal receives $50 million a year through the Gurkha soldiers who serve in the Indian and British armies and are highly esteemed for their skill and bravery. As of 2010[update], the total remittance value is around $3.5 billion.[103] In 2009 alone, the remittance contributed to 22.9% of the nation's GDP.[103]		A long-standing economic agreement underpins a close relationship with India. The country receives foreign aid from the UK,[104][105] India, Japan, the US, the EU, China, Switzerland, and Scandinavian countries. Poverty is acute; per-capita income is around $1,000.[106] The distribution of wealth among the Nepali is consistent with that in many developed and developing countries: the highest 10% of households control 39.1% of the national wealth and the lowest 10% control only 2.6%.		The government's budget is about $1.153 billion, with an expenditure of $1.789 billion (FY 20005/06). The Nepali rupee has been tied to the Indian rupee at an exchange rate of 1.6 for many years. Since the loosening of exchange rate controls in the early 1990s, the black market for foreign exchange has all but disappeared. The inflation rate has dropped to 2.9% after a period of higher inflation during the 1990s.		Nepal's exports of mainly carpets, clothing, hemp, leather goods, jute goods and grain total $822 million. Import commodities of mainly gold, machinery and equipment, petroleum products and fertiliser total US$2 billion. European Union (EU) (46.13%), the US (17.4%), and Germany (7.1%) are its main export partners. The European Union has emerged the largest buyer of Nepali ready-made garments (RMG). Exports to the EU accounted for "46.13 percent of the country's total garment exports".[107] Nepal's import partners include India (47.5%), the United Arab Emirates (11.2%), China (10.7%), Saudi Arabia (4.9%), and Singapore (4%).		Besides having landlocked, rugged geography, few tangible natural resources and poor infrastructure, the ineffective post-1950 government and the long-running civil war are also factors in stunting the nation's economic growth and development.[108][109][110]		The bulk of the energy in Nepal comes from fuel wood (68%), agricultural waste (15%), animal dung (8%), and imported fossil fuels (8%).[111][112] Except for some lignite deposits, Nepal has no known oil, gas or coal deposits. All commercial fossil fuels (mainly oil and coal) are either imported from India or from international markets routed through India and China. Fuel imports absorb over one-fourth of Nepal's foreign exchange earnings.[112]		Only about 1% energy need is fulfilled by electricity. The perennial nature of Nepali rivers and the steep gradient of the country's topography provide ideal conditions for the development of some of the world's largest hydroelectric projects. Current estimates put Nepal's economically feasible hydropower potential to be approximately 83,000 MW from 66 hydropower project sites.[112][113] However, currently Nepal has been able to exploit only about 600 MW from 20 medium to large hydropower plants and a number of small and micro hydropower plants.[111] There are 9 major hydropower plants under construction, and additional 27 sites considered for potential development.[111] Only about 40% of Nepal's population has access to electricity.[111] There is a great disparity between urban and rural areas. The electrification rate in urban areas is 90%, whereas the rate for rural areas is only 5%.[112] Power cuts of up to 22 hours a day take place in peak demand periods of winter and the peak electricity demand is almost the double the capability or dependable capacity.[114] The position of the power sector remains unsatisfactory because of high tariffs, high system losses, high generation costs, high overheads, over staffing, and lower domestic demand.[112]		Nepal remains isolated from the world's major land, air and sea transport routes although, within the country, aviation is in a better state, with 47 airports, 11 of them with paved runways;[115] flights are frequent and support a sizable traffic. The hilly and mountainous terrain in the northern two-thirds of the country has made the building of roads and other infrastructure difficult and expensive. In 2007 there were just over 10,142 km (6,302 mi) of paved roads, and 7,140 km (4,437 mi) of unpaved road, and one 59 km (37 mi) railway line in the south.[115]		More than one-third of its people live at least a two hours walk from the nearest all-season road; 15 out of 75 district headquarters are not connected by road. In addition, around 60% of the road network and most rural roads are not operable during the rainy season.[116] The only practical seaport of entry for goods bound for Kathmandu is Kolkata in West Bengal state of India. Internally, the poor state of development of the road system makes access to markets, schools, and health clinics a challenge.[108]		According to the Nepal Telecommunication Authority MIS May 2012 report,[117] there are seven operators and the total voice telephony subscribers including fixed and mobile are 16,350,946 which gives a penetration rate of 61.42%. The fixed telephone service account for 9.37%, mobile for 64.63%, and other services (LM, GMPCS) for 3.76% of the total penetration rate. Similarly, the numbers of subscribers to data/internet services are 4,667,536 which represents 17.53% penetration rate. Most of the data service is accounted by GPRS users. Twelve months earlier the data/internet penetration was 10.05%, thus this represents a growth rate of 74.77%.[117]		Not only has there been strong subscriber growth, especially in the mobile sector, but there was evidence of a clear vision in the sector, including putting a reform process in place and planning for the building of necessary telecommunications infrastructure. Most importantly, the Ministry of Information and Communications (MoIC) and the telecom regulator, the National Telecommunications Authority (NTA), have both been very active in the performance of their respective roles.[118]		Despite all the effort, there remained a significant disparity between the high coverage levels in the cities and the coverage available in the underdeveloped rural regions. Progress on providing some minimum access had been good. Of a total of 3,914 village development committees across the country, 306 were unserved by December 2009.[118] In order to meet future demand, it was estimated that Nepal needed to invest around US$135 million annually in its telecom sector.[118] In 2009, the telecommunication sector alone contributed to 1% of the nation's GDP.[119] As of 30 September 2012, Nepal has 1,828,700 Facebook users.[120]		As of 2007[update], the state operates two television stations as well as national and regional radio stations. There are roughly 30 independent TV channels registered, with only about half in regular operation. Nearly 400 FM radio stations are licensed with roughly 300 operational.[115] According to the 2011 census, the percentage of households possessing radio was 50.82%, television 36.45%, cable TV 19.33%, computer 7.23%.[2] According to the Press Council Nepal, as of 2012[update] there are 2,038 registered newspapers in Nepal, among which 514 are in publication.[121] In 2013, Reporters Without Borders ranked Nepal at 118th place in the world in terms of press freedom.[122][123]		The overall literacy rate (for population age 5 years and above) increased from 54.1% in 2001 to 65.9% in 2011. The male literacy rate was 75.1% compared to the female literacy rate of 57.4%. The highest literacy rate was reported in Kathmandu district (86.3%) and lowest in Rautahat (41.7%).[2] While the net primary enrollment rate was 74% in 2005;[124] in 2009, that enrollment rate was 90%.[125]		However, increasing access to secondary education (grade 9–12) remains a major challenge, as evidenced by the low net enrollment rate of 24% at this level. More than half of primary students do not enter secondary schools, and only one-half of them complete secondary schooling. In addition, fewer girls than boys join secondary schools and, among those who do, fewer complete the 10th grade.[126]		Nepal has seven universities: Tribhuvan University, Kathmandu University, Pokhara University, Purbanchal University, Mahendra Sanskrit University, Far-western University, and Agriculture and Forestry University.[127] Some newly proposed universities are Lumbini Bouddha University, and Mid-Western University. Some fine scholarship has emerged in the post-1990 era.[128]		Public health and health care services in Nepal are provided by both the public and private sectors and fare poorly by international standards.[citation needed] According to 2011 census, more than one-third (38.17%) of the total households do not have a toilet.[2] Tap water is the main source of drinking water for 47.78% of households, tube well/hand pump is the main source of drinking water for about 35% of households, while spout, uncovered well/kuwa, and covered well/kuwa are the main source for 5.74%, 4.71%, and 2.45% respectively.[2] Based on 2010 World Health Organization (WHO) data, Nepal ranked 139th in life expectancy in 2010 with the average Nepali living to 65.8 years.[129][130]		Diseases are more prevalent in Nepal than in other South Asian countries, especially in rural areas. Leading diseases and illnesses include diarrhea, gastrointestinal disorders, goitres, intestinal parasites, leprosy, visceral leishmaniasis and tuberculosis.[131] About 4 out of 1,000 adults aged 15 to 49 had human immunodeficiency virus (HIV), and the HIV prevalence rate was 0.5%.[132][133] Malnutrition also remains very high: about 47% of children under five are stunted, 15 percent wasted, and 36 percent underweight, although there has been a declining trend for these rates over the past five years, they remain alarmingly high.[134] In spite of these figures, improvements in health care have been made, most notably in maternal-child health. In 2012, the under-five infant mortality was estimated to be 41 out of every 1000 children.[135][136] Overall Nepal's Human Development Index (HDI) for health was 0.77 in 2011, ranking Nepal 126 out of 194 countries, up from 0.444 in 1980.[137][138]		The Community Forestry Program in Nepal is a participatory environmental governance that encompasses well-defined policies, institutions, and practices. The program addresses the twin goals of forest conservation and poverty reduction. As more than 70 percent of Nepal's population depends on agriculture for their livelihood, community management of forests has been a critically important intervention. Through legislative developments and operational innovations over three decades, the program has evolved from a protection-oriented, conservation-focused agenda to a much more broad-based strategy for forest use, enterprise development, and livelihood improvement. By April 2009, one-third of Nepal's population was participating in the program, directly managing more than one-fourth of Nepal's forest area.[139][140]		The immediate livelihood benefits derived by rural households bolster strong collective action wherein local communities actively and sustainably manage forest resources. Community forests also became the source of diversified investment capital and raw material for new market-oriented livelihoods. Community forestry shows traits of political, financial, and ecological sustainability, including an emergence of a strong legal and regulatory framework, and robust civil society institutions and networks. However, a continuing challenge is to ensure equitable distribution of benefits to women and marginalised groups. Lessons for replication emphasise experiential learning, establishment of a strong civil society network, flexible regulation to encourage diverse institutional modalities, and responsiveness of government and policymakers to a multistakeholder collaborative learning process.[141][142]		Historical kingdoms that existed in the Kathmandu valley are found to have made use of some clever technologies in numerous areas such as architecture, agriculture, civil engineering, water management, etc. The Gopals and Abhirs, who ruled the valley up until c. 1000 BC, used temporary materials for construction such as bamboo, hay, timber, etc. The Kirat period (700 BC – 110 AD) employed the technology of brick firing as well as produced quality woolen shawls. Similarly, stupas, idols, canals, self-recharging ponds, reservoirs, etc. constructed during the Lichhavi era (110–879 AD) are intact to this day, which manifests the ingenuity of traditional architecture. Moreover, the Malla period (1200–1768 AD) saw an impressive growth in architecture, on par with its advanced contemporaries. An archetypal example of Malla architecture is Nyatapola, a five-storied, 30-metre tall temple in Bhaktapur, which has strangely survived at least four major earthquakes, including the April 2015 Nepal earthquake.[143]		Nepal was a late entrant into the modern world of science and technology. Nepal’s first institution of higher education, Tri-Chandra College, was established by Chandra Shumsher in 1918. The college introduced science at the Intermediate level a year later, marking the genesis of formal science education in the country.[143] However, the college was not accessible to the general public, but only to a handful of members of the Rana regime. Throughout the Rana regime that lasted for well over a century, Nepal was effectively isolated from the rest of the world. Owing to this isolation, Nepal was relatively untouched by and unfamiliar of social transformations brought about by the British invasion in India and the Industrial Revolution in the West.[144] However, after the advent of democracy and abolition of Rana regime in 1951, Nepal broke free from the shackles of self-imposed isolation and opened up to the outside world. This opening marked the initiation of S&T activities in the country.[145]		An underdeveloped country, Nepal is plagued with problems such as poverty, illiteracy, unemployment, and the like. Consequently, science and technology have invariably lagged behind in the priority list of the government. On the other hand, citing poor university education at home, tens of thousands of Nepali students leave the country every year, with half of them never returning.[146][147] These factors have been huge deterrents to the development of science and technology in Nepal.		Law enforcement in Nepal is primarily the responsibility of the Nepali Police Force which is the national police of Nepal.[148] It is independent of the Nepali Army. In the days of its establishment, Nepal Police personnel were mainly drawn from the armed forces of the Nepali Congress Party which fought against the feudal Rana autocracy in Nepal. The Central Investigation Bureau (CIB) and National Investigation Department of Nepal (NID) are the investigation agencies of Nepal. They have offices in all 75 administrative districts including regional offices in five regions and zonal offices in 14 zones. Numbers vary from three to five members at each district level in rural districts, and numbers can be higher in urban districts. They have both Domestic and International surveillance unit which mainly deals with cross border terrorists, drug trafficking and money laundering.[149][150][151][152]		A 2010 survey estimated about 46,000 hard drug users in the country, with 70% of the users to be within the age group of 15 to 29.[153] The same survey also reported that 19% of the users had been introduced to hard drugs when they were less than 15 years old; and 14.4% of drug users were attending school or college.[153] Only 12 of the 17 municipalities studied had any type of rehabilitation centre.[153][154] There has been a sharp increase in the seizure of drugs such as hashish, heroin and opium in the past few years; and there are indications that drug traffickers are trying to establish Nepal as a transit point.[155]		Human trafficking is a major problem in Nepal.[156][157][158] Nepali victims are trafficked within Nepal, to India, the Middle East, and other areas such as Malaysia and forced to become prostitutes, domestic servants, beggars, factory workers, mine workers, circus performers, child soldiers, and others. Sex trafficking is particularly rampant within Nepal and to India, with as many as 5,000 to 10,000 women and girls trafficked to India alone each year.[159][160][161]		With wider availability of information technology, cyber crime is a growing trend. The police handled 16 cases of cyber crime in fiscal year 2010/2011, 47 cases in 2011/2012 and 78 in the current fiscal year.[162][clarification needed]		Capital punishment was abolished in Nepal in 1997.[163] In 2008, the Nepali government abolished the Haliya system of forced labour, freeing about 20,000 people.[164] However, the effectiveness of this has been questioned by the Asian Legal Resource Centre.[165]		According to the 2011 census, Nepal's population grew from 9 million people in 1950 to 26.5 million. From 2001 to 2011, the average family size declined from 5.44 to 4.9. The census also noted some 1.9 million absentee people, over a million more than in 2001; most are male labourers employed overseas, predominantly in South Asia and the Middle East. This correlated with the drop in sex ratio from 94.41 as compared to 99.80 for 2001. The annual population growth rate is 1.35%.[2]		The citizens of Nepal are known as Nepali or Nepalese. The country is home to people of many different national origins. As a result, Nepalese do not equate their nationality with ethnicity, but with citizenship and allegiance. Although citizens make up the majority of Nepalese, non-citizen residents, dual citizens, and expatriates may also claim a Nepalese identity. Nepal is multicultural and multiethnic country because it became a country by occupying several small kingdoms such as Mustang, Videha (Mithila), Madhesh, and Limbuwan in the 18th century. The oldest settlements in Mithila and Tharuhat are Maithil. Northern Nepal is historically inhabited by Kirants Mongoloid, Rai and Limbu people. The mountainous region is sparsely populated above 3,000 m (9,800 ft), but in central and western Nepal ethnic Sherpa and Lamapeople inhabit even higher semi-arid valleys north of the Himalaya. The Nepali speaking Khas people mostly inhabitate in central and southern regions. Kathmandu Valley, in the middle hill region, constitutes a small fraction of the nation's area but is the most densely populated, with almost 5 percent of the nation's population. The Nepali are descendants of three major migrations from India, Tibet, and North Burma and the Chinese state of Yunnan via Assam. Among the earliest inhabitants were the Kirat of east mid-region, Newars of the Kathmandu Valley, aboriginal Tharus of Tharuhat,		Despite the migration of a significant section of the population to the Madhesh (southern plains) in recent years, the majority of Nepalese still live in the central highlands; the northern mountains are sparsely populated. Kathmandu, with a population of over 2.6 million (metropolitan area: 5 million),[dubious – discuss] is the largest city in the country and the cultural and economic heart.		According to the World Refugee Survey 2008, published by the US Committee for Refugees and Immigrants, Nepal hosted a population of refugees and asylum seekers in 2007 numbering approximately 130,000. Of this population, approximately 109,200 persons were from Bhutan and 20,500 from People's Republic of China.[166][167] The government of Nepal restricted Bhutanese refugees to seven camps in the Jhapa and Morang districts, and refugees were not permitted to work in most professions.[166] At present, the United States is working towards resettling more than 60,000 of these refugees in the US.[47]		Nepal's diverse linguistic heritage stems from three major language groups: Indo-Aryan, Tibeto-Burman, and various indigenous language isolates. The major languages of Nepal (percent spoken as native language) according to the 2011 census are Nepali (44.6%), Maithili (11.7%), Bhojpuri (Awadhi Language) (6.0%), Tharu (5.8%), Tamang (5.1%), Nepal Bhasa (3.2%), Bajjika (3%) and Magar (3.0%), Doteli (3.0%), Urdu (2.6%) and Sunwar. Nepal is home to at least four indigenous sign languages.		Derived from Sanskrit, Nepali is written in Devanagari script. Nepali is the official language and serves as lingua franca among Nepali of different ethnolinguistic groups. The regional languages Maithili, Awadhi, Bhojpuri and rarely Urdu of Nepali Muslims are spoken in the southern Madhesh region. Many Nepali in government and business speak Maithili as the main language and Nepali as their de facto lingua franca. Varieties of Tibetan are spoken in and north of the higher Himalaya where standard literary Tibetan is widely understood by those with religious education. Local dialects in the Terai and hills are mostly unwritten with efforts underway to develop systems for writing many in Devanagari or the Roman alphabet.		The overwhelming majority of the Nepalese population follows Hinduism. Shiva is regarded as the guardian deity of the country.[169] Nepal is home to the famous Lord Shiva temple, the Pashupatinath Temple, where Hindus from all over the world come for pilgrimage. According to Hindu mythology, the goddess Sita of the epic Ramayana, was born in the Mithila Kingdom of King Janaka Raja.		Lumbini is a Buddhist pilgrimage site and UNESCO World Heritage Site in the Kapilavastu district. Traditionally it is held to be the birthplace in about 563 B.C. of Siddhartha Gautama, a Kshatriya caste prince of the Sakya clan, who as the Buddha Gautama, founded Buddhism.		The holy site of Lumbini is bordered by a large monastic zone, in which only monasteries can be built. All three main branches of Buddhism exist in Nepal and the Newa people have their own branch of the faith.[170] Buddhism is also the dominant religion of the thinly populated northern areas, which are mostly inhabited by Tibetan-related peoples, such as the Sherpa.		The Buddha, born as a Hindu, is also said to be a descendant of Vedic Sage Angirasa in many Buddhist texts.[171] The Buddha's family surname is associated with Gautama Maharishi.[172] Differences between Hindus and Buddhists have been minimal in Nepal due to the cultural and historical intermingling of Hindu and Buddhist beliefs. Moreover, traditionally Buddhism and Hinduism were never two distinct religions in the western sense of the word. In Nepal, the faiths share common temples and worship common deities. Among other natives of Nepal, those more influenced by Hinduism were the Magar, Sunwar, Limbu and Rai and the Gurkhas.[36] Hindu influence is less prominent among the Gurung, Bhutia, and Thakali groups who employ Buddhist monks for their religious ceremonies.[36] Most of the festivals in Nepal are Hindu.[173] The Machendrajatra festival, dedicated to Hindu Shaiva Siddha, is celebrated by many Buddhists in Nepal as a main festival.[174] As it is believed that Ne Muni established Nepal,[175] some important priests in Nepal are called "Tirthaguru Nemuni". Islam is a minority religion in Nepal, with 4.2% of the population being Muslim according to a 2006 Nepali census.[176] Mundhum, Christianity and Jainism are other minority faiths.[177]		Folklore is an integral part of Nepali society. Traditional stories are rooted in the reality of day-to-day life, tales of love, affection and battles as well as demons and ghosts and thus reflect local lifestyles, culture, and beliefs. Many Nepali folktales are enacted through the medium of dance and music.		Most houses in the rural lowlands of Nepal are made up of a tight bamboo framework and walls of a mud and cow-dung mix. These dwellings remain cool in summer and retain warmth in winter. Houses in the hills are usually made of unbaked bricks with thatch or tile roofing. At high elevations construction changes to stone masonry and slate may be used on roofs.		Nepal's flag is the only national flag in the world that is not rectangular in shape.[178] The constitution of Nepal contains instructions for a geometric construction of the flag.[179] According to its official description, the red in the flag stands for victory in war or courage, and is also the colour of the rhododendron, the national flower of Nepal. Red also stands for aggression. The flag's blue border signifies peace. The curved moon on the flag is a symbol of the peaceful and calm nature of Nepali, while the sun represents the aggressiveness of Nepali warriors.		With 36 days a year, Nepal is the country that enjoys the most number of public holidays in the world.[180] The Nepali year begins in 1st of Baisakh in official Hindu Calendar of the country, the Bikram Sambat, which falls in mid-April and is divided into 12 months. Saturday is the official weekly holiday. Main annual holidays include the Martyr's Day (18 February), and a mix of Hindu and Buddhist festivals such as Dashain in autumn, Tihar in mid-autumn and Chhath in late autumn. During Swanti, the Newars perform the Mha Puja ceremony to celebrate New Year's Day of the lunar calendar Nepal Sambat. Being a Secular country Nepal has holiday on main festivals of minority religions in the nation too.[173]		The national cuisine of Nepal is Dhindo and Gundruk.The staple Nepali meal is dal bhat. Dal is a lentil soup, and is served over bhat (boiled rice), with tarkari (curried vegetables) together with achar (pickles) or chutni (spicy condiment made from fresh ingredients). It consists of non-vegetarian as well as vegetarian items. Mustard oil is a common cooking medium and a host of spices, including cumin, coriander, black pepper, sesame seeds, turmeric, garlic, ginger, methi (fenugreek), bay leaves, cloves, cinnamon, chilies and mustard seeds are used in cooking. Momo is a type of steamed dumpling with meat or vegetable fillings, and is a popular fast food in many regions of Nepal.		Association football is the most popular sport in Nepal[181] and was first played during the Rana dynasty in 1921.[182] The one and only international stadium in the country is the Dasarath Rangasala Stadium where the national team plays its home matches.[183]		Cricket has been gaining popularity since the last decade. Since the establishment of the national team, Nepal has played its home matches on the Tribhuvan University International Cricket Ground.[184] The national team has since won the 2012 ICC World Cricket League Division Four and the 2013 ICC World Cricket League Division Three[185] simultaneously, hence qualifying for 2014 Cricket World Cup Qualifier. They also qualified for the 2014 ICC World Twenty20 in Bangladesh,[186] and this qualification has been the farthest the team have ever made in an ICC event. On 28 June 2014, the ICC awarded T20I status to Nepal, who took part and performed exceptionally well in the 2014 ICC World Twenty20.[187][188] Nepal had already played three T20I matches before gaining the status, as ICC had earlier announced that all matches at the 2014 ICC World Twenty20 would have T20I status.[189] Nepal won the 2014 ICC World Cricket League Division Three held in Malaysia and qualified for the 2015 ICC World Cricket League Division Two.[190]		Nepal finished fourth in the 2015 ICC World Cricket League Division Two in Namibia[191] and qualified for the 2015–17 ICC World Cricket League Championship.[192] But Nepal failed to secure promotion to Division One and qualification to 2015–17 ICC Intercontinental Cup after finishing third in the round-robin stage.[193][194] Basanta Regmi became the first bowler to take 100 wickets in the World Cricket League. He achieved this feat after taking 2 wickets against Netherlands in the tournament.[195]		Although the country has adopted the metric system as its official standard since 1968,[196] traditional units of measurement are still commonplace. The customary units of area employed in the Terai region – such as katha, bigha, etc. – sound similar to those used elsewhere in South Asia. However, they vary markedly in size, as they seem to have been standardised to different measures of area. For instance, a katha in Nepal is arbitrarily set at 338.63 m², while a katha in Bangladesh means about 67 m² of land area. In addition to native ones, imperial units pertaining to length (specifically inch and foot) and metric units such as kilogram and litre are also fairly common in everyday trade and commerce.		Some notable books and films set against the backdrop of Nepal include:		Holi festival celebrations in Nepal		Pashupatinath Temple		Traditional Pahadi folk dress		Nepal cricket team		Musicians playing devotional songs		One of the Rani palace of Nepal		Urban Newari cuisine		Nepali Momos		Coordinates: 28°10′N 84°15′E﻿ / ﻿28.167°N 84.250°E﻿ / 28.167; 84.250		
Western Australia (abbreviated as WA[a]) is a state occupying the entire western third of Australia. It is bounded by the Indian Ocean to the north and west, the Great Australian Bight and Southern Ocean to the south,[b] the Northern Territory to the north-east and South Australia to the south-east. Western Australia is Australia's largest state with a total land area of 2,529,875 square kilometres (976,790 sq mi), and the second-largest country subdivision in the world, surpassed only by Russia's Sakha Republic – however, a significant part of it is sparsely populated. The state has about 2.6 million inhabitants, around 11% of the national total. Ninety-two per cent of the population lives in the south-west corner of the state.[3]		The first European visitor to Western Australia was the Dutch explorer Dirk Hartog, who visited the Western Australian coast in 1616. The first European settlement of Western Australia occurred following the landing by Major Edmund Lockyer on 26 December 1826 of an expedition on behalf of the New South Wales colonial government.[4] He established a convict-supported military garrison at King George III Sound, at present-day Albany, and on 21 January 1827[4] formally took possession of the western third of the continent for the British Crown. This was followed by the establishment of the Swan River Colony in 1829, including the site of the present-day capital, Perth.		York was the first inland settlement in Western Australia. Situated 97 kilometres east of Perth, it was settled on 16 September 1831.[5]		Western Australia achieved responsible government in 1890, and federated with the other British colonies in Australia in 1901. Today its economy mainly relies on mining, agriculture and tourism. The state produces 46% of Australia's exports.[6] Western Australia is the second-largest iron ore producer in the world.[7]						Western Australia is bounded to the east by longitude 129°E, the meridian 129 degrees east of Greenwich, which defines the border with South Australia and the Northern Territory, and bounded by the Indian Ocean to the west and north. The International Hydrographic Organization (IHO) designates the body of water south of the continent as part of the Indian Ocean; in Australia it is officially gazetted as the Southern Ocean.[b][8]		The total length of the state's eastern border is 1,862 km (1,157 mi).[9] There are 20,781 km (12,913 mi) of coastline, including 7,892 km (4,904 mi) of island coastline.[10] The total land area occupied by the state is 2.5 million km2.[11]		The bulk of Western Australia consists of the extremely old Yilgarn craton and Pilbara craton which merged with the Deccan Plateau of India, Madagascar and the Karoo and Zimbabwe cratons of Southern Africa, in the Archean Eon to form Ur, one of the oldest supercontinents on Earth (3 – 3.2  billion years ago). In May 2017, evidence of the earliest known life on land may have been found in 3.48-billion-year-old geyserite and other related mineral deposits (often found around hot springs and geysers) uncovered in the Pilbara craton.[12][13]		Because the only mountain-building since then has been of the Stirling Range with the rifting from Antarctica, the land is extremely eroded and ancient, with no part of the state above 1,245 metres (4,085 ft) AHD (at Mount Meharry in the Hamersley Range of the Pilbara region). Most of the state is a low plateau with an average elevation of about 400 metres (1,200 ft), very low relief, and no surface runoff. This descends relatively sharply to the coastal plains, in some cases forming a sharp escarpment (as with the Darling Range/Darling Scarp near Perth).		The extreme age of the landscape has meant that the soils are remarkably infertile and frequently laterised. Even soils derived from granitic bedrock contain an order of magnitude less available phosphorus and only half as much nitrogen as soils in comparable climates in other continents. Soils derived from extensive sandplains or ironstone are even less fertile, nearly devoid of soluble phosphate and deficient in zinc, copper, molybdenum and sometimes potassium and calcium.		The infertility of most of the soils has required heavy application by farmers of chemical fertilisers, particularly superphosphate, insecticides and herbicides. These have resulted in damage to invertebrate and bacterial populations. The grazing and use of hoofed mammals and, later, heavy machinery through the years have resulted in compaction of soils and great damage to the fragile soils.		Large-scale land clearing for agriculture has damaged habitats for native flora and fauna. As a result, the South West region of the state has a higher concentration of rare, threatened or endangered flora and fauna than many areas of Australia, making it one of the world's biodiversity "hot spots". Large areas of the state's wheatbelt region have problems with dryland salinity and the loss of fresh water.		The southwest coastal area has a Mediterranean climate. It was originally heavily forested, including large stands of karri, one of the tallest trees in the world.[14] This agricultural region is one of the nine most bio-diverse terrestrial habitats, with a higher proportion of endemic species than most other equivalent regions. Thanks to the offshore Leeuwin Current, the area is one of the top six regions for marine biodiversity and contains the most southerly coral reefs in the world.		Average annual rainfall varies from 300 millimetres (12 in) at the edge of the Wheatbelt region to 1,400 millimetres (55 in) in the wettest areas near Northcliffe, but from November to March, evaporation exceeds rainfall, and it is generally very dry. Plants are adapted to this as well as the extreme poverty of all soils.		The central two-thirds of the state is arid and sparsely inhabited. The only significant economic activity is mining. Annual rainfall averages less than 300 millimetres (8–10 in), most of which occurs in sporadic torrential falls related to cyclone events in summer.[15]		An exception to this is the northern tropical regions. The Kimberley has an extremely hot monsoonal climate with average annual rainfall ranging from 500 to 1,500 millimetres (20–60 in), but there is a very long almost rainless season from April to November. Eighty-five percent of the state's runoff occurs in the Kimberley, but because it occurs in violent floods and because of the insurmountable poverty of the generally shallow soils, the only development has taken place along the Ord River.		Snow is rare in the state and typically occurs only in the Stirling Range near Albany, as it is the only mountain range far enough south and sufficiently elevated. More rarely, snow can fall on the nearby Porongurup Range. Snow outside these areas is a major event; it usually occurs in hilly areas of southwestern Australia. The most widespread low-level snow occurred on 26 June 1956 when snow was reported in the Perth Hills, as far north as Wongan Hills and as far east as Salmon Gums. However, even in the Stirling Range, snowfalls rarely exceed 5 cm (2 in) and rarely settle for more than one day.[16]		The highest observed maximum temperature of 50.5 °C (122.9 °F) was recorded at Mardie Station on 19 February 1998. The lowest minimum temperature recorded was −7.2 °C (19.0 °F) at Eyre Bird Observatory on 17 August 2008.[17]		Western Australia is home to around 540 species of birds (depending on the taxonomy used). Of these around 15 are endemic to the state. The best areas for birds are the southwestern corner of the state and the area around Broome and the Kimberley.		The Flora of Western Australia comprises 10,162 published native vascular plant species, along with a further 1,196 species currently recognised but unpublished. They occur within 1,543 genera from 211 families; there are also 1,276 naturalised alien or invasive plant species, more commonly known as weeds.[19][20] In the southwest region are some of the largest numbers of plant species for its area in the world.		Specific ecoregions of Western Australia include: the sandstone gorges of The Kimberley on the northern coast and below that areas of dry grassland (Ord Victoria Plain) or semi-desert (Western Australian Mulga shrublands), with Tanami Desert inland from there. Following the coast south there is the Southwest Australia savanna and the Swan Coastal Plain around Perth, and then farther south the Warren on the southwest corner of the coast around the wine-growing area of Margaret River.		Going east along the Southern Ocean coast is the Goldfields-Esperance region, including the Esperance grasslands and the Coolgardie grasslands inland around town of Coolgardie.		The first inhabitants of Australia arrived from the north about 40,000 to 60,000 years ago. Over thousands of years they eventually spread across the whole landmass. These Indigenous Australians were long established throughout Western Australia by the time European explorers began to arrive in the early 17th century.		The first European to visit Western Australia was a Dutch explorer, Dirk Hartog, who on 25 October 1616 landed at what is now known as Cape Inscription, Dirk Hartog Island. For the rest of the 17th century, other Dutch and British navigators encountered the coast, usually unintentionally, as demonstrated by the many shipwrecks along the coast of ships that deviated from the Brouwer Route (because of poor navigation and storms).[21] Two hundred years passed before Europeans believed that the great southern continent actually existed. By the late 18th century, British and French sailors had begun to explore the Western Australian coast.		The origins of the present state began with the establishment by Lockyer[4] of a convict-supported settlement from New South Wales at King George III Sound. The settlement was formally annexed on 21 January 1827 by Lockyer when he commanded the Union Jack be raised and a feu de joie fired by the troops. The settlement was founded in response to British concerns about the possibility of a French colony being established on the coast of Western Australia.[4] On 7 March 1831 it was transferred to the control of the Swan River Colony,[5] and named Albany in 1832.		In 1829 the Swan River Colony was established on the Swan River by Captain James Stirling. By 1832, the British settler population of the colony had reached around 1,500, and the official name of the colony was changed to Western Australia. The two separate townsites of the colony developed slowly into the port city of Fremantle and the state's capital, Perth. York was the first inland settlement in Western Australia, situated 97 kilometres east of Perth and settled on 16 September 1831. York was the staging point for early explorers who discovered the rich gold reserves of Kalgoorlie.		Population growth was very slow until significant discoveries of gold were made in the 1890s around Kalgoorlie.		In 1887, a new constitution was drafted, providing for the right of self-governance of European Australians and in 1890, the act granting self-government to the colony was passed by the British Parliament. John Forrest became the first Premier of Western Australia.		In 1896, the Western Australian Parliament authorised the raising of a loan to construct a pipeline to transport 23 megalitres (5 million imperial gallons) of water per day to the Goldfields of Western Australia. The pipeline, known as the Goldfields Water Supply Scheme, was completed in 1903. C.Y. O'Connor, Western Australia's first engineer-in-chief, designed and oversaw the construction of the pipeline. It carries water 530 km (330 mi) from Perth to Kalgoorlie, and is attributed by historians as an important factor driving the state's population and economic growth.[22]		Following a campaign led by Forrest, residents of the colony of Western Australia (still informally called the Swan River Colony) voted in favour of federation, resulting in Western Australia officially becoming a state on 1 January 1901.		Europeans began to settle permanently in 1826 when Albany was claimed by Britain to forestall French claims to the western third of the continent. Perth was founded as the Swan River Colony in 1829 by British and Irish settlers, though the outpost languished. Its officials eventually requested convict labour to augment its population. In the 1890s, interstate immigration, resulting from a mining boom in the Goldfields region, resulted in a sharp population increase.		Western Australia did not receive significant flows of immigrants from Britain, Ireland or elsewhere in the British Empire until the early 20th century. At that time, its local projects—such as the Group Settlement Scheme of the 1920s, which encouraged farmers to settle the southwest—increased awareness of Australia's western third as a destination for colonists.		Led by immigrants from the British Isles, Western Australia's population developed at a faster rate during the twentieth century than it had previously. After World War II, both the eastern states and Western Australia received large numbers of Italians, Croatians and Macedonians. Despite this, Britain has contributed the greatest number of immigrants to this day. Western Australia—particularly Perth—has the highest proportion of British-born of any state: 10.3% in 2011, compared to a national average of 5.1%. This group is heavily concentrated in certain parts, where they account for a quarter of the population.[23]		In terms of ethnicity, the 2001 census data revealed that 77.5% of Western Australia's population was of European descent: the largest single group was those reporting English ethnicity, accounting for 733,783 responses (32.7%), followed by Australian with 624,259 (27.8%), Irish with 171,667 (7.6%), Italian with 96,721 (4.3%), Scottish with 62,781 (2.8%), German with 51,672 (2.3%), and Chinese with 48,894 responses (2.2%). There were 58,496 Indigenous Australians in Western Australia in 2001, forming 3.1% of the population.		According to the 2011 census data, the most common ancestries in Western Australia were English 29.0% (848,230), Australian 24.8% (724,360), Irish 6.4% (187,038), Scottish 6.4% (186,475) and Italian 3.8% (111,894).[24] There were 69,664 (3.1%) Aboriginal and Torres Strait Islander Australians living in Western Australia in the 2011 census.[24]		In terms of birthplace, in the 2011 census 33.2% of the population were born overseas – the highest proportion of any state or territory. People born in the United Kingdom (230,410), New Zealand (70,736) and South Africa (35,326) were the largest groups of immigrants, accounting for 45% of the state's overseas-born population.[23]		Perth's metropolitan area (including Mandurah) had an estimated population of 1.729 million in 2011 (77% of the state).[25] Other significant population centres include Bunbury (64,385),[26] Geraldton (31,349),[27] Kalgoorlie-Boulder (30,841),[28] Albany (26,643),[29] Karratha (16,475),[30] Broome (12,766)[31] and Port Hedland (13,772).[32]		Western Australia's economy is largely driven by extraction and processing of a diverse range of mineral and petroleum commodities. The structure of the economy is closely linked to these natural resources, providing a comparative advantage in resource extraction and processing. As a consequence:		Western Australia's overseas exports accounted for 46% of the nation's total.[6][36] The state's major export commodities include iron-ore, alumina, nickel, gold, ammonia, wheat, wool, crude oil and liquefied natural gas (LNG).		Western Australia is a major extractor of bauxite, which is also processed into alumina at four refineries providing more than 20% of total world production. It is the world's third-largest iron-ore producer (15% of the world's total) and extracts 75% of Australia's 240 tonnes of gold. Diamonds are extracted at Argyle diamond mine in far north of the Kimberley region. Coal mined at Collie is the main fuel for baseload electricity generation in the state's south-west.		Agricultural production in WA is a major contributor to the state and national economy. Although tending to be highly seasonal, 2006–07 wheat production in WA was nearly 10 million tonnes, accounting for almost half the nation's total.[37] and providing $1.7 billion in export income.[38]		Other significant farm output includes barley, peas,[37] wool, lamb and beef. There is a high level of overseas demand for live animals from WA, driven mainly by southeast Asia's feedlots and Middle Eastern countries, where cultural and religious traditions and a lack of storage and refrigeration facilities favour live animals over imports of processed meat. About half of Australia's live cattle exports come from Western Australia.[39]		Resource sector growth in recent years has resulted in significant labour and skills shortages, leading to recent efforts by the state government to encourage interstate and overseas immigration.[40] According to the 2006 census,[41] the median individual income was A$500 per week in Western Australia (compared to A$466 in Australia as a whole). The median family income was A$1246 per week (compared to A$1171 for Australia). Recent growth has also contributed to significant rises in average property values in 2006, although values plateaued in 2007. Perth property prices are still the second highest in Australia behind Sydney, and high rental prices continue to be a problem.		Located south of Perth, the heavy industrial area of Kwinana has the nation's largest oil refinery with a capacity of 146,000 barrels of oil per day, producing most of the state's petrol and diesel.[42][43][44] Kwinana also hosts alumina and nickel processing plants, port facilities for grain and other bulk exports, and support industries for mining and petroleum such as heavy and light engineering, and metal fabrication. Shipbuilding (e.g. Austal Ships) and associated support industries are found at nearby Henderson, just north of Kwinana. Significant secondary industries include cement and building product manufacturing, flour milling, food processing, animal feed production, automotive body building and printing.		In recent years, tourism has grown in importance, with significant numbers of visitors to the state coming from the UK and Ireland (28%), other European countries (14%) Singapore (16%), Japan (10%) and Malaysia (8%).[38] Revenue from tourism is a strong economic driver in many of the smaller population centres outside of Perth, especially in coastal locations.		Western Australia has a significant fishing industry. Products for local consumption and export include western rock lobsters, prawns, crabs, shark and tuna, as well as pearl fishing in the Kimberley region of the state. Processing is conducted along the west coast. Whaling was a key marine industry but ceased at Albany in 1978.		Tourism forms a major part of the Western Australian economy with 833,100 international visitors making up 12.8% of the total international tourism to Australia in the year ending March 2015. The top three source markets include the United Kingdom (17%), Singapore (10%) and New Zealand (10%) with the majority of purpose for visitation being holiday/vacation reasons.[45] The tourism industry contributes $9.3 billion to the Western Australian economy and supports 94,000 jobs within the state. Both directly and indirectly, the industry makes up 3.2% of the state's economy whilst comparatively, WA's largest revenue source, the mining sector, brings in 31%.[46]		Tourism WA is the government agency responsible for promoting Western Australia as a holiday destination.[47]		Western Australia was granted self-government in 1889 with a bicameral Parliament located in Perth, consisting of the Legislative Assembly (or lower house), which has 59 members; and the Legislative Council (or upper house), which has 36 members. Suffrage is universal and compulsory for citizens over 18 years of age.		With the federation of the Australian colonies in 1901, Western Australia became a state within Australia's federal structure; this involved ceding certain powers to the Commonwealth (or Federal) government in accordance with the Constitution; all powers not specifically granted to the Commonwealth remained solely with the State, however over time the Commonwealth has effectively expanded its powers through increasing control of taxation and financial distribution.		Whilst the sovereign of Western Australia is the Queen of Australia (Elizabeth II), and executive power nominally vested in her State representative the Governor (currently Kerry Sanderson), executive power rests with the premier and ministers drawn from the party or coalition of parties holding a majority of seats in the Legislative Assembly. Mark McGowan is the Premier, having defeated Colin Barnett at the state election on 11 March 2017.		Secessionism has been a recurring feature of Western Australia's political landscape since shortly after European settlement in 1826. Western Australia was the most reluctant participant in the Commonwealth of Australia.[48] Western Australia did not participate in the earliest federation conference. Longer-term residents of Western Australia were generally opposed to federation; however, the discovery of gold brought many immigrants from other parts of Australia. It was these residents, primarily in Kalgoorlie but also in Albany who voted to join the Commonwealth, and the proposal of these areas being admitted separately under the name Auralia was considered.[citation needed]		In a referendum in April 1933, 68% of voters voted for the state to leave the Commonwealth of Australia with the aim of returning to the British Empire as an autonomous territory. The State Government sent a delegation to Westminster, but the British Government refused to intervene and therefore no action was taken to implement this decision.[49]		Western Australia is divided into 139 Local Government Areas, including Christmas Island and the Cocos (Keeling) Islands. Their mandate and operations are governed by the Local Government Act 1995.[50]		Education in Western Australia consists of one year of pre-school at age 4 or 5, followed by six years of primary education for all students as of 2015.[51] At age 12 or 13, students begin six years of secondary education. The final two years of secondary education are now compulsory.[52] From 2005, all students who completed Year 10 were required to undertake further studies in Year 11, and to complete the year in which they turned 16 (usually Year 11). Since 2008, all students are required to complete 12 years of study before leaving school.[52] Students have the option to study at a TAFE college in their eleventh year or continue through high school with a vocational course or a specific university entrance course.		There are five universities in Western Australia. They consist of four Perth-based public universities; the University of Western Australia, Curtin University, Edith Cowan University and Murdoch University; and one Fremantle-based private Roman Catholic university, the University of Notre Dame. The University of Notre Dame is also one of only two private universities in Australia, along with Bond University, a not-for-profit private education provider based in Gold Coast, Queensland.		Western Australia has two daily newspapers: the independent tabloid The West Australian and The Kalgoorlie Miner. Also published is one weekend paper The Weekend West and one Sunday tabloid newspaper, News Corporation's The Sunday Times. There are also 17 weekly Community Newspapers with distribution from Yanchep in the North to Mandurah in the South. There are two major weekly rural papers in the state, Countryman the Rural Press-owned Farm Weekly. The interstate broadsheet publication The Australian is also available, although with sales per capita lagging far behind those in other states. With the advent of the Internet, local news websites like WAtoday, which provide free access to their content, are becoming a popular alternative source of news. Other online publications from around the world like the New South Wales based The Sydney Morning Herald and The Australian are also available.		Metropolitan Perth has six broadcast television stations;		Regional WA has a similar availability of stations, with the exception of West TV. Geographically, it is one of the largest television markets in the world, including almost one-third of the continent.		In addition, broadcasters operate digital multichannels:		Pay TV services are provided by Foxtel, which acquired many of the assets and all the remaining subscribers of the insolvent Galaxy Television satellite service in 1998. Some metropolitan suburbs are serviced by Pay TV via cable; however, most of the metropolitan and rural areas can only access Pay TV via satellite.		Perth has many radio stations on both AM and FM frequencies. ABC stations include ABC NewsRadio (6PB 585 am), 720 ABC Perth (6WF 720 am), ABC Radio National (6RN 810 am), ABC Classic FM (6ABC 97.7FM) and Triple J (6JJJ 99.3FM). The six commercial stations are: FM 92.9 (6PPM), Nova 93.7 (6PER), Mix 94.5 (6MIX), 96fm (6NOW), and AM 882 (6PR), AM 1080 (6IX) and AM 1116 (6MM)		The leading community radio stations are Curtin FM 100.1, 6RTR FM 92.1, Sonshine FM 98.5 (6SON) and 91.3 SportFM (6WSM).		Winemaking regions are concentrated in the cooler climate of the south-western portion of the state. Western Australia produces less than 5% of the country's wine output, but in quality terms is considered to be very much near the top.[53][54][55][56] Major wine producing regions include: Margaret River, The Great Southern, Swan Valley as well as smaller districts including Blackwood Valley, Manjimup, Pemberton, Peel, Chittering Valley, Perth Hills, and Geographe.[57]		A number of national or international sporting teams and events are based in the state, including:		International sporting events hosted in the past in Western Australia include the Tom Hoad Cup (water polo), the Perth International (golf), the 2006 Gravity Games (extreme sports), the 2002 Women's Hockey World Cup, the 1991 FINA World Aquatics Championships and the 1962 British Empire and Commonwealth Games.		Western Australia is home to one of the country's leading performance training institutions, the acclaimed Western Australian Academy of Performing Arts (WAAPA), as well as a burgeoning theatrical and musical scene. Notable musicians and bands to have been born in or lived in Western Australia include Adam Brand, Karnivool, Birds of Tokyo, Bon Scott, Eskimo Joe, Johnny Young, Gyroscope, the John Butler Trio, Tame Impala, Kevin Mitchell, Tim Minchin, The Kill Devil Hills, Pendulum, The Pigram Brothers, Rolf Harris and The Triffids. The West Australian Music Industry Awards (WAMis) have been awarded every year to the leading musicians and performers in WA since 2001.		Notable actors and television personalities from Western Australia include Heath Ledger, Sam Worthington, Ernie Dingo, Jessica Marais, Megan Gale, Rove McManus, Isla Fisher, and Melissa George. Films and television series filmed or partly filmed in Western Australia include These Final Hours, Cloudstreet, Australia, Bran Nu Dae, ABBA: the Movie and Last Train to Freo.		Noted Western Australian indigenous painters and artisans include Jack Dale Mengenen, Paddy Bedford, Queenie McKenzie, and Rover Thomas.[58]		The West Australian Symphony Orchestra (WASO) is based at the Perth Concert Hall. Other concert, performance and indoor sporting venues in Western Australia include His Majesty's Theatre, the now demolished Perth Entertainment Centre, the Burswood Dome and Theatre and the Perth Arena, which opened in 2012.		In 1981, a sister state agreement was drawn up between Western Australia and Hyogo Prefecture in Japan that was aimed at improving cultural ties between the two states.[59][60] To commemorate the 10th anniversary of this agreement, the Hyogo Prefectural Government Cultural Centre was established in Perth in 1992.[61] Prior to that, the Western Australian government opened an office in Kobe, the largest city in Hyogo, to facilitate maintenance of the relationship in 1989.[60][62]		Following the Great Hanshin earthquake that devastated southern Hyogo in January 1995, Western Australian groups and businesses raised funds and provided materials, whilst individuals travelled to Hyogo to help with emergency relief and the subsequent reconstruction process.[63][64][65] The two governments signed a memorandum of understanding on the 20th anniversary in 2001 that aimed to improve the economic relationship between the two states.[60]		Further to the sister state relationship, the City of Rockingham in Western Australia and the City of Ako in Hyogo signed a sister city agreement in 1997. It is one of nine sister city relationships between Western Australian and Japanese cities.[66]		Lists:		a "West Australia" and its related demonym "West Australian" are occasionally used, including in the names of the main daily newspaper, The West Australian, and the state-based West Australian Football League, but are rarely used in an official sense. The terms "Westralia" and "Westralian" were regularly used in the 19th and 20th century.[67][68] The terms are still found in the names of certain companies and buildings, e.g. Westralia House in Perth and Westralia Airports Corporation, which operates Perth Airport, as well as in the names of several ships.[69][70] b In Australia, the body of water south of the continent is officially gazetted as the Southern Ocean, whereas the International Hydrographic Organization (IHO) designates it as part of the Indian Ocean.[71][72]		
A sixth form college is an educational institution in England, Wales, Northern Ireland, Belize, the Caribbean, Malta, Norway, Brunei, and Malaysia, among others, where students aged 16 to 19 typically study for advanced school-level qualifications, such as A-levels, BTEC and the International Baccalaureate Diploma, or school-level qualifications such as GCSEs. In Singapore and India, this is known as a junior college. The municipal government of the city of Paris uses the phrase "sixth form college" as the English name for a lycée.[1]		In England and the Caribbean, education is compulsory until the end of year 13, the school year in which the pupil turns 18 (although education was only compulsory until year 11 before August 2013 and until year 12 between August 2013 and 2015).[2] In the English state educational system, pupils may either stay at a secondary school with an attached sixth form, transfer to a local sixth form college, or go to a more vocational further education college, although in some places there may in practice be little choice which of these options can be taken. In the independent sector, sixth forms are an integral part of secondary schools (public schools), and there is also a number of smaller-scale independent sixth form colleges. In Wales, education is only compulsory until the end of year 11.		Students at sixth form college typically study for two years (known as Years 12 and 13, Years 13 and 14 in Northern Ireland and/or lower sixth and upper sixth). Some students sit AS examinations at the end of the first year, and A-level examinations at the end of the second. These exams are called C.A.P.E. (Caribbean Advanced Proficiency Examination) in the Caribbean. In addition, in recent years a variety of vocational courses have been added to the curriculum.		There are currently over 90 sixth form colleges in England and Wales. Most perform extremely well in national examination league tables. In addition, they offer a broader range of courses at a lower cost per student than most school sixth forms. In a few areas, authorities run sixth form schools which function like sixth form colleges but are completely under the control of the local education authorities. Unlike further education colleges, sixth form colleges rarely accept part-time students or run evening classes,[citation needed] although one boarding sixth form college exists.						There are a few schools in Brunei providing sixth form education. Five of them are dedicated sixth form colleges, with four located in Brunei-Muara District and one in Tutong District. Belait has yet to have its own sixth form centre and sixth form education is presently housed in Sayyidina Ali Secondary School, sharing facilities with the secondary education. There is no sixth form education in Temburong — prospective students go to sixth form colleges in Brunei-Muara where they may stay in dormitories.		Almost all sixth form schools are government schools. Five of them provide education leading up to Brunei-Cambridge GCE A Level qualification. Jerudong International School is the only non-government school which has sixth form education and its A Level is independent of those offered by its counterpart.		Another school, Hassanil Bolkiah Boys' Arabic Secondary School, is a government sixth form centre for students in the specialised Arabic stream. Instead of A Level subjects, students generally learn subjects pertaining to Islamic knowledge in Arabic medium. The schooling culminates in the sitting of Sijil Tinggi Pelajaran Ugama Brunei (STPUB), translatable as the Higher Certificate of Brunei Religious Education. They may then proceed to Islamic universities, locally or abroad such as Al-Azhar University.		In the English-speaking Caribbean, there are many sixth form colleges, usually attached to secondary schools. Students must usually attain a grade A-C in 1-3 in the Caribbean Examinations Council (C.X.C), examinations. Students that fail these exams are not accepted into the sixth form program and either can do courses in other tertiary facilities, or begin working with high school degrees. After sixth form, students are presented with an Associate Degree.		Scotland does not, in general, have separate sixth form colleges (or, indeed, the same concept of the terminal two years of secondary education as being distinct from the other time spent there); as such, Scottish students who opt to remain in full-time education will typically remain in the same school for fifth and sixth year (the equivalent to the English lower- and upper-sixth forms), studying Higher Grade and Advanced Higher qualifications. Higher Grade qualifications can be taken in both the fifth and sixth years.		The first comprehensive intake sixth form college in England was established in Mexborough, South Yorkshire, and took its first intake of students in September 1964. Since then sixth form colleges have spread across the UK and have proved popular with students, their parents, and other groups in the community. Until 1992, these colleges were controlled and funded by local education authorities (LEAs), but the Further and Higher Education Act 1992 transferred all institutions within the sector to the Further Education Funding Council for England (FEFC), a national agency with strategic responsibility for the operation of general further education (FE) colleges. Later the FEFC's functions were taken over by the Learning and Skills Council (LSC), a reorganisation that included changes in the funding and supervision of sixth form colleges.		Sixth form colleges take responsibility for their own employment, pensions and pay arrangements with the support and advice of the Sixth Form Colleges' Association (SFCA, formerly SFCF). The SFCA is made up of representative principals from SFCs across the UK. The SFCA sets up several Committees to deliver its range of support services for SFCs as well as facilitating lobbying work with Central Government. Colleges for the most part do not charge full-time daytime students; however, adult students (most of whom attend evening classes) may have to pay a fee (for examinations, tutors' time and other costs).		There are also some sixth form colleges in the independent sector, specialising in A levels for which fees are paid; these are unconnected with the SFCA.		In Wales, sixth form education falls under the remit of the Welsh Assembly, and sixth form colleges are sources of further education alongside FE Colleges and sixth forms integrated into secondary schools. They typically offer the Welsh Baccalaureate and Key Skills qualifications.		Even though Hong Kong had changed from having two public examinations to one, its first sixth form college, PLK Vicwood KT Chong Sixth Form College, remains in operation as an upper secondary college.		
South Australia (abbreviated as SA) is a state in the southern central part of Australia. It covers some of the most arid parts of the country. With a total land area of 983,482 square kilometres (379,725 sq mi), it is the fourth-largest of Australia's states and territories. It has a total of 1.7 million people, and its population is the most highly centralised of any state in Australia, with more than 75 percent of South Australians living in the capital, Adelaide, or its environs. Other population centres in the state are relatively small.		South Australia shares borders with all of the other mainland states, and with the Northern Territory; it is bordered to the west by Western Australia, to the north by the Northern Territory, to the north-east by Queensland, to the east by New South Wales, to the south-east by Victoria, and to the south by the Great Australian Bight.[4] The state comprises less than 8 percent of the Australian population and ranks fifth in population among the six states and two territories. The majority of its people reside in Adelaide. Most of the remainder are settled in fertile areas along the south-eastern coast and River Murray. The state's colonial origins are unique in Australia as a freely settled, planned British province,[5] rather than as a convict settlement. Official settlement began on 28 December 1836, when the colony was proclaimed at the Old Gum Tree by Governor John Hindmarsh.		As with the rest of the continent, the region had been long occupied by Aboriginal peoples, who were organised into numerous tribes and languages. The first British settlement to be established was Kingscote, Kangaroo Island, on 26 July 1836, five months before Adelaide was founded.[6] The guiding principle behind settlement was that of systematic colonisation, a theory espoused by Edward Gibbon Wakefield that was later employed by the New Zealand Company.[citation needed] The goal was to establish the province as a centre of civilisation for free immigrants, promising civil liberties and religious tolerance. Although its history is marked by economic hardship, South Australia has remained politically innovative and culturally vibrant. Today, it is known for its fine wine and numerous cultural festivals. The state's economy is dominated by the agricultural, manufacturing and mining industries. The state has an increasingly significant finance sector as well.[citation needed]						Evidence of human activity in South Australia dates back as far as 20,000 years, with flint mining activity and rock art in the Koonalda Cave on the Nullarbor Plain. In addition wooden spears and tools were made in an area now covered in peat bog in the South East. Kangaroo Island was inhabited long before the island was cut off by rising sea levels. [7] The first recorded European sighting of the South Australian coast was in 1627 when the Dutch ship the Gulden Zeepaert, captained by François Thijssen, examined and mapped a section of the coastline as far east as the Nuyts Archipelago. Thijssen named his discovery "Pieter Nuyts Land", after the highest ranking individual on board.[citation needed] The complete coastline of South Australia was first mapped by Matthew Flinders and Nicolas Baudin in 1802. The land which now forms the state of South Australia was claimed for Britain in 1788 as part of the colony of New South Wales. Although the new colony included almost two-thirds of the continent, early settlements were all on the eastern coast and only a few intrepid explorers ventured this far west. It took more than forty years before any serious proposal to establish settlements in the south-western portion of New South Wales were put forward. In 1834, the British Parliament passed the South Australia Act 1834 (Foundation Act), which enabled the province of South Australia to be established. The act stated that 802,511 square kilometres (309,851 sq mi) would be allotted to the colony and it would be convict-free. In contrast to the rest of Australia, terra nullius did not apply to the new province. The Letters of Patent attached to the Act acknowledged Aboriginal ownership and stated that no actions could be undertaken that would affect the rights of any Aboriginal natives of the said province to the actual occupation and enjoyment in their own persons or in the persons of their descendants of any land therein now actually occupied or enjoyed by such natives. Although the patent guaranteed land rights under force of law for the indigenous inhabitants it was ignored by the South Australian Company authorities and squatters.[8]		Settlement of seven vessels and 636 people was temporarily made at Kingscote on Kangaroo Island, until the official site of the colony was selected where Adelaide is currently located. The first immigrants arrived at Holdfast Bay (near the present day Glenelg) in November 1836, and the colony was proclaimed on 28 December 1836, now known as Proclamation Day. South Australia is the only Australian state to be settled entirely under a program of free settlement, although some emancipated or escaped convicts or expirees did make their way there, both prior to 1836, or later, and may have constituted 1-2% of the early population.[9]		The plan for the colony was that it would be the ideal embodiment of the best qualities of British society, that is, no religious discrimination or unemployment and, as it was believed that this would also result in very little crime within the small cohort of initial settlers, no professional police were sent. The Colonisation Commissioners intended to establish a police service as soon as misconduct within the increasing population warranted it. In the meantime, temporary volunteer special constables, whose appointment was provided for under English law, would provide law and order. Neither was provision made for a permanent gaol. In early 1838 the colonists became concerned after it was reported that convicts who had escaped from the eastern states may make their way to South Australia. The South Australia Police was formed in April 1838 to protect the community and enforce government regulations. Their principal role was to run the first temporary gaol, a two-room hut.[10]		The current flag of South Australia was adopted on 13 January 1904, and is a British blue ensign defaced with the state badge. The badge is described as a piping shrike with wings outstretched on a yellow disc. The state badge is believed to have been designed by Robert Craig of Adelaide's School of Design.		The terrain consists largely of arid and semi-arid rangelands, with several low mountain ranges. The most important (but not tallest) is the Mount Lofty-Flinders Ranges system, which extends north about 800 kilometres (497 mi) from Cape Jervis to the northern end of Lake Torrens. The highest point in the state is not in those ranges; Mount Woodroffe (1,435 metres (4,708 ft)) is in the Musgrave Ranges in the extreme northwest of the state.[11] The south-western portion of the state consists of the sparsely inhabited Nullarbor Plain, fronted by the cliffs of the Great Australian Bight. Features of the coast include Spencer Gulf and the Eyre and Yorke Peninsulas that surround it.		The principal industries and exports of South Australia are wheat, wine and wool.[citation needed] More than half of Australia's wines are produced in the South Australian wine regions which principally include: Barossa Valley, Clare Valley, McLaren Vale, Coonawarra, the Riverland and the Adelaide Hills. See South Australian wine.		South Australia has boundaries with every other Australian mainland state and territory except the Australian Capital Territory and the Jervis Bay Territory. The Western Australia border has a history involving the South Australian government astronomer, Dodwell, and the Western Australian Government Astronomer, Curlewis, marking the border on the ground in the 1920s.		In 1863, that part of New South Wales to the north of South Australia was annexed to South Australia, by letters patent, as the "Northern Territory of South Australia", which became shortened to the Northern Territory (6 July 1863).[12] The Northern Territory was handed to the federal government in 1911 and became a separate territory.		According to Australian maps, South Australia's south coast is flanked by the Southern Ocean, but official international consensus defines the Southern Ocean as extending north from the pole only to 60°S or 55°S, at least 17 degrees of latitude further south than the most southern point of South Australia. Thus the south coast is officially adjacent to the south-most portion of the Indian Ocean. See Southern Ocean: Existence and definitions		The southern part of the state has a Mediterranean climate, while the rest of the state has either an arid or semi-arid climate.[13] South Australia's main temperature range is 29 °C (84 °F) in January and 15 °C (59 °F) in July. Daily temperatures in parts of the state in January and February can be up to 48 °C (118 °F).		The highest maximum temperature was recorded as 50.7 °C (123.3 °F) at Oodnadatta on 2 January 1960, which is also the highest official temperature recorded in Australia. The lowest minimum temperature was −8.2 °C (17.2 °F) at Yongala on 20 July 1976.[14]		South Australia's average annual employment for 2009–10 was 800,600 persons, 18% higher than for 2000–01.[17] For the corresponding period, national average annual employment rose by 22%.[17]		South Australia's largest employment sector is health care and social assistance,[16][18] surpassing manufacturing in SA as the largest employer since 2006–07.[16][18] In 2009–10, manufacturing in SA had average annual employment of 83,700 persons compared with 103,300 for health care and social assistance.[16] Health care and social assistance represented nearly 13% of the state average annual employment.[17]		The retail trade is the second largest employer in SA (2009–10), with 91,900 jobs, and 12 per cent of the state workforce.[17]		The manufacturing industry plays an important role in South Australia's economy, generating 11.7%[16] of the state's gross state product (GSP) and playing a large part in exports. The manufacturing industry consists of automotive (44% of total Australian production, 2006) and component manufacturing, pharmaceuticals, defence technology (2.1% of GSP, 2002–03) and electronic systems (3.0% of GSP in 2006). South Australia's economy relies on exports more than any other state in Australia.[citation needed][19]		State export earnings stood at A$10 billion per year[when?][citation needed] and grew by 8.8% from 2002 to 2003. Production of South Australian food and drink (including agriculture, horticulture, aquaculture, fisheries and manufacturing) is a $10 billion industry.[when?][citation needed]		South Australia's credit rating was upgraded to AAA by Standard & Poor's Rating Agency in September 2004 and to AAA by Moody's Rating Agency November 2004, the highest credit ratings achievable by any company or sovereign. The State had previously lost these ratings in the State Bank collapse. However, in 2012 Standard & Poor's downgraded the state's credit rating to AA+ due to declining revenues, new spending initiatives and a weaker than expected budgetary outlook.[20]		South Australia's Gross State Product was A$48.9 billion starting 2004, making it A$32,996 per capita. Exports for 2006 were valued at $9.0bn with imports at $6.2bn. Private Residential Building Approvals experienced 80% growth over the year of 2006.[citation needed]		South Australia's economy includes the following major industries: meat and meat preparations, wheat, wine, wool and sheepskins, machinery, metal and metal manufactures, fish and crustaceans, road vehicles and parts, and petroleum products. Other industries, such as education and defence technology, are of growing importance.[when?][citation needed]		South Australia receives the least amount of federal funding for its local road network of all states on a per capita and a per kilometre basis.[21]		In 2013, South Australia was named by Commsec Securities as the second lowest performing economy in Australia.[22] While some sources have pointed at weak retail spending and capital investment, others have attributed poor performance to declines in public spending.[22][23]		The Olympic Dam mine near Roxby Downs in northern South Australia is the largest deposit of uranium in the world, possessing more than a third of the world's low-cost recoverable reserves and 70% of Australia's. The mine, owned and operated by BHP Billiton, presently accounts for 9% of global uranium production.[24][25] The Olympic Dam mine is also the world's fourth-largest remaining copper deposit, and the world's fifth largest gold deposit.[citation needed] There was a proposal to vastly expand the operations of the mine, making it the largest open-cut mine in the world,[26] but in 2012 the BHP Billiton board decided not to go ahead with it at that time due to then lower commodity prices.[27]		Crown land held in right of South Australia is managed under the Crown Land Management Act 2009.		South Australia is a constitutional monarchy with the Queen of Australia as sovereign, and the Governor of South Australia as her representative.[28] It is a state of the Commonwealth of Australia. The bicameral Parliament of South Australia consists of the lower house known as the House of Assembly and the upper house known as the Legislative Council. General elections are held every four years, the last being the 2014 election.		Initially, the Governor of South Australia held almost total power, derived from the letters patent of the imperial government to create the colony. He was accountable only to the British Colonial Office, and thus democracy did not exist in the colony. A new body was created to advise the governor on the administration of South Australia in 1843 called the Legislative Council.[29] It consisted of three representatives of the British Government and four colonists appointed by the governor. The governor retained total executive power.		In 1851, the Imperial Parliament enacted the Australian Colonies Government Act which allowed for the election of representatives to each of the colonial legislatures and the drafting of a constitution to properly create representative and responsible government in South Australia. Later that year, propertied male colonists were allowed to vote for 16 members on a new 24 seat Legislative Council. Eight members continued to be appointed by the governor.		The main responsibility of this body was to draft a constitution for South Australia. The body drafted the most democratic constitution ever seen in the British Empire and provided for universal manhood suffrage.[30] It created the bicameral Parliament of South Australia. For the first time in the colony, the executive was elected by the people and the colony used the Westminster system, where the government is the party or coalition that exerts a majority in the House of Assembly.		Women's suffrage in Australia took a leap forward – enacted in 1895 and taking effect from the 1896 colonial election, South Australia was the first in Australia and only the second in the world after New Zealand to allow women to vote, and the first in the world to allow women to stand for election.[31] In 1897 Catherine Helen Spence was the first woman in Australia to be a candidate for political office when she was nominated to be one of South Australia's delegates to the conventions that drafted the constitution. South Australia became an original state of the Commonwealth of Australia on 1 January 1901.		South Australia is divided into 74 local government areas. Local councils are responsible for functions delegated by the South Australian parliament, such as road infrastructure and waste management. Council revenue comes mostly from property taxes and government grants.		At a 2016 census the population of South Australia was 1.7 million residents.		A majority of the state's population lives within Greater Adelaide's metropolitan area which had an estimated population of 1,262,940 in 2011 (77.1% of the state). Other significant population centres include Mount Gambier (28,313), Whyalla (22,489), Murray Bridge (17,152), Port Lincoln (15,682), Port Pirie (14,281), Port Augusta (14,196), and Victor Harbor (13,671). [32]		On 1 January 2009, the school leaving age was raised to 17 (having previously been 15 and then 16).[33] Education is compulsory for all children until age 17, unless they are working or undergoing other training. The majority of students stay on to complete their South Australian Certificate of Education (SACE). School education is the responsibility of the South Australian government, but the public and private education systems are funded jointly by it and the Commonwealth Government.		The South Australian Government provides, to schools on a per student basis, 89 percent of the total Government funding while the Commonwealth contributes 11 percent. Since the early 1970s it has been an ongoing controversy[34] that 68 percent of Commonwealth funding (increasing to 75% by 2008) goes to private schools that are attended by 32% of the states students.[35] Private schools often refute this by saying that they receive less State Government funding than public schools and in 2004 the main private school funding came from the Australian government, not the state government.[36]		On 14 June 2013, South Australia became the third Australian state to sign up to the Australian Federal Government's Gonski Reform Program. This will see funding for primary and secondary education to South Australia increased by $1.1 billion before 2019.[37]		There are three public and four private universities in South Australia. The three public universities are the University of Adelaide (established 1874, third oldest in Australia), Flinders University (est. 1966) and the University of South Australia (est. 1991). The four private universities are Torrens University Australia (est. 2013), Carnegie Mellon University - Australia (est. 2006), University College London's School of Energy and Resources (Australia), and Cranfield University. All six have their main campus in the Adelaide metropolitan area: Adelaide and UniSA on North Terrace in the city; CMU, UCL and Cranfield are co-located on Victoria Square in the city, and Flinders at Bedford Park.		Tertiary vocational education is provided by a range of Registered Training Organisations (RTOs) which are regulated at Commonwealth level. The range of RTOs delivering education include public, private and 'enterprise' providers i.e. employing organisations who run an RTO for their own employees or members.		The largest public provider of vocational education is TAFE South Australia which is made up of colleges throughout the state, many of these in rural areas, providing tertiary education to as many people as possible. In South Australia, TAFE is funded by the state government and run by the South Australian Department of Further Education, Employment, Science and Technology (DFEEST). Each TAFE SA campus provides a range of courses with its own specialisation.		After settlement, the major form of transport in South Australia was ocean transport. Limited land transport was provided by horses and bullocks. In the mid 19th century, the state began to develop a widespread rail network, although a coastal shipping network continued until the post war period.		Roads began to improve with the introduction of motor transport. By the late 19th century, road transport dominated internal transport in South Australia.		South Australia has four interstate rail connections, to Perth via the Nullarbor Plain, to Darwin through the centre of the continent, to New South Wales through Broken Hill, and to Melbourne–which is the closest capital city to Adelaide.		Rail transport is important for many mines in the north of the state.		The capital Adelaide has limited commuter rail transport.		South Australia has extensive road networks linking towns and other states. Roads are also the most common form of transport within the major metropolitan areas with car transport predominating. Public transport in Adelaide is mostly provided by buses with regular services throughout the day.		Adelaide Airport provides regular flights to other capitals, major South Australian towns, and most international locations. The Airport also has daily flights to several Asian hub airports. Adelaide Metro buses J1 and J1X connect to the City (approx. 30 minutes travel time). Standard fares apply and tickets may be purchased from the driver. Maximum charge (September 2016) for Metroticket $5.30; off-peak and seniors discounts may apply.		The River Murray was formerly an important trade route for South Australia, with paddle steamers linking inland areas and the ocean at Goolwa.		South Australia has a container port at Port Adelaide. There are also numerous important ports along the coast for minerals and grains.		The passenger terminal at Port Adelaide periodically sees cruise liners.		Kangaroo Island is dependent on the Sea Link ferry Service between Cape Jervis and Penneshaw.		Australian rules football is the most popular spectator sport in South Australia, with South Australians having the highest attendance rate in Australia.[38] The state also has the highest participation rate of people taking part in Australian rules football.[citation needed]		South Australia fields two teams in the Australian Football League national competition: the Adelaide Football Club and Port Adelaide Football Club's. As of 2015 the two clubs are in the top five in terms of membership numbers, with both clubs' membership figures reaching over 60,000 in 2015. Both teams have used the Adelaide Oval as their home ground since 2014, having previously used Football Park (AAMI Stadium) as their home ground until 2013.		The South Australian National Football League, who owns Football Park, is a popular local league comprising ten teams (Sturt, Port Adelaide, Adelaide, West Adelaide, South Adelaide, North Adelaide, Norwood, Woodville/West Torrens, Glenelg and Central District).		The South Australian Amateur Football League comprises 68 member clubs playing over 110 matches per week across ten Senior divisions and three Junior Divisions. The SAAFL is one of Australia's largest and strongest Australian rules football associations.[39]		Cricket is the most popular summer sport in South Australia and attracts big crowds. South Australia has a cricket team, the South Australian Redbacks, who play at Adelaide Oval in the Adelaide Park Lands during the summer; they won their first title since 1996 in the summer of 2010–11. Many international matches have been played at the Adelaide Oval; it was one of the host cities of 2015 Cricket World Cup, and for many years it hosted the Australia Day One Day International. South Australia is also home to the Adelaide Strikers an Australian men's professional Twenty20 cricket team that competes in Australia's domestic Twenty20 cricket competition, the Big Bash League.		South Australia's Association Football (soccer) team in the A-League is Adelaide United F.C. The club's home ground is Hindmarsh Stadium (Coopers Stadium), but occasionally play games at the Adelaide Oval.		The club was founded in 2003 and are the season 2015/16 champions of the Hyundai A-League. The club was also premier in the inaugural 2005–06 A-League season, finishing 7 points clear of the rest of the competition, before finishing 3rd in the finals. Adelaide United was also a Grand Finalist in the 2006–07 and 2008–09 seasons. Adelaide is the only A-League club to have progressed past the group stages of the Asian Champions League on more than one occasion,[40] making it the most successful Australian club in the International competition. Until the Western Sydney Wanderers won the 2014 Asian Champions League in their maiden attempt.		Basketball also has a big following in South Australia, with the Adelaide 36ers playing out of an 8,070 seat stadium in Findon. The 36ers have won four championships in the last 20 years in the National Basketball League. The Titanium Security Arena, located in Findon, is the home of basketball in the state.		Mount Gambier also has a national basketball team – the Mount Gambier Pioneers. The Pioneers play at the Icehouse (Mount Gambier Basketball Stadium) which seats over 1,000 people and is also home to the Mount Gambier Basketball Association. The Pioneers won the South Conference in 2003 and the Final in 2003; this team was rated second in the top 5 teams to have ever played in the league. In 2012, the club entered its 25th season, with a roster of 10 senior players (2 imports) and 3 development squad players.		Australia's premier motor sport series, the Supercars Championship, has visited South Australia each year since 1999. South Australia's Supercars event, the Clipsal 500 Adelaide, is staged on the Adelaide Street Circuit, a temporary track laid out through the streets and parklands to the east of the Adelaide city centre. Attendance for the 2010 event totalled 277,800.[41] An earlier version of the Adelaide Street Circuit played host to the Australian Grand Prix, a round of the FIA Formula One World Championship, each year from 1985 to 1995.		Mallala Motor Sport Park, a permanent circuit located near the town of Mallala, 58 km north of Adelaide, caters for both state and national level motor sport throughout the year.		Sixty-three percent of South Australian children took part in organised sports in 2002–2003.[42]		The ATP Adelaide was a tennis tournament held from 1972 to 2008 that then moved to Brisbane and was replaced with The World Tennis Challenge a Professional Exhibition Tournament that is part of the Australian Open Series. Also, the Royal Adelaide Golf Club has hosted nine editions of the Australian Open, with the most recent being in 1998.		The state has hosted the Tour Down Under cycle race since 1999.[43]		Regions		Rivers		Lakes		Islands		Main highways		Food and drink		Lists		
The Europa-Institut was founded at Saarland University in 1951, long before the signing of the Treaties of Rome, and it is consequently the second eldest institution focused on European Integration (after the College of Europe, Bruges, Belgium). Over 5,000 students from over 40 different countries have since graduated from the Institute. Having built on the content of its study program continuously and adapted to developments on the European level over time, the Europa-Institut today focuses on European law and international law with the possibility of specialization in specific study units.						The Europa-Institut was intended to be the "jewel and symbol" of Saarland University, a university itself based on the merger of German and French educational traditions, founded under the aegis of France and the University of Nancy in 1948 and boasting personalities such as Robert Schuman amongst the first of its students.		Aims and tasks of the Europa-Institut are to research the Europe of the future, to teach young people educated in the traditional manner of each different country about Europe, to offer education from a uniform European perspective for students from these countries and, perhaps before long, to produce Europe’s driving forces.		The Europa-Institut dedicated itself to following the European integrational process from the very beginning, providing a curriculum independent of that of Saarland University and taught by personalities such as the French politician, academic and pioneer of the European movement, André Philip.		To start with, almost all "European disciplines" were included in the study program. The focus during the first two years, namely 1951 and 1952, was on comparative literature, philosophy, history and musicology. Law and economics were disciplines which played a complimentary role.		The gradual integration of the European Community influenced the development of the curriculum so that the program began to reflect formation of the Community’s legal, economical and political character. As such, the former program description of the Europa-Institut stated that, "The moment at which Europe, driven by its historical development, becomes conscious of its unity and the reality surrounding it and where consequently new political, legal, economic and cultural organs are formed and unfold," would mark the point at which it was imperative to offer a corresponding, uniform European education.		The importance of the Institute also grew with the development of project Europe. In 1953 a structural change led to the establishment of a diplomacy department within the Institute. The purpose of this department was to train students wishing to pursue a career in diplomacy or in the civil service of the, at that time, semi-autonomous Saarland region. In the meantime, the law, culture, economics and independent language department established themselves further.		When Saarland joined the Federal Republic of Germany in 1957, Saarland University adopted the German university system. The Europa-Institut, which up to that point was geared primarily towards cultural and literary studies, was transformed into a European research institute with law and economics as the focal point. The Europa-Institut began operating with this concept in the winter term of 1957/58.		In the mid 1960s, the study program was combined so as to constitute a single integrated course. The emphasis lay on the specific problems pertaining to the European integrational process and the related instruments and methods. Law-based courses formed the core of program and these were complemented by courses in history, politics and economics.		The postgraduate program LL.M. "European Integration" was established in 1980 by the law department of the Institute, with Prof. Dr. H. C. Mult. Georg Ress and Prof. Dr. Michael R. Will as the first to head it. In 1991, Prof. Dr. Torsten Stein from Heidelberg became co-director of the Europa-Institut, which he led together with Prof. Dr. Werner Meng from Halle since 1999. In 2012, Prof. Dr. Thomas Giegerich has been appointed the new co-director in succession to Prof. Dr. Torsten Stein.		In 1990 the postgraduate MBA program "European Management" was established by the economics department of the Institute.		Today, the Europa-Institut qualifies as a model program for European studies and is actively supported by the German Federal Ministry of Foreign Affairs, the German Federal Ministry of Education, Science, Research and Technology, the European Commission and the "Stifterverband für die Deutsche Wissenschaft".[2]		The Master's program European Integration is a 12-month, full-time LL.M course centering on substantive, institutional and procedural European law and international law. To successfully complete the program, students need to gain 60 credits. The study comprises 2 semesters (9 months) of lectures (45 credits) and a written thesis (usually 3 months, 15 credits).		It offers the following specializations:		Students can specialize in a maximum two fields. To gain a specialization, a student needs to successfully complete the given unit, i.e. gain a minimum of 12 credits as well as pass unit's obligatory courses.[3] The institute as well as the university use French grading system; students with an average of over 15 gain the right to be admitted to doctoral study. The current fee can be checked here [4]		The MBA program European Management focuses on European market. As a prerequisite completion of undergraduate study of at least 240 credits as well as 2 years of corresponding working experience are required (in addition to fluency in English).		It focuses on three aspects: people (their needs & cultures), markets and morals (the responsibility both towards environment and employees). It comprises 15 study units (9 months, 45 credits) and final thesis (usually 3 months, 15 credits). There is also possibility of part-time study, which may take as long as four years. Students attend the 15 study units, but the extended length allows them to do it while working. Also the term for final thesis is extended to 6 months. As of 2011 the fees are €12,000 for full-time study and €14,500 for part-time study.[5]		The alumni association EVER was established by current and former students of the Europa-Institut, Law Department, of Saarland University, in 1996. By means of numerous activities, the alumni association aims to:		In addition, EVER unites students and alumni of different nationalities across professional and geographic borders and thus constitutes, alongside the study program itself, a contact forum that contributes to international understanding worldwide.		Since 1972, the Europa-Institut holds one of 52 European Documentation and Information Centers (EDC) in Germany and forms part of a network of 600 EDZs worldwide. The goal of the EDCs is to make available to the public (both in and outside the university system) information on the European Union and its policies and to support the research and teaching of the European integrational process.		All publicly available official publications of the EU (Official Gazette of the EU, documents of the Commission, case law of the European Court of Justice), as well as periodicals, brochures and information material are collected in the EDC. Access to numerous databases of the EU as well as to an increasing amount of electronic documentation is also provided		The Europa-Institut has been publishing its own academic journal, ZEuS ("Zeitschrift für europarechtliche Studien") since 1998. It appears on a quarterly basis and is dedicated to topical problems relating to European integration, European and international law, as well as international aspects of constitutional law. It focuses particularly on European media law and on the European protection of human rights, as well as on European and international economic law. Since ZEuS contains not only German, but also English, French and Spanish contributions, it constitutes an ideal forum both for academia and practice.		
The Quebec education system is governed by the Ministry of Education, Recreation and Sports (Ministère de l'Éducation, du Loisir et du Sport). It is administered at the local level by publicly elected French and English school boards. Teachers are represented by province-wide unions that negotiate province-wide working conditions with local boards and the provincial government.						Optional preschool, also known as pre-kindergarten (prématernelle), is available in select inner city areas for children that have attained 4 years of age on September 30 of the school year. Kindergarten (maternelle) is available province wide for children that have attained 5 years of age on September 30th of the school year.		Mandatory elementary education (école primaire) starts with grade 1, through to grade 6. Secondary school (école secondaire) has five grades, called secondary I-V (Sec I-V for short) or simply grades 7-11. Students are 12 to 16 years old (age of September 30), unless they repeat a grade. Upon completion of grade 11, students receive their high school diploma from the provincial government.		Quebec has publicly funded French and English schools. In primary and secondary schools, according to the Charter of the French Language, all students must attend a French language school, except:		May attend publicly funded English schools. These rules do not apply to temporary residents of Quebec or First Nation children. If a parent had the right to attend English schools, but did not, they do not lose the right for their children.		English is taught as a second language in French primary schools from grade 1 onward, and a few schools also offer English immersion programs for advanced students. English schools offer a large range of programs that include French as a second language, French immersion, and fully bilingual programs that teach both English and French as first languages.		Formerly, school boards were divided between Roman catholic and Protestant (called "confessional schools"). Attempts were made to set up a Jewish school board before the Second World War, but it failed partly due to divisions within the Jewish community. This confessional system was established through the British North America Act, 1867 (today the Constitution Act, 1867), which granted power over education to the provinces. Article 93 of the act made it unconstitutional for Quebec to change this system. consequently, a constitutional amendment was required to operate what some see as the separation of the State and the church in Quebec.		The Quebec Education Act of 1988 provided a change to linguistic school boards. In 1997, a unanimous vote by the National Assembly of Quebec allowed for Quebec to request that the Government of Canada exempt the province from Article 93 of the Constitution Act. This request was passed by the federal parliament, resulting in Royal Assent being granted to the Constitutional Amendment, 1997, (Quebec).		In the 1996-1997 school year, Quebec had 156 school districts including 135 Catholic districts, 18 Protestant school districts, and three First Nations districts. The school districts operated 2,670 public schools, including 1,895 primary schools, 576 general or professional secondary schools, and 199 combined primary and secondary schools.[1]		When public schools were deconfessionalized in 2000, Catholic and Protestant religious education classes along with nonreligious moral education classes continued to be part of the curriculum. Article 5 of the Quebec Public Education Act had been modified in 1997 so as to allow minority religious groups to be allowed religious education classes of their faith where their number were large enough, but this was removed in 2000. Then, in order to prevent court challenges by these same minority religious groups wanting specialist religious education in schools, the government invoked the notwithstanding clause, which expires after a maximum of 5 years. In 2005 the government of Premier Jean Charest decided not to renew the clause, abrogate Article 5 of the Public Education Act, modify Article 41 of the Quebec Charter of Rights and then eliminate the choice in moral and religious instruction that existed previously and, finally, impose a controversial new Ethics and religious culture curriculum to all schools, even the private ones.[citation needed] The ERC course has been taught starting in September 2008. Several court challenges have been launched against its compulsory nature.		Quebec has the highest proportion of children going to private schools in North America. The phenomenon is not restricted to the well-to-do. Many middle class, lower middle class and even working class families scrimp and save to send their children to private schools. The government of Quebec gives a Pro rata subsidy for each child to any private school which meets its standards and follows its prescriptions, reducing tuition costs to approximately 30% of non-subsidized private schools.		Most of the private schools are secondary institutions, though there are a few primary schools, most of them serving precise religious or cultural groups such as Armenian Orthodox Christians or certain Jewish faiths.		17% of the high school population of Quebec currently attends a private high school. The figure is even higher in urban centres such as Montreal, where 30% of high school students are in the private sector. A study released in August 2004 by the Quebec Ministry of Education revealed that, over the preceding five years, the private sector had grown by 12% while the public sector had shrunk 5.6%, with slightly steeper rate in the last year.		Private secondary schools usually select their students by having them go through their own scholastic exams and by making a study of the entire primary school record.		The Quebec public sector teachers' unions oppose any form of subsidy to private schools. They claim (1) that private schools select only the brightest and most capable students and reject children with learning difficulties; and argue (2) that by doing this they leave a burden to the public sector. Private schools usually have teachers who are not unionized, or who belong to associations not affiliated with the main body of Quebec public sector teachers' unions. The debate over the subsidies has been going on for several decades.		It should be noted that the term 'post-secondary' in this entry is used within the context of Quebec, specifically. As a rule, Canadian provinces other than Quebec do not consider completion of grade 11 in Quebec (Sec V)—or, more simply, the secondary diploma of Quebec—to be sufficient for university admission (or admission at other post-secondary institutions), since secondary education in all other provinces continues to and includes grade 12.		Both private and public colleges exist side by side, public institutions called general and professional education colleges (Official French only name: Collège d'enseignement général et professionnel or CÉGEP) and private independent college institutions in Quebec straddle the definitions of both secondary and post-secondary education. In Quebec, these institutions are readily considered post-secondary, but Quebec is the only province that requires 11 (rather than 12) years of study in order to obtain the high school diploma. While standard admission to college is based on the secondary school diploma of Quebec (representing completion of grade 11), completion of the two-year college program does not give students the equivalent of a university Diploma (university diplomas throughout Canada are awarded following completion of at least a two-year post-secondary program of study). Rather, holders of the two-year college diploma still must complete a minimum of three years of university education in order to obtain a bachelor's degree. Under Canadian law, Bachelor's degrees from government-accredited universities in Canada are considered equal, whether from Quebec or other provinces. Those unfamiliar with Quebec may wonder if three-year university programs there are therefore equal to four-year university programs in other provinces, or in other countries where four-year first university degree programs are the norm. However, given that college diploma holders are granted up to one year of advanced standing credit at any university, it is clear that this is not the case. What exists in Quebec is simply a different structure of education than in other provinces, which ultimately yields exactly the same total duration of study when years of secondary and post-secondary study are combined.		Most students continue to a general and professional education college (called CEGEP an acronym for the French Collège d' enseignement général et professionel) after high/secondary school. These students can specialize in a number of different vocational or pre-university fields. The term of study is two years for pre-university and three years for most vocational diplomas. Students completing college earn the Diplôme d'études collégiales, sometimes with other designations attached to this title. Like primary and secondary schools, both state-run (public) colleges and private colleges exist.		The word/acronym CEGEP can only legally be used to describe the state-run post-secondary (post-grade 11) schools, where tuition is free, but in fact very little attention is paid to this. The 26 private institutions which offer a post-secondary program recognized by the Quebec Ministry of Education receive a pro rata subsidy for each of their 15,000 students, and grant the same diplomas as the public colleges. Unlike the state-run colleges, the private post-secondary schools do not have to combine pre-university and vocational programs in one institution. About half offer pre-university and the other half offer vocational programs.		Graduates of two-year college programs often receive up to one year of advanced standing at universities outside of Quebec, but no more than this. Effectively, the first year of college study is considered equivalent to grade twelve in all other provinces, while the second year is considered to be equal to the freshman university year. Chronologically and legally, this is true and has been in effect for the entire modern era of education in Canada.		Primary school, secondary school, and college add up to 13 years of pre-university study, one more than other provinces (although part of college study is post-secondary, as evidenced by the treatment of college diplomas in and outside of Quebec). For this reason, most undergraduate university degrees in Quebec universities are three years in length for Quebec students who have obtained a college diploma. Universities from outside Quebec have four-year bachelor's degree programs, because secondary study in all provinces outside of Quebec ends with grade 12 (rather than secondary study ending with grade 11 and then being followed by two years of college study, as in Quebec). University education in Quebec is much like in other North American jurisdictions. In addition to formerly private institutions, the government of Quebec founded a network of universities in several Quebec cities, called the Université du Québec. All universities in the province have since become public in a similar fashion to other Canadian provinces.		From the standpoint of post-secondary institutions outside of Quebec who may be trying to determine transfer credit, there are essentially two ways in which to interpret the two-year college program, bolstered by local and countrywide legislation. The first option is to remove the first year of college study from consideration, since it is in fact the twelfth year of study overall in Quebec, and the laws of the land throughout Canada dictate that a high school diploma from Quebec lacks one additional year in order to be considered the equivalent of a high school diploma elsewhere. The second option would be to include both years of college study in the evaluation, knowing that the maximum of possible transfer credit/advanced standing is one year at the freshman level. This second option is viable if you are uncomfortable with using the chronological separation of year 12 and year 13 as your rationale, especially since college courses are not necessarily all taken in a predetermined chronological order (the order can vary from student to student).		Quebec subsidizes post-secondary education and controls tuition fees, resulting in low student costs in university education. There are three levels of tuition: Quebec resident (lowest level), Out-of-province Canadian resident (tuition set to average Canadian tuition) and International tuition (highest). The Quebec resident tuition is only available to residents of Quebec, residents of jurisdictions that have bilateral agreements with the Quebec government, and to students enrolled in French literature or Quebec studies programme.		Greater Montreal has 11 universities, founded over the course of 200 years. In 2015 it had more than 155,000 students (full time equivalent), or 65% of Quebec's student population.[2]		
Victoria (abbreviated as Vic) is a state in southeastern Australia. Victoria is Australia's most densely-populated state and its second-most populous state overall. Most of its population lives concentrated in the area surrounding Port Phillip Bay, which includes the metropolitan area of its state capital and largest city, Melbourne, Australia's second-largest city. Geographically the smallest state on the Australian mainland, Victoria is bordered by Bass Strait and Tasmania to the south,[note 1] New South Wales to the north, the Tasman Sea to the east, and South Australia to the west.		Prior to British European settlement, a large[quantify] number of Aboriginal peoples, collectively known[by whom?] as the Koori, lived in the area now constituting Victoria. With Great Britain having claimed the entire Australian continent east of the 135th meridian east in 1788, Victoria formed part of the wider colony of New South Wales. The first European settlement in the area occurred in 1803 at Sullivan Bay, and much of what is now Victoria was included in 1836 in the Port Phillip District, an administrative division of New South Wales. Named in honour of Queen Victoria who signed the division's separation from New South Wales, the colony was officially established in 1851 and achieved self-government in 1855.[6] The Victorian gold rush in the 1850s and 1860s significantly increased both the population and wealth of the colony, and by the time of the Federation of Australia in 1901, Melbourne had become the largest city and leading financial centre in Australasia. Melbourne served as federal capital of Australia until the construction of Canberra in 1927, with the Federal Parliament meeting in Melbourne's Parliament House and all principal offices of the federal government being based in Melbourne.		Politically, Victoria has 37 seats in the Australian House of Representatives and 12 seats in the Australian Senate. At state level, the Parliament of Victoria consists of the Legislative Assembly (the lower house) and the Legislative Council (the upper house). As of 2017[update] the Labor Party governs in Victoria, with Daniel Andrews serving as Premier since 2014. The personal representative of the Queen of Australia in the state is the Governor of Victoria, currently Linda Dessau (in office since 2015). Local government is concentrated in 79 municipal districts, including 33 cities, although a number of unincorporated areas still exist, which the state administers directly.		The economy of Victoria is highly diversified: service sectors including financial and property services, health, education, wholesale, retail, hospitality and manufacturing constitute the majority of employment. Victoria's total gross state product (GSP) ranks second in Australia, although Victoria ranks fourth in terms of GSP per capita because of its limited mining activity. Culturally, Melbourne hosts a number of museums, art galleries and theatres and is also described as the "sporting capital of World" [7][8][9]. The Melbourne Cricket Ground, home of the largest stadium in Australia, hosted the 1956 Summer Olympics and the 2006 Commonwealth Games. The ground is also considered[by whom?] the "spiritual home" of Australian cricket and Australian rules football [10], and hosts the grand final of the Australian Football League (AFL) each year, drawing crowds of approximately 100,000. Victoria has eight public universities, with the oldest, the University of Melbourne, dating from 1853.						Victoria, like Queensland, was named after Queen Victoria, who had been on the British throne for 14 years when the colony was established in 1851.[11]		After the founding of the colony of New South Wales in 1788, Australia was divided into an eastern half named New South Wales and a western half named New Holland, under the administration of the colonial government in Sydney. The first British settlement in the area later known as Victoria was established in October 1803 under Lieutenant-Governor David Collins at Sullivan Bay on Port Phillip. It consisted of 402 people (5 Government officials, 9 officers of marines, 2 drummers, and 39 privates, 5 soldiers' wives, and a child, 307 convicts, 17 convicts' wives, and 7 children).[12] They had been sent from England in HMS Calcutta under the command of Captain Daniel Woodriff, principally out of fear that the French, who had been exploring the area, might establish their own settlement and thereby challenge British rights to the continent.		In the year 1826 Colonel Stewart, Captain S. Wright, and Lieutenant Burchell were sent in HMS Fly (Captain Wetherall) and the brigs Dragon and Amity, took a number of convicts and a small force composed of detachments of the 3rd and 93rd regiments. The expedition landed at Settlement Point (now Corinella), on the eastern side of the bay, which was the headquarters until the abandonment of Western Port at the insistence of Governor Darling about twelve months afterwards.[13][14]		Victoria's next settlement was at Portland, on the south west coast of what is now Victoria. Edward Henty settled Portland Bay in 1834.[15]		Melbourne was founded in 1835 by John Batman, who set up a base in Indented Head, and John Pascoe Fawkner. From settlement the region around Melbourne was known as the Port Phillip District, a separately administered part of New South Wales. Shortly after the site now known as Geelong was surveyed by Assistant Surveyor W. H. Smythe, three weeks after Melbourne. And in 1838 Geelong was officially declared a town, despite earlier white settlements dating back to 1826.				flag (1870-1901)		On 1 July 1851, writs were issued for the election of the first Victorian Legislative Council, and the absolute independence of Victoria from New South Wales was established proclaiming a new Colony of Victoria.[16] Days later, still in 1851 gold was discovered near Ballarat, and subsequently at Bendigo. Later discoveries occurred at many sites across Victoria. This triggered one of the largest gold rushes the world has ever seen. The colony grew rapidly in both population and economic power. In ten years the population of Victoria increased sevenfold from 76,000 to 540,000. All sorts of gold records were produced including the "richest shallow alluvial goldfield in the world" and the largest gold nugget. Victoria produced in the decade 1851–1860 20 million ounces of gold, one third of the world's output[citation needed].		Immigrants arrived from all over the world to search for gold, especially from Ireland and China. Many Chinese miners worked in Victoria, and their legacy is particularly strong in Bendigo and its environs. Although there was some racism directed at them, there was not the level of anti-Chinese violence that was seen at the Lambing Flat riots in New South Wales. However, there was a riot at Buckland Valley near Bright in 1857. Conditions on the gold fields were cramped and unsanitary; an outbreak of typhoid at Buckland Valley in 1854 killed over 1,000 miners.		In 1854 at Ballarat there was an armed rebellion against the government of Victoria by miners protesting against mining taxes (the "Eureka Stockade"). This was crushed by British troops, but the discontents prompted colonial authorities to reform the administration (particularly reducing the hated mining licence fees) and extend the franchise. Within a short time, the Imperial Parliament granted Victoria responsible government with the passage of the Colony of Victoria Act 1855. Some of the leaders of the Eureka rebellion went on to become members of the Victorian Parliament.		The first foreign military action by the colony of Victoria was to send troops and a warship to New Zealand as part of the Māori Wars. Troops from New South Wales had previously participated in the Crimean War.		In 1901 Victoria became a state in the Commonwealth of Australia. As a result of the gold rush, Melbourne had by then become the financial centre of Australia and New Zealand. Between 1901 and 1927, Melbourne was the capital of Australia while Canberra was under construction. It was also the largest city in Australia at the time.[citation needed].		Victoria has a parliamentary form of government based on the Westminster System. Legislative power resides in the Parliament consisting of the Governor (the representative of the Queen), the executive (the Government), and two legislative chambers. The Parliament of Victoria consists of the lower house Legislative Assembly, the upper house Legislative Council and the Queen of Australia.		Eighty-eight members of the Legislative Assembly are elected to four-year terms from single-member electorates.		In November 2006, the Victorian Legislative Council elections were held under a new multi-member proportional representation system. The State of Victoria was divided into eight electorates with each electorate represented by five representatives elected by Single Transferable Vote. The total number of upper house members was reduced from 44 to 40 and their term of office is now the same as the lower house members—four years. Elections for the Victorian Parliament are now fixed and occur in November every four years. Prior to the 2006 election, the Legislative Council consisted of 44 members elected to eight-year terms from 22 two-member electorates.		The Premier of Victoria is the leader of the political party or coalition with the most seats in the Legislative Assembly. The Premier is the public face of government and, with cabinet, sets the legislative and political agenda. Cabinet consists of representatives elected to either house of parliament. It is responsible for managing areas of government that are not exclusively the Commonwealth's, by the Australian Constitution, such as education, health and law enforcement. The current Premier of Victoria is Daniel Andrews.		Executive authority is vested in the Governor of Victoria who represents and is appointed by Queen Elizabeth II. The post is usually filled by a retired prominent Victorian. The governor acts on the advice of the premier and cabinet. The current Governor of Victoria is Linda Dessau.		Victoria has a written constitution enacted in 1975,[17] but based on the 1855 colonial constitution, passed by the United Kingdom Parliament as the Victoria Constitution Act 1855, which establishes the Parliament as the state's law-making body for matters coming under state responsibility. The Victorian Constitution can be amended by the Parliament of Victoria, except for certain "entrenched" provisions that require either an absolute majority in both houses, a three-fifths majority in both houses, or the approval of the Victorian people in a referendum, depending on the provision.		Victorians, and Melburnians in particular, are said to be "more progressive than other Australians", and although "most Australians support gay marriage", it is supported "nowhere more strongly than in Victoria." At the republic referendum in 1999, the state with the highest yes vote was Victoria. Victorians are also said to be "generally socially progressive, supportive of multiculturalism, wary of extremes of any kind."[18]		Premier Daniel Andrews leads the Australian Labor Party that won the November 2014 Victorian state election.		The centre-left Australian Labor Party (ALP), the centre-right Liberal Party of Australia, the rural-based National Party of Australia, and the environmentalist Australian Greens are Victoria's main political parties. Traditionally, Labor is strongest in Melbourne's working class western and northern suburbs, and the regional cities of Ballarat, Bendigo and Geelong. The Liberals' main support lies in Melbourne's more affluent eastern and outer suburbs, and some rural and regional centres. The Nationals are strongest in Victoria's North Western and Eastern rural regional areas. The Greens, who won their first lower house seats in 2014, are strongest in inner Melbourne.		Victorian voters elect 49 representatives to the Parliament of Australia, including 37 members of the House of Representatives and 12 members of the Senate. Since 2013, the ALP has held 19 Victorian house seats, the Liberals 14, the Nationals two, the Greens one, and one held by an Independent. As of 1 July 2014, the Liberals have held three senate seats, the Nationals one, the ALP four, the Greens two, and the Democratic Labor Party one.		Victoria is incorporated into 79 municipalities for the purposes of local government, including 39 shires, 32 cities, seven rural cities and one borough. Shire and city councils are responsible for functions delegated by the Victorian parliament, such as city planning, road infrastructure and waste management. Council revenue comes mostly from property taxes and government grants.[19]		The 2011 Australian census reported that Victoria had 5,354,042 people resident at the time of the census.[20] The Australian Bureau of Statistics estimates that the population may well reach 7.2 million by 2050.		Victoria's founding Anglo-Celtic population has been supplemented by successive waves of migrants from southern and eastern Europe, Southeast Asia and, most recently, the Horn of Africa and the Middle East. Victoria's population is ageing in proportion with the average of the remainder of the Australian population.		About 72% of Victorians are Australian-born. This figure falls to around 66% in Melbourne but rises to higher than 95% in some rural areas in the north west of the state. Around two-thirds of Victorians claim Scottish, English or Irish ancestry. Less than 1% of Victorians identify themselves as Aboriginal. The largest groups of people born outside Australia came from the British Isles, China, Italy, Vietnam, Greece and New Zealand.		More than 75% of Victorians live in Melbourne, located in the state's south. The greater Melbourne metropolitan area is home to an estimated 4.17 million people.[21] Leading urban centres outside Melbourne include Geelong, Ballarat, Bendigo, Shepparton, Mildura, Warrnambool, Wodonga and the Latrobe Valley.		Victoria is Australia's most urbanised state: nearly 90% of residents living in cities and towns. State Government efforts to decentralise population have included an official campaign run since 2003 to encourage Victorians to settle in regional areas,[22] however Melbourne continues to rapidly outpace these areas in terms of population growth.[23]		The government predicts that nearly a quarter of Victorians will be aged over 60 by 2021. The 2011 census reveals that Australian median age has crept upward from 35 to 37 since 2001, which reflects the population growth peak of 1969–72.[24] In 2011, Victoria recorded a TFR of 1.88, the highest after 1978.[25]		In 2011–2012 there were 173 homicides.[26]		In 2015, the average male prisoner in Victoria is:[27]		In 2015, the average female prisoner in Victoria is:[27]		About 61.1% of Victorians describe themselves as Christian. Roman Catholics form the single largest religious group in the state with 26.7% of the Victorian population, followed by Anglicans and members of the Uniting Church. Buddhism is the state's largest non-Christian religion, with 168,637 members as of the most recent census. Victoria is also home of 152,775 Muslims and 45,150 Jews. Hinduism is the fastest growing religion. Around 20% of Victorians claim no religion. Amongst those who declare a religious affiliation, church attendance is low.[28]		In 2012 the proportion of couples marrying in a church had dropped to 28.2%; the other 71.7% registered their marriage with a civil celebrant.[29]		Victoria's state school system dates back to 1872, when the colonial government legislated to make schooling both free and compulsory. The state's public secondary school system began in 1905. Before then, only private secondary schooling was available. Today, a Victorian school education consists of seven years of primary schooling (including one preparatory year) and six years of secondary schooling.		The final years of secondary school are optional for children aged over 17. Victorian children generally begin school at age five or six. On completing secondary school, students earn the Victorian Certificate of Education. Students who successfully complete their secondary education also receive a tertiary entrance ranking, or ATAR score, to determine university admittance.		Victorian schools are either publicly or privately funded. Public schools, also known as state or government schools, are funded and run directly by the Victoria Department of Education [3]. Students do not pay tuition fees, but some extra costs are levied. Private fee-paying schools include parish schools run by the Roman Catholic Church and independent schools similar to British public schools. Independent schools are usually affiliated with Protestant churches. Victoria also has several private Jewish and Islamic primary and secondary schools. Private schools also receive some public funding. All schools must comply with government-set curriculum standards. In addition, Victoria has four government selective schools, Melbourne High School for boys, MacRobertson Girls' High School for girls, the coeducational schools John Monash Science School, Nossal High School and Suzanne Cory High School, and The Victorian College of the Arts Secondary School. Students at these schools are exclusively admitted on the basis of an academic selective entry test.		As of August 2010, Victoria had 1,548 public schools, 489 Catholic schools and 214 independent schools. Just under 540,800 students were enrolled in public schools, and just over 311,800 in private schools. Over 61 per cent of private students attend Catholic schools. More than 462,000 students were enrolled in primary schools and more than 390,000 in secondary schools. Retention rates for the final two years of secondary school were 77 per cent for public school students and 90 per cent for private school students. Victoria has about 63,519 full-time teachers.[30]		Victoria has nine universities. The first to offer degrees, the University of Melbourne, enrolled its first student in 1855. The largest, Monash University, has an enrolment of over 70,000 students—more than any other Australian university.		The number of students enrolled in Victorian universities was 241,755 at 2004, an increase of 2% on the previous year. International students made up 30% of enrolments and account for the highest percentage of pre-paid university tuition fees. The largest number of enrolments were recorded in the fields of business, administration and economics, with nearly a third of all students, followed by arts, humanities, and social science, with 20% of enrolments.		Victoria has 18 government-run institutions of “technical and further education” (TAFE). The first vocational institution in the state was the Melbourne Mechanics' Institute (established in 1839), which is now the Melbourne Athenaeum. More than 1,000 adult education organisations are registered to provide recognised TAFE programs. In 2004, there were about 480,700 students enrolled in vocational education programs in the state.[31]		The State Library of Victoria is the State's research and reference library. It is responsible for collecting and preserving Victoria's documentary heritage and making it available through a range of services and programs. Material in the collection includes books, newspapers, magazines, journals, manuscripts, maps, pictures, objects, sound and video recordings and databases.		In addition, local governments maintain local lending libraries, typically with multiple branches in their respective municipal areas.		The state of Victoria is the second largest economy in Australia after New South Wales, accounting for a quarter of the nation's gross domestic product. The total gross state product (GSP) at current prices for Victoria was at just over A$293 billion, with a GSP per capita of A$52,872. The economy grew by 2.0 per cent in 2010, less than the Australian average of 2.3 per cent.		Finance, insurance and property services form Victoria's largest income producing sector, while the community, social and personal services sector is the state's biggest employer. Despite the shift towards service industries, the manufacturing sector remains Victoria's single largest employer and income producer.		During 2003–04, the gross value of Victorian agricultural production increased by 17% to $8.7 billion. This represented 24% of national agricultural production total gross value. As of 2004, an estimated 32,463 farms occupied around 136,000 square kilometres (52,500 sq mi) of Victorian land. This comprises more than 60% of the state's total land surface. Victorian farms range from small horticultural outfits to large-scale livestock and grain productions. A quarter of farmland is used to grow consumable crops.		More than 26,000 square kilometres (10,000 sq mi) of Victorian farmland are sown for grain, mostly in the state's west. More than 50% of this area is sown for wheat, 33% for barley and 7% for oats. A further 6,000 square kilometres (2,300 sq mi) is sown for hay. In 2003–04, Victorian farmers produced more than 3 million tonnes of wheat and 2 million tonnes of barley. Victorian farms produce nearly 90% of Australian pears and third of apples. It is also a leader in stone fruit production. The main vegetable crops include asparagus, broccoli, carrots, potatoes and tomatoes. Last year, 121,200 tonnes of pears and 270,000 tonnes of tomatoes were produced.		More than 14 million sheep and 5 million lambs graze over 10% of Victorian farms, mostly in the state's north and west. In 2004, nearly 10 million lambs and sheep were slaughtered for local consumption and export. Victoria also exports live sheep to the Middle East for meat and to the rest of the world for breeding. More than 108,000 tonnes of wool clip was also produced—one-fifth of the Australian total.		Victoria is the centre of dairy farming in Australia. It is home to 60% of Australia's 3 million dairy cattle and produces nearly two-thirds of the nation's milk, almost 6.4 billion litres. The state also has 2.4 million beef cattle, with more than 2.2 million cattle and calves slaughtered each year. In 2003–04, Victorian commercial fishing crews and aquaculture industry produced 11,634 tonnes of seafood valued at nearly A$109 million. Blacklipped abalone is the mainstay of the catch, bringing in A$46 million, followed by southern rock lobster worth A$13.7 million. Most abalone and rock lobster is exported to Asia.		Victoria has a diverse range of manufacturing enterprises and Melbourne is Victoria's (and Australia's) most important industrial city, followed by Geelong.[citation needed] Additionally, energy production has aided industrial growth in the Latrobe Valley.[citation needed]		Machinery and equipment manufacturing is the state's most valuable manufacturing activity, followed by food and beverage products, petrochemicals and chemicals.[citation needed] More than 15% of Victorian workers, are employed directly in manufacturing, the highest percentage in Australia.[citation needed] The state is marginally behind New South Wales in the total value of manufacturing output.[citation needed]		Prominent manufacturing plants in the state include the Portland and Point Henry aluminium smelters, owned by Alcoa; oil refineries at Geelong and Altona; a major petrochemical facility at Laverton; and Victorian-based CSL, a global biotechnology company that produces vaccines and plasma products, among others. Victoria also plays an important role in providing goods for the defence industry.		Historically, Victoria has been the base for the manufacturing plants of the major car brands Ford, Toyota and Holden; however, closure announcements by all three companies in the 21st century will mean that Australia will no longer be a base for the global car industry, with Toyota's statement in February 2014 outlining a closure year of 2017. Holden's announcement occurred in May 2013, followed by Ford's decision in December of the same year (Ford's Victorian plants—in Broadmeadows and Geelong—will close in October 2016).[32][33]		The Victorian Government will sponsor and support industry and research, for example the Victorian Photonics Network from 2002 to 2015.[34]		Crown land held in Victoria is managed under the Crown Land (Reserves) Act 1978 and the Land Act 1958		Mining in Victoria contributes around A$3 billion to the gross state product (~1%) but employs less than 1% of workers. The Victorian mining industry is concentrated on energy producing minerals, with brown coal, petroleum and gas accounting for nearly 90% of local production. The oil and gas industries are centred off the coast of Gippsland in the state's east, while brown coal mining and power generation is based in the Latrobe Valley.		In the 2005/2006 fiscal year, the average gas production was over 700 million cubic feet (20,000,000 m3) per day (M cuft/d) and represented 18% of the total national gas sales, with demand growing at 2% per year.[35]		In 1985, oil production from the offshore Gippsland Basin peaked to an annual average of 450,000 barrels (72,000 m3) per day. In 2005–2006, the average daily oil production declined to 83,000 bbl (13,200 m3)/d, but despite the decline Victoria still produces almost 19.5% of crude oil in Australia.[35]		Brown coal is Victoria's leading mineral, with 66 million tonnes mined each year for electricity generation in the Latrobe Valley, Gippsland.[36] The region is home to the world's largest known reserves of brown coal.		Despite being the historic centre of Australia's gold rush, Victoria today contributes a mere 1% of national gold production. Victoria also produces limited amounts of gypsum and kaolin.		The service industries sector is the fastest growing component of the Victorian economy. It includes the wide range of activities generally classified as community, social and personal services; finances, insurance and property services, government services, transportation and communication, and wholesale and retail trade. Most service industries are located in Melbourne and the state's larger regional centres.		As of 2004–05, service industries employed nearly three-quarters of Victorian workers and generated three-quarters of the state's GSP. Finance, insurance and property services, as a group, provide a larger share of GSP than any other economic activity in Victoria. More than a quarter of Victorian workers are employed by the community, social and personal services sector.[37]		Some major tourist destinations in Victoria are:		Other popular tourism activities are gliding, hang-gliding, hot air ballooning and scuba diving.		Major events also play a big part in tourism in Victoria, particularly cultural tourism and sports tourism. Most of these events are centred on Melbourne, but others occur in regional cities, such as the V8 Supercars and Australian Motorcycle Grand Prix at Phillip Island, the Grand Annual Steeplechase at Warrnambool and the Australian International Airshow at Avalon and numerous local festivals such as the popular Port Fairy Folk Festival, Queenscliff Music Festival, Bells Beach Surf Classic and the Bright Autumn Festival.		Victoria's northern border follows a straight line from Cape Howe to the start of the Murray River and then follows the Murray River as the remainder of the northern border. On the Murray River, the border is the southern bank of the river. This precise definition was not established until 1980, when a ruling by Justice Ninian Stephen of the High Court of Australia settled the question as to which state had jurisdiction in the unlawful death of a man on an island in the middle of the river. The ruling clarified that no part of the watercourse is in Victoria.[39] The border also rests at the southern end of the Great Dividing Range, which stretches along the east coast and terminates west of Ballarat. It is bordered by South Australia to the west and shares Australia's shortest land border with Tasmania. The official border between Victoria and Tasmania is at 39°12' S, which passes through Boundary Islet in the Bass Strait for 85 metres.[40][41][42]		Victoria contains many topographically, geologically and climatically diverse areas, ranging from the wet, temperate climate of Gippsland in the southeast to the snow-covered Victorian alpine areas which rise to almost 2,000 m (6,600 ft), with Mount Bogong the highest peak at 1,986 m (6,516 ft). There are extensive semi-arid plains to the west and northwest. There is an extensive series of river systems in Victoria. Most notable is the Murray River system. Other rivers include: Ovens River, Goulburn River, Patterson River, King River, Campaspe River, Loddon River, Wimmera River, Elgin River, Barwon River, Thomson River, Snowy River, Latrobe River, Yarra River, Maribyrnong River, Mitta River, Hopkins River, Merri River and Kiewa River. The state symbols include the pink heath (state flower), Leadbeater's possum (state animal) and the helmeted honeyeater (state bird).		The state's capital, Melbourne, contains about 70% of the state's population and dominates its economy, media, and culture. For other cities and towns, see list of localities (Victoria) and local government areas of Victoria.		Victoria has a varied climate despite its small size. It ranges from semi-arid temperate with hot summers in the north-west, to temperate and cool along the coast. Victoria's main land feature, the Great Dividing Range, produces a cooler, mountain climate in the centre of the state. Winters along the coast of the state, particularly around Melbourne, are relatively mild (see chart at right).		Victoria's southernmost position on the Australian mainland means it is cooler and wetter than other mainland states and territories. The coastal plain south of the Great Dividing Range has Victoria's mildest climate. Air from the Southern Ocean helps reduce the heat of summer and the cold of winter. Melbourne and other large cities are located in this temperate region. The autumn months of April/May are mild and bring some of Australia's colourful foliage across many parts of the state.		The Mallee and upper Wimmera are Victoria's warmest regions with hot winds blowing from nearby semi-deserts. Average temperatures exceed 32 °C (90 °F) during summer and 15 °C (59 °F) in winter. Except at cool mountain elevations, the inland monthly temperatures are 2–7 °C (4–13 °F) warmer than around Melbourne (see chart). Victoria's highest maximum temperature since World War II, of 48.8 °C (119.8 °F) was recorded in Hopetoun on 7 February 2009, during the 2009 southeastern Australia heat wave.[43]		The Victorian Alps in the northeast are the coldest part of Victoria. The Alps are part of the Great Dividing Range mountain system extending east–west through the centre of Victoria. Average temperatures are less than 9 °C (48 °F) in winter and below 0 °C (32 °F) in the highest parts of the ranges. The state's lowest minimum temperature of −11.7 °C (10.9 °F) was recorded at Omeo on 15 June 1965, and again at Falls Creek on 3 July 1970.[43] Temperature extremes for the state are listed in the table below:		Victoria is the wettest Australian state after Tasmania. Rainfall in Victoria increases from south to the northeast, with higher averages in areas of high altitude. Mean annual rainfall exceeds 1,800 millimetres (71 inches) in some parts of the northeast but is less than 280 mm (11 in) in the Mallee.		Rain is heaviest in the Otway Ranges and Gippsland in southern Victoria and in the mountainous northeast. Snow generally falls only in the mountains and hills in the centre of the state. Rain falls most frequently in winter, but summer precipitation is heavier. Rainfall is most reliable in Gippsland and the Western District, making them both leading farming areas. Victoria's highest recorded daily rainfall was 375 mm (14.8 in) at Tanybryn in the Otway Ranges on 22 March 1983.[43]		Average January maximum temperatures: Victoria's north is almost always hotter than coastal and mountainous areas.		Average July maximum temperatures: Victoria's hills and ranges are coolest during winter. Snow also falls there.		Average yearly precipitation: Victoria's rainfall is concentrated in the mountainous north-east and coast.		Victoria has the highest population density in any state in Australia, with population centres spread out over most of the state; only the far northwest and the Victorian Alps lack permanent settlement.		The Victorian road network services the population centres, with highways generally radiating from Melbourne and other major cities and rural centres with secondary roads interconnecting the highways to each other. Many of the highways are built to freeway standard ("M" freeways), while most are generally sealed and of reasonable quality.		Rail transport in Victoria is provided by several private and public railway operators who operate over government-owned lines. Major operators include: Metro Trains Melbourne which runs an extensive, electrified, passenger system throughout Melbourne and suburbs; V/Line which is now owned by the Victorian Government, operates a concentrated service to major regional centres, as well as long distance services on other lines; Pacific National, CFCL Australia which operate freight services; Great Southern Rail which operates The Overland Melbourne—Adelaide; and NSW TrainLink which operates XPTs Melbourne—Sydney.		There are also several smaller freight operators and numerous tourist railways operating over lines which were once parts of a state-owned system. Victorian lines mainly use the 1,600 mm (5 ft 3 in) broad gauge. However, the interstate trunk routes, as well as a number of branch lines in the west of the state have been converted to 1,435 mm (4 ft 8 1⁄2 in) standard gauge. Two tourist railways operate over 760 mm (2 ft 6 in) narrow gauge lines, which are the remnants of five formerly government-owned lines which were built in mountainous areas.		Melbourne has the world's largest tram network,[45] currently operated by Yarra Trams. As well as being a popular form of public transport, over the last few decades trams have become one of Melbourne's major tourist attractions. There are also tourist trams operating over portions of the former Ballarat and Bendigo systems. There are also tramway museums at Bylands and Haddon.		Melbourne Airport is the major domestic and international gateway for the state. Avalon Airport is the state's second busiest airport, which complements Essendon and Moorabbin Airports to see the remainder of Melbourne's air traffic. Hamilton Airport, Mildura Airport, Mount Hotham and Portland Airport are the remaining airports with scheduled domestic flights. There are no fewer than 27 other airports in the state with no scheduled flights.		The Port of Melbourne is the largest port for containerised and general cargo in Australia,[46] and is located in Melbourne on the mouth of the Yarra River, which is at the head of Port Phillip. Additional seaports are at Westernport, Geelong, and Portland.		As of October 2013, smoking tobacco is prohibited in the sheltered areas of train stations, and tram and bus stops—between 2012 and 2013, 2002 people were issued with infringement notices. The state government announced a plan in October 2013 to prohibit smoking on all Victorian railway station platforms and raised tram stops.[47]		Victoria's major utilities include a collection of brown-coal-fired power stations, particularly in the Latrobe Valley. One of these is Hazelwood Power Station, which is number 1 in the worldwide List of least carbon efficient power stations.		Victoria's water infrastructure includes a series of dams and reservoirs, predominantly in Central Victoria, that hold and collect water for much of the state. The water collected is of a very high quality and requires little chlorination treatment, giving the water a taste more like water collected in a rainwater tank. In regional areas however, such as in the west of the state, chlorination levels are much higher.		The Victorian Water Grid consists of a number of new connections and pipelines being built across the State. This allows water to be moved around Victoria to where it is needed most and reduces the impact of localised droughts in an era thought to be influenced by climate change. Major projects already completed as part of the Grid include the Wimmera Mallee Pipeline and the Goldfields Superpipe.[48]		Victoria is the home of Australian rules football, with ten of the eighteen Australian Football League (AFL) clubs based in the state. The AFL Grand Final is traditionally held at the Melbourne Cricket Ground on the last Saturday of September. Victoria's newest public holiday is Grand Final Friday. The holiday is celebrated the day before the AFL Grand Final and was designated in 2015 "to celebrate Australia’s national game."		Victoria's cricket team, the Victorian Bushrangers play in the national Sheffield Shield cricket competition. Victoria is represented in the National Rugby League by the Melbourne Storm and in Super Rugby by the Melbourne Rebels. It is represented in the National Basketball League by Melbourne United. It is also represented in soccer by Melbourne Victory and Melbourne City in the A-League.		Melbourne has held the 1956 Summer Olympics, 2006 Commonwealth Games and the FINA World Swimming Championship.		Melbourne is also home to the Australian Open tennis tournament in January each year, the first of the world's four Grand Slam tennis tournaments, and the Australian Formula One Australian Grand Prix in March. It hosted the Australian Masters golf tournament from 1979 to 2015.		Victoria's Bells Beach hosts one of the world's longest-running surfing competition, the Bells Beach SurfClassic, which is part of The ASP World Tour.		Netball is a big part of sport in Victoria.[citation needed] The Melbourne Vixens represent Victoria in the ANZ Championship. Some of the world's best netballers such as Sharelle McMahon, Renae Hallinan, Madison Browne, Julie Corletto and Bianca Chatfield come from Victoria.		Possibly Victoria's most famous island, Phillip Island, is home of the Phillip Island Grand Prix Circuit which hosts the Australian motorcycle Grand Prix which features MotoGP (the world's premier motorcycling class), as well as the Australian round of the World Superbike Championship and the domestic V8 Supercar racing, which also visits Sandown Raceway and the rural Winton Motor Raceway circuit.		Australia's most prestigious footrace, the Stawell Gift, is an annual event.		Victoria is also home to the Aussie Millions poker tournament, the richest in the Southern Hemisphere.		The main horse racing tracks in Victoria are Caulfield Racecourse, Flemington Racecourse and Sandown Racecourse. The Melbourne Spring Racing Carnival is one of the biggest horse racing events in the world and is one of the world's largest sporting events. The main race is for the $6 million Melbourne Cup, and crowds for the carnival exceed 700,000.		Major professional teams include:		Geography:		Lists:		
Charles University, known also as Charles University in Prague (Czech: Univerzita Karlova; Latin: Universitas Carolina; German: Karls-Universität) or historically as the University of Prague (Latin: Universitas Pragensis), is the oldest and largest university in the Czech Republic. Founded in 1348, it was the first university in Central Europe.[3] It is one of the oldest universities in Europe in continuous operation and ranks in the upper 1.5 percent of the world’s best universities.[4][5]		Its seal shows its protector Emperor Charles IV, with his coats of arms as King of the Romans and King of Bohemia, kneeling in front of St. Wenceslas, the patron saint of Bohemia. It is surrounded by the inscription, Sigillum Universitatis Scolarium Studii Pragensis (English: Seal of the Prague academia).[6]						The establishment of a medieval university in Prague was inspired by Holy Roman Emperor Charles IV.[7] He asked his friend and ally, Pope Clement VI, to do so. On 26 January 1347 the pope issued the bull establishing a university in Prague, modeled on the University of Paris, with the full (4) number of faculties, that is including theological. On 7 April 1348 Charles, the king of Bohemia, gave to the established university privileges and immunities from the secular power in a Golden Bull[8] and on 14 January 1349 he repeated that as the King of the Romans. Most Czech sources since the 19th century—encyclopedias, general histories, materials of the University itself—prefer to give 1348 as the year of the founding of the university, rather than 1347 or 1349. This was caused by an anticlerical shift in the 19th century, shared by both Czechs and Germans.		The university was opened in 1349. The university was sectioned into parts called nations: the Bohemian, Bavarian, Polish and Saxon. The Bohemian natio included Bohemians, Moravians, southern Slavs, and Hungarians; the Bavarian included Austrians, Swabians, natives of Franconia and of the Rhine provinces; the Polish included Silesians, Poles, Ruthenians; the Saxon included inhabitants of the Margravate of Meissen, Thuringia, Upper and Lower Saxony, Denmark, and Sweden.[9] Ethnically Czech students made 16–20% of all students.[10] Archbishop Arnošt of Pardubice took an active part in the foundation by obliging the clergy to contribute and became a chancellor of the university (i.e., director or manager).		The first graduate was promoted in 1359. The lectures were held in the colleges, of which the oldest was named for the king the Carolinum, established in 1366. In 1372 the Faculty of Law became an independent university.[11]		In 1402 Jerome of Prague in Oxford copied out the Dialogus and Trialogus of John Wycliffe. The dean of the philosophical faculty, Jan Hus, translated Trialogus into the Czech language. In 1403 the university forbade its members to follow the teachings of Wycliffe, but his doctrine continued to gain in popularity.		In the Western Schism, the Bohemian natio took the side of king Wenceslaus and supported the Council of Pisa (1409). The other nationes of the university declared their support for the side of Pope Gregory XII, thus the vote was 1:3 against the Bohemians. Hus and other Bohemians, though, took advantage of Wenceslaus' opposition to Gregory. By the Decree of Kutná Hora (German: Kuttenberg) on 18 January 1409, the king subverted the university constitution by granting the Bohemian masters three votes. Only a single vote was left for all other three nationes combined, compared to one vote per each natio before. The result of this coup was the emigration of foreign (mostly German) professors and students, founding the University of Leipzig in May 1409. Before that, in 1408, the university had about 200 doctors and magisters, 500 bachelors, and 30,000 students; it now lost a large part of this number, accounts of the loss varying from 5000 to 20,000 including 46 professors.[9] In the autumn of 1409, Hus was elected rector of the now Czech-dominated rump university.		Thus, the Prague university lost the largest part of its students and faculty. From then on the university declined to a merely regional institution with a very low status.[12] Soon, in 1419, the faculties of theology and law disappeared, and only the faculty of arts remained in existence.		The faculty of arts became a centre of the Hussite movement, and the chief doctrinal authority of the Utraquists. No degrees were given in the years 1417–30; at times there were only eight or nine professors.[9] Emperor Sigismund, son of Charles IV, took what was left into his personal property and some progress was made. The emperor Ferdinand I called the Jesuits to Prague and in 1562 they opened an academy—the Clementinum. From 1541 till 1558 the Czech humanist Mattheus Collinus (1516–1566) was a professor of Greek language.[13] Some progress was made again when the emperor Rudolph II took up residence in Prague. In 1609 the obligatory celibacy of the professors was abolished.[14] In 1616 the Jesuit Academy became a university. (It could award academic degrees.)[14]		Jesuits were expelled 1618–1621 during the early stages of the Thirty Years' War, which was started in Prague by anti-Catholic and anti-Imperial Bohemians. By 1622 the Jesuits had a predominant influence over the emperor. An Imperial decree of 19 September 1622 gave the Jesuits supreme control over the entire school system of Bohemia, Moravia and Silesia. The last four professors at the Carolinum resigned and all of the Carolinum and nine colleges went to the Jesuits. The right of handing out degrees, of holding chancellorships and of appointing the secular professors was also granted to the Jesuits.		Cardinal Ernst Adalbert von Harrach actively opposed union of the university with another institution and the withdrawal of the archiepiscopal right to the chancellorship and prevented the drawing up of the Golden Bull for the confirmation of the grant to Jesuits. Cardinal Ernst funded the Collegium Adalbertinum and in 1638 emperor Ferdinand III limited the teaching monopoly enjoyed by the Jesuits. He took from them the rights, properties and archives of the Carolinum making the university once more independent under an imperial protector. During the last years of the Thirty Years' War the Charles Bridge in Prague was courageously defended by students of the Carolinum and Clementinum. Since 1650 those who received any degrees took an oath to maintain the Immaculate Conception of the Blessed Virgin, renewed annually.		On 23 February 1654 emperor Ferdinand III merged Carolinum and Clementinum and created a single university with four faculties—Charles-Ferdinand University.[15] Carolinum had at that time only the faculty of arts, as the only faculty surviving the period of the Hussite Wars. Starting at this time, the university designated itself Charles-Ferdinand University (Latin: Universitatis Carolinae Ferdinandeae). The dilapidated Carolinum was rebuilt in 1718 at the expense of the state.		The rebuilding and the bureaucratic reforms of universities in the Habsburg monarchy in 1752 and 1754 deprived the university of many of its former privileges. In 1757 a Dominican and an Augustinian were appointed to give theological instruction. However, there was a gradual introduction of enlightened reforms, and this process culminated at the end of the century when even non-Catholics were granted the right to study. On 29 July 1784, German replaced Latin as the language of instruction.[16] For the first time Protestants were allowed, and soon after Jews. The university acknowledged the need of a Czech language and literature chair. Emperor Leopold II established it by a courtly decree on 28 October 1791. On 15 May 1792, scholar and historian František Martin Pelcl[17] was named the professor of the chair. He started his lectures on 13 March 1793.[18]		In the revolution of 1848, German and Czech students fought for the addition of the Czech language at the Charles-Ferdinand University as a language of lectures. Due to the demographic changes of the 19th century, Prague ceased to have a German-language majority around 1860. By 1863, 22 lecture courses were held in Czech, the remainder (out of 187) in German. In 1864, Germans suggested the creation of a separate Czech university. Czech professors rejected this because they did not wish to lose the continuity of university traditions.		It soon became clear that neither the Germans nor the Czechs were satisfied with the bilingual arrangement that the University arranged after the revolutions of 1848. The Czechs also refused to support the idea of the reinstitution of the 1349 student nations, instead declaring their support for the idea of keeping the university together, but dividing it into separate colleges, one German and one Czech. This would allow both Germans and Czechs to retain the collective traditions of the University. German-speakers, however, quickly vetoed this proposal, preferring a pure German university: they proposed to split Charles-Ferdinand University into two separate institutions.		After long negotiations, Charles-Ferdinand was divided into a German Charles-Ferdinand University (German: Deutsche Karl-Ferdinands-Universität) and a Czech Charles-Ferdinand University (Czech: Česká universita Karlo-Ferdinandova) by an act of the Cisleithanian Imperial Council, which Emperor Franz Joseph sanctioned on 28 February 1882.[19] Each section was entirely independent of the other, and enjoyed equal status. The two universities shared medical and scientific institutes, the old insignia, aula, library, and botanical garden, but common facilities were administrated by the German University. The first rector of the Czech University became Václav Vladivoj Tomek.		In 1890, the Royal and Imperial Czech Charles Ferdinand University had 112 teachers and 2,191 students and the Royal and Imperial German Charles Ferdinand University had 146 teachers and 1,483 students. Both universities had three faculties; the Theological Faculty remained the common until 1891, when it was divided as well. In the winter semester of 1909–10 the German Charles-Ferdinand University had 1,778 students; these were divided into: 58 theological students, for both the secular priesthood and religious orders; 755 law students; 376 medical; 589 philosophical. Among the students were about 80 women. The professors were divided as follows: theology, 7 regular professors, 1 assistant professor, 1 docent; law, 12 regular professors, 2 assistant professors, 4 docents; medicine, 15 regular professors, 19 assistant, 30 docents; philosophy, 30 regular professors, 8 assistant, 19 docents, 7 lecturers. The Czech Charles-Ferdinand University in the winter semester of 1909–10 included 4,319 students; of these 131 were theological students belonging both to the secular and regular clergy; 1,962 law students; 687 medical; 1,539 philosophical; 256 students were women. The professors were divided as follows: theological faculty, 8 regular professors, 2 docents; law, 12 regular, 7 assistant professors, 12 docents; medicine, 16 regular professors, 22 assistant, 24 docents; philosophy, 29 regular, 16 assistant, 35 docents, 11 lecturers.[9]		The high point of the German University was the era preceding the First World War, when it was home to world-renowned scientists such as physicist and philosopher Ernst Mach, Moritz Winternitz and Albert Einstein. In addition, the German-language students included prominent individuals such as future writers Max Brod, Franz Kafka, and Johannes Urzidil.[20] The "Lese- und Redehalle der deutschen Studenten in Prag" ("Reading and Lecture Hall of the German students in Prague"), founded in 1848, was an important social and scientific centre. Their library contained in 1885 more than 23,519 books and offered 248 scientific journals, 19 daily newspapers, 49 periodicals and 34 papers of entertainment. Regular lectures were held to scientific and political themes.		Even before the Austro-Hungarian Empire was abolished in late 1918, to be succeeded by Czechoslovakia, Czech politicians demanded that the insignia of 1348 were exclusively to be kept by the Czech university.[citation needed] The Act No. 197/1919 Sb. z. a n. established the Protestant Theological Faculty, but not as a part of the Charles University.[21] That changed on 10 May 1990, when it finally became a faculty of the university.[22] In 1920, the so-called Lex Mareš (No. 135/1920 Sb. z. a n.) was issued, named for its initiator, professor of physiology František Mareš, which determined that the Czech university was to be the successor to the original university.[23] Dropping the Habsburg name Ferdinand, it designated itself Charles University, while the German university was not named in the document, and then became officially called the German University in Prague (German: Deutsche Universität Prag).[24][25]		In 1921 the Germans considered moving[26] their university to Liberec (German: Reichenberg), in northern Bohemia. In 1930, about 42,000 inhabitants of Prague spoke German as their native language, while millions lived in northern Bohemia near the border with Germany.		In October 1932, after Naegle's death, the Czechs started again a controversy over the insignia. Ethnic tensions intensified, although some professors of the German University were members of the Czechoslovak government. Any agreement to use the insignia for both the universities was rejected.[citation needed] On 21 November 1934, the German University had to hand over the insigniae to the Czechs. The German University senate sent a delegation to Minister of Education Krčmář to protest the writ. At noon on 24 November 1934, several thousand students of the Czech University protested in front of the German university building. The Czech rector Karel Domin gave a speech urging the crowd to attack, while the outnumbered German students tried to resist. Under the threat of violence, on 25 November 1934 rector Otto Grosser (1873–1951) handed over the insigniae. These troubles of 1934 harmed relations between the two universities and nationalities.		The tide turned in 1938 when, following the Munich Agreement, German troops entered the border areas of Czechoslovakia (the so-called Sudetenland), as did Polish and Hungarian troops elsewhere. On 15 March 1939 Germans forced Czecho-Slovakia to split apart and the Czech lands were occupied by Nazis as the Protectorate of Bohemia and Moravia. Reichsprotektor Konstantin von Neurath handed the historical insigniae to the German University, which was officially renamed Deutsche Karls-Universität in Prag. On 1 September 1939 the German University was subordinated to the Reich Ministry of Education in Berlin and on 4 November 1939 it was proclaimed to be Reichsuniversität.[27]		On 28 October 1939, during a demonstration, Jan Opletal was shot. His burial on 15 November 1939 became another demonstration.[28] On 17 November 1939 (now marked as International Students' Day) the Czech University and all other Czech institutions of higher learning were closed, remaining closed until the end of the War. Nine student leaders were executed and about 1,200 Czech students were interned in Sachsenhausen and not released until 1943. About 20[29] or 35[30] interned students died in the camp. On 8 May 1940 the Czech University was officially renamed Czech Charles University (Czech: Česká universita Karlova) by government regulation 188/1940 Coll.		World War II marks the end of the coexistence of the two universities in Prague.		Although the university began to recover rapidly after 1945, it did not enjoy academic freedom for long. After the communist coup in 1948, the new regime started to arrange purges and repress all forms of disagreement with the official ideology, and continued to do so for the next four decades, with the second wave of purges during the "normalization" period in the beginning of the 1970s.[31] Only in the late 1980s did the situation start to improve; students organized various activities and several peaceful demonstrations in the wake of the Revolutions of 1989 abroad.[32] This initiated the "Velvet Revolution" in 1989, in which both students and faculty of the university played a large role. Václav Havel—a writer, dramatist and philosopher was recruited from the independent academic community and appointed president of the republic in December 1989.		Charles University does not have one joint campus. The academic facilities occupy many locations throughout the city of Prague and three of the faculties are located in other cities (two of them in Hradec Králové, one in Pilsen). The historical main building from the 14th century called Carolinum is situated in the Old Town of Prague and constitutes the university's center. It is the seat of the rector and of the Academic Senate of Charles university. Carolinum is also the venue for official academic ceremonies such as matriculations or graduations.		The Botanical Garden of Charles University, maintained by its Faculty of Science, is located in the New Town.		Among the four original faculties of Charles University were: the faculty of law, medicine, art (philosophy) and theology (now catholic theology). Today, Charles University consists of 17 faculties, based primarily in Prague, two houses in Hradec Králové and one in Plzeň.		Together with Academy of Sciences of the Czech Republic, Charles University founded the prestigious economics institute CERGE-EI.		According to Academic Ranking of World Universities (Shanghai Ranking), Charles University ranked in the upper 1.5 percent of the world’s best universities in 2011. It came 201st to 300th out of 17,000 universities worldwide.[4] It is the best university in the Czech Republic and one of the best universities in Central and Eastern Europe only overtaken by Russian Lomonosov Moscow State University at 74th place.[33][34] It was placed 31st in Times BRICS & Emerging Economies Rankings 2014 (after 23rd University of Warsaw).[35]		It was ranked in 2013 as 201–300 best in the World among 500 universities evaluated by Academic Ranking of World Universities (Shanghai Ranking), 233rd among 500 in QS World University Rankings, 351–400 among 400 universities in Times Higher Education World University Rankings and 485th in CWTS Leiden Ranking of 500 universities. Earlier rankings are presented in following table:[36]		Rector of the University Václav Hampl said in 2008: “I am very pleased that Charles University achieved such a great success and I would like to thank to all who have contributed to it. An overwhelming majority of schools with a similar placement like Charles University have incomparably better financing and therefore this success is not only a reflection of professional qualities of our academics but also their personal efforts and dedication.“[37]		According to QS Subject Ranking is Charles University among 51–100 best universities in world in geography and linguistics.[38]		In Germany the Charles University in Prague cooperates with the Goethe-University in Frankfurt/Main. Both cities are linked by a long-lasting partnership agreement.[40]		Coordinates: 50°05′18″N 14°24′13″E﻿ / ﻿50.0884°N 14.4037°E﻿ / 50.0884; 14.4037		
University of St Andrews		St Mary's College		School of Medicine		St Leonard's College		The University of St Andrews (informally known as St Andrews University or simply St Andrews; abbreviated as St And, from the Latin Sancti Andreae, in post-nominals) is a British public research university in St Andrews, Fife, Scotland. It is the oldest of the four ancient universities of Scotland and the third oldest university in the English-speaking world (following Oxford and Cambridge). St Andrews was founded between 1410 and 1413, when the Avignon Antipope Benedict XIII issued a papal bull to a small founding group of Augustinian clergy.		St Andrews is made up from a variety of institutions, including three constituent colleges (United College, St Mary's College, and St Leonard's College) and 18 academic schools organised into four faculties.[5] The university occupies historic and modern buildings located throughout the town. The academic year is divided into two terms, Martinmas and Candlemas. In term time, over one-third of the town's population is either a staff member or student of the university.[6] The student body is notably diverse: over 135 nationalities are represented with 45% of its intake from countries outside the UK; about one-eighth of the students are from the rest of the EU and the remaining third are from overseas — 15% from North America alone.[7][8] The university's sport teams compete in BUCS competitions,[9] and the student body is known for preserving ancient traditions such as Raisin Weekend, May Dip, and the wearing of distinctive academic dress.[10]		It is ranked as the third best university in the United Kingdom in national league tables, behind Oxbridge.[11][12][13] The Guardian ranks first in the United Kingdom the Schools of Physics and Astronomy, International Relations, Computer Science, Geography, English and Mathematics,[12] whilst The Times and Sunday Times ranks the Schools of English, Management, Philosophy, Anatomy and Physiology and Middle Eastern and African Studies first[14] and the Complete University Guide ranks Management, Divinity and Middle Eastern and African Studies first.[15] The Times Higher Education World Universities Ranking names St Andrews among the world's Top 50 universities for Social Sciences, Arts and Humanities.[16][17] St Andrews has the highest student satisfaction (joint first) amongst all multi-faculty universities in the United Kingdom.[18]		St Andrews has many notable alumni and affiliated faculty, including eminent mathematicians, scientists, theologians, philosophers, and politicians. Recent alumni include the former First Minister of Scotland Alex Salmond; Secretary of State for Defence Michael Fallon; HM British Ambassador to China Barbara Woodward; United States Ambassador to Hungary Colleen Bell; Olympic cycling gold medalist Chris Hoy; and royals Prince William, Duke of Cambridge, and Catherine, Duchess of Cambridge. Six Nobel Laureates are among St Andrews' alumni and former staff: two in Chemistry and Physiology or Medicine, and one each in Peace and Literature.						The university was founded in 1410 when a group of Augustinian clergy, driven from the University of Paris by the Avignon schism and from the universities of Oxford and Cambridge by the Anglo-Scottish Wars, formed a society of higher learning in St Andrews, which offered courses of lectures in divinity, logic, philosophy, and law. A charter of privilege was bestowed upon the society of masters and scholars by the Bishop of St Andrews, Henry Wardlaw, on 28 February 1411.[19] Wardlaw then successfully petitioned the Avignon Pope Benedict XIII to grant the school university status by issuing a series of papal bulls, which followed on 28 August 1413.[20] King James I of Scotland confirmed the charter of the university in 1432. Subsequent kings supported the university with King James V "confirming privileges of the university" in 1532.[21][22]		A college of theology and arts called St John's College was founded in 1418[23] by Robert of Montrose and Lawrence of Lindores. St Salvator's College was established in 1450, by Bishop James Kennedy.[24] St Leonard's College was founded in 1511 by Archbishop Alexander Stewart, who intended it to have a far more monastic character than either of the other colleges. St John's College was refounded by Cardinal James Beaton under the name St Mary's College in 1538 for the study of divinity and law. It was intended to encourage traditional Catholic teachings in opposition to the emerging Scottish Reformation, but once Scotland had formally split with the Papacy in 1560, it became a teaching institution for Protestant clergy.[25] Some university buildings that date from this period are still in use today, such as St Salvator's Chapel, St Leonard's College Chapel and St Mary's College quadrangle. At this time, the majority of the teaching was of a religious nature and was conducted by clerics associated with the cathedral.		During the 17th and 18th centuries, the university had mixed fortunes and was often beset by civil and religious disturbances. In a particularly acute depression in 1747, severe financial problems triggered the dissolution of St Leonard's College, whose properties and staff were merged into St Salvator's College to form the United College of St Salvator and St Leonard.[19] Throughout this period student numbers were very low; for instance, when Samuel Johnson visited the university in 1773, the university had fewer than 100 pupils, and was in his opinion in a steady decline. He described it as "pining in decay and struggling for life".[26] The poverty of Scotland during this period also damaged St Andrews, as few were able to patronise the university and its colleges, and with state support being improbable, the income they received was scarce.		In the second half of the 19th century, pressure was building upon universities to open up higher education to women.[27] In 1876, the University Senate decided to allow women to receive an education at St Andrews at a level roughly equal to the Master of Arts degree that men were able to take at the time. The scheme came to be known as the 'L.L.A. examination' (Lady Literate in Arts). It required women to pass five subjects at an ordinary level and one at honours level and entitled them to hold a degree from the university.[28] In 1889 the Universities (Scotland) Act[29] made it possible to formally admit women to St Andrews and to receive an education equal to that of male students. Agnes Forbes Blackadder became the first woman to graduate from St Andrews on the same level as men in October 1894, gaining her MA. She entered the university in 1892, making St Andrews the first university in Scotland to admit female undergraduates on the same level as men.[30] In response to the increasing number of female students attending the university, the first women's hall was built in 1896 and was named University Hall.[31]		Up until the start of the 20th century, St Andrews offered a traditional education based on classical languages, divinity and philosophical studies, and was slow to embrace more practical fields such as science and medicine that were becoming more popular at other universities. In response to the need for modernisation and in order to increase student numbers and alleviate financial problems, the university merged with University College, Dundee in 1897, which had a focus on scientific and professional subjects. After the incorporation of University College Dundee, St Andrews' various problems generally receded. Of note is that, up until 1967, many students who obtained a degree from the University of St Andrews had in fact spent most, and sometimes all, of their undergraduate career based in Dundee.		As the 20th century progressed, it became increasingly popular among the Scottish upper classes to send their children to the country's oldest higher learning institution,[citation needed] and the university's student population rose sharply.[tone] This revival has been maintained to the present day.[tone] Despite this, there have been some notable changes.[citation needed] In 1967, the union with University College Dundee ended, when that College became an independent institution under the name of the University of Dundee. As a result of this, St Andrews lost its capacity to provide degrees in many areas such as Law, Accountancy, Dentistry and Engineering, while it also lost the right to confer the undergraduate medical degree MBChB. However, the university has prospered in other ways.[citation needed][tone] In 1972, the College of St Leonard was reconstituted as a postgraduate institute.[32]		St Andrews' historical links with the United States predate the country's independence. One of the signatories of the Declaration of Independence attended (but did not graduate from) St Andrews. James Wilson was one of six original justices appointed by George Washington to the Supreme Court of the United States and founder of the University of Pennsylvania Law School. Other prominent American figures associated with St Andrews include Scottish American industrialist Andrew Carnegie, who was elected Rector in 1901 and whose name is given to the prestigious Carnegie Scholarship, and Edward Harkness, an American philanthropist who in 1930 provided for the construction of St Salvator's Hall. American Bobby Jones, co-founder of the Augusta National Golf Club and the Masters Tournament, was named a Freeman of the City of St Andrews in 1958, becoming only the second American to be so honored, the other being Benjamin Franklin in 1759.[33] Today a highly competitive scholarship exchange, The Robert T. Jones Scholarship, exists between St Andrews and Emory University in Atlanta.		Links with the United States have been maintained into the present day and continue to grow. In 2009, Louise Richardson, an Irish-American political scientist specialized in the study of terrorism, was drawn from Harvard to serve as the first female Principal and Vice Chancellor of St Andrews.[34] She later went on to her next appointment as the Vice Chancellor to the University of Oxford.[35]		Active recruitment of students from North America first began in 1984, with Americans now making up around 1 in 6 of the student population in 2017.[36] Students from almost every state in the United States and province in Canada are represented.[7][37] This is the highest proportion and absolute number of American students amongst all British universities.[38][39] Media reports indicate growing numbers of American students are attracted to the university's academics, traditions, prestige, internationalism, and comparatively low tuition fees.[40][41][42][43] The university also regularly features as one of the few non-North American universities in the Fiske Guide to Colleges, an American college guide, as a 'Best Buy'.[44][45] St Andrews has developed a sizable alumni presence in the United States, with over 8000 alumni spread across all 50 states.[46] Most major cities host alumni clubs, the largest of which is in New York.[47] Both London and New York also host the St Andrews Angels, an alumni led angel investment network, which centres upon the wider university communities in both the United Kingdom and United States.[48] St Andrews has also established relationships with other university alumni clubs and private membership clubs in the United States to provide alumni with social and networking opportunities. For example, alumni are eligible for membership at the Princeton Club of New York and the Algonquin Club in Boston.[49][50]		In 2013, Hillary Clinton, former United States Secretary of State, took part in the academic celebration marking the 600th anniversary of the founding of the University of St Andrews.[51] Clinton received an honorary degree of Doctor of Laws and provided the graduation address, in which she said, "I do take comfort from knowing there is a long tradition of Americans being warmly welcomed here at St Andrews. Every year I learn you educate more than one thousand American students, exposing them to new ideas and perspectives as well as according them with a first class education. I’ve been proud and fortunate to hire a few St Andrews alumni over the years and I thank you for training them so well."[52]		As with the other ancient universities of Scotland, the governance of the university is determined by the Universities (Scotland) Act 1858. This Act created three bodies: the General Council, University Court and Academic Senate (Senatus Academicus).		The General Council is a standing advisory body of all the graduates, academics and former academics of the university. It meets twice a year and appoints a business committee to manage business between these meetings. Its most important functions are to appoint two assessors to the University Court and elect the university's chancellor.		The University Court is the body responsible for administrative and financial matters, and is in effect the governing body of the university. It is chaired by the rector, who is elected by the matriculated students of the University. Members are appointed by the General Council, Academic Senate and Fife Council. The President of the Students' Representative Council and Director of Representation are ex officio members of the Court. Several lay members are also co-opted and must include a fixed number of alumni of the University.		The Academic Senate (Latin Senatus Academicus) is the supreme academic body for the university. Its members include all the professors of the university, certain senior readers, a number of senior lecturers and lecturers and three elected student senate representatives – one from the arts and divinity faculty, one from the science and medicine faculty and one postgraduate student. It is responsible for authorising degree programmes and issuing all degrees to graduates, and for managing student discipline. The President of the Senate is the University Principal.		The Principal is the chief executive of the university and is assisted in that role by several key officers, including the Deputy Principal, Master of the United College and Quaestor. The principal has responsibility for the overall running of the university and presides over the University Senate.[53]		In Scotland, the position of rector exists at the four ancient universities (St Andrews, Glasgow, Aberdeen and Edinburgh) – as well as the University of Dundee. The post was made an integral part of these universities by the Universities (Scotland) Act 1889. The Rector of the University of St Andrews chairs meetings of the University Court, the governing body of the university; and is elected by the matriculated student body to ensure that their needs are adequately considered by the university's leadership. Through St Andrews' history a number of notable people have been elected to the post, including the actor John Cleese, industrialist and philanthropist Andrew Carnegie, author and poet Rudyard Kipling and the British Prime Minister Archibald Primrose, 5th Earl of Rosebery.[54]		The university encompasses three colleges, although their purpose is mainly ceremonial as students are housed in separate residential halls or private accommodations. United College has responsibility for all students in the faculties of arts, sciences and medicine, and is based around St Salvator's Quadrangle;[55] St Mary's College has responsibility for all students studying in the Faculty of Divinity, and has its own dedicated site in St Mary's Quadrangle;[56] and St Leonard's College, in its current incarnation, has responsibility for all postgraduate students.[57]		The four academic faculties collectively encompass 18 schools. A dean is appointed by the Master of the United College to oversee the day-to-day running of each faculty. Students apply to become members of a particular faculty, as opposed to the school within which teaching is based. The faculties and their affiliated schools are:		Certain subjects are offered both within the Faculties of Arts and Sciences, the six subjects are: economics, geography, management, mathematics, psychology and sustainable development. The content of the subject is the same regardless of the faculty.[62]		The academic year at St Andrews is divided into two semesters, Martinmas and Candlemas, named after two of the four Scottish Term and Quarter Days. Martinmas, on 11 November, was originally the feast of Saint Martin of Tours, a 4th-century bishop and hermit. Candlemas originally fell on 2 February, the day of the feast of the Purification, or the Presentation of Christ. Martinmas semester runs from early September until mid-December, with examinations taking place just before the Christmas break. There follows an inter-semester period when Martinmas semester business is concluded and preparations are made for the new Candlemas semester, which starts in January and concludes with examinations at the end of May. Graduation is celebrated at the end of June.[63]		In a ranking conducted by The Guardian, St Andrews is placed 5th in the UK for national reputation behind Oxford, Cambridge, Imperial & LSE.[74] When size is taken into account, St Andrews ranks second in the world out of all small to medium-sized fully comprehensive universities (after Brown University) using metrics from the QS Intelligence Unit in 2015.[75] The 2008 Research Assessment Exercise (RAE 2008) ranked St Andrews as 16th by grade point average and quality index across the units of assessment it submitted.[76] St Andrews was ranked 9th overall in The Sunday Times 10-year (1998–2007) average ranking of British universities based on consistent league table performance,[77] and is a member of the 'Sutton 13' of top ranked Universities in the UK.[78]		Nearly 86% of its graduates obtain a First Class or an Upper Second Class Honours degree.[79] The ancient Scottish universities award Master of Arts degrees (except for science students who are awarded a Bachelor of Science degree) which are classified upon graduation, in contrast to Oxbridge where one becomes a Master of Arts after a certain number of years, and the rest of the UK, where graduates are awarded BAs. These can be awarded with honours; the majority of students graduate with honours.		St Andrews is placed 7th in the UK (1st in Scotland) for the employability of its graduates as chosen by recruiters from the UK's major companies[80] with graduates expected to have the best graduate prospects and highest starting salaries in Scotland as ranked by The Times and Sunday Times Good University Guide 2016 and 2017.[81] An independent report conducted by Swedish investment firm, Skandia found that St Andrews is, despite its small undergraduate body, the joint-5th best university in the UK for producing millionaires.[82] A study by High Fliers confirmed this by reporting that the university also features in the top 5 of UK universities for producing self-made millionaires.[83] According to a study by the Institute of Employment Research, St Andrews has produced more directors of FTSE 100 companies in proportion to its size than any other educational institution in Britain.[84]		The Times and Sunday Times Good University Guide 2017 revealed that 24 of the 26 subjects offered by St Andrews ranked within the top 6 nationally with 10 subjects placing within the top 3 including English, Management, Philosophy, International Relations, Italian, Physics and Astronomy and Classics and Ancient History.[85] In the 2018 Complete University Guide, 23 out of the 25 subjects offered by St Andrews rank within the top 10 nationally making St Andrews one of only four multi-faculty universities (along with Cambridge, Oxford and Durham) in the UK to have over 90% of their subjects in the top 10.[86] The Guardian University Guide 2016 ranked Computer Science, Geography, International Relations and Divinity first in the UK. Chemistry, History, Philosophy, History of Art, Physics, English and Earth and Marine Science were ranked within the top three whilst Management, Classics and Mathematics placed within the top five.[87] In the 2015-16 Times Higher Education World University Rankings, St Andrews is ranked 46th in the world for Social Sciences,[16] 50th in the world for Arts and Humanities[17] and 74th in the world for Life Sciences.[88] The 2014 CWTS Leiden rankings, which "aims to provide highly accurate measurements of the scientific impact of universities", placed St Andrews 39th in the world, ranking it 5th domestically.[89] The philosophy department is ranked 14th worldwide (4th in Europe) in the 2015 QS World University Rankings[90] whilst the graduate programme was ranked 17th worldwide (2nd in the UK) by the 2009 Philosophical Gourmet's biennial report on Philosophy programs in the English-speaking world.[91]		The university receives applications mainly through UCAS and the Common Application with the latest figures showing that there are generally 12 applications per undergraduate place available with some schools - 'International Relations' and 'Economics and Finance' - seeing 30 applicants per place.[95][96][97] The university is one of the most competitive universities in the UK, with 2012-13 having an acceptance rate of 7.87% and offer rate of 18.4% for Scottish/EU applicants where places are capped by the Scottish Government.[98][99] The standard offer of a place tends to require five best Highers equivalent to AAAAB, three best A-levels equivalent to AAA or a score of at least 38 points on the International Baccalaureate.[100] Successful entrants have, on average, 525 UCAS points (the equivalent of just above A*A*AA at A Level) ranking it as the 5th highest amongst higher education institutions in the UK for the 2015 admissions cycle[101] with The Telegraph naming it as the hardest university to gain admission into in Scotland.[102]		The university has one of the smallest percentages of students (13%) from lower income backgrounds, out of all higher education institutions in the UK.[103][104] Around 40% of the student body is from independent schools[105] and the university hosts the highest proportion of financially independent students (58%) in the UK.[106] The university participates in widening access schemes such as the Sutton Trust Summer School, First Chances Programme, REACH & SWAP Scotland, and Access for Rural Communities (ARC) in order to promote a more widespread uptake of those traditionally under represented at university.[107] In the seven-year period between 2008 and 2015, the number of pupils engaged with annual outreach programmes at the university has increased by about tenfold whilst the number of students arriving at St Andrews from the most deprived backgrounds has increased by almost 50 per cent in the past year of 2015.[108] The university has a slightly higher proportion of female than male students with a male to female ratio of 44:56 in the undergraduate population.[109]		To commemorate the university's 600th anniversary the 600th Lecture Series was commissioned in 2011, which brought diverse speakers such as former Prime Minister Gordon Brown, naturalist David Attenborough and linguist Noam Chomsky to St Andrews.[110]		As part of the celebration of the 400th establishment of the King James Library, the King James Library lectures were initiated in 2009 on the subject of 'The Meaning of the Library'.[111]		The Andrew Lang Lecture series was initiated in 1927, and named for alumnus and poet Andrew Lang. The most famous lecture in this series is that given by J. R. R. Tolkien in March 1939, entitled 'Fairy Stories', but published subsequently as 'On Fairy-Stories'.[112]		The computing Distinguished Lecture Series was initiated in 1969 by Jack Cole.[113]		St Andrews has developed student exchange partnerships with universities around the globe, though offerings are largely concentrated in North America, Europe, and Asia. Exchange opportunities vary by School and eligibility requirements are specific to each exchange program.[114]		In North America, the highly competitive Bachelor of Arts International Honours program, run in conjunction with The College of William and Mary in Williamsburg, Virginia, allows students studying International Relations, English, History, or Economics to spend two years at each institution and earn a joint degree from both.[115] The Robert T. Jones Memorial Trust funds the Robert T. Jones Jr. Scholarship, which allows select St Andrews students to study, fully funded, for a year at Emory University in Atlanta, and Western University and Queen's University in Canada.[116] The Robert Lincoln McNeil Scholarship allows students to study at the University of Pennsylvania. One of the largest North American exchanges is with the University of California System, in which students can study at Berkeley, Los Angeles (UCLA), Santa Cruz (UCSC) and San Diego (UCSD). Other North American partners offering multiple exchanges include the University of Virginia, the University of North Carolina at Chapel Hill, Washington and Lee University, Elon University, and the University of Toronto.[114] Some exchanges are offered within specific research institutes at St Andrews, rather than across entire Schools. For example, the Handa Centre for the Study of Terrorism and Political Violence, within the School of International Relations, offers student exchanges in partnership with the School of Foreign Service at Georgetown University.[117]		St Andrews participates in the Erasmus Programme and has direct exchanges with universities across Europe.[114] For example, in France exchanges are offered at the Sorbonne, Sciences Po, and University of Paris VI. In the Netherlands students can study at Leiden University and Utrecht University. Narrower exchanges include those with the University of Copenhagen, the University of Oslo, and Trinity College Dublin. Exchanges are also available for postgraduate research students, such as the opportunity for social scientists to study at the European University Institute in Florence, Italy.		More recently, St Andrews has developed exchanges with partners in Asia and Australia.[114] Notable partners include the University of Hong Kong and Renmin University of China, National University of Singapore, and the University of Melbourne in Australia.		The University of St Andrews is situated in the small town of St Andrews in rural Fife, Scotland. The University has teaching facilities, libraries, student housing and other buildings spread throughout the town. Generally, university departments and buildings are concentrated on North Street, South Street, The Scores, and the North Haugh. The university has two major sites within the town. The first is the United College, St Andrews (also known as the Quad or St Salvator's) on North Street, which functions both as a teaching space and venue for student events, incorporating the Departments of Social Anthropology and Modern Languages. The second is St Mary's College, St Andrews, based on South Street, which houses the Schools of Divinity, Psychology and Neuroscience, as well as the King James Library. Several schools are located on The Scores including Classics, English, History, Philosophy, the School of Economics and Finance, and International Relations, as well as the Admissions department, the Museum of the University of St Andrews, and the Principal's residence, University House. North Street is also the site of several departments including, the Principal's Office, The Younger Hall, Department of Film Studies, and the University Library. The North Haugh is principally home to the Natural Sciences such as Chemistry, Physics, Biology, as well as Mathematics, Computer Science, Medicine and the School of Management.		The University of St Andrews maintains one of the most extensive university library collections in the United Kingdom, which includes significant holdings of books, manuscripts, muniments and photographs. The library collection contains over a million volumes and over two hundred thousand rare and antique books.[118]		The university library was founded by King James VI in 1612, with the donation of 350 works from the royal collection, at the urging of George Gledstanes, the then chancellor of St Andrews, although the libraries of the colleges of St Leonard's College, St Salvator's College and St Mary's College had existed prior to this.[119][120] From 1710 to 1837 the library functioned as a legal deposit library, and as a result has an extensive collection of 18th century literature.[121]		The library's main building is located on North Street, and houses over 1,000,000 books.[122] The library was designed by the leading firm of architects Faulkner-Brown Hendy Watkinson Stonor led by Harry Faulkner-Brown and based in the North East England at Killingworth.[123] Faulkner-Brown specialised in libraries and leisure facilities and also designed the National Library of Canada in Ottawa and the Robinson Library at Newcastle University [124][125] In 2011 the main library building underwent a £7 million re-development.[126] The historic King James library, built in 1643, houses the university's Divinity and Medieval history collections.[127]		In 2012 the University purchased the vacant Martyrs' Church on North Street, with the purpose of providing reading rooms for the Special Collections department and University research students and staff.[128]		The University maintains several museums and galleries, open for free to the public.[129] The Museum of the University of St Andrews (MUSA) opened in 2008 and displays some highlights of the university's extensive collection of over 100,000 artefacts.[130] It displays objects relating both to the history of the university, such as its collection of 15th century maces,[131] and also unrelated objects, such as paintings by John Opie, Alberto Morrocco and Charles Sims.[132] Several of the university's collections have been recognised as being of 'national significance for Scotland' by Museums Galleries Scotland.[133]		The Bell Pettigrew Museum houses the University's natural history collections. Founded in 1912, it is housed in the old Bute Medical School Building in St. Mary's Quad. Among its collections are the remains of several extinct species such as the dodo and Tasmanian tiger as well as fossilised fish from the nearby Dura Den, Fife, which when found in 1859 stimulated the debate on evolution.[134]		The University has two collegiate chapels. The chapel of St Salvator's (or "Sallies" as it is affectionately known) was founded in 1450 by Bishop James Kennedy, and today it is a centre of university life.[135] St Salvator's has a full peal of six bells, and is therefore the only university chapel in Scotland suitable for change ringing.[136] The Chapel of St Leonard's is located in the grounds of the nearby St Leonards School. It is the university's oldest building, some parts dating from 1144[137] and is the smaller of the two chapels. St Salvator's and St Leonard's both have their own choirs, whose members are drawn from the student body.		St Andrews is characterised amongst Scottish universities as having a significant number of students who live in university-maintained accommodation. As of 2012, 52% of the student population live in university halls.[138] The halls vary widely in age and character, the oldest, Deans Court dates from the 12th century, and the newest, David Russell Apartments, was built in 2004.[139] They are built in styles from gothic revival to brutalist. All are now co-educational and non-smoking, and several are catered.[140] The University guarantees every first year student a place of accommodation, and many students return to halls in their second, third and final years at St Andrews.[141] From September 2015 onwards, students will have the option of living in alcohol-free flats in David Russell Apartments on the grounds of medical conditions that do not allow drinking or for religious reasons.[142]		Halls of residence include:		Since 2013, the university's endowment has been invested under the United Nations Principles of Responsible Investment (UNPRI) initiative with a sustainable ethical policy enforced since 2007.[143] The university has the target of being the UK's first carbon neutral university and has invested in creating two new macro-scale renewable energy sites.		The Guardbridge Biomass Energy Centre will generate power using locally sourced wood-fuelled biomass, hot water will be transported to the university through underground pipes to heat and cool laboratories and student residences.[144] The £25 million project is expected to save 10,000 tonnes of carbon annually and the university aims to establish the site as a knowledge exchange hub which would provide "missing link" facilities to allow research and discoveries made in university labs to be translated to working prototypes. Work began onsite in 2014 and the centre is expected to be operational by December 2015.[145]		In October 2013, the university received permission to build six medium-sized turbines at Kenly Wind Farm, near Boarhills. The wind farms are expected to be operational by 2017 and will bring an estimated £22 million boost to the local and national economy with 19,000 tonnes of carbon saved annually.[146]		The University of St Andrews Students' Association is the organisation which represents the student body of the University of St Andrews.[147] It was founded in 1885 and comprises the Students' Representative Council (SRC)[148] and the Students' Union Council. The Students' Association has 9 subcommittees: The Entertainments "Ents" Committee, Societies Committee, Charities Campaign, Union Debating Society, STAR (St Andrews Radio), Mermaids Productions, LGBT, Design Team, SVS (Student Voluntary Service). Every matriculated student is automatically a member of each subcommittee.		The Students' Association Building (informally known as the Union) is located on St Mary's Place, St Andrews. Union facilities include a Blackwells bookshop, several bars and the University's Student Support Services. In 2013 the Students' Association Building is scheduled to undergo a £12 million refurbishment.[149] The Students' Association is affiliated to, and a founding member of, the Coalition of Higher Education Students in Scotland but unlike many other students' unions in the UK is not a member of the National Union of Students, having most recently rejected membership in a referendum in November 2012.		St Andrews is home to over 140 student societies which cover a wide range of interests.[150] In 2015, the university ranked top in Scotland and 3rd in the UK for Clubs & Societies by university review platform, StudentCrowd.[151]		All matriculated students are members of the "Union Debating Society", a student debating society that holds weekly public debates in Lower Parliament Hall, often hosts notable speakers, and participates in competitive debating in both national and international competitions. Founded in 1794, it claims to be the oldest continuously-run student debating society in the world.[152]		There is a strong tradition of student media at St Andrews. The university's two newspapers are The Saint, a fortnightly publication and The Stand, an online publication founded in 2011.[153] There is also the Foreign Affairs Review ran by the Foreign Affairs Society.[154] There are also a number of smaller student publications including The Tribe, a student-run magazine[155] and The Regulus, a student magazine focusing on politics and current affairs.[156] In addition to this there are several student-led academic journals, most notably, Stereoscope Magazine which is focused on student photography and raising awareness of the university's historic photographic collection,[157] Ha@sta, an annual journal for those interested in art history,[158] Aporia, the journal of the Philosophy Society,[159] and the Postgraduate Journal of Art History and Museum Studies.[160] The University's radio station is STAR radio, an online station that broadcasts 24/7 during term time.[161] The Sinner is an independent website and discussion forum set up by students of the university.[162]		The university's Music Society comprises many student-run musical groups, including the university's flagship symphony orchestra, wind band, and chorus. One of the oldest choirs in the university is the St Andrews University Madrigal Group which performs a concert each term and has an annual summer tour. The A Cappella Society represents all four a cappella groups at St Andrews: The Other Guys, The Alleycats, The Accidentals and The Hummingbirds. From 2009–2011, all four of these groups participated in The Voice Festival UK(VF-UK) competition, and The Other Guys, The Accidentals and The Alleycats all reached the London final.[163] In 2011, The Other Guys released a music video onto YouTube, entitled Royal Romance, a tribute to the wedding of Prince William and Catherine Middleton, which earned them significant recognition in both Scottish and international media.		Student theatre at the University of St Andrews is funded by the Mermaids Performing Arts fund. There are regular dramatic and comedic performances staged at the Barron theatre.[164] Blind Mirth is the university's improvisational theatre troupe, which performs weekly in the town, and annually takes a production to the Edinburgh Fringe Festival.[165]		The Kate Kennedy Club plays a significant role in the life of the university, maintaining university traditions such as the Kate Kennedy Procession, in which students parade through the town dressed as eminent figures from the university's history, and organising social events such as the Opening and May balls. Founded in 1926, the club is composed of around thirty matriculated students, who are selected by the club's members. The club has received criticism from the university's principal, Louise Richardson, and alumna the Duchess of Cambridge Kate Middleton over its previously male-only admission policy.[166][167] In 2012, the club decided to allow female students to join.[168][169]		The University of St Andrews Athletic Union is the student representative body for sport. Established in 1901, it is affiliated to BUCS and encompasses around sixty sport clubs,[170] who compete at both a recreational and high-performance level.[171] A notable club is the University of St Andrews Rugby Football Club, which played a pivotal role in shaping the sport and has produced Scottish international players such as J. S. Thomson and Alfred Clunies-Ross.[172] The university is currently undergoing through a £14 million five-phase development of the student sport centre which will include a new 400-seat eight-court sports hall, a new reception area and expanded gym facilities.[173] The Scottish Varsity, also known as the "world's oldest varsity match", is played annually against the University of Edinburgh.[174]		In order to become a student at the university[175] a person must take an oath in Latin at the point of matriculation, called the Sponsio Academica, although this tradition now has been digitised and is agreed to as part of an online matriculation process.		Nos ingenui adolescentes, nomina subscribentes, sancte pollicemur nos preceptoribus obsequium debitum exhibituros in omnibus rebus ad disciplinam et bonos mores pertinentibus, Senatus Academici autoritati obtemperaturos, et hujus Academiae Andreanae emolumentum et commodum, quantum in nobis sit, procuraturos, ad quemcunque vitae statum pervenerimus. Item agnoscimus si quis nostrum indecore turbulenterve se gesserit vel si parum diligentem in studiis suis se praebuerit neque admonitus se in melius correxerit eum licere Senatui Academico vel poena congruenti adficere vel etiam ex Universitate expellere.		In English:		We students who set down our names hereunder in all good faith make a solemn promise that we shall show due deference to our teachers in all matters relating to order and good conduct, that we shall be subject to the authority of the Senatus Academicus and shall, whatever be the position we attain hereafter, promote, so far as lies in our power, the profit and the interest in our University of St Andrews. Further, we recognise that, if any of us conducts ourselves in an unbecoming or disorderly manner or shows insufficient diligence in their studies and, though admonished, does not improve, it is within the power of the Senatus Academicus to inflict on such students a fitting penalty or even expel them from the University.		One of the most conspicuous traditions at St Andrews is the wearing of academic dress, particularly the distinctive red undergraduate gown of the United College. Undergraduates in Arts and Science subjects can be seen wearing these garments at the installation of a Rector or Chancellor, at chapel services, on 'Pier Walks', at formal hall dinners, at meetings of the Union Debating Society, giving tours to prospective students and visitors as well as on St Andrews day and to sit examinations. Divinity students wear a black undergraduate gown with a purple saltitr cross on the left facing. Postgraduates wear the graduate gown or, as members of St Leonard College, may wear a black gown trimmed with burgundy, introduced for graduate students whose original university is without academic dress. . (See Academic dress of the University of St Andrews.) St Mary's College Post Graduates, however, wear their graduate gown with a purple saltire cross on the left facing.		Bejant is a term used to refer to first year male students; females being described as Bejantines. Second-year students are known as a Semis, a student in their third year may be referred to as a Tertian, and in their final year as a Magistrand. These terms are thought to be unique to St Andrews. When wearing their traditional red gowns, students in each year may be identified according to the way they wear their gowns. In the first year, the gown is worn on the shoulders, in the second year it is worn slightly off the shoulders. In the third year arts students wear their gowns off their left shoulders, and science students off their right shoulders. Finally, fourth years wear their gowns right down to their elbows, ready to shed their scarlet gowns for the black graduation gown. The gown is never to be joined at the top as this is considered bad luck.		The students of the University enjoy an unusual family tradition designed to make new students feel at home and build relationships within the student body. Traditionally, a Bejant or Bejantine acquires academic parents who are at least in their third year as students. These older students act as informal mentors in academic and social matters and it is not uncommon for such academic family ties to stretch well beyond student days. Tradition has it that a Bejant may ask a man to be his Senior Man but must be invited by a woman who is prepared to be his Senior Woman. Similarly, a Bejantine may ask a male to be her Senior Man but there is no overt rule regarding how she acquires a Senior Woman. The establishment of these relationships begins at the very start of the first semester – with the aim of being in place ahead of Raisin Weekend.[176]		Raisin Weekend celebrates the relationship between the Bejants/Bejantines (First-Year students) and their respective Academic Parents who, in St. Andrews' tradition, guide and mentor them in their time at the University. It is traditionally said that students went up to study with a sack of oatmeal and a barrel of salt-herring as staple foods to last them a term and that therefore anything more exotic was seen as a luxury. In return for the guidance from academic parents a further tradition sprang up of rewarding these "parents" with a pound of raisins. Since the 19th century the giving of raisins was steadily transformed into the giving of a more modern alternative – such as a bottle of wine. In return for the raisins or equivalent present the parents give their "children" a formal receipt – the Raisin Receipt – composed in Latin. Over time this receipt progressively became more elaborate and often humorous. The receipt can be written on anything and is to be carried everywhere by the Bejant/Bejantine on the morning of Raisin Monday until midday.[177]		Raisin Weekend is held annually over the last weekend of November. Affairs often begin with a tea party (or similar) thrown by the mother(s) and then a pub-crawl or house party led by the father(s). It is fairly common for several academic families to combine in the latter stages of the revels.[178] At midday all the First-Years gather in Quad of St Salvator's College to compare their receipts and also to be open to challenge from older students who may look for errors in the Latin of the receipt (an almost inevitable occurrence). Upon detection of such error(s) the bearer may be required to sing the Gaudie. In recent years the gathering has culminated in a shaving foam fight.[179] Raisin Weekend has also become synonymous with binge drinking and a certain amount of humiliation of "academic children", commonly involving embarrassing costumes or drinking games. The University Students' Association provides a special First Aid hotline for Raisin Weekend.[180]		Situated around the town of St Andrews are cobblestone markings denoting where Protestant martyrs were burnt at the stake. To students, the most notable of these is the cobblestone initials "PH" located outside the main gate of St Salvator's College. These cobblestones denote where Patrick Hamilton was martyred in 1528.[181] According to student tradition, stepping on the "PH" will cause a student to become cursed, with the effect that the offender will fail his or her degree and so students are known to jump over the cobblestones when passing. The 'curse' is said to be lifted by running backwards around St. Salvator's College 8 times while naked, or by participating in the May Dip.[182]		The May Dip is a student tradition held annually at dawn on May Day. Students usually stay awake until dawn, at which time they collectively run into the North Sea to the sound of madrigals sung by the University Madrigal Group.[183] Students purportedly do so to cleanse themselves of any academic sins (which they may have acquired by stepping on the PH cobblestone) before they sit exams in May.[184] In 2011, the event was 'officially' moved by the Students' Association to East Sands in response to concerns for health and safety in its former location on Castle Sands.		Thomas Chalmers		Edward Jenner		John Knox		John Napier		John Pringle		James Wilson		Notable University of St Andrews alumni include King James II of Scotland; United States Declaration of Independence signatory James Wilson (1761); Governor General of Canada John Campbell; discoverer of logarithms John Napier (1563); founder of the Church of Scotland and leader of the Protestant Reformation John Knox (1531); notable Leader of the Church of Scotland Thomas Chalmers; founder of and the first Chancellor of the University of Glasgow William Turnbull; founder of the University of Edinburgh Robert Reid; founder of the world's first commercial savings bank Henry Duncan (1823); journalist and politician during the French Revolution Jean-Paul Marat (1775 MD); inventor of beta-blockers, H2 receptor antagonists and Nobel Prize in Medicine winner James W. Black (1946 MB ChB); the 'father of military medicine' Sir John Pringle, 1st Baronet; pioneer of the smallpox vaccine Edward Jenner (1792 MD); Prince William, Duke of Cambridge (2005) and Catherine, Duchess of Cambridge (2005).		Alumni in the fields of academia and education have gone on to found the University of Melbourne Medical School (Anthony Brownless) and the Scottish Church College in Calcutta (Alexander Duff was also the first Scottish missionary to India), become the first Regent and first Principal of the University of Edinburgh (Robert Rollock), Dean of Harvard Divinity School (David Hempton), the Vice Chancellors of Aberdeen University (Ian Diamond), Open University (Walter Perry was also the first Vice-Chancellor) and Sydney University (Gavin Brown), Chancellor of the University of Maine system (James H. Page), provost of Eton College (Eric Anderson), discoverer of the Berry Phase (Sir Michael Berry) and inventor of the Leslie cube John Leslie.		In business and finance, St Andrews graduates have become the CEOs of multinational companies including the Bank of Scotland (Peter Burt was also the Chairman of ITV plc), BHP Billiton (Andrew Mackenzie), BP (Robert Horton), FanDuel (Nigel Eccles co-founded the company with fellow St Andrews graduate, Lesley Eccles),[185] Rolls-Royce Holdings (John Rose), Royal Dutch Shell (Robert Paul Reid), Tate & Lyle (Iain Ferguson) and The Royal Bank of Scotland Group (George Mathewson). Other notable businesspeople include Banker Olivier Sarkozy, Director of the Edinburgh Festival Fringe Alistair Moffat and the CEO of Scottish Rugby Union and ATP World Tour Finals Phil Anderton.		Former St Andrews students active in politics and national intelligence include former Deputy Director of the Secret Intelligence Service (MI6) George Kennedy Young, Chief of MI6 John Sawers, Secretary of State for Scotland Michael Forsyth (Forsyth also former Deputy Chairman of JP Morgan), former First Minister of Scotland and leader of the SNP for over 20 years Alex Salmond, current Permanent Secretary of the UK Home Office Mark Sedwill, current Secretary of State for Defence Michael Fallon, Deputy Leader of the Liberal Democrats Malcolm Bruce and leader of the Christian Party James George Hargreaves. Outside of the UK, alumni include the Financial Secretary of Hong Kong credited with laying the foundations for Hong Kong's economic success John James Cowperthwaite and the first female cabinet minister in Egypt Hikmat Abu Zayd. Alumni have also gone on to serve as diplomats including Barbara Woodward (China), Anne Pringle (Russia) and Thomas Bruce who is known for the removal of the Elgin Marbles from the Parthenon.		Alumni from the media and the arts include founder of Forbes magazine B. C. Forbes, founder of The Week Jolyon Connell, current Downing Street Director of Communications and former Controller of BBC World News Craig Oliver, Political Editor of BBC Scotland Brian Taylor, BBC News Presenter Louise Minchin, BBC Sport TV Presenter Hazel Irvine, Primetime Emmy Award winning screenwriter David Butler, Pulitzer Prize winning author James Michener, feminist writer Fay Weldon, musician The Pictish Trail and actors Siobhan Redmond, Crispin Bonham-Carter, Ian McDiarmid and Jonathan Taylor Thomas.		Other notable alumni include 'father of the poll tax' Douglas Mason, founders of the Adam Smith Institute, Madsen Pirie and Eamonn Butler, former Lord Justice General Lord Cullen, two currently sitting members of the Inner House, Lord Eassie and Baroness Clark of Calton, one of the leading figures in the formation of the United States Golf Association Charles B. Macdonald and captain of Tottenham Hotspur F.C. during its double-winning season Danny Blanchflower.		The university also boasts of a rich roll of honorary graduates whose members vary from Benjamin Franklin to Hillary Clinton, from Bob Dylan to Arvo Pärt, from Maggy Smith to Sean Connery, from Nora K. Chadwick to Noam Chomsky, from Joseph Stevenson to Lisa Jardine, from Seamus Heaney to Bahram Beyzai, from Georg Cantor to David Attenborough.[186]		Notable University of St Andrews faculty include Nobel Prize in Medicine winner Maurice Wilkins (Lecturer in Physics 1945-46) and discoverer of herring bodies Percy Theodore Herring (Chandos Chair of Medicine and Anatomy 1908-1948). The Morris water navigation task was developed by Richard Morris at the university's Gatty Marine Laboratory.		Anthropology		Biology		Business and Management		Chemistry		Classics		Computer Science		Divinity		Economics		Engineering		English, Literature, and Poetry		Languages and Linguistics		Geology		History and Art History		International Relations and Politics		Mathematics and Astronomy		Media and Film Studies		Medicine and Physiology		Philosophy and Logic		Physics and Astronomy		Psychology		Zoology		The University of St Andrews has appeared in or been referenced by a number of popular media works, in film and literature.		
Student voice is "any expression of any learner regarding anything related to education"[1] and describes "the distinct perspectives and actions of young people throughout schools focused on education.[2] Tech educator Dennis Harper writes, "Student voice is giving students the ability to influence learning to include policies, programs, contexts and principles."[3]		Student voice is the individual and collective perspective and actions of students within the context of learning and education.[4] It is identified in schools as both a metaphorical practice[5] and as a pragmatic concern.[6]						Student voice work is premised on the following convictions:		Several typologies differentiate the practices that identify as student voice.[8][9][10] One identifies multiple roles for students throughout the education system, including education planning, research, teaching, evaluating, decision-making and advocacy.[11]		The presence and engagement of student voice has been seen as essential to the educational process since at least the time of John Dewey, if not long before. In 1916 Dewey wrote extensively about the necessity of engaging student experience and perspectives in the curriculum of schools, summarizing his support by saying,:		The essence of the demand for freedom is the need of conditions which will enable an individual to make his own special contribution to a group interest, and to partake of its activities in such ways that social guidance shall be a matter of his own mental attitude, and not a mere authoritative dictation of his acts.[12]		Today student voice is seeing a resurgence of importance as a growing body of literature[13] increasingly identifies student voice as necessary throughout the educational process.[14] Areas where advocates encourage actively acknowledging student voice include curriculum design and instructional methods, Educational leadership and general school reform activities, including research and evaluation.[15]		Specific types of activities that can specifically engage student voice include learning by teaching, education decision-making, school planning, participatory action research, learning and teaching evaluations, educational advocacy, and student advisories for principals and superintendents.[16]		Engaging student voice is a primary objective of service learning, which commonly seeks to entwine classroom learning objectives with community service opportunities. Student voice is also present in student government programs, experiential education activities, and other forms of student-centered learning.		Engaging students as educational decision-makers is the practice of actively teaching young people responsibility for their education by systematically engaging them in making choices about learning, schooling, and the education system in areas ranging from what affects them personally to what affects an entire student body to what affects the entire school system.		Choosing curricula, calendar year planning, school building design, teacher hiring, and many more issues are often seen as the duties of a school principal or teachers. Today those roles are increasingly seen as avenues for student voice. Students are joining boards of education at all levels, including local, district, and state boards. Some education agencies engage students as staff in programs where they make decisions about grant making, school assessment, and other areas.[17] Students are also participating in decision-making by establishing and enforcing codes of conduct and in personal education decision-making, such as choosing classes and deciding whether to attend school.		Education reform has long been the domain of parents, teachers, school administrators and politicians. In some nations, however, there is a growing trend of greater student participation in scholastic affairs.		The Connect journal, published in Melbourne, features dozens of examples of student voice throughout education in its bi-monthly publication.		The Victorian Student Representative Council is the umbrella or peak body of Student Councils in Victoria, Australia. It is supported with funding from the Victorian Department of Education and Early Childhood Development (DEECD) and auspiced by the Youth Affairs Council of Victoria (YACVic). The VicSRC is an organisation run by secondary school students, elected by their peers.		The New South Wales Student Representative Council is the peak student leadership consultative and decision-making forum in New South Wales.[18]		Including student voice on district school boards was mandated by the Ontario Education Act in 1998. Students in each one of the 72 provincial school boards are represented by a 'pupil representative', commonly called "Student Trustee". They are meant to represent the needs and concerns of students in discussions with the school board administration and the province. The Ontario Student Trustees' Association, OSTA-AECO, has become Ontario's chief student stakeholder, providing professional development to its members and advocates for students' educational interests.[19] The Society for Democratic Education is an organization in Toronto that includes many aspects of heightened student inclusion in education reform policy. The Society for Democratic Education was founded in early 2005 by Bianca Wylie. It has published several essays and position papers that discuss the importance of wide-scale education reform, especially in how it applies to secondary level education and civic education.[20]		Another Canadian organization of note is Learning for a Cause founded in 2004 by educator and poet Michael Ernest Sweet Learning for a Cause which promotes student voices for social change through creative writing and publishing opportunities for Canadian students.		Provincial governments and Ministries of Education across Canada are also getting on board with student engagement and student voice. Alberta Education launched Speak Out – the Alberta Student Engagement Initiative in November 2008 and thousands of students have been sharing their ideas on how to improve how education looks and feels for them.		Ontario's SpeakUp initiative seeks students ideas on what strengthens their engagement in their learning. Ontario's student voice program is centered on four main initiatives, the Minister's Student Advisory Council (MSAC), SpeakUp projects, SpeakUp in a Box and Student Regional Forums.		The Minister’s Student Advisory Council (MSAC) is composed of sixty students, from Grades 7 to 12, they are selected annually to share their ideas and submit recommendations directly to the Ontario Minister of Education. MSAC also determines the themes for Regional Student Forums taking place during the school year. The members of the Minister's Student Advisory Council have been selected in each year since the inaugural year including 2010, 2011, and 2012. SpeakUp projects are micro-grants for students. Student submit applications for projects they have designed that support the goals of the Student Voice initiative, over 1.2 million dollars in grant money is available yearly. Over 5000 SpeakUp projects have been led since 2008. Regional Student Forums are held across the province where students are invited to explore, discuss, and make recommendations about factors that facilitate/hinder their learning. Last, SpeakUp in a box allows students to hold their own forums for 30 people free of charge with the Ontario Ministry of Education providing the materials to do so. More information is available at SpeakUp.		The Calgary Board of Education, in 2010, launched the Chief Superintendent's Student Advisory Council – a group of high school students with student representation from each of the Calgary Board of Education's high school programs. They meet regularly with the Calgary Board of Education's Chief Superintendent, Naomi Johnson, to discuss issues in the system and propose solutions.[21]		Student Voice Initiative is a national movement in Canada to give students a voice in their education. Student Voice Initiative operates on a foundation of support from policy-makers, school administrators, academics, and students from across North America and the world in support of giving students a greater voice in their own education.[22] The core mandate of the organization arose from the success of the 'student trustee' position within the Ontario education community, which has fostered a student leadership framework ranging from student councils at every school, to student senates and student trustees at the regional or district school board level, to the formation of a provincial stakeholder in the Ontario Student Trustees' Association.		A powerful example of student voice in school improvement comes from the 2006 student protests in Chile. Throughout the spring of that year, public high school students from across the country began a series of protests, school takeovers, and negotiations designed to bolster support for public education improvement. After seeing the massive effect of the students, government officials met their demands and are working to support ongoing reforms as necessitated by students.		The government's failure at meeting the core student proposals triggered the biggest social protests in Chile since the return of democracy, in 2011.		England has had a long history of student voice, from Robert Owen's school in New Lanark (allowing the children to direct their learning through questioning, 1816) to Neillie Dick's[23] anarchist school in Whitechapel (set-up by her in 1908 aged 13); A. S. Neill's Summerhill School and Alexander Bloom's[24] St Georges-in-the-East (1945–55). Summerhill School children and staff have been fighting for greater children's rights in schools, running training sessions, presentations and workshops for teachers and children at the House of Commons, London's City Hall, Universities and Schools. They lobbied at the UN Special Session on the Child,[25] spoke at UNESCO[26] and have lobbied the Select Committee on Education.[27] Summerhill School children facilitated the first secondary school children's conference in Dover,[28] involving some 10 schools. Tower Hamlets primary school children have learnt about Summerhill and their legal fight[29] for their children's rights; and regularly work with their local town hall to express their views with the support of HEC Global Learning Centre, including primary conferences.[30]		The most extensive, sustained programme of student voice research in the UK was carried out by the late Professor Jean Rudduck (Faculty of Education, University of Cambridge)[31] and Jean's pioneering work spanned 20 years, helping to establish the principles of student consultation and student participation in practice, policy and research. Jean co-ordinated the ESRC Teaching and Learning Research Programme's Network Project, 'Consulting Pupils about Teaching and Learning'[32] and her work has had a profound influence on the student voice movement, both in the UK and beyond.		StudentVoice is the representative body for secondary students in England. It aims to support students in expressing their views about education by providing workshops and a network of support with other secondary school students. The National College for School Leadership provides career-long learning and development opportunities, professional and practical support for England's existing and aspiring school leaders. Their goal is to ensure that school leaders have the skills, recognition, capacity and ambition to transform the school education system into the best in the world.[33]		Youth councils in the UK also allow student voice, such as the Dartford Youth Council in Dartford, Kent which allows young people including students to give their opinion. The Dartford Youth Council allows students to give their opinion and contribute to issues affecting the youth of the Borough of Dartford.		The Phoenix Education Trust supports democratic education and helped to found StudentVoice It aims to explore and support education in which children are trusted and respected and their participation in decision-making is encouraged.[34] involver supports schools to develop sustainable structures for effective student voice, school councils and participation, and work with teachers and pupils in primary, secondary and special schools.[35] involver provides training, resources, ongoing support and access to a large UK network of schools.		Some state schools are also pushing student Voice internally and independently across the UK. Schools like Quintin Kynaston Community Academy are now recognised for having one of the largest and most active Student Voice 'faculties' in the country.		In Ireland, the Irish Second-Level Students' Union (ISSU) is the national umbrella body for second-level school Student Councils.[36]		Many national organizations and media outlets across the United States have addressed student voice recently, including KQED,[37] Edutopia,[38] the Washington Post, and others. They are finding organizations like Student Voice, What Kids Can Do and SoundOut, as well as local efforts happening across the country.		SoundOut is an international organization that has promoted student voice since it was founded in 2002.[39] In addition to projects across North America[40] and numerous academic citations of their works, SoundOut has also been recognized by UNICEF as "a helpful organization that focuses on promoting student voice in schools."[41] SoundOut's founder, Adam Fletcher, is author of The Guide to Student Voice and the forthcoming Meaningful Student Involvement Handbook. The organization has also published several works related to meaningful student involvement, students on school boards, and student voice.		Student Voice is a nationwide grassroots organization that works to unite and elevate the student voice. Through the use of their @Stu_Voice Twitter page, thousands have come together to speak out using the #StuVoice hashtag during weekly Student Voice chats. Student Voice allows any student to publish blog posts on their website, providing a platform for their voices to be heard. Student Voice hosted the first-ever student voice summit on April 13, 2013 in New York City.[42]		What Kids Can Do shares stories of student voice throughout the educational process, both within the school system and throughout the community. Their highlights emphasize exceptional learning, belonging, and engagement of students in a variety of capacities for a variety of purposes, the greatest of which is in order to promote student voice. WKCD has authored several books about student voice, primarily written by Kathleen Cushman working with high school students, including Fires in the Bathroom: Advice from high schools students for teachers and Sent to the Principal's Office.[43] The High School Survey of Student Engagement works with high schools across the country to capture students' beliefs and experiences, and strengthen student engagement in schools. Their work is used nationally to influence school policy making.[44]		An organization in Minnesota called "Education|Evolving" integrates student voices with current major topics in education policy and maintains an online clearinghouse of student voices on education policy. Their website also has students describing the learning experiences on video.[45] The Quaglia Institute for Student Aspirations promotes student voice as well, teaching schools in Maine how to engage learners in different ways.,[46] while UP For Learning in Vermont implements programs across the state to support deep student voice.[47] The Prichard Committee for Academic Excellence in Kentucky also has a Student Voice Team promoting the concept statewide by working with policymakers and educators, as well as students and parents.[48]		The Organising Bureau of European School Student Unions (OBESSU) is the body which connects school student unions in secondary education across Europe.[49]		Student voice is increasingly identified as a pillar of successful school reform, as educational researchers, academic institutions, and educational support organizations around the world increasingly advocate for the inclusion of students in the reform process after identifying student voice as a vital element of student engagement.[50]		Critical educators including bell hooks, Paulo Freire, and Henry Giroux have voiced concern with the singular notion of a student voice. Adam Fletcher, an internationally recognized expert on student voice, has written about this over-simplification, saying that:		It is not enough to simply listen to student voice. Educators have an ethical imperative to do something with students, and that is why meaningful student involvement is vital to school improvement.[51]		This is echoed by other advocates, including Sam Levin. Levin was an eleventh grade student in Massachusetts when he worked with adults at Monument Mountain Regional High School and his peers to establish an independent learning program for high school students. In a 2014 article in the Washington Post, Levin wrote,		"Students don't need a voice... The change involves giving something to students, but it's not a voice. Students already have a voice. They have student senates, and student advisory committees. When people talk about student voice, they're talking about feedback sessions and letting students be part of hiring committees. When they say, 'Let's give students a voice,' they mean, 'let's give them a seat at school board meetings.' ...Don't give them a voice. Give them our schools."[52]		
Coordinates: 40°N 100°W﻿ / ﻿40°N 100°W﻿ / 40; -100		The United States of America (/əˈmɛrɪkə/; USA), commonly known as the United States (U.S.) or America, is a federal republic[19][20] composed of 50 states, a federal district, five major self-governing territories, and various possessions.[fn 6] Forty-eight of the fifty states and the federal district are contiguous and located in North America between Canada and Mexico. The state of Alaska is in the northwest corner of North America, bordered by Canada to the east and across the Bering Strait from Russia to the west. The state of Hawaii is an archipelago in the mid-Pacific Ocean. The U.S. territories are scattered about the Pacific Ocean and the Caribbean Sea, stretching across nine time zones. The extremely diverse geography, climate and wildlife of the United States make it one of the world's 17 megadiverse countries.[22][23]		At 3.8 million square miles (9.8 million km2)[11] and with over 324 million people, the United States is the world's third- or fourth-largest country by total area,[fn 7] third-largest by land area, and the third-most populous. It is one of the world's most ethnically diverse and multicultural nations, and is home to the world's largest immigrant population.[28] The capital is Washington, D.C., and the largest city is New York City; nine other major metropolitan areas—each with at least 4.5 million inhabitants—are Los Angeles, Chicago, Dallas, Houston, Philadelphia, Miami, Atlanta, Boston, and San Francisco.		Paleo-Indians migrated from Asia to the North American mainland at least 15,000 years ago.[29] European colonization began in the 16th century. The United States emerged from 13 British colonies along the East Coast. Numerous disputes between Great Britain and the colonies following the Seven Years' War led to the American Revolution, which began in 1775. On July 4, 1776, during the course of the American Revolutionary War, the colonies unanimously adopted the Declaration of Independence. The war ended in 1783 with recognition of the independence of the United States by Great Britain, representing the first successful war of independence against a European power.[30] The current constitution was adopted in 1788, after the Articles of Confederation, adopted in 1781, were felt to have provided inadequate federal powers. The first ten amendments, collectively named the Bill of Rights, were ratified in 1791 and designed to guarantee many fundamental civil liberties.		The United States embarked on a vigorous expansion across North America throughout the 19th century,[31] displacing Native American tribes, acquiring new territories, and gradually admitting new states until it spanned the continent by 1848.[31] During the second half of the 19th century, the American Civil War led to the end of legal slavery in the country.[32][33] By the end of that century, the United States extended into the Pacific Ocean,[34] and its economy, driven in large part by the Industrial Revolution, began to soar.[35] The Spanish–American War and World War I confirmed the country's status as a global military power. The United States emerged from World War II as a global superpower, the first country to develop nuclear weapons, the only country to use them in warfare, and a permanent member of the United Nations Security Council. The end of the Cold War and the dissolution of the Soviet Union in 1991 left the United States as the world's sole superpower.[36] The U.S. is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States (OAS), and other international organizations.		The United States is a highly developed country, with the world's largest economy by nominal GDP and second-largest economy by PPP. Though its population is only 4.3% of the world total,[37] Americans hold nearly 40% of the total wealth in the world.[38] The United States ranks among the highest in several measures of socioeconomic performance, including average wage,[39] human development, per capita GDP, and productivity per person.[40] While the U.S. economy is considered post-industrial, characterized by the dominance of services and knowledge economy, the manufacturing sector remains the second-largest in the world.[41] Accounting for approximately a quarter of global GDP[42] and a third of global military spending,[43] the United States is the world's foremost economic and military power. The United States is a prominent political and cultural force internationally, and a leader in scientific research and technological innovations.[44]						In 1507, the German cartographer Martin Waldseemüller produced a world map on which he named the lands of the Western Hemisphere "America" in honor of the Italian explorer and cartographer Amerigo Vespucci (Latin: Americus Vespucius).[45] The first documentary evidence of the phrase "United States of America" is from a letter dated January 2, 1776, written by Stephen Moylan, Esq., George Washington's aide-de-camp and Muster-Master General of the Continental Army. Addressed to Lt. Col. Joseph Reed, Moylan expressed his wish to carry the "full and ample powers of the United States of America" to Spain to assist in the revolutionary war effort.[47][48][49]		The first known publication of the phrase "United States of America" was in an anonymous essay in The Virginia Gazette newspaper in Williamsburg, Virginia, on April 6, 1776.[50][51] The second draft of the Articles of Confederation, prepared by John Dickinson and completed by June 17, 1776, at the latest, declared "The name of this Confederation shall be the 'United States of America.'"[52] The final version of the Articles sent to the states for ratification in late 1777 contains the sentence "The Stile of this Confederacy shall be 'The United States of America'".[53] In June 1776, Thomas Jefferson wrote the phrase "UNITED STATES OF AMERICA" in all capitalized letters in the headline of his "original Rough draught" of the Declaration of Independence.[54][55] This draft of the document did not surface until June 21, 1776, and it is unclear whether it was written before or after Dickinson used the term in his June 17 draft of the Articles of Confederation.[52] In the final Fourth of July version of the Declaration, the title was changed to read, "The unanimous Declaration of the thirteen united States of America".[56] The preamble of the Constitution states "...establish this Constitution for the United States of America."		The short form "United States" is also standard. Other common forms are the "U.S.", the "USA", and "America". Colloquial names are the "U.S. of A." and, internationally, the "States". "Columbia", a name popular in poetry and songs of the late 18th century, derives its origin from Christopher Columbus; it appears in the name "District of Columbia".[57] In non-English languages, the name is frequently the translation of either the "United States" or "United States of America", and colloquially as "America". In addition, an abbreviation (e.g. USA) is sometimes used.[58]		The phrase "United States" was originally plural, a description of a collection of independent states—e.g., "the United States are"—including in the Thirteenth Amendment to the United States Constitution, ratified in 1865. The singular form—e.g., "the United States is"—became popular after the end of the American Civil War. The singular form is now standard; the plural form is retained in the idiom "these United States".[59] The difference is more significant than usage; it is a difference between a collection of states and a unit.[60]		A citizen of the United States is an "American". "United States", "American" and "U.S." refer to the country adjectivally ("American values", "U.S. forces"). In English, the word "American" rarely refers to topics or subjects not connected with the United States.[61]		The first inhabitants of North America migrated from Siberia by way of the Bering land bridge and arrived at least 15,000 years ago, though increasing evidence suggests an even earlier arrival.[29] Some, such as the pre-Columbian Mississippian culture, developed advanced agriculture, grand architecture, and state-level societies.[62] The first Europeans to arrive in territory of the modern United States were Spanish conquistadors such as Juan Ponce de León, who made his first visit to Florida in 1513.		In the Hawaiian Islands, the earliest indigenous inhabitants arrived around 1 AD from Polynesia. Europeans under the British explorer Captain James Cook arrived in the Hawaiian Islands in 1778.		After Spain sent Columbus on his first voyage to the New World in 1492, other explorers followed. The Spanish set up the first settlements in Florida and New Mexico such as Saint Augustine[63] and Santa Fe. The French established their own as well along the Mississippi River. Successful English settlement on the eastern coast of North America began with the Virginia Colony in 1607 at Jamestown and the Pilgrims' Plymouth Colony in 1620. Many settlers were dissenting Christian groups who came seeking religious freedom. The continent's first elected legislative assembly, Virginia's House of Burgesses created in 1619, the Mayflower Compact, signed by the Pilgrims before disembarking, and the Fundamental Orders of Connecticut, established precedents for the pattern of representative self-government and constitutionalism that would develop throughout the American colonies.[64][65]		Most settlers in every colony were small farmers, but other industries developed within a few decades as varied as the settlements. Cash crops included tobacco, rice and wheat. Extraction industries grew up in furs, fishing and lumber. Manufacturers produced rum and ships, and by the late colonial period Americans were producing one-seventh of the world's iron supply.[66] Cities eventually dotted the coast to support local economies and serve as trade hubs. English colonists were supplemented by waves of Scotch-Irish and other groups. As coastal land grew more expensive freed indentured servants pushed further west.[67]		A large-scale slave trade with English privateers was begun.[68] The life expectancy of slaves was much higher in North America than further south, because of less disease and better food and treatment, leading to a rapid increase in the numbers of slaves.[69][70] Colonial society was largely divided over the religious and moral implications of slavery and colonies passed acts for and against the practice.[71][72] But by the turn of the 18th century, African slaves were replacing indentured servants for cash crop labor, especially in southern regions.[73]		With the British colonization of Georgia in 1732, the 13 colonies that would become the United States of America were established.[74] All had local governments with elections open to most free men, with a growing devotion to the ancient rights of Englishmen and a sense of self-government stimulating support for republicanism.[75] With extremely high birth rates, low death rates, and steady settlement, the colonial population grew rapidly. Relatively small Native American populations were eclipsed.[76] The Christian revivalist movement of the 1730s and 1740s known as the Great Awakening fueled interest in both religion and religious liberty.[77]		During the Seven Years' War (in America, known as the French and Indian War), British forces seized Canada from the French, but the francophone population remained politically isolated from the southern colonies. Excluding the Native Americans, who were being conquered and displaced, the 13 British colonies had a population of over 2.1 million in 1770, about one-third that of Britain. Despite continuing new arrivals, the rate of natural increase was such that by the 1770s only a small minority of Americans had been born overseas.[78] The colonies' distance from Britain had allowed the development of self-government, but their success motivated monarchs to periodically seek to reassert royal authority.[79]		With the progress of European colonization in the territories of the contemporary United States, the Native Americans were often conquered and displaced.[80] The native population of America declined after Europeans arrived, and for various reasons, primarily diseases such as smallpox and measles. Violence was not a significant factor in the overall decline among Native Americans, though conflict among themselves and with Europeans affected specific tribes and various colonial settlements.[81][82][83][84][85][86]		In the early days of colonization, many European settlers were subject to food shortages, disease, and attacks from Native Americans. Native Americans were also often at war with neighboring tribes and allied with Europeans in their colonial wars. At the same time, however, many natives and settlers came to depend on each other. Settlers traded for food and animal pelts, natives for guns, ammunition and other European wares.[87] Natives taught many settlers where, when and how to cultivate corn, beans and squash. European missionaries and others felt it was important to "civilize" the Native Americans and urged them to adopt European agricultural techniques and lifestyles.[88][89]		The American Revolutionary War was the first successful colonial war of independence against a European power. Americans had developed an ideology of "republicanism" asserting that government rested on the will of the people as expressed in their local legislatures. They demanded their rights as Englishmen and "no taxation without representation". The British insisted on administering the empire through Parliament, and the conflict escalated into war.[90]		Following the passage of the Lee Resolution, on July 2, 1776, which was the actual vote for independence, the Second Continental Congress adopted the Declaration of Independence on July 4, which proclaimed, in a long preamble, that humanity is created equal in their unalienable rights and that those rights were not being protected by Great Britain, and declared, in the words of the resolution, that the Thirteen Colonies were independent states and had no allegiance to the British crown in the United States. The fourth day of July is celebrated annually as Independence Day. In 1777, the Articles of Confederation established a weak government that operated until 1789.[91]		Britain recognized the independence of the United States following their defeat at Yorktown in 1781.[92] In the peace treaty of 1783, American sovereignty was recognized from the Atlantic coast west to the Mississippi River. Nationalists led the Philadelphia Convention of 1787 in writing the United States Constitution, ratified in state conventions in 1788. The federal government was reorganized into three branches, on the principle of creating salutary checks and balances, in 1789. George Washington, who had led the revolutionary army to victory, was the first president elected under the new constitution. The Bill of Rights, forbidding federal restriction of personal freedoms and guaranteeing a range of legal protections, was adopted in 1791.[93]		Although the federal government criminalized the international slave trade in 1808, after 1820, cultivation of the highly profitable cotton crop exploded in the Deep South, and along with it, the slave population.[94][95][96] The Second Great Awakening, especially 1800–1840, converted millions to evangelical Protestantism. In the North, it energized multiple social reform movements, including abolitionism;[97] in the South, Methodists and Baptists proselytized among slave populations.[98]		Americans' eagerness to expand westward prompted a long series of American Indian Wars.[99] The Louisiana Purchase of French-claimed territory in 1803 almost doubled the nation's area.[100] The War of 1812, declared against Britain over various grievances and fought to a draw, strengthened U.S. nationalism.[101] A series of military incursions into Florida led Spain to cede it and other Gulf Coast territory in 1819.[102] Expansion was aided by steam power, when steamboats began traveling along America's large water systems, which were connected by new canals, such as the Erie and the I&M; then, even faster railroads began their stretch across the nation's land.[103]		From 1820 to 1850, Jacksonian democracy began a set of reforms which included wider white male suffrage; it led to the rise of the Second Party System of Democrats and Whigs as the dominant parties from 1828 to 1854. The Trail of Tears in the 1830s exemplified the Indian removal policy that resettled Indians into the west on Indian reservations. The U.S. annexed the Republic of Texas in 1845 during a period of expansionist Manifest destiny.[104] The 1846 Oregon Treaty with Britain led to U.S. control of the present-day American Northwest.[105] Victory in the Mexican–American War resulted in the 1848 Mexican Cession of California and much of the present-day American Southwest.[106]		The California Gold Rush of 1848–49 spurred western migration and the creation of additional western states.[107] After the American Civil War, new transcontinental railways made relocation easier for settlers, expanded internal trade and increased conflicts with Native Americans.[108] Over a half-century, the loss of the American bison (sometimes called "buffalo") was an existential blow to many Plains Indians cultures.[109] In 1869, a new Peace Policy sought to protect Native-Americans from abuses, avoid further war, and secure their eventual U.S. citizenship, although conflicts, including several of the largest Indian Wars, continued throughout the West into the 1900s.[110]		Differences of opinion and social order between northern and southern states in early United States society, particularly regarding Black slavery, ultimately led to the American Civil War.[111] Initially, states entering the Union alternated between slave and free states, keeping a sectional balance in the Senate, while free states outstripped slave states in population and in the House of Representatives. But with additional western territory and more free-soil states, tensions between slave and free states mounted with arguments over federalism and disposition of the territories, whether and how to expand or restrict slavery.[112]		With the 1860 election of Abraham Lincoln, the first president from the largely anti-slavery Republican Party, conventions in thirteen slave states ultimately declared secession and formed the Confederate States of America, while the federal government maintained that secession was illegal.[112] The ensuing war was at first for Union, then after 1863 as casualties mounted and Lincoln delivered his Emancipation Proclamation, a second war aim became abolition of slavery. The war remains the deadliest military conflict in American history, resulting in the deaths of approximately 618,000 soldiers as well as many civilians.[113]		Following the Union victory in 1865, three amendments were added to the U.S. Constitution: the Thirteenth Amendment prohibited slavery, the Fourteenth Amendment provided citizenship to the nearly four million African Americans who had been slaves,[114] and the Fifteenth Amendment ensured that they had the right to vote. The war and its resolution led to a substantial increase in federal power[115] aimed at reintegrating and rebuilding the Southern states while ensuring the rights of the newly freed slaves.		Southern white conservatives, calling themselves "Redeemers" took control after the end of Reconstruction. By the 1890–1910 period Jim Crow laws disenfranchised most blacks and some poor whites. Blacks faced racial segregation, especially in the South.[116] Racial minorities occasionally experienced vigilante violence.[117]		In the North, urbanization and an unprecedented influx of immigrants from Southern and Eastern Europe supplied a surplus of labor for the country's industrialization and transformed its culture.[118] National infrastructure including telegraph and transcontinental railroads spurred economic growth and greater settlement and development of the American Old West. The later invention of electric light and the telephone would also affect communication and urban life.[119]		The end of the Indian Wars further expanded acreage under mechanical cultivation, increasing surpluses for international markets.[120] Mainland expansion was completed by the purchase of Alaska from Russia in 1867.[121] In 1893, pro-American elements in Hawaii overthrew the monarchy and formed the Republic of Hawaii, which the U.S. annexed in 1898. Puerto Rico, Guam, and the Philippines were ceded by Spain in the same year, following the Spanish–American War.[122]		Rapid economic development during the late 19th and early 20th centuries fostered the rise of many prominent industrialists. Tycoons like Cornelius Vanderbilt, John D. Rockefeller, and Andrew Carnegie led the nation's progress in railroad, petroleum, and steel industries. Banking became a major part of the economy, with J. P. Morgan playing a notable role. Edison and Tesla undertook the widespread distribution of electricity to industry, homes, and for street lighting. Henry Ford revolutionized the automotive industry. The American economy boomed, becoming the world's largest, and the United States achieved great power status.[123] These dramatic changes were accompanied by social unrest and the rise of populist, socialist, and anarchist movements.[124] This period eventually ended with the advent of the Progressive Era, which saw significant reforms in many societal areas, including women's suffrage, alcohol prohibition, regulation of consumer goods, greater antitrust measures to ensure competition and attention to worker conditions.[125][126][127][128]		The United States remained neutral from the outbreak of World War I, in 1914, until 1917 when it joined the war as an "associated power", alongside the formal Allies of World War I, helping to turn the tide against the Central Powers. In 1919, President Woodrow Wilson took a leading diplomatic role at the Paris Peace Conference and advocated strongly for the U.S. to join the League of Nations. However, the Senate refused to approve this, and did not ratify the Treaty of Versailles that established the League of Nations.[129]		In 1920, the women's rights movement won passage of a constitutional amendment granting women's suffrage.[130] The 1920s and 1930s saw the rise of radio for mass communication and the invention of early television.[131] The prosperity of the Roaring Twenties ended with the Wall Street Crash of 1929 and the onset of the Great Depression. After his election as president in 1932, Franklin D. Roosevelt responded with the New Deal, which included the establishment of the Social Security system.[132] The Great Migration of millions of African Americans out of the American South began before World War I and extended through the 1960s;[133] whereas the Dust Bowl of the mid-1930s impoverished many farming communities and spurred a new wave of western migration.[134]		At first effectively neutral during World War II while Germany conquered much of continental Europe, the United States began supplying material to the Allies in March 1941 through the Lend-Lease program. On December 7, 1941, the Empire of Japan launched a surprise attack on Pearl Harbor, prompting the United States to join the Allies against the Axis powers.[135] During the war, the United States was referred as one of the "Four Policemen"[136] of Allies power who met to plan the postwar world, along with Britain, the Soviet Union and China.[137][138] Though the nation lost more than 400,000 soldiers,[139] it emerged relatively undamaged from the war with even greater economic and military influence.[140]		The United States played a leading role in the Bretton Woods and Yalta conferences with the United Kingdom, the Soviet Union and other Allies, which signed agreements on new international financial institutions and Europe's postwar reorganization. As an Allied victory was won in Europe, a 1945 international conference held in San Francisco produced the United Nations Charter, which became active after the war.[141] The United States developed the first nuclear weapons and used them on Japan in the cities of Hiroshima and Nagasaki; causing the Japanese to surrender on September 2, ending World War II.[142][143] Parades and celebrations followed in what is known as Victory Day, or V-J Day.[144]		After World War II the United States and the Soviet Union jockeyed for power during what became known as the Cold War, driven by an ideological divide between capitalism and communism[145] and, according to the school of geopolitics, a divide between the maritime Atlantic and the continental Eurasian camps. They dominated the military affairs of Europe, with the U.S. and its NATO allies on one side and the USSR and its Warsaw Pact allies on the other. The U.S. developed a policy of containment towards the expansion of communist influence. While the U.S. and Soviet Union engaged in proxy wars and developed powerful nuclear arsenals, the two countries avoided direct military conflict.		The United States often opposed Third World movements that it viewed as Soviet-sponsored. American troops fought communist Chinese and North Korean forces in the Korean War of 1950–53.[146] The Soviet Union's 1957 launch of the first artificial satellite and its 1961 launch of the first manned spaceflight initiated a "Space Race" in which the United States became the first nation to land a man on the moon in 1969.[146] A proxy war in Southeast Asia eventually evolved into full American participation, as the Vietnam War.		At home, the U.S. experienced sustained economic expansion and a rapid growth of its population and middle class. Construction of an Interstate Highway System transformed the nation's infrastructure over the following decades. Millions moved from farms and inner cities to large suburban housing developments.[147][148] In 1959 Hawaii became the 50th and last U.S. state added to the country.[149] The growing Civil Rights Movement used nonviolence to confront segregation and discrimination, with Martin Luther King Jr. becoming a prominent leader and figurehead. A combination of court decisions and legislation, culminating in the Civil Rights Act of 1968, sought to end racial discrimination.[150][151][152] Meanwhile, a counterculture movement grew which was fueled by opposition to the Vietnam war, black nationalism, and the sexual revolution.		The launch of a "War on Poverty" expanded entitlements and welfare spending, including the creation of Medicare and Medicaid, two programs that provide health coverage to the elderly and poor, respectively, and the means-tested Food Stamp Program and Aid to Families with Dependent Children.[153]		The 1970s and early 1980s saw the onset of stagflation. After his election in 1980, President Ronald Reagan responded to economic stagnation with free-market oriented reforms. Following the collapse of détente, he abandoned "containment" and initiated the more aggressive "rollback" strategy towards the USSR.[154][155][156][157][158] After a surge in female labor participation over the previous decade, by 1985 the majority of women aged 16 and over were employed.[159]		The late 1980s brought a "thaw" in relations with the USSR, and its collapse in 1991 finally ended the Cold War.[160][161][162][163] This brought about unipolarity[164] with the U.S. unchallenged as the world's dominant superpower. The concept of Pax Americana, which had appeared in the post-World War II period, gained wide popularity as a term for the post-Cold War new world order.		After the Cold War, the conflict in the Middle East triggered a crisis in 1990, when Iraq under Sadaam Hussein invaded and attempted to annex Kuwait, an ally of the United States. Fearing that the instability would spread to other regions, President George H.W. Bush launched Operation Desert Shield, a defensive force buildup in Saudi Arabia, and Operation Desert Storm, in a staging titled the Gulf War; waged by coalition forces from 34 nations, led by the United States against Iraq ending in the successful expulsion of Iraqi forces from Kuwait, restoring the former monarchy.[165]		Originating in U.S. defense networks, the Internet spread to international academic networks, and then to the public in the 1990s, greatly affecting the global economy, society, and culture.[166]		Due to the dot-com boom, stable monetary policy under Alan Greenspan, and reduced social welfare spending, the 1990s saw the longest economic expansion in modern U.S. history, ending in 2001.[167] Beginning in 1994, the U.S. entered into the North American Free Trade Agreement (NAFTA), linking 450 million people producing $17 trillion worth of goods and services. The goal of the agreement was to eliminate trade and investment barriers among the U.S., Canada, and Mexico by January 1, 2008. Trade among the three partners has soared since NAFTA went into force.[168]		On September 11, 2001, Al-Qaeda terrorists struck the World Trade Center in New York City and the Pentagon near Washington, D.C., killing nearly 3,000 people.[169] In response, the United States launched the War on Terror, which included war in Afghanistan and the 2003–11 Iraq War.[170][171] In 2007, the Bush administration ordered a major troop surge in the Iraq War,[172] which successfully reduced violence and led to greater stability in the region.[173][174]		Government policy designed to promote affordable housing,[175] widespread failures in corporate and regulatory governance,[176] and historically low interest rates set by the Federal Reserve[177] led to the mid-2000s housing bubble, which culminated with the 2008 financial crisis, the largest economic contraction in the nation's history since the Great Depression.[178] Barack Obama, the first African American[179] and multiracial[180] president, was elected in 2008 amid the crisis,[181] and subsequently passed stimulus measures and the Dodd-Frank Wall Street Reform and Consumer Protection Act in an attempt to mitigate its negative effects. While the stimulus facilitated infrastructure improvements[182] and a relative decline in unemployment,[183] Dodd-Frank has had a negative impact on business investment and small banks.[184]		In 2010, the Obama administration passed the Affordable Care Act, which made the most sweeping reforms to the nation's healthcare system in nearly five decades, including mandates, subsidies and insurance exchanges. The law caused a significant reduction in the number and percentage of people without health insurance, with 24 million covered during 2016,[185] but remains controversial due to its impact on healthcare costs, insurance premiums, and economic performance.[186] Although the recession reached its trough in June 2009, voters remained frustrated with the slow pace of the economic recovery. The Republicans, who stood in opposition to Obama's policies, won control of the House of Representatives with a landslide in 2010 and control of the Senate in 2014.[187]		American forces in Iraq were withdrawn in large numbers in 2009 and 2010, and the war in the region was declared formally over in December 2011.[188] The withdrawal caused an escalation of sectarian insurgency,[189] leading to the rise of the Islamic State of Iraq and the Levant, the successor of al-Qaeda in the region.[190] In 2014, Obama announced a restoration of full diplomatic relations with Cuba for the first time since 1961.[needs update][191] The next year, the United States as a member of the P5+1 countries signed the Joint Comprehensive Plan of Action, an agreement aimed to slow the development of Iran's nuclear program.[192]		The land area of the contiguous United States is 2,959,064 square miles (7,663,940.6 km2). Alaska, separated from the contiguous United States by Canada, is the largest state at 663,268 square miles (1,717,856.2 km2). Hawaii, occupying an archipelago in the central Pacific, southwest of North America, is 10,931 square miles (28,311 km2) in area. The populated territories of Puerto Rico, American Samoa, Guam, Northern Mariana Islands, and U.S. Virgin Islands together cover 9,185 square miles (23,789 km2).[193]		The United States is the world's third- or fourth-largest nation by total area (land and water), ranking behind Russia and Canada and just above or below China. The ranking varies depending on how two territories disputed by China and India are counted and how the total size of the United States is measured: calculations range from 3,676,486 square miles (9,522,055.0 km2)[194] to 3,717,813 square miles (9,629,091.5 km2)[195] to 3,796,742 square miles (9,833,516.6 km2)[10] to 3,805,927 square miles (9,857,306 km2).[11] Measured by only land area, the United States is third in size behind Russia and China, just ahead of Canada.[196]		The coastal plain of the Atlantic seaboard gives way further inland to deciduous forests and the rolling hills of the Piedmont.[197] The Appalachian Mountains divide the eastern seaboard from the Great Lakes and the grasslands of the Midwest.[198] The Mississippi–Missouri River, the world's fourth longest river system, runs mainly north–south through the heart of the country. The flat, fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.[198]		The Rocky Mountains, at the western edge of the Great Plains, extend north to south across the country, reaching altitudes higher than 14,000 feet (4,300 m) in Colorado.[199] Farther west are the rocky Great Basin and deserts such as the Chihuahua and Mojave.[200] The Sierra Nevada and Cascade mountain ranges run close to the Pacific coast, both ranges reaching altitudes higher than 14,000 feet (4,300 m). The lowest and highest points in the contiguous United States are in the state of California,[201] and only about 84 miles (135 km) apart.[202] At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali (Mount McKinley) is the highest peak in the country and North America.[203] Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands. The supervolcano underlying Yellowstone National Park in the Rockies is the continent's largest volcanic feature.[204]		The United States, with its large size and geographic variety, includes most climate types. To the east of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south.[205] The Great Plains west of the 100th meridian are semi-arid. Much of the Western mountains have an alpine climate. The climate is arid in the Great Basin, desert in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon and Washington and southern Alaska. Most of Alaska is subarctic or polar. Hawaii and the southern tip of Florida are tropical, as are the populated territories in the Caribbean and the Pacific.[206] Extreme weather is not uncommon—the states bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur within the country, mainly in Tornado Alley areas in the Midwest and South.[207]		The U.S. ecology is megadiverse: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and over 1,800 species of flowering plants are found in Hawaii, few of which occur on the mainland.[209] The United States is home to 428 mammal species, 784 bird species, 311 reptile species, and 295 amphibian species.[210] About 91,000 insect species have been described.[211] The bald eagle is both the national bird and national animal of the United States, and is an enduring symbol of the country itself.[212]		There are 59 national parks and hundreds of other federally managed parks, forests, and wilderness areas.[213] Altogether, the government owns about 28% of the country's land area.[214] Most of this is protected, though some is leased for oil and gas drilling, mining, logging, or cattle ranching; about .86% is used for military purposes.[215][216]		Environmental issues have been on the national agenda since 1970. Environmental controversies include debates on oil and nuclear energy, dealing with air and water pollution, the economic costs of protecting wildlife, logging and deforestation,[217][218] and international responses to global warming.[219][220] Many federal and state agencies are involved. The most prominent is the Environmental Protection Agency (EPA), created by presidential order in 1970.[221] The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act.[222] The Endangered Species Act of 1973 is intended to protect threatened and endangered species and their habitats, which are monitored by the United States Fish and Wildlife Service.[223]		The U.S. Census Bureau estimated the country's population to be 323,425,550 as of April 25, 2016, and to be adding 1 person (net gain) every 13 seconds, or about 6,646 people per day.[228] The U.S. population almost quadrupled during the 20th century, from about 76 million in 1900.[229] The third most populous nation in the world, after China and India, the United States is the only major industrialized nation in which large population increases are projected.[230] In the 1800s the average woman had 7.04 children, by the 1900s this number had decreased to 3.56.[231] Since the early 1970s the birth rate has been below the replacement rate of 2.1 with 1.86 children per woman in 2014. Foreign born immigration has caused the US population to continue its rapid increase with the foreign born population doubling from almost 20 million in 1990 to over 40 million in 2010, representing one third of the population increase.[232] The foreign born population reached 45 million in 2015.[233][fn 8]		The United States has a birth rate of 13 per 1,000, which is 5 births below the world average.[237] Its population growth rate is positive at 0.7%, higher than that of many developed nations.[238] In fiscal year 2012, over one million immigrants (most of whom entered through family reunification) were granted legal residence.[239] Mexico has been the leading source of new residents since the 1965 Immigration Act. China, India, and the Philippines have been in the top four sending countries every year since the 1990s.[240] As of 2012[update], approximately 11.4 million residents are illegal immigrants.[241] As of 2015, 47% of all immigrants are Hispanic, 26% are Asian, 18% are white and 8% are black. The percentage of immigrants who are Asian is increasing while the percentage who are Hispanic is decreasing.[233]		According to a survey conducted by the Williams Institute, nine million Americans, or roughly 3.4% of the adult population identify themselves as homosexual, bisexual, or transgender.[242][243] A 2016 Gallup poll also concluded that 4.1% of adult Americans identified as LGBT. The highest percentage came from the District of Columbia (10%), while the lowest state was North Dakota at 1.7%.[244] In a 2013 survey, the Centers for Disease Control and Prevention found that 96.6% of Americans identify as straight, while 1.6% identify as gay or lesbian, and 0.7% identify as being bisexual.[245]		In 2010, the U.S. population included an estimated 5.2 million people with some American Indian or Alaska Native ancestry (2.9 million exclusively of such ancestry) and 1.2 million with some native Hawaiian or Pacific island ancestry (0.5 million exclusively).[246] The census counted more than 19 million people of "Some Other Race" who were "unable to identify with any" of its five official race categories in 2010, over 18.5 million (97%) of whom are of Hispanic ethnicity.[246]		The population growth of Hispanic and Latino Americans (the terms are officially interchangeable) is a major demographic trend. The 50.5 million Americans of Hispanic descent[246] are identified as sharing a distinct "ethnicity" by the Census Bureau; 64% of Hispanic Americans are of Mexican descent.[247] Between 2000 and 2010, the country's Hispanic population increased 43% while the non-Hispanic population rose just 4.9%.[248] Much of this growth is from immigration; in 2007, 12.6% of the U.S. population was foreign-born, with 54% of that figure born in Latin America.[249][fn 9]		About 82% of Americans live in urban areas (including suburbs);[10] about half of those reside in cities with populations over 50,000.[255] The US has numerous clusters of cities known as megaregions, the largest being the Great Lakes Megalopolis followed by the Northeast Megalopolis and Southern California. In 2008, 273 incorporated municipalities had populations over 100,000, nine cities had more than one million residents, and four global cities had over two million (New York, Los Angeles, Chicago, and Houston).[256] There are 52 metropolitan areas with populations greater than one million.[257] Of the 50 fastest-growing metro areas, 47 are in the West or South.[258] The metro areas of San Bernardino, Dallas, Houston, Atlanta, and Phoenix all grew by more than a million people between 2000 and 2008.[257]				English (American English) is the de facto national language. Although there is no official language at the federal level, some laws—such as U.S. naturalization requirements—standardize English. In 2010, about 230 million, or 80% of the population aged five years and older, spoke only English at home. Spanish, spoken by 12% of the population at home, is the second most common language and the most widely taught second language.[261][262] Some Americans advocate making English the country's official language, as it is in 32 states.[263]		Both Hawaiian and English are official languages in Hawaii, by state law.[264] Alaska recognizes twenty Native languages as well as English.[265] While neither has an official language, New Mexico has laws providing for the use of both English and Spanish, as Louisiana does for English and French.[266] Other states, such as California, mandate the publication of Spanish versions of certain government documents including court forms.[267] Many jurisdictions with large numbers of non-English speakers produce government materials, especially voting information, in the most commonly spoken languages in those jurisdictions.		Several insular territories grant official recognition to their native languages, along with English: Samoan[268] and Chamorro[269] are recognized by American Samoa and Guam, respectively; Carolinian and Chamorro are recognized by the Northern Mariana Islands;[270] Cherokee is officially recognized by the Cherokee Nation within the Cherokee tribal jurisdiction area in eastern Oklahoma;[271] Spanish is an official language of Puerto Rico and is more widely spoken than English there.[272]		The most widely taught foreign languages in the United States, in terms of enrollment numbers from kindergarten through university undergraduate studies, are: Spanish (around 7.2 million students), French (1.5 million), and German (500,000). Other commonly taught languages (with 100,000 to 250,000 learners) include Latin, Japanese, ASL, Italian, and Chinese.[273][274] 18% of all Americans claim to speak at least one language in addition to English.[275]		The First Amendment of the U.S. Constitution guarantees the free exercise of religion and forbids Congress from passing laws respecting its establishment.		Christianity is by far the most common religion practiced in the U.S., but other religions are followed, too. In a 2013 survey, 56% of Americans said that religion played a "very important role in their lives", a far higher figure than that of any other wealthy nation.[278] In a 2009 Gallup poll, 42% of Americans said that they attended church weekly or almost weekly; the figures ranged from a low of 23% in Vermont to a high of 63% in Mississippi.[279] Experts, researchers and authors have referred to the United States as a "Protestant nation" or "founded on Protestant principles,"[280][281][282][283] specifically emphasizing its Calvinist heritage.[284][285][286]		As with other Western countries, the U.S. is becoming less religious. Irreligion is growing rapidly among Americans under 30.[287] Polls show that overall American confidence in organized religion has been declining since the mid to late 1980s,[288] and that younger Americans in particular are becoming increasingly irreligious.[9][289] According to a 2012 study, the Protestant share of the U.S. population had dropped to 48%, thus ending its status as religious category of the majority for the first time.[290][291] Americans with no religion have 1.7 children compared to 2.2 among Christians. The unaffiliated are less likely to get married with 37% marrying compared to 52% of Christians.[292]		According to a 2014 survey, 70.6% of adults identified themselves as Christian,[293] Protestant denominations accounted for 46.5%, while Roman Catholicism, at 20.8%, was the largest individual denomination.[294] The total reporting non-Christian religions in 2014 was 5.9%.[294] Other religions include Judaism (1.9%), Islam (0.9%), Buddhism (0.7%), Hinduism (0.7%).[294] The survey also reported that 22.8% of Americans described themselves as agnostic, atheist or simply having no religion, up from 8.2% in 1990.[294][295][296] There are also Unitarian Universalist, Baha'i, Sikh, Jain, Shinto, Confucian, Taoist, Druid, Native American, Wiccan, humanist and deist communities.[297]		Protestantism is the largest Christian religious grouping in the United States. Baptists collectively form the largest branch of Protestantism, and the Southern Baptist Convention is the largest individual Protestant denomination. About 26% of Americans identify as Evangelical Protestants, while 15% are Mainline and 7% belong to a traditionally Black church. Roman Catholicism in the United States has its origin in the Spanish and French colonization of the Americas, and later grew because of Irish, Italian, Polish, German and Hispanic immigration. Rhode Island has the highest percentage of Catholics with 40 percent of the total population.[298] Lutheranism in the U.S. has its origin in immigration from Northern Europe and Germany. North and South Dakota are the only states in which a plurality of the population is Lutheran. Presbyterianism was introduced in North America by Scottish and Ulster Scots immigrants. Although it has spread across the United States, it is heavily concentrated on the East Coast. Dutch Reformed congregations were founded first in New Amsterdam (New York City) before spreading westward. Utah is the only state where Mormonism is the religion of the majority of the population. The Mormon Corridor also extends to parts of Idaho, Nevada and Wyoming.[299]		The Bible Belt is an informal term for a region in the Southern United States in which socially conservative Evangelical Protestantism is a significant part of the culture and Christian church attendance across the denominations is generally higher than the nation's average. By contrast, religion plays the least important role in New England and in the Western United States.[279]		As of 2007[update], 58% of Americans age 18 and over were married, 6% were widowed, 10% were divorced, and 25% had never been married.[300] Women now work mostly outside the home and receive a majority of bachelor's degrees.[301]		The U.S. teenage pregnancy rate is 26.5 per 1,000 women. The rate has declined by 57% since 1991.[302] In 2013, the highest teenage birth rate was in Alabama, and the lowest in Wyoming.[302][303] Abortion is legal throughout the U.S., owing to Roe v. Wade, a 1973 landmark decision by the Supreme Court of the United States. While the abortion rate is falling, the abortion ratio of 241 per 1,000 live births and abortion rate of 15 per 1,000 women aged 15–44 remain higher than those of most Western nations.[304] In 2013, the average age at first birth was 26 and 40.6% of births were to unmarried women.[305]		The total fertility rate (TFR) was estimated for 2013 at 1.86 births per woman.[306] Adoption in the United States is common and relatively easy from a legal point of view (compared to other Western countries).[307] In 2001, with over 127,000 adoptions, the U.S. accounted for nearly half of the total number of adoptions worldwide.[308] Same-sex marriage is legal nationwide and it is legal for same-sex couples to adopt. Polygamy is illegal throughout the U.S.[309]		The United States is the world's oldest surviving federation. It is a representative democracy, "in which majority rule is tempered by minority rights protected by law".[310] The government is regulated by a system of checks and balances defined by the U.S. Constitution, which serves as the country's supreme legal document.[311] For 2016, the U.S. ranked 21st on the Democracy Index[312] (tied with Italy) and 18th on the Corruption Perceptions Index.[313]		In the American federalist system, citizens are usually subject to three levels of government: federal, state, and local. The local government's duties are commonly split between county and municipal governments. In almost all cases, executive and legislative officials are elected by a plurality vote of citizens by district. There is no proportional representation at the federal level, and it is rare at lower levels.[314]		The federal government is composed of three branches:		The House of Representatives has 435 voting members, each representing a congressional district for a two-year term. House seats are apportioned among the states by population every tenth year. At the 2010 census, seven states had the minimum of one representative, while California, the most populous state, had 53.[319]		The Senate has 100 members with each state having two senators, elected at-large to six-year terms; one third of Senate seats are up for election every other year. The President serves a four-year term and may be elected to the office no more than twice. The President is not elected by direct vote, but by an indirect electoral college system in which the determining votes are apportioned to the states and the District of Columbia.[320] The Supreme Court, led by the Chief Justice of the United States, has nine members, who serve for life.[321]		The state governments are structured in roughly similar fashion; Nebraska uniquely has a unicameral legislature.[323] The governor (chief executive) of each state is directly elected. Some state judges and cabinet officers are appointed by the governors of the respective states, while others are elected by popular vote.		The original text of the Constitution establishes the structure and responsibilities of the federal government and its relationship with the individual states. Article One protects the right to the "great writ" of habeas corpus. The Constitution has been amended 27 times;[324] the first ten amendments, which make up the Bill of Rights, and the Fourteenth Amendment form the central basis of Americans' individual rights. All laws and governmental procedures are subject to judicial review and any law ruled by the courts to be in violation of the Constitution is voided. The principle of judicial review, not explicitly mentioned in the Constitution, was established by the Supreme Court in Marbury v. Madison (1803)[325] in a decision handed down by Chief Justice John Marshall.[326]		The United States is a federal republic of 50 states, a federal district, five territories and eleven uninhabited island possessions.[328] The states and territories are the principal administrative districts in the country. These are divided into subdivisions of counties and independent cities. The District of Columbia is a federal district that contains the capital of the United States, Washington DC.[329] The states and the District of Columbia choose the President of the United States. Each state has presidential electors equal to the number of their Representatives and Senators in Congress; the District of Columbia has three.[330]		Congressional Districts are reapportioned among the states following each decennial Census of Population. Each state then draws single member districts to conform with the census apportionment. The total number of Representatives is 435, and delegate Members of Congress represent the District of Columbia and the five major U.S. territories.[331]		The United States also observes tribal sovereignty of the American Indian nations to a limited degree, as it does with the states' sovereignty. American Indians are U.S. citizens and tribal lands are subject to the jurisdiction of the U.S. Congress and the federal courts. Like the states they have a great deal of autonomy, but also like the states tribes are not allowed to make war, engage in their own foreign relations, or print and issue currency.[332]		The United States has operated under a two-party system for most of its history.[334] For elective offices at most levels, state-administered primary elections choose the major party nominees for subsequent general elections. Since the general election of 1856, the major parties have been the Democratic Party, founded in 1824, and the Republican Party, founded in 1854. Since the Civil War, only one third-party presidential candidate—former president Theodore Roosevelt, running as a Progressive in 1912—has won as much as 20% of the popular vote. The President and Vice-president are elected through the Electoral College system.[335]		Within American political culture, the center-right Republican Party is considered "conservative" and the center-left Democratic Party is considered "liberal".[336][337] The states of the Northeast and West Coast and some of the Great Lakes states, known as "blue states", are relatively liberal. The "red states" of the South and parts of the Great Plains and Rocky Mountains are relatively conservative.		Republican Donald Trump, the winner of the 2016 presidential election, is currently serving as the 45th President of the United States.[338] Current leadership in the Senate includes Republican Vice President Mike Pence, Republican President Pro Tempore Orrin Hatch, Majority Leader Mitch McConnell, and Minority Leader Chuck Schumer.[339] Leadership in the House includes Speaker of the House Paul Ryan, Majority Leader Kevin McCarthy, and Minority Leader Nancy Pelosi.[340]		In the 115th United States Congress, both the House of Representatives and the Senate are controlled by the Republican Party. The Senate currently consists of 52 Republicans, and 46 Democrats with 2 Independents who caucus with the Democrats; the House consists of 241 Republicans and 194 Democrats.[341] In state governorships, there are 33 Republicans, 16 Democrats, and 1 Independent.[342] Among the DC mayor and the 5 territorial governors, there are 2 Republicans, 1 Democrat, 1 New Progressive, and 2 Independents.[343]		The United States has an established structure of foreign relations. It is a permanent member of the United Nations Security Council, and New York City is home to the United Nations Headquarters. It is a member of the G7,[345] G20, and Organisation for Economic Co-operation and Development. Almost all countries have embassies in Washington, D.C., and many have consulates around the country. Likewise, nearly all nations host American diplomatic missions. However, Iran, North Korea, Bhutan, and the Republic of China (Taiwan) do not have formal diplomatic relations with the United States (although the U.S. still maintains relations with Taiwan and supplies it with military equipment).[346]		The United States has a "Special Relationship" with the United Kingdom[347] and strong ties with Canada,[348] Australia,[349] New Zealand,[350] the Philippines,[351] Japan,[352] South Korea,[353] Israel,[354] and several European Union countries, including France, Italy, Germany, and Spain. It works closely with fellow NATO members on military and security issues and with its neighbors through the Organization of American States and free trade agreements such as the trilateral North American Free Trade Agreement with Canada and Mexico. In 2008, the United States spent a net $25.4 billion on official development assistance, the most in the world. As a share of America's large gross national income (GNI), however, the U.S. contribution of 0.18% ranked last among 22 donor states. By contrast, private overseas giving by Americans is relatively generous.[355]		The U.S. exercises full international defense authority and responsibility for three sovereign nations through Compact of Free Association with Micronesia, the Marshall Islands and Palau. These are Pacific island nations, once part of the U.S.-administered Trust Territory of the Pacific Islands after World War II, which gained independence in subsequent years.[356]		Taxes in the United States are levied at the federal, state, and local government levels. These include taxes on income, payroll, property, sales, imports, estates and gifts, as well as various fees. In 2010 taxes collected by federal, state and municipal governments amounted to 24.8% of GDP.[358] During FY2012, the federal government collected approximately $2.45 trillion in tax revenue, up $147 billion or 6% versus FY2011 revenues of $2.30 trillion. Primary receipt categories included individual income taxes ($1,132B or 47%), Social Security/Social Insurance taxes ($845B or 35%), and corporate taxes ($242B or 10%).[359] Based on CBO estimates,[360] under 2013 tax law the top 1% will be paying the highest average tax rates since 1979, while other income groups will remain at historic lows.[361]		U.S. taxation is generally progressive, especially the federal income taxes, and is among the most progressive in the developed world.[362][363][364][365][366] The highest 10% of income earners pay a majority of federal taxes,[367] and about half of all taxes.[368] Payroll taxes for Social Security are a flat regressive tax, with no tax charged on income above $118,500 (for 2015 and 2016) and no tax at all paid on unearned income from things such as stocks and capital gains.[369][370] The historic reasoning for the regressive nature of the payroll tax is that entitlement programs have not been viewed as welfare transfers.[371][372] However, according to the Congressional Budget Office the net effect of Social Security is that the benefit to tax ratio ranges from roughly 70% for the top earnings quintile to about 170% for the lowest earning quintile, making the system progressive.[373]		The top 10% paid 51.8% of total federal taxes in 2009, and the top 1%, with 13.4% of pre-tax national income, paid 22.3% of federal taxes.[374] In 2013 the Tax Policy Center projected total federal effective tax rates of 35.5% for the top 1%, 27.2% for the top quintile, 13.8% for the middle quintile, and −2.7% for the bottom quintile.[375][376] The incidence of corporate income tax has been a matter of considerable ongoing controversy for decades.[365][377] State and local taxes vary widely, but are generally less progressive than federal taxes as they rely heavily on broadly borne regressive sales and property taxes that yield less volatile revenue streams, though their consideration does not eliminate the progressive nature of overall taxation.[365][378]		During FY 2012, the federal government spent $3.54 trillion on a budget or cash basis, down $60 billion or 1.7% vs. FY 2011 spending of $3.60 trillion. Major categories of FY 2012 spending included: Medicare & Medicaid ($802B or 23% of spending), Social Security ($768B or 22%), Defense Department ($670B or 19%), non-defense discretionary ($615B or 17%), other mandatory ($461B or 13%) and interest ($223B or 6%).[359]		The total national debt of the United States in the United States was $18.527 trillion (106% of the GDP) in 2014.[379][fn 11]		The President holds the title of commander-in-chief of the nation's armed forces and appoints its leaders, the Secretary of Defense and the Joint Chiefs of Staff. The United States Department of Defense administers the armed forces, including the Army, Marine Corps, Navy, and Air Force. The Coast Guard is run by the Department of Homeland Security in peacetime and by the Department of the Navy during times of war. In 2008, the armed forces had 1.4 million personnel on active duty. The Reserves and National Guard brought the total number of troops to 2.3 million. The Department of Defense also employed about 700,000 civilians, not including contractors.[384]		Military service is voluntary, though conscription may occur in wartime through the Selective Service System.[385] American forces can be rapidly deployed by the Air Force's large fleet of transport aircraft, the Navy's 10 active aircraft carriers, and Marine expeditionary units at sea with the Navy's Atlantic and Pacific fleets. The military operates 865 bases and facilities abroad,[386] and maintains deployments greater than 100 active duty personnel in 25 foreign countries.[387]		The military budget of the United States in 2011 was more than $700 billion, 41% of global military spending and equal to the next 14 largest national military expenditures combined. At 4.7% of GDP, the rate was the second-highest among the top 15 military spenders, after Saudi Arabia.[388] U.S. defense spending as a percentage of GDP ranked 23rd globally in 2012 according to the CIA.[389] Defense's share of U.S. spending has generally declined in recent decades, from Cold War peaks of 14.2% of GDP in 1953 and 69.5% of federal outlays in 1954 to 4.7% of GDP and 18.8% of federal outlays in 2011.[390]		The proposed base Department of Defense budget for 2012, $553 billion, was a 4.2% increase over 2011; an additional $118 billion was proposed for the military campaigns in Iraq and Afghanistan.[391] The last American troops serving in Iraq departed in December 2011;[392] 4,484 service members were killed during the Iraq War.[393] Approximately 90,000 U.S. troops were serving in Afghanistan in April 2012;[394] by November 8, 2013 2,285 had been killed during the War in Afghanistan.[395]		Law enforcement in the United States is primarily the responsibility of local police and sheriff's departments, with state police providing broader services. The New York City Police Department (NYPD) is the largest in the country. Federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have specialized duties, including protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws.[397] At the federal level and in almost every state, a legal system operates on a common law. State courts conduct most criminal trials; federal courts handle certain designated crimes as well as certain appeals from the state criminal courts. Plea bargaining in the United States is very common; the vast majority of criminal cases in the country are settled by plea bargain rather than jury trial.[398]		In 2015, there were 15,696 murders which was 1,532 more than in 2014, a 10.8 per cent increase, the largest since 1971.[399] The murder rate in 2015 was 4.9 per 100,000 people.[400] The national clearance rate for homicides in 2015 was 64.1%, compared to 90% in 1965.[401] In 2012 there were 4.7 murders per 100,000 persons in the United States, a 54% decline from the modern peak of 10.2 in 1980.[402] In 2001–2, the United States had above-average levels of violent crime and particularly high levels of gun violence compared to other developed nations.[403] A cross-sectional analysis of the World Health Organization Mortality Database from 2010 showed that United States "homicide rates were 7.0 times higher than in other high-income countries, driven by a gun homicide rate that was 25.2 times higher."[404] Gun ownership rights continue to be the subject of contentious political debate.		From 1980 through 2008 males represented 77% of homicide victims and 90% of offenders. Blacks committed 52.5% of all homicides during that span, at a rate almost eight times that of whites ("whites" includes most Hispanics), and were victimized at a rate six times that of whites. Most homicides were intraracial, with 93% of black victims killed by blacks and 84% of white victims killed by whites.[405] In 2012, Louisiana had the highest rate of murder and non-negligent manslaughter in the U.S., and New Hampshire the lowest.[406] The FBI's Uniform Crime Reports estimates that there were 3,246 violent and property crimes per 100,000 residents in 2012, for a total of over 9 million total crimes.[407]		Capital punishment is sanctioned in the United States for certain federal and military crimes, and used in 31 states.[408][409] No executions took place from 1967 to 1977, owing in part to a U.S. Supreme Court ruling striking down arbitrary imposition of the death penalty. In 1976, that Court ruled that, under appropriate circumstances, capital punishment may constitutionally be imposed. Since the decision there have been more than 1,300 executions, a majority of these taking place in three states: Texas, Virginia, and Oklahoma.[410] Meanwhile, several states have either abolished or struck down death penalty laws. In 2015, the country had the fifth-highest number of executions in the world, following China, Iran, Pakistan and Saudi Arabia.[411]		The United States has the highest documented incarceration rate and total prison population in the world.[412] At the start of 2008, more than 2.3 million people were incarcerated, more than one in every 100 adults.[413] In December 2012, the combined U.S. adult correctional systems supervised about 6,937,600 offenders. About 1 in every 35 adult residents in the United States was under some form of correctional supervision in December 2012, the lowest rate observed since 1997.[414] The prison population has quadrupled since 1980,[415] and state and local spending on prisons and jails has grown three times as much as that spent on public education during the same period.[416] However, the imprisonment rate for all prisoners sentenced to more than a year in state or federal facilities is 478 per 100,000 in 2013[417] and the rate for pre-trial/remand prisoners is 153 per 100,000 residents in 2012.[418] The country's high rate of incarceration is largely due to changes in sentencing guidelines and drug policies.[419] According to the Federal Bureau of Prisons, the majority of inmates held in federal prisons are convicted of drug offenses.[420] The privatization of prisons and prison services which began in the 1980s has been a subject of debate.[421][422] In 2008, Louisiana had the highest incarceration rate,[423] and Maine the lowest.[424]		The United States has a capitalist mixed economy[433] which is fueled by abundant natural resources and high productivity.[434] According to the International Monetary Fund, the U.S. GDP of $16.8 trillion constitutes 24% of the gross world product at market exchange rates and over 19% of the gross world product at purchasing power parity (PPP).[435]		The US's nominal GDP is estimated to be $17.528 trillion as of 2014[update][436] From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the G7.[437] The country ranks ninth in the world in nominal GDP per capita and sixth in GDP per capita at PPP.[435] The U.S. dollar is the world's primary reserve currency.[438]		The United States is the largest importer of goods and second-largest exporter, though exports per capita are relatively low. In 2010, the total U.S. trade deficit was $635 billion.[439] Canada, China, Mexico, Japan, and Germany are its top trading partners.[440] In 2010, oil was the largest import commodity, while transportation equipment was the country's largest export.[439] Japan is the largest foreign holder of U.S. public debt.[441] The largest holder of the U.S. debt are American entities, including federal government accounts and the Federal Reserve, who hold the majority of the debt.[442][443][444][445][fn 12]		In 2009, the private sector was estimated to constitute 86.4% of the economy, with federal government activity accounting for 4.3% and state and local government activity (including federal transfers) the remaining 9.3%.[448] The number of employees at all levels of government outnumber those in manufacturing by 1.7 to 1.[449] While its economy has reached a postindustrial level of development and its service sector constitutes 67.8% of GDP, the United States remains an industrial power.[450] The leading business field by gross business receipts is wholesale and retail trade; by net income it is manufacturing.[451] In the franchising business model, McDonald's and Subway are the two most recognized brands in the world. Coca-Cola is the most recognized soft drink company in the world.[452]		Chemical products are the leading manufacturing field.[453] The United States is the largest producer of oil in the world, as well as its second-largest importer.[454] It is the world's number one producer of electrical and nuclear energy, as well as liquid natural gas, sulfur, phosphates, and salt. The National Mining Association provides data pertaining to coal and minerals that include beryllium, copper, lead, magnesium, zinc, titanium and others.[455][456]		Agriculture accounts for just under 1% of GDP,[450] yet the United States is the world's top producer of corn[457] and soybeans.[458] The National Agricultural Statistics Service maintains agricultural statistics for products that include peanuts, oats, rye, wheat, rice, cotton, corn, barley, hay, sunflowers, and oilseeds. In addition, the United States Department of Agriculture (USDA) provides livestock statistics regarding beef, poultry, pork, and dairy products. The country is the primary developer and grower of genetically modified food, representing half of the world's biotech crops.[459]		Consumer spending comprises 68% of the U.S. economy in 2015.[460] In August 2010, the American labor force consisted of 154.1 million people. With 21.2 million people, government is the leading field of employment. The largest private employment sector is health care and social assistance, with 16.4 million people. About 12% of workers are unionized, compared to 30% in Western Europe.[461] The World Bank ranks the United States first in the ease of hiring and firing workers.[462] The United States is ranked among the top three in the Global Competitiveness Report as well. It has a smaller welfare state and redistributes less income through government action than European nations tend to.[463]		The United States is the only advanced economy that does not guarantee its workers paid vacation[464] and is one of just a few countries in the world without paid family leave as a legal right, with the others being Papua New Guinea, Suriname and Liberia.[465] While federal law currently does not require sick leave, it is a common benefit for government workers and full-time employees at corporations.[466] 74% of full-time American workers get paid sick leave, according to the Bureau of Labor Statistics, although only 24% of part-time workers get the same benefits.[466] In 2009, the United States had the third-highest workforce productivity per person in the world, behind Luxembourg and Norway. It was fourth in productivity per hour, behind those two countries and the Netherlands.[467]		The 2008–2012 global recession significantly affected the United States, with output still below potential according to the Congressional Budget Office.[468] It brought high unemployment (which has been decreasing but remains above pre-recession levels), along with low consumer confidence, the continuing decline in home values and increase in foreclosures and personal bankruptcies, an escalating federal debt crisis, inflation, and rising petroleum and food prices. There remains a record proportion of long-term unemployed, continued decreasing household income, and tax and federal budget increases.[469][470][471]		Americans have the highest average household and employee income among OECD nations, and in 2007 had the second-highest median household income.[472][473][474] According to the Census Bureau, median household income was $53,657 in 2014.[475] Despite accounting for only 4.4% of the global population, Americans collectively possess 41.6% of the world's total wealth,[476] and Americans make up roughly half of the world's population of millionaires.[477] The Global Food Security Index ranked the U.S. number one for food affordability and overall food security in March 2013.[478] Americans on average have over twice as much living space per dwelling and per person as European Union residents, and more than every EU nation.[479] For 2013 the United Nations Development Programme ranked the United States 5th among 187 countries in its Human Development Index and 28th in its inequality-adjusted HDI (IHDI).[480]		There has been a widening gap between productivity and median incomes since the 1970s.[481] However, the gap between total compensation and productivity is not as wide because of increased employee benefits such as health insurance.[482] While inflation-adjusted ("real") household income had been increasing almost every year from 1947 to 1999, it has since been flat on balance and has even decreased recently.[483] According to Congressional Research Service, during this same period, immigration to the United States increased, while the lower 90% of tax filers incomes became stagnant, and eventually decreasing since 2000.[484] The rise in the share of total annual income received by the top 1 percent, which has more than doubled from 9 percent in 1976 to 20 percent in 2011, has significantly affected income inequality,[485] leaving the United States with one of the widest income distributions among OECD nations.[486] The post-recession income gains have been very uneven, with the top 1 percent capturing 95 percent of the income gains from 2009 to 2012.[487] The extent and relevance of income inequality is a matter of debate.[488][disputed – discuss][489]		Wealth, like income and taxes, is highly concentrated; the richest 10% of the adult population possess 72% of the country's household wealth, while the bottom half claim only 2%.[491] Between June 2007 and November 2008 the global recession led to falling asset prices around the world. Assets owned by Americans lost about a quarter of their value.[492] Since peaking in the second quarter of 2007, household wealth was down $14 trillion, but has since increased $14 trillion over 2006 levels.[493][494] At the end of 2014, household debt amounted to $11.8 trillion,[495] down from $13.8 trillion at the end of 2008.[496]		There were about 578,424 sheltered and unsheltered homeless persons in the U.S. in January 2014, with almost two-thirds staying in an emergency shelter or transitional housing program.[497] In 2011 16.7 million children lived in food-insecure households, about 35% more than 2007 levels, though only 1.1% of U.S. children, or 845,000, saw reduced food intake or disrupted eating patterns at some point during the year, and most cases were not chronic.[498] According to a 2014 report by the Census Bureau, one in five young adults lives in poverty today, up from one in seven in 1980.[499]		Personal transportation is dominated by automobiles, which operate on a network of 4 million miles (6.4 million km) of public roads,[501] including one of the world's longest highway systems at 57,000 miles (91700 km).[502] The world's second-largest automobile market,[503] the United States has the highest rate of per-capita vehicle ownership in the world, with 765 vehicles per 1,000 Americans.[504] About 40% of personal vehicles are vans, SUVs, or light trucks.[505] The average American adult (accounting for all drivers and non-drivers) spends 55 minutes driving every day, traveling 29 miles (47 km).[506]		Mass transit accounts for 9% of total U.S. work trips.[508][509] Transport of goods by rail is extensive, though relatively low numbers of passengers (approximately 31 million annually) use intercity rail to travel, partly because of the low population density throughout much of the U.S. interior.[510][511] However, ridership on Amtrak, the national intercity passenger rail system, grew by almost 37% between 2000 and 2010.[512] Also, light rail development has increased in recent years.[513] Bicycle usage for work commutes is minimal.[514]		The civil airline industry is entirely privately owned and has been largely deregulated since 1978, while most major airports are publicly owned.[515] The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways.[516] Of the world's 50 busiest passenger airports, 16 are in the United States, including the busiest, Hartsfield–Jackson Atlanta International Airport, and the fourth-busiest, O'Hare International Airport in Chicago.[517] In the aftermath of the 9/11 attacks of 2001, the Transportation Security Administration was created to police airports and commercial airliners.		The United States energy market is about 29,000 terawatt hours per year.[518] Energy consumption per capita is 7.8 tons (7076 kg) of oil equivalent per year, the 10th-highest rate in the world. In 2005, 40% of this energy came from petroleum, 23% from coal, and 22% from natural gas. The remainder was supplied by nuclear power and renewable energy sources.[519] The United States is the world's largest consumer of petroleum.[520]		For decades, nuclear power has played a limited role relative to many other developed countries, in part because of public perception in the wake of a 1979 accident. In 2007, several applications for new nuclear plants were filed.[521] The United States has 27% of global coal reserves.[522] It is the world's largest producer of natural gas and crude oil.[523]		Issues that affect water supply in the United States include droughts in the West, water scarcity, pollution, a backlog of investment, concerns about the affordability of water for the poorest, and a rapidly retiring workforce. Increased variability and intensity of rainfall as a result of climate change is expected to produce both more severe droughts and flooding, with potentially serious consequences for water supply and for pollution from combined sewer overflows.[524][525][fn 13]		American public education is operated by state and local governments, regulated by the United States Department of Education through restrictions on federal grants. In most states, children are required to attend school from the age of six or seven (generally, kindergarten or first grade) until they turn 18 (generally bringing them through twelfth grade, the end of high school); some states allow students to leave school at 16 or 17.[528]		About 12% of children are enrolled in parochial or nonsectarian private schools. Just over 2% of children are homeschooled.[529] The U.S. spends more on education per student than any nation in the world, spending more than $11,000 per elementary student in 2010 and more than $12,000 per high school student.[530] Some 80% of U.S. college students attend public universities.[531]		The United States has many competitive private and public institutions of higher education. The majority of the world's top universities listed by different ranking organizations are in the U.S.[532][533][534] There are also local community colleges with generally more open admission policies, shorter academic programs, and lower tuition. Of Americans 25 and older, 84.6% graduated from high school, 52.6% attended some college, 27.2% earned a bachelor's degree, and 9.6% earned graduate degrees.[535] The basic literacy rate is approximately 99%.[10][536] The United Nations assigns the United States an Education Index of 0.97, tying it for 12th in the world.[537]		As for public expenditures on higher education, the U.S. trails some other OECD nations but spends more per student than the OECD average, and more than all nations in combined public and private spending.[530][538] As of 2012[update], student loan debt exceeded one trillion dollars, more than Americans owe on credit cards.[539]		The United States is home to many cultures and a wide variety of ethnic groups, traditions, and values.[28][540] Aside from the Native American, Native Hawaiian, and Native Alaskan populations, nearly all Americans or their ancestors settled or immigrated within the past five centuries.[541] Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa.[28][542] More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as both a homogenizing melting pot, and a heterogeneous salad bowl in which immigrants and their descendants retain distinctive cultural characteristics.[28]		Core American culture was established by Protestant British colonists and shaped by the frontier settlement process, with the traits derived passed down to descendants and transmitted to immigrants through assimilation. Americans have traditionally been characterized by a strong work ethic, competitiveness, and individualism,[543] as well as a unifying belief in an "American creed" emphasizing liberty, equality, private property, democracy, rule of law, and a preference for limited government.[544] Americans are extremely charitable by global standards. According to a 2006 British study, Americans gave 1.67% of GDP to charity, more than any other nation studied, more than twice the second place British figure of 0.73%, and around twelve times the French figure of 0.14%.[545][546]		The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants.[547] Whether this perception is realistic has been a topic of debate.[548][549][550][551][437][552] While mainstream culture holds that the United States is a classless society,[553] scholars identify significant differences between the country's social classes, affecting socialization, language, and values.[554] Americans' self-images, social viewpoints, and cultural expectations are associated with their occupations to an unusually close degree.[555] While Americans tend greatly to value socioeconomic achievement, being ordinary or average is generally seen as a positive attribute.[556]		Mainstream American cuisine is similar to that in other Western countries. Wheat is the primary cereal grain with about three-quarters of grain products made of wheat flour[557] and many dishes use indigenous ingredients, such as turkey, venison, potatoes, sweet potatoes, corn, squash, and maple syrup which were consumed by Native Americans and early European settlers.[558] These home grown foods are part of a shared national menu on one of America's most popular holidays; Thanksgiving, when some Americans make traditional foods to celebrate the occasion.[559]		Characteristic dishes such as apple pie, fried chicken, pizza, hamburgers, and hot dogs derive from the recipes of various immigrants. French fries, Mexican dishes such as burritos and tacos, and pasta dishes freely adapted from Italian sources are widely consumed.[561] Americans drink three times as much coffee as tea.[562] Marketing by U.S. industries is largely responsible for making orange juice and milk ubiquitous breakfast beverages.[563][564]		American eating habits owe a great deal to that of their British culinary roots with some variations. Although American lands could grow newer vegetables that Britain could not, most colonists would not eat these new foods until accepted by Europeans.[565] Over time American foods changed to a point that food critic, John L. Hess stated in 1972: "Our founding fathers were as far superior to our present political leaders in the quality of their food as they were in the quality of their prose and intelligence".[566]		The American fast food industry, the world's largest,[567] pioneered the drive-through format in the 1940s.[568] Fast food consumption has sparked health concerns. During the 1980s and 1990s, Americans' caloric intake rose 24%;[561] frequent dining at fast food outlets is associated with what public health officials call the American "obesity epidemic".[569] Highly sweetened soft drinks are widely popular, and sugared beverages account for nine percent of American caloric intake.[570]		In the 18th and early 19th centuries, American art and literature took most of its cues from Europe. Writers such as Nathaniel Hawthorne, Edgar Allan Poe, and Henry David Thoreau established a distinctive American literary voice by the middle of the 19th century. Mark Twain and poet Walt Whitman were major figures in the century's second half; Emily Dickinson, virtually unknown during her lifetime, is now recognized as an essential American poet.[571] A work seen as capturing fundamental aspects of the national experience and character—such as Herman Melville's Moby-Dick (1851), Twain's The Adventures of Huckleberry Finn (1885), F. Scott Fitzgerald's The Great Gatsby (1925) and Harper Lee's To Kill a Mockingbird (1960)—may be dubbed the "Great American Novel".[572]		Twelve U.S. citizens have won the Nobel Prize in Literature, most recently Bob Dylan in 2016. William Faulkner, Ernest Hemingway and John Steinbeck are often named among the most influential writers of the 20th century.[573] Popular literary genres such as the Western and hardboiled crime fiction developed in the United States. The Beat Generation writers opened up new literary approaches, as have postmodernist authors such as John Barth, Thomas Pynchon, and Don DeLillo.[574]		The transcendentalists, led by Thoreau and Ralph Waldo Emerson, established the first major American philosophical movement. After the Civil War, Charles Sanders Peirce and then William James and John Dewey were leaders in the development of pragmatism. In the 20th century, the work of W. V. O. Quine and Richard Rorty, and later Noam Chomsky, brought analytic philosophy to the fore of American philosophical academia. John Rawls and Robert Nozick led a revival of political philosophy. Cornel West and Judith Butler have led a continental tradition in American philosophical academia. Chicago school economists like Milton Friedman, James M. Buchanan, and Thomas Sowell have affected various fields in social and political philosophy.[575][576]		In the visual arts, the Hudson River School was a mid-19th-century movement in the tradition of European naturalism. The realist paintings of Thomas Eakins are now widely celebrated. The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene.[577] Georgia O'Keeffe, Marsden Hartley, and others experimented with new, individualistic styles. Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States. The tide of modernism and then postmodernism has brought fame to American architects such as Frank Lloyd Wright, Philip Johnson, and Frank Gehry.[578] Americans have long been important in the modern artistic medium of photography, with major photographers including Alfred Stieglitz, Edward Steichen, and Ansel Adams.[579]		One of the first major promoters of American theater was impresario P. T. Barnum, who began operating a lower Manhattan entertainment complex in 1841. The team of Harrigan and Hart produced a series of popular musical comedies in New York starting in the late 1870s. In the 20th century, the modern musical form emerged on Broadway; the songs of musical theater composers such as Irving Berlin, Cole Porter, and Stephen Sondheim have become pop standards. Playwright Eugene O'Neill won the Nobel literature prize in 1936; other acclaimed U.S. dramatists include multiple Pulitzer Prize winners Tennessee Williams, Edward Albee, and August Wilson.[581]		Though little known at the time, Charles Ives's work of the 1910s established him as the first major U.S. composer in the classical tradition, while experimentalists such as Henry Cowell and John Cage created a distinctive American approach to classical composition. Aaron Copland and George Gershwin developed a new synthesis of popular and classical music.		Choreographers Isadora Duncan and Martha Graham helped create modern dance, while George Balanchine and Jerome Robbins were leaders in 20th-century ballet.		The rhythmic and lyrical styles of African-American music have deeply influenced American music at large, distinguishing it from European traditions. Elements from folk idioms such as the blues and what is now known as old-time music were adopted and transformed into popular genres with global audiences. Jazz was developed by innovators such as Louis Armstrong and Duke Ellington early in the 20th century. Country music developed in the 1920s, and rhythm and blues in the 1940s.[582]		Elvis Presley and Chuck Berry were among the mid-1950s pioneers of rock and roll. In the 1960s, Bob Dylan emerged from the folk revival to become one of America's most celebrated songwriters and James Brown led the development of funk. More recent American creations include hip hop and house music. American pop stars such as Presley, Michael Jackson, and Madonna have become global celebrities,[582] as have contemporary musical artists such as Taylor Swift, Britney Spears, Katy Perry, and Beyoncé as well as hip hop artists Jay-Z, Eminem and Kanye West.[583] Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing in worldwide sales.[584][585][586]		Hollywood, a northern district of Los Angeles, California, is one of the leaders in motion picture production.[587] The world's first commercial motion picture exhibition was given in New York City in 1894, using Thomas Edison's Kinetoscope.[588] The next year saw the first commercial screening of a projected film, also in New York, and the United States was in the forefront of sound film's development in the following decades. Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization.[589]		Director D. W. Griffith, the top American filmmaker during the silent film period, was central to the development of film grammar, and producer/entrepreneur Walt Disney was a leader in both animated film and movie merchandising.[590] Directors such as John Ford redefined the image of the American Old West and history, and, like others such as John Huston, broadened the possibilities of cinema with location shooting, with great influence on subsequent directors. The industry enjoyed its golden years, in what is commonly referred to as the "Golden Age of Hollywood", from the early sound period until the early 1960s,[591] with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures.[592][593] In the 1970s, film directors such as Martin Scorsese, Francis Ford Coppola and Robert Altman were a vital component in what became known as "New Hollywood" or the "Hollywood Renaissance",[594] grittier films influenced by French and Italian realist pictures of the post-war period.[595] Since, directors such as Steven Spielberg, George Lucas and James Cameron have gained renown for their blockbuster films, often characterized by high production costs, and in return, high earnings at the box office, with Cameron's Avatar (2009) earning more than $2 billion.[596]		Notable films topping the American Film Institute's AFI 100 list include Orson Welles's Citizen Kane (1941), which is frequently cited as the greatest film of all time,[597][598] Casablanca (1942), The Godfather (1972), Gone with the Wind (1939), Lawrence of Arabia (1962), The Wizard of Oz (1939), The Graduate (1967), On the Waterfront (1954), Schindler's List (1993), Singin' in the Rain (1952), It's a Wonderful Life (1946) and Sunset Boulevard (1950).[599] The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929,[600] and the Golden Globe Awards have been held annually since January 1944.[601]		American football is by several measures the most popular spectator sport;[603] the National Football League (NFL) has the highest average attendance of any sports league in the world, and the Super Bowl is watched by millions globally. Baseball has been regarded as the U.S. national sport since the late 19th century, with Major League Baseball (MLB) being the top league. Basketball and ice hockey are the country's next two leading professional team sports, with the top leagues being the National Basketball Association (NBA) and the National Hockey League (NHL). These four major sports, when played professionally, each occupy a season at different, but overlapping, times of the year. College football and basketball attract large audiences.[604] In soccer, the country hosted the 1994 FIFA World Cup, the men's national soccer team qualified for ten World Cups and the women's team has won the FIFA Women's World Cup three times; Major League Soccer is the sport's highest league in the United States (featuring 19 American and 3 Canadian teams). The market for professional sports in the United States is roughly $69 billion, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.[605]		Eight Olympic Games have taken place in the United States. As of 2014, the United States has won 2,400 medals at the Summer Olympic Games, more than any other country, and 281 in the Winter Olympic Games, the second most behind Norway.[606] While most major U.S. sports have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, some of which have become popular in other countries. Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate Western contact.[607] The most watched individual sports are golf and auto racing, particularly NASCAR.[608][609] Rugby union is considered the fastest growing sport in the U.S., with registered players numbered at 115,000+ and a further 1.2 million participants.[610]		The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), the American Broadcasting Company (ABC), and Fox. The four major broadcast television networks are all commercial entities. Cable television offers hundreds of channels catering to a variety of niches.[611] Americans listen to radio programming, also largely commercial, on average just over two-and-a-half hours a day.[612]		In 1998, the number of U.S. commercial radio stations had grown to 4,793 AM stations and 5,662 FM stations. In addition, there are 1,460 public radio stations. Most of these stations are run by universities and public authorities for educational purposes and are financed by public or private funds, subscriptions and corporate underwriting. Much public-radio broadcasting is supplied by NPR (formerly National Public Radio). NPR was incorporated in February 1970 under the Public Broadcasting Act of 1967; its television counterpart, PBS, was also created by the same legislation. (NPR and PBS are operated separately from each other.) As of September 30, 2014[update], there are 15,433 licensed full-power radio stations in the U.S. according to the U.S. Federal Communications Commission (FCC).[613]		Well-known newspapers are The Wall Street Journal, The New York Times and USA Today. Although the cost of publishing has increased over the years, the price of newspapers has generally remained low, forcing newspapers to rely more on advertising revenue and on articles provided by a major wire service, such as the Associated Press or Reuters, for their national and world coverage. With very few exceptions, all the newspapers in the U.S. are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or in a situation that is increasingly rare, by individuals or families. Major cities often have "alternative weeklies" to complement the mainstream daily papers, for example, New York City's The Village Voice or Los Angeles' LA Weekly, to name two of the best-known. Major cities may also support a local business journal, trade papers relating to local industries, and papers for local ethnic and social groups. Early versions of the American newspaper comic strip and the American comic book began appearing in the 19th century. In 1938, Superman, the comic book superhero of DC Comics, developed into an American icon.[614] Aside from web portals and search engines, the most popular websites are Facebook, YouTube, Wikipedia, Yahoo!, eBay, Amazon, and Twitter.[615]		More than 800 publications are produced in Spanish, the second most commonly used language in the United States behind English.[616][617]		The United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts were developed by the U.S. War Department by the Federal Armories during the first half of the 19th century. This technology, along with the establishment of a machine tool industry, enabled the U.S. to have large scale manufacturing of sewing machines, bicycles and other items in the late 19th century and became known as the American system of manufacturing. Factory electrification in the early 20th century and introduction of the assembly line and other labor saving techniques created the system called mass production.[618]		In 1876, Alexander Graham Bell was awarded the first U.S. patent for the telephone. Thomas Edison's research laboratory, one of the first of its kind, developed the phonograph, the first long-lasting light bulb, and the first viable movie camera.[619] The latter lead to emergence of the worldwide entertainment industry. In the early 20th century, the automobile companies of Ransom E. Olds and Henry Ford popularized the assembly line. The Wright brothers, in 1903, made the first sustained and controlled heavier-than-air powered flight.[620]		The rise of Fascism and Nazism in the 1920s and 1930s led many European scientists, including Albert Einstein, Enrico Fermi, and John von Neumann, to immigrate to the United States.[621] During World War II, the Manhattan Project developed nuclear weapons, ushering in the Atomic Age, while the Space Race produced rapid advances in rocketry, materials science, and aeronautics.[622][623]		The invention of the transistor in the 1950s, a key active component in practically all modern electronics, led to many technological developments and a significant expansion of the U.S. technology industry.[624][625][626] This in turn led to the establishment of many new technology companies and regions around the country such as in Silicon Valley in California. Advancements by American microprocessor companies such as Advanced Micro Devices (AMD), and Intel along with both computer software and hardware companies that include Adobe Systems, Apple Inc., IBM, Microsoft, and Sun Microsystems created and popularized the personal computer. The ARPANET was developed in the 1960s to meet Defense Department requirements, and became the first of a series of networks which evolved into the Internet.[627]		These advancements then lead to greater personalization of technology for individual use.[628] As of 2013[update], 83.8% of American households owned at least one computer, and 73.3% had high-speed Internet service.[629] 91% of Americans also own a mobile phone as of May 2013[update].[630] The United States ranks highly with regard to freedom of use of the internet.[631]		In the 21st century, approximately two-thirds of research and development funding comes from the private sector.[632] The United States leads the world in scientific research papers and impact factor.[633]		The United States has a life expectancy of 79.8 years at birth, up from 75.2 years in 1990.[634][635][636] The infant mortality rate of 6.17 per thousand places the United States 56th-lowest out of 224 countries.[637]		Increasing obesity in the United States and health improvements elsewhere contributed to lowering the country's rank in life expectancy from 11th in the world in 1987, to 42nd in 2007.[638] Obesity rates have more than doubled in the last 30 years, are the highest in the industrialized world, and are among the highest anywhere.[639][640] Approximately one-third of the adult population is obese and an additional third is overweight.[641] Obesity-related type 2 diabetes is considered epidemic by health care professionals.[642]		In 2010, coronary artery disease, lung cancer, stroke, chronic obstructive pulmonary diseases, and traffic accidents caused the most years of life lost in the U.S. Low back pain, depression, musculoskeletal disorders, neck pain, and anxiety caused the most years lost to disability. The most deleterious risk factors were poor diet, tobacco smoking, obesity, high blood pressure, high blood sugar, physical inactivity, and alcohol use. Alzheimer's disease, drug abuse, kidney disease and cancer, and falls caused the most additional years of life lost over their age-adjusted 1990 per-capita rates.[636] U.S. teenage pregnancy and abortion rates are substantially higher than in other Western nations, especially among blacks and Hispanics.[643]		The U.S. is a global leader in medical innovation. America solely developed or contributed significantly to 9 of the top 10 most important medical innovations since 1975 as ranked by a 2001 poll of physicians, while the European Union and Switzerland together contributed to five.[644] Since 1966, more Americans have received the Nobel Prize in Medicine than the rest of the world combined. From 1989 to 2002, four times more money was invested in private biotechnology companies in America than in Europe.[645] The U.S. health-care system far outspends any other nation, measured in both per capita spending and percentage of GDP.[646]		Health-care coverage in the United States is a combination of public and private efforts and is not universal. In 2014, 13.4% of the population did not carry health insurance.[647] The subject of uninsured and underinsured Americans is a major political issue.[648][649] In 2006, Massachusetts became the first state to mandate universal health insurance.[650] Federal legislation passed in early 2010 would ostensibly create a near-universal health insurance system around the country by 2014, though the bill and its ultimate effect are issues of controversy.[651][652]		
The Wayback Machine is a digital archive of the World Wide Web and other information on the Internet created by the Internet Archive, a nonprofit organization, based in San Francisco, California, United States.		The Internet Archive launched the Wayback Machine in October 2001.[4][5] It was set up by Brewster Kahle and Bruce Gilliat, and is maintained with content from Alexa Internet.[citation needed] The service enables users to see archived versions of web pages across time, which the archive calls a "three dimensional index".[citation needed]		Since 1996, the Wayback Machine has been archiving cached pages of websites onto its large cluster of Linux nodes.[citation needed] It revisits sites every few weeks or months and archives a new version.[citation needed] Sites can also be captured on the fly by visitors who enter the site's URL into a search box.[citation needed] The intent is to capture and archive content that otherwise would be lost whenever a site is changed or closed down.[citation needed] The overall vision of the machine's creators is to archive the entire Internet.[citation needed]		The name Wayback Machine was chosen as a reference to the "WABAC machine" (pronounced way-back), a time-traveling device used by the characters Mr. Peabody and Sherman in The Rocky and Bullwinkle Show, an animated cartoon.[6][7] In one of the animated cartoon's component segments, Peabody's Improbable History, the characters routinely used the machine to witness, participate in, and, more often than not, alter famous events in history.[citation needed]		Software has been developed to "crawl" the web and download all publicly accessible World Wide Web pages, the Gopher hierarchy, the Netnews (Usenet) bulletin board system, and downloadable software.[8] The information collected by these "crawlers" does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. To overcome inconsistencies in partially cached websites, Archive-It.org was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives.[citation needed]		Information had been kept on digital tape for five years, with Kahle occasionally allowing researchers and scientists to tap into the clunky database.[9] When the archive reached its fifth anniversary, it was unveiled and opened to the public in a ceremony at the University of California, Berkeley.[citation needed]		Snapshots usually become available more than six months after they are archived or, in some cases, even later; it can take twenty-four months or longer.[10] The frequency of snapshots is variable, so not all tracked website updates are recorded. Sometimes there are intervals of several weeks or years between snapshots.[citation needed]		After August 2008 sites had to be listed on the Open Directory in order to be included.[11] According to Jeff Kaplan of the Internet Archive in November 2010, other sites were still being archived,[12] but more recent captures would become visible only after the next major indexing, an infrequent operation.[citation needed]		As of 2009[update], the Wayback Machine contained approximately three petabytes of data and was growing at a rate of 100 terabytes each month;[13] the growth rate reported in 2003 was 12 terabytes/month. The data is stored on PetaBox rack systems manufactured by Capricorn Technologies.[14]		In 2009, the Internet Archive migrated its customized storage architecture to Sun Open Storage, and hosts a new data center in a Sun Modular Datacenter on Sun Microsystems' California campus.[15]		In 2011 a new, improved version of the Wayback Machine, with an updated interface and fresher index of archived content, was made available for public testing.[16]		In March 2011, it was said on the Wayback Machine forum that "The Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year".[17]		In January 2013, the company announced a ground-breaking milestone of 240 billion URLs.[18]		In October 2013, the company announced the "Save a Page" feature[19] which allows any Internet user to archive the contents of a URL. This became a threat of abuse by the service for hosting malicious binaries.[20][21]		As of December 2014[update], the Wayback Machine contained almost nine petabytes of data and was growing at a rate of about 20 terabytes each week.[22]		As of July 2016[update], the Wayback Machine reportedly contained around 15 petabytes of data.[23]		Between October 2013 and March 2015 the website's global Alexa rank changed from 162[24] to 208.[25]		Historically, Wayback Machine respected the robots exclusion standard (robots.txt) in determining if a website would be crawled or not; or if already crawled, if its archives would be publicly viewable. Website owners had the option to opt-out of Wayback Machine through the use of robots.txt. It applied robots.txt rules retroactively; if a site blocked the Internet Archive, any previously archived pages from the domain were immediately rendered unavailable as well. In addition the Internet Archive stated, "Sometimes a website owner will contact us directly and ask us to stop crawling or archiving a site. We comply with these requests."[37] In addition, the website says: "The Internet Archive is not interested in preserving or offering access to Web sites or other Internet documents of persons who do not want their materials in the collection."[38]		This policy began to relax in 2017, when it stopped honoring robots.txt on U.S. government and military web sites for both crawling and displaying web pages. As of April 2017, Wayback is exploring ignoring robots.txt more broadly, not just for U.S. government websites.[39][40][41][42]		The site is frequently used by journalists and citizens to review dead websites, dated news reports or changes to website contents. Its content has been used to hold politicians accountable and expose battlefield lies.[43]		In 2014 an archived social media page of separatist rebel leader in Ukraine Igor Girkin showed him boasting about his troops having shot down a suspected Ukrainian military airplane before it became known that the plane actually was a civilian Malaysian Airlines jet after which he deleted the post and blamed Ukraine's military.[43][44]		In 2017 the March for Science originated from a discussion on reddit that indicated someone had visited Archive.org and discovered that all references to climate change had been deleted from the White House website. In response, a user commented, "There needs to be a Scientists' March on Washington".[45][46][47]		Furthermore, the site is used heavily for verification, providing access to references and content creation by Wikipedia editors.[citation needed]		In a 2009 case, Netbula, LLC v. Chordiant Software Inc., defendant Chordiant filed a motion to compel Netbula to disable the robots.txt file on its website that was causing the Wayback Machine to retroactively remove access to previous versions of pages it had archived from Netbula's site, pages that Chordiant believed would support its case.[48]		Netbula objected to the motion on the ground that defendants were asking to alter Netbula's website and that they should have subpoenaed Internet Archive for the pages directly.[49] An employee of Internet Archive filed a sworn statement supporting Chordiant's motion, however, stating that it could not produce the web pages by any other means "without considerable burden, expense and disruption to its operations."[48]		Magistrate Judge Howard Lloyd in the Northern District of California, San Jose Division, rejected Netbula's arguments and ordered them to disable the robots.txt blockage temporarily in order to allow Chordiant to retrieve the archived pages that they sought.[48]		In an October 2004 case, Telewizja Polska USA, Inc. v. Echostar Satellite, No. 02 C 3293, 65 Fed. R. Evid. Serv. 673 (N.D. Ill. Oct. 15, 2004), a litigant attempted to use the Wayback Machine archives as a source of admissible evidence, perhaps for the first time. Telewizja Polska is the provider of TVP Polonia and EchoStar operates the Dish Network. Prior to the trial proceedings, EchoStar indicated that it intended to offer Wayback Machine snapshots as proof of the past content of Telewizja Polska's website. Telewizja Polska brought a motion in limine to suppress the snapshots on the grounds of hearsay and unauthenticated source, but Magistrate Judge Arlander Keys rejected Telewizja Polska's assertion of hearsay and denied TVP's motion in limine to exclude the evidence at trial.[50][51] At the trial, however, district Court Judge Ronald Guzman, the trial judge, overruled Magistrate Keys' findings,[citation needed] and held that neither the affidavit of the Internet Archive employee nor the underlying pages (i.e., the Telewizja Polska website) were admissible as evidence. Judge Guzman reasoned that the employee's affidavit contained both hearsay and inconclusive supporting statements, and the purported web page printouts were not self-authenticating.[citation needed]		Provided some additional requirements are met (e.g., providing an authoritative statement of the archivist), the United States patent office and the European Patent Office will accept date stamps from the Internet Archive as evidence of when a given Web page was accessible to the public. These dates are used to determine if a Web page is available as prior art for instance in examining a patent application.[52]		There are technical limitations to archiving a website, and as a consequence, it is possible for opposing parties in litigation to misuse the results provided by website archives. This problem can be exacerbated by the practice of submitting screen shots of web pages in complaints, answers, or expert witness reports, when the underlying links are not exposed and therefore, can contain errors. For example, archives such as the Wayback Machine do not fill out forms and therefore, do not include the contents of non-RESTful e-commerce databases in their archives.[53]		In Europe the Wayback Machine could be interpreted as violating copyright laws. Only the content creator can decide where their content is published or duplicated, so the Archive would have to delete pages from its system upon request of the creator.[54] The exclusion policies for the Wayback Machine may be found in the FAQ section of the site.[citation needed]		A number of cases have been brought against the Internet Archive specifically for its Wayback Machine archiving efforts.		In late 2002, the Internet Archive removed various sites that were critical of Scientology from the Wayback Machine.[55] An error message stated that this was in response to a "request by the site owner".[56] Later, it was clarified that lawyers from the Church of Scientology had demanded the removal and that the site owners did not want their material removed.[57]		In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive's Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their website from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the DMCA and the Computer Fraud and Abuse Act. Healthcare Advocates claimed that, since they had installed a robots.txt file on their website, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff website from the Wayback Machine.[58] The lawsuit was settled out of court.[59]		In December 2005, activist Suzanne Shell filed suit demanding Internet Archive pay her US $100,000 for archiving her website profane-justice.org between 1999 and 2004.[60][61] Internet Archive filed a declaratory judgment action in the United States District Court for the Northern District of California on January 20, 2006, seeking a judicial determination that Internet Archive did not violate Shell's copyright. Shell responded and brought a countersuit against Internet Archive for archiving her site, which she alleges is in violation of her terms of service.[62] On February 13, 2007, a judge for the United States District Court for the District of Colorado dismissed all counterclaims except breach of contract.[61] The Internet Archive did not move to dismiss copyright infringement claims Shell asserted arising out of its copying activities, which would also go forward.[63]		On April 25, 2007, Internet Archive and Suzanne Shell jointly announced the settlement of their lawsuit.[60] The Internet Archive said it "...has no interest in including materials in the Wayback Machine of persons who do not wish to have their Web content archived. We recognize that Ms. Shell has a valid and enforceable copyright in her Web site and we regret that the inclusion of her Web site in the Wayback Machine resulted in this litigation." Shell said, "I respect the historical value of Internet Archive's goal. I never intended to interfere with that goal nor cause it any harm."[64]		In 2013–14, a pornographic actor tried to remove archived images of himself from the WayBack Machine's archive, first by sending multiple DMCA requests to the archive, and then by appealing to the Federal Court of Canada.[65][66]		In 2005, Yahoo! Search began to provide links to other versions of pages archived on the Wayback Machine.[67]		Archive.org is currently blocked in China.[68][69] After the site enabled the encrypted HTTPS protocol, the Internet Archive was blocked in its entirety in Russia in 2015.[70][71][43][needs update?]		Alison Macrina, director of the Library Freedom Project, notes that "while librarians deeply value individual privacy, we also strongly oppose censorship".[43]		There are known rare cases where online access to content which "for nothing" has put people in danger was disabled.[43]		In April 2017, emails of French presidential candidate Emmanuel Macron were leaked to the site and elsewhere.[72][73] As archive.org is not a website for publishing original leaks, uploading this data to the site first may have been intended to or may effectively cause harm to the site.[citation needed]		Other threats include natural disasters,[74] destruction (remote or physical),[citation needed] manipulation of the archive's contents (see also: cyberattack, backup), problematic copyright laws[75] and surveillance of the site's users.[76]		Kevin Vaughan suspects that in the long-term of multiple generations "next to nothing" will survive in a useful way besides "if we have continuity in our technological civilization" by which "a lot of the bare data will remain findable and searchable".[77]		Some find the Internet Archive, which describes itself to be built for the long-term,.[78] to be working furiously to capture data before it disappears without any long-term infrastructure to speak of.[79][copyright violation?]		
The Oxford English Dictionary (OED) is a descriptive dictionary of the English language, published by the Oxford University Press.[1] It traces the historical development of the English language, providing a comprehensive resource to scholars and academic researchers, as well as describing usage in its many variations throughout the world.[2][3] The second edition came to 21,728 pages in 20 volumes, published in 1989.		Work began on the dictionary in 1857, but it was not until 1884 that it began to be published in unbound fascicles as work continued on the project, under the name of A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society. In 1895, the title The Oxford English Dictionary (OED) was first used unofficially on the covers of the series, and in 1928 the full dictionary was republished in ten bound volumes. In 1933, the title The Oxford English Dictionary fully replaced the former name in all occurrences in its reprinting as twelve volumes with a one-volume supplement. More supplements came over the years until 1989, when the second edition was published. Since 2000, a third edition of the dictionary has been underway, approximately a third of which is now complete.[citation needed]		The first electronic version of the dictionary was made available in 1988. The online version has been available since 2000, and as of April 2014 was receiving over two million hits per month. The third edition of the dictionary will probably only appear in electronic form; Nigel Portwood, chief executive of Oxford University Press, thinks it unlikely that it will ever be printed.[4][5]						As a historical dictionary, the Oxford English Dictionary explains words by showing their development rather than merely their present-day usages.[6] Therefore, it shows definitions in the order that the sense of the word began being used, including word meanings which are no longer used. Each definition is shown with numerous short usage quotations; in each case, the first quotation shows the first recorded instance of the word that the editors are aware of and, in the case of words and senses no longer in current usage, the last quotation is the last known recorded usage. This allows the reader to get an approximate sense of the time period in which a particular word has been in use, and additional quotations help the reader to ascertain information about how the word is used in context, beyond any explanation that the dictionary editors can provide.		The format of the OED's entries has influenced numerous other historical lexicography projects. The forerunners to the OED, such as the early volumes of the Deutsches Wörterbuch, had initially provided few quotations from a limited number of sources, whereas the OED editors preferred larger groups of quite short quotations from a wide selection of authors and publications. This influenced later volumes of this and other lexicographical works.[7]		According to the publishers, it would take a single person 120 years to "key in" the 59 million words of the OED second edition, 60 years to proofread them, and 540 megabytes to store them electronically.[8] As of 30 November 2005, the Oxford English Dictionary contained approximately 301,100 main entries. Supplementing the entry headwords, there are 157,000 bold-type combinations and derivatives;[9] 169,000 italicized-bold phrases and combinations;[10] 616,500 word-forms in total, including 137,000 pronunciations; 249,300 etymologies; 577,000 cross-references; and 2,412,400 usage quotations. The dictionary's latest, complete print edition (second edition, 1989) was printed in 20 volumes, comprising 291,500 entries in 21,730 pages. The longest entry in the OED2 was for the verb set, which required 60,000 words to describe some 430 senses. As entries began to be revised for the OED3 in sequence starting from M, the longest entry became make in 2000, then put in 2007, then run in 2011.[11][12][13]		Despite its impressive size, the OED is neither the world's largest nor the earliest exhaustive dictionary of a language. Another earlier large dictionary is the Grimm brothers' dictionary of the German language, begun in 1838 and completed in 1961. The first edition of the Vocabolario degli Accademici della Crusca is the first great dictionary devoted to a modern European language (Italian) and was published in 1612; the first edition of Dictionnaire de l'Académie française dates from 1694. The official dictionary of Spanish is the Diccionario de la lengua española (produced, edited, and published by the Real Academia Española), and its first edition was published in 1780. The Kangxi dictionary of Chinese was published in 1716.[14]		The dictionary began as a Philological Society project of a small group of intellectuals in London (and unconnected to Oxford University):[15]:103–4,112 Richard Chenevix Trench, Herbert Coleridge, and Frederick Furnivall, who were dissatisfied with the existing English dictionaries. The Society expressed interest in compiling a new dictionary as early as 1844,[16] but it was not until June 1857 that they began by forming an "Unregistered Words Committee" to search for words that were unlisted or poorly defined in current dictionaries. In November, Trench's report was not a list of unregistered words; instead, it was the study On Some Deficiencies in our English Dictionaries, which identified seven distinct shortcomings in contemporary dictionaries:[17]		The Society ultimately realized that the number of unlisted words would be far more than the number of words in the English dictionaries of the 19th century, and shifted their idea from covering only words that were not already in English dictionaries to a larger project. Trench suggested that a new, truly comprehensive dictionary was needed. On 7 January 1858, the Society formally adopted the idea of a comprehensive new dictionary.[15]:107–8 Volunteer readers would be assigned particular books, copying passages illustrating word usage onto quotation slips. Later the same year, the Society agreed to the project in principle, with the title A New English Dictionary on Historical Principles (NED).[18]:ix–x		Richard Chenevix Trench (1807–1886) played the key role in the project's first months, but his Church of England appointment as Dean of Westminster meant that he could not give the dictionary project the time that it required. He withdrew and Herbert Coleridge became the first editor.[19]:8–9		On 12 May 1860, Coleridge's dictionary plan was published and research was started. His house was the first editorial office. He arrayed 100,000 quotation slips in a 54 pigeon-hole grid.[19]:9 In April 1861, the group published the first sample pages; later that month, Coleridge died of tuberculosis, aged 30.[18]:x		Furnivall then became editor; he was enthusiastic and knowledgeable, yet temperamentally ill-suited for the work.[15]:110 Many volunteer readers eventually lost interest in the project, as Furnivall failed to keep them motivated. Furthermore, many of the slips had been misplaced.		Furnivall believed that, since many printed texts from earlier centuries were not readily available, it would be impossible for volunteers to efficiently locate the quotations that the dictionary needed. As a result, he founded the Early English Text Society in 1864 and the Chaucer Society in 1868 to publish old manuscripts.[18]:xii Furnivall's preparatory efforts lasted 21 years and provided numerous texts for the use and enjoyment of the general public, as well as crucial sources for lexicographers, but they did not actually involve compiling a dictionary. Furnivall recruited more than 800 volunteers to read these texts and record quotations. While enthusiastic, the volunteers were not well trained and often made inconsistent and arbitrary selections. Ultimately, Furnivall handed over nearly two tons of quotation slips and other materials to his successor.[20]		In the 1870s, Furnivall unsuccessfully attempted to recruit both Henry Sweet and Henry Nicol to succeed him. He then approached James Murray, who accepted the post of editor. In the late 1870s, Furnivall and Murray met with several publishers about publishing the dictionary. In 1878, Oxford University Press agreed with Murray to proceed with the massive project; the agreement was formalized the following year.[15]:111–2 The dictionary project finally had a publisher 20 years after the idea was conceived. It was another 50 years before the entire dictionary was complete.		Late in his editorship, Murray learned that a prolific reader named W. C. Minor was a criminal lunatic.[15]:xiii Minor was a Yale University-trained surgeon and military officer in the American Civil War, and was confined to Broadmoor Asylum for the Criminally Insane after killing a man in London. Minor invented his own quotation-tracking system, allowing him to submit slips on specific words in response to editors' requests. The story of Murray and Minor later served as the central focus of The Surgeon of Crowthorne (US title: The Professor and the Madman[15]), a popular book about the creation of the OED.		During the 1870s, the Philological Society was concerned with the process of publishing a dictionary with such an immense scope. They had pages printed by publishers, but no publication agreement was reached; both the Cambridge University Press and the Oxford University Press were approached. The OUP finally agreed in 1879 (after two years of negotiating by Sweet, Furnivall, and Murray) to publish the dictionary and to pay Murray, who was both the editor and the Philological Society president. The dictionary was to be published as interval fascicles, with the final form in four 6,400-page volumes. They hoped to finish the project in ten years.[19]:1		Murray started the project, working in a corrugated iron outbuilding called the "Scriptorium" which was lined with wooden planks, book shelves, and 1,029 pigeon-holes for the quotation slips.[18]:xiii He tracked and regathered Furnivall's collection of quotation slips, which were found to concentrate on rare, interesting words rather than common usages. For instance, there were ten times as many quotations for abusion as for abuse.[21] He appealed, through newspapers distributed to bookshops and libraries, for readers who would report "as many quotations as you can for ordinary words" and for words that were "rare, obsolete, old-fashioned, new, peculiar or used in a peculiar way".[21] Murray had American philologist and liberal arts college professor Francis March manage the collection in North America; 1,000 quotation slips arrived daily to the Scriptorium and, by 1880, there were 2,500,000.[19]:15		The first dictionary fascicle was published on 1 February 1884—twenty-three years after Coleridge's sample pages. The full title was A New English Dictionary on Historical Principles; Founded Mainly on the Materials Collected by The Philological Society; the 352-page volume, words from A to Ant, cost 12s 6d.[19]:251 The total sales were a disappointing 4,000 copies.[22]:169		The OUP saw that it would take too long to complete the work with unrevised editorial arrangements. Accordingly, new assistants were hired and two new demands were made on Murray.[19]:32–33 The first was that he move from Mill Hill to Oxford, which he did in 1885. Murray had his Scriptorium re-erected on his new property.[18]:xvii[19]		Murray resisted the second demand: that if he could not meet schedule, he must hire a second, senior editor to work in parallel to him, outside his supervision, on words from elsewhere in the alphabet. Murray did not want to share the work, feeling that he would accelerate his work pace with experience. That turned out not to be so, and Philip Gell of the OUP forced the promotion of Murray's assistant Henry Bradley (hired by Murray in 1884), who worked independently in the British Museum in London beginning in 1888. In 1896, Bradley moved to Oxford University.[19]		Gell continued harassing Murray and Bradley with his business concerns—containing costs and speeding production—to the point where the project's collapse seemed likely. Newspapers reported the harassment, particularly the Saturday Review, and public opinion backed the editors.[22]:182–83 Gell was fired, and the university reversed his cost policies. If the editors felt that the dictionary would have to grow larger, it would; it was an important work, and worth the time and money to properly finish.		Neither Murray nor Bradley lived to see it. Murray died in 1915, having been responsible for words starting with A–D, H–K, O–P, and T, nearly half the finished dictionary; Bradley died in 1923, having completed E–G, L–M, S–Sh, St, and W–We. By then, two additional editors had been promoted from assistant work to independent work, continuing without much trouble. William Craigie started in 1901 and was responsible for N, Q–R, Si–Sq, U–V, and Wo–Wy.[18]:xix The OUP had previously thought London too far from Oxford but, after 1925, Craigie worked on the dictionary in Chicago, where he was a professor.[18]:xix[19] The fourth editor was Charles Talbut Onions, who compiled the remaining ranges starting in 1914: Su–Sz, Wh–Wo, and X–Z.[23]		In 1919–1920, J. R. R. Tolkien was employed by the OED, researching etymologies of the Waggle to Warlock range;[24] later he parodied the principal editors as "The Four Wise Clerks of Oxenford" in the story Farmer Giles of Ham.[25]		By early 1894, a total of 11 fascicles had been published, or about one per year: four for A–B, five for C, and two for E.[18] Of these, eight were 352 pages long, while the last one in each group was shorter to end at the letter break (which eventually became a volume break). At this point, it was decided to publish the work in smaller and more frequent instalments; once every three months beginning in 1895 there would be a fascicle of 64 pages, priced at 2s 6d. If enough material was ready, 128 or even 192 pages would be published together. This pace was maintained until World War I forced reductions in staff.[18]:xx Each time enough consecutive pages were available, the same material was also published in the original larger fascicles.[18]:xx Also in 1895, the title Oxford English Dictionary (OED) was first used. It then appeared only on the outer covers of the fascicles; the original title was still the official one and was used everywhere else.[18]:xx		The 125th and last fascicle covered words from Wise to the end of W and was published on 19 April 1928, and the full dictionary in bound volumes followed immediately.[18]:xx		William Shakespeare is the most-quoted writer in the completed dictionary, with Hamlet his most-quoted work. George Eliot (Mary Ann Evans) is the most-quoted female writer. Collectively, the Bible is the most-quoted work (but in many different translations); the most-quoted single work is Cursor Mundi.[8]		Between 1928 and 1933, enough additional material had been compiled to make a one-volume supplement, so the dictionary was reissued as the set of 12 volumes and a one-volume supplement in 1933.[18]		In 1933, Oxford had finally put the dictionary to rest; all work ended, and the quotation slips went into storage. However, the English language continued to change and, by the time 20 years had passed, the dictionary was outdated.[26]		There were three possible ways to update it. The cheapest would have been to leave the existing work alone and simply compile a new supplement of perhaps one or two volumes; but then anyone looking for a word or sense and unsure of its age would have to look in three different places. The most convenient choice for the user would have been for the entire dictionary to be re-edited and retypeset, with each change included in its proper alphabetical place; but this would have been the most expensive option, with perhaps 15 volumes required to be produced. The OUP chose a middle approach: combining the new material with the existing supplement to form a larger replacement supplement.		Robert Burchfield was hired in 1957 to edit the second supplement;[27] Onions turned 84 that year but was still able to make some contributions, as well. The work on the supplement was expected to take about seven years.[26] It actually took 29 years, by which time the new supplement (OEDS) had grown to four volumes, starting with A, H, O, and Sea. They were published in 1972, 1976, 1982, and 1986 respectively, bringing the complete dictionary to 16 volumes, or 17 counting the first supplement.		Burchfield emphasized the inclusion of modern-day language and, through the supplement, the dictionary was expanded to include a wealth of new words from the burgeoning fields of science and technology, as well as popular culture and colloquial speech. Burchfield said that he broadened the scope to include developments of the language in English-speaking regions beyond the United Kingdom, including North America, Australia, New Zealand, South Africa, India, Pakistan, and the Caribbean. Burchfield also removed some smaller entries that had been added to the 1933 supplement, for reasons of space;[28] in 2012, an analysis by lexicographer Sarah Ogilvie revealed that many of these entries were in fact foreign loanwords, despite Burchfield's attempt to include more such words. The proportion was estimated from a sample calculation to amount to 17% of the foreign loan words and words from regional forms of English. Many of these had only a single recorded usage, but it ran against what was thought to be the established OED editorial practice and a perception that he had opened up the dictionary to "World English".[29][30][31]		By the time the new supplement was completed, it was clear that the full text of the dictionary would now need to be computerized. Achieving this would require retyping it once, but thereafter it would always be accessible for computer searching – as well as for whatever new editions of the dictionary might be desired, starting with an integration of the supplementary volumes and the main text. Preparation for this process began in 1983, and editorial work started the following year under the administrative direction of Timothy J. Benbow, with John A. Simpson and Edmund S. C. Weiner as co-editors.[32] In 2016, Simpson published his memoir chronicling his years at the OED. See The Word Detective: Searching for the Meaning of It All at the Oxford English Dictionary - A Memoir. Basic Books, New York.		And so the New Oxford English Dictionary (NOED) project began. In the United States, more than 120 typists of the International Computaprint Corporation (now Reed Tech) started keying in over 350,000,000 characters, their work checked by 55 proof-readers in England.[32] Retyping the text alone was not sufficient; all the information represented by the complex typography of the original dictionary had to be retained, which was done by marking up the content in SGML.[32] A specialized search engine and display software were also needed to access it. Under a 1985 agreement, some of this software work was done at the University of Waterloo, Canada, at the Centre for the New Oxford English Dictionary, led by Frank Tompa and Gaston Gonnet; this search technology went on to become the basis for the Open Text Corporation.[33] Computer hardware, database and other software, development managers, and programmers for the project were donated by the British subsidiary of IBM; the colour syntax-directed editor for the project, LEXX, was written by Mike Cowlishaw of IBM.[34] The University of Waterloo, in Canada, volunteered to design the database. A. Walton Litz, an English professor at Princeton University who served on the Oxford University Press advisory council, was quoted in Time as saying "I've never been associated with a project, I've never even heard of a project, that was so incredibly complicated and that met every deadline."[35]		By 1989, the NOED project had achieved its primary goals, and the editors, working online, had successfully combined the original text, Burchfield's supplement, and a small amount of newer material, into a single unified dictionary. The word "new" was again dropped from the name, and the second edition of the OED, or the OED2, was published. The first edition retronymically became the OED1.		The Oxford English Dictionary 2 was printed in 20 volumes. For the first time, there was no attempt to start them on letter boundaries, and they were made roughly equal in size. The 20 volumes started with A, B.B.C., Cham, Creel, Dvandva, Follow, Hat, Interval, Look, Moul, Ow, Poise, Quemadero, Rob, Ser, Soot, Su, Thru, Unemancipated, and Wave.		The content of the OED2 is mostly just a reorganization of the earlier corpus, but the retypesetting provided an opportunity for two long-needed format changes. The headword of each entry was no longer capitalized, allowing the user to readily see those words that actually require a capital letter.[36] Murray had devised his own notation for pronunciation, there being no standard available at the time, whereas the OED2 adopted the modern International Phonetic Alphabet.[36][37] Unlike the earlier edition, all foreign alphabets except Greek were transliterated.[36]		The British quiz show Countdown has awarded the leather-bound complete version to the champions of each series since its inception in 1982.[38]		When the print version of the second edition was published in 1989, the response was enthusiastic. Author Anthony Burgess declared it "the greatest publishing event of the century", as quoted by the Los Angeles Times.[39] Time dubbed the book "a scholarly Everest",[35] and Richard Boston, writing for The Guardian, called it "one of the wonders of the world".[40]		The supplements and their integration into the second edition were a great improvement to the OED as a whole, but it was recognized that most of the entries were still fundamentally unaltered from the first edition. Much of the information in the dictionary published in 1989 was already decades out of date, though the supplements had made good progress towards incorporating new vocabulary. Yet many definitions contained disproven scientific theories, outdated historical information, and moral values that were no longer widely accepted.[41][42] Furthermore, the supplements had failed to recognize many words in the existing volumes as obsolete by the time of the second edition's publication, meaning that thousands of words were marked as current despite no recent evidence of their use.[43]		Accordingly, it was recognized that work on a third edition would have to begin to rectify these problems.[41] The first attempt to produce a new edition came with the Oxford English Dictionary Additions Series, a new set of supplements to complement the OED2 with the intention of producing a third edition from them.[44] The previous supplements appeared in alphabetical installments, whereas the new series had a full A–Z range of entries within each individual volume, with a complete alphabetical index at the end of all words revised so far, each listed with the volume number which contained the revised entry.[44]		However, in the end only three Additions volumes were published this way, two in 1993 and one in 1997,[45][46][47] each containing about 3,000 new definitions.[8] The possibilities of the World Wide Web and new computer technology in general meant that the processes of researching the dictionary and of publishing new and revised entries could be vastly improved. New text search databases offered vastly more material for the editors of the dictionary to work with, and with publication on the Web as a possibility, the editors could publish revised entries much more quickly and easily than ever before.[48] A new approach was called for, and for this reason it was decided to embark on a new, complete revision of the dictionary.		Beginning with the launch of the first OED Online site in 2000, the editors of the dictionary began a major revision project to create a completely revised third edition of the dictionary (OED3), expected to be completed in 2037[49][50] at a projected cost of about £34 million.[51]		Revisions were started at the letter M, with new material appearing every three months on the OED Online website. The editors chose to start the revision project from the middle of the dictionary in order that the overall quality of entries be made more even, since the later entries in the OED1 generally tended to be better than the earlier ones. However, in March 2008, the editors announced that they would alternate each quarter between moving forward in the alphabet as before and updating "key English words from across the alphabet, along with the other words which make up the alphabetical cluster surrounding them".[52] With the relaunch of the OED Online website in December 2010, alphabetical revision was abandoned altogether.[53]		The revision is expected to roughly double the dictionary in size.[5][54] Apart from general updates to include information on new words and other changes in the language, the third edition brings many other improvements, including changes in formatting and stylistic conventions to make entries clearer to read and enable more thorough searches to be made by computer, more thorough etymological information, and a general change of focus away from individual words towards more general coverage of the language as a whole.[48][55] While the original text drew its quotations mainly from literary sources such as novels, plays, and poetry, with additional material from newspapers and academic journals, the new edition will reference more kinds of material that were unavailable to the editors of previous editions, such as wills, inventories, account books, diaries, journals, and letters.[54]		John Simpson was the first chief editor of the OED3. He retired in 2013 and was replaced by Michael Proffitt, who is the eighth chief editor of the dictionary.[56]		The production of the new edition takes full advantage of computer technology, particularly since the June 2005 inauguration of the whimsically named "Perfect All-Singing All-Dancing Editorial and Notation Application", or "Pasadena". With this XML-based system, the attention of lexicographers can be directed more to matters of content than to presentation issues such as the numbering of definitions. The new system has also simplified the use of the quotations database, and enabled staff in New York to work directly on the dictionary in the same way as their Oxford-based counterparts.[57]		Other important computer uses include internet searches for evidence of current usage, and e-mail submissions of quotations by readers and the general public.[58]		Wordhunt was a 2005 appeal to the general public for help in providing citations for 50 selected recent words, and produced antedatings for many. The results were reported in a BBC TV series, Balderdash and Piffle. The OED's small army of devoted readers continue to contribute quotations: the department currently receives about 200,000 a year.[59]		OED currently contains over 600,000 entries.[60]		In 1971, the 13-volume OED1 (1933) was reprinted as a two-volume Compact Edition, by photographically reducing each page to one-half its linear dimensions; each compact edition page held four OED1 pages in a four-up ("4-up") format. The two volume letters were A and P; the first supplement was at the second volume's end.		The Compact Edition included, in a small slip-case drawer, a magnifying glass to help in reading reduced type. Many copies were inexpensively distributed through book clubs. In 1987, the second supplement was published as a third volume to the Compact Edition. In 1991, for the OED2, the compact edition format was re-sized to one-third of original linear dimensions, a nine-up ("9-up") format requiring greater magnification, but allowing publication of a single-volume dictionary. It was accompanied by a magnifying glass as before and A User's Guide to the "Oxford English Dictionary", by Donna Lee Berg.[61] After these volumes were published, though, book club offers commonly continued to sell the two-volume 1971 Compact Edition.[25]		Once the text of the dictionary was digitized and online, it was also available to be published on CD-ROM. The text of the first edition was made available in 1987.[62] Afterward, three versions of the second edition were issued. Version 1 (1992) was identical in content to the printed second edition, and the CD itself was not copy-protected. Version 2 (1999) included the Oxford English Dictionary Additions of 1993 and 1997.		Version 3.0 was released in 2002 with additional words from the OED3 and software improvements. Version 3.1.1 (2007) added support for hard disk installation, so that the user does not have to insert the CD to use the dictionary. It has been reported that this version will work on operating systems other than Microsoft Windows, using emulation programs.[63][64] Version 4.0 of the CD has been available since June 2009 and works with Windows 7 and Mac OS X (10.4 or later).[65] This version uses the CD drive for installation, running only from the hard drive.		On 14 March 2000, the Oxford English Dictionary Online (OED Online) became available to subscribers.[66] The online database contains the entire OED2 and is updated quarterly with revisions that will be included in the OED3 (see above). The online edition is the most up-to-date version of the dictionary available. The OED web site is not optimized for mobile devices, but the developers have stated that there are plans to provide an API that would enable developers to develop different interfaces for querying the OED.[67]		The price for an individual to use this edition is £195 or US$295 every year, even after a reduction in 2004; consequently, most subscribers are large organizations such as universities. Some public libraries and companies have subscribed, as well, including public libraries in the United Kingdom, where access is funded by the Arts Council,[68] and public libraries in New Zealand.[69][70] Individuals who belong to a library which subscribes to the service are able to use the service from their own home without charge.		The OED's utility and renown as a historical dictionary have led to numerous offspring projects and other dictionaries bearing the Oxford name, though not all are directly related to the OED itself.		The Shorter Oxford English Dictionary, originally started in 1902 and completed in 1933,[72] is an abridgement of the full work that retains the historical focus, but does not include any words which were obsolete before 1700 except those used by Shakespeare, Milton, Spenser, and the King James Bible.[73] A completely new edition was produced from the OED2 and published in 1993,[74] with further revisions following in 2002 and 2007.		The Concise Oxford Dictionary is a different work, which aims to cover current English only, without the historical focus. The original edition, mostly based on the OED1, was edited by Francis George Fowler and Henry Watson Fowler and published in 1911, before the main work was completed.[75] Revised editions appeared throughout the twentieth century to keep it up to date with changes in English usage.		In 1998 the New Oxford Dictionary of English (NODE) was published. While also aiming to cover current English, NODE was not based on the OED. Instead, it was an entirely new dictionary produced with the aid of corpus linguistics.[76] Once NODE was published, a similarly brand-new edition of the Concise Oxford Dictionary followed, this time based on an abridgement of NODE rather than the OED; NODE (under the new title of the Oxford Dictionary of English, or ODE) continues to be principal source for Oxford's product line of current-English dictionaries, including the New Oxford American Dictionary, with the OED now only serving as the basis for scholarly historical dictionaries.		The OED lists British headword spellings (e.g., labour, centre) with variants following (labor, center, etc.). For the suffix more commonly spelt -ise in British English, OUP policy dictates a preference for the spelling -ize, e.g., realize vs. realise and globalization vs. globalisation. The rationale is etymological, in that the English suffix is mainly derived from the Greek suffix -ιζειν, (-izein), or the Latin -izāre.[77] However, -ze is also sometimes treated as an Americanism insofar as the -ze suffix has crept into words where it did not originally belong, as with analyse (British English), which is spelt analyze in American English.[78][79]		Despite, and at the same time precisely because of, its claim of authority[80] on the English language, the Oxford English Dictionary has been criticised since at least the 1960s from various angles. It has become a target precisely because of its scope, its claims to authority, its British-centredness and relative neglect of World Englishes,[81] its implied but not acknowledged focus on literary language and, above all, its influence. The OED, as a commercial product, has always had to manoeuvre a thin line between PR, marketing and scholarship and one can argue that its biggest problem is the critical uptake of the work by the interested public. In his review of the 1982 supplement,[82] University of Oxford linguist Roy Harris writes that criticizing the OED is extremely difficult because "one is dealing not just with a dictionary but with a national institution", one that "has become, like the English monarchy, virtually immune from criticism in principle". He further notes that neologisms from respected "literary" authors such as Samuel Beckett and Virginia Woolf are included, whereas usage of words in newspapers or other less "respectable" sources hold less sway, even though they may be commonly used. He writes that the OED's "[b]lack-and-white lexicography is also black-and-white in that it takes upon itself to pronounce authoritatively on the rights and wrongs of usage", faulting the dictionary's prescriptive rather than descriptive usage. To Harris, this prescriptive classification of certain usages as "erroneous" and the complete omission of various forms and usages cumulatively represent the "social bias[es]" of the (presumably well-educated and wealthy) compilers. However, the identification of "erroneous and catachrestic" usages is being removed from third edition entries,[83][84] sometimes in favour of usage notes describing the attitudes to language which have previously led to these classifications.[85]		Harris also faults the editors' "donnish conservatism" and their adherence to prudish Victorian morals, citing as an example the non-inclusion of "various centuries-old 'four-letter words'" until 1972. However, no English dictionary included such words, for fear of possible prosecution under British obscenity laws, until after the conclusion of the Lady Chatterley's Lover obscenity trial in 1960. The first dictionary to include the word fuck was the Penguin English Dictionary of 1965.[86] Joseph Wright's English Dialect Dictionary had included shit in 1905.[87]		The OED's claims of authority have also been questioned by linguists such as Pius ten Hacken, who notes that the dictionary actively strives towards definitiveness and authority but can only achieve those goals in a limited sense, given the difficulties of defining the scope of what it includes.[88]		Founding editor James Murray was also reluctant to include scientific terms, despite their documentation, unless he felt that they were widely enough used. In 1902, he declined to add the word "radium" to the dictionary.[89][90]		In contrast, Tim Bray, co-creator of Extensible Markup Language (XML), credits the OED as the developing inspiration of that markup language.[91] Similarly, author Anu Garg, founder of Wordsmith.org, has called the Oxford English Dictionary a "lex icon".[92]		
The German student movement (also called 68er-Bewegung, movement of 1968, or soixante-huitaires) was a protest movement that took place during the late 1960s in West Germany. It was largely a reaction against the perceived authoritarianism and hypocrisy of the German government and other Western governments, and the poor living conditions of students. A wave of protests—some violent—swept West Germany, fueled by violent over-reaction by the police and encouraged by contemporary protest movements across the world. Following more than a century of conservatism among German students, the German student movement also marked a significant major shift to the left and radicalization of student activism.						In 1966, for the first time in fifteen years, the German economy went into recession and the FDP finally withdrew from Ludwig Erhard's CDU/CSU/FDP coalition government. With the forming of the CDU/CSU/SPD coalition government under Kurt Georg Kiesinger the voice of the opposition within the Bundestag was seriously weakened. This led some students to conclude that this encouraged authoritarian and anti-democratic attitudes in government and therefore justified and indeed necessitated the transfer of opposition from parliament to bodies outside it. At the same time, the shock of realizing that the Wirtschaftswunder could not last forever led many in the student body, influenced by Marxist economic theory, to believe that the economic wealth of the nation, instead of improving the standard of living of the working class, would destroy it and lead to an ever-growing gap between the rich and the poor.		Through their critical work on many different topics and the reactions of the public and the government itself, these main goals formed in the minds of the students:		The first goal was the source of all the others and thus the most important in their minds.		To summarize, the students rejected traditional, parliamentary decision making-processes, social injustice and the inequalities of wealth. They felt the need to overcome and change these things.		To the students, the German chapter of Fascism was not yet closed. Many former National Socialists were still working for the government or at the universities (in fact, then-Chancellor Kurt Georg Kiesinger had formerly been a member of the NSDAP) and the newly formed right-wing National Democratic Party of Germany (NPD) was attracting more and more voters. In addition to that, the students had to deal with the fact that they were identified as Germans and blamed for the crimes committed by their parents’ generation.		The students did not want to be held responsible for their parents’ deeds. But their parents acted as if it were no concern of theirs; when the students tried to show the public that the anti-fascist idea of the constitution was not yet established in German society, the government and the press felt extremely offended, feeling they had formed a democratic society and did not want it to be attacked.		To support its new economic policies the government wanted to change the universities, producing graduates faster by introducing a time limit on courses and limiting the number of students. The students, however, did not want to adjust to the needs of the economy and the government. Rather, they wanted to adjust the universities to their own principles. They wanted more rights in the running of universities, better-equipped workplaces and the expulsion of the professors who had been active during the Third Reich. The university boards did not react to the students' protest and introduced the time limit for studying.		When this time limit was introduced at the Free University of Berlin during the summer holidays of 1966 the students were not there, and so were unable to protest against it; instead, the first big sit-in of the German student movement happened when they returned after the holidays, with about 4,000 participants.[1] The events at the Free University of Berlin are representative of the events at all universities in Germany, as the same events were quickly repeated elsewhere.		Through their increasing interest in politics the students quickly engaged in discussions concerning the war in Vietnam. They formed the opinion that the United States had no right to fight in Vietnam, not only because of the victims but mainly because of what they saw as an imperialistic foreign policy.		The government, however, supported the USA in the Vietnam War, and provided humanitarian and economic support to South Vietnam.[2] For this reason, university boards put a ban on political activities by students (e.g., discussions) at the universities. They explained this act by saying that science should always be neutral. The students wanted to be able to act politically, not only because of the war in Vietnam, but to protest against the conditions in the Third World as students and not only as individuals.		The students were strongly opposed to the German Emergency Acts which were due to be passed, which would allow the government to limit civil rights in the case of an emergency. Among other things, they would allow the government to restrict freedom of movement and to limit privacy and confidentiality of telecommunications correspondence.		By 1966 the number of students that were interested in the conflict between the students and the authorities had increased. Many of those who had not been interested before became at least passively interested by now. This newly formed public took part in the demonstrations, sit-ins and other protest actions arranged by the students and their organizations, e.g., the Sozialistischer Deutscher Studentenbund (SDS).		The government tried to defuse the situation by decreasing the funds for universities and student organizations and by turning public opinion against the students with the help of the press. The view that students should study and not demonstrate grew stronger. The students were also repressed in the streets by the police. Yet, the more pressure the government put on the students, the more they came together.		On June 2, 1967 the conflict would finally escalate. Students had organized demonstrations against the official visit by the Shah of Iran. In their opinion, the German government was demonstrating a positive attitude towards a dictatorial government that was suppressing and torturing its own people.		During the first demonstration in front of the Opera House, which the Shah was visiting, the police of Berlin and the Iranian service attacked the protestors. In the turmoil, the unarmed student Benno Ohnesorg was shot in the head from behind and killed by Polizeiobermeister (Police Sergeant) Karl-Heinz Kurras (in fact, Karl-Heinz Kurras was an agent of the East German secret service, the Stasi).		The following days saw many demonstrations throughout the whole republic against what was perceived as police brutality. The students in Berlin, however, were anxious and in a desperate situation. The police were preventing them from gathering in public, the universities had submitted their authority to the government and the press wrote that the students were the brutal and aggressive component of the demonstrations and that they had provoked the death of Benno Ohnesorg. Even though there were some students groups supporting the idea of a violent revolution the protesting students were mostly peaceful.		For the following days the students took over control of the Free University of Berlin. Finally being able to meet again, they used the time to discuss and reflect on the events of the past days.		The spirit of the students in Berlin spread across the whole country. In autumn 1967 there were organized protest groups at nearly all universities in Germany. In the following months some of the largest and most brutal demonstrations in the history of the German republic happened. The press, especially the tabloid Bild-Zeitung newspaper was telling the public what to think about those protesters[citation needed]. Its publisher, Axel Springer, did not publish any positive articles about the students. Springer supported the government and was spreading the government's views among its readers[citation needed].		At Easter 1968, there was an attempted assassination of one of the most important members of the SDS, Rudi Dutschke. The students were outraged because the “Springer” press and the government had named Rudi Dutschke their “public enemy”. Overnight students all over Germany organized actions to block the delivery of the Bild-Zeitung by building blockades and protesting in front of “Springer” buildings. During these actions about 400 students were injured and two died. Rudi Dutschke died in 1979 of the late after-effects of his injury.		The revolt against the government reached its climax in May 1968. Students, schoolchildren and members of workers' unions formed a group of 80,000 people who demonstrated in the capital Bonn against the emergency legislature. Even though the students mobilized as many people as possible to support their actions they could not stop the Bundestag from passing the new law.		This failure marks the beginning of the end for the student movement. The former union of many small student groups representing different theories on the same topics was falling apart because they were blaming other groups' theories and thinking for the failure of the whole movement. By the end of the year even the SDS, the strongest of all student organizations, was falling into pieces.		Through their political work women came to the conclusion that they were being suppressed by a patriarchal society, and that they had to change this condition. In addition to that the student movement brought up many theories on education and the raising of children which have influenced the modern forms of these processes. These changes and the huge influence on culture and art were probably the most important effects of the student movement.		An indirect effect was the "radical decree" which was passed in the year 1972. It allowed the government to prevent the employment of people in the public services if there were grounds to believe that they did not support the free and democratic principles (freiheitliche demokratische Grundordnung) outlined in the constitution (Grundgesetz). Under the decree, which gradually fell into disuse after 1976, around 3.5 million individuals were investigated and 10,000 refused employment (fewer than 0.3%); 130 were dismissed.[3]		A number of ministers in the Gerhard Schröder government were student activists in the 1960s and early 1970s.		
A lecture (from the French 'lecture', meaning 'reading' [process]) is an oral presentation intended to present information or teach people about a particular subject, for example by a university or college teacher. Lectures are used to convey critical information, history, background, theories, and equations. A politician's speech, a minister's sermon, or even a businessman's sales presentation may be similar in form to a lecture. Usually the lecturer will stand at the front of the room and recite information relevant to the lecture's content.		Though lectures are much criticised as a teaching method, universities have not yet found practical alternative teaching methods for the large majority of their courses.[1] Critics point out that lecturing is mainly a one-way method of communication that does not involve significant audience participation but relies upon passive learning. Therefore, lecturing is often contrasted to active learning. Lectures delivered by talented speakers can be highly stimulating; at the very least, lectures have survived in academia as a quick, cheap, and efficient way of introducing large numbers of students to a particular field of study.		Lectures have a significant role outside the classroom, as well. Academic and scientific awards routinely include a lecture as part of the honor, and academic conferences often center on "keynote addresses", i.e., lectures. The public lecture has a long history in the sciences and in social movements. Union halls, for instance, historically have hosted numerous free and public lectures on a wide variety of matters. Similarly, churches, community centers, libraries, museums, and other organizations have hosted lectures in furtherance of their missions or their constituents' interests. Lectures represent a continuation of oral tradition in contrast to textual communication in books and other media. Lectures may be considered a type of grey literature.[2]						The noun "lecture" dates from 14th century, meaning "action of reading, that which is read," from the Latin lectus, pp. of legere "to read." Its subsequent meaning as "oral discourse on a given subject before an audience for purposes of instruction" is from the 16th century. The verb "to lecture" is attested from 1590. The noun "lectern" refers to the reading desk used by lecturers.		The practice in the medieval university was for the instructor to read from an original source to a class of students who took notes on the lecture. The reading from original sources evolved into the reading of glosses on an original and then more generally to lecture notes. Throughout much of history, the diffusion of knowledge via handwritten lecture notes was an essential element of academic life.		Even in the twentieth century, the lecture notes taken by students, or prepared by a scholar for a lecture, have sometimes achieved wide circulation (see, for example, the genesis of Ferdinand de Saussure's Cours de linguistique générale). Many lecturers were, and still are, accustomed to simply reading their own notes from the lectern for exactly that purpose. Nevertheless, modern lectures generally incorporate additional activities, e.g. writing on a chalk-board, exercises, class questions and discussions, or student presentations.		The use of multimedia presentation software such as Microsoft PowerPoint has changed the form of lectures, e.g. video, graphics, websites, or prepared exercises may be included. Most commonly, however, only outlines composed of "bullet points" are presented. Critics such as Edward Tufte contend that this style of lecture bombards the audience with unnecessary and possibly distracting or confusing graphics.[3]		A modified lecture format, generally presented in 5 to 15 minute short segments, is now commonly presented as video, for example in massive open online courses (MOOCs) or in programs such as the Khan Academy.[4]		Bligh, in What's the Use of Lectures?, argues that lectures "represent a conception of education in which teachers who know give knowledge to students who do not and are therefore supposed to have nothing worth contributing." Based on his review of numerous studies, he concludes that lecturing is as effective, but not more effective, as any other teaching method in transmitting information. Nevertheless, lecturing is not the most effective method for promoting student thought, changing attitudes, or teaching behavioral skills.[5] Bligh summarises research on memory to show the significance of the meaningfulness of material on retention (Marks and Miller 1964) and the importance of immediate rehearsal of information (Bassey 1968). He relates his own research on arousal during lectures to suggest a decrement in attention during the first 25 minutes. Lloyd (1968) and Scerbo et al. (1992) showed that students take less and less notes as lectures proceed. Bligh shows that after a short break filled by buzz group discussion, attention will recover somewhat. The largest section of Bligh's book is devoted to lecturing technique, particularly the organisation of lectures, how to make a point, the effectiveness of taking notes, the use of handouts, and ways of obtaining feedback. Early editions of the book contained a reply paid evaluation card. This research showed that the section on alternative teaching methods within lectures was the most highly praised.[6]		The conception of the lecture as needing to be a didactic event has been challenged by Meltzer and Manivannan (2002) and Sandry (2005) who maintain that lectures can involve active learning.[7] However, Elliot (2005) sees difficulties in the encouragement of active learning with phenomena such as social loafing and evaluation apprehension causing audience members to be reluctant to participate.[7] A possible solution to the encouragement of audience involvement in lectures is the use of an audience response system which allows audience members to participate anonymously.[8]		The effectiveness of traditional lecture is and has been debated. Some advantages of lecturing include: quick exposure to new material, greater teacher control in the classroom, an engaging format, which may complement and clarify course material, and facilitating large-class communication.[1] Lecturing also permits the dissemination of unpublished or not readily available material.[9]		There has been much debate as to whether or not lecturing actually improves student learning in the classroom. Commonly cited disadvantages of lecture include: placing students in a passive (rather than an active) role, encouraging one-way communication, requiring significant out-of-class time for students to engage with the material, and requiring the speaker to possess effective speaking skills.[1]		The criticisms of lectures are often summarized by a quote generally misattributed[10] to Mark Twain:		College is a place where a professor’s lecture notes go straight to the students’ lecture notes, without passing through the brains of either.[10]		While lecturing is generally accepted as an effective form of instruction, there have been some prominent educators who have succeeded without the help of lectures.		Many university courses relying on lectures supplement them with smaller discussion sections, tutorials, or laboratory experiment sessions as a means of further actively involving students. Often these supplemental sections are led by graduate students, tutors, teaching assistants, or teaching fellows rather than senior faculty. Those other forms of academic teaching include discussion (recitation if conducted by a teaching assistant), seminars, workshops, observation, practical application, case examples/case study, experiential learning/active learning, computer-based instruction, and tutorials.		In schools, the prevalent mode of student-teacher interaction is lessons.		The term "parlor lecture" gained currency throughout the British Commonwealth of Nations and the United States of America during the mid-19th century. It referred to the custom of inviting noted speakers to deliver private lectures, which were typically hosted in the parlors of wealthy and socially influential families.[11]		
A law school (also known as a law centre or college of law) is an institution specializing in legal education, usually involved as part of a process for becoming a lawyer within a given jurisdiction.		In Brazil the legal education begins between 1827/28 in Olinda/PE and São Paulo/SP where the first Schools of Law were established by the new Empire using as educational model the Coimbra Faculty of Law.		Nowadays the legal education consists in a 5-year-long course in which, afterwards, the scholar is granted a bachelor´s degree.[1]		Therefore, it is considered part of the higher education, hence the educational system is regulated as: i) basic education - primary, middle anda high school; and ii) higher education: licentiate, bachelor and vocational ed.[2]		The practice of law is conditioned upon admission to the bar of a particular state or other territorial jurisdiction (Ordem dos Advogados do Brasil - OAB [3]).		Public attorneys, public prossecutors and magistrates (judges) admission is made, mainly, through an entrance examination and a constitutional mandatory three years of legal experience. Starting from the second degree courts it is mandatory a 1/5 of its composition to be fulfilled with members of the lawyers/attorneys/barristers association and also from federal/state/labour processcutors (ministério público) regarding the court jurisdiction (it is not applied for electoral and military courts).		After achieving the bachelor´s degree of laws it is possible to follow an i) specialization or follow ii) academically (or both), in either case it is called postgraduation: i) lato sensu; or ii) stricto sensu; respectively.		The postgraduation, stricto sensu, consists in a: a) master´s degree, which is usually a two-year degree; and a b) doctorate´s degree, which can take up another four years.[4]		The oldest civil law faculty in Canada offering law degrees was established in 1848 at McGill University in Montreal, and the oldest common law faculty in Canada offering law degrees was established in 1883 at Dalhousie University in Halifax. The typical law degree required to practice law in Canada is now the Juris Doctor,[5] which requires previous university coursework and is similar to the first law degree in the United States. There is some scholarly content in the coursework (such as an academic research paper required in most schools).[6] The programs consist of three years, and have similar content in their mandatory first year courses. Beyond first year and the minimum requirements for graduation, course selection is elective with various concentrations such as business law, international law, natural resources law, criminal law, Aboriginal law, etc.[7] Some schools, however, have not switched from LL.B. to the J.D. – one notable university that still awards the LL.B is McGill University.		Given that the Canadian legal system includes both the French civil law and the Anglo-American common law, some law schools offer both an LL.B. or J.D. (common law) and a B.C.L., LL.L. or LL.B. (civil law) degree, such as McGill University, University of Ottawa and the Université de Montréal. In particular, McGill University Faculty of Law offers a combined civil law and common law program, which has been called "transsystemic."[8] At other faculties, if a person completes a common law degree, then a civil law degree can be obtained with only an extra year of study. This is also true for civil law graduates who wish to complete a common law degree.		Despite changes in designation, schools opting for the J.D. have not altered their curricula. Neither the J.D. or LL.B. alone is sufficient to qualify for a Canadian license, as each Province's law society requires an apprenticeship and successful completion of provincial skills and responsibilities training course, such as the British Columbia Law Society's Professional Legal Training Course,[9] the Law Society of Upper Canada's Skills and Responsibilities Training Program.[10] and the École du Barreau du Québec.		The main reason for implementing the J.D. in Canada was to distinguish the degree from the European counterpart that requires no previous post-secondary education,[11] However, in the eyes of the Canadian educational system, the J.D. awarded by Canadian universities has retained the characteristics of the LL.B. and is considered a second entry program, but not a graduate program.[12] (This position is analogous to the position taken by Canadian universities that the M.D. and D.D.S. degrees are considered second entry programs and not graduate programs.) Nevertheless, disagreement persists regarding the status of the degrees, such as at the University of Toronto, where the J.D. degree designation has been marketed by the Faculty of Law as superior to the LL.B. degree designation.[13]		Some universities have developed joint Canadian LL.B or J.D. and American J.D programs, such as York University and New York University,[14] the University of Windsor and the University of Detroit Mercy,[15] and the University of Ottawa and Michigan State University program.[16]		In France, the legal education is a three tier system. The student may study for a LLB (licence de droit), then a LLM (master de droit) and, for those interested in Law theory, a PhD in Law (doctorat de droit).		Many French universities offers Law courses in department labelled as Research and Education Units (unité de formation et de recherche) and/or Faculties of Law or Law Schools.		A LLM-level is a prerequisite for some legal professions, but is combined with vocational education, such as the école nationale de la magistrature for judges and the Certificat d'aptitude aux fonctions d'avocat for advocates.		Law Degree in Indonesia consists of three tier systems. The first tier is the Degree of which carries the title of Sarjana Hukum/S.H. (Bachelor of Law). This can be obtained in 4–7 years after they enter Law School straight from Senior High School.		The second tier varies depending on the legal specialties taken after the first tier. The general title for this tier is Magister Hukum / M.H. (Master in Law). Although it is also common to see other title for secondary tier such as Magister Kenotariatan / M.Kn. (Master in Notary) for Notarial professionals line of work. The second tier can be obtained normally in 1-2 year.		The third tier in Indonesian Law Degree is Doctor / DR. (Doctor in Law).		To work in legal professions of choice in Indonesia, a Bachelor Law Degree (S.H.) is obligatory. Graduates can pursue their career as Legal in-house counsel, Judge profession (requires admission and further training at Supreme Court Educational Center), Public Prosecutor (requires admission and further training at Public Prosecutor Educational and Training Center), other legal-related work and Advocate.		To become an Advocate, Law Graduate should attend an Advocate Special Course (1–2 months) and pass the Bar exam. The title Advocate can be obtained after a graduate passes the Bar exam and fulfill several obligation and requirements created by the Indonesian Advocates Association (PERADI), and is a prerequisite for practicing trial law in Indonesia.		List of some School of Law in Indonesia		In India, legal education has been traditionally offered as a three-year graduate degree. However, the structure has been changed since 1987. Law degrees in India are granted and conferred in terms of the Advocates Act, 1961, which is a law passed by the Parliament both on the aspect of legal education and also regulation of conduct of legal profession.[17] Under the act, the Bar Council of India is the supreme regulatory body to regulate the legal profession in India and also to ensure the compliance of the laws and maintenance of professional standards by the legal profession in the country.		To this regard, the Bar Council of India prescribes the minimum curriculum required to be taught in order for an institution to be eligible for the grant of a law degree. The Bar Council also carries on a periodic supervision of the institutions conferring the degree and evaluates their teaching methodology and curriculum and having determined that the institution meets the required standards, recognizes the institution and the degree conferred by it.		Traditionally the degrees that were conferred carried the title of LL.B. (Bachelor of Laws) or B.L. (Bachelor of Law). The eligibility requirement for these degrees was that the applicant already have a Bachelor's degree in any subject from a recognized institution. Thereafter the LL.B. / B.L. course was for three years, upon the successful completion of which the applicant was granted either degree.		However, upon the suggestion by the Law Commission of India and also given the prevailing cry for reform, the Bar Council of India instituted upon an experiment in terms of establishing specialized law universities solely devoted to legal education and thus to raise the academic standards of legal profession in India. This decision was taken somewhere in 1985 and thereafter the first law University in India was set up in Bangalore which was named as the National Law School of India University (popularly 'NLS'). These law universities were meant to offer a multi-disciplinary and integrated approach to legal education. It was therefore for the first time that a law degree other than LL.B. or B.L. was granted in India. NLS offered a five-year law course, upon the successful completion of which an integrated degree with the title of "B.A.,LL.B. (Honours)" would be granted.		Thereafter, other law universities were set up, all offering five-year integrated law degrees with different nomenclature. The next in line was National Law Institute University set up in Bhopal in 1997. It was followed by NALSAR university of law in 1998. The Guru Gobind Singh Indraprastha University in Delhi offered a five-year integrated law degree course of LL.B (Honours) from 1998 and subsequently from 2007 started to award the B.A.,LL.B / B.B.A.LL.B (Honours). The Mysore University School of Justice set up by the University of Mysore in Mysore offered a five-year integrated law degree course of B.A.,LL.B (Honours) from 2007. The course for three years LL.B. is also regularized in University of Delhi as an option for post graduation after the completion of graduation degree. The National Law University, Jodhpur offered for the first time in 2001 the integrated law degree of "B.B.A, LL.B. (Honours)" which was preceded by the West Bengal National University of Juridical Sciences offering the "B.Sc., LL.B. (Honours)" degree. Gujarat National Law University established in Gandhinagar also offers LL.B.		However, despite these specialized law universities, the traditional three-year degree continues to be offered in India by other institutions and are equally recognized as eligible qualifications for practicing law in India. Another essential difference that remains is that while the eligibility qualification for the three year law degree is that the applicant must already be a holder of a bachelor's degree, for being eligible for the five years integrated law degree, the applicant must have successfully completed Class XII from a recognized Boards of Education in India.		Both the holders of the three-year degree and of the five-year integrated degree are eligible for enrollment with the Bar Council of India upon the fulfillment of eligibility conditions and upon enrollment, may appear before any court in India.[18]		In Hong Kong, which generally follows the English common law system, an undergraduate L.L.B. is common, followed by a one or two year Postgraduate Certificate in Laws before one can begin a training contract (solicitors) or a pupillage (barristers).		City University of Hong Kong - http://www.cityu.edu.hk/		The Chinese University of Hong Kong - http://www.cuhk.edu.hk/		The University of Hong Kong - https://www.hku.hk/		In Iran, the legal education has been influenced both by civil law and Islamic Shari'ah law. Like many countries, after high school, one can enter the law school. The first law degree is LL.B. It takes about four years to get LL.B. The first graduate program in law is LL.M. It takes about two to three years to earn an LL.M. The LL.M. is a mix of course work in a specific field of law and a dissertation. The Ph.D. in law is the highest law degree offered by some law schools. It takes about 5–7 years depending on the school as well as the students. The faculty of law in Shahid Beheshti University (formerly known as Iran's National University) and the faculty of law and political sciences at University of Tehran are the top two law schools in Iran.		See Legal education#Japan.		Law degree programs are considered graduate programs in the Philippines. As such, admission to law schools requires the completion of a bachelor's degree, with a sufficient number of credits or units in certain subject areas.[18]		Graduation from a Philippine law school constitutes the primary eligibility requirement for the Philippine Bar Examination, the national licensure examination for practicing lawyers in the country. The bar examination is administered by the Supreme Court of the Philippines during the month of September every year.		As of 2011 the bar examinations were held during November.		The University of Santo Tomas Faculty of Civil Law was the first secular faculty, and hence the oldest law school in the Philippines.		In Singapore, the primary route for obtaining a legal education to qualify as a practicing lawyer is via a 4-year Bachelor of Laws (LL.B.) degree from either the National University of Singapore Faculty of Law, or the Singapore Management University School of Law. A third law school affiliated to the Singapore University of Social Sciences will commence its first intake in 2017, aimed primarily at producing law graduates focused on family, criminal and community justice law, as there is a pressing need for lawyers practicing in these areas.		Additionally, the SMU School of Law offers the 3-year Juris Doctor degree for aspiring candidates who have already completed a prior undergraduate course of study and who have been award a bachelor's degree in another field. The SMU J.D. is recognised for qualification to the Singapore Bar.		There are several private law schools in Singapore that are run by private education providers and which also award the Bachelor of Laws degree. These private law schools are neither recognised nor supported by the government and their graduates are, in the vast majority of cases, ineligible for qualification to the Singapore Bar.		National University of Singapore		Singapore Management University		Singapore University of Social Sciences		Stansfield College		In Serbia, prospective students are required to pass an admission test for enrollment in a law school. The legal education is a three tier system – 4-year bachelor's degree studies, 1-year Master of Law and 5-year doctoral studies. The Belgrade Law School is the most distinguished and largest by capacity in Serbia. Courses are offered in Serbian and English.		On July 3, 2007, the Korean National Assembly passed legislation introducing 'Law School', closely modeled on the American post-graduate system.[19] Moreover, naturally, since March 2, 2009, 25 (both public and private) 3-year professional Law Schools that officially approved by Korean Government, has been opened to teach future Korean lawyers.[20] The first bar test to the lawschool graduates was scheduled in 2012.		In Sri Lanka to practice law, one must be admitted and enrolled as an Attorney-at-Law of the Supreme Court of Sri Lanka. This is achieved by passing law exams at the Sri Lanka Law College, which are administered by the Council of Legal Education and spending a period of six months under a practicing attorney of at least 8 years standing. To undertake law exams students must gain admission to the Sri Lanka Law College and study law or directly undertake exams after gaining a LL.B. from a local or foreign university.[21]		In Taiwan, law is primarily studied as an undergraduate program resulting in a Bachelor of Law (B.L.).[22] Students receive academic rather than practical training.[22] Practical training is arranged after the individual passes the lawyer, judge, or prosecutor exams.		A degree in law (bachelor, master or doctor) is a pre-requisite for Taiwan's bar examination. According to Ministry of Examination figures, the pass rate for the 2011 exam was 10.6%. Since the bar exam is conducted in Chinese, a native level of language fluency is expected.		A non-citizen may take the Attorney Qualification Examination.[23] According to Articles 5 and 20 of the Regulations on Attorney Qualification Examination (Bar Exam), non-citizens are allowed to participate in the bar examination with a degree in law earned in Taiwan. Non-citizens are not allowed to sit in the prosecutor or judge examinations unless they are naturalized citizens of Taiwan. Once a non-citizen is approved to practice law in Taiwan, he or she must abide by all statutes related to the legal practice, Codes of Legal Ethics, and the Articles of Incorporation of the Bar Association to which they are members.[24]		In the United States, law school is a postgraduate program usually lasting three years and resulting in the conferral upon graduates of the Juris Doctor (J.D.) law degree. Some schools in Louisiana concurrently award a Graduate Diploma in Civil Law (D.C.L.). To gain admission to a law school that is accredited by the American Bar Association (ABA), applicants must usually take the Law School Admission Test (LSAT),[25][26]:33 and have an undergraduate (bachelor's) degree in any major.[26]:38 Currently, there are 205 ABA-approved law schools that grant the JD degree.[27] There currently are five online law schools that are unaccredited by the ABA but registered by the State Bar of California. Non-ABA approved law schools have much lower bar passage rates than ABA-approved law schools,[28] and do not submit or disclose employment outcome data to the ABA.		According to a study by labor economists Michael Simkovic and Frank McIntyre, a law degree increases the present value of lifetime earnings in the U.S. by $1,000,000 compared to a bachelor's degree.[29][30][31][32][33] According to the United States Department of Labor, Bureau of Labor Statistics, the national average salary for lawyers in 2012 was above $130,000.[34] Salaries vary by geography, with higher average salaries in big cities—especially New York, Washington, D.C., Chicago, and Los Angeles—and lower salaries in rural areas. An unpublished table produced by the U.S. Bureau of Labor Statistics shows that unemployment rates among experienced lawyers are lower than those for most high-income occupations.[35] BLS data also suggests that lawyer employment has grown slightly faster than other occupations, with lawyers comprising a growing share of the work force over the last decade.		However, not all recent law graduates work as lawyers. According to Simkovic and McIntyre's study of U.S. Census Bureau data, around 40 percent of U.S. residents with law degrees do not practice law.[29] Law graduates are disproportionately represented in leadership positions in business and government.[29] The National Association for Legal Career Professionals produces an annual report summarizing the employment of recent graduates of U.S. law schools at a single point in time, 9 months of graduation. Employment at that point is typically around 90 percent, although from 2009 to 2011, the numbers have been lower, at around 86 to 88 percent.[36] Approximately 2 percent of graduates were employed in non-professional jobs.[36] Approximately 75 to 85 percent work in jobs classified by NALP as "JD required" or "JD preferred", and another 5 percent work in other professional jobs.[36] However, a law degree increases earnings, even including those who do not practice law.[29]		Some schools offer a Master of Laws (LL.M.) program as a way of specializing in a particular area of law. A further possible degree is the academic doctoral degree in law of Doctor of Juridical Science (S.J.D.) (in the U.S or Canada)., or the Doctor of Laws (LL.D.) in Canada or the UK, or the Ph.D. in Law from European or Australasian universities.		In addition to attending law school, many jurisdictions require law school graduates to pass a state or provincial bar examination before they may practice law. The Multistate Bar Examination is part of the bar examination in almost all United States jurisdictions. Generally, the standardized, common law subject matter of the MBE is combined with state-specific essay questions to produce a comprehensive bar examination.		In other common law countries the bar exam is often replaced by a period of work with a law firm known as articles of clerkship.[citation needed]		While law schools in the U.S. and Canada are typically post-graduate institutions with considerable autonomy, legal education in other countries is provided within the mainstream educational system from university level and/or in non-degree conferring vocational training institutions.		In countries such as the United Kingdom and most of continental Europe, academic legal education is provided within the mainstream university system starting at the undergraduate level, and the legal departments of universities are simply departments like any other rather than separate "law schools". In these countries, the term "law school" may be used, but it does not have the same definition as it does in North America. The same is true for private Law Schools, e.g. in Germany two private law schools have been established, Bucerius Law School in Hamburg and EBS Law School in Wiesbaden which are termed law schools but follow the usual German path of legal education.		There are also sometimes legal colleges that provide vocational training as a post-academic stage of legal education. One example is the University of Law in the United Kingdom, which provides certain professional qualifications lawyers in England and Wales must obtain before they may practice as solicitors or barristers.		In Australia, law schools such as the Melbourne Law School, the Adelaide Law School, and the Sydney Law School have emphasised a combination of the British and American systems. However, other universities such as the University of New South Wales, the Australian National University and Monash University are known for their practical work.[37]		
Queensland (abbreviated as QLD, or less frequently Q) is the second-largest and third-most-populous state in the Commonwealth of Australia. Situated in the north-east of the country, it is bordered by the Northern Territory, South Australia and New South Wales to the west, south-west and south respectively. To the east, Queensland is bordered by the Coral Sea and Pacific Ocean. The state is the world's sixth largest sub-national entity, with an area of 1,852,642 km2.		Queensland has a population of 4,750,500, concentrated along the coast and particularly in the state's South East. The capital and largest city in the state is Brisbane, Australia's third largest city. Often referred to as the "Sunshine State", Queensland is home to 10 of Australia's 30 largest cities and is the nation's third largest economy. Tourism in the state, fuelled largely by its warm tropical climate, is a major industry.		Queensland was first inhabited by Aboriginal Australians and Torres Strait Islanders.[5][6] The first European to land in Queensland (and Australia) was Dutch navigator Willem Janszoon in 1606, who explored the west coast of the Cape York Peninsula near present-day Weipa. In 1770, Lieutenant James Cook claimed the east coast of Australia for the Kingdom of Great Britain. The colony of New South Wales was founded in 1788 by Governor Arthur Phillip at Sydney; New South Wales at that time included all of what is now Queensland, Victoria and Tasmania. Queensland was explored in subsequent decades until the establishment of a penal colony at Brisbane in 1824 by John Oxley. Penal transportation ceased in 1839 and free settlement was allowed from 1842.		The state was named in honour of Queen Victoria,[7] who on 6 June 1859 signed Letters Patent separating the colony from New South Wales. The 6th of June is now celebrated annually statewide as Queensland Day. Queensland was one of the six colonies which became the founding states of Australia with federation on 1 January 1901.						The history of Queensland spans thousands of years, encompassing both a lengthy indigenous presence, as well as the eventful times of post-European settlement. The north-eastern Australian region was explored by Dutch, Spanish and French navigators before being encountered by Lieutenant James Cook in 1770. The state has witnessed frontier warfare between European settlers and Indigenous inhabitants (which did not result in any settlement or treaty), as well as the exploitation of cheap Kanaka labour sourced from the South Pacific through a form of forced recruitment known at the time as "blackbirding". The Australian Labor Party has its origin as a formal organisation in Queensland and the town of Barcaldine is the symbolic birthplace of the party.[8] June 2009 marked the 150th anniversary of its creation as a separate colony from New South Wales.[9] A rare record of early settler life in north Queensland can be seen in a set of ten photographic glass plates taken in the 1860s by Richard Daintree, in the collection of the National Museum of Australia.[10]		The Aboriginal occupation of Queensland is thought to predate 50,000 BC, likely via boat or land bridge across Torres Strait, and became divided into over 90 different language groups.		During the last ice age Queensland's landscape became more arid and largely desolate, making food and other supplies scarce. This led to the world's first seed-grinding technology. Warming again made the land hospitable, which brought high rainfall along the eastern coast, stimulating the growth of the state's tropical rainforests.[11]		In February 1606, Dutch navigator Willem Janszoon landed near the site of what is now Weipa, on the western shore of Cape York. This was the first recorded landing of a European in Australia, and it also marked the first reported contact between European and Aboriginal Australian people.[11] The region was also explored by French and Spanish explorers (commanded by Louis Antoine de Bougainville and Luís Vaez de Torres, respectively) prior to the arrival of Lieutenant James Cook in 1770. Cook claimed the east coast under instruction from King George III of the United Kingdom on 22 August 1770 at Possession Island, naming Eastern Australia, including Queensland, 'New South Wales'.[12]		The Aboriginal population declined significantly after a smallpox epidemic during the late 18th century.[13] (There has been controversy regarding the origins of smallpox in Australia; while many sources have claimed that it originated with British settlers, this theory has been contradicted by scientific evidence.[14][15][16] There is circumstantial evidence that Macassan mariners visiting Arnhem Land introduced smallpox to Australia.[15] )		In 1823, John Oxley, a British explorer, sailed north from what is now Sydney to scout possible penal colony sites in Gladstone (then Port Curtis) and Moreton Bay. At Moreton Bay, he found the Brisbane River. He returned in 1824 and established a settlement at what is now Redcliffe. The settlement, initially known as Edenglassie, was then transferred to the current location of the Brisbane city centre. Edmund Lockyer discovered outcrops of coal along the banks of the upper Brisbane River in 1825.[17] In 1839 transportation of convicts was ceased, culminating in the closure of the Brisbane penal settlement. In 1842 free settlement was permitted. In 1847, the Port of Maryborough was opened as a wool port. The first free immigrant ship to arrive in Moreton Bay was the Artemisia, in 1848. In 1857, Queensland's first lighthouse was built at Cape Moreton.		A war, sometimes called a "war of extermination"[citation needed], erupted between Aborigines and settlers in colonial Queensland[citation needed]. The Frontier War was notable for being the most bloody in Australia[citation needed], perhaps due to Queensland's larger pre-contact indigenous population when compared to the other Australian colonies. About 1,500 European settlers and their allies (consisting of Chinese, Aboriginal and Melanesian assistants)[citation needed], were killed in frontier skirmishes during the nineteenth century.[citation needed] Casualties among the Aboriginal people may have exceeded 30,000.[citation needed] The "Native Police Force", employed by the Queensland government, was key in the oppression of the indigenous people.[18]		On 27 October 1857, aborigines of Martha Fraser's Hornet Bank station on the Dawson River killed eleven Europeans. This was the largest massacre of Australian colonists by aborigines.[19][20][21][22] One author[23] estimates 24,000 Aboriginal men, women and children died at the hands of the Native Police in colonial Queensland between 1859 and 1897 alone.		A public meeting was held in 1851 to consider the proposed separation of Queensland from New South Wales. On 6 June 1859, Queen Victoria signed Letters Patent to form the separate colony of what is now Queensland. Brisbane was appointed as the capital city. On 10 December 1859, a proclamation was read by British author George Bowen, whereby Queensland was formally separated from the state of New South Wales.[24] As a result, Bowen became the first Governor of Queensland. On 22 May 1860 the first Queensland election was held and Robert Herbert, Bowen's private secretary, was appointed as the first Premier of Queensland. Queensland also became the first Australian colony to establish its own parliament rather than spending time as a Crown Colony. In 1865, the first rail line in the state opened between Ipswich and Grandchester.		Queensland's economy expanded rapidly in 1867 after James Nash discovered gold on the Mary River near the town of Gympie, sparking a gold rush. While still significant, they were on a much smaller scale than the gold rushes of Victoria and New South Wales. During the period from the 1860s till the early 20th century, many labourers, known at the time as Kanakas, were brought to Queensland from neighbouring Pacific Island nations to work in the state's sugar cane fields. Some of these people had been kidnapped under a process known as blackbirding or press ganging, and their employment conditions amounted to indentured labour or even slavery. During the Australian federation of 1901, the White Australia policy came into effect, which saw all foreign workers in Australia deported under the Pacific Island Labourers Act of 1901, which saw the Pacific Islander population of the state decrease rapidly.[25]		On 1 January 1901, Australia was federated following a proclamation by Queen Victoria. During this time, Queensland had a population of half a million people. Brisbane was subsequently proclaimed a city in 1902. In 1905, women voted in state elections for the first time, and the University of Queensland was established in 1909. In 1911, The first alternative treatments for polio were pioneered in Queensland and remain in use across the world today.		World War I had a major impact on Queensland. Over 58,000 Queenslanders fought in World War I and over 10,000 of them died.[26]		Australia's first major airline, Qantas, was founded in 1920 to serve outback Queensland.		In 1922, Queensland abolished the Upper House, becoming the only State with a unicameral State Parliament in Australia.		In 1935, cane toads were deliberately introduced to Queensland from Hawaii in a poorly-thought-out and unsuccessful attempt to reduce the number of French's cane and greyback cane beetles that were destroying the roots of sugar cane plants, which are integral to Queensland's economy. In 1962, the first commercial production of oil in Queensland and Australia began at Moonie.		The humid climate—regulated by the availability of air conditioning—saw Queensland become a more accommodating place to work and live for Australian migrants.[27] To this day, it is one of Australia's economic powerhouses and the third most populous state in the country.		In 2009, Queensland celebrated Q150, its 150th anniversary as an independent colony and state.[28] The Queensland government and other Queensland organisations commemorated the occasion with many events and publications, including the announcement of the top 150 icons of Queensland by the Queensland Premier Anna Bligh,[29][30] and the creation of monuments at significant survey points in Queensland's history to honour the many early explorer/surveyors who mapped the state.[31][32]		Queensland borders the Torres Strait to the north, with Boigu Island off the coast of New Guinea representing the absolute northern extreme of its territory. The triangular Cape York Peninsula, which points toward New Guinea, is the northernmost part of the state's mainland. West of the peninsula's tip, northern Queensland is bordered by the Gulf of Carpentaria, while the Coral Sea, an arm of the Pacific Ocean, borders Queensland to the east. To the west, Queensland is bordered by the Northern Territory, at the 138°E longitude, and to the southwest by the northeastern corner of South Australia.		In the south, there are three sections that constitute its border: the watershed from Point Danger to the Dumaresq River; the river section involving the Dumaresq, the Macintyre and the Barwon; and 29°S latitude (including some minor historical encroachments below the 29th parallel) over to the South Australian border.		The state capital is Brisbane, located on the coast 100 kilometres (60 mi) by road north of the New South Wales border. The state is divided into several officially recognised regions. Other smaller geographical regions of note include the Atherton Tablelands, the Granite Belt, and the Channel Country in the far southwest.		Queensland has many areas of natural beauty, including the Sunshine Coast and the Gold Coast, home to some of the state's most popular beaches; the Bunya Mountains and the Great Dividing Range, with numerous lookouts, waterfalls and picnic areas; Carnarvon Gorge; Whitsunday Islands; and Hinchinbrook Island. The state contains six World Heritage-listed preservation areas: Australian Fossil Mammal Sites at Riversleigh in the Gulf Country, Gondwana Rainforests of Australia, Fraser Island, Great Barrier Reef, Lamington National Park and the Wet Tropics of Queensland.		Mangrove swampland in Cape Tribulation		Noosa Main Beach		Mossman River during the wet season		Glasshouse Mountains		Because of its size, there is significant variation in climate across the state. Low rainfall and hot humid summers are typical for the inland and west, a monsoonal "wet" season in the far north, and warm, temperate conditions along the coastal strip. Elevated areas in the south-east inland can experience temperatures well below freezing in mid-winter. The climate of the coastal strip is influenced by warm ocean waters, keeping the region free from extremes of temperature and providing moisture for rainfall.[33]		Natural disasters are often a threat in Queensland; severe tropical cyclones can impact the coast and cause severe damage,[34] with recent examples including Larry, Yasi, Ita and Debbie. Flooding from rain-bearing systems can also be severe and can occur anywhere in Queensland. One of the deadliest and most damaging floods in the history of the state occurred in early 2011.[35] Droughts and bushfires can also occur; however, the latter are generally less severe than those that occur in southern states. Severe springtime thunderstorms generally affect the south-east and inland of the state and can bring damaging winds, torrential rain, large hail and even tornadoes.[36] The strongest tornado ever recorded in Australia occurred in Queensland near Bundaberg.[37]		There are five predominant climatic zones in Queensland,[38] based on temperature and humidity:		However, most of the Queensland populace experience two weather seasons: a "winter" period of mild to warm temperatures and minimal rainfall, and a sultry summer period of hot, sticky temperatures and higher levels of rainfall.		The coastal far north of the state is the wettest place in Australia, with Mount Bellenden Ker, south of Cairns, holding many Australian rainfall records with its annual average rainfall of over 8 metres.[39] It is not uncommon for locations in this area to receive more rain in 24 hours during the wet season than the majority of Queensland receives in a year. Snow is rare in Queensland, although it does fall with some regularity along the far southern border with New South Wales, predominantly in the Stanthorpe district although on rare occasions further north and west. The most northerly snow ever recorded in Australia occurred near Mackay; however, this was exceptional.[40]		The annual mean statistics[41] for some Queensland centres are shown below:		The highest official maximum temperature recorded in the state was 49.5 °C (121.1 °F) at Birdsville Police Station on 24 December 1972,[46] although the Moderate-Resolution Imaging Spectroradiometer (MODIS) on NASA's Aqua satellite measured a ground surface temperature of 69.3 °C (156.7 °F). This temperature was the hottest value worldwide measured by MODIS in 2003.[47] Queensland has the highest average maximums of any Australian state, and Toowoomba, Stanthorpe, Hervey Bay, Mackay, Atherton, Weipa and Thursday Island are the only large population centres not to have recorded a temperature above 40 °C (104 °F).		The lowest minimum temperature is −10.6 °C (12.9 °F) at Stanthorpe on 23 June 1961 and at The Hermitage (near Warwick) on 12 July 1965.[48] Temperatures below 0 °C (32 °F) are, however, generally uncommon over the majority of populated Queensland.		Queensland is less centralised than most other Australian states, with 50% of the population living outside the state capital, and 25% living outside the South East Queensland urban agglomeration. Queensland is home to many regional cities, the most populous being the Gold Coast, the Sunshine Coast, Townsville, Cairns, Toowoomba, Mackay, Rockhampton and Bundaberg. For decades, Queensland has consistently been the fastest growing state in Australia, while Western Australia has grown faster in the 2010s.[52] At its peak growth in 2007, it was estimated that over 1,500 people moved to the state per week including 1,000 to the southern part of the state alone.[53]		The 2016 census showed that the majority of Queenslanders are Christians (2.64 million; 56.0%), including 1.02 million Roman Catholics (21.7%), 720,000 Anglicans (15.3%), and 240,000 Uniting Church Protestants (5.1%). There are also 1.40 million people who follow no religion (29.7%), and 470,000 who did not state their religion (9.9%). The largest distinct religious minorities consist of those who follow Buddhism (70,000; 1.5%), Hinduism (46,000; 1.0%), and Islam (45,000; 1.0%).[54] The percentage of Christians in Queensland, and particularly non-Catholic Christians, is therefore greater than the Australian average, while representation of non-Christian minority religions is lower than the national average.		In the 1880s and 1890s, sea ports were established on the coast, adjacent to the mouth of the Fitzroy River. Broadmount was on the northern side and Port Alma on the south. Railways were subsequently constructed to carry goods to the wharves at these locations, the railway to Broadmount opening on 1 January 1898 and the line to Port Alma opened on 16 October 1911. Maintenance on the Broadmount line ceased in August 1929. The following month, the wharf caught fire and the line was effectively closed in July 1930. The line to Port Alma closed on 15 October 1986.[55] Queensland's economy has enjoyed a boom in the tourism and mining industries over the past 20 years. A sizeable influx of interstate and overseas migrants, large amounts of federal government investment, increased mining of vast mineral deposits and an expanding aerospace sector have contributed to the state's economic growth. The 2008–09 saw the expansion slow to just 0.8%, the state's worst performance in 18 years.[56]		Between 1992 and 2002, the growth in the gross state product of Queensland outperformed that of all the other states and territories. In that period Queensland's GSP grew 5.0% each year, while growth in Australia's gross domestic product (GDP) rose on average 3.9% each year. Queensland's contribution to the Australian GDP increased by 10.4% in that period, one of only three states to do so.[57]		In 2003 Brisbane had the lowest cost of living of all Australia's capital cities. In late 2005 Brisbane was the third most expensive capital for housing after Sydney and Canberra and just ahead of Melbourne by $15,000.		Primary industries include: bananas, pineapples, peanuts, a wide variety of other tropical and temperate fruit and vegetables, grain crops, wineries, cattle raising, cotton, sugar cane, wool and a mining industry including bauxite, coal, silver, lead, zinc, gold, and copper. Secondary industries are mostly further processing of the above-mentioned primary produce. For example, bauxite is shipped by sea from Weipa and converted to alumina at Gladstone.[58] There is also copper refining and the refining of sugar cane to sugar at a number of mills along the eastern coastline. Major tertiary industries are the retail trade and tourism.		Interests in Crown land in Queensland are primarily regulated by the Land Act 1994.[59][60]		Tourism is Queensland's leading tertiary industry with millions of interstate and overseas visitors flocking to the Sunshine State each year. The industry generates $4.0 billion annually, accounting for 4.5% of Queensland's GSP.[61] Queensland is a state of many landscapes which range from sunny tropical coastal areas, lush rainforests to dry inland areas and temperate highland ranges.		The main tourist destinations of Queensland[62][63][64] include, Brisbane, Cairns, Port Douglas and the Daintree Rainforest, Gold Coast, the Great Barrier Reef, Hervey Bay and nearby Fraser Island, Townsville, Magnetic Island, North Stradbroke Island and South Stradbroke Island, Sunshine Coast, Hamilton Island, Daydream Island and the Whitsundays known for Airlie Beach and Whitehaven Beach.		Cairns is renowned as the "Gateway to the Barrier Reef" and the heritage listed Daintree Rainforests. The Gold Coast of Queensland is also sometimes referred to as "Australia's Theme Park Capital", with five major amusement parks. These are Dreamworld, Movie World, Sea World, Wet 'n' Wild and WhiteWater World.		There are numerous wildlife parks in Queensland. On the Gold Coast there is Currumbin Wildlife Sanctuary at Currumbin and David Fleay Wildlife Park at Burleigh Heads. On the Sunshine Coast there is UnderWater World at Mooloolaba and Australia Zoo near Beerwah/Glass House Mountains, home of Steve Irwin until his death in 2006.		Lone Pine Koala Sanctuary at Fig Tree Pocket and Brisbane Forest Park at The Gap are located in Brisbane. North of Brisbane is Alma Park Zoo which is relocating to Logan City and Kumbartcho Wildlife Sanctuary which was originally called Bunya Park Wildlife Sanctuary.		Accommodation in Queensland caters for nearly 22% of the total expenditure, followed by restaurants/meals (15%), airfares (11%), fuel (11%) and shopping/gifts (11%).[65]		Queensland is served by a number of National Highways and, particularly in South East Queensland, motorways such as the M1. The Department of Transport & Main Roads oversees the development and operation of main roads and public transport, including taxis and local aviation.		Principal rail services are provided by Queensland Rail and Pacific National, predominantly between the major towns along the coastal strip east of the Great Dividing Range.		Major seaports include the Port of Brisbane and subsidiary ports at Gladstone, Townsville and Bundaberg. There are large coal export facilities at Hay Point / Dalrymple Bay, Gladstone and Abbot Point. Sugar is another major export, with facilities at Lucinda and Mackay.		Brisbane Airport is the main international and domestic gateway serving the state. Gold Coast Airport, Cairns International Airport and Townsville Airport are the next most prominent airports, all with scheduled international flights. Other regional airports, with scheduled domestic flights, include Brisbane West Wellcamp Airport, Great Barrier Reef Airport, Hervey Bay Airport, Mackay Airport, Mount Isa Airport, Proserpine / Whitsunday Coast Airport, Rockhampton Airport, and Sunshine Coast Airport.		South East Queensland has an integrated public transport system operated by TransLink, which provides services bus, rail, light rail and ferry services through contracted bus, ferry and light rail operators and Queensland Rail. The TransLink network operates a fare system which allows a single ticket to be used across all modes for the same price irrespective of the number of transfers made on the trip. Regional bus and long-distance rail services are also provided throughout the State. Local bus services are also available in most regional centres.		Executive authority is nominally vested in the Governor, who represents - and is formally appointed (on the advice of the Premier) by - Elizabeth II, Queen of Australia. The current governor is His Excellency, The Hon. Paul de Jersey, AC. The Head of Government - the Premier - fulfils in reality the day-to-day functions of the state's executive, and is assisted in this by the Cabinet. He or she is appointed by the Governor but must have the support of the Legislative Assembly. The Premier is in practice a leading member of the Assembly and parliamentary leader of his or her political party, or coalition of parties. The current Premier is Annastacia Palaszczuk of the Labor Party. Other ministers, forming the Executive Council (which includes members of the Cabinet), are appointed by the Governor from among the notable members of the Legislative Assembly on the Premier's recommendation. They are in practice members of the Premier's party, or allied with it. A Speaker is elected by the Assembly to facilitate proceedings and communicate between the Assembly and the Governor, usually on matters relating to prorogation or dissolution of the Assembly.		The Queensland Parliament or the Legislative Assembly, is unicameral. It is the only Australian state with a unicameral legislature. A bicameral system existed until 1922, when the Legislative Council was abolished by the Labor members' "suicide squad", so called because they were appointed for the purpose of voting to abolish their own offices.[66] The Parliament is housed in the 19th century Parliament House and 20th century Parliamentary Annexe in Brisbane. The state's politics are traditionally regarded as being conservative relative to other states.[67][68][69][70][71]		There are several factors that differentiate Queensland's government from other Australian states. The legislature has no upper house. For a large portion of its history, the state was under a gerrymander that heavily favoured rural electorates. This, combined with the already decentralised nature of Queensland, meant that politics has been dominated by regional interests. Queensland, along with New South Wales, formerly operated a balloting system known as Optional Preferential Voting for state elections. This is different from the predominant Australian electoral system, the instant-runoff voting system, and in practice is closer to a first past the post ballot (similar to the ballot used in the UK), which some say is to the detriment of minor parties. The next Queensland election will use instant-runoff voting.		These conditions have had notable practical ramifications for politics in Queensland. The lack of an upper house for substantial legislative review has meant that Queensland has had a tradition of domination by strong-willed, populist premiers, often with arguably authoritarian tendencies, holding office for long periods.		The judicial system of Queensland consists of the Supreme Court and the District Court, established by the Constitution of Queensland, and various other courts and tribunals established by ordinary Acts of the Queensland Parliament.		In 2001 Queensland adopted a new codified constitution, repealing most of the assorted Acts of Parliament that had previously made up the constitution. The new constitution took effect on 6 June 2002, the anniversary of the formation of the colony of Queensland by the signing of Letters patent by Queen Victoria in 1859.		Local government is the mechanism by which towns and districts can manage their own affairs to the extent permitted by the Local Government Act 1993–2007. Queensland is divided into 73 local government areas which may be called Cities, Towns, Shires or Regions.[72]		Each area has a council which is responsible for providing a range of public services and utilities, and derives its income from both rates and charges on resident ratepayers, and grants and subsidies from the State and Commonwealth governments.[73]		The state's first university, The University of Queensland, was established in 1909. It was moved to St Lucia in 1945, where it remains today. The University of Queensland ranks amongst the top 60 universities in all major global rankings.		James Cook University was set up in 1970 to become the first tertiary education institution in North Queensland. Griffith University was established in the Brisbane suburb of Nathan in 1971. In 1989, the Queensland University of Technology was opened (previously the Queensland Institute of Technology) in the Brisbane central business district at Gardens Point. These Universities all have more than one campus, and all are recognised as leading Australian universities.		Bond University was established in 1989 as a private not-for-profit university, the first of its type in Queensland and is located at Robina on the Gold Coast. In 1992, the Central Queensland University and University of Southern Queensland gained university status from previously operating and Institutes of Technologies, and the new University of the Sunshine Coast was established in 1994.		In 1997 the National Aboriginal Centre for the Performing Arts (ACPA) was established and in 2010 Southern Cross University opened a new campus at the southern part of the Gold Coast. The Australian Catholic University also operates a campus in Brisbane.		The state of Queensland is represented in all of Australia's national sporting competitions and is also host to a number of domestic and international sporting events. The most popular winter and summer team sports are Rugby league, Rugby union and cricket, respectively. Rugby league's annual State of Origin series is a major event in the Queensland sporting calendar, with the Queensland Maroons in 2013 winning a record eighth series in a row.		The Brisbane Broncos are the state's most successful team of any sport, having won 3 premierships in the NRL rugby league era and 6 in total during their 23-year existence. The Brisbane Broncos participated in the 2015 NRL Grand Final losing to the North Queensland Cowboys 17–16 in extra time, claiming their first premiership in its history. It is considered one of the greatest Grand Finals in NRL history. The other two NRL teams in Queensland are the North Queensland Cowboys and Gold Coast Titans.		Queensland's dominance is not restricted to rugby league. The early part of this century saw the AFL's Brisbane Lions claim a hat-trick of premierships between 2001 and 2003 inclusive, whilst in soccer, Brisbane Roar FC won back to back A-League titles in the 2010/11 and 2011/12 season, and also set an Australian sporting record of 36 consecutive games unbeaten. Just four years after being branded "the joke of rugby union", the Queensland Reds won its first Super Rugby title in July 2011. In netball the Queensland Firebirds went undefeated in the 2011 season as they went on to win the Grand Final. Other sports teams are the Brisbane Bullets and the Cairns Taipans, who compete in the National Basketball League.		Swimming is also a popular sport in Queensland, with a majority of Australian team members and international medalists hailing from the state. At the 2008 Summer Olympics, Queensland swimmers won all six of Australia's gold medals, all swimmers on Australia's three female (finals) relays teams were from Queensland, two of which won gold.		Events include:		
Authentic assessment is the measurement of "intellectual accomplishments that are worthwhile, significant, and meaningful,"[1] as contrasted to multiple choice standardized tests.[2] Authentic assessment can be devised by the teacher, or in collaboration with the student by engaging student voice. When applying authentic assessment to student learning and achievement, a teacher applies criteria related to “construction of knowledge, disciplined inquiry, and the value of achievement beyond the school.” [3]		Authentic assessment tends to focus on contextualised tasks, enabling students to demonstrate their competency in a more 'authentic' setting. Examples of authentic assessment categories include:		According to Ormiston, "Authentic learning mirrors the tasks and problem solving that are required in the reality outside of school".[4]		This framework for assessment begins the same way curriculum design begins, with the question: What should students be able to do?[5] Once the instructor answers that question, they can then devise a rubric to evaluate how well a student demonstrates the ability to complete the task. Because most authentic assessments require a judgement of the degree of quality, they tend toward the subjective end of the assessment scale. Rubrics are an "attempt to make subjective measurements as objective, clear, consistent, and as defensible as possible by explicitly defining the criteria on which performance or achievement should be judged."[6]						Traditionally, assessment follows curriculum. Authentic assessment is an example of "backwards design" because the curriculum follows from the assessment.[7]		One case study was presented by Edutopia's Schools That Work series on New York based institution, School of the Future. This school stresses the process of authentically assessing students rather than focusing solely on test results or term papers.[8] The school measures the full range of student ability through formative assessments, presentations, exhibitions, and tests that focus on authentic tasks to assess students' skills and knowledge as they relate to real-world endeavors and skills such as effective group communication and presentation. 98% of students at this school go on to college after graduating.[9]		Teachers from The School of The Future in New York utilize authentic assessment in their school and recommend that other teachers can do the same by following the guidelines outlined below:		[10]		A goal of authentic assessment is to determine if student knowledge can be applied outside of the classroom. This means that a physics assessment should involve doing physics by performing experiments and solving problems the same way that a real-life physicist would. An authentic history assessment requires students to ask questions, do independent research, and formulate answers to their questions, just like a real-life historian does.[10] What else is authentic assessment meant to do?		
Student financial aid in the United States is funding that is available exclusively to students attending a post-secondary educational institution in the United States. This funding is to assist in covering the many costs incurred in the pursuit of post-secondary education. Financial aid is available from federal, state, educational institutions, and private agencies (foundations), and can be awarded in the forms of grants, education loans, work-study and scholarships. Please note that in order to apply for any federal financial aid students must first complete the Free Application for Federal Student Aid (FAFSA).						In the United States, grants come from a wide range of government departments, colleges, universities or public and private trusts. Grant eligibility is typically determined by financial need and academic merit. The application process is set by the agency providing the funds and often relies on data submitted via the FAFSA.		While the terms grant and scholarship are frequently used interchangeably, there is a difference. Scholarships may have a financial need component but rely on other criteria as well. Some private need-based awards are confusingly called scholarships, and require the results of a FAFSA (the family's EFC). However, scholarships are often merit-based, while grants tend to be need-based.[1]		Some examples of grants commonly applied for in the U.S.:		An education loan is taken out by the student (or parent) in order to pay for educational expenses. Unlike scholarships and grants, this money must be repaid with interest. Educational loan options include federal student loans, federal parent loans, private loans, and consolidation loans.		Federal student loans are loans directly to the student; the student is responsible for repayment of the loan. These loans typically have low interest rates and do not require a credit check or any other sort of collateral. Student loans provide a wide variety of deferment plans, as well as extended repayment terms, making it easier for students to select payment methods that reflect their financial situation. There are federal loan programs that consider financial need. For more information on federal student loans please visit: https://studentaid.ed.gov/sa/types/loans.		Direct subsidized loans are the most sought, as they have few requirements other than enrollment and demonstration of financial need. However, the amount you may borrow is determined by your school and may not exceed your financial need, which is based on the EFC from your FAFSA. You are not required to begin repaying these loans for as long as you are in school at least half-time. They also offer a six-month grace period, meaning you do not begin repaying them until six months after you leave school. These loans also offer a deferment period in some cases.[5]		Direct Unsubsidized Loans are available to all undergraduate and graduate students, with no requirement to demonstrate financial need. Your school will determine how much you are allowed to borrow based on your cost of attendance and adjust for any other financial aid you are receiving. However, you are responsible for paying the interest on these loans even during school. If you choose not to pay interest while enrolled, your interest will accrue and be added to the principal amount of your loan.[6]		Federal parent loans are a federally funded loan option if the student is dependent on his or her parents. Parent loans allow parents to take out student loans, the repayment of which will be their responsibility. The parents use these loans to pay for educational expenses on behalf of the student. For undergraduate students there is the parent loan for undergraduate students or PLUS Loan. This loan allows parents to borrow up to the total cost of attendance, minus any other financial aid the student receives. Eligibility will be determined upon review of the parent's credit history.[7]		Private student loans are offered by private lenders (financial institutions). These loans typically have much higher interest rates, have fewer repayment/deferment options, and are not supervised by any agency.[8]		Consolidation Loans combine two or more student and/or parent loans into one loan. They are an option for those who find themselves struggling with multiple student loan payments. Consolidation loans are available for most federal loan types, and some private lenders offer private consolidation loans for private education loans.[9]		The Federal Work-Study Program is a form of financial aid that can be used not only as a means of maintain a stable bank account, but to also earn money toward paying off tuition. Work study jobs allow students to get jobs within their field or given interest, and are more flexible than off-campus part-time jobs because they are designed to accommodate student schedules.		While the terms grant and scholarship are frequently used interchangeably, there is a difference. Scholarships may have a financial need component but rely on other criteria as well. Some private need-based awards are confusingly called scholarships, and require the results of a FAFSA (the family's EFC). However, scholarships are often merit-based, while grants tend to be need-based.[10]		Scholarships, similar to grants, do not need to be repaid. Scholarships come from state, educational institutions, and private agencies. Scholarships can be awarded based on merit, financial need, student characteristics (such as gender, race, religion, family and medical history, and the like), creativity, career field, college, athletic ability, among other categories.		There are search engines available to find scholarships such as Peterson’s, Unigo, Fastweb, Cappex, Chegg, The College Board, Niche (formerly known as College Prowler), Scholarships.com, Collegenet.com, and Scholarship Monkey.[11]		To qualify for need-based aid a student must have a significant amount of financial need, which is determined by the federal government based on the FAFSA. Using the information submitted on the FAFSA, the U.S. Department of Education calculates a figure called the Expected Family Contribution (EFC). If the EFC is less than the cost of attending a college, the student has financial need (as the term is used in the U.S. financial aid system).		Some well-to-do colleges have need-based aid of their own to distribute, in addition to federal and state aid (if any). These colleges require, in addition to the FAFSA, the CSS Profile financial form, which goes into greater detail.[12]		Need-based financial aid is awarded on the basis of the financial need of the student. The Free Application for Federal Student Aid application (FAFSA) is generally used for determining federal, state, and institutional need-based aid eligibility. At private institutions, a supplemental application may be necessary for institutional need-based aid.		A recent trend shows that what is purely need-based aid is not entirely clear. According to the National Postsecondary Aid Survey (NPSAS), SAT scores affect the size of institutional need-based financial aid.[13] If a student has a high SAT score and a low family income, they will receive larger institutional need-based grants than a student with a low family income that has low SAT scores. In 1996, public higher education institutions gave students with high SAT scores and a low family income $1,255 in need-based grants. However, only $565 in need-based grants were given to students with low SAT scores who had low family incomes. The lower a student’s SAT score, the smaller the amount of need-based grants a student received no matter what their family income level was. The same trend holds true for higher education private institutions. In 1996, private institutions gave students with high SAT scores and a low family income $7,123 versus $2,382 for students with low SAT scores and a low family income. Thus, “institutional need-based awards are less sensitive to need and more sensitive to ‘academic merit’ than the principles of needs analysis would lead us to expect.” [14] It has been found that increasing an SAT score in the range of 100-200 points can result in hundreds of dollars more in institutional grants and on average substantially more if one is attending a private institution.[15]		While providing financial information to the government is a reasonable expectation to calculate a student’s financial need, it does not necessarily follow that colleges should have access to this information. Providing that information to schools may be problematic because schools learn about students’ other sources of funding and may adjust their financial aid packages accordingly. There is an asymmetric information problem since schools have full knowledge of their customers' ability to pay while students and their families have little information about costs that colleges face to provide their services. That is, when planning for the next academic year, a school will know its current and projected costs as well as each student’s ability to pay after receiving state and federal grants. According to the Center for College Affordability and Productivity (CCAP), “If the federal or state authorities increase financial support per student, the institution has the opportunity to capture part or all of that increased ability to pay by reducing institutional grants and/or raising their charges for tuition, fees, room, or board.” Importantly, it also notes that “the exception to this general pattern is modest aid targeted at only low-income students, like the Pell grant.” The center uses data about net proceeds (tuition plus room, board and other fees) as a percentage of median income to show that financial aid practices have not been effective in decreasing prices in an effort to increase access. Net proceeds at public four-year institutions rose from 15% to 20% of median income from 1987 to 2008. In that same time, productivity has declined in the form of lighter teaching loads for professors and increased expenditures on administrative staff.[16]		Non-need based loans are available for students and families who cannot afford to pay the entire cost of college. These loans are directed toward those individuals and families who did not qualify for need-based loans due to the amount of their personal assets. There is usually a higher interest rate associated with non-need based loans. Because these loans are not need-based, the U.S. government does not pay the interest for the student while enrolled in school; they are often referred to as unsubsidized loans. The Unsubsidized Stafford Loan and Grad PLUS loan are non-need based loans available for both undergraduate and graduate students who do not qualify for need-based financial aid. [17]		Even though these loans are not subsidized, interest rates are set by Congress, the programs are closely supervised, and they provide many protections that private loans rarely offer.		There are also non-need based grants and scholarships that consider merit rather than financial need. These awards are granted by the college or university as well as outside organizations. Merit-based scholarships are typically awarded for outstanding academic achievements and maximum SAT or ACT scores. However, some scholarships may be awarded due to special talents like athletic scholarships, leadership potential, and other personal characteristics. In order to be considered for such awards some institutions require an additional application process while others automatically consider all admitted students for their merit-based scholarships.		With the yearly rising cost of tuition, room and board, and fees among schools across the nation, low-income students are finding it harder to pay for their education. In an attempt to help students meet the high, costly demands of college, schools have increased merit-based grants, for students with outstanding academic position, involvement in organizations, or high athletic talent. The issue is that these reasons for awarding scholarships take away from low-income students who often do not meet these merit standards. In other words, funds for merit-based scholarships are taking away from the already small amount of federal aid available to low-income students who simply cannot pay for college without some kind of financial aid.		In recent years, government has responded to the financial crisis students are facing and therefore passed legislation that boosted the value of grants for low-income students and trimmed subsidies for private education lenders.[18] Schools have also taken action for the sake of students. Harvard University, a well-known costly but wealthy institution that had previously cut tuition for students whose families earned less than $60,000 a year, proceeded to cut costs by nearly fifty percent for those students whose families earned between $120,000 and $180,000 a year.[18] Institutions will consider students' financial needs as well as their academic merit standing when applying for financial aid. Merit-based aid and need-based aid have been linked together for many financial aid scholarships. This relationship is beneficial as it underlies that one form of financial aid, particularly merit-based, is not completely taking over need-based aid. Statistics do show results of studies performed from 1992-2000 that the increase in financial aid awarded was based entirely on merit.[19] However, when viewing numbers of both merit-based and need-based aid closely, the differences are not significant.		The following types of federal financial aid are available to graduate and professional students. Aid for these students is primarily loans.		Graduate students may also be eligible for these financial aid programs:		There is little financial aid is available for foreign students, with the unique exception of Canadian and Mexican students. A majority of aid is awarded as grants, scholarships, and loans that come through public and private sources which restrict their awards to American citizens. That being said there is financial aid still available for international students.		There are colleges and universities that offer aid to international students. To find out if the school in question offers such assistance inquire of the financial aid office of the institution. Some schools offer grants, loans and jobs, and give anywhere from 15 to 150 awards to foreign students. For example, schools such as Harvard, Princeton, University of Pennsylvania, University of Miami, Ithaca College, Cornell University, Johns Hopkins, University of Chicago, and University of Oregon all offer packages to foreign students. Graduate students may have more luck with financial aid. This is because graduate and teaching assistantships are offered on the basis of academic achievement, regardless of citizenship.[22] Although International students are not eligible for the US government aid programs like the Pell Grant, SEOG Grant, Stafford Loan, Perkins Loan, PLUS Loan, and Federal Work study, many schools will ask international students to submit a FAFSA so that they may use the data for assessing financial need.[23]		There is also assistance a student can seek from their native country. Canadian students attending colleges in the USA may obtain loans through the Canadian government’s Ministry of Skills, Training, and Labour. Alternative loans Canadian international students may apply for are the Canadian Higher Education Loan Program,[24] Global Student Loan Corporation (GLSC),[25] and International Student Loan Program (ISLP).[26][27] Financial Aid for European Students can be looked by using Noopolis, a database in Italy run by CNR (the Italian equivalent of the US’s National Science Foundation). It has information regarding financial aid for Italian citizens to study abroad. There are also U.S. Educational Advising Centers throughout the world that assist prospective students by answering the questions they have about studying in the United States.[22]		Post-secondary institutions post a Cost of Attendance or Price of Attendance, also known as a "sticker price." However, that price is not how much an institution will cost an individual student. To make higher education costs more transparent before a student actually applies to college, federal law requires all post-secondary institutions receiving Title IV funds (federal funds for student aid) to post net price calculators on their websites by October 29, 2011.		As defined in The Higher Education Opportunity Act of 2008, the net price calculator’s purpose is:		“…to help current and prospective students, families, and other consumers estimate the individual net price of an institution of higher education for a student. The net price calculator shall be developed in a manner that enables current and prospective students, families, and consumers to determine an estimate of a current or prospective student’s individual net price at a particular institution.”		The law defines estimated net price as the difference between an institution’s average total Price of Attendance (the sum of tuition and fees, room and board, books and supplies, and other expenses including personal expenses and transportation for a first-time, full-time undergraduate students who receive aid) and the institution’s median need- and merit-based grant aid awarded.[28]		Elise Miller, program director for the U.S. Department of Education's Integrated Postsecondary Education Data System (IPEDS) stated the idea behind the requirement: "We just want to break down the myth of sticker price and get beyond it. This is to give students some indication that they will not necessarily be paying that full price."[29]		The template was developed based on the suggestions of an IPEDS’ Technical Review Panel (TRP), which met on January 27–28, 2009, and included 58 individuals representing federal and state governments, post-secondary institutions from all sectors, association representatives, and template contractors. Mary Sapp, Ph.D., assistant vice president for planning and institutional research at the University of Miami, served as the panel’s chair. She described the mandate’s goal as “to provide prospective and current undergraduate students with some insight into the difference between an institution’s sticker price and the price they will end up paying.”[30]		To meet the requirement, post-secondary institutions may choose between a basic template developed by the U.S. Department of Education or an alternative net price calculator that offers at least the minimum elements the law requires.[31] A recent report issued by the Institute for College Access and Success, "“Adding it all up 2012: are net price calculators easy to find, use and compare?”, found key issues with the implementation of the net price calculator requirement.[32] In “Adding it all up,” the authors state, “this report takes a more in-depth look at the net price calculators from 50 randomly selected colleges. While we found some positive practices that were not evident at the time of our previous report, net price calculators are still not reliably easy for prospective college students and their families to find, use, and compare”.[32]		After the requirement came into effect, the free website CollegeAbacus.org began creating a system that would allow students to enter the personal information once, and then use and compare net-prices of multiple schools.[33] The Gates Foundation's College Knowledge Challenge announced College Abacus as one its winners in January 2013; the $100,000 grant from the Gates Foundation will enable College Abacus to expand from its beta version with 2500+ schools to a fully comprehensive version with all the colleges and universities in the United States.[34]		In 2001, Princeton University became the first university in the United States to eliminate loans from its financial aid packages. Since then, many other schools have followed in eliminating some or all loans from their financial aid programs. Many of these programs are aimed at students whose parents earn less than a certain income — the figures vary by college or university. These new initiatives were designed to attract more students and applicants from lower socioeconomic backgrounds, reduce student debt loads, and provide the offering institutions with an advantage over their rivals in attracting commitments from accepted students. Most students prefer no-loan financial aid as a way to relieve the amount of debt they are in after college		The following colleges and universities offer such no-loan financial aid packages as of March 2008:		Some universities have opted to have a "loan cap" program, which is a maximum loan — either per year or for the four years combined — designed to reduce the cost of attendance for low-income and middle-class students. The following schools have a loan cap program:		In a study on the correlation between the price of higher education and enrollment rates, Donald Heller finds that the amount of financial aid available for students is a strong factor in enrollment rates.[66]		Different factors have different effects on financial aid:		Need-blind admissions do not consider a student’s financial need. In a time when colleges are low on financial funds, it is difficult to maintain need-blind admissions because schools cannot meet the full need of the poor students that they admit.[67]		There are different levels of need-blind admissions. Few institutions are fully need-blind. Others are not need-blind for students who apply after certain deadlines, international students, and students from a waitlist.[67] Some institutions are moving away from need-blind admissions so that they can fulfill the full need of the students that are admitted.[67] Meeting the full-need will probably increase the funds for financial aid.[67] For example, Wesleyan University is only need-blind if it has enough money to satisfy the full need of admitted students.[67]		Most national governments provide student financial aid to students attending a university, even and especially in countries where education is free. Several of these countries even provide financial aid to students in secondary schools, especially but not only if they don't live with their parents. See student financial aid for a list of articles on such countries.		Proposed changes to government-financed student financial aid have engendered considerable debate in many countries such as Canada, the United Kingdom, Germany, the Netherlands, and Scandinavian countries. The heavy reliance on private subsidies seen in the United States is not common, although this may be changing.		In Germany, the main source of financial aid is provided by the Bundesausbildungsförderungsgesetz, colloquially known as BAFöG.		
The 2010 United Kingdom student protests were a series of demonstrations in November and December 2010 that took place in several areas of the country, with the focal point of protests being in central London. Largely student-led, the protests were held in opposition to planned spending cuts to further education and an increase of the cap on tuition fees by the Conservative-Liberal Democrat coalition government following their review into higher education funding in England. Student groups said that the intended cuts to education were excessive, would damage higher education, give students higher debts, and broke campaign promises made by politicians.		The first major demonstration occurred on 10 November, jointly organised by the National Union of Students (NUS) and the University and College Union (UCU). It involved between 30,000 and 50,000 demonstrators marching through central London, with several hundred branching off to attack and occupy the Conservative Party headquarters. This measure brought condemnation from the establishment and a divide within the student movement over the appropriateness of such tactics. The National Campaign Against Fees and Cuts (NCAFC) called for a mass walk-out and demonstration on 24 November, with occupations taking place at campuses throughout the UK. A march in central London was kettled in Whitehall, resulting in violent confrontation with protesters. Further demonstrations were held in central London on 30 November, when police clashed with protesters and kettled them in Trafalgar Square, while other protests took place throughout the country. Another central London protest took place on 9 December, the day that the proposed reforms were passed into law, with protesters clashing with police and being kettled in Parliament Square.		The student protests were unsuccessful in their aim of preventing the government's reforms. The demonstrations had been highly controversial in the UK, being condemned for instances of violence and vandalism by the establishment. The behaviour of the Metropolitan Police in dealing with the protests were also widely criticised for instances of untruthfulness and excessive use of force.						In November 2009, the centrist Labour Party government of Prime Minister Gordon Brown commissioned a study into higher education funding in England. Chaired by Lord Browne of Madingley, the former chief executive of BP, the report was to be titled the Browne Review.[1] In the build up to the United Kingdom General Election in May 2010, the leader of the Liberal Democrats, Nick Clegg, pledged that he would vote against any proposed increase in tuition fees if elected to Parliament.[2]		Following the election and resulting hung parliament, Clegg made an agreement with the Conservative Party to form a coalition government in which Conservative leader David Cameron became Prime Minister and Clegg became Deputy Prime Minister. The Browne Review was subsequently published in October 2010, and contained the suggestion that the government should remove outright the existing cap of £3,290 on tuition fees. The government rejected this proposal, instead choosing to keep a cap but increasing it to £9,000.[3]		David Willetts, the Minister of State for Universities and Science, stated that the measures were "a very progressive package" and "at the end of this we will have a better university system than we have at the moment."[4] Contravening his pre-election pledge, Clegg expressed support for the rise in the cap on tuition fees, which would result in future students paying higher fees for their university education.[5]		Student union leaders were critical of the cuts. The National Union of Students (NUS) feared that the increased cap on tuition fees would prevent potential students from poorer backgrounds from attending university.[6] Many protesters focused in particular on Clegg's campaign promise that he would oppose any rise in tuition fees. David Barclay, the president of the University of Oxford's student union, said: "This is the day a generation of politicians learn that though they might forget their promises, students won't."[4] Rahul Mansigani, the students' union president for the University of Cambridge, said: "Large numbers of students voted for the Liberal Democrats, and there is no question that the pledge is a binding commitment."[7]		Two weeks before, on 28 October, a protest was held in the University of Oxford to coincide with a visit from the Liberal Democrat minister and Business Secretary Vince Cable. Cable cancelled his visit after taking advice from the police about the protest.[8] Several days later, on 3 November, there was a student protest in Dublin. The subsequent London protest was described by one Irish reporter as "scenes bizarrely similar" to those in the Irish capital.[9]		The initial event was the largest student protest in Britain since the Labour government first proposed the Teaching and Higher Education Act in 1998.[10]		The first major demonstration in protest at the government's proposed reforms was held on 10 November 2010 in central London, jointly organised by the National Union of Students (NUS) and the University and College Union (UCU). This demonstration was officially known as "Fund Our Future: Stop Education Cuts", although also termed "Demo 2010" or "Demo-lition 10.11.10".[11] Arriving from all regions of Great Britain and Northern Ireland,[4][12] approximately 30,000 to 52,000 protesters attended the demonstration.[4][13][14] The demonstration's route was pre-approved with the Metropolitan Police Service, with marchers moving from Whitehall past Downing Street, the home of the Prime Minister, and then past the Houses of Parliament, chanting such slogans as "no ifs, no buts – no education cuts", "they say cut back – we say fight back", "I say Tories – you say scum".[12][13][14][15] Journalist Harry Mount of The Daily Telegraph, said: "Perhaps because their cause was justified, the students I saw had none of the swaggering, self-righteous manner of the student protester of legend."[16]		Political groups that sent contingents to take part included the Labour Party, Plaid Cymru, the Green Party, Socialist Workers Party, Socialist Party, Revolution, Young Communist League, Revolutionary Communist Group, and Communist Students.[17][18][19] A few members of parliament (MPs) joined the demonstration, among them Labour MP John McDonnell, who told reporters: "This is the biggest workers' and students' demonstration in decades. It just shows what can be done when people get angry. We must build on this".[17] Representatives of the National Pensioners Convention also took part, with the group's banner carrier, Janet Shapiro, stating that: "We're here because we believe education should be free, funded by the taxpayer. It is something that benefits the community, the country."[20]		At the end of the march, a rally took place outside Tate Britain where demonstrators were addressed by the UCU general secretary Sally Hunt, who introduced a series of clips displayed on a giant plasma screen featuring Clegg giving a series of promises to the electorate on tuition fees, all of which he had subsequently broken.[20] Hunt stated that making the public university system in Britain "the most expensive in the world" wasn't fair, that discouraging young people from going to college was not progressive, and that the increase in tuition fees represented further debts for students.[11] The rally was also addressed by NUS President Aaron Porter and the Trades Union Congress Deputy general secretary Frances O'Grady.[11] O'Grady offered the message to the government that: "Don't you dare tell us we're all in this together. The deficit certainly wasn't caused by the students."[20] The protest was scheduled to end at 2 pm, but slightly overran.[21] The Metropolitan Police were expecting 20,000 demonstrators to turn out, well below the 50,000 figure most widely quoted in the press after the event, and did not expect any violence, so deployed only 225 officers to police the event.[14]		In the afternoon, as protesters passed the Houses of Parliament and moved towards Tate Britain for the rally, several thousand surrounded 30 Millbank in Westminster, campaign headquarters of the Conservative Party, despite attempts by NUS organisers to stop them.[21][22] Forcing their way past the limited police presence,[23] approximately 200 people broke in and occupied the building, whilst a thousand more cheered and supported them from outside.[14][24] These protesters lit placards on fire, and smashed windows before occupying and vandalising the reception area. Staff working in the building were evacuated by police around 1 pm.[14] Around 100 protesters proceeded to the roof of the building, chanting slogans including "Greece! France! Now here too".[12] Initial press sources blamed this action on a group of anarchists.[13][16] This was later disputed, with a reporter from The Guardian noting that most of the protesters occupying the building were students who had been radicalised in opposition to the cuts, and were not all political anarchists.[24]		Riot police from the Territorial Support Group arrived an hour after the building was occupied to remove the protesters.[23] In retaliation, the police were pelted with eggs, rotten fruit, banners, and shards of glass.[25] One of the protesters on the building's roof, Edward Woolard, threw a fire extinguisher onto the police below. He received instant criticism from some of the protesting crowds, who called on those on the roof to "stop throwing shit".[14][nb 1] To control the situation, police used the controversial technique of kettling to trap the protesters within Millbank Square, the forecourt to 30 Millbank, whilst protesters on the outside of the building were pushed back.[21] The police began letting demonstrators out of the building from 6:30 pm, arresting those whom they believed were responsible for vandalism.[14]		The demonstration led to a disruption in London's transport, with journalist Harry Mount stating: "I have never seen London traffic so jammed in 39 years living in the city."[16] Alongside the occupation of Millbank, a smaller number of protesters had travelled to the headquarters of the Liberal Democrats in Cowley Street, where a car window was smashed.[4] In all, 14 people were injured and required hospitalisation, at least three of whom were police officers, whilst police arrested 35 of the demonstrators, sending them off to various police stations around the city.[4][14] Later accounts that the numbers arrested had risen to 54, (33 men and 21 women), ten of whom were aged under 18 and the majority of whom were students.[28]		NUS President Aaron Porter condemned the occupation of 30 Millbank, claiming that it was caused by "those who are here to cause trouble" and that he was "disgusted that the actions of a minority of idiots are trying to undermine 50,000 who came to make a peaceful protest."[13] General Secretary of UCU Sally Hunt also condemned the occupation, declaring that "the overwhelming majority of staff and students on the march came here to send a clear and peaceful message to the politicians.... The actions of a minority, out of 50,000 people, is regrettable."[12] Oxford University's Student Union (OUSU) President David Barclay issued a statement declaring that "OUSU supports the rights of students to protest non-violently. It is hugely unfortunate that some people yesterday were injured and that arrests were made."[29]		Contrastingly, other student leaders, trade unionists, and academics expressed support for elements of the occupation of 30 Millbank. Amongst others, the president of University of London Union Clare Solomon, the Education Officer of the London School of Economics, Ashok Kumar, the Education and Campaigns Officer at University College London, Michael Chessum, the National Union of Students' black students' officer Kanjay Sesay, the NUS' LGBT students' officers Vicki Baars and Alan Bailey, the President of the RMT trade union Alex Gordon and the playwright Lee Hall all signed a statement declaring that:		We reject any attempt to characterise the Millbank protest as small, "extremist" or unrepresentative of our movement. We celebrate the fact that thousands of students were willing to send a message to the Tories that we will fight to win. Occupations are a long established tradition in the student movement that should be defended. It is this kind of action in France and Greece that has been an inspiration to many workers and students in Britain faced with such a huge assault on jobs, benefits, housing and the public sector. We stand with the protesters, and anyone who is victimised as a result of the protest.[30]		Solomon informed the BBC that she had "no problem with direct actions or occupation", and when questioned regarding the damage done to Millbank, responded that "these were a few windows of the Tory Party headquarters – what they're doing to our education is absolutely millions... and they want to complain about a few windows."[6] Some socialist and student commentators criticised Porter and the NUS for their response to the situation, accusing them of careerism.[31] Meanwhile, various university Conservative societies around London condemned the protests, and criticised students' unions for providing the "false impression that the majority of students are left-wing" and opposed to the governments' proposed reforms.[32]		The Metropolitan Police Service admitted that they were unprepared to deal with the occupation of 30 Millbank, something which they had not been expecting. Sir Paul Stephenson, the Metropolitan Police's Commissioner, told the press that he was "embarrassed" by how police had lost control of the situation, and condemned what he saw as "thuggish, loutish behaviour by criminals."[23] Stephenson remarked that "the one thing I would say is that it must have been an awful time for the people trying to go about their daily business in those buildings. I feel terribly sorry that they have had to go through what must have been quite a traumatic experience… We are determined to make sure that sort of thing does not happen again on our streets. I'm clear on that, the Met is clear on that."[23] His views were echoed by Mayor of London Boris Johnson, who stated: "This is intolerable and all those involved will be pursued and they will face the full force of the law… The Metropolitan Police commissioner has assured me that there will be a vigorous post-incident investigation. He will also be reviewing police planning and response."[23]		Prime Minister David Cameron condemned the occupation of Millbank, stating that he would not abandon his position on the issue of education cuts. Speaking from the 2010 G-20 Seoul summit in South Korea, Cameron said the occasion had been "extremely serious" and praised the "bravery" of the police officers. He also stated that the actions of the protesters were "unacceptable" and that "I was worried for the safety of the people in the building because I know people who work there".[6] The following morning, Clegg went on ITV's Daybreak to state: "I should have been more careful perhaps in signing that pledge [to not increase tuition fees]... At the time [prior to his election] I really thought we could do it. I just didn't know, of course, before we came into government, quite what the state of the finances were."[33]		A reporter from The Daily Telegraph commented that the "anarchic behaviour" of those occupying Millbank was "counter-productive" to the students' cause, and that it was the photographs of "a few hundred vicious hotheads" that would "linger" in the public imagination rather than that of the main march.[34] The Financial Times reported that an anonymous vice-chancellor from a London university had told them that the violence would undermine the campaign, and that it "could not have gone better for the government. George Osborne will be delighted."[21]		In the following days, smaller protests were held in Manchester and Cambridge, with no violent confrontations.[35] On 11 November, student protesters occupied a building at the University of Manchester for three hours, demanding to see the accounts that discussed how government funding cuts would affect students. Between 60 and 100 students held a peaceful sit-in at Manchester's John Owens Building on Oxford Road after an NUS meeting earlier that day.[35] Representing this group, protester Jeremy Buck said: "This is just what a few students who had the energy left after the London demo managed to achieve… Imagine what will happen when they have enough time to organise properly for the 24th. It is a matter of watch this space."[30] That same day, protesters at the University of Cambridge held a demonstration against the cuts at the university's annual science, engineering and technology careers fair.[30] On 23 November, anti-education cuts protesters had assembled outside of the offices of The Guardian newspaper, where Clegg was giving his Hugo Young lecture. They brought out an effigy of Clegg, sentencing it to death and executing it by hanging, shouting the slogan "Nick Clegg, shame on you, shame on you for turning blue".[36]		A second significant demonstration took place in London on 24 November, which again led to clashes with police, this time outside Whitehall, after police kettled a large crowd.[37][full citation needed][38]		An organisation known as the National Campaign Against Fees and Cuts (NCAFC) organised a mass national walk-out of education and protest for 24 November.[39] As a part of this, demonstrations were held in London and other locations across the United Kingdom.[37][38][40] According to a group on the social networking website Facebook, 25,000 people had signed up to take the day off from studies and protest prior to the actual event,[39] and these protesters included not only university students, but also school children who had walked out of lessons to join the demonstration.[38] The NCAFC encouraged protesters to use social media to invite others to join them.[39] One of those protesting was Jessica Linley, a law student at the University of Nottingham who had been crowned Miss England in September 2010 and who used her status to gain media coverage for the cause, telling press that she would not be able to afford to go to university if the tuition fees were increased, and that "these sweeping austerity measures are unacceptable."[39]		Several thousand protesters, most of whom were students or school children, gathered in Trafalgar Square, central London, late in the morning. They proceeded toward Whitehall shouting the slogan, "fuck David Cameron".[39] Believing that there had been a lack of officers at 10 November demonstration, the Metropolitan Police drafted in 1000 police officers from across Greater London to oversee the event, almost five times the number at 10 November.[41] Police informed press that they planned to monitor fringe elements within the demonstration whom they believed planned to encourage "vandalism and violence".[39]		At Whitehall, just before 1 pm, police prevented the protesters from reaching Parliament Square and the Houses of Parliament, setting up a line of riot police to kettle the protesters. A reporter The Guardian described the crowd at this point as being "predominantly good natured, although very noisy". Nonetheless, the demonstrators tried to push through the police line, leading to clashes. An unoccupied police van which had been left in the midst of the crowd was vandalised; protesters rocked it back and forth, climbed on its roof, smashed its windows, wrote graffiti on it and threw a smoke bomb inside.[38][39] A group of school girls encircled the van, urging people not to vandalise it. One of them later told reporters that "the cause that we're here for today isn't about 'I hate the police, I want to burn the police and I want to destroy everything they represent.'"[42]		At around 6 pm, mounted police charged at the north end of the crowd to push them back; despite video evidence police denied that this was an actual charge, describing it as crowd control using horses.[43][44] Roughly 1000 protesters broke free of the police kettle, running throughout central London while pursued by police; The Guardian's Paul Lewis stated that "police were caught in a game of cat and mouse, along Charing Cross, Covent Garden and Picadilly Circus."[39] Some of these protesters committed acts of vandalism along the side streets, including knocking over rubbish bins and throwing traffic cones into the road.[39] Various bus shelters and ticket machines in the area were vandalised, with Transport for London diverting buses away from the unrest.[38][41]		Approximately 200 protesters were unable to escape and remained kettled in Whitehall. Police falsely informed press that the crowd were provided with toilet facilities and water; protesters disputed these claims on social media, with ULU President Clare Solomon commenting on Facebook: "we're still illegally kettled in the freezing cold on Whitehall. No food, water or toilets despite what the police are telling the media. Thousands of young people needing to go home."[39] To keep warm, protesters set fire to a ticket machine within the Square, prompting riot police to secure the machine and put out the flames.[39] From 9 to 10 pm, police permitted the rest of the Whitehall protesters to leave, approximately nine to ten hours after they had first been contained, some being searched as they left.[39][44] During the clashes, police arrested 41 protesters.[41] A number of individuals were also injured, including seven police officers and 11 others.[41]		Across the UK, protesters occupied university buildings in at least 12 universities.[38] On 23 November, protesters occupied the picture gallery corridor of Royal Holloway, University of London. They were later joined by supportive members of university staff who took part in what was labelled a "teach-in".[39] On 24 November, the Language Centre at London South Bank University was occupied for 51 hours by over 100 students as part of the "Defend LSBU! Defend Our Education!" campaign.[45] That same day, students occupied the Jeremy Bentham Room in University College London, stating that they were protesting against "savage cuts to higher education and government attempts to force society to pay for a crisis it didn't cause."[38] A BBC reporter visited the occupation, remarking that the protesters "seem as distant from the old left as they do from the new right" and that "you get a reminder that these are students born in the 1990s. They're quoting Harry Potter rather than Che Guevara."[46] Protesters also occupied the University of East London, demanding that university managers "put pressure on the government on the issue of H[higher] E[ducation] cuts and tuition fee rises".[39]		On 24 November, students and supporters went into occupation of Appleton Tower at the University of Edinburgh, stating "We stand firm alongside all other students, university staff and others nationwide affected by education cuts and the risk this poses to the future of higher education."[47] At Manchester's University Place, 3000 students assembled to demonstrate, but several hundred broke away to march towards the town hall. A group of about 100 occupied a lecture theatre in the Roscoe Building. At the University of Oxford university sixth form students initially occupied the Radcliffe Camera.[48] At the University of Cambridge, 200 students scaled the fence of Senate House and marched onto the grounds of King's College, then on 26 November students started an 11-day occupation of part of Old Schools, the main administrative block of the University.[49]		At the University of Bristol, 2000 protesters clashed with police when they tried to move into the city centre. Four were arrested. In Brighton, 3000 demonstrators marched through the city, with nearly 50 occupying a university building.[50] Hundreds of students from Kingston University and various local schools staged an impromptu march through the town and a sit down protest at College Roundabout, leading to one arrest. In the morning of 24 November, demonstrators at the University of Birmingham occupied the Aston Webb building, the site of the Prime Ministerial debates earlier in the year; they issued a statement in which they declared that "we believe the government's cuts to be economically unnecessary, unfair and ideologically motivated" and that "if [the government] continue to destroy the livelihoods of the majority to benefit the rich and powerful minority, they will face increasingly widespread and radical action."[39] In Leeds, protesters amassed at the University of Leeds. Hundreds of them had walked out of the local Allerton Grange High School to join the demonstration, and later occupied the Michael Sadler lecture theatre and a room at Leeds Metropolitan University.[39] A room at the University of Plymouth was also occupied.[51] At Cardiff University, around 200 protesters occupied a lecture theatre after failing to gain entry to the vice-chancellor's building.[39]		During the Whitehall incident, the Metropolitan Police publicly defended their use of kettling, with Chief Inspector Jane Connors claiming that they had only used it as "a last resort". She stated: "it's a valid tactic. Police officers came under attack and we needed to make sure the violence didn't spread out across the London streets."[39] The police came under increasing criticism however, particularly as there were large numbers of children and young teenagers in the crowd, who were held for nearly ten hours in near-zero temperatures.[39][44] Green Party MP Caroline Lucas brought up the topic in the House of Commons that afternoon, stating: "there are many hundreds of students and school children who have been kettled for over four hours and are going to be out there for another several hours, according to the police, in the freezing cold… whatever one thinks of the student protest, [holding people against their will in the contained crowd was] neither proportionate, nor, indeed, effective."[39]		At a meeting of the Metropolitan Police Authority the following day, the Metropolitan Police's Commissioner Sir Paul Stephenson defended the tactics. He condemned the protest, stating: "we have not seen this sort of behaviour for some considerable time… it was thuggery, it was disgraceful, [and] we are determined to find [those responsible for vandalism]." He further warned that "the likelihood is for more disorder on our streets. We must be prepared for it."[41] He was criticised by Jenny Jones, a Green Party member of the London Assembly, who told him: "when you imprison thousands of people, which is essentially what you did yesterday, you do have a duty of care to them... You kept people for nine-and-a-half hours. You punished innocent people for going on a protest."[44]		A spokesperson for Prime Minister Cameron stated that "people have a right to engage in lawful and peaceful protest, but there is no place for violence or intimidation",[38] while education minister David Willetts responded by claiming that protesting students did not understand the government's plans.[38] Clegg stated on BBC Radio 2 that "I hate in politics, as in life, to make promises that you then find you can't keep. We made a promise we can't deliver – we didn't win the election outright and there are compromises in coalition."[38] Labour Party leader Ed Miliband stated that he would not rule out joining further demonstrations, remarking that "I was quite tempted to go out and talk to them [the protesters]. Peaceful demonstrations are part of our society. As Labour leader I am willing to talk to people who are part of them."[44]		A third central London protest was organised for Tuesday 30 November, a day that saw cold temperatures and snow in the city. Protesters assembled at Trafalgar Square, but a police line prevented their march down Whitehall towards the Houses of Parliament; the NCAFC accused the Police of "pre-emptively block[ing]" the protest route.[52] Fearing that they would be kettled in the Square, protesters dispersed throughout the city center, pursued by police.[53] Other officers stood photographing and filming the students for later identification, while police vans blocked off certain streets.[54] Some protesters chanted "Peaceful protest! Peaceful protest!" and "no ifs, no buts, no education cuts", and others blew vuvuzelas and played reggae from portable stereo systems.[54] A BBC reporter talked to members of the demonstration, noting that while many desired a peaceful outcome, others believed that violent confrontation with the police was inevitable. Many were angry with how they had been portrayed in the media, with one commenting that confrontation was not caused by "hardcore anarchists" but by "students who are very, very angry."[54]		Many protesters ran onto Pall Mall and then past St. James's Park. Denied access to Parliament Square by police, they turned around and headed toward Westminster Abbey.[54] A reporter from The Guardian noted that "the march is fracturing – people are going up different streets and getting lost. Texts come through from the front, giving information." The reporter noted that police pursued protesters, and that "it feels like 'kiss chase' – or, when I see a policeman punch a boy out of the way, entirely without provocation, 'punch chase'."[55] Many protesters returned to Trafalgar Square, where they were still unable to march down to Whitehall due to the heavy police line.[53]		The police kettled those in the Square, whilst some of the protesters waved banners with slogans such as "Don't put the kettle on, Mr. Cameron" and "I Can't Believe It's Not Thatcher".[55] The police "put lines [of officers] across all the exits" to the kettled area of the Square, but reportedly allowed small groups of protesters to leave, even though the majority, around 150 to 200, decided to stay and continue protesting in the snow.[56] Some protesters burned placards and one spray-painted the word "Revolution" on Nelson's Column. Others threw plastic bottles and fireworks at police lines, and at one point there "was a scuffle as a knot of policemen rushed one of the protesters, grabbing him to arrest him, and the crowd flocked angrily to the area."[56] At another time, a group of riot police moved into the crowd of protesters to attempt to secure Nelson's Column, only to be surrounded by demonstrators shouting "Who's kettling who? We're kettling you!"[54] Other slogans shouted at police during the protest included "Shame on you!" and "Your job's next".[54] Police then arrested 146 demonstrators who refused to leave the Square; 139 of them were arrested for breach of the peace, whilst seven were arrested on suspicion of violent disorder.[57] Seven more had been arrested in central London earlier in the day.[57] As one reporter noted, "above us, on the steps of the National Gallery, tourists look confused at this vision of Britain 2010, angry and fighting in the snow."[55]		The night before, on 28 November, a crowd of two to three hundred protesters gathered outside of Lewisham Town Hall in Catford, south London, where a council meeting was then in progress, to protest against wider public sector cuts. Several Youtube videos of the incident were shown on national news, including the BBC. Many of the protesters had come from nearby sixth forms (due to potential Education Maintenance Allowance cuts) and also from Goldsmiths College. Demonstrators, playing music and political slogans from boomboxes forced their way into the building, where a smoke bomb was let off, while another protester climbed onto the roof and unfurled a banner. Several more flares were set off outside and windows smashed, with riot police from the Territorial Support Group close by, were called and several arrests were made.[58][59][60] One of the protesters, Sue Luxton, a former Green Party councillor who had subsequently become a teacher, told the press that "I wanted to peacefully express my anger at the cuts... People were angry that they couldn't get in."[60] Jeremy Burton, the Lewisham Borough Commander, later told press that "unfortunately due to the actions of a minority of people present a number of my officers were injured whilst carrying out their police duties", with 16 officers being treated for minor injuries.[58]		On the day of the main demonstration, there were also further protests across the United Kingdom, including in Cardiff, Cambridge, Colchester, Newcastle, Bath, Leeds, Sheffield, Edinburgh, Liverpool, Belfast, Brighton, York, Manchester, Plymouth, Scunthorpe and Bristol.[56] About 1,500 students, including school children, took part in the protest in Brighton,[61] whilst protests in Bristol involved police being pelted with mustard and ten demonstrators were arrested.[57] In Sheffield, police were pelted with snowballs as they guarded the constituency office of Nick Clegg from a crowd of two hundred protesters.[62] The British protests coincided with those in Italy, where demonstrations occurred in Milan, Turin, Naples, Venice, Palermo, Bari, Genoa, and Rome where riot police were called in to prevent students from gaining access to the parliament building.[63]		Meanwhile, whilst occupations that had begun the previous week continued at University College London, Newcastle University and the University of Cambridge,[64] a new one began at the University of Nottingham, where 150 protesters occupied a building.[52] University buildings and local government buildings were occupied in Birmingham and Oxford while police blocked an attempt at occupation of the council building in York.[65] The protesters occupying the council chamber in Birmingham left after four hours, with a police spokesperson commending the protesters for their "wholly peaceful" behaviour, and noting that it "couldn't have been more different from the violent clashes seen recently in London".[62][66]		On Thursday 9 December, the day of the scheduled vote on education reform in the Houses of Parliament, two separate protests were organised in central London; one led by the National Union of Students (NUS), the other jointly by the University of London Union (ULU) and the NCAFC, with an expected 40,000 people attending.[67] ULU members handed out green hard hats with the words "Tax the banks, not the students" on them, whilst a rally was held in Bloomsbury at midday, where ULU President Solomon addressed the crowd.[67] The Metropolitan Police had positioned lines of riot police and police vans along the Houses of Parliament, to prevent any protesters gaining access to it. Superintendent Julia Pendry issued a statement saying: "Protesters will be allowed sight and sound of parliament. However, there is evidence to suggest a number of people will come to London intent on causing violence and disorder. They are jumping on the bandwagon of these demonstrations with no intention to protest or interest in student tuition fees… those who are intent on committing crime will also be dealt with and they will suffer the consequences of their actions."[67]		The protesters marched from Bloomsbury to Parliament Square in the afternoon, where they pushed down metal barriers and occupied the Square.[67] Around 3:30 pm, police kettled those several thousand protesters in the square, preventing them from leaving, stating that it was necessary "due to the level of violence that our officers are facing."[67] Mounted police charged into the crowd on one side of the Square in an attempt to disperse them. A field hospital was set up on the green providing emergency first aid to protesters as well as tea and food within the containment area. Around 30 protesters were treated, most for head injuries.[68] Police used batons to hit protesters, and a St John Ambulance member told press that he had treated ten protesters for head injuries from being struck by police batons by 4.30 pm.[67] One protester, philosophy student Alfie Meadows, suffered a blow to the head from a police truncheon that knocked him unconscious. Taken to hospital, it was discovered that he was suffering from bleeding of the brain, and required brain surgery.[69][nb 2] Reporter Jonathan Haynes of The Guardian, who was present, characterised the police tactics as "very heavy handed".[67]		Police informed press that they were allowing young and vulnerable protesters to leave the kettle, but those inside the kettle, including journalists, asserted that this was untrue.[67] At 5:41 pm, news reached the protesters that the government had voted to support the proposals. Clashes ensued between the crowd and the police, and the protesters pulled along metal fencing to separate themselves from riot police, who were trying to push them all into the centre of the Square.[67] Later on in the evening, with the protesters still kettled in the Square, protesters smashed all of the windows on the ground floor of Her Majesty's Treasury.[72] At 21:15, the protest was forced onto Westminster Bridge where it was kettled until approximately 23:30.[citation needed]		Due to the Parliament Square protest being kettled, many other demonstrators could not enter the Square and so disseminated across much of the rest of central London. Some were separately kettled around The Cenotaph, where Charlie Gilmour, the adopted son of Pink Floyd guitarist David Gilmour and a student at Girton College, Cambridge, was pictured swinging from a Union Flag on the memorial; he later apologised, claiming that he "did not realise" it was the Cenotaph.[73] Gilmour was also photographed attempting to start a fire at the Supreme Court; and tossing a lump of concrete while wearing latex gloves.[74]		Meanwhile, many of those students who remained around the area of Trafalgar Square continued to protest, with about 150 holding a sit-in in the adjacent National Gallery, while others attempted to set fire to the Trafalgar Square Christmas tree.[75]		During the protests, a car taking The Prince of Wales and The Duchess of Cornwall to the evening's Royal Variety Performance at the London Palladium was attacked on Regent Street[76][77] and Home Secretary Theresa May confirmed a protester 'made contact' with Camilla.[78] In Oxford Street, Topshop was damaged, as rioters sprayed "pay your tax" on the building and broke windows.[79]		Home Secretary Theresa May issued a statement in which she "utterly condemned" the actions of the protesters, and declared that "What we are seeing in London tonight, the wanton vandalism, smashing of windows, has nothing to do with peaceful protest... Attacks on police officers and property show that some of the protesters have no respect for London or its citizens."[75]		Nearly 50 people complained to the IPCC about police behaviour during the various protests held around the country, with the majority directed against the Metropolitan Police, including complaints of violence used against protesters.[80]		Following the protests, video footage of Jody McIntyre, who has cerebral palsy, being pulled out of his wheelchair by police, who claimed they were acting in the interest of his own safety (he had positioned himself to the front-line facing the police) was posted to YouTube. The footage showed him being pulled out of his wheelchair and dragged across the ground by officers from the Metropolitan Police during the protests.[81] McIntyre said he had been pulled out of his wheelchair twice; only one incident was shown in the video footage. An interview with McIntyre about the incident by BBC journalist Ben Brown on 13 December 2010 was described by The Guardian newspaper as "having a distinct lack of sympathy from the BBC" and that the incident had "attracted thousands of complaints."[82] Mcintyre's complaint was later rejected by Scotland Yard.[83] His subsequent appeal was partially upheld by the IPCC.[84]		Street medics treated student protesters during the parliament square protest on Thursday 9 December, the day of the scheduled vote to raise university tuition fees. A field tent was set up on the green providing emergency first aid to protesters as well as tea and food within the containment area. Around 30 protesters were treated, most for head injuries.[68]		On 30 November, following the third main day of protesting, the Welsh Assembly announced that it would not permit an increase in fees for Welsh students. A reporter from the BBC noted that this meant that if the plans went through in England, "it would mean that an English student at a university in England could pay more than £17,000 more for a three-year degree than a Welsh student on the same course." .[56]		A writer in British newspaper The Guardian, writing several hours before the government vote on the topic, noted that "It seems likely the tuition fees bill will pass but I'd still argue that – whatever your view on the merits of the new fees system – the protests have been a success at least in calling politicians to account for broken pledges, something you see rarely theses [sic] days."[67]		In July 2011, three school children will challenge the kettling of children at the 24 November 2010 protest. They will seek a Judicial Review in the High Court, arguing in particular that children had a right to protest and that their safety was jeopardised, breaking the laws of the European Convention on Human Rights, the United Nations Convention on the Rights of the Child and the Children Act 2004,[85]		In November 2012, two years after the 2010 demonstrations, protests ignited again from the student movement. Organised by the NUS, around 10,000 demonstrators marched in central London.[86]		After a five-day trial in the High Court in June 2012, 27-year-old assistant tutor Luke Cooper, reported to be completing a PhD in international relations at the University of Sussex, was awarded £35,000 over a front page Evening Standard article and £25,000 over a follow-up piece in the Daily Mail that implied he was the "ringleader" of the protesters who invaded the Conservative Party's headquarters. Cooper complained that the allegations were untrue, threatened his future academic prospects and left his reputation "as badly trashed" as the Millbank Tower.[87]		
School district drug policies are measures that administrators of a school district put into place to discourage drug use by students.						Over the decades of the War on Drugs in the United States, primary and secondary school drug and alcohol policies have become increasingly strict, in punishment and in the kinds of behavior regulated. Some school districts include off-campus and out-of-school behavior in their policy's jurisdiction. These policies are frequently part of comprehensive "Drug and alcohol" policies, and are particularly common in urban school districts.		Aspects of the policies may include random drug testing, searches of lockers and personal effects, anti-drug education (e.g., "Just Say No" curricula), and punitive measures including expulsion and suspension.		Advocates of random drug testing argue that it is not just a punitive measure, but may deter drug use. Opponents, however, have argued that drugs commonly used by students, such as alcohol, MDMA, and prescription drugs are either not detected by the tests or are metabolized within a short period of time.[citation needed]		There are about 600 school districts in about 15,000 nationwide that use drug tests, according to officials from the White House Office of National Drug Control Policy.[citation needed] White House officials liken drug testing to programs that screen for tuberculosis or other diseases, and said students who test positive don't face criminal charges.		Civil libertarians have raised concerns with these policies, citing student civil rights and student privacy as principle objections. These cases have resulted in a number of legal challenges in the United States, as well as in related case law (e.g., Morse v. Frederick, the so-called "Bong Hits 4 Jesus" case").		In a 1995 case, Vernonia School District 47J v. Acton, the Supreme Court upheld the legality of random drug tests of student athletes who were not suspected of drug use. The Court reasoned that because school athletes routinely face mandatory physicals and other similar invasions of privacy, they have lower expectations of privacy than the average student. The Court specified that its decision should not be seen as a justification for further expansion of drug testing programs.		In the 2002 case Board of Education v. Earls the Supreme Court extended the holding in Vernonia, holding that all students who participate in voluntary activities, like cheerleading, band, or debate, could be subjected to random tests as part of a comprehensive program. The Court, in an opinion by Justice Thomas, stated that the diminished expectations of privacy of athletes was less important to their decision in Vernonia than a school's innate custodial responsibility and authority over its students.[1]		In December 2009, a challenge was made to the Haddonfield, New Jersey, Board of Education's 24/7 policy regulating drug and alcohol use of students outside of school property and off school time. The lawsuit contends that the Board of Education does not have the authority to discipline students unless the conduct in question has some connection to school safety and discipline.[2] A federal judge denied a request to stop the policy because the student who filed the lawsuit had already graduated.[3] In 2012, the plaintiff's lawyer convinced a different judge to find that the school's 24/7 policy was illegal and the judge found that Haddonfield’s policy fails to differentiate between off-campus offenses that simply break the law and those that affect the school district’s ability to provide a safe environment.[4] The NJ Commissioner of Education then reviewed the policy.[5] In 2013, after the Commissioner of Education ruled against the Haddonfield School Board, the Board voted to scrap the policy for good. [6]		In late 2001, in Ashland, Oregon, the Ashland School Board enacted a Drug and Alcohol Policy for students in leadership positions, igniting a local controversy. The policy extended to off-campus and after-school conduct, but the controversy reached the general efficacy and constitutionality of drug testing policies.[7]		Opposing the policy were local student groups and the local Oregon American Civil Liberties Union, which had advocated on behalf of various students expelled by the Ashland School District for drug use in May 2001 at a national forensics tournament. Students at Ashland High School argued that their off campus behavior after school hours should have no effect on their academic standing.		To resolve the dispute, a community committee was formed, meeting for five months. The committee's recommendations led to a rewriting of the Code of Conduct and a re-evaluation of the School District's entire Drug and Alcohol Policy.		
Seniority in the United States Senate is valuable as it confers a number of benefits and is based on length of continuous service, with ties broken by a series of factors. Customarily, the terms "senior senator" and "junior senator" are used to distinguish the two senators representing a particular state.						The United States Constitution does not mandate differences in rights or power, but Senate rules give more power to senators with more seniority. Generally, senior senators will have more power, especially within their own caucuses. In addition, by custom, senior senators from the president's party control federal patronage appointments in their states.		The president pro tempore of the Senate is traditionally the most senior member of the majority party.		There are several benefits, including the following:		A term does not necessarily coincide with the date the Senate convenes or when the new Senator is sworn in. In the case of Senators first elected in a general election for the upcoming Congress, their terms begin on the first day of the new Congress.[citation needed] Since 1935, that means January 3 of odd-numbered years. The seniority date for an appointed senator is the date of the appointment, not necessarily the date of taking the oath of office.[citation needed] In the case of Senators taking vacant seats in special elections, the term begins on Election Day.[citation needed] However, in both of these cases, if the incoming Senator is a Member of the U.S. House of Representatives at the time, he/she must resign from the House before her/his term in the Senate begins.		A senator's seniority is primarily determined by length of continuous service; for example, a senator who has served for 12 years is more senior than one who has served for 10 years. Because several new senators usually join at the beginning of a new Congress, seniority is determined by prior federal or state government service. These tiebreakers in order are:[1]		When more than one senator has served in the same previous role, length of time in that prior office is used to break the tie. For instance, Richard Shelby and John McCain both took office on January 3, 1987, and each had previously served in the House of Representatives. Shelby, having served 8 years, is more senior than McCain, who served 4.		Only relevant factors are listed below. For senators whose seniority is based on their states' respective populations, the state population ranking is given as determined by the relevant United States Census current at the time they first took their seat.[2][3][4][5]		  Republican R (52)       Democratic D (46)       Independent I (2)		1 (1789) 2 (1791) 3 (1793) 4 (1795) 5 (1797) 6 (1799) 7 (1801) 8 (1803) 9 (1805) 10 (1807)		11 (1809) 12 (1811) 13 (1813) 14 (1815) 15 (1817) 16 (1819) 17 (1821) 18 (1823) 19 (1825) 20 (1827)		21 (1829) 22 (1831) 23 (1833) 24 (1835) 25 (1837) 26 (1839) 27 (1841) 28 (1843) 29 (1845) 30 (1847)		31 (1849) 32 (1851) 33 (1853) 34 (1855) 35 (1857) 36 (1859) 37 (1861) 38 (1863) 39 (1865) 40 (1867)		41 (1869) 42 (1871) 43 (1873) 44 (1875) 45 (1877) 46 (1879) 47 (1881) 48 (1883) 49 (1885) 50 (1887)		51 (1889) 52 (1891) 53 (1893) 54 (1895) 55 (1897) 56 (1899) 57 (1901) 58 (1903) 59 (1905) 60 (1907)		61 (1909) 62 (1911) 63 (1913) 64 (1915) 65 (1917) 66 (1919) 67 (1921) 68 (1923) 69 (1925) 70 (1927)		71 (1929) 72 (1931) 73 (1933) 74 (1935) 75 (1937) 76 (1939) 77 (1941) 78 (1943) 79 (1945) 80 (1947)		81 (1949) 82 (1951) 83 (1953) 84 (1955) 85 (1957) 86 (1959) 87 (1961) 88 (1963) 89 (1965) 90 (1967)		91 (1969) 92 (1971) 93 (1973) 94 (1975) 95 (1977) 96 (1979) 97 (1981) 98 (1983) 99 (1985) 100 (1987)		101 (1989) 102 (1991) 103 (1993) 104 (1995) 105 (1997) 106 (1999) 107 (2001) 108 (2003) 109 (2005) 110 (2007)		111 (2009) 112 (2011) 113 (2013) 114 (2015) 115 (2017) Current		
A medical school is a tertiary educational institution—or part of such an institution—that teaches medicine, and awards a professional degree for physicians and surgeons. Such medical degrees include the Bachelor of Medicine, Bachelor of Surgery (MBBS, MBChB, BMBS), Doctor of Medicine (MD), or Doctor of Osteopathic Medicine (DO). Many medical schools offer additional degrees, such as a Doctor of Philosophy, Master's degree, a physician assistant program, or other post-secondary education.		Medical schools can also employ medical researchers and operate hospitals. Around the world, criteria, structure, teaching methodology, and nature of medical programs offered at medical schools vary considerably. Medical schools are often highly competitive, using standardized entrance examinations, as well as grade point average and leadership roles, to narrow the selection criteria for candidates. In most countries, the study of medicine is completed as an undergraduate degree not requiring prerequisite undergraduate coursework. However, an increasing number of places are emerging for graduate entrants who have completed an undergraduate degree including some required courses. In the United States and Canada, almost all medical degrees are second entry degrees, and require several years of previous study at the university level.		Medical degrees are awarded to medical students after the completion of their degree program, which typically lasts five or more years for the undergraduate model and four years for the graduate model. Many modern medical schools integrate clinical education with basic sciences from the beginning of the curriculum (e.g.[1][2]). More traditional curricula are usually divided into preclinical and clinical blocks. In preclinical sciences, students study subjects such as biochemistry, genetics, pharmacology, pathology, anatomy, physiology and medical microbiology, among others. Subsequent clinical rotations usually include internal medicine, general surgery, pediatrics, psychiatry, and obstetrics and gynecology, among others.		Although medical schools confer upon graduates a medical degree, a physician typically may not legally practice medicine until licensed by the local government authority.[3] Licensing may also require passing a test, undergoing a criminal background check, checking references, paying a fee, and undergoing several years of postgraduate training. Medical schools are regulated by each country and appear in the World Directory of Medical Schools which was formed by the merger of the AVICENNA Directory for medicine and the FAIMER International Medical Education Directory.						By 2005 there were more than 100 medical schools across Africa, most of which had been established after 1970.[4]		There are seven medical schools in Ghana: The University of Ghana Medical School in Accra, the KNUST School of Medical Sciences in Kumasi, University for Development Studies School of Medicine in Tamale, University of Cape Coast Medical School and the University of Allied Health Sciences in Ho, Volta Region, the leading private medical school in Ghana - the Accra College of Medicine,[5] and Family Health Medical School another private medical school.		Basic Medical education lasts 6 years in all the medical schools. Entry into these medical schools are highly competitive and it is usually based on successful completion of the Senior High School Examinations. The University of Ghana Medical School has however introduced a graduate entry medical program to admit students with mainly science-related degrees into a 4-year medical school program.		Students graduating from any of these medical schools get the MBChB degree and the title "Dr". For the First 3 years Students are awarded BSc in the field of Medical science for University of Ghana medical school; and Human biology for KNUST and UDS medical schools. The University of Ghana Medical School and KNUST School of Medical Sciences in Kumasi use the Tradition medical education model whiles University for Development Studies School of Medicine uses the Problem-based learning model.		Medical graduates are then registered provisionally with the Medical and Dental Council (MDC) of Ghana as House Officers (Interns). Upon completion of the mandatory 2-year housemanship, these medical doctors are permanently registered with the MDC and can practice as medical officers (General Practitioners) anywhere in the country. The housemanship training is done only in hospitals accredited for such purposes by the Medical and Dental Council of Ghana		Following the permanent registration with the medical and dental council, doctors can specialize in any of the various fields that is organized by either the West African college of Physicians and Surgeons or the Ghana College of Physician and Surgeons.		Medical officers are also sometimes hired by the Ghana Health Service to work in the Districts/Rural areas as Primary Care Physicians.		In Kenya, medical school is a faculty of a university. Medical education lasts for 5 years after which the student graduates with an undergraduate (MBChB) degree. This is followed by a mandatory 12-month full-time internship at an approved hospital after which one applies for registration with the Kenya Medical Practitioners and Dentists Board if they intend to practice medicine in the country. The first two years of medical school cover the basic medical (preclinical) sciences while the last four years are focused on the clinical sciences and internship.		There are no medical school entry examinations or interviews and admission is based on students' performance in the high school exit examination (Kenya Certificate of Secondary Education - KCSE). Students who took the AS Level or the SAT can also apply but there is a very strict quota limiting the number of students that get accepted into public universities. This quota does not apply to private universities.		There are four established public medical schools:		Both Nairobi and Moi Universities run post graduate medical training programs that run over 3 years and lead to the award of master of medicine, MMed, in the respective specialty.		There has been progress made by the Aga Khan University in Karachi, Pakistan and the Aga Khan University Hospital (AKUH) in Nairobi towards the establishment of a Health Sciences University in Kenya with an associated medical school. AKUH in Nairobi, already offers post graduate MMed programmes. These are run over 4 years.		Completion of formal specialty training in Kenya is followed by two years of supervised clinical work before one can apply for recognition as a specialist, in their respective field, by the medical board.		There are several medical schools in this populous nation, of which the first and foremost is University of Ibadan. Entrance into these schools is highly competitive. Candidates graduating from high school must attain high scores on the West African Examination Council's (WAEC) Senior School Certificate Exam (SSCE/GCE) and high scores in four subjects (Physics, English, Chemistry, and Biology) in the University Matriculation Examination (UME). Students undergo rigorous training for 6 years and culminate with a Bachelor of Medicine and Bachelor of Surgery (MBBS/MBChB). The undergraduate program is six years and one year of work experience in government hospitals. After medical school, graduates are mandated to spend one year of housemanship (internship) and one year of community service before they are eligible for residency.		There are eight medical schools in South Africa, each under the auspices of a public university. As the country is a former British colony, most of the institutions follow the British-based undergraduate method of instruction, admitting students directly from high school into a 6 or occasionally five-year program. Some universities such as the University of the Witwatersrand in Johannesburg and the University of Cape Town have started offering post-graduate medical degrees that run concurrently with their undergraduate programs. In this instance, a student having completed an appropriate undergraduate degree with basic sciences can enter into a four-year postgraduate program.		South African medical schools award the MBChB degree, except the University of the Witwatersrand, which styles its degree MBBCh. Some universities allow students to earn an intercalated degree, completing a BSc (Medical) with an additional year of study after the second or third year of the MBChB. The University of Cape Town, in particular, has spearheaded a recent effort to increase the level of medical research training and exposure of medical students through an Intercalated Honours Programme, with the option to extend this to a PhD.[6]		Following successful completion of study, all South African medical graduates must complete a two-year internship as well as a further year of community service in order to register with the Health Professions Council and practice as a doctor in the country.		Specialisation is usually a five- to seven-year training process (depending on the specialty) requiring registering as a medical registrar attached to an academic clinical department in a large teaching hospital with appropriate examinations. The specialist qualification may be conferred as a Fellowship by the independent Colleges of Medicine of South Africa (CMSA), following British tradition, or as a Magisterial degree by the university (usually the M Med, Master of Medicine, degree). The Medical schools and the CMSA also offer Higher Diplomas in many fields. Research degrees are the M.Med and Ph.D. or M.D., depending on university.		Medical students from all over the world come to South Africa to gain practical experience in the country's many teaching hospitals and rural clinics. The language of instruction is English but a few indigenous languages are studied briefly. The University of the Free State has a parallel medium policy, meaning all English classes are also presented in Afrikaans, therefore students who choose to study in Afrikaans, do so separately from the English class.		In Sudan, medical school is a faculty of a university. Medical school is usually 6 years, and by the end of the 6 years the students acquires a bachelor's degree of Medicine and Surgery. Post graduating there is a mandatory one-year full-time internship at one of the university or Government Teaching hospitals, then a license is issued.		During the first three years the curriculum is completed, and throughout the next three years it is repeated with practical training. Students with high grades are accepted for free in Government Universities. Students who score a grade less than the required would have to pay and must also acquire a still high grade. Students who take foreign examinations other than the Sudanese High School Examination are also accepted in Universities, students taking IGCSE/SATs and the Saudi Arabia examination.		In Tunisia, education is free for all Tunisian citizens and for foreigners who have scholarships. The oldest Medical school is a faculty of the University of Tunis. There are four medicine faculties situated in the major cities of Tunis, Sfax, Sousse and Monastir. Admission is bound to the success and score in the baccalaureate examination. Admission score threshold is very high, based on competition among all applicants throughout the nation. Medical school curriculum consists of five years. The first two years are medical theory, containing all basic sciences related to medicine, and the last three years consists of clinical issues related to all medical specialties. During these last three years, the student gets the status of "Externe". The student has to attend at the university hospital every day, rotating around all wards. Every period is followed by a clinical exam regarding the student's knowledge in that particular specialty. After those five years, there are two years on internship, in which the student is a physician but under the supervision of the chief doctor; the student rotates over the major and most essential specialties during period of four months each. After that, student has the choice of either passing the residency national exam or extending his internship for another year, after which he gains the status of family physician. The residency program consists of four to five years in the specialty he qualifies, depending on his score in the national residency examination under the rule of highest score chooses first. Whether the student chooses to be a family doctor or a specialist, he has to make a doctorate thesis, which he will be defending in front of a jury, after which he gains his degree of Doctor of Medicine (MD).		As of April 2017[update], there are nine accredited medical schools in Uganda.[7] Training leading to the award of the degree of Bachelor of Medicine and Bachelor of Surgery (MBChB) lasts five years, if there are no re-takes. After graduating, a year of internship in a hospital designated for that purpose, under the supervision of a specialist in that discipline is required before an unrestricted license to practice medicine and surgery is granted by the Uganda Medical and Dental Practitioners Council (UMDPC).		There is Postgraduate training such as the degree of Master of Medicine (MMed) which is a three-year programme, available at Makerere University School of Medicine in several disciplines. Makerere University School of Public Health, offers the degree of Master of Public Health (MPH) following a twenty-two (22)-month period of study, which includes field work.[8]		In Zimbabwe there are three medical schools is offering Medical degrees. For undergrads, these are University of Zimbabwe - College of Health Sciences {MBChB}, National University of Science and Technology (NUST) Medical school {MBBS} and Midlands State University (MSU) {MBChB}. Only UZ is offering postgrad degrees in the Medical faculty.		Training lasts 5 1/2 years. The curriculum is as follows:		Internship is 2 years duration, with the first year spent in medicine and surgery and the second year doing pediatrics, anesthesia/psychiatry and obstetrics and gynecology. Thereafter one can apply for MMED at the university which last 4–5 years depending on specialty. Currently no subspecialist education is available.		Medical degree programs in Argentina typically are six years long, with some universities opting for 7 year programs. Each one of the 3000 medical students who graduate each year in Argentina are required before graduation to dedicate a minimum of 8 months to community service without pay; although in some provinces (especially round the more developed south) there are government-funded hospitals who pay for this work. Some universities have cultural exchange programmes that allow a medical student in their final year to serve their community time overseas.		Upon graduation, one of the following degrees is obtained, according to the university: Doctor of Medicine, or both Doctor of Medicine and Doctor of Surgery. Public universities usually confer both degrees, and private universities bestow only Doctor of Medicine. In daily practice, however, there is no substantial difference between what a Doctor of Medicine or a Doctor of Medicine and Doctor of Surgery are allowed to do. When the degree is obtained, a record is created for that new doctor in the index of the National Ministry of Education (Ministerio Nacional de Educación) and the physician is given their corresponding medical practitioner's ID, which is a number that identifies him and his academic achievements. In addition, there is a provincial ID, i. e. a number to identify doctors in the province they practise medicine in.		Doctors wishing to pursue a speciality must take entrance exams at the public/private institution of their choice that offers them. It is easier for students in private Medical Schools to obtain a residency in a Private Hospital, especially when the university has its own hospital, as the university holds positions specifically for its graduates. Speciality courses last about two to five years, depending on the branch of medicine the physician has chosen. There is no legal limit for the number of specialities a doctor can learn, although most doctors choose to do one and then they sub-specialise for further job opportunities and less overall competition, along with higher wages.		In Argentina there are public and private medical schools, however the prestige of the public institutions is undeniable and the private institutions do not normally appear in international rankings. A person who can afford to attend a private university, quite expensive for the average Argentinian, will choose that option over public education because of the smaller groups of students in each class and because of the lack of strictness in course evaluation. By law entrance into public institutions is open and tuition-free to all who have a high school diploma, and universities are expressly forbidden[9] from restricting access with difficult entrance exams. Point in case, in 2016 La Universidad Nacional de la Plata was obligated by the governing bodies to stop forcing its students to write an entrance exam. As a result, that university experienced a major increase in the size of its student population. When it comes to educational quality, la Universidad de Buenos Aires, a public university, is widely recognised as the top medical school in the country.[10]		In Bolivia, all medical schools are faculties within a university and offer a five-year M.D. equivalent. To acquire a license to exercise medical science from the government, all students must also complete 1 year and 3 months of internship. This consists of 3 months each of surgery, internal medicine, gynecology, pediatrics and public health. At least one of the internships must be done in a rural area of the country. After getting the degree and license, a doctor may take a post-graduate residency in order to acquire a specialty.		The Brazilian medical schools follow the European model of a six-year curriculum, divided into three cycles of two years each.[11] The first two years are called basic cycle (ciclo básico). During this time students are instructed in the basic sciences (anatomy, physiology, pharmacology, immunology etc.) with activities integrated with the medical specialties, allowing the student an overview of the practical application of such content. After its completion, the students advance to the clinical cycle (ciclo clinico). At this stage contacts with patients intensify and work with tests and diagnostics, putting into practice what was learned in the first two years. The last two are called cycle internship (ciclo do internato). In this last step the students focus on clinical practice, through training in teaching hospitals and clinics. The teaching of this last step respecting an axis of increasing complexity, enabling students to make decisions and participate effectively in form and operative care under the direct supervision of faculty and qualified to act as teaching aids physicians. The performance of the internal develops redemption of ethical and humanistic dimensions of care, causing the student to recognize the values and principles that guide the physician-patient relationship.		After six years of training, students graduate and are awarded the title of physician (Médico) allowing them to register with the Regional Council of Medicine (Conselho Regional de Medicina). The recent graduate will be able to exercise the medical profession as a general practitioner and may apply to undertake postgraduate training. In 2012, the Regional Council of Medicine of São Paulo (Conselho Regional de Medicina do Estado de São Paulo) established that physicians who graduate from this year must pass a test to obtain professional registration. Passing the exam, however, is not linked to obtaining registration. It required only the presence of the candidate and the test performance. Already at the national level, pending in the Senate a bill creating the National Proficiency Examination in Medicine (Exame Nacional de Proficiência em Medicina), which would make the race a prerequisite for the exercise of profession.		Physicians who want to join a specialization program must undergo a new selection examination considered as competitive as that required to join a medical school. Works in health institutions under the guidance of medical professionals with high ethical and professional qualification. The specialization programs are divided into two categories: direct access and prerequisite. The specialties with direct access are those in which the doctor can enroll without having any prior expertise. Any physicians can apply to examinations for these specialties, regardless of time of training or prior experience. To apply to proprietary pre-requisite, the doctor should have already completed a specialty prior. The programs may range from 2 to 6. In Brazil are currently recognized by the Federal Council of Medicine, the Brazilian Medical Association and the National Commission of Medical Residency 53 residency programs. Fully complied with, gives the title of resident physician specialist.		In 2013, the Association of American Medical Colleges lists 17 accredited MD-granting medical schools in Canada.		In Canada, a medical school is a faculty or school of a university that offers a three- or four-year Doctor of Medicine (M.D. or M.D.C.M.) degree. Generally, medical students begin their studies after receiving a bachelor's degree in another field, often one of the biological sciences. however, admittance can still be granted during third and fourth year. Minimum requirements for admission vary by region from two to four years of post-secondary study. The Association of Faculties of Medicine of Canada publishes a detailed AFMC.ca, guide to admission requirements of Canadian faculties of medicine on a yearly basis.		Admission offers are made by individual medical schools, generally on the basis of a personal statement, undergraduate record (GPA), scores on the Medical College Admission Test (MCAT), and interviews. Volunteer work is often an important criterion considered by admission committees. All four medical schools in Quebec and two Ontario schools (University of Ottawa, Northern Ontario School of Medicine) do not require the MCAT. McMaster requires that the MCAT be written, though they only look for particular scores (6 or better) on the verbal reasoning portion of the test.		The first half of the medical curriculum is dedicated mostly to teaching the basic sciences relevant to medicine. Teaching methods can include traditional lectures, problem-based learning, laboratory sessions, simulated patient sessions, and limited clinical experiences. The remainder of medical school is spent in clerkship. Clinical clerks participate in the day-to-day management of patients. They are supervised and taught during this clinical experience by residents and fully licensed staff physicians.		Students enter into the Canadian Resident Matching Service, commonly abbreviated as CaRMS in the fall of their final year. Students rank their preferences of hospitals and specialties. A computerized matching system determines placement for residency positions. 'Match Day' usually occurs in March, a few months before graduation.[12] The length of post-graduate training varies with choice of specialty.		During the final year of medical school, students complete part 1 of the Medical Council of Canada Qualifying Examination (MCCQE). Upon completion of the final year of medical school, students are awarded the degree of M.D. Students then begin training in the residency program designated to them by CaRMS. Part 2 of the MCCQE, an Objective Structured Clinical Examination, is taken following completion of twelve months of residency training. After both parts of the MCCQE are successfully completed, the resident becomes a Licentiate of the Medical Council of Canada. However, in order to practice independently, the resident must complete the residency program and take a board examination pertinent to his or her intended scope of practice. In the final year of residency training, residents take an exam administered by either the College of Family Physicians of Canada or the Royal College of Physicians and Surgeons of Canada, depending on whether they are seeking certification in family medicine or another specialty.		In 2011, the International Medical Education Directory listed 59 current medical schools in the Caribbean. 54 grant the MD degree, 3 grant the MBBS degree, and 2 grant either the MD or MBBS degree.		30 of the medical schools in the Caribbean are regional, which train students to practice in the country or region where the school is located. The remaining 29 Caribbean medical schools are known as offshore schools, which primarily train students from the United States and Canada who intend to return home for residency and clinical practice after graduation.[13] At most offshore schools, basic sciences are completed in the Caribbean while clinical clerkships are completed at teaching hospitals in the United States.		Several agencies may also accredit Caribbean medical schools, as listed in the FAIMER Directory of Organizations that Recognize/Accredit Medical Schools (DORA). 25 of the 29 regional medical schools in the Caribbean are accredited, while 14 of the 30 offshore medical schools are accredited.		Curaçao currently (2015), has 5 medical schools and one other medical university under construction. The majority are located within the city of Willemstad. All six medical schools on the island of Curaçao, only provide education in Basic Medical Science (BMS) which goes towards the degree of Medical Doctor or Doctor of Medicine (2016). Presently, none of the medical schools offer other degrees; such as MBBS or PhD (2016). All students after completing their medical school's Basic Medical Science program in Curaçao; will then have to apply to either take USMLE Step Exams, The Canadian or UK Board Exams. A large percentage of these medical students who attend these medical schools in Curaçao are either from North America, Africa, Europe or Asia.		In Chile, there are 21 medical schools. Principal medical schools are Pontificia Universidad Católica de Chile in Santiago, Universidad de Chile, Universidad de Concepción and Universidad de Santiago de Chile. The pre-grade studies are distributed in 7 years, where the last 2 are the internship, that include at least surgery, internal medicine, gynecology and pediatrics. After getting the degree of Licenciate in Medicine (General Medicine) the M.D. must pass a medicine knowledge exam called National Unic Exam of Medical Knowledge (EUNACOM "Examen Único Nacional de Conocimientos de Medicina" in Spanish) and can take a direct specialty or work before in primary attention in order to gain access to a residency.		In Colombia, there are 50 medical schools listed in the World Directory of Medical Schools, 27 of which have active programs and are currently registered and accredited as high-quality programs by the Colombian Ministry of Education. The main medical programs are offered by the Universidad Nacional de Colombia, Pontificia Universidad Javeriana, Universidad del Rosario, Universidad El Bosque, Universidad de los Andes, Universidad del Valle, Universidad de Antioquia, and Universidad de la Sabana. Most programs require between 6–7 years of study, and all offer a Doctor of Medicine (MD) degree. In some cases the school also allows for a second degree to be studied for at the same time (this is chosen by the student, though most students end up needing to do alternate semesters between their degrees, and mostly in careers like microbiology or biomedical engineering). For example, the Universidad de los Andes has a program whereby the medical student could graduate with both an MD and a Master of Business Administration (MBA) degree, or an MD and a master's degree in public health. Admission to medical school varies with the school, but is usually dependent on a combination of a general application to the university, an entrance exam, a personal statement or interview, and secondary (high) school performance mostly as reflected on the ICFES score (the grade received on the state exam in the final year of secondary/high school).		In most medical programs, the first two years deal with basic scientific courses (cellular and molecular biology, chemistry, organic chemistry, mathematics, and physics), and the core medical sciences (anatomy, embryology, histology, physiology, and biochemistry). The following year may change in how it is organized in different schools, but is usually organ system-based pathophysiology and therapeutics (general and systems pathology, pharmacology, microbiology, parasitology, immunology, and medical genetics are also taught in this block). In the first two years, the programs also usually begin the courses in the epidemiology track (which may or may not include biostatistics), a clinical skills track (semiology and the clinical examination), a social medicine/public health track, and a medical ethics and communication skills track. Modes of training vary, but are usually based on lectures, simulations, standardized-patient sessions, problem-based learning sessions, seminars, and observational clinical experiences. By year three, most schools have begun the non-elective, clinical-rotation block with accompanying academic courses (these include but are not limited to internal medicine, pediatrics, general surgery, anaesthesiology, orthopaedics, gynaecology and obstetrics, emergency medicine, neurology, psychiatry, oncology, urology, physical medicine and rehabilitation, ophthalmology, and otorhinolaryngology). Elective rotations are usually introduced in the fourth or fifth year, though as in the case of the non-elective rotations, the hospitals the medical students may be placed in or apply to for a given rotation depend entirely on the medical schools. This is important in terms of the medical training, given the particular distinction of patients, pathologies, procedures, and skills seen and learned in private vs. public hospitals in Colombia. Most schools, however, have placements in both types of hospitals for many specialties.		The final year of medical school in Colombia is referred to as the internship year ("internado"). The internship year is usually divided into two semesters. The first semester is made up of obligatory rotations that every student does though in different orders, and the medical intern serves in 5-7 different specialties, typically including internal medicine, paediatrics, general surgery, anaesthesiology, orthopaedics, gynaecology and obstetrics, and emergency medicine. The extent of the responsibilities of the intern varies with the hospital, as does the level of supervision and teaching, but generally, medical interns in Colombia extensively take, write, and review clinical histories, answer and discuss referrals with their seniors, do daily progress notes for the patients under their charge, participate in the service rounds, present and discuss patients at rounds, serve shifts, assist in surgical procedures, and assist in general administrative tasks. Sometimes, they are charged with ordering diagnostic testing, but, under Colombian law they cannot prescribe medication as they are not graduate physicians. This, of course, are to be completed in addition to their academic responsibilities. The second semester is made up of elective rotations, which can be at home or abroad, in the form of clerkships or observerships. A final graduation requirement is to sit a standardized exam, the State Exam for Quality in Higher Education ("Examen de Estado de Calidad de la Educación Superior" or ECAES, also known as SABER PRO) specific to medicine, which tests, for example, knowledge in public health and primary care.		After graduation, the physician is required to register with the Colombian Ministry of Health, in order to complete a year of obligatory social service ("servicio social obligatorio"), after which they qualify for a professional license to practice general medicine and apply for a medical residency within Colombia. If, however, the student wishes to practice general medicine abroad or continue onto their postgraduate studies, for example, they can independently begin the appropriate application/equivalency process, without doing their obligatory social service. In this case they would not be licenced to practice medicine in Colombia and if they wish to do so, will have to register with the Ministry of Health. N.B. If the graduate physician gets accepted immediately into a residency within Colombia in internal medicine, paediatrics, family medicine, gynecology and obstetrics, general surgery or anaesthesiology, they are allowed to complete a 6-month-long social service after their residency.		In contrast with most countries, residencies in Colombia are NOT paid positions, since one applies for the program through the university offering the post, which requires a tuition. However, on 9th May, 2017, legislation was formally introduced in Congress that would seek to regulate payment for medical residents, regulate their tuitions, and advocate for their vacation time and working hours. As in other countries, length of residency training depends upon the specialty chosen, and, following its completion, the physician may choose to apply for a fellowship (subspecialty) at home or abroad depending on the availability of their desired training programs, or practice in their specialty.		The Universidad de El Salvador (University of El Salvador) has a program of 8 years for students who want to study medicine. The first six years are organized in a two semesters fashion, the seventh year is used for a rotating internship through the mayor specialty areas in a 10-week periods fashion (psychiatry and public health share a period) and the eighth year is designated for Social service in locations approved by the Ministry of Health (usually as attending physician in Community Health Centers or non-profit organizations). The graduates receive the degree of MD and must register in the Public Health Superior Council(CSSP) to get the medical license and a registered national number that allows them to prescribe barbiturates and other controlled drugs. In order to attend further studies (Surgery, Internal medicine, G/OB, Pediatrics, Psychiatry), the students in the year of Social service or graduates of any Salvadorian university must apply independently for the residency to the hospital of choice; the preliminary selection process is based on the results of clinical knowledge tests, followed by psychiatric evaluations and interviews with the hospital medical and administrative staff. The basic residencies mentioned above commonly last 3 years; at the last trimester of the third year, the residents can apply to the position of Chief of residents (1 year) or follow further studies as resident (3 years) of a specialty (for example:orthopedic surgery, urology, neurology, endocrinology...). No further studies are offered to the date; therefore, specialist looking for training or practice in a specific area (For example: a neurosurgeon looking for specialty in endovascular neurosurgery, spine surgery or pediatric neurosurgery) must attend studies in other countries and apply for such positions independently.		In Guyana the medical school is accredited by the National Accreditation Council of Guyana. The medical program ranges from 4 years to 6 years. Students are taught the basic sciences aspect of the program within the first 2 years of medical school. In the clinical sciences program, students are introduced to the hospital setting where they gain hands on training from the qualifying physicians and staff at the various teaching hospitals across Guyana.		Students graduating from the University of Guyana are not required to sit a board exams before practicing medicine in Guyana. Students graduating from the American International School of Medicine sit the USMLE, PLAB or CAMC exams.		Medical schools in Haiti conduct training in French. The universities offering medical training in Haiti are the Université Notre Dame d'Haïti, Université Quisqueya, Université d'Etat d'Haïti and Université Lumière.		The Université Notre Dame d'Haïti (UNDH) is a private Catholic university established by the Episcopal Conference of Haiti. According to the UNDH website, "the UNDH is not just about academic degrees, it is mainly the formation of a new type of Haiti, which includes in its culture and moral values of the Gospel, essential for serious and honest people that the country needs today."		The other two private schools offering medical degrees are Université Quisqueya and Université Lumière. The Université d'Etat d'Haïti is a public school.[14]		Attending medical school in Haiti may be less expensive than attending medical universities located in other parts of the world, but the impact of the country's political unrest should be considered, as it affects the safety of both visitors and Haitians.		Duration of basic medical degree course, including practical training: 6 years		Title of degree awarded: Docteur en Médecine (Doctor of Medicine)		Medical registration/license to practice: Registration is obligatory with the Ministère de la Santé publique et de la Population, Palais des Ministères, Port-au-Prince. The license to practice medicine is granted to medical graduates who have completed 1 year of social service. Those who have qualified abroad must have their degree validated by the Faculty of Medicine in Haiti. Foreigners require special authorization to practice.		The system of Medical education in Panama usually takes students from high school directly into Medical School for a 6-year course, typically with a two years internship.		In 2012, the Association of American Medical Colleges and American Association of Colleges of Osteopathic Medicine listed 141 accredited M.D.-granting[15] and 30 accredited D.O.-granting medical schools[16] in the United States.		The Doctor of Medicine (MD) and Doctor of Osteopathic Medicine (DO) are graded to be equivalent to a Professional Doctorate.[17][18][19]		Admission to medical school in the United States is based mainly on a GPA, MCAT score, admissions essay, interview, clinical work experience, and volunteering activities, along with research and leadership roles in an applicant's history. While obtaining an undergraduate degree is not an explicit requirement for a few medical schools, virtually all admitted students have earned at least a bachelor's degree. A few medical schools offer pre-admittance to students directly from high school by linking a joint 3-year accelerated undergraduate degree and a standard 4-year medical degree with certain undergraduate universities, sometimes referred to as a "7-year program", where the student receives a bachelor's degree after their first year in medical school.		As undergraduates, students must complete a series of prerequisites, consisting of biology, physics, and chemistry (general chemistry and organic). Many medical schools have additional requirements including calculus, genetics, statistics, biochemistry, English, and/or humanities classes. In addition to meeting the pre-medical requirements, medical school applicants must take and report their scores on the MCAT, a standardized test that measures a student's knowledge of the sciences and the English language. Some students apply for medical school following their third year of undergraduate education while others pursue advanced degrees or other careers prior to applying to medical school.		In the nineteenth century, there were over four hundred medical schools in the United States. By 1910, the number was reduced to one hundred and forty-eight medical schools and by 1930 the number totaled only seventy-six. Many early medical schools were criticized for not sufficiently preparing their students for medical professions, leading to the creation of the American Medical Association in 1847 for the purpose of self-regulation of the profession. Abraham Flexner (who in 1910 released the Flexner report with the Carnegie Foundation), the Rockefeller Foundation, and the AMA are credited with laying the groundwork for what is now known as the modern medical curriculum.[20] The restriction of the supply of physicians that resulted from the Flexner Report has been criticized by classical economists as one of the principal factors in the increased prices relative to quality observed in medicine over the past 100 years.[21]		The standard U.S. medical school curriculum is four years long. Traditionally, the first two years are composed mainly of classroom basic science education, while the final two years primarily include rotations in clinical settings where students learn patient care firsthand. Today, clinical education is spread across all four years with the final year containing the most clinical rotation time. The Centers for Medicare and Medicaid Services (CMS) of the U.S. Department of Health and Human Services (HHS) has published mandatory rules, obliging on all inpatient and outpatient teaching settings, laying down the guidelines for what medical students in the United States may do, if they have not completed a clerkship or sub-internship. These rules apply to when they are in the clinical setting in school, not when they are, for example, helping staff events or in other non-formal educational settings, even if they are helping provide certain clinical services along with nurses and the supervising physicians- for example, certain basic screening procedures. In the formal clinical setting in school, they can only assist with certain patient evaluation and management tasks, after the vital signs, chief complaint and the history of present illness have been discerned, but prior to the physical examination: reviewing the patient's signs and symptoms in each body system, and then reviewing the patient's personal medical, genetic, family, educational/occupational, and psychosocial history. The student's supervising physician (or another physician with supervisory privileges if the original doctor is no longer available, for some reason) must be in the room during the student's work, and must conduct this same assessment of the patient before performing the actual physical examination, and after finishing and conferring with the student, will review his or her notes and opinion, editing or correcting them if necessary, and will also have his or her own professional notes; both must then sign and date and I.D. the student's notes and the medical record. They may observe, but not perform, physical examinations, surgeries, endoscopic or laparoscopic procedures, radiological or nuclear medicine procedures, oncology sessions, and obstetrics. The patient must give consent for their presence and participation in his or her care, even at a teaching facility. Depending on the time they have completed in school, their familiarity with the area of medicine and the procedure, and the presence of their supervisor, and any others needed, in the room or nearby, they may be allowed to conduct certain very minor tests associated with the physical examination, such as simple venipuncture blood draws, and electrocardiograms and electroencephalograms, for learning and experience purposes, especially when there is no intern or resident available.		Upon successful completion of medical school, students are granted the title of Doctor of Medicine (M.D.) or Doctor of Osteopathic Medicine (D.O.). Residency training, which is a supervised training period of three to seven years (usually incorporating the 1st year internship)typically completed for specific areas of specialty. Physicians who sub-specialize or who desire more supervised experience may complete a fellowship, which is an additional one to four years of supervised training in their area of expertise.		Unlike those in many other countries, US medical students typically finance their education with personal debt. In 1992, the average debt of a medical doctor after residency was $25,000. For the class of 2009, the average debt of a medical student is $157,990 and 25.1% of students had debt in excess of $200,000 (prior to residency).[22] For the past decade the cost of attendance has increased 5-6% each year (roughly 1.6 to 2.1 times inflation).[23]		Licensing of medical doctors in the United States is coordinated at the state level. Most states require that prospective licensees complete the following requirements:		The University of Montevideo in Uruguay is the oldest in Latin America, being public and free, co-governed by students, graduates and teachers. The progress of medical and biological sciences in the nineteenth century, the impact of the work of Claude Bernard (1813–1878), Rudolf Virchow (1821–1902) Robert Koch (1843–1910), Louis Pasteur (1822–1895) and all the splendor of French medical schools, Vienna, Berlin and Edinburgh, was a stimulus for the creation of a medical school in the country. The basic medical school program lasts seven years. There is also a second medical school in the country, it is private and located in Punta del Este, Maldonado.		These are the universities with a medical school in Venezuela:		Historically, Australian medical schools have followed the British tradition by conferring the degrees of Bachelor of Medicine and Bachelor of Surgery (MBBS) to its graduates whilst reserving the title of Doctor of Medicine (MD) for their research training degree, analogous to the PhD, or for their honorary doctorates. Although the majority of Australian MBBS degrees have been graduate programs since the 1990s, under the previous Australian Qualifications Framework (AQF) they remained categorised as Level 7 Bachelor degrees together with other undergraduate programs.		The latest version of the AQF includes the new category of Level 9 Master's (Extended) degrees which permits the use of the term 'Doctor' in the styling of the degree title of relevant professional programs. As a result, various Australian medical schools have replaced their MBBS degrees with the MD to resolve the previous anomalous nomenclature.[24] With the introduction of the Master's level MD, universities have also renamed their previous medical research doctorates. The University of Melbourne was the first to introduce the MD in 2011 as a basic medical degree, and has renamed its research degree to Doctor of Medical Science (DMedSc).[25]		In Bangladesh, admission to medical colleges is organized by the Governing Body of University of Dhaka. A single admission test is held for government and private colleges. Due to the highly competitive nature of these exams, the total number of applicants across the country is around 78 times the number of students accepted.[citation needed] Admission is based on the entrance examination, as well as students' individual academic records.[citation needed]		The entrance examination consists carries a time limit of one hour. 100 marks are allocated based on objective questions, in which the mark allocation is distributed between a variety of subjects. Biology questions carry 30 marks, Chemistry carries 25, Physics carries 20, English carries 15, and general knowledge carries 10.		Additionally, students' previous SSC (Secondary School Certificate) and HSC (Higher Secondary School Certificate) scores each carry up to 100 marks towards the overall examination result.		English students prepare themselves for the admission exam ahead of time. This is because as the GCSE and A-Level exams do not cover parts of the Bangladesh syllabus.[citation needed]		The undergraduate program consists of five years study, followed by a one-year internship. The degrees granted are Bachelor of Medicine and Bachelor of Surgery (M.B.B.S.). Further postgraduate qualifications may be obtained in the form of Diplomas or Degrees (MS or MD), M.Phil and FCPS (Fellowship of the College of Physicians and Surgeons).		The University of Dhaka launched[when?] a new BSc in "Radiology and Imaging Technology," offering 30 students the opportunity to contribute towards their entrance exam grade. For students who have passed the HSC, this course contributes towards 25% of the mark. The course contributes up to 75% for Diploma-holding students. The duration of the course is four years (plus 12 weeks for project submission). The course covers a variety of topics, including behavioural science, radiological ethics, imaging physics and general procedure.		After 6 years of general medical education (a foundation year + 5 years), all students will graduate with Bachelor of Medical Sciences (BMedSc) បរិញ្ញាប័ត្រ វិទ្យាសាស្រ្តវេជ្ជសាស្ត្រ. This degree does not allow graduates to work independently as Physician, but it is possible for those who wish to continue to master's degrees in other fields relating to medical sciences such as Public Health, Epidemiology, Biomedical Science, Nutrition...		Medical graduates, who wish to be fully qualified as physicians or specialists must follow the rule as below:		All Medical graduates must complete Thesis Defense and pass the National Exit Exam ប្រឡងចេញថ្នាក់ជាតិក្នុងវិស័យសុខាភិបាល to become either GPs or Medical or Surgical Specialists.		Hong Kong has only two comprehensive medical faculties, the Li Ka Shing Faculty of Medicine, University of Hong Kong and the Faculty of Medicine, Chinese University of Hong Kong, and they are also the sole two institutes offering medical and pharmacy programs. Other healthcare discipline programs (like nursing) are dispersed among some other universities which do not host a medical faculty.		Prospective medical students enter either one of the two faculties of medicine available (held by The University of Hong Kong and The Chinese University of Hong Kong) from high schools. The medical program consists of 5 years for those who take the traditional Hong Kong's Advanced Level Examination (HKALE) for admission, or 6 years for those who take the new syllabus Hong Kong's Diploma of Secondary School Education Examination (HKDSE). International students who take examinations other the two mentioned will be assessed by the schools to decide if they will take the 5-year program or the 6-year one.		The competition of entering the medical undergraduate programs is cut-throat as the number of intake each year is very limited with a quota of 210 from each school (420 in total) and candidates need to attain an excellent examination result and good performance in interview. The schools put a great emphasis on students' languages (both Chinese and English) and communication skills as they need to communicate with other health care professionals and patients or their family in the future.		During their studies at the medical schools, students need to accumulate enough clinical practicing hours in addition before their graduation.		The education leads to a degree of Bachelor of medicine and Bachelor of surgery (M.B., B.S. by HKU or M.B., Ch.B. by CUHK). After a 5- or 6-year degree, one year of internship follows in order to be eligible to practice in Hong Kong.		Both HKU and CUHK provide a prestigious bachelor of pharmacy course that is popular among local and overseas students. Students of most other health care disciplines have a study duration of 4 years, except nursing programs which require 5 years.		In India, admission to medical colleges is organized both by the central government CBSE as well as the state governments through tests known as entrance examination. Students who have successfully completed their 10+2 (Physics, Chemistry and Biology Marks are considered and PCB is mandatory) education (higher secondary school) can appear for the tests the same year.		The All-India Pre Medical/Dental Test for filling up of 15% of total MBBS seats in India, conducted by CBSE (Central Board for Secondary Education) in the month of April/May intakes about only 2,500 students out of a total applicants of over 600,000.[26] The Supreme Court Of India has mandated the necessity of entrance examination based upon multiple choice questions and negative marking for wrong answers with subsequent merit over 50% for selection into MBBS as well as higher medical education. The entrance exams are highly competitive.		The graduate program consists of three professionals consisting of 9 semesters, followed by one-year internship (rotating housemanship). The degree granted is Bachelor of Medicine and Bachelor of Surgery (M.B.B.S.) of five years and six months.		The graduate degree of MBBS is divided into 3 professionals, with each professional ending with a professional exam conducted by the university (a single university may have up to dozens of medical colleges offering various graduate/post-graduate/post-doctoral degrees). After clearing this the student moves into the next professional. Each professional exam consists of a theory exam and a practical exam conducted not only by the same college but also external examiners. The exams are tough and many students are unable to clear them, thereby prolonging their degree time. The first professional is for 1 year and includes preclinical subjects, anatomy, physiology and biochemistry. The second professional is for 1 and a half year and has subjects pathology, pharmacology, microbiology (including immunology) and forensic medicine. Clinical exposure starts in the second professional. The third professional is divided into two parts. Part 1 consists of ophthalmology, ENT, and PSM (preventive and social medicine) and part 2 consists of general-medicine[including dermatology, psychiatry as short subjects], general surgery [including radiology, anaesthesiology and orthopaedics as short subjects] and pediatrics and gynaecology and obstetrics . This is followed by one-year of compulsory internship (rotatory house-surgeonship[disambiguation needed]). After internship, the degree of MBBS is awarded by the respective university. Some states have made rural service compulsory for a certain period of time after MBBS.		Selection for higher medical education is through entrance examinations as mandated by the Supreme Court Of India. Further postgraduate qualifications may be obtained as Post-graduate Diploma of two years residency or Doctoral Degree (MS: Master of Surgery, or MD) of three years of residency under the aegis of the Medical Council of India. 50% of all MD/MS seats in India are filled up through "All-India Post-Graduate Medical Entrance Examination conducted by AIIMS (All-India Institute Of Medical Sciences) under the supervision of the Directorate General Of Health Services. Theses/Dissertations are mandatory to be submitted and cleared by university along with examinations(written and clinicals) to obtain MD/MS degree. Further sub-speciality post-doctoral qualification (DM - Doctorate of Medicine, or MCh - Magister of Chirurgery) of three years of residency followed by university examinations may also be obtained.[27]		PG (post-graduate) qualification is equivalent to M.D./M.S., consisting of two/three-years residency after MBBS. A PG diploma may also be obtained through the National Board of Examinations (NBE), which also offers three-years residency for sub-specialisation. All degrees by NBE are called DNB (Diplomate of National Board). DNB's are awarded only after clearance of theses/dissertations and examinations. DNBs equivalent to DM/MCh have to clear examinations mandatorily.[28]		In Indonesia, high school graduates who want to enroll to public medical schools must have their names enlisted by their high school faculty in the "SNMPTN Undangan" program, arranged by Directorate General of Higher Education, Ministry of National Education. Depending on the high school accreditation, only the class' top 10%-15% will be considered for admissions. Fewer places are available through entrance exam conducted autonomously by each university. These exams are highly competitive for medicine, especially in prestigious institutions such as University of Indonesia in Jakarta, Airlangga University in Surabaya, and Gadjah Mada University in Yogyakarta. For private medical school, almost all places are offered through independently run admission tests.		The standard Indonesian medical school curriculum is six years long. The four years undergraduate program is composed mainly of classroom education, continued with the last two years in professional program primarily includes rotations in clinical settings where students learn patient care firsthand. If they pass undergraduate program they will have "S.Ked" (Bachelor of Medicine) in their title and if they finished the professional program and pass the national examination arranged by IDI (Indonesian Medical Association) they will become general physician and receive "dr. (doctor)".		Upon graduation, a physician planning to become a specialist in specific field of medicine must complete a residency, which is a supervised training with period of three to four years. A physician who sub-specializes or who desires more supervised experience may complete a fellowship, which is an additional one to three years of supervised training in his/her area of expertise		General medicine education in Iran takes 7 to 7.5 years. Students enter the university after high school. Students study basic medical science (such as anatomy, physiology, biochemistry, histology, biophysics, embryology, etc.) for 2.5 years. At the end of this period they should pass a "basic science" exam. Those who passed the exam will move on to study physiopathology of different organs in the next 1.5 years. The organ-based learning approach emphasizes critical thinking and clinical application. In the next period of education students enter clinics and educational hospitals for two years. During this period, they will also learn practical skills such as history taking and physical examination. Students should then pass the "pre-internship" exam to enter the last 1.5 years of education in which medical students function as interns. During this period, medical students participate in all aspects of medical care of the patients and they take night calls. At the end of these 7.5 years students are awarded an M.D degree. M.D doctors can continue their educations through residency and fellowship.		There are five university medical schools in Israel: the Technion in Haifa, Ben Gurion University in Be'er Sheva, Tel Aviv University, the Hebrew University in Jerusalem and the Medical school of the Bar-Ilan University in Ramat Gan. These all follow the European 6-year model except Bar-Ilan University which has a four-year program similar to the US system.[29]		The Technion Medical School, Ben Gurion University, and Tel Aviv University Sackler Faculty of Medicine[30] offer 4-year MD programs for American Bachelor's graduates who have taken the MCAT, interested in completing rigorous medical education in Israel before returning to the US or Canada.		The entrance requirements of the various schools of medicine are very strict. Israeli students require a high school Baccalaureate average above 100 and psychometric examination grade over 700. The demand for medical education is strong and growing and there is a lack of doctors in Israel.		The degree of Doctor of Medicine (MD) is legally considered to be equivalent to a Masters degree within the Israeli Educational System .[31]		In Japan, medical schools are faculties of universities and thus they are undergraduate programs that generally last for six years. Admission is based on an exam taken at the end of high school and an entrance exam at the university itself, which is the most competitive.		Medical students study Liberal Arts and Science for the first 1–2 years, which include Physics, Mathematics, Chemistry, and Foreign Languages together with 2 years long Basic Medicine (Anatomy, Physiology, Pharmacology, Immunology), Clinical Medicine, Public health, and Forensics for the next two years.		Medical students train in the University Hospital for the last two years. Clinical training is a part of the curriculum. Upon completion of the graduation examination, students are awarded an M.D. Medical graduates are titled as Doctor, as are Ph.D. holders. The University does have an MD/PhD program that enables Doctors of Medicine to become Ph.D. holders, as well.		At the end, Medical students take the National Medical License examination and, if they pass it, become a Physician and register in the record in the Ministry of Health, Labour and Welfare. The scope of this exam encompasses every aspect of medicine.		The Bachelor of Medicine and Surgery (MBBS) degree is awarded in Jordan after completion of six years comprising three years of medical sciences and three clinical years. Currently, four state supported universities include a medical school and grant the degree, which are:		In Kyrgyzstan, the Government university Kyrgyz State Medical Academy offers 6 years duration undergraduate (bachelor's degree) program whereas the other institutions mostly private such as the International School of Medicine at the International University of Kyrgyzstan[32][33] offers a five-year medical program, with a requisite for English knowledge, that is recognized by the World Health Organization, the General Medical Council, and UNESCO. The medical school is also partnered with the University of South Florida School of Medicine, the University of Heidelberg (Germany), the Novosibirsk Medical University (Russia), and the University of Sharjah (UAE).		Other medical schools located in Kyrgyzstan include the 5 years duration MD/MBBS undergraduate degree program at International University of Science and Business or Mezhdunarodnyy Universitet Nauki i Biznesa, Kyrgyzstan[34] others are the Asian Medical Institute, Kyrgyzstan[35] and the Medical Institute, Osh State University[36] and so on.		In Lebanon, there are two programs of medical education followed: the American system (4 years) and the European system (6 years). Programs are offered in English and French. Admission requirements to the American system requires a candidate to complete a bachelor's degree along with specific pre-medical courses during the undergraduate years, and writing the MCAT examination. European programs usually requires a candidate to complete 1 year of general science followed by a selection exam by the end of the year.		Schools following the American system (M.D. degree) are:		The language of instruction in all three is English.		Schools following the European system (MBBS degree) are:		In Malaysia, getting into medical school is regarded as difficult, due to high fees and a rigorous selection process.[citation needed] Some new medical schools do offer a foundation in medicine course before admission into a full-time medical programme. Most government, and some private medical schools offer M.D., and others mostly offer MBBS degrees.		There are five medical institutions - UM 1, UM 2, DSMA, UM Mdy, and UM Mgy - in Myanmar. Myanmar medical schools are government-funded and require Myanmar citizenship for eligibility. No private medical school exists at this moment. In Myanmar, admission to medical colleges is organized under the Department of Health Science, which is the branch of Ministry of Health of Myanmar. A student can join one of the Five medical universities of Myanmar if he gets the highest scores in the science combination of the matriculation examination. This exam is highly competitive. Entrance is solely based on this examination and academic records have very minor consequences on an application. The undergraduate program is five years plus one year for work experience in government hospitals. After medical school, Myanmar medical graduates are under contract to spend one year of internship and three years of tenure in rural areas before they are eligible for most residency positions. The degree granted is Bachelor of Medicine and Bachelor of Surgery (M.B.B.S.). Further postgraduate qualifications may be obtained as a Degree (M.Med. Sc) and (Dr.Med.Sc).		In Nepal, medical studies start at undergraduate level. As of 2016[update], there are twenty institutions recognised by the Nepal Medical Council.[37] There are four main medical bodies in Nepal:[citation needed]		New Zealand medical programs are undergraduate-entry programs of six years duration. Students are considered for acceptance only after a year of undergraduate basic sciences or, as alternative, following the completion of a bachelor's degree. There are two main medical schools in New Zealand: the University of Auckland and the University of Otago. Each of these has subsidiary medical schools such as Otago's Wellington School of Medicine and Health Sciences and Auckland's Waikato Clinical School.		The first year of the medical degree is the basic sciences year, which comprises study in chemistry, biology, physics, and biochemistry as well as population health and behavioural sciences. The following two years are spent studying human organ systems and pathological processes in more detail as well as professional and communication development. Toward the end of the third year, students begin direct contact with patients in hospital settings.		The clinical years begin fully at the beginning of year 4, where students rotate through various areas of general clinical medicine with rotation times varying from between two and six weeks. Year 5 continues this pattern, focusing more on specialized areas of medicine and surgery. Final medical school exams (exit exams) are actually held at the end of year 5, which is different from most other countries, where final exams are held near the very end of the medical degree. Final exams must be passed before the student is allowed to enter year 6.		The final year (Year 6) of medical school is known as the "Trainee Intern" year, wherein a student is known as a "Trainee Intern" (commonly referred to in the hospitals as a "T.I."). Trainee interns repeat most rotations undertaken in years 4 and 5 but at a higher level of involvement and responsibility for patient care. Trainee interns receive a stipend grant from the New Zealand government (not applicable for international students). At the current time, this is $NZ 26,756/year (about $US 18,500). Trainee interns have responsibility under supervision for the care of about one-third the patient workload of a junior doctor. However, all prescriptions and most other orders (e.g., radiology requests and charting of IV fluids) made by trainee interns must be countersigned by a registered doctor.		New Zealand medical schools currently award the degrees of Bachelor of Medicine and Bachelor of Surgery (MBChB).		Upon completion of the 6th year, students go on to become "House Officers," also known as "House Surgeons" for 1–2 years where they rotate through specialities in the first year and then begin to narrow down to what they'd like to do for speciality training in the second year. After 2 years of house officer work they apply to get into a training scheme and start to train towards the speciality.		In Pakistan a medical school is more often referred to as a medical college. A medical college is affiliated with a university as a department. There are however several medical universities and medical institutes with their own medical colleges. All medical colleges and universities are regulated by the respective provincial department of health. They however have to be recognized after meeting a set criteria by a central regulatory authority called Pakistan Medical and Dental Council (PMDC) in Islamabad. There are almost equal number of government and private medical colleges and universities, with their number exceeding 50. Admission to a government medical college is highly competitive. Entrance into the medical colleges is based on merit under the guidelines of PMDC. Both the academic performance at the college (high school, grades 11-12) level and an entrance test like MCAT are taken into consideration for the eligibility to enter most of the medical colleges. After successfully completing five years of academic and clinical training in the medical college and affiliated teaching hospitals the graduates are awarded a Bachelor of Medicine and Bachelor of Surgery (MBBS) degree. The graduates are then eligible to apply for a medical license from the PMDC. A house job of one-year duration is mandatory in a teaching hospital after completing five years of academic and clinical training in the medical college.		Medical education is normally a five-year Bachelor degree, including one-year internship (or clinical rotation, during which students are actively involved in patient care) before the final degree is awarded. Clinical specialization usually involves a two- or three-year Master degree. Acceptance is based on the national entrance examination used for all universities. There are a few colleges that teach in English and accept foreign medical students. Some of those universities have increased their course duration to 6 years.		The degree conferred is known as Bachelor of Clinical Medicine (BCM).		The Dominicans, under the Spanish Government, established the oldest medical school in the Philippines in 1871, known as the Faculty of Medicine and Surgery (at that time was one with the University of Santo Tomas Faculty of Pharmacy, also considered the oldest pharmacy school in the Philippines) of the Pontifical and Royal University of Santo Tomas in Intramuros, Manila.		Medical education in the Philippines became widespread under the American administration. The Americans, led by the insular government's Secretary of the Interior, Dean Worcester, built the University of the Philippines College of Medicine and Surgery in 1905. By 1909, nursing instruction was also begun at the Philippine Normal School.		At present there are a number of medical schools in the Philippines, notable examples include the University of the Philippines College of Medicine, Our Lady of Fatima University, Far Eastern University – Nicanor Reyes Medical Foundation, Saint Louis University International School of Medicine, De La Salle Health Sciences Institute, University of Santo Tomas Faculty of Medicine and Surgery, Pamantasan ng Lungsod ng Maynila, UERMMMC College of Medicine, St. Luke's College of Medicine–William H. Quasha Memorial, Cebu Doctors' University, Cebu Institute of Medicine, Mindanao State University College of Medicine, Cagayan State University College of Medicine in Tuguegarao, Southwestern University - Matias H. Aznar Memorial College of Medicine Inc., West Visayas State University in Iloilo City, University of St. La Salle College of Medicine in Bacolod City, Davao Medical School Foundation in Davao City, Xavier University – Ateneo de Cagayan, Dr. Jose P. Rizal School of Medicine in Cagayan de Oro, Ago medical educational center AMEC-BCCM in Legazpi, Bicol and University of Northern Philippines in Vigan.		In 2007, the Ateneo School of Medicine and Public Health was established. It is the first medical school in the country to offer a double degree program leading to the degrees Doctor of Medicine and Masters in Business Administration.		Any college graduate may apply for medical school given that they satisfy the requirements set by the institutions. There is also a test known as the National Medical Admission Test or NMAT. Scores are given on a percentile basis and a high ranking is a must to enter the top medical schools in the country.		In most institutions, medical education lasts for four years. Basic subjects are taken up in the first and second years, while clinical sciences are studied in the second and third years. In their fourth year, students rotate in the various hospital departments, spending up to two months each in the fields of internal medicine, surgery, obstetrics and gynecology, and pediatrics, and several weeks in the other specialties. After this, students graduate with a Doctorate in Medicine and apply for postgraduate internship (PGI) in an accredited hospital of their choice. After PGI, the student is eligible to take the Medical Licensure Examination. Passing the examinations confers the right to practice medicine as well as to apply in a Residency Training Program.		The medical education in the Republic of China (Taiwan) is usually 7 years (6-year learning plus 1-year internship) in duration, starting right after high schools. The first 2 years in the 7-year system is composed of basic sciences and liberal art courses. Doctor-patient classes are emphasized, and most schools require compulsory amounts of volunteer hours. Clinical sciences are compressed into a two-year program in the 3rd and 4th years. The duration of clerkships and internships varies from school to school, but all of them end at the 7th grade. Taiwan's medical education began in 1897 and is over 100 years old now. Students graduate with a Doctor of Medicine (MD) degree. Starting from the year 2013, incoming students will have a 6+2 year curriculum, in which the first 6 years are oriented similarly as before and the last two years are Post Graduate Years; this change aims to increase primary care capabilities of medical school graduates.[38]		In Saudi Arabia medical education is free for all Saudi citizens. A medical student must pass an entrance examination and complete a 1-year pre-medical course containing some basic medical subjects including: Biology, Organic Chemistry, Inorganic Chemistry, Physics, Medical Biostatistics, and English for medical uses. Passing this year is commonly considered as the most challenging. It offers an MBBS (Bachelor of Medicine, Bachelor of Surgery) degree. after one pre-medical course, five medical years and one training year. By 2010, there are 24 medical schools in KSA- 21 nonprofit and three private medical schools the last college opened was Sulaiman AlRajhi Colleges with its partnership with Maastricht in the Netherlands.		Currently, there are 41 medical schools in South Korea.[39] Medical programs in South Korea used to be direct-entry programs such as in the UK, taking six years to complete. However, most universities were going through a transition from direct-entry to a 4+4 year system, such as those found in the United States and Canada.[40] Recently, about half of the universities are converting back to six years direct-entry program by 2015, and almost all of the universities are converting it back by 2017.		There are eight medical schools in Sri Lanka that teach evidence based (sometimes called "western") medicine. The oldest medical school is the Faculty of Medicine, University of Colombo, established as Ceylon Medical School in 1870. There are medical faculties in Peradeniya, Kelaniya, Sri Jayawardanepura, Galle, Batticaloa, Jaffna and Rajarata as well.		Kelaniya Medical Faculty initially started as the North Colombo Medical College (NCMC), a private medical institution. It was one of the earliest private higher educational institutions (1980). Heavy resistance by the medical professionals, university students and other professionals led to its nationalization and to its renaming as the Kelaniya Medical Faculty.		Faculty of Health-Care Sciences is the faculty that offers MBBS together with other para-medical courses. It is an entity of the Eastern University - Sri Lanka.		The Open International University for Complementary Medicines(OIUCM), established under World Health Organization teaches various field of Medicines and related program of Environmental Sciences.		[41] [42] despite having basic problems of training programme.		Postgraduate Institute of Medicine (PGIM) is the only institution that provides specialist training of medical doctors.		The Institute of Indigenous Medicine of the University of Colombo, the Gampaha Wickramarachchi Ayurvedhic Medicine Institute of the University of Kelaniya and the Faculty of Siddha Medicine, University of Jaffna teach Ayurvedha/ Unani / Siddha Medicine.		The first medical school in Thailand has been established back in 1890 at Siriraj Hospital, which is now become Faculty of Medicine Siriraj Hospital, Mahidol University. At the current time, there are 22[43] medical programs offers nationwide. Most of the Thai medical schools are government-funded and require Thai citizenship for eligibility. Only one private medical school exists at the moment. Some Thais choose to attend the private medical school or attend a medical school in a foreign country due to relatively few openings and high college entrance examination scores required for enrollment in public medical schools.		The Thai medical education is 6 years system, consisting of 1 year in basic-science, 2 years in pre-clinical training, and 3 years for clinical training. Upon graduation, all medical students must pass national medical licensing examinations and a university-based comprehensive test. After medical school, newly graduated doctor are under contract to spend a year of internship and 2 years of tenure in rural areas before they are eligible for any other residency positions or specialized training.		The students will receive Doctor of Medicine (MD) degree. However the degree is equivalent to master's degree in Thailand.		There are four Medical Schools (Fakultete te Mjeksise) in Albania:		These medical schools are usually affiliated with regional hospitals. The course of study lasts 6 years. Students are conferred degree Doctor of Medicine (M.D.) upon graduation.		There are 4 Medical Schools (Medical Universities) in Belarus:		There are five Medical Schools (Medicinski Fakultet) in Bosnia and Herzegovina:		These medical schools are usually affiliated with regional hospitals.		The course of study lasts 6 years or 12 semesters. Students are conferred degree Doctor of Medicine (M.D.) upon graduation.		Entry to BH Medical Schools are very competitive due to limited places imposed by the government quota. Students are required to complete Secondary School Leaving Diploma (Gimnazija-Gymnasium (school) or Medicinska skola matura/svedocanstvo/svjedodzba).		Entrance examination is usually held in June/July. Combined score of Secondary School Diploma assessment (on scale 1-5, with 2 minimum passing grade and 5 maximum grade) and entrance examination is taken into consideration. Usually, 5 in Chemistry, Biology, Mathematics, and Physics are required for entry to medicine.		Course structure is more traditional and divided in pre-clinical (year 1-3) /clinical part (year 3-6) and subject-based.		Practical examinations are held throughout the degree (Anatomy, Biochemistry, Pathology, Physiology practicals etc.). Dissection is part of all medical curricula in Bosnian and Herz. Medical Schools.		In Bulgaria, a medical school is a type of college or a faculty of a university. The medium of instruction is officially in Bulgarian. A six- to one-year course in Bulgarian language is required prior to admittance to the medical program. For European candidates, an exam in Biology and Chemistry in Bulgarian is also required. While a number of Bulgarian medical schools have now started offering medical programmes in English, Bulgarian is still required during the clinical years.[44][45]		Students join medical school after completing high-school. Admission offers are made by individual medical schools. Bulgarian applicants have to pass entrance examinations in the subjects of Biology and Chemistry. The competitive result of every candidate is the based on their marks these exams plus their secondary-school certificate marks in the same subjects. Those applicants with the highest results achieved are classified for admission.		The course of study is offered as a six-year program. The first 2 years are pre-clinical, the next 3 years are clinical training and the sixth year is the internship year, during which students work under supervision at the hospitals. During the sixth year, students have to appear for 'state exams' in the 5 major subjects of Internal Medicine, Surgery, Gynaecology and Obstetrics, Social Medicine, and Pediatrics. Upon successful completion of the six years of study and the state exams the degree of 'Physician' is conferred.		For specialization, graduates have to appear for written tests and interviews to obtain a place in a specialization program. For specialization in general medicine, general practice lasts three years, cardiology lasts four years, internal medicine lasts five years, and general surgery lasts five years.		In Croatia, there are four out of seven universities that offer a medical degree, the University of Zagreb (offers medical studies in English), University of Rijeka, University of Split (also offers medical studies in English), and the University of Osijek. The Medical schools are a faculties of those four universities. Medical students enroll into medical school after finishing secondary education, typically after a Gymnasium, or after a four-year nursing school, or any other high school lasting four years. During the application process, their high school grades, and the grades of their matriculation exam at the end of high school (Matura) and the score at the obligatory admission exam are taken into account, and the best students are enrolled.		The course of study lasts 6 years or 12 semesters. During the first 3 years, students are engaged in pre-clinical courses (Anatomy, Histology, Chemistry, Physics, Cell Biology, Genetics, Physiology, Biochemistry, Immunology, Pathologic Physiology And Anatomy, Pharmacology, Microbiology, etc.). Contact with patients begins at the third year. The remaining 3 years are composed of rotations at various departments, such as Internal Medicine, Neurology, Radiology, Dermatology, Psychiatry, Surgery, Pediatrics, Gynecology and Obstetrics, Anesthesiology, and others. During each academic year, students also enroll into two or three elective courses. After each rotation, the students take a total of about 60 exams. In the end, the students must pass a final multiple-choice exam comprising questions about clinical courses, after which they finally gain an MD, and the title of Doctor of Medicine, which they put after their name. Now the doctors must complete a one-year, supervised, paid internship in a hospital of their choice, after which they take the state (license) examination, which is an eight-part oral examination containing the eight most important clinical branches. After that, the doctors are eligible to practice medicine as general practitioners. Residencies are offered at various hospitals throughout Croatia, and at numerous medical specialities.		Medical study in Czech Republic has a long tradition dating from the 14th century, with the first medical school starting at the First Faculty of Medicine, Charles University in Prague in 1348, making it the 11th oldest in the world and highly prestigious. Students from all over the world are attracted to study medicine in Czech Republic because of the high standards of education provided. Most Czech Universities offer a 6-year General Medicine program in Czech and in English separately for international students.		The admission to medical studies in Czech Republic is based on the performance in high school diploma (Biology, Chemistry and Physics), English proficiency and performance in the entrance exams. Entrance examination is conducted at the university and by some representative offices abroad. The entrance exams are competitive due to students from all over the world fighting to secure a place. After the entrance exams, successful candidates are further scrutinised by conducting interviews.		Most of the international students studying medicine in the Czech Republic originate from USA, Canada, UK, Norway, Sweden, Germany, Israel, Malaysia and the Middle East.		Most faculties of Medicine in Czech Republic have been approved by the U.S. Department of Education for participation in Federal Student Financial Aid Programs and is listed in the Directory of Postsecondary Institutions published by the U.S. Department of Education. The qualifications are also approved in Canada by the Canadian Ministry of Education and Training, and in the UK by the General Medical Council. Most medical schools are globally recognised and carry a good reputation.		There are nine public government owned medical schools in the Czech Republic:		There is one military medical school, Faculty of Military Health Sciences, University of Defence.		In Denmark, basic medical education is given in four universities: University of Copenhagen, Aarhus University, University of Southern Denmark and Aalborg University. The duration of basic medical education is six years and the course leads to the degree of Candidate of Medicine (M.D.) after swearing the Hippocratic Oath upon graduation.[46]		Medical school is usually followed by a year residency called clinical basic education (Danish: Klinisk basisuddannelse or just KBU) which upon completion grants the right to practices medicine without supervision.		In Finland, basic medical education is given in five universities: Helsinki, Kuopio, Oulu, Tampere and Turku. Admission is regulated by an entrance examination. Studies involve an initial two-year preclinical period of mainly theoretical courses in anatomy, biochemistry, pharmacology etc. However, students have contact with patients from the beginning of their studies. The preclinical period is followed by a four-year clinical period, when students participate in the work of various hospitals and health care centres, learning necessary medical skills. Some Finnish universities have integrated clinical and preclinical subjects along the six-year course, diverging from the traditional program. A problem-based learning method is widely used, and inclusion of clinical cases in various courses and preclinical subjects is becoming common. All medical schools have research programs for students who wish to undertake scientific work. The duration of basic medical education is six years and the course leads to the degree of Licentiate of Medicine.		Medical studies in France are organized as follow:		Right after graduating from High School with a Baccalaureat, any student can register at a university of medicine (there are about 30 of them throughout the country). At the end of first year, an internal ranking examination takes place in each of these universities in order to implement the numerus clausus. First year consists mainly of theoretical classes such as biophysics and biochemistry, anatomy, ethics or histology. Passing first year is commonly considered as challenging and requires hard and continuous work. Each student can only try twice. For example, the Université René Descartes welcomes about 2000 students in first year and only 300 after numerus clausus.		The second and third year are usually mainly quite theoretical although the teachings are often accompanied by placements in the field (e.g. internships as nurses or in the emergency room, depending on the university).		During 4th, 5th and 6th years, medical students get a special status called 'Externe' (In some universities, such as Pierre et Marie Curie, the 'Externe' status is given starting in the 3rd year). They work as interns every morning at the hospital plus a few night shifts a month and study in the afternoon. Each internship lasts between 3 and 4 months and takes place in a different department. Med students get 5 weeks off a year.		At the end of sixth year, they need to pass a national ranking exam, which will determine their specialty. Indeed, the first student gets to choose first, then the second, etcetera. Usually students work pretty hard during 5th and 6th years in order to train properly for the national ranking exam. During these years, actual practice at the hospital and some theoretical courses are meant to balance the training. Such externs' average wage stands between 100 and 300 euros a month.		After that ranking exams, students can start as residents in the specialty they have been able to pick. That is the point from which they also start getting paid.		Towards the end of the medical program, French medical students are provided with more responsibilities and are required to defend a thesis. At the conclusion of the thesis defense, French medical students receive a State Diploma of Doctor of Medicine (MD) or "Diplôme d'Etat de Doctorat en Medecine for general medicine. For those who are in speciality training will also receive a Diploma of Specialized Studies (DES= Diplôme d'Etudes Specialisees) to mark their specialties. Some students may also receive a Diploma of Specialized Complementary Studies (DESC)= Diplôme d'Etudes Specialisees Complementaires.[47]		In Germany, admission to medical schools is currently administered jointly by the Stiftung für Hochschulzulassung (SfH), a centralized federal organization, and the universities themselves. The most important criterion for admission is the Numerus clausus, the final GPA scored by the applicant on the Abitur (highest secondary school diploma). However, in light of the recent gain in influence of medical schools in regards to applicant selection, additional criteria are being used to select students for admission. These criteria vary among medical faculties and the final Abitur GPA is always a core indicator and strongly influences admission. Admission remains highly competitive. A very small number of slots per semester are reserved for selected applicants which already hold a university degree (Zweitstudium) and for medical officer candidates (Sanitätsoffizieranwärter).		The first two years of medical school consist of the so-called pre-clinical classes. During this time, the students are instructed in the basic sciences (e.g. physics, chemistry, biology, anatomy, physiology, biochemistry, etc.) and must pass a federal medical exam (Erster Abschnitt der ärztlichen Prüfung), administered nationally. Upon completion, the students advance to the clinical stage, where they receive three years of training and education in the clinical subjects (e.g., internal medicine, surgery, obstetrics and gynecology, pediatrics, pharmacology, pathology, etc.). After these three years, they have to pass the second federal medical exam (Zweiter Abschnitt der ärztlichen Prüfung) before continuing with the sixth and final year. The last year of medical school consists of the so-called "practical year" (Praktisches Jahr, PJ). Students are required to spend three four-month clerkships, two of them in a hospital (internal medicine and surgery) as well as one elective, which can be one of the other clinical subjects (e. g. family medicine, anesthesiology, neurology, pediatrics, radiology etc.).		After at least six years of medical school, the students graduate with a final federal medical exam (Dritter Abschnitt der ärztlichen Prüfung). Graduates receive the license to practice medicine or dentistry and the professional title of physician (Arzt) or dentist (Zahnarzt). The academic degrees Doctor of Medicine (Dr. med.) and Doctor of dental Medicine (Dr. med. dent.) are awarded if the graduate has, in addition, successfully completed a scientific study and dissertation. It is a doctoral degree and therefore different from the MD or DDS degrees in the U.S., which as professional degrees are awarded after passing the final exams and do not require additional scientific work. Many medical students opt to perform their thesis during their studies at medical school, but only a fraction of them is able to finish the dissertation-process during their studies. The requirements for getting a Dr. med. degree across the board are not as hard as for the doctor in natural science (Dr. rer. nat.). Therefore, many critics advocate to adopt a system similar to that of the Anglo-Saxon countries with an MD as a professional degree and a PhD showing additional scientific qualification. If physicians wish to open up a doctor's office, they are required to further complete residency in order to fulfill the federal requirements of becoming Facharzt (specialized in a certain field of medicine like internal medicine, surgery, pediatrics etc.). Oral and maxillofacial surgeons must complete both studies, medicine and dentistry, then afterwards specializing another 5 years.		There are 36 medical faculties in Germany.		There are seven medical schools in Greece. The most prominent one of them is the University of Athens Medical School. The rest of them are in Patras, Thessaloniki, Ioannina, Larissa, Heraklion, and Alexandroupoli. The duration of the studies in Greece is 6 years.		Hungary has four medical schools, in Budapest, Debrecen, Pécs and Szeged. Medical school takes six years to complete, of which the last year is a practical year. Students receive the degree dr. med. univ. or dr. for short, equivalent to the M.D. degree upon graduation. All Hungarian medical schools have programs fully taught in English.		In Iceland, admission to medical school requires passing an organized test, controlled by the University of Iceland, which anyone with a gymnasium degree can take. Only the top 48 scores on the exam are granted admission each year. Medical school in Iceland takes 6 years to complete. Students receive a cand.med. degree upon graduation. Following this, Icelandic regulations require 12 months of clinical internship before granting a full medical license.[48] This internship consists of internal medicine (4 months), surgery (2 months), family medicine (3 months) and a three-month elective period. Upon receiving a license to practice, a physician can start specialist training, in Iceland or abroad.[49][50]		There are six medical schools in Ireland. They are at Trinity College Dublin, the Royal College of Surgeons in Ireland, University College Dublin, University College Cork, University of Limerick and the National University of Ireland, Galway (the National University of Ireland is the degree-awarding institution for all except the University of Limerick and Trinity College). Training lasts four, five or six years, with the last two years in the affiliated teaching hospitals (UCD - St. Vincents University Hospital, Mater Misericordiae University Hospital) (Trinity - St. James's Hospital, Adelaide and Meath Hospital incorporating the National Children's Hospital) (UCC - Cork University Hospital) (RCSI - Beaumont Hospital, Connolly Hospital, Waterford Regional Hospital). For Programmes that are six years in length, entry is based on secondary school qualifications. Programmes that are four years in length require previous university degrees. The Royal College of Surgeons in Ireland and the University of Limerick were the first medical institutions to offer Graduate Entry Medicine of four years in duration in the Ireland. This is now also offered in University College Dublin and University College Cork. The National University of Ireland, Galway also launched a graduate entry programme in 2010.		Medical education is regulated by the Irish Medical Council, the statutory body that is also responsible for maintaining a register of medical practitioners. After graduation with the degrees of BM BS (Bachelor of Medicine and Bachelor of Surgery) or MB BCh BAO (Medicinae Baccalaureus, Baccalaureus in Chirurgia, Baccalaureus in Arte Obstetricia), a doctor is required to spend one year as an intern under supervision before full registration is permitted. Graduates of the Royal College of Surgeons in Ireland also receive the traditional "Licenciate of the Royal Colleges of Surgeons and Physicians in Ireland" (LRCP&SI), which was awarded before the Royal College of Surgeons in Ireland became an Affiliate of the National University of Ireland and thus was allowed grant degrees, under the Medical Practitioners Act (1978).		In Italy, the contents of the medical school admission test is decided each year by the Ministry of Education, Universities and Research (MIUR) and consists of eighty questions divided in five categories: logics and "general education" ("cultura generale"), mathematics, physics, chemistry, and biology. Results are expressed in a national ranking.		As a general rule, all state-run medical schools in the country administer it on the same day, whereas all privately run medical schools administer it on another day, so that a candidate may take the test once for state-run schools and once for a private school of his or her choice, but no more.		Some universities in Italy provide an international degree course in medicine taught entirely in English for both Italian and non-Italian students. A number of these medical schools are at public universities, and have relatively low tuition fees compared to the English-speaking world, because the cost of the medical education is subsidized by the state for both Italian and non-Italian students. These public medical schools include the International Medical School at the University of Milan, the University of Pavia, Rome "La Sapienza", Rome "Tor Vergata", Naples Federico II, the Second University of Naples, and the University of Bari. These universities require applicants to rank highly on the International Medical Admissions Test. Italy also has private or parochial, more expensive English-language medical schools such as Vita-Salute San Raffaele University and Humanitas University in Milan, and at the Università Cattolica del Sacro Cuore Rome campus.		Medicine is one of the university faculties implementing numerus clausus ("numero chiuso"): the overall number of medical students admitted every year is constant, as each medical school is assigned a maximum number of new admission per year by MIUR.		Medical school lasts 6 years (12 semesters). Traditionally, the first three years are devoted to "biological" subjects (physics, chemistry, biology, biochemistry, genetics, anatomy, physiology, immunology, pathophysiology, microbiology, and usually English language courses), whereas the later three years are devoted to "clinical" subjects. However, most schools are increasingly devoting the second semester of the third year to clinical subjects and earlier patient contact. In most schools, there are about 36 exams over the 6-year cycle, as well as a number of compulsory rotations and elective activities.		At the end of the cycle, students have to discuss a final thesis before a board of professors; the subject of this thesis may be a review of academic literature or an experimental work, and usually takes more than a year to complete, with most students beginning an internato (internship) in the subject of their choice in their fifth or sixth year. The title awarded at the end of the discussion ceremony is that of "Dottore Magistrale", styled in English as a Doctor of Medicine, which in accordance with the Bologna process is comparable with a master's degree qualification or a US MD.		After graduating, new doctors must complete a three-month, unpaid, supervised tirocinio post-lauream ("post-degree placement") consisting of two months in their university hospital (one month in a medical service and one in a surgical service) as well as one month shadowing a general practitioner. After getting a statement of successful completion of each month from their supervisors, new doctors take the esame di stato ("state exame") to obtain full license to practise medicine. They will then have to register with one of the branches of the Ordine dei Medici ("Order of Physicians"), which are based in each of the Provinces of Italy.		Registration makes new doctors legally able to practice medicine without supervision. They will then have to choose between various career paths, each usually requiring a specific admission exam: most either choose to train as general practitioner (a 3-year course run by each Region, including both general practice and rotation at non-university hospitals), or to enter a Scuola di Specializzazione ("specialty school") at a university hospital 5-year or 6-year course.		Lithuania has two medical schools, in Kaunas - LSMU http://lsmuni.lt/ and Vilnius. Studies are of six years, of which the last year is a practical year. All Lithuanian medical schools have ams in English. Since 1990, LSMU has been the Alma Mater of many international students and 550 full-time foreign students from 42 countries (mainly Israel, Germany, Finland, Norway, Spain, Sweden, Lebanon, Poland, India, South Korea, Ireland and the United Kingdom) are currently enrolled here.		The Lithuanian University of Health Sciences (LSMU) is the largest university-type school in Lithuania preparing the health specialists. It unites the Veterinary Academy and the Medical Academy. LSMU traditions of studies and scientific work go back to the times of the Faculty of Medicine at Vytautas Magnus Universitythat was later turned into Kaunas Institute of Medicine.		LSMU collaborates with more than 140 European, American and Asian universities for study and research purposes.		The university is a member of numerous international organizations, such as the European University Association (EUA), Association of Schools of Public Health in The European Region (ASPHER), Association of Medical Schools in Europe (AMSE), Association for Medical Education in Europe (AMEE), Organisation for PhD Education in Biomedicine and Health Sciences in the European System (ORPHEUS), European Association of Establishments for Veterinary Education (EAEVE), World Veterinary Association, and more.		LSMU is also a member of the World Health Organization (WHO), where it fulfils the role of a collaboration centre for research and training in epidemiology, as well as for the prevention of cardiovascular and other chronic non-communicable diseases.		The study programmes at LSMU meet university education standards applied in EU countries.		In the Netherlands and Belgium, medical students receive 6 years of university education prior to their graduation.		In the Netherlands, students used to receive four years of preclinical training, followed by two years of clinical training (co-assistentschappen, or co-schappen for short) in hospitals. However, for a number of medical schools this has recently changed to three years of preclinical training, followed by three years of clinical training. At least one medical faculty, that of the Utrecht University, clinical training already begins in the third year of medical school. After 6 years, students graduate as basisartsen (comparable to Doctors of Medicine). As a result of the Bologna process, medical students in the Netherlands now receive a bachelor's degree after three years in medical school and a master's degree upon graduation. Prospective students can apply for medical education directly after finishing the highest level of secondary school, vwo; previous undergraduate education is not a precondition for admittance.		The Belgian medical education is much more based on theoretical knowledge than the Dutch system. In the first 3 years, which are very theoretical and lead to a university bachelor degree, general scientific courses are taken such as chemistry, biophysics, physiology, biostatistics, anatomy, virology, etc. To enter the bachelor course in Flanders, prospective students have to pass an exam, as a result of the numerus clausus. In the French-speaking part of Belgium, only the best students that pass the first year of the bachelor course in medicine are admitted to the second and third year.		After the bachelor courses, students are allowed to enter the 'master in medicine' courses, which consist of 4 years of theoretical and clinical study. In general, the first 2 master years are very theoretical and teach the students in human pathology, diseases, pharmacology. The third year is a year full of internships in a wide range of specialities in different clinics. The seventh, final year serves as a kind of 'pre-specialization' year in which the students are specifically trained in the specialty they wish to pursue after medical school. This contrasts with the Dutch approach, in which graduates are literally 'basic doctors' (basisartsen) who have yet to decide on a specialty.		Medical education in Norway begins with a six- to six-and-a-half-year undergraduate university program. Admission requires a very high GPA from secondary school - medicine consistently ranks as the most difficult university programme to be admitted to in Norway. Furthermore, certain high school subjects are required for admission (chemistry, mathematics and physics). Upon completion, students are awarded a candidatus/candidata medicinae (cand. med.) degree (corresponding to e.g. and MD in the USA) and medical license. Those completing a research programme (Forskerlinje) get this added to their degree. Following this, it is required a minimum of 18 months of internship (turnustjeneste) before applying on a specialist training in Norway. The internship consist of 6 months of internal medicine, 6 months of surgery and 6 months family medicine. There are currently 43 recognized medical specialties in Norway.		In Romania, medical school is a department of a medical university, which typically includes Dentistry and Pharmacy departments as well. The name facultate is used for departments in their universities too, but the Medicine departments distinguish themselves by the length of studies (6 years), which grants to graduates a status equivalent to that of a Master in Science. The Medicine departments are also marked by reduced flexibility - in theory, a student in a regular university can take courses from different departments, like Chemistry and Geography (although it usually does not happen, majors being clearly defined), while the medical universities do not have any extra offers for their students, due to their specialization. Admission to medical faculty is usually awarded by passing a Human Biology, Organic Chemistry and/or Physics test. The program lasts 6 years, with first 2 years being preclinical and last 4 years being mostly clinical. After these six years, one has to take the national licence exam (which consists of mostly clinically oriented questions, but some questions also deal with basic sciences) and has to write a thesis in any field he/she studied. Final award is Doctor-Medic (titlu onorific) (shortened Dr.), which is not an academic degree (similar to Germany). All graduates have to go through residency and specialization exams after that in order to practice, although older graduates had different requirements and training (e.g., clinical rotations similar to sub-internship) and might still be able to practice Family Medicine / General Medicine.		Medical schools in Russia offer a 6-year curriculum leading to award Doctor of Medicine (MD) "Physician". Russian medical authorities reluctantly agrees with inclusion in list of international medical schools FAIMER-IMED. FAIMER can't include medical schools without cooperation from Russia. For example, Orel State University Medical Institute isn't included in this list.		Medical education in Sweden begins with a five-and-a-half-year undergraduate university program leading to the degree "Master of Science in Medicine" (Swedish: Läkarexamen). Following this, the National Board of Health and Welfare requires a minimum of 18 months of clinical internship (Swedish: Allmäntjänstgöring) before granting a medical license to be fully qualified as Medical Doctor (MD).[51]		This internship consists of surgery (3–6 months), internal medicine (3–6 months), psychiatry (three months) and family medicine (six months). Upon receiving a license to practice, a physician is able to apply for a post to start specialist training. There are currently 52 recognized medical specialties in Sweden. The specialist training has a duration of minimum five years, which upon completion grants formal qualification as a specialist.		There are five universities granting medical degrees in Switzerland (plus the University of Fribourg that provides the bachelor but not the master in medicine) and five university hospitals:		All high school graduates who wish to pursue further education are required to take an MCQ exam. The exam covers most of the high school and secondary school curricula.		A student who scores high enough gets a place in a faculty of his/her desire. Entrance to medical schools is extremely competitive, only very top scoring students are accepted to medical schools.		Medical education takes six years, first three years being Pre-clinical years and the latter three being Clinical years. Right after graduation, graduates can either work as GPs or take another exam called TUS (Medical Specialization Examination) to do residency in a particular department of a particular hospital.		Most of the medical schools in Turkey are state schools but the number of private schools is on the rise. MCQ exam (YGS and LYS) scores required to be accepted to private medical schools are lower compared to their public counterparts. The language of instruction is, in general, Turkish, but few universities also offer schools with English as the language of instruction. This makes Turkey a popular place to study medicine for students from nearby areas like the Balkans, the Middle East, and to a lesser extent North Africa.		Medical degrees in Ukraine were offered only in institutions called medical universities, which are separate from traditional universities. However, some medical schools are now associated with classical universities. These include:		Due to the UK code for higher education, first degrees in medicine comprise an integrated programme of study and professional practice spanning several levels. While the final outcomes of the qualifications themselves typically meet the Expectations of the descriptor for higher education qualification at level 7 (the UK master's degree). These degrees may retain, for historical reasons, "Bachelor of Medicine, Bachelor of Surgery" and are abbreviated to MBChB or MBBS.[53]		There are currently 32 institutions that offer medical degrees in the United Kingdom.[54] Completion of a medical degree in the UK results in the award of the degrees of Bachelor of Medicine and Bachelor of Surgery. Admission requirements to the schools varies; most insist on solid A-Levels/Highers, a good performance in an aptitude test such as the UKCAT, the BMAT or the GAMSAT, and usually an interview. As of 2008 the UK has approximately 8000 places for medical students.[55]		Methods of education range from courses that offer a problem-based learning approach (alongside lectures etc.), and others having a more traditional pre-clinical/clinical structure. Others combine several approaches in an integrated approach.		Following qualification, UK doctors enter a generalised two-year, competency-based "foundation programme", gaining full GMC (General Medical Council) registration at the end of foundation year one, and applying for specialist training (in medicine, surgery, general practice etc.) after foundation year two.		Many medical schools offer intercalated degree programmes to allow students to focus on an area of research outside their medical degree for a year.		Some medical schools offer graduate entry programmes, which are four years long. The name refers to the fact that students on these courses already have a degree in another subject (i.e. they are graduates). Due to the shorter length of the course, the timetable of these degrees are more intense and the holidays are shorter, compared to students on the 5-year course. In terms of entrance requirements, the 4-year degree restricts entry to those who already hold a first degree, and have previously worked in an area of healthcare. The first degree doesn't necessarily have to be a BSc degree (this is the criteria for some of the medical schools), whereas other medical schools specify that the prior degree has to be in a science subject. Competition for this course is fierce, with students having to also sit an entrance exam prior to being considered for an interview.		Medical schools typically admit more students into undergraduate programmes than into graduate entry programmes.		A person accepted into a medical school and enrolled in an educational program in medicine, with the goal of becoming a medical doctor, is referred to as a medical student. Medical students are generally considered to be at the earliest stage of the medical career pathway. In some locations they are required to be registered with a government body.		Medical students typically engage in both basic science and practical clinical coursework during their tenure in medical school.[56] Course structure and length vary greatly among countries (see above).		Upon completion of medical school in the United States, students transition into residency programs through the National Resident Match Program (NRMP). Each year, approximately 16,000 US medical school students participate in the residency match. An additional 18,000 independent applicants—former graduates of U.S. medical schools, U.S. osteopathic medical schools, U.S. podiatry students, Canadian students, and graduates of foreign medical schools—compete for the approximately 25,000 available residency positions.[57]		Medical students, perhaps being vulnerable because of their relatively low status in health care settings, commonly experience verbal abuse, humiliation and harassment (nonsexual or sexual). Discrimination based on gender and race is less common.[58]		A meta-analysis of the American JAMA magazine suggested depressive symptoms in 24% to 29% of all medical students and 25% to 33% of all resident physicians.[59][60] Burnout in medical students, in addition, seems to be associated with increased likelihood of subsequent suicidal ideation.[61]		It has been estimated by a US study that approximately 14% of medical students have symptoms of moderate to severe depression, and roughly 5% have suicidal thoughts at some point during training.[62] Internationally depression as well as distress in medical school is widely studied and gained more attention over the years. A recent study among German medical students at international universities displayed the significantly higher risk of depression symptoms being 2.4 times higher than the average population. 23.5% of these German medical students showed clinically relevant depressive symptoms.[63] In a South Korean study, 40% of medical students appeared to have depression.[64] Medical students with more severe depression also may be less likely to seek treatment, largely from fear that faculty members would view them as being unable to handle their responsibilities.[62] Students who feel that they lack a social support system are 10 times more likely to be depressed compared with students that consider themselves to have good social support.[64]		Approximately 10% experience suicidal ideation during medical school.[61]		Lemon and Stone hypothesised in what has become termed the 'Lemon Stone Hypothesis', that medical students from lower socioeconomic backgrounds increase in prevalence during times of national economic adversity. Their hypothesis was a formulation of Becker Maimans' health belief model and Adaption theory. This hypothesis has to some extent been supported by a series of surveys.[65]		
A students' union, student government, free student union, student senate, students' association, guild of students, or government of student body is a student organization present in many colleges, universities, and high schools. In higher education, the students' union is often accorded its own building on the campus, dedicated to social, organizational activities, representation, and academic support of the membership.		In the United States, student union often only refers to a physical building owned by the university with the purpose of providing services for students without a governing body. This building is also referred to as a student activity center, although the Association of College Unions International (largely US-based) has hundreds of campus organizational members. Outside the US, student union and students' union refer to a representative body, as distinct from a student activity centre.						Depending on the country, the purpose, assembly, method, and implementation of the group might vary. Universally, the purpose of students' union or student government is to represent fellow students in some fashion.		In some cases, students' unions are run by students, independent of the educational facility. The purpose of these organizations is to represent students both within the institution and externally, including on local and national issues. Students' unions are also responsible for providing a variety of services to students. Depending on the organization's makeup, students can get involved in the union by becoming active in a committee, by attending councils and general meetings, or by becoming an elected officer.		Some students' unions are politicized bodies, and often serve as a training ground for aspiring politicians. Students' unions generally have similar aims irrespective of the extent of politicization, usually focusing on providing students with facilities, support, and services.		Some students' unions often officially recognize and allocate an annual budget to other organizations on campus. In some institutions, postgraduate students are within the general students' unions, whereas in others they have their own postgraduate representative body. In some cases, graduate students lack formal representation in student government.		As mentioned before universally the purpose of students' union or student government is to represent fellow students. Many times student's unions usually focusing on providing students with facilities, support, and services. Simple variations on just the name include the name differences between the United States (student government) and other countries (student's union). Depending on the country there are different methods of representation compulsory education to Higher education or tertiary.		In Australia, all universities have one or more student organizations.		Australian student unions typically provide such services as eateries, small retail outlets (e.g., news agencies), student media (e.g., campus newspapers), advocacy, and support for a variety of social, arts, political, recreational, special interest and sporting clubs and societies. Most also operate specialized support services for female, LGBT, international and indigenous students. Many expressed concerns over the introduction of voluntary student unionism (VSU) in 2006.[1]		In 2011, the Government passed legislation to allow universities to charge students a compulsory service fee to fund amenities such as sporting facilities, childcare and counselling, as well as student media and "advocating students’ interests".[2] The legislation passed after the Greens took the balance of power in the senate.[11]		The National Union of Students of Australia represents most undergraduate students' unions at a national level; the Council of Australian Postgraduate Associations is the umbrella organisation for postgraduate students' unions.		Azerbaijan Students Union (ASU) was established by students from Baku on 15 September 2008. ASU is an organization which was established on basis of international experience and it was the first student organization which united students irrespective of gender, race, creed, nationality.		During its action period ASU has formed stable structure, presented new suggestions about student policy to appropriate bodies, made close relations with international and regional student organizations, prepared new action plan according to the universities-students-companies' relations in Azerbaijan.		ASU considered international relations very important. For the first time ASU's delegates were participants of the First Asia IAESTE Forum in Shanghai during 12–15 November 2009. After that forum ASU established close relations with IAESTE which is one of the biggest student exchange organizations. As a result of relations on 21 January 2010 ASU was accepted a member of IAESTE. Our union gained right to represent Azerbaijan students in IAESTE. That membership was the union's first success on international level. During 20–27 January Azerbaijan Students Union was accepted as associative member of IAESTE in 64th Annual Conference in Thailand. Also Azerbaijan Students Union is a full member of European Students' Union.		In China, the student body is usually referred to as 学生会 (pinyin: Xuéshēng Hùi; literally: "student union") or 学生联合会 (pinyin: Xuéshēng Liánhé Hùi; literally: "student league").		Membership in different universities has different functions. Some universities may give the membership a task of recording the students' attendance and the complex grades. Student associations of Chinese universities are mostly under the leadership of Communist Youth League of China, which to a large extent limit its function as an organization purely belonging to students themselves.		All universities in Hong Kong have students' unions. Most of these students' unions are members of the Hong Kong Federation of Students.[3] Many secondary schools also have students' unions or the equivalent.		India has developed a complex tradition of student politics dating from the era of Congress Party domination. Student unions are organised both within universities, like the Student Council of IISc and across universities, but affiliated with political parties, as in the case of Students Federation of India (SFI), National Students Union of India (NSUI) etc who compete in elections to control posts in universities and colleges. Examples of activist unions include the Delhi University Students Union. Recently few school administrations had also started including the student government system as co-curricular activities in one form or another.		Indian National Students Organization, Students Federation of India , National Students Union of India, Bharatiya Janata Yuva Morcha, Students Federation of India, All India Democratic Students' Organisation,Akhil Bhartiya Vidyarthi Parishad, All India Students Association, All India Students Federation, Muslim Students Federation are major Student's Organisations in India.[		In Indonesia, every university, college and higher education school has a student union. The student body in high school, which is called Organisasi Siswa Intra Sekolah (Student Organization Inside School), abbreviated OSIS, is the official student organization formed by the government. A general election to choose the leader is usually held every year. OSIS organizes the school's extracurricular activities such as music shows and art gallery (pentas seni/pensi).		In Japan, the student body is called 学生自治会 (gakusei-jichi-kai). In Japanese, the word 学生自治会 (gakusei-jichi-kai) means students' self-government-organizations. The student body in Japan promotes extracurricular activities. Usually, a cultural association, 文化会 (bunka-kai), and a sports association, 体育会 (taiiku-kai), are included within a student body as autonomous organizations. A student belongs to one or more students' organizations, and he or she does extracurricular activities through these students' organizations. However, the extracurricular activities of universities and colleges have been declining since the 1990s.		Malaysia has 20 public universities. Each of them has one Students' Representative Council (Malay: Majlis Perwakilan Pelajar, MPP), the highest student body of such university.		A general election is held every year, usually in September, to elect representatives to MPP. The percentage of voter turn-outs are usually high (70 to 95 percent) largely due to enforcements from the universities' management which, at the same time, acts as the Election Committee.		Every year, the Malaysian Ministry of Higher Education would set meetings and arrange programmes with all MPPs. Nevertheless, each MPP has their own autonomous right to govern their own membership. The size of MPP differs from each university, from as little as 12 to as many as 50. All MPP members are part-time and unpaid officers.		In 2011, Universiti Sains Malaysia established Students' Consultative Assembly (Malay: Dewan Perundingan Pelajar, DPP), the first student parliament established and the oldest of its kind in Malaysia, to involve participation of more student leaders in decision and policy making as well as to establish a legislative branch in its Students' Union system instead of having the only executive branch. A university student parliament is composed of MPP members and other elected or appointed student leaders representing their respective student body. As at May 2016, 7 public universities in Malaysia had their student parliament established.		Students associations have a strong history in New Zealand of involvement in political causes, notably the Halt All Racist Tours campaign during the 1981 Springbok Tour. All universities, and most polytechnics and colleges of education have a students association. Since the economic reforms of the 1990s and the introduction of user pays in tertiary education, students associations and the national body have shifted their focus to challenging inequities in the student loan scheme and high levels of student debt.[citation needed] Part-time work alongside the introduction of internal assessment and the change of semester structure has been attributed to the declining involvement in extracurricular activities and a shift in focus of the student movement from mass protest to lobbying.[citation needed]		Previous to 1998 membership of Students' Associations (pep) was compulsory at all public Tertiary Education providers (universities, polytechnics and colleges of education). In 1997 the centre-right National party proposed the Voluntary Student Membership amendment to the Education act which would have made membership of Students' Associations voluntary at all Tertiary Education Providers.		However the National Party relied on support from the centrist New Zealand First party to pass legislation. The New Zealand First party preferred that Tertiary Students themselves choose whether their provider should be voluntary or compulsory and pushed through a compromise to the amendment that allowed for a Compulsory Vs Voluntary referendum to be held at every public Tertiary Education Provider. The amendment also allowed for subsequent referendums which could not be held until at least two years had passed since the previous referendum and only if a petition was signed by 10% of the student populace.[citation needed]		The first wave of referendums were held in 1999, in which several Polytechnics and two Universities (the University of Waikato and the University of Auckland) elected to become voluntary.[citation needed] In 2002 a second referendum was held at the University of Waikato and students choose to return to compulsory student membership.[citation needed] Similar referendums at Auckland University in 2001, 2003 and 2005 have all elected to retain voluntary student membership.[citation needed]		Most of New Zealand Tertiary students' associations are confederated under the New Zealand Union of Students' Associations.		The Philippines has a complex student union. The usual names used are "student government", "student council", "student body" The Student Government Program of the elementary and secondary schools is handled by the Department of Education while the student councils in state universities and colleges is under the Commission on Higher Education. As well, Every public and private elementary and secondary schools under the Department of Education (DepED) has its own student governments. At the Tertiary Level, also known as higher education or "university" every private and state college and universities in the Philippines has its own student councils. In a university, there is a university student council and every college has its college student council. If a university has external campus(es), the external campuses has its own student council. The student councils within the university (main campus and external campuses) has the student regent, which would represent studentry of the whole university. One example of that is of the West Visayas State University and Central Philippine University.		In Sri Lanka, each state university has several Students' unions with formal links to respective faculties.Inter University Students' Federation is the umbrella organization of 14 unions of university students. However, most of these have political affiliations and function as proxies of these political factions. Many unions take an active political role within the university and in the country as a whole. This frequently lead to much clashes between rival students' unions or the authorities.		The role of students' unions in Myanmar were attached with Politics. From 1902, the students' union were outcoming and tried to get the Interdependence of Nation. There were many students boycotts in Myanmar:1920, 1936, 1962, 1974-75-76, 1988, 1996, 2007 and recently 2015 March. Most of them were connected with political issues. The Students' union in Myanmar tried to initiate to get the Interdependence of Burma. They always stood together with the Public difficulties. Now, U Thein Sein Government approved the National Education Law. So, the followed Technical Laws and attached laws are now wishing to get approved. The role of students' union is the greatest power in Myanmar or Burma according to History.		In South Africa student representative councils are the executive and plenary body of student governance and charters and provides most of the funding for other student groups, and represents students' interests when dealing with the administration. In several instances representatives of these bodies are members of the university's Senate.		In 2012 the first student union, after 42 years of suppression, was founded . Students from Al Mimona Ebem Alharth school public school in Tripoli-Libya successfully established the union and promoted for other schools around the country to do the same . The founder Alaa Amed received local media attention for this achievement .		While higher education and student activism might vary depending on the country the National Association of Nigerian Students' (NANS) is an organization with well over 50 university union across the nation.		Tunisia has a lot of students unions, we can see Organization of RCD students (Of the party Democratic Constitutional Rally but It was dissolved in 2011 after the Tunisian revolution), but the largest two poles for students in the country are Union générale des étudiants de Tunisie (UGET) since 1952 and Union générale tunisienne des étudiants (UGTE) since 1985.		While each student's union varies by countries the European Students' Union is an umbrella organization of 45 national unions of students from 36 European countries.		In Denmark the higher education system comprises two parallel sectors: universities and university colleges of applied sciences (e.g. nursing and engineering schools or teachers' colleges). Universities are characterised by scientific research and the highest education based thereon. University colleges of applied sciences are oriented towards working life and base their operations on the high vocational skill requirements set by it. These vocational institutions offer 3-4 year 'professional bachelor's degrees'. Besides that there are a number of art schools. Universities belong under the Ministry of Science, University Colleges belong under the Ministry of Education, and the Art Schools belong under the Ministry of Culture. There are 12 universities at the moment, but in 2006 there is a major merger process going on to make fewer, bigger institutions. The student unions at universities (and some of the art schools' student unions) are generally members of the National Union of Students in Denmark which represents these students on the national level.		Every university has a student union (In Danish, Studenterråd). Membership is not mandatory. The student unions are funded by the university and the Ministry of Science on the basis of the percentage of votes received every year at the university election. The student union is autonomous, its internal life organized by its by-laws. The student unions are responsible for all representation of the students and elect the student members of different administrative organs. They usually coordinate and finance the activities of smaller, more specialized student organizations. For the financing of their activities, some student unions exact a membership fee and/or engage in different businesses.		In the Ministry of Culture institutions there are also local student organisations. In the Ministry of Education institutions, The student activities are very much related to a student division of the Labor Union in the different areas. For instance, the teachers' students are organised in the national labor union for teachers and so forth.		The Upper-Secondary schools (In Danish, Gymnasier), It is in Denmark a law that there has to be a studentscouncill at the Upper-Secondary schools. The studentscouncils are organized in the following organisations:		In Finland the higher education system comprises two parallel sectors: universities and universities of applied sciences (polytechnics). Universities are characterized by scientific research and the highest education based thereon. Universities of applied sciences are oriented towards working life and base their operations on the high vocational skill requirements set by it.		The central organization of the French higher education system means that local university bodies have restricted decision-making power. As a consequence, student unions are generally established at national level with local sections in most universities. The largest national student unions have a strong political identity and their actions are generally restricted to the defense of their vision of higher education rather than being focused on the particular interests of the membership of a single university. Union membership is regarded as an essentially political decision, without any particular advantage for students. The strength of unions is often measured by their effectiveness in national protests rather than by membership figures. The most important student union in France is the left-leaning National Union of Students of France (UNEF).		There are also class-struggle student unions such as the Solidaires Étudiant-e-s (formed in January 2013 through the fusion of the former organizations SUD Étudiant and Féderation syndicale étudiante (FSE)) which refuse to cooperate with the universities' direction and work to organize students.		In the Grandes écoles, the premium league in the French higher education system, students are generally members of the official Student Office (Bureau des étudiants or Bureau des élèves) in charge of the organization of social activities and sports events. The constitutions of these societies, which work in close partnership with the school administration, usually prevent union members from running for executive positions in order to keep the school independent from political groups liable to harm the school's prestige.		In Germany the actual form of student representation depends strongly on the federal state. In most states there is a General Students' Committee (AStA) at every university. It is the executive organ of the parliaments and councils of all faculties.		In Greece every university department has its corresponding Student Union (in Greek: Σύλλογος Φοιτητών) and all students belonging to the department have the right to register as members. The main objective of a student union is to solve students' problems that can either be related to academic life or have a general political and social nature. Furthermore, Student Unions organize and support numerous activities such as political debates, demonstrations, university occupations, educational lectures, cultural and artistic events, conferences and so on.		The structure of a Student Union is rather simple and comprises two bodies: The General Students' Assembly (Greece) and the Board of Directors. The General Assembly consists of all student-members of the Union. It takes place on a regular basis and is the only decision-making body. During the General Assembly, many topics of student interest are discussed and the decisions are taken after open vote. The Board of Directors makes sure that the decisions of the General Assembly will be materialized. Moreover, the members of the Board of Directors, among which is the Union's President, participate in various university administrative bodies as representatives of all students in the Union.		Every year in early spring the Student Elections take place nationwide, during which students vote for their representatives. All Student Unions in Greece are members of the "National Student Union of Greece" (ΕΦΕΕ - Εθνική Φοιτητική Ένωση Ελλάδας).		Most of Ireland's universities and colleges have students' unions which were established to represent the students in the context of internal college issues and on wider student related issues and also a means of solidarity with other movements globally. An ongoing campaign of virtually every students' union in Ireland is to prevent the reintroduction of tuition fees which were abolished in 1995. Most, but not all, of the students' unions are affiliated with the Union of Students in Ireland. Notably, two of the seven university students' unions are not members. The students' unions are operated in accordance with the rules set down in their constitution which invariable enumerates a strong democratic and inclusive procedure for the governance on the union. Some Students' Unions run retail businesses in the interests of its students and run referendums, such as on whether or not to support same-sex marriage or abortion.		All Universities and Institutes of Technologies in the Republic of Ireland have Students' Unions.		National Union of Students-Union of Students in Ireland (NUS-USI), the student movement in Northern Ireland was formed in 1972 by bilateral agreement between the National Union of Students of the United Kingdom (NUS) and the Union of Students in Ireland (USI), to address the particular problems of representing students in Northern Ireland.		There are several students' unions in The Netherlands which act as labor unions for students. The largest ones are VSSD[4] in Delft and ASVA Studentenunie[5] in Amsterdam. These students' unions are all members of LSVb,[6] the national students' union. There's also a similar organization called ISO (Interstedelijk Studenten Overleg), which consists of several formal participation organizations, as well as ASVA Studentenunie and VSSD. Both ISO and LSVb are members of European Students' Union.		There is also a students' union at Twente University.[7] It was founded in 1999, succeeding the 'Raad voor de Campusvoorzieningen' and the 'Campuscollege'. This students' union is largely funded by the university and responsible for most activities not related to education, such as sports and culture. It is also an umbrella organization for the more than 130 student organizations at the university. The board is not elected: any student can apply for a one-year term. Selection is performed by a subcommittee of the 'Raad van Toezicht'. The board consists of six members, all full-time.		Lastly, the Netherlands has an (unofficial) student union for its students that study abroad: Netherlands Worldwide Students or simply NEWS.		The Netherlands is also home to a unique case of student representation in which a local political party completely run by student gained seats during local town hall elections: STIP.		In Norway, every university is instructed and required by law to have a Student Union elected by the pupils/students at the school. The goal for every Student Union is to improve their school environment through encouraging social, cultural and other extracurricular events that is happening in the local community. The student unions in Norway is governed by a Board of Directors which is elected directly from the Student Council.		In Portugal, every university, polytechnic institute and any other higher education schools has their own students' unions. Union organizations are generally aimed to organize and promote extracurricular activities such as sports and culture events, parties, and academic festivities. At the same time, they also act as "labour unions for students" promoting and defending the student's points of view and rights, and dealing with the teaching institutions and the State's education agencies policies. The oldest union of Portugal is the Associação Académica de Coimbra (founded in 1887) which belongs to the students of the University of Coimbra. The biggest students' union is Associação dos Estudantes do Instituto Superior Técnico (AEIST).		At Swedish universities, students' unions are responsible for representing the students in evaluation of the education and decision-making within the universities. Not Swedish universities[clarification needed] are affiliated with the Swedish National Union of Students. The union normally holds about one-third of the votes within every decision making body and thus holds a great deal of power.		The unions are usually governed by a general assembly of elected representatives. Students' unions generally provide counselling services to its members and publishes their own magazines or newspapers. Large universities often have several students' unions, where the smaller students' unions only provide basic services. Larger students' unions often own and run their own facilities at the university such as shops, restaurants and night clubs.		The United Kingdom has a long history of student unionism at a local and national level. The oldest students' union in Britain is St Andrews, founded in 1864, and the oldest in England is believed to be the Liverpool Guild of Students, formed in 1892.[8] Most bodies are termed unions, however there exist a number of guilds and students' associations. Students' associations is a popular term in Scotland, as historically there were separate men's and women's unions focused on societies and entertainment with representation to the university carried out by separate Students' Representative Councils. Most students' unions in the UK are affiliated to the National Union of Students, although there exist other national representative bodies, such the National Postgraduate Committee, the Coalition of Higher Education Students in Scotland and the Aldwych Group, the association of students' unions of members of the Russell Group.		National Union of Students-Union of Students in Ireland (NUS-USI), the student movement in Northern Ireland was formed in 1972 by bilateral agreement between the National Union of Students of the United Kingdom (NUS) and the Union of Students in Ireland (USI), to address the particular problems of representing students in Northern Ireland.		In Canada, membership in a college or university students' union is mandatory across Canada under various provincial statutes.[citation needed] Included in Canadian students' tuition fees is anywhere from an additional $10–$500 fee to pay for the services of the union. The money raised from dues is often used to support a staff and office. Student elections usually happen around March as the membership elect their unions' executives. Student voter turnout for student elections is low for all institutions. The current largest undergraduate student union in Canada is the York Federation of Students, at the York University, with around 49,000 members.		Most students' unions are charged by their membership to protect their best interests at the university, municipal, provincial and federal government levels. Many students' unions in Canada are members of one of the national student organizations, the Canadian Alliance of Student Associations (CASA) or the Canadian Federation of Students (CFS). Those that belong to the CFS at the national level also belong to the CFS at the provincial level. Those that do not belong to the CFS may belong to a provincial student organization like the New Brunswick Student Alliance, the Ontario Undergraduate Student Alliance (OUSA), the College Student Alliance, StudentsNS (formerly known as ANSSA), the Alliance of British Columbia Students, the Council of Alberta University Students, or the Alberta Students Executive Council. In Quebec, the provincial student organizations are the Quebec Federation of University Students (Fédération étudiante universitaire du Québec or FEUQ) for university students, college students are represented by the Fédération étudiante collégiale du Québec or FECQ, while the Association pour une solidarité syndicale étudiante or ASSE is a more radical organisation that includes students from both levels of education.		Different provinces have different rules governing student unions. In British Columbia under Section 27.1 of the University Act, student unions may only raise or rescind mandatory student union fees through a democratic referendum of the membership. Once fees are passed through this mechanism, the Board of Governors of the respective institutions are then required to remit those fees to the student union, and may only interfere in the internal affairs of the student union if it fails to complete annual audits of its finances, giving these student unions strong autonomy over their institutions. In Ontario, the autonomy of student unions is set by standards outlined by each institution, giving University administrators a broader scope of powers over the finances of student unions.		In Mexico, students unions are mostly predominant in universities. Mexican universities have an elected student committee each year, but the faculties or schools within the universities have also their own union. This practice is also extended to other levels of education, such as high and junior high school, but to a lesser extent. An example of this is the Sociedad de Alumnos de HPA Mexico.		Jesuit student groups played an important role in Mexico's history, particularly in opposing the imposition of socialist education in Mexico in the 1930s. Leaders from the Unión Nacional de Estudiantes Católicos (UNEC) had long-term importance in Mexico's political history, since a number of them helped form the conservative National Action Party (Mexico).[9]		In the United States, these groups are often known as student government, associated students, student senate, or less commonly a student's union. In the U.S., the phrase "student union" often refers to a "student activity center" (also known as a "student center" or "student commons"), a building with dining halls, game rooms, lounges, student offices, and other spaces for student activities. At institutions with large graduate, medical school, and individual "college" populations, there are often student governments that serve those specific constituencies.		The Brazilian students union is also called the Brazilian Student Association. It was formed in 1991 in order to provide the Brazilian students with representation that was fair and even within the LSU campus. The union has also set a side goal of promoting their Brazilian culture, since they are a Portuguese-speaking Latin American country.		The Brazilian Society		This society has existed since 2003 and now has 190 members, where only one third are Brazilian. The other two thirds are people who are just interested in Brazilian politics, famous parties and culture. This is one of the societies that students may join if they so wish. The society has social events and it hosts lectures. They also have a Brazilian discussion group on a weekly basis in order to discuss current events.[10]		
Postgraduate education, or graduate education in North America, involves learning and studying for academic or professional degrees, academic or professional certificates, academic or professional diplomas, or other qualifications for which a first or bachelor's degree generally is required, and it is normally considered to be part of higher education. In North America, this level is generally referred to as graduate school (or sometimes colloquially as grad school).		The organization and structure of postgraduate education varies in different countries, as well as in different institutions within countries. This article outlines the basic types of courses and of teaching and examination methods, with some explanation of their history.						There are two main types of degrees studied for at the postgraduate level: academic and vocational degrees.		The term degree in this context means the moving from one stage or level to another (from French degré, from Latin dē- + gradus), and first appeared in the 13th century.		Although systems of higher education date back to ancient Greece, ancient Rome, China, the Indian subcontinent and Arabian Peninsula, the concept of postgraduate education depends upon the system of awarding degrees at different levels of study, and can be traced to the workings of European medieval universities, mostly Italians.[1][2] University studies took six years for a bachelor's degree and up to twelve additional years for a master's degree or doctorate. The first six years taught the faculty of the arts, which was the study of the seven liberal arts: arithmetic, geometry, astronomy, music theory, grammar, logic, and rhetoric. The main emphasis was on logic. Once a Bachelor of Arts degree had been obtained, the student could choose one of three faculties—law, medicine, or theology—in which to pursue master's or doctor's degrees.		The degrees of master (from Latin magister) and doctor (from Latin doctor) were for some time equivalent, "the former being more in favour at Paris and the universities modeled after it, and the latter at Bologna and its derivative universities. At Oxford and Cambridge a distinction came to be drawn between the Faculties of Law, Medicine, and Theology and the Faculty of Arts in this respect, the title of Doctor being used for the former, and that of Master for the latter."[3] Because theology was thought to be the highest of the subjects, the doctorate came to be thought of as higher than the master's.[4]		The main significance of the higher, postgraduate degrees was that they licensed the holder to teach ("doctor" comes from Latin docere, "to teach").		In most countries, the hierarchy of postgraduate degrees is as follows:		In the UK and countries whose education systems were founded on the British model, such as the US, the master's degree was for a long time the only postgraduate degree normally awarded, while in most European countries apart from the UK, the master's degree almost disappeared. In the second half of the 19th century, however, US universities began to follow the European model by awarding doctorates, and this practice spread to the UK. Conversely, most European universities now offer master's degrees parallelling or replacing their regular system, so as to offer their students better chances to compete in an international market dominated by the American model.[6]		In the UK, an equivalent formation to doctorate is the NVQ 5 or QCF 8.[7]		Most universities award honorary degrees, usually at the postgraduate level. These are awarded to a wide variety of people, such as artists, musicians, writers, politicians, businesspeople, etc., in recognition of their achievements in their various fields. (Recipients of such degrees do not normally use the associated titles or letters, such as "Dr".)		Postgraduate education can involve studying for qualifications such as postgraduate certificates and postgraduate diplomas. They are sometimes used as steps on the route to a degree, as part of the training for a specific career, or as a qualification in an area of study too narrow to warrant a full degree course.		In Argentina, the admission to a Postgraduate program at an Argentine University requires the full completion of a Licenciado or Ingeniero degree.		While a significant portion of postgraduate students finance their tuition and living costs with teaching or research work at private and state-run institutions, international institutions, such as the Fulbright Program and the Organization of American States (OAS), have been known to grant full scholarships for tuition with apportions for housing.[8]		Upon completion of at least two years' research and course work as a postgraduate student, a candidate must demonstrate truthful and original contributions to his or her specific field of knowledge within a frame of academic excellence.[9] The Master and Doctoral candidate's work should be presented in a dissertation or thesis prepared under the supervision of a tutor or director, and reviewed by a postgraduate Committee. This Committee should be composed of examiners external to the program, and at least one of them should also be external to the institution.[10]		Programmes are divided into coursework-based and research-based degrees. Coursework programs typically include qualifications such as [11]		Research degrees generally consist of either Masters or Doctorate programs. In some disciplines it is acceptable to go straight from the undergraduate degree into a Ph.D. program if one achieves a very good Honors degree (see Admissions below), and in others, it may be encouraged or expected or simply advantageous in varying amounts for the student to first undertake a research Masters before applying to Ph.D. programs. Research master's degrees may be still called an M.A. or M.Sc., like a course work Masters, or may have a special appellation, e.g. M.Phil. Doctorate programs may lead to the award of a Ph.D. or a D.Phil. depending on the university or faculty.		The D.Litt is a higher research degree for exemplary achievement.		Generally, the Australian higher education system follows that of its British counterpart (with some notable exceptions). Entrance is decided by merit, entrance to coursework-based programmes is usually not as strict; most universities usually require a "Credit" average as entry to their taught programmes in a field related to their previous undergraduate. On average, however, a strong "Credit" or "Distinction" average is the norm for accepted students. Not all coursework programs require the student to already possess the relevant undergraduate degree, they are intended as "conversion" or professional qualification programs, and merely any relevant undergraduate degree with good grades is required.		Ph.D. entrance requirements in the higher ranked schools typically require a student to have postgraduate research honours or a master's degree by research, or a master's with a significant research component. Entry requirements depend on the subject studied and the individual university. The minimum duration of a Ph.D. programme is two years, but completing within this time span is unusual, with Ph.D.s usually taking an average of three to four years to be completed.		Most of the confusion with Australian postgraduate programmes occurs with the research-based programmes, particularly scientific programmes. Research degrees generally require candidates to have a minimum of a second-class four-year honours undergraduate degree to be considered for admission to a Ph.D. programme (M.Phil. are an uncommon route).[citation needed] In science, a British first class honours (3 years) is not equivalent to an Australian first class honours (1 year research postgraduate programme that requires a completed an undergraduate (pass) degree with a high grade-point average).[12] In scientific research, it is commonly accepted that an Australian postgraduate honours is equivalent to a British master's degree (in research). There has been some debate over the acceptance of a three-year honours degree (as in the case of graduates from British universities) as equivalent entry requirement to graduate research programmes (M.Phil., Ph.D.) in Australian universities.[citation needed] The letters of Honours programmes also added to the confusion. For example: B.Sc. (Hons) are the letters gained for postgraduate research honours at the University of Queensland. B.Sc. (Hons) does not indicate that this honours is postgraduate qualification. Difficulty also arises between different universities in Australia—some universities have followed the UK system.		There are many professional programs such as medical and dental school require a previous bachelors for admission and are considered graduate or Graduate Entry programs even though they culminate in a bachelor's degree. Example, the Bachelor of Medicine (MBBS) or Bachelor of Dentistry (BDent).		There has also been some confusion over the conversion of the different marking schemes between British, US, and Australian systems for the purpose of assessment for entry to graduate programmes. The Australian grades are divided into four categories: High Distinction, Distinction, Credit, and Pass (though many institutions have idiosyncratic grading systems). Assessment and evaluation based on the Australian system is not equivalent to British or US schemes because of the "low-marking" scheme used by Australian universities. For example, a British student who achieves 70+ will receive an A grade, whereas an Australian student with 70+ will receive a Distinction which is not the highest grade in the marking scheme.		The Australian government usually offer full funding (fees and a monthly stipend) to its citizens and permanent residents who are pursuing research-based higher degrees. There are also highly competitive scholarships for international candidates who intend to pursue research-based programmes. Taught-degree scholarships (certain master's degrees, Grad. Dip., Grad. Cert., D.Eng., D.B.A.) are almost non-existent for international students, so they are usually required to be self-funded.		Requirements for the successful completion of a taught master's programme are that the student pass all the required modules. Some universities require eight taught modules for a one-year programme, twelve modules for a one-and-a-half-year programme, and twelve taught modules plus a thesis or dissertation for a two-year programme. The academic year for an Australian postgraduate programme is typically two semesters (eight months of study).		Requirements for research-based programmes vary among universities. Generally, however, a student is not required to take taught modules as part of their candidacy. It is now common that first-year Ph.D. candidates are not regarded as permanent Ph.D. students for fear that they may not be sufficiently prepared to undertake independent research. In such cases, an alternative degree will be awarded for their previous work, usually an M.Phil. or M.Sc. by research.		In Brazil, a Bachelor's, Licenciate or Technologist degree is required in order to enter a graduate program, called pós-graduação. Generally, in order to be accepted, the candidate must have above average grades and it is highly recommended to be initiated on scientific research through government programs on undergraduate areas, as a complement to usual coursework.		The competition for public universities is very large, as they are the most prestigious and respected universities in Brazil. Public universities do not charge fees for any level/course. Funding, similar to wages, is available but is usually granted by public agencies linked to the university in question (i.e. FAPESP, CAPES, CNPq, etc.), given to the students previously ranked based on internal criteria.		There are two types of postgraduate; lato sensu (Latin for "in broad sense"), which generally means a specialization course in one area of study, mostly addressed to professional practice, and stricto sensu (Latin for "in narrow sense"), which means a Master of Science or Doctorate, encompassing broader and profound activities of scientific research.[13]		Admission to a graduate certificate program requires a university degree (or in some cases, a diploma with years of related experience). English speaking colleges require proof of English language proficiency such as IELTS. Some colleges may provide English language upgrading to students prior to the start of their graduate certificate program.		Admission to a master's program generally requires a bachelor's degree in a related field, with sufficiently high grades usually ranging from B+ and higher (note that different schools have different letter grade conventions, and this requirement may be significantly higher in some faculties), and recommendations from professors. Some schools require samples of the student's writing as well as a research proposal. Some programs require Graduate Record Exams (GRE) in both the general examination and the examination for its specific discipline, with minimum scores for admittance. At English-speaking universities, applicants from countries where English is not the primary language are required to submit scores from the Test of English as a Foreign Language (TOEFL). Nevertheless, some French speaking universities, like HEC Montreal, also requires candidates to submit TOEFL score or to pass their own English test.		Admission to a doctoral program typically requires a master's degree in a related field, sufficiently high grades, recommendations, samples of writing, a research proposal, and typically an interview with a prospective supervisor. Requirements are often set higher than those for a master's program. In exceptional cases, a student holding an honours B.A. with sufficiently high grades and proven writing and research abilities may be admitted directly to a Ph.D. program without the requirement to first complete a master's. Many Canadian graduate programs allow students who start in a master's to "reclassify" into a Ph.D. program after satisfactory performance in the first year, bypassing the master's degree.		Graduate students must usually declare their research goal or submit a research proposal upon entering grad school; in the case of master's degrees, there will be some flexibility (that is, one is not held to one's research proposal, although major changes, for example from premodern to modern history, are discouraged). In the case of Ph.D.s, the research direction is usually known as it will typically follow the direction of the master's research.		Master's degrees can possibly be completed in one year but normally take at least two; they typically do not exceed five years. Doctoral degrees require a minimum of two years but frequently take much longer, not usually exceeding six years.		Graduate students may take out student loans, but instead they often work as teaching or research assistants. Students often agree, as a condition of acceptance to a programme, not to devote more than twelve hours per week to work or outside interests.		Funding is available to first-year masters students whose transcripts reflect exceptionally high grades; this funding is normally given in the second year.		Funding for Ph.D. students comes from a variety of sources, and many universities waive tuition fees for doctoral candidates.[citation needed]		Funding is available in the form of scholarships, bursaries and other awards, both private and public.		Graduate certificates require between eight and sixteen months of study. The length of study depends on the program. Graduate certificates primarily involve coursework. However, some may require a research project or a work placement.		Both master's and doctoral programs may be done by coursework or research or a combination of the two, depending on the subject and faculty. Most faculties require both, with the emphasis on research, and with coursework being directly related to the field of research.		Master's candidates undertaking research are typically required to complete a thesis comprising some original research and ranging from seventy to two-hundred pages. Some fields may require candidates to study at least one foreign language if they have not already earned sufficient foreign-language credits. Some faculties require candidates to defend their thesis, but many do not. Those that do not, often have a requirement of taking two additional courses, at minimum, in lieu of preparing a thesis.		Ph.D. candidates undertaking research must typically complete a thesis, or dissertation, consisting of original research representing a significant contribution to their field, and ranging from two-hundred to five-hundred pages. Most Ph.D. candidates will be required to sit comprehensive examinations—examinations testing general knowledge in their field of specialization—in their second or third year as a prerequisite to continuing their studies, and must defend their thesis as a final requirement. Some faculties require candidates to earn sufficient credits in a third or fourth foreign language; for example, most candidates in modern Japanese topics must demonstrate ability in English, Japanese, and Mandarin, while candidates in pre-modern Japanese topics must demonstrate ability in English, Japanese, Classical Chinese, and Classical Japanese.		At English-speaking Canadian universities, both master's and Ph.D. theses may be presented in English or in the language of the subject (German for German literature, for example), but if this is the case an extensive abstract must be also presented in English. In exceptional circumstances, a thesis may be presented in French.[citation needed] The exception to this rule is McGill University, where all work can be submitted in either English or French, unless the purpose of the course of study is acquisition of a language.[14]		French-speaking universities have varying sets of rules; some (e.g. HEC Montreal[15]) will accept students with little knowledge of French if they can communicate with their supervisors (usually in English).		There are 87 public universities in France, and they are based upon the European education ladder including bachelors, Masters, and Ph.D.s. You gain each degree though the successful completion of a predetermined number of years in education. You use these years to gain credits via the European Credit Transfer System (ECTS). There are over 300 doctoral programs that collaborate with 1200 research laboratories and centers. Each degree has a certain set of national diplomas that are all of equal value, irrespective of where they were issued. There are also other diplomas that are exclusive to France and are very hard to attain.		In some programs in the traditional German system and the traditional Dutch system, there is no legal distinction between "undergraduate" and "postgraduate". In such programs, all education aims towards the master's degree, whether introductory (Bachelor's level) or advanced (master's level). These one-tier programmes take between 4.5 and 5 years.		In the meantime, Germany introduced the Bologna process with a separation between Bachelor and Master programmes in many fields, except for education studies, law and other specially regulated subjects.		Admission to a postgraduate degree programme in Nigeria requires a bachelor's degree with at least a Second Class Lower Division (not less than 2.75/5). Admission to Doctoral programmes requires an Academic master's degree with a minimum weighted average of 60% (B average or 4.00/5). In addition to this, applicants may be subjected to written and oral examinations depending on the school. Most universities with high numbers of applicants have more stringent admission processes.		Postgraduate degrees in Nigeria include M.A., M.Sc., M.Ed., M.Eng., LL.M, M.Arch., M.Agric., M.Phil., PhD. The master's degree typically take 18–36 months with students undertaking coursework and presenting seminars and a dissertation. The doctoral degree is for a minimum of 36 months and may involve coursework alongside the presentation of seminars and a research thesis. Award of postgraduate degrees requires a defence of the completed research before a panel of examiners comprising external and internal examiners, Head of Department, Departmental Postgraduate Coordinator, Representative(s) of Faculty and Postgraduate School, and any other member of staff with a PhD in the department/faculty.		Admission to undertake a research degree in the UK typically requires a good bachelor's degree, or Scottish M.A., (at least lower second, but usually an upper second or first class). In some institutions Doctoral candidates are initially admitted to a Masters in Research Philosophy (M.Phil. or M.Res.), then later transfer to a Ph.D./D.Phil. if they can show satisfactory progress in their first 8–12 months of study.[16] Candidates for the degree of Doctor of Education (Ed.D) are typically required to hold a good bachelor's degree as well as an appropriate master's degree before being admitted.		Funding for postgraduate study in the UK is awarded competitively, and usually is disseminated by institution (in the form of a certain allocation of studentships for a given year) rather than directly to individuals. There are a number of scholarships for master's courses, but these are relatively rare and dependent on the course and class of undergraduate degree obtained (usually requiring at least a lower second). Most master's students are self-funded.		Funding is available for some Ph.D./D.Phil. courses. As at the master's level, there is more funding available to those in the sciences than in other disciplines. Such funding generally comes from Research Councils such as the Engineering and Physical Sciences Research Council (EPSRC), Arts and Humanities Research Council (AHRC), Medical Research Council (MRC) and others. Masters students may also have the option of a Postgraduate loan introduced by the British Government in 2016.		For overseas students, most major funding applications are due as early as twelve months or more before the intended graduate course will begin. This funding is also often highly competitive. The most widely available, and thus important, award for overseas students is the Overseas Research Student (ORS) Award, which pays the difference in university fees between an overseas student and a British or EU resident. However, a student can only for one university apply for the ORS Award, often before he or she knows whether they have been accepted. As of the 2009/2010 academic year, the HEFCE has cancelled the Overseas Research Student Award scheme for English and Welsh universities.[17] The state of the scheme for Scottish and Northern Irish universities is currently unclear.		Students studying part-time for a master's degree can apply for income-based Jobseeker's Allowance provided their timetabled hours are fewer than 16 hours per week. This also entitles the student to housing benefit provided by their local council.[citation needed] Full-time students (of any type) are not normally eligible for state benefits, including during vacation time.[18]		Additionally, doctoral students who have advanced to candidacy but not filed a dissertation ("ABD," for "all but dissertation") often receive master's degrees and an additional master's called a Master of Philosophy, or M.Phil., or C.Phil. "Candidate in Philosophy" degree. The master's component of a doctorate program often requires one or two years, and some students, because doctoral programs are sometimes better-funded, apply for doctoral programs while only intending to earn a master's degree.[19] This is generally not acceptable and, if a student's advisor learns of the student's plans, can result in early termination.		Many graduate programs require students to pass one or several examinations in order to demonstrate their competence as scholars.[20] In some departments, a comprehensive examination is often required in the first year of doctoral study, and is designed to test a student's background undergraduate-level knowledge. Examinations of this type are more common in the sciences and some social sciences, and relatively unknown in most humanities disciplines.		Some graduate students perform teaching duties, often serving as graders, tutors, or teaching assistants. In some departments, they can be promoted to Lecturer status, a position that comes with more responsibilities.		Doctoral students generally spend roughly their first two to three years doing coursework, and begin research by their second year if not before. Many master's and all specialist students will perform research culminating in a paper, presentation, and defense of their research. This is called the master's thesis (or, for Educational Specialist students, the specialist paper). However, many US master's degree programs do not require a master's thesis, focusing instead primarily on course work or on "practicals" or "workshops". Such "real-world" experience may typically require a candidate work on a project alone or in a team as a consultant, or consultants, for an outside entity approved or selected by the academic institution, and under faculty supervision.		In the second and third years of study, doctoral programs often require students to pass more examinations.[20] Programs often require a Qualifying Examination ("Quals"), a Ph.D. Candidacy Examination ("Candidacy"), or a General Examination ("Generals") designed to test the students' grasp of a broad sample of their discipline, or one or several Special Field Examinations ("Specials") which test students in their narrower selected areas of specialty within the discipline. If these examinations are held orally, they may be known colloquially as "orals". For some social science and many humanities disciplines, where graduate students may or may not have studied the discipline at the undergraduate level, these exams will be the first set, and be based either on graduate coursework or specific preparatory reading (sometimes up to a year's work in reading). In all cases, comprehensive exams are normally both stressful and time-consuming and must be passed to be allowed to proceed on to the thesis. Passing such examinations allows the student to stay, begin doctoral research, and rise to the status of a doctoral candidate while failing usually results in the student leaving the program or re-taking the test after some time has passed (usually a semester or a year). Some schools have an intermediate category, passing at the master's level, which allows the student to leave with a master's without having completed a master's thesis.		For the next several years, the doctoral candidate primarily performs his or her research. Usually this lasts three to eight years, though a few finish more quickly and some take substantially longer. In total, the typical doctoral degree takes between four and eight years from entering the program to completion though this time varies depending upon the department, thesis topic, and many other factors.		For example, astronomy degrees take five to six years on average, but observational astronomy degrees take six to seven due to limiting factors of weather, while theoretical astronomy degrees take five. Though there is substantial variation among universities, departments, and individuals, humanities and social science doctorates on average take somewhat longer to complete than natural science doctorates. These differences are due to the differing nature of research between the humanities and some social sciences and the natural sciences, and to the differing expectations of the discipline in coursework, languages and length of thesis. However, time required to complete a doctorate also varies according to the candidate's abilities and choice of research. Some students may also choose to remain in a program if they fail to win an academic position, particularly in disciplines with a tight job market; by remaining a student, they can retain access to libraries and university facilities, while also retaining an academic affiliation, which can be essential for conferences and job-searches.		Traditionally, doctoral programs were only intended to last three to four years and, in some disciplines (primarily the natural sciences), with a helpful advisor, and a light teaching load, it is possible for the degree to be completed in that amount of time. However, increasingly many disciplines, including most humanities, set their requirements for coursework, languages and the expected extent of thesis research by the assumption that students will take five years minimum or six to seven years on average; competition for jobs within these fields also raises expectations on the length and quality of theses considerably.		In some disciplines, doctoral programs can average seven to ten years. Archaeology, which requires long periods of research, tends towards the longer end of this spectrum. The increase in length of degree is a matter of great concern for both students and universities, though there is much disagreement on potential solutions to this problem.		Many departments, especially those in which students have research or teaching responsibilities, offer tuition-forgiveness and a stipend that pays for most expenses. At some elite universities, there may be a minimum stipend established for all Ph.D. students, as well as a tuition waiver. The terms of these stipends vary greatly, and may consist of a scholarship or fellowship, followed by teaching responsibilities. At many elite universities, these stipends have been increasing, in response both to student pressure and, especially, to competition among the elite universities for graduate students.		In some fields, research positions are more coveted than teaching positions because student researchers are typically paid to work on the dissertation they are required to complete anyway, while teaching is generally considered a distraction from one's work. Research positions are more typical of science disciplines; they are relatively uncommon in humanities disciplines, and where they exist, they rarely allow the student to work on their own research.		Departments often have money for limited discretionary funding to supplement minor expenses such as research trips and travel to conferences.		A few students can attain outside fellowships such as the National Science Foundation (NSF) and National Physical Science Consortium (NPSC). Funding differs greatly by departments and universities; some universities give five years of full funding to all Ph.D. students, though often with a teaching requirement attached; other universities do not.[21]		Foreign students are typically funded the same way as domestic (US) students, although federally subsidized student and parent loans and work-study assistance are generally limited to US citizens and nationals, permanent residents, and approved refugees.[22] Moreover, some funding sources (such as many NSF fellowships) may only be awarded to domestic students. Other factors contributing to possible financial difficulties include high costs to visit their families back home, supporting one's family who is not allowed to work due to immigration laws, tuition that is steep by world standards, and large fees: visa fees by US Citizenship and Immigration Services, surveillance fees (such as Student and Exchange Visitor Information Systems, or SEVIS[23]) by the United States Congress and the United States Department of Homeland Security.		
Traffic cones, also called pylons, road cones, highway cones, safety cones, or construction cones, are usually cone-shaped markers that are placed on roads or footpaths to temporarily redirect traffic in a safe manner. They are often used to create separation or merge lanes during road construction projects or automobile accidents, although heavier, more permanent markers or signs are used if the diversion is to stay in place for a long period of time.						Traffic cones were invented by Charles D. Scanlon, an American who got the idea while working as a painter for the Street Painting Department of the City of Los Angeles.[1] The patent for his invention was granted in 1943.[2]		The first traffic cones used in the United Kingdom occurred In 1958, when the M6 motorway opened. These traffic cones were a substitute for red lantern paraffin burners being used during construction on the Preston Bypass.[3] In 1961, David Morgan of Burford, Oxfordshire, UK believes that he constructed the first experimental plastic traffic cones, which replaced pyramid-shaped wooden ones previously used.[4]		In the United States on May 1, 1959 the Pacific Gas and Electric Company in Oakland, California adopted the policy of placing the orange safety cones at left front and the left rear corners of their service trucks while parked on the street to increase visibility and safety for the workers. This policy was implemented as the result of a suggestion by their employee, Russell Storch, a cable splicer. He was awarded $45 for his suggestion. This policy is still in use today.[5]		Although originally made of concrete, today's versions are more commonly brightly colored thermoplastic or rubber cones. Recycled PVCs from bottles can be used to create modern traffic cones.[6] Not all traffic cones are conical. Pillar-shaped movable bollards fulfill a similar function.[7]		Traffic cones are typically used outdoors during road work or other situations requiring traffic redirection or advance warning of hazards or dangers, or the prevention of traffic. Traffic cones are also used to mark where children are playing or to block off an area. For night time use or low-light situations traffic cones are usually fitted with a retroreflective sleeve to increase visibility. On occasion, traffic cones may also be fitted with flashing lights for the same reason.		In the US, cones are required by the US Federal Highway Administration's Manual on Uniform Traffic Control Devices (MUTCD) to be fitted with reflective white bands to increase night-time visibility. Reflective collars, white strips made from white reflective plastic, slip over cones snugly, and tape or adhesive can be used to permanently attach the collars to the cones.		Traffic cones are designed to be highly visible and easily movable. Various sizes are used, commonly ranging from around 30 cm (11.8 in) to a little over 1 m (39.4 in). Traffic cones come in many different colors, with orange, yellow, pink, and red being the most common colors due to their brightness. Others come in green and blue, and may also have a retroreflective strip (commonly known as "flash tape") to increase their visibility.		Typical traffic cones are fluorescent "safety" orange, as well as lime green. Traffic cones also commonly come with reflective striping around them, to increase visibility.		In the United States, they come in such sizes as:		Cones are easy to move or remove. Where sturdier (and larger) markers are needed, construction sites use traffic barrels (plastic orange barrels with reflective stripes, normally about the same size as a 55 US gallons (46 imp gal; 208 L) drum). When a lane closure must also be a physical barrier against cars accidentally crossing it, a Jersey barrier is preferred. See also Fitch Barrier.		In many countries such as Australia or American states such as California, traffic barrels are rarely seen. Devices called bollards are used instead of cones where larger and sturdier warning or delineation devices are needed. Typically, bollards are 1,150 mm (45 in) high fluorescent orange posts with reflective sleeve and heavyweight rubber bases. Larger devices such as barrier boards may be used instead of cones where larger areas need to be excluded or for longer periods. In Canada they are often referred to as pylons.		Cones are used to lay out courses for autocross competitions.		Cones are also frequently used in indoor public spaces to mark off areas which are closed to pedestrians, such as a restroom being out of order, or to denote a dangerous condition, such as a slippery floor. They can be used on school playgrounds to limit areas of a playing field, and on ice rinks to define class, private party, or private lesson areas. Some of the cones used for this purpose are miniature, as small as 5 cm (2.0 in) tall, and some are disposable full-size cones made of biodegradable paper.		Being distinctive, easily portable and usually left unguarded, traffic cones are often stolen. Students are frequently blamed, to the extent that the British National Union of Students has attempted to play down this "outdated stereotype".[8]		The term "road cone" is also commonly used in the construction industry as a lighthearted insult. It is used to describe an individual who spends most of the day just standing still, making no attempt to get involved in the work they should be doing.[citation needed]		In 2007 the artist Dennis Oppenheim commemorated the traffic cone with a monumental sculpture of five five-metre-tall cones. They were installed temporarily in Miami,[9] Seattle's Olympic Sculpture Park,[10] and Seoul, Korea. An orange-and-white cone is the logo used by VideoLAN (best known for its VLC media player software). German group Kraftwerk featured traffic cones on their first two albums, as well as in their concerts at the time. Traditionally, but unofficially, the Wellington Statue in Glasgow is decorated with a traffic cone. The presence of the cone is given as the reason the statue is in the Lonely Planet 1000 Ultimate Sights guide (at number 229) as a "most bizarre monument".[11]		The Traffic Cones is a Belgian TV series on Nickelodeon.[12]		Giant traffic cone in Seattle, Washington		Duke of Wellington statue, with cone		Prank in Raglan, New Zealand				
A primary school (or elementary school in American English and often in Canadian English) is a school in which children receive primary or elementary education from the age of about five to twelve, coming after preschool and before secondary school. (In some countries there is an intermediate stage of middle school between primary and secondary education.) In most parts of the world, primary education is the first stage of compulsory education, and is normally available without charge, but may be offered in a fee-paying independent school. The term grade school is sometimes used in the US, although this term may refer to both primary education and secondary education.		The term primary school is derived from the French école primaire, which was first used in 1802.[1]		In the United States, "primary school" may refer to a school with grades Kindergarten through second grade or third grade. (K-2 or 3). In these municipalities, the "elementary school" includes grade three through five or grades four to six.		In some places, primary schooling has historically further been divided between lower primary schools (LP schools), which were the elementary schools, and higher primary schools (HP schools), which were established to provide a more practical instruction to poorer classes than what was provided in the secondary schools.[3]		
University student retention, sometimes referred to as persistence, is of increasing importance to college administrators as they try to improve graduation rates and decrease a loss of tuition revenue from students that either drop out or transfer to another school.		Transfer rates are very high in the United States with 60% of all bachelor's degrees being awarded to students that began their college at another institution.[1] Some transfer is planned; many community colleges have articulation agreements with four-year colleges.		Other university systems have so-called feeder schools offering the first two years of the degree at a local campus with transfer into the flagship university in the junior year.		Universities are now creating a number of new programs for students that help keep them engaged in their classes and involved on campus. This includes campus funded tutoring, freshman seminar courses, and intramural sports among many other things. These programs are important when it comes to campus life because it has been shown that student involvement is directly related to student success.[2] When a student participates, he or she forms both social and emotional ties to the university that both encourage the student to do well academically and reduce the chance that the student will drop out of school entirely or leave for another university.		The economy also has a noticeable effect on retention rates. In general, tuition has been steadily climbing at universities since the mid-1980s. The cost of public and private institutions in the 1999-2000 school year, which includes tuition and on campus housing, averaged $7,302 and $20,277, respectively. After adjusting for inflation, this represented a 22% cost increase at public institutions and a 27% increase at private institutions for the 10-year period between the 1989-1990 and 1999-2000 academic years.[3] This rise in cost has made it difficult for many students and their families to pay for college. According to the National Center for Public Policy and Higher Education, tuition at a 4-year college represented 12% of the total income for families that fell into the lowest income bracket in 1980, and rose drastically to encompass 25% of their income by 2000.[4] This has created an influx of part-time students and working students. In the undergraduate population, 50% of students describe themselves as working primarily to pay for their education at an average of 25 hours per week.[5] This leaves working students little time to become involved on campus and actively participate in university life. Indeed, working-class students, who spend more time in paid employment, are significantly less integrated into university life than middle-class students.[6] In spite of all of the programs and services to help retain students, according to the U.S. Department of Education, National Center for Educational Statistics, only 50% of those who enter higher education actually earn a bachelor's degree.[7] Though research is still needed in this area, it is becoming clear that there may be a link between the increased amount of working students and declining retention rates.		Additional counseling is often available for financial issues. Private counseling and private tutoring are other options for students.		Private corporations are looking into the business of student retention as a potential new field of revenue. This has led to problematic outsourcing strategies, such as the case of the University of Texas' system $ 10 million investment into the private company Myedu.[8] Data on the amount of corporate lobbying addressed to the Board of Regents of State Universities is not available.		At the same time, there is a great deal that administrators at the school and college level as well as faculty at the course level can do to improve student retention. For instance, in online courses where attrition has been reported even higher than in traditional face-to-face courses, faculty can strive to make connections and meet the needs of individual students[9]		
University students have a long association with pranks and japes.[1][2][3][4][5] These can often involve petty crime, such as the theft of traffic cones and other public property,[6] or hoaxes.[7][8][9][10][11][12][13][14][15][16] In fact, practical jokes play such a significant part in student culture that numerous books have been published that solely focus on the issue of student pranks.[17][18]		In some university towns, misbehaviour on the part of students became such an issue that a report has been released which studies the issue. The report, Studentification: A Guide to Opportunities, Challenges and Practice, by Universities UK, focuses on six British universities as case studies.		One classic target of student theft are traffic cones. The issue of the theft and misuse of traffic cones by students has gained enough prominence that a spokesperson from the UK National Union of Students has been forced to argue that "stereotypes of students stealing traffic cones" are "outdated".[19]		Some universities have gone as far as to devote entire pages of legislation and advice for students with regards to the consequences and laws involving the theft of traffic cones.[20] Misuse of traffic cones in Scotland has even resulted in serious physical injury.[21]		The traffic cone theft issue came to such a head in the 1990s that it was brought up in parliament.[22]		In 2002, Fife Constabulary declared a "traffic cone amnesty" allowing University of St Andrews students to return stolen traffic cones without fear of prosecution. A police spokesman had said that the theft of traffic cones had become "an almost weekly occurrence".[23]		
Pre-kindergarten (also called Pre-K or PK) is a classroom-based preschool program for children at or below the age of five in the United States, Canada and Turkey (when kindergarten starts).[1][2] It may be delivered through a preschool or within a reception year in elementary school. Pre-kindergartens play an important role in early childhood education. They have existed in the US since 1922,[citation needed] normally run by private organizations. The U.S. Head Start program, the country's first federally funded pre-kindergarten program, was founded in 1967. This attempts to prepare children (especially disadvantaged children) to succeed in school.[3]						The term "pre-kindergarten" is often used interchangeably with the concepts of "day care", and "child care"; however, these other early childhood settings focus their goal on substitutionary care for children while their legal parents/guardians are absent as opposed to pre-K's focus on skill building. They could involve academic training, or they could involve solely socializing activities.		Pre-kindergartens, though, differentiate themselves by equally focusing on building a child's (1) social development, (2) physical development, (3) emotional development, and (4) cognitive development. They commonly follow a set of organization-created teaching standards in shaping curriculum and instructional activities/goals. The term "preschool" more accurately approximates the name "pre-kindergarten", for both focus on harvesting the same four child development areas in subject-directed fashion. The term "preschool" often refers to such schools that are owned and operated as private or parochial schools. Pre-kindergartens refer to such school classrooms that function within a public school under the supervision of a public school administrator and funded completely by state or federally allocated funds, and private donations.		Most school districts describe Pre-Kindergarten as "an early learning program to prepare children for kindergarten who are identified as at risk." Pre-kindergarten provides learning to children who are 4 years old on or before September 1. Pre-kindergarten for three-year-olds provides learning to children who are 3 on or before September 1. Most programs are 3 hours but extended day is offered in some schools.		"K-2" is often (and controversially) used interchangeably with "pre-kindergarten". Although early childhood education experts criticize the use of the term as a way to rationalize utilizing a kindergarten model and teaching kindergarten skills in pre-kindergarten classes, public school districts continue to incorporate the term as a way to integrate pre-kindergarten into the stable of accountability under the No Child Left Behind Act.		To qualify for free pre-kindergarten in some states the child generally must be:		"Pre-Kindergarten" may have different meanings in other countries. In India, this is also known as Nursery in Kindergartens such as Applekids.[4]		In 2013 Alabama, Michigan, Minnesota, and the city of San Antonio enacted or expanded Pre-K programs. In New York City new mayor Bill de Blasio was elected on a pledge of Pre-K for all city children. A poll conducted in July for an early education nonprofit advocate found that 60 percent of registered Republicans and 84 percent of Democrats supported expanding public preschool by raising the federal tobacco tax.[5]		Funding for Pre-K has proven a substantial obstacle to creating and expanding programs. The issue produced multiple approaches. Several governors targeted existing budgets. San Antonio increased sales taxes, while Virginia and Maine look to gambling. In Oregon, currently 20% of kids have access to publicly funded Pre-K of any kind, and a 2016 campaign is working to fully fund Pre-K to 12 education, for all kids whose parents want them to have the option of Pre-K.[5][6]		A 2012 review by the National Institute for Early Education Research at Rutgers University identified Oklahoma, Georgia and West Virginia as among the leaders in public program quality and fraction of enrolled children. Florida had the highest enrollment in 2012 — almost four-fifths of all four-year-olds. About 84 percent were in private, religion-based or family centers. That state's preschool programs did not fare well on quality measures. Other states with more than 50 percent enrollment included Wisconsin, Iowa, Texas and Vermont.[5]		The US Census Bureau forecast that the foreign born population in the United States would make up 19% of the US population by 2060 (up from 13% in 2014).[7] Children of immigrant families face special challenges.		Children of immigrants represent the fastest growing US population. Asians and Latinos are the two largest racial groups. Like all families, immigrants have choices when pursuing childcare options. Cultural difference childcare choices, such as attitudes towards early academic development. These differences help explain differences in childcare selection. Compared to Latino immigrant groups, Asians are more likely than Latinos to enroll their children in pre-kindergarten programs due to the inclusion of academics.[8] The focus of pre-academic, school readiness is important to Asian parents. Latino immigrant parents by contrast generally opt for more informal childcare options, such as parental, relative or non-relative in-home care.[9] This is due in part to the opinion that academic skills are taught through formal instruction after children enter primary school.[10] While Latino families value the acquisition of academic skills, the in-home childcare choice is a reflection of the importance of cultural and linguistic values and traditional family dynamics. Parents with limited English proficiency are more likely to choose parental or in-home care instead of pre-kindergarten programs.[8]		According to information from the Survey of Income and Program Participation (SIPP) and the National Institute of Child Health and Human Development (NICHD), low-income immigrant families are less likely to use center-based childcare, such as pre-kindergarten, than children of non-immigrants.[11] While some Latino families prefer in-home childcare, many report wanting to enroll their children in a pre-kindergarten program. Interviews with immigrant mothers revealed common motivations for seeking pre-kindergarten placements for their children, including maternal employment, opportunity to learn English and social and emotional development.[12] Obstacles immigrant mothers reported facing included high cost, long wait-lists, a need to provide documentation (especially for undocumented immigrants and those who lacked English-language proficiency) and a lack of information regarding eligibility for subsidized programs. On average, immigrants tend to experience higher poverty rates due to low wages, less education and a lack of English proficiency.		While all children would benefit from pre-kindergarten and early childhood education, immigrant children, particularly those from lower socio-economic households, stand to benefit the most. Studies indicate that first and second generation immigrants lag behind children of non-immigrant families in cognitive and language skills.[13] Pre-K’s focus on cognitive, social, emotional and physical development would address these skills and reduce the inequalities in school readiness between children from immigrant and non-immigrant families. Educators must be sensitive to sensitivities of immigrant groups regarding the acquisition of the English language versus their native-language. Pre-K could help children build either or both skills. For most US students, English fluency is essential.[14]		
High school clubs and organizations, are student-based school organizations, consisting of administration-approved organizations functioning with myriad tasks, varying on the specific purpose of each respective club. Clubs composed of students, with adults as advising figures to maintain the functionality of clubs. Clubs primarily focus on four aspects: fundraising, community service, career interest, and interpersonal dynamics (also known as group dynamics). In general, clubs are broken down into two main categories: State and/or Nationwide organizations, and local clubs. Within major, nationwide club organizations, each individual charter within each school is referred to as a "chapter". Clubs are started by either corporations, counterpart adult organizations, or campus students looking to satisfy a need or demand.						The first high school student-based organization chartered in Sacramento High School in California, in May 1925.[1] The concept of instilling an organized, separate entity separate from the school itself came from Albert Olney, and Frank Vincent. They were school administrators and Kiwanis Club members who were looking to form a junior service club in the school. This organization later became known as Key Club. Key Club now stands today as the largest student-based organization in the world, though not the largest high school organization in the world.		Tracking down precise history of high school organizations is difficult as several thousand types of clubs exist. Prominent clubs include high school subdivisions of Red Cross, Make-A-Wish Foundation, UNICEF Clubs, National Honor Society, National Beta Club, Junior State of America, Interact, Future Business Leaders of America-Phi Beta Lambda, among many other organizations. Each club has their own timeline, with hallmark internal achievements only known by members of each respective club. As well as single-school clubs, such as a Debate Club, Geocaching Club, and others of the like.		There are four main club categories: fundraising, community service, career interest, and interpersonal dynamics. Many clubs offer a combination of each element.		Many people polarize toward fundraising for a major organization or movement. Fundraising appeals to people as high school students make a direct impact on international affairs, such as funding cancer research or environmental preservation.		Many schools require that students perform a certain community service quota. To obtain such a threshold, many people turn to community service organizations such as Exchange Clubs, Key Club, Interact Club, Lion's Club, Red Cross, FCCLA,and local clubs. In doing so, many teenagers experience more camaraderie while performing community service. Other individuals just enjoy helping the local community around them.		Many teenagers join clubs that revolve around their career interest. Many clubs, such as High Schools Society [2] in Ghana, Junior State of America[3] and Future Scientist and Engineers of America focus on specific career fields and help students understand them better. Many competitions, awards, and conventions are held to give club members advantages in these fields by exposing them to new opportunities. In addition, members of career interest clubs network with other students who will enter similar fields.		Many teenagers join clubs that offer no academic, organizational, or community benefit. These clubs tend to focus around culture, social dynamics, and self-interest. These clubs look to satisfy the needs and demands of teenagers in each school, based on environment, tradition, and culture. Clubs such as an Anime Club can bring students to socialize.		UNICEF High School Clubs		
In the education systems of England, Northern Ireland, Wales and some other Commonwealth countries, sixth form (sometimes referred to as Key Stage 5) represents the final 1-3 years of secondary education (high school), where students (typically between 16 and 18 years of age) prepare for their A-level (or equivalent) examinations.						The term sixth form describes the school years numbered 12 and 13, which are called the Lower Sixth (L6) and Upper Sixth (U6) by many schools.		The term survives from an earlier system when the first five years of English secondary schooling were known as forms (which would originally have been long backless benches on which rows of pupils sat in the classroom). Pupils started their first year of secondary school in the first form or first year, and this was the academic year in which pupils would normally become 12 years of age. Pupils would move up a form each year before entering the fifth form in the academic year in which they would have their sixteenth birthday. Those who stayed on at school to study for A-levels moved up into the sixth form, which was divided into the Lower Sixth and the Upper Sixth. In some private schools, the term Middle Sixth was used in place of Upper Sixth, with the latter being used for those who stayed on for an extra term to take the entrance examinations that were previously set for candidates to Oxford or Cambridge universities. Other schools described these Oxbridge examination students as being in the Seventh Form or Third Year Sixth.		The system was changed for the 1990–1991 academic year and school years are now numbered consecutively from primary school onwards. Year 1 is the first year of primary school after Reception. The first year of secondary school (the old first form) is now known as Year 7. The Lower Sixth is now Year 12 and the Upper Sixth is Year 13. However, the term "Sixth Form" has still been retained as a vestige of the old system and is used as a collective term for Years 12 and 13. Public (fee-charging) schools, along with some state schools, tend to use the old system of numbering.		In some parts of the country, special sixth form colleges were introduced beginning in a particularly important phase of student life. A large proportion of English secondary schools no longer have an integral sixth form. This is mainly related to reforms in the later 20th century, where different political areas became a factor in the introduction of colleges instead of the original sixth forms. There are now numerous sixth form colleges throughout England and Wales, and in areas without these, sixth form schools and specialist further education (FE) colleges called tertiary colleges may fill the same role.		Sixth form is not compulsory in England and Wales (although from 2013 onwards, people of sixth form age must remain in some form of education or training in England only, the school leaving age remains 16 in Wales); however, university entrance normally requires at least three A2-level qualifications and perhaps one AS-level. Students usually select three or four subjects from the GCSEs they have just taken, for one "AS" year, the AS exams being taken at the end of Lower Sixth. Three subjects are then carried into the A2 year (the dropped AS being "cashed in" as a qualification) and further exams are taken at the end of that year. The marks attained in both sets of exams are converted into UCAS points, which must meet the offer made by the student's chosen university.		In Northern Ireland, the equivalent of Reception is "P1", and the equivalent of the English Year 1 "P2", while the first year of secondary school is known as Year 8 or first year (rather than Year 7 as in England), and following that Lower and Upper Sixth are Year 13 and Year 14 respectively.		In the Scottish education system, the final year of school is known as Sixth Year or S6. During this year, students typically study Advanced Higher and/or Higher courses in a wide range of subjects, taking SQA exams at the end of both S5 and S6. Pupils in Scotland may leave once they have reached the age of 16; those who reach 16 before 30 September may leave after national examinations in May, whilst those who are 16 by the end of February may leave the previous Christmas.		It is not essential for candidates to do a sixth year if they wish to attend a Scottish university, as they have obtained adequate Higher grades in S5 they may apply and receive acceptance, but this is conditional on being successful in the examinations. However, the vast majority of Scottish students return for S6 if they plan to attend university. Some English universities will also accept Scottish students who have obtained adequate Higher grades in S5. It was announced in December 2008 that, as from 2010, UCAS will increase the number of points awarded to those who achieve Highers and Advanced Highers.[1]		In some cases, particularly in independent schools, the term sixth form is also used for the last two years of secondary education. An increasing number of independent schools are offering their students the International Baccalaureate Programme.		In some secondary schools in Hong Kong, Jamaica, Barbados, Sierra Leone and Trinidad and Tobago, the sixth and seventh years are called Lower and Upper Sixth respectively. In India and Nepal, it is the "+2" part of the "10+2" educational system.		In 2009, Malaysia, which previously used Tingkatan Enam Bawah dan Atas (Lower and Upper Sixth), switched to Pra-Universiti 1 (Pre-University 1, replacing Lower Sixth) and Pra-Universiti 2 (Pre-University 2, replacing Upper Sixth) to reflect that the sixth and seventh years prepare students for university.		Similarly, the term sixth form is also used to define the final two years of education before entering university in Malta.		In Singapore, however, the equivalent of a sixth form college would be called a junior college, where pupils take their Cambridge GCE A-levels after two years. Prior to the 1990s, these two years were known as "Pre-University" (Pre-U) 1 and 2.		In some college preparatory schools in the United States, such as Choate Rosemary Hall, The Hill School, Woodberry Forest School, Ethical Culture Fieldston School, Kent School, Pomfret School, The Church Farm School, The Haverford School, Portsmouth Abbey School and more, sixth form refers to the final year of education prior to college. It is the equivalent of twelfth grade in the US education system.		In New Zealand, under the old system of Forms, Standards and Juniors, Sixth Form was the equivalent of Year 12 in today's system. Year 13 was known as Seventh Form. Australia also sometimes uses the term for Year 12, though the Australian Year 12 is equivalent to the NZ Year 13 / Seventh Form and the UK's Upper Sixth / Year 13.		In Brunei, sixth form comprises Year 12 and 13, which may also be referred to as Lower and Upper Six. At the end of the schooling, students sit for Brunei-Cambridge GCE A Level.[2] Students may also opt to take Advanced Subsidiary Level or AS Level halfway at the end of Lower Six or halfway through Upper Six. Sixth form is not compulsory, but a preferable choice for students wishing to continue in academic studies leading to university level.		
Unschooling is an educational method and philosophy that advocates learner-chosen activities as a primary means for learning. Unschooling students learn through their natural life experiences including play, household responsibilities, personal interests and curiosity, internships and work experience, travel, books, elective classes, family, mentors, and social interaction. Unschooling encourages exploration of activities initiated by the children themselves, believing that the more personal learning is, the more meaningful, well-understood and therefore useful it is to the child. While courses may occasionally be taken, unschooling questions the usefulness of standard curricula, conventional grading methods, and other features of traditional schooling in the education of each unique child.		The term "unschooling" was coined in the 1970s and used by educator John Holt, widely regarded as the father of unschooling.[2] While often considered a subset of homeschooling, unschoolers may be as philosophically separate from other homeschoolers as they are from advocates of conventional schooling. While homeschooling has been subject to widespread public debate, little media attention has been given to unschooling in particular. Critics of unschooling see it as an extreme educational philosophy, with concerns that unschooled children will lack the social skills, structure, and motivation of their schooled peers, while proponents of unschooling say exactly the opposite is true: self-directed education in a natural environment better equips a child to handle the "real world."[3]						A fundamental premise of unschooling is that curiosity is innate and that children want to learn. From this an argument can be made that institutionalizing children in a so-called "one size fits all" or "factory model" school is an inefficient use of the children's time, because it requires each child to learn specific subject matter in a particular manner, at a particular pace, and at a specific time regardless of that individual's present or future needs, interests, goals, or any pre-existing knowledge he or she might have about the topic.		Many unschoolers believe that opportunities for valuable hands-on, community-based, spontaneous, and real-world experiences may be missed when educational opportunities are limited to, or dominated by, those inside a school building.		Unschoolers note that psychologists have documented many differences between children in the way they learn,[4] and assert that unschooling is better equipped to adapt to these differences.[5]		People vary in their "learning styles", that is, the preference in how they acquire new information. However, research has demonstrated that this preference is not related to increased learning or improved performance.[6] Students have different learning needs. In a traditional school setting, teachers seldom evaluate an individual student differently from other students, and while teachers often use different methods, this is sometimes haphazard and not always with regard to an individual student.[7]		Developmental psychologists note that just as children reach growth milestones at different ages from each other, children are also prepared to learn different things at different ages.[8] Just as some children learn to walk during a normal range of eight to fifteen months, and begin to talk across an even larger range, unschoolers assert that they are also ready and able to read, for example, at different ages, girls usually earlier, boys later. In fact, experts have discovered that natural learning produces far greater changes in behavior than do traditional learning methods, though not necessarily an increase in the amount of information learned.[9] Traditional education requires all children to begin reading at the same time and do multiplication at the same time; unschoolers believe that some children cannot help but be bored because this was something that they had been ready to learn earlier, and even worse, some children cannot help but fail, because they are not yet ready for this new information being taught.[10]		Unschoolers sometimes state that learning any specific subject is less important than learning how to learn.[11] They assert, in the words of Holt:		Since we can't know what knowledge will be most needed in the future, it is senseless to try to teach it in advance. Instead, we should try to turn out people who love learning so much and learn so well that they will be able to learn whatever must be learned.[11]		It is asserted that this ability to learn on their own makes it more likely that later, when these children are adults, they can continue to learn what they need to know to meet newly emerging needs, interests, and goals;[11] and that they can return to any subject that they feel was not sufficiently covered or learn a completely new subject.[11]		Many unschoolers disagree that there is a particular body of knowledge that every person, regardless of the life they lead, needs to possess.[12] Unschoolers argue that, in the words of John Holt, "[I]f [children] are given access to enough of the world, they will see clearly enough what things are truly important to themselves and to others, and they will make for themselves a better path into that world than anyone else could make for them."[13]		Parents of unschoolers provide resources, support, guidance, information, and advice to facilitate experiences that aid their children in accessing, navigating, and making sense of the world.[5] Common parental activities include sharing interesting books, articles, and activities with their children, helping them find knowledgeable people to explore an interest with (anyone from physics professors to automotive mechanics), and helping them set goals and figure out what they need to do to meet their goals. Unschooling's interest-based nature does not mean that it is a "hands off" approach to education. Parents tend to involve themselves, especially with younger children (older children, unless new to unschooling, often need less help finding resources and making and carrying out plans).[5]		Unschooling opposes many aspects of what the dominant culture insists are true. In fact, in may be impossible to fully understand the unschooling philosophy without active participation paired with a major paradigm shift. The cognitive dissonance that frequently accompanies this paradigm shift is uncomfortable. New unschoolers are advised that they should not expect to understand the unschooling philosophy at first.[14] Not only are there many commonplace assumptions about education, there are many unspoken and unwritten expectations. One step towards overcoming the necessary paradigm shift is accepting that, "what we do is nowhere near as important as why we do it."[15]		Unschoolers question schools for lessening the parent/child bond and reducing family time and creating atmospheres of fear, or atmospheres that are not conducive for learning and may not even correspond with later success.		Often those in school have a community consisting mainly of a peer group, of which the parent has little influence and even knowledge. Unschoolers may have time to share a role in their greater community, therefore relating more to older and younger individuals and finding their place within more diverse groups of people. Parents of school children also have little say regarding who their instructors and teachers are, whereas parents of unschoolers may be more involved in the selection of the coaches or mentors their children work with and with whom they build lasting and ongoing relationships.		According to unschooling pioneer John Holt, "...the anxiety children feel at constantly being tested, their fear of failure, punishment, and disgrace, severely reduces their ability both to perceive and to remember, and drives them away from the material being studied into strategies for fooling teachers into thinking they know what they really don't know." Proponents of unschooling assert that individualized, child-led learning is more efficient and respectful of children's time, takes advantage of their interests, and allows deeper exploration of subjects than what is possible in conventional education.		Unschoolers may question the school environment as one that is optimal for daily learning. According to Brain Rules by John Medina, "If you wanted to create an education environment that was directly opposed to what the brain was good at doing, you probably would create something like a classroom...." According to the Victorian Institute of Teaching here: [15]		Others point out that some schools can be non-coercive and cooperative, in a manner consistent with the philosophies behind unschooling.[16] Sudbury model schools are non-coercive, non-indoctrinative, cooperative, democratically run partnerships between children and adults, including full parents' partnership, where learning is individualized and child-led, and complements home education.[16]		Success and schooling also show little correlation according to some studies, and this is a subject for debate. In the United States, school often takes a well-rounded approach that may attempt to compensate for students' weaknesses rather than building upon individual strengths and skills that they will eventually utilize professionally. Further, many highly successful people, including US presidents, scientists, actors, writers, inventors, and educators were home-schooled or dropped out of school, suggesting that education is a matter of curiosity and desire rather than academic achievement.[citation needed]		The term "unschooling" probably derives from Ivan Illich's term "deschooling", and was popularized through John Holt's newsletter Growing Without Schooling. In an early essay, Holt contrasted the two terms:		GWS will say 'unschooling' when we mean taking children out of school, and 'deschooling' when we mean changing the laws to make schools non-compulsory...[17]		At this point the term was equivalent with "home schooling" (itself a neologism). Subsequently, home schoolers began to differentiate between various educational philosophies within home schooling. The term "unschooling" became used as a contrast to versions of home schooling that were perceived as politically and pedagogically "school-like," using textbooks and exercises at home, the same way they would be used at school. In 2003, in Holt's book Teach Your Own (originally published in 1981) Pat Farenga, co-author of the new edition, provided a definition:		When pressed, I define unschooling as allowing children as much freedom to learn in the world as their parents can comfortably bear.[18]		In the same passage Holt stated that he was not entirely comfortable with this term, and that he would have preferred the term "living". Holt's use of the term emphasizes learning as a natural process, integrated into the spaces and activities of everyday life, and not benefiting from adult manipulation. It follows closely on the themes of educational philosophies proposed by Jean-Jacques Rousseau, Paul Goodman, and A.S. Neill.		At Holt's death the newsletter GWS ceased. Thereafter a range of unschooling practitioners and observers defined the term in various ways. For instance, the Freechild Project defines unschooling as:		the process of learning through life, without formalized or institutionalized classrooms or schoolwork.[19]		New Mexico homeschooling parent Sandra Dodd proposed the term "Radical Unschooling" to emphasize the complete rejection of any distinction between educational and non-educational activities.[20] Radical Unschooling emphasizes that unschooling is a non-coercive, cooperative practice, and seeks to promote those values in all areas of life. These usages share an opposition to traditional schooling techniques and the social construction of schools. Most emphasize the integration of learning into the everyday life of the family and wider community. Points of disagreement include whether unschooling is primarily defined by the initiative of the learner and their control over the curriculum, or by the techniques, methods, and spaces being used.		Unschooling is a form of home education, which is the education of children at home rather than in a school. Home education is often considered synonymous with homeschooling.		Unschooling contrasts with other forms of home education in that the student's education is not directed by a teacher and curriculum. Unschooling is a real-world implementation of "The Open Classroom" methods promoted in the late 1960s and early 1970s, without the school, classrooms or grades. Parents who unschool their children act as facilitators, providing a range of resources, helping their children access, navigate, and make sense of the world, and aiding them in making and implementing goals and plans for both the distant and immediate future. Unschooling expands from children's natural curiosity as an extension of their interests, concerns, needs, goals, and plans.		Concerns about socialization are often a factor in the decision to unschool. Many unschoolers believe that the conditions common in conventional schools, like age segregation, a low ratio of adults to children, a lack of contact with the community, a lack of people in professions other than teachers or school administration, an emphasis on the smarter children, shaming of the failing children, and an emphasis on sitting, create an unhealthy social environment.[21]		Commonly, unschooling is said to broaden the diversity of people or places, which an unschooler may access, while simultaneously noting that, compared to many student populations, unschoolers may be more selective in choosing peer groups, mentors, etc. Unschoolers cite studies that report that home educated students tend to be more mature than their schooled peers,[21][22][23] and some believe this is a result of the wide range of people they have the opportunity to interact with.[24] Critics of unschooling, on the other hand, argue that unschooling inhibits social development by removing children from a ready-made peer group of diverse individuals.[3][25]		Like other forms of alternative education, unschooling is subject to legal restrictions in some countries and is illegal in others.		Questions about the merits of unschooling raise concerns on its absence of the following qualities, compared to established systems:		Some unschooling families may incorporate the following philosophies into their lifestyles.		Many other forms of alternative education also place a great deal of importance on student control of learning, albeit not necessarily of the individual learner. This includes free democratic schools, like the Sudbury school, Stonesoup School and "open learning" virtual universities.		
The issue of school speech or curricular speech as it relates to the First Amendment to the United States Constitution has been the center of controversy and litigation since the mid-20th century. The First Amendment's guarantee of freedom of speech applies to students in the public schools. In the landmark decision Tinker v. Des Moines Independent Community School District, the U.S. Supreme Court formally recognized that students do not "shed their constitutional rights to freedom of speech or expression at the schoolhouse gate".[1]		The core principles of Tinker remain unaltered, but are tempered by several important decisions, including Bethel School District v. Fraser, Hazelwood School District v. Kuhlmeier, and Morse v. Frederick.[2] Despite respect for the legitimate educational interests of school officials, the Supreme Court has not abandoned Tinker; it continues to recognize the basis precept of Tinker that viewpoint-specific speech restrictions are an egregious violation of the First Amendment.[2] In Rosenberger v. Rector and Visitors of the University of Virginia, the Supreme Court declared: "Discrimination against speech because of its message is presumed to be unconstitutional". Rosenberger held that denial of funds to a student organization on the sole basis that the funds were used to publish a religiously oriented student newspaper was an unconstitutional violation of the right of free speech guaranteed by the First Amendment. Accordingly, for other on-campus speech that is neither obscene, vulgar, lewd, indecent, or plainly offensive under Fraser nor school-sponsored under Hazelwood nor advocating illegal drugs at a school-sponsored event under Frederick, Tinker applies limiting the authority of schools to regulate the speech, whether on or off-campus, unless it would materially and substantially disrupt classwork and discipline in the school.						In Tinker, several students were suspended for wearing black armbands that protested against the Vietnam War.[2]		In Fraser, a high school student was disciplined following his speech to a school assembly at which he nominated a fellow student for a student elective office. The speech contained sexual innuendos, but not obscenity. The Supreme Court found that school officials could discipline the student. In doing so, it recognized that "the process of educating our youth for citizenship in public schools is not confined to books, the curriculum, and the civics class; schools must teach by example the shared values of a civilized social order". Recognizing that one of the important purposes of public education is to inculcate the habits and manners of civility as valued conducive both to happiness and to the practice of self-government, the Supreme Court emphasized that "consciously or otherwise, teachers—and indeed the older students—demonstrate the appropriate form of civil discourse and political expression by their conduct and deportment in and out of class".[3] Under the Fraser standard, school officials look not merely to the reasonable risk of disruption—the Tinker standard—but would also balance the freedom of a student's speech rights against the school's interest in teaching students the boundaries of socially appropriate behavior. Schools have discretion to curtail not only obscene speech, but speech that is vulgar, lewd, indecent, or plainly offensive.		The Hazelwood School District case applies the principles set forth in Fraser to curricular matters. In Hazelwood, the Supreme Court upheld a school's decision to censor certain articles in the school newspaper which was produced as part of the school's journalism curriculum. Echoing Fraser, the Supreme Court observed that "[a] school need not tolerate student speech that is inconsistent with 'its basic educational mission'...even though the government could not censor similar speech outside the school". School authorities and educators do not offend the First Amendment by exercising editorial control over the style and content of student speech in school-sponsored expressive activities so long as their actions are reasonably related to legitimate pedagogical concerns.[4]		Morse v. Frederick blends Fraser and Hazelwood, applying them to a school-sanctioned event or activity.[2] While students were along a public street in front of school watching the Olympic Torch Relay pass through, Frederick unfurled a banner bearing the phrase: "BONG HiTS [sic] 4 JESUS". The banner was in plain view of other students. The high school principal seized the banner and suspended Frederick because the banner was perceived to advocate the use of illegal drugs. The Supreme Court held that a principal may, consistent with the First Amendment, restrict student speech at a school event, when that speech is reasonably viewed as promoting illegal drug use. Not only was a school activity involved, but the banner's promotion of illegal drugs was contrary to the school's policy or mission to prevent student drug abuse.		The right of free speech is not itself absolute: the Court has consistently upheld regulations as to time, place, and manner of speech, provided that they are "reasonable".[5] In applying this reasonableness test to regulations limiting student expression, the Court has recognized that the age and maturity of students is an important factor to be considered.[6][7]		In the school context, the United States Supreme Court has identified three major relevant considerations:[6]		Each of these considerations has given rise to a separate mode of analysis, and in Morse v. Frederick the Court implied that any one of these may serve as an independent basis for restricting student speech.[6]		The problem of disruption is perhaps the most fundamental issue addressed by the courts in student free speech cases.[6]		The second major question addressed by the courts is closely related to, but nevertheless distinct from, the question of disruption. This is the question of speech which is offensive to prevailing community standards by reason of being vulgar, lewd, indecent, racist, or otherwise inappropriate in a school setting.[6] In Bethel School District v. Fraser, the Supreme Court recognized the special responsibility of the public schools to inculcate moral values and to teach students the boundaries of socially acceptable behavior. It therefore permitted a public school to discipline a student for making sexually suggestive remarks in an address to a school assembly, even though the remarks were not obscene in the traditional sense.		The ability to regulate inappropriate speech has been found to be especially important in situations where the student speech may have the appearance of being sponsored or endorsed by the school.[8]		The third major area of concern addressed in student free speech cases is whether a particular instance of student speech may be viewed as impairing the school's ability to carry out its educational mission.[6] This concern arises where the speech in question occurs in connection with a school-sponsored or school-controlled activity but is inconsistent with a legitimate pedagogical concern. In such circumstances, the United States Supreme Court has found that student speech may be regulated. For example, in Hazelwood School District v. Kuhlmeier, it held that a school may exercise control over the content of a student newspaper when it attempts to address issues of divorce and teenage pregnancy; in Morse v. Frederick, it permitted a school to exercise control over the words displayed on a large banner at a school-sponsored event, when those words convey a message promoting the use of illegal drugs.		Other factors are relevant to First Amendment cases generally.[6]		One of these factors is whether the activity sought to be controlled is "pure speech", or sufficiently related to the expression of ideas to fall under the umbrella of the First Amendment. "Pure speech" does not need to involve words but is generally represented by symbols or actions.		The focus of the protected speech activity, whether pure speech or not, may affect the propriety of regulation by school officials.		
A preschool (also nursery school, pre-primary school, kindergarten outside the US and UK) is an educational establishment or learning space offering early childhood education to children, usually between the ages of three and five, prior to the commencement of compulsory education at primary school. They may be privately operated or government run, and one option is to subsidize the costs.						Terminology varies by country. In some European countries the term "kindergarten" refers to formal education of children classified as ISCED level 0 - with one or several years of such education being compulsory - before children start primary school at ISCED level 1.[1]		The following terms may be used for educational establishments for this age group:		In an age when school was restricted to children who had already learned to read and write at home, there were many attempts to make school accessible to orphans or to the children of women who worked in factories.		In 1779, Johann Friedrich Oberlin and Louise Scheppler founded in Strassbourg an early establishment for caring for and educating pre-school children whose parents were absent during the day.[5] At about the same time, in 1780, similar infant establishments were established in Bavaria[6] In 1802, Pauline zur Lippe established a preschool center in Detmold.		In 1816, Robert Owen, a philosopher and pedagogue, opened the first British and probably globally the first infant school in New Lanark, Scotland.[7][8][9] In conjunction with his venture for cooperative mills Owen wanted the children to be given a good moral education so that they would be fit for work. His system was successful in producing obedient children with basic literacy and numeracy.[10]		Samuel Wilderspin opened his first infant school in London in 1819,[11] and went on to establish hundreds more. He published many works on the subject, and his work became the model for infant schools throughout England and further afield. Play was an important part of Wilderspin's system of education. He is credited with inventing the playground. In 1823, Wilderspin published On the Importance of Educating the Infant Poor, based on the school. He began working for the Infant School Society the next year, informing others about his views. He also wrote "The Infant System, for developing the physical, intellectual, and moral powers off all children from 1 to seven years of age".		Countess Theresa Brunszvik (1775–1861), who had known and been influenced by Johann Heinrich Pestalozzi, was influenced by this example to open an Angyalkert ('angel garden' in Hungarian) on 27 May 1828 in her residence in Buda, the first of eleven care centers that she founded for young children.[12][13] In 1836 she established an institute for the foundation of preschool centers. The idea became popular among the nobility and the middle class and was copied throughout the Hungarian kingdom.		Friedrich Fröbel (1782–1852) opened a Play and Activity institute in 1837 in the village of Bad Blankenburg in the principality of Schwarzburg-Rudolstadt, Thuringia, which he renamed Kindergarten on 28 June 1840.		Women trained by Fröbel opened Kindergartens throughout Europe and around the World. The first kindergarten in the United States was founded in Watertown, Wisconsin in 1856 and was conducted in German.[14] Elizabeth Peabody founded America's first English-language kindergarten in 1860 and the first free kindergarten in America was founded in 1870 by Conrad Poppenhusen, a German industrialist and philanthropist, who also established the Poppenhusen Institute and the first publicly financed kindergarten in the United States was established in St. Louis in 1873 by Susan Blow. Canada's first private kindergarten was opened by the Wesleyan Methodist Church in Charlottetown, Prince Edward Island in 1870 and by the end of the decade, they were common in large Canadian towns and cities.[15][16] The country's first public-school kindergartens were established in Berlin, Ontario in 1882 at Central School.[17] In 1885, the Toronto Normal School (teacher training) opened a department for Kindergarten teaching.[17]		Elizabeth Harrison wrote extensively on the theory of early childhood education and worked to enhance educational standards for kindergarten teachers by establishing what became the National College of Education in 1886.		Head Start was the first publicly funded preschool program in the US, created in 1965 by President Johnson for low-income families—only 10% of children were then enrolled in preschool. Due to large demand, various states subsidized preschool for low-income families in the 1980s.		The most important years of learning begin at birth.[18] During these early years, humans are capable of absorbing more information than later on. The brain grows most rapidly in the early years. High quality teachers and preschools can have a long-term effect on improving outcomes for disadvantaged students.[19][20]		The areas of development that preschool education covers varies. However, the following main themes are typically offered.[21][22]		Preschool systems observe standards for structure (administration, class size, student–teacher ratio, services), process (quality of classroom environments, teacher-child interactions, etc.) and alignment (standards, curriculum, assessments) components. Curriculum is designed for differing ages. For example, counting to 10 is generally after the age of four.[23]		Some studies dispute the benefits of preschool education,[24] finding that preschool can be detrimental to cognitive and social development.[25][26] A study by UC Berkeley and Stanford University on 14,000 preschools revealed that while there is a temporary cognitive boost in pre-reading and math, preschool holds detrimental effects on social development and cooperation.[27] Research has also shown that the home environment has a greater impact on future outcomes than preschool.[18]		There is emerging evidence that high quality preschools are "play based," rather than attempting to provide early formal instruction in academic subjects. “Playing with other children, away from adults, is how children learn to make their own decisions, control their emotions and impulses, see from others’ perspectives, negotiate differences with others, and make friends,” according to Dr. Peter Gray, Boston College professor and an expert on the evolution of play and its vital role in child development. “In short, play is how children learn to take control of their lives.”[28]		Preschools have adopted various methods of teaching, such as Montessori, Waldorf, Head Start, HighScope,[29] Reggio Emilia approach, Bank Street and Forest kindergartens.		While a majority of American preschool programs remain tuition-based, support for some public funding of early childhood education has grown over the years. As of 2008, 38 states and the District of Columbia invested in at least some preschool programs, and many school districts were providing preschool services on their own, using local and federal funds.[30] The United States spends .04% of its GDP or $63 billion on preschool education.[4]		The benefits and challenges of a public preschool reflect the available funding. Funding can range from federal, state, local public allocations, private sources, and parental fees.[31] The problem of funding a public preschool occurs not only from limited sources but from the cost per child. As of 2007, the average cost across the lower 48 states was $6,582.[32] Four categories determine the costs of public preschools: personnel ratios, personnel qualifications, facilities and transportation, and health and nutrition services. These costs depend heavily on the cost and quality of services provided.[33] The main personnel factor related to cost is teacher qualifications. Another determinant of cost is the length of the school day. Longer sessions cost more.[32]		Collaboration has helped fund programs in several districts. Collaborations with area Head Start and other private preschools helped fund a public preschool in one district. "We’re very pleased with the interaction. It’s really added a dimension to our program that’s been very positive".[34] The National Head Start Bureau has been looking for more opportunities to partner with public schools. Torn Schultz of the Bureau states, "We’re turning to partnership as much as possible, either in funds or facilities to make sure children get everything necessary to be ready for school".[35]		The Universal Preschool movement is an international effort to make preschool available to families, as it is for primary education. Various jurisdictions and advocates have differing priorities for access, availability and funding sources.		In the United States, most preschool advocates support the National Association for the Education of Young Children's Developmentally Appropriate Practices.[citation needed]		The National Association for the Education of Young Children (NAEYC) and the National Association of Child Care Professionals (NACCP) publicize and promote the idea of developmentally appropriate practice, although many institutions have not taken that approach. NAEYC claimed that although 80% of kindergarten classrooms claim to be developmentally appropriate, only 20% actually are.[citation needed]		Main article: Curricula in early childhood care and education		Curricula for pre-school children have long been a hotbed for debate. Much of this revolves around content and pedagogy; the extent to which academic content should be included in the curriculum and whether formal instruction or child-initiated exploration, supported by adults, is more effective.[36] Proponents of an academic curriculum are likely to favour a focus on basic skills, especially literacy and numeracy, and structured pre-determined activities for achieving related goals. Internationally, there is strong opposition to this type of Early childhood care and education curriculum and defence of a broad-based curriculum that supports a child’s overall development including health and physical development, emotional and spiritual well-being, social competence, intellectual development and communication skills.[37] The type of document that emerges from this perspective is likely to be more open, offering a framework which teachers and parents can use to develop curricula specific to their contexts.[38]		Preschool education, like all other forms of education, is intended by the society that controls it to transmit important cultural values to the participants. As a result, different cultures make different choices about preschool education. Despite the variations, there are a few common themes. Most significantly, preschool is universally expected to increase the young child's ability to perform basic self-care tasks such as dressing, feeding, and toileting.[39]		The study of early childhood education (ECE) in China has been intimately influenced by the reforms and progress of Chinese politics and the economy. Currently, the Chinese government has shown interest in early childhood education, implementing policies in the form of The Guidance for Kindergarten Education (Trial Version) in 2001 and The National Education Reform and Development of Long-Term Planning Programs (2010–2020) in 2010. It has been found that China’s kindergarten education has dramatically changed since 1990. In recent years, various Western curricula and pedagogical models have been introduced to China, such as Montessori programs, Reggio Emilia, Developmentally Appropriate Practice (DAP), and the Project Approach. Many kindergartens have faced difficulties and challenges in adapting these models in their programs. Therefore, a heated debate about how the Western curricula can be appropriated in the Chinese cultural context has been initiated between early childhood researchers and practitioners. Research has revealed that the most important aim for promoting curriculum reform is to improve kindergarten teachers’ professional knowledge, such as their understanding of the concept of play and pedagogy, and perceptions of inclusion and kindergarten-based curriculum. Furthermore, within the process of reform, family education and family collaborations cannot be ignored in child development. Early childhood education in China has made dramatic progress since the 1980s. In Tobin, et al. 2009, which studies across three cultures, the continuity and change across the systems of early childhood education are evident. The project report Zhongguo Xueqian Jiaoyu Fazhan Zhanlue Yanjiu Ketizu 2010 reflects upon the development of China’s early childhood education and locates the current situation of the development of early childhood education. The historical development of Chinese early childhood education indicates three distinct cultural threads, including traditional culture, communist culture, and Western culture, that have shaped early childhood education in China, as demonstrated in Zhu and Zhang 2008 and Lau 2012. Furthermore, currently, administrative authorities intend to establish an independent budget for the ECE field in order to support early childhood education in rural areas (Zhao and Hu 2008). A higher quality of educational provisions for children living in rural areas will be another goal for the Chinese government. Many researchers have detailed the important issues of early childhood education, especially teacher education. The exploratory study in Hu and Szente 2010 (cited under Early Childhood Inclusive Education) has indicated that Chinese kindergarten teachers hold negative attitudes toward inclusion of children with disabilities, as they do not have enough knowledge and skills for working with this population. This indicates that kindergarten teachers need to improve their perceptions of children with disabilities. Furthermore, Gu 2007 has focused on the issues of new early childhood teachers’ professional development and puts forward some feasible suggestions about how new teachers deal with key events in their everyday teaching practices. With regard to families’ support of their children’s early development at home, family education should be focused and the collaborative partnership between kindergarten and family needs to be enhanced. Teachers’ attitudes toward family intervention are a vital aspect of teacher-family collaboration. Therefore, kindergarten teachers should support family members in their role as the child’s first teacher and build collaborative partnerships with family, as presented in Ding 2007. Furthermore, kindergarten teachers should be considered as active researchers in children’s role play. This supports the co-construction of their teaching knowledge in relation to children’s initiation/subjectivity in role play (Liu, et al. 2003).		Preschool education is starting in Turkey at the age of 5 while primary level education is starting at the age of 6.		In Japan, development of social skills and a sense of group belonging are major goals. Classes tend to have up to 40 students, to decrease the role of the teacher and increase peer interactions. Participation in group activities is highly valued, leading some schools to for example, count a child who is standing still near a group an exercise session as participating. Children are taught to work harmoniously in large and small groups, and to develop cooperativeness, kindness and social consciousness. The most important goal is to provide a rich social environment that increasingly isolated nuclear families do not provide; unstructured play time is valued.		Children are allowed to resolve disputes with each other, including physical fighting. Most behavioral problems are attributed to the child's inappropriately expressed emotional dependency. Remedies involve accepting the child, rather than treatment with drugs or punishment. Japanese culture attributes success to effort rather than inborn talent, leading teachers to ignore innate differences between children by encouraging and praising perseverance. They work to ensure that all students meet the standard rather that each reaches his or her own potential. Although preschools exhibit great variety, most target age-appropriate personal development, such as learning empathy, rather than academic programs. Academic programs tend to be more common among Westernized and Christian preschools.[40]		Children in North Korea are taught to enjoy military games and to hate the miguk nom, or "American bastard."[41]		In the United States, nursery school is provided in a variety of settings. In general, pre-school is meant to promote development in children through planned programs. Pre-school is defined as: "center-based programs for four-year olds that are fully or partially funded by state education agencies and that are operated in schools or under the direction of state and local education agencies".[42] Pre-schools, both private and school sponsored, are available for children from ages three to five. Many of these programs follow similar curriculum as pre-kindergarten.		In the United States, preschool education emphasizes individuality. Children are frequently permitted to choose from a variety of activities, using a learning center approach. During these times, some children draw or paint, some play house, some play with puzzles while some listen to the teacher read a story aloud. Activities vary in each session. Each child is assumed to have particular strengths and weaknesses to be encouraged or ameliorated by the teachers. A typical belief is that "children's play is their work" and that allowing them to select the type of play, the child will meet his or her developmental needs. Preschools also adopt American ideas about justice, such as the rule of law and the idea that everyone is innocent until proven guilty. Teachers actively intervene in disputes and encourage children to resolve them verbally ("use your words") rather than physically. Children may be punished with a time out or required to apologize or make reparations for misbehavior. Teachers assist children to explain what happened, before any decision to punish is made. Self-expressive language skills are emphasized through informal interactions with teachers and through structured group activities such as show and tell exercises to enable the child to describe an experience to an adult. Resources vary depending on the wealth of the students, but generally are better equipped than other cultures. Most programs are not subsidized by government, making preschools relatively expensive even though the staff is typically poorly compensated. Student-teacher ratios are lower than in other cultures, ideally about 15 students per group. Parents and teachers see teachers as extensions of or partial substitutes for parents and consequently emphasize personal relationships and consistent expectations at home and at school.[43]		In the United States, students who may benefit from special education receive services in preschools. Since the enactment of the Individuals with Disabilities Education Act (IDEA) Public Law 101-476 in 1975 and its amendments, PL 102-119 and PL 105-17 in 1997, the educational system has moved away from self-contained special education classrooms to inclusion, leading special education teachers to practice in a wider variety of settings. As with other stages in the life of a child with special needs, the Individualized Education Plan (IEP) or an Individual Family Service Plan (IFSP) is an important way for teachers, administrators and parents to set guidelines for a partnership to help the child succeed in preschool.		The goal of Head Start and of Early Head Start is to increase the school readiness of young children in low income families. These programs serve children from birth to age five, pregnant women, and their families. Head Start was started by the Federal Government in 1964 to help meet the needs of disadvantaged pre-school children.		The office of Economic Opportunity launched Project Head Start as an eight-week summer program in 1965. It was then transferred to the Office of Child Development in the US Department of Health, Education, and Welfare in 1969. Today it is a program within the Administration on Children, Youth and Families in the Department of Health and Human Services. Programs are administered locally by school systems and non-profit organizations.		However, a rigorous preschool can be developmentally detrimental to children and cause social, emotional, and educational problems later in life. Although an essential based preschool is not focused on academics and kindergarten routines, children learn a lot more valuable lessons that they will use for the rest of their life. Research has shown that of the two an essential based preschool is the better option for children, because of their specific ways of learning.		In the UK, pre-school education in nursery classes or schools is fully funded by local government for children aged between two and four. Pre-school education can be provided by childcare centres, playgroups, nursery schools and nursery classes within primary schools. Private voluntary or independent (PVI sector) nursery education is also available throughout the UK and varies between structured pre-school education and a service offering child-minding facilities.		Nursery in England is also called FS1 which is the first year of foundation before they go into primary or infants.		The curriculum goals of a nursery school are more specific than for childcare, but less strenuous than for primary school. For example, the Scottish Early Years Framework[44] and the Curriculum for Excellence[45] define expected outcomes even at this age. In some areas, the provision of nursery school services is on a user pays or limited basis while other governments fund nursery school services.		Each child in England at the first school term after their third birthday, is entitled to 15 hours per week free childcare funding. This entitlement is funded by the government through the local council.[46] Pre-schools in England follow the Early Learning Goals, set by the Early Years Foundation Stage,[47] for education produced by the Department for Children, Schools and Families which carries on into their first year of school at the age of four. This year of school is usually called Reception. The Early Learning Goals cover the main areas of education without being subject driven. These areas include[48]		Until the mid-1980s, nursery schools only admitted pupils in the final year (three terms) leading up to their admission to primary school, but pupils now attend nursery school for four or five terms. It is also common practise for many children to attend nursery much earlier than this. Many nurseries have the facilities to take on babies, using the 'Early Years Foundation Stage', framework as a guide to give each child the best possible start to becoming a competent learner and skilful communicator.		Early years education in Wales is provided half-time for children aged 3–4 (Nursery) and full-time for those between the ages of 4 and 5 (Reception). Since 2005 it has been a statutory duty for all Local Education Authorities to secure sufficient nursery education in their area for children from the term following their third birthday.		Currently, the Early Years curriculum in Wales, produced by the Welsh Assembly Government Department for Children, Education, Lifelong Learning and Skills,is set out in the booklet "Desirable Outcomes for Children's Learning Before Compulsory School Age".[49] However, a new 'Foundation Phase' covering 3-7 year olds is being rolled out across Wales from 2008, with a focus on 'learning through play',[50] which covers seven areas of learning:		In Northern Ireland funded Nursery School places can be applied for from ages 3 and up. Preschool education is delivered also by PreSchools, also referred to as Playschools or Playgroups. A Nursery School is allowed to enrol up to 26 children into a class, with the curriculum being delivered by a qualified teacher and a Nursery Assistant. A preschool, which delivers the same curriculum, is also permitted to admit a maximum of 26 children to any single session. However, the regulations for personnel differ. The Preschool must have a Supervisor with an NVQ 3 qualification in Child Care (or Equivalent). There must be one qualified and vetted adult for every 8 children. Funding is applied for through PEAGs (Preschool Education Advisory Group). Both nursery and preschool settings are inspected by the Education and Training Inspectorate. Preschools are also subject to inspection by local Social Services.		Starting in the year of 2010, Ireland passed a law stating that all children of the age 3 years and 2 months and less than 4 years and 7 months are qualified to attend a preschool free of charge. Before this law was passed there was a large amount of children who did not attend an Early Childhood Education Program. The programs that were offered operated voluntary and required the parents to pay a steep fee per child. This left many families with no option but to keep the kids at home. The government soon realized that a large amount of children were having trouble in their first years of primary school and parents were having to stay home becoming jobless. Once the government issued the free preschool scheme, Ireland's preschool enrollment rate increased to about 93%.[51][52]		In Scotland children are entitled to a place in a nursery class when they reach their third birthday. This gives parents the option of two years of funded pre-school education before beginning primary one, the first year of compulsory education. Nursery children who are three years old are referred to as ante-pre-school whilst children who are four years old are termed pre-school. Pre-school education in Scotland is planned around the Early Level of the Curriculum for Excellence which identifies Outcomes & Experiences around the following eight curricular areas:		Responsibility for the review of care standards in Scottish nurseries rests with the Care Commission.		Davidson, Dana H.; Tobin, Joseph Jay; Wu, David Y. H. (1989). Preschool in three cultures: Japan, China, and the United States. New Haven, Conn: Yale University Press. pp. 188–221. ISBN 0-300-04235-3. 		hu∫		 h																																																																																																																																										jk		
The Integrated Programme (IP) is a scheme that allows high-performing students in secondary schools in Singapore to skip the GCE Ordinary Level (O-level) examination (typically taken by students at the end of their fourth or fifth year in secondary school) and proceed to sit for the GCE Advanced Level (A-level) examination, International Baccalaureate (IB), or an equivalent examination, after six years of secondary education. The A-level examination is typically taken by students at the end of their second or third year in junior college.						The programme allows for more time allocated to enrichment activities. That is, without the O-level examinations, the students have more time and flexibility to immerse themselves in a more broad-based education which will eventually lead to the A-level examination. In addition, the students enjoy more freedom in the combination of subjects. Generally, only the top performers are eligible to be part of the IP programme, do use to implement it, as it is currently regarded as experimental. Thus most of the main body of the students pursue their secondary education at the current pace by first completing a four-year O-level course before proceeding to a two-year A-level education.		The integrated programme was first implemented in Hwa Chong Institution (formerly The Chinese High School), Nanyang Girls' High School, Raffles Girls School and Raffles Institution in 2004.[citation needed]		The IP allows students to skip the O-level at secondary four and be admitted directly to junior colleges. All the schools allowed in the scheme accepts the top 10% of the national cohort. This ensures that students who are under the IP are able to cope with their A-level after bypassing their O-level.[citation needed]		Some junior colleges including National Junior College, Temasek Junior College and Victoria Junior College, offer it independently. VJC is now offering it with Cedar Girls Secondary and Victoria School (more info below)		For the four-year IP, secondary two students from various schools are allowed to apply for this programme. These students have their secondary three and four education in the junior college itself, followed by the A-level course.		Since 2009, National Junior College has also accepted students who have taken the Primary School Leaving Examination (PSLE), making it a six-year program.		Dunman High School applied for the IP system in mid-2004 standalone, and the Ministry of Education approved the first batch to be enrolled in 2005, with Year 1 and Year 3 students, each having a cap of about 135 out of 380 students. Full IP was granted in late 2005, and the school went full IP at the beginning of 2006.		River Valley High School joined the IP system in 2006 by operating a six-year course standalone.		Since 2012, Victoria School and Cedar Girls' Secondary School are offering the IP with Victoria Junior College which will build upon the four-year Victoria Integrated Programme (VIP) in the junior college. It is [called the VCA IP( Victoria Cedar Alliance Integrated Programme).		In January 2013, the Joint Integrated Programme, commonly referred to as the JIP, offered by Catholic High School, CHIJ Saint Nicholas Girls' School and Singapore Chinese Girls' School, alongside with Eunoia Junior College, was established. The four schools are the newest additions to the Integrated Programme landscape.		The three secondary schools will continue to offer the 'O' Level track alongside the IP track, thus they are termed by the MOE as dual track schools. Such schools allow students to have the flexibility to switch to the stream that is better suited to the students' needs.		Anglo-Chinese School (Independent) and St Joseph's Institution, the only IP leading to IB schools in the country, have allowed their students to abandon the British system and "go Swiss" by dropping the GCE examinations all together and adopting the International Baccalaureate (IB), having been fully authorized as IB World Schools. The IB is generally regarded as a better track to go on if the students have plans to study overseas in future. The IB allows students to take both arts and science subjects as well as philosophy courses and extensive research papers. Anglo-Chinese School (Independent) and St Joseph's Institution are currently the only secondary schools in Singapore to offer the IP which leads to the International Baccalaureate examinations, and Anglo-Chinese School(Independent) is regarded as one of the top schools in terms of results of the IB in the world, having averages as high as 42 out of a total of 45 points. St Joseph's Institution started their IP in 2013.		The NUS High School of Mathematics and Science is a school which specialises in math and science, but also aims to develop all-rounded students through its diploma curriculum, which allows rigour and depth, or flexibility and breadth. NUS High School is also affiliated and very closely linked to the National University of Singapore (NUS). Students in this school graduate with the NUS High School diploma, which has been accredited by NUS, Nanyang Technological University (NTU), and Singapore Management University (SMU), and is still in the process of gaining more recognition from universities and colleges in other countries. For placement into overseas universities, they also take the Scholastic Assessment Test (SAT) and Advanced Placement (AP).		Some people[who?] have raised a point in the Singapore newspaper The Straits Times that since the A-level system follows a structured format, some IP students may be at a disadvantage as compared to their mainstream counterparts even though they may have a higher intellectual capacity.		The success of an IP student is based on an assumption that students are self-disciplined enough to ensure that they manage their time well and be diligent in their studies, so that they will remember all the core content taught to them and yet find enough time to engage actively in independent learning.[3]		However, this may be considered a utopian ideal. Without an important watershed intervening national examination to help them focus, students may simply let their guard down.[3]		This programme is allegedly for clearly university-bound students. It can thus be inferred that if an IP student under-performs in the A-level examination, a rare case, because the number of IP retainees is small, the student will face drastic consequences. For non-IP students who fail to perform well in the A-level, they still have their O-level qualifications, which act as a "safety net". However, in the absence of this "safety net", IP students who under-perform in the A-level will have only their Primary School Leaving Examination (PSLE) certificate to fall back on. [3]		Hwa Chong Institution is one of the first institutions to offer joint Integrated Programme in 2005.		National Junior College is one of the first junior colleges in Singapore to offer its independent Integrated Programme in 2005.		The Performing Arts Centre of Dunman High School, one of the Special Assistance Plan schools with Integrated Programme that enriches students with culture and the arts.		
Student protest encompasses a wide range of activities that indicate student dissatisfaction with a given political or academic issue and mobilization to communicate this dissatisfaction to the authorities (university or civil or both) and society in general and hopefully remedy the problem. Protest forms include but are not limited to: sit-ins, occupations of university offices or buildings, strikes etc. More extreme forms include suicide such as the case of Jan Palach's and Jan Zajíc's protests against the end of the Prague Spring and Kostas Georgakis' protest against the Greek military junta of 1967–1974.						A common tactic of student protest is to go on strike (sometimes called a boycott of classes), which occurs when students enrolled at a teaching institution such as a school, college or university refuse to go to class. It is meant to resemble strike action by organized labour. The purpose of these strikes is often to put pressure on the governing body of the university, particularly in countries where education is free, and the government cannot afford to have a student cohort miss an entire year. This can cause an overload of students in one academic term and the absence of an entire class in the following term.		In the West, student strikes date to the early days of universities in the Middle Ages, with one of the earliest and most significant being the University of Paris strike of 1229, which lasted two years and yielded significant concessions. In more recent times, significant walkouts occurred in the late 1960s and early 1970s: the French May 1968 uprisings began as a series of student strikes. The largest student strike/boycott in American history occurred in May and June 1970, in the aftermath of the American invasion of Cambodia and the killings of student protesters at Kent State University in Ohio. An estimated four million students at more than 450 universities, colleges and high schools participated in the Student Strike of 1970.[1][2]		The term "student strike" has been criticized as inaccurate by some unions[3] and commentators in the news media.[4] These groups have indicated that they believe the term boycott is more accurate.[3][4]		
Student orientation or new student orientation (often encapsulated into an Orientation week, Frosh Week, Welcome Week[1] or Freshers' Week) is a period of time at the beginning of the academic year at a university or other tertiary institution during which a variety of events are held to orient and welcome new students. The name of the period varies by country.		Although usually described as a week, the length of this period varies widely from university to university and country to country, ranging from about three days to a month or even more (e.g. four or five weeks, depending on program, at Chalmers). The length of the week is often affected by each university's tradition as well as financial and physical constraints. During this period, students participate in a wide range of social activities, including live music and other performances, sports challenges, stunts, and open-air markets.						The week before the term starts is known as: Frosh (or frosh week) in some[citation needed] colleges and universities in Canada. In the US, most call it by the acronym SOAR for Student Orientation And Registration;[2] Freshers' week in the majority of the United Kingdom and Ireland and Orientation week or O-week in countries such as Australia, South Africa and New Zealand, and also in many Canadian universities. In Sweden, it is known as nollning (from nolla, "zero", in this case meaning the students have not earned any credit points yet) or inspark (being "kicked in" to university life). Orientation week is the coming phrase[clarification needed] in the United States. Some schools use the acronym WOW for Week of Welcome.		In Canada, first year students are called "Frosh" or "first years". The terms "freshies" and "freshers" are also emerging. In the United States, first year university students are typically[citation needed] referred to as freshmen. In Australia and New Zealand first year students are known simply as "first years", although in some the colleges of the University of Melbourne and the University of Sydney they are also called "Freshers". In the U.K. and Ireland first year students are known as freshers or first years. Freshies is also an emerging term in New Zealand. In Sweden, the student is a nolla (a "zero") during the orientation period and usually upgraded to the status of an etta (student who is in her/his first college term) at a ceremony involving a fancy three-course dinner and a lots of singing.		In Australia, some universities require students to arrive at university a week before classes start in order to gain course approval. This also allows students a chance to orient themselves to student life without the pressure of lectures—hence the term Orientation week is used to describe this week of induction into university life.		In Australian universities, such as the University of Melbourne, University of New South Wales and University of Sydney, the last or second last night is usually celebrated with a large-scale event such as a famous band playing at an entertainment venue on campus. This is generally followed by continued partying and drinking, especially among students living in residential colleges such as Janet Clarke Hall and Ormond College.		The Adelaide University O-Week[3] runs from Monday to Thursday in the week before lectures begin. During O-Week sporting clubs and societies set up a variety of tented areas where clubs display their activities. The Adelaide University Union coordinates a variety of events centering around beer, bands and barbecues on the lawns near the Union complex. A major event for the week is the O-Ball (live entertainment and licensed areas) which takes place in the Cloisters (Union House). The O-Ball attracts many thousands of revellers, not all of whom are Adelaide University students. In recent times Sports and Clubs have sought to distance themselves from the student union and student association controlled activities and have set themselves up on the Maths lawns.		The Australian National University has a full week (Sunday to Sunday)[4] of events, parties and social activities open to all students of the university, organised by the Australian National University Students Association. The residential colleges often have their own "O-week" activities catered primarily for residents as well as the annual "Burgmann Toga Party" held at Burgmann College open to students from all residential colleges. "Burgmann Toga" is the largest party held at a university residence in the Southern Hemisphere.		In Canada, the nature and length of orientation week varies considerably between Universities. For instance, Ottawa, has two universities within its urban centre; the University of Ottawa and Carleton University, both with orientations spanning approximately 7 days. At The University of Ottawa, Frosh Week is Called 101 week. At Carleton University there are multiple orientations, SPROSH (Sprott Frosh), ENG Frosh, Radical Frosh, and the largest, CUSA/RRRA/SEO Frosh. In the province of Quebec, because of the CEGEP system, "froshies" are of legal drinking age and Frosh activities may include the option to drink alcohol. Moreover, the proximity of the two Ottawa universities also allows them to take advantage of the drinking age in neighbouring Gatineau, Quebec. The University of British Columbia cancels the first day of class for all students, and hosts an orientation day for new students, called Imagine Day. As of 2007, the Faculty of Science also holds an annual, day-long Science Frosh event for approximately 300 first-year students, while the commerce faculty holds a 3-day-long frosh weekend before classes begin. The University of Toronto has a number of different "Frosh Weeks" organized concurrently by different student groups within the university; including college societies, professional faculties (perhaps the best known being organized by Engineering Society, Skule (engineering society), in which 'F!ROSH' and 'F!ROSH Leedurs' dye their bodies purple) and the University of Toronto Students' Union. Similarly, Ryerson University also has a number of "Frosh Weeks" organized by different student groups, although it also has a central frosh team known as the 'Ryerson Orientation Crew'. At the Friday of frosh week, the Ryerson Students' Union holds a concert that is free for all Ryerson students; the headliners for the 2015 concert included Drake and Future. McMaster University also organizes many events during what they term "Welcome Week". The week strongly encourages solidarity, first with members of one's own residence or for off-campus students, and later the members of a student's faculty. University of Guelph holds many orientation activities for its incoming students. The main event is the pep rally in which students from each residence perform a dance on the football field. The Guelph Engineering Society also hosts a series of special events for Engineering Frosh including frosh olympics, beach day, and a scavenger hunt. Western University hosts the largest orientation program in Canada, involving 1200 student volunteers and an entire week of activities. St Thomas University, in Fredericton, New Brunswick, hosts a week-long event including activities for each residence and activities for new students. As a rule, Frosh week at Queen's University is so secretive and confidential that no one knows what happens during the week long adventure except Queen's Students and Alumni. Wilfred Laurier University has by definition a Lit[disambiguation needed] orientation week.		In Finnish universities, the student organizations for each department independently organize orientation activities for the new students in their respective departments. New students are often assigned in groups to an upperclassman tutor and participate in many activities with their tutoring group. New students may be referred to as piltti (child), fuksi (freshman), fetus or other names according to their major subject. Activities for new students may include "orienteering", pub crawls, sporting events, swimming in fountains or other forms of "baptism", sitsit parties and saunas, often done wearing homemade fancy-dress costumes. It is also considered important for the new students to participate in the regular activities of the student department organizations.		In past years a typical orientation may consist of verbal harassment as well as initiation leading to humiliation. An orientation of freshers in Indonesia is usually called OSPEK for some universities and MOS in middle and high school. Orientations in Indonesia has event organizers that consists of seniors and the presidium of universities. The most basic form of orientation in Indonesia consist of an educational board run and introduction of campus cultural behavior.[5] What makes orientation in Indonesia (for some universities and schools) distinctive to other countries would arguably be the freshmens' requirement to wear unusual accessories or hairstyles (i.e. Freshmens were asked to wear hats made of bird's nest, necktie made of folded paper, military hairstyle for male students or intricate braids for females, and the usage of a sack instead of a rucksack). Harsh physical punishments were not uncommon during the Suharto era, and mass media continues to report inhumane activities during those orientation that led to a few cases of death.		Nowadays, however, orientation is more tolerable as physical abuse is now forbidden by the law, however it is still criticized by many psychologists and people as 'too much' because of excessive verbal harassment like dissing and insulting the juniors, and the usage of unusual and humiliating attributes typically found in orientations on Junior High and High Schools. As well, it is also criticized by many parents for being economically inconvenient. The reason cited by psychologists is that orientation is often used as a tool of revenge done by the board of organizers for what the seniors did to them during their freshman year.[6] And because of this there are so many people who believes that "MOS" or "OSPEK" is a useless traditions that needs to be erased.[7][8] The 'cruelty' of MOS and OSPEK varies between universities and schools in Indonesia,[9] although in (most) major universities and institutes that kind of humiliation and harassment doesn't exist anymore, or greatly limited to pending applicants or pledges for certain campus organizations.[10]		As in Australia, in New Zealand students have a week to orient themselves to university life before the start of formal classes. This orientation week is a time for many social events, and is often a reason for alcohol fests.[11] Flat warmings are often held within the time limit to couple the alcohol oriented event with the general party week.		In New Zealand's main university towns such as Dunedin and Palmerston North (where students make up around one fifth of the population) orientation week leads a wide range of events. Many top overseas and local bands tour the country at this time, and the orientation tour is one of the highlights of the year's music calendar. The University of Otago in the Scottish-settled city of Dunedin traditionally holds a parody of the Highland Games called the Lowland Games, including such esoteric events as porridge wrestling.		Student pranks were once common during orientation week, but have fallen out of favour in recent years.[citation needed] Until recent years, many halls of residence also inducted new residents with "Initiation" (a form of hazing, though considerably milder than the rituals found among American college fraternities).		Although officially designated as a week, in several New Zealand universities and polytechnics orientation week stretches to over ten days.		Most Swedish universities have some kind of nollning ("zeroing") or "inspark" ("kicking-in"). This is most extensive at the technical faculties and at the student nation communities of Uppsala and Lund. Since student union membership was mandatory in Sweden (until July 2010), the nollning is usually centrally organized from the student union with support from the universities.		At the old universities, these traditions have often turned civilized after a dark history of hazing. Today, many student unions have strict rules against inappropriate drunkenness, sexual harassment and other problematic behaviour.		At the technical faculties, the people who organize the nollning play roles in a theatrical manner and often wear sunglasses and some form of weird clothes. Most senior students who are mentors during the nollning wear their student boilersuits or the b-frack (a worn tailcoat). This kind of organized nollning developed at KTH and Chalmers and spread to the rest of the country.		In Thailand, the activity is commonly called rapnong (รับน้อง), translated as "welcoming of freshmen". It takes place in the first week or month of the academic year at universities and some high schools. The purpose is to adapt new students to university culture. Activities include games, entertainment and recreation. These let the newcomers get to know other members of the university and reduce tension in the changing environment. It sometimes includes alcohol. The main object is to let juniors carry on the universities' tradition and identity and to bind together the new generation into one. Long-term activity often includes seniors taking freshman or older years to meals and meetings, usually the most senior pays for it all. Hazing is a concern in this activity, as many students have been humiliated, abused, and dehumanized by their upperclassmen.		For over 50 years, SOTUS – a Hazing based system used for college initiation in Thailand – has been involved in Thai universities. It stands for Seniority, Order, Tradition, Unity, and Spirit.[12] It is the system for freshmen to bring harmony to their friends and to show their pride through their institute. By seniors, freshmen have to do activities such as singing university songs. Moreover, freshmen are required to do a lot of things; for example, wear a nametag, and show respect to seniors. These requirements lead seniors to try to make their juniors do what they desire and punish them if they don't do seniors' orders.		Presently, there are adolescents and adults opposing those who had committed unethical or deadly actions to juniors. This group of adolescents has distributed "Anti-SOTUS"[13] group and it becomes one of the main issues in Thailand recently. They consider the SOTUS system to be "old-fashioned and source of brutality". Since it was established, this has become the group of people who share their opinions about SOTUS system based on how they have encountered it.		On the other hand, some seniors that support this system resisting the anti SOTUS attitude for many years. They tend to say that SOTUS makes them get along together and feel proud of themselves by becoming part of their institute. Some seniors, however, coerce their freshmen to attend every activity held by them as parts of preparing them to be able to live happily in university. These become worse when some freshmen suffer from what their senior have done to them.		In Thai society, news related to this system has been reported almost every year. For example, recent news about a male freshman[14] who died in this tradition. This news has resulted in people thinking that rapnong should end or, at least, be controlled.		As well as providing a chance to learn about the university, Freshers' week allows students to become familiar with the representatives of their Student Union and to get to know the city or town which is home to the university, often through some form of pub crawl (the legal drinking age is 18 in the UK).		Live music is also common, as are a number of organized social gatherings especially designed to allow freshers to make new friends and to get to know their course colleagues. Because of the intensity of activities, there are often many new friendships made, especially in group accommodation, some not lasting past Freshers' Week and others lasting for the whole University career and longer.		Typically a Freshers' Fair for student clubs and societies is included as part of the activities to introduce new students to facilities on offer, typically outside their course of study, such as societies, clubs and sports. The various societies and clubs available within the University have stalls and aim to entice freshers to join. Most campuses take the opportunity to promote safe sex to their students and sometimes offer leaflets on the subject and free condoms, as well as promoting the Drinksafe campaign. The aim is to lower the rate of sexually transmitted disease and to reduce the level of intoxication commonly witnessed in Freshers' Week.		Freshers' Flu is a predominately British term which describes the increased rates of illness during the first few weeks of university. Although called Freshers' Flu, it is often not a flu at all.		"Freshmen" is the traditional term for first-year students arriving at school in the United States, but the slang term 'frosh'[15] is also used. Due to the perceived gender exclusiveness of the term, some institutions including the University of North Carolina have adopted "first-year student" as the preferred nomenclature.[16] Lasting between a few days and a week, the orientation is these students' informal introduction and inauguration to the institution. Typically, the first-year students are led by fellow students from upper years over the course of the week through various events ranging from campus tours, games, competitions, and field trips. At smaller liberal arts colleges, the faculty may also play a central role in orientation.		In many colleges, incoming freshmen are made to perform activities such as singing of songs, engaging in group physical activities, and playing games. These activities are often done to help freshmen make friends at their new establishment, and also to bond with each other and the upperclassmen.		Despite the fact that most first-year students are below the legal drinking age (currently 21 years in all states), heavy drinking and binge drinking may occur outside the orientation curriculum. Some programs require their organizers to sign waivers stating they will not be under the influence of any substances over the course of the week as they are responsible for the well-being of the students. Most programs have one final party on the final night to finish off the week of celebrating, in which the organizers join in.[citation needed]		Although it has been officially banned at many schools, hazing is not uncommon during the week. This can be anywhere from the organizers treating the first-year students in a playfully discouraging manner to forcing them to endure rigorous trials.		The attitude of the events also depends on the school. Many colleges encourage parents to come to the first day to help new students move into their dormitory, fill out paper work, and get situated.[17] Some schools view their week as an initiation or rite of passage while others view it as a time to build school spirit and pride. In towns with more than one university, there may be a school rivalry that is reflected in the events throughout the week.		At most schools, incoming freshmen arrive at the school for a couple of days during the summer and are put into orientation groups led by an upperclassman trained for the position. Their Orientation Leader will take them around campus, do activities with them, have discussions with them, help them register for the next semester's classes and make them feel comfortable about coming to school in the fall.		Freshmen orientation is usually mandatory for all new students, especially international students which is one way to activate the status of their visa.		After first-year students have completed some time at their university, they may find that they did not make the right choice, miss being close to home, or simply want to attend a different institution. When this occurs, they may transfer to another university, usually after their first year.		Many universities will hold another student orientation similar to freshman orientation for these transfer students. Freshman orientation lasts a few days or a week, on the other hand, transfer student orientation will typically last between one and three days. Transfer orientation's purpose is to acquaint transfer students with their new university. This usually includes campus tours, introducing transfer students to their adviser or perhaps a few of their teachers, and filling out paperwork for proper enrollment. At some colleges, transfer orientation is mandatory for all transfer students.[18]		Unlike freshmen, transfer students are already familiar with the independence of college life. Therefore, their orientation focuses mostly on becoming familiar with the layout and policies of their new institution, providing information about essential campus resources, and getting acquainted with other transfer students so they may make friends at their new university.[19] Transfer students may engage in games, conversations with University faculty, and discussions with current students to make acquaintances and learn more about the university.		At Roskilde University in Denmark, orientation week (in Danish rusvejledning) normally lasts from 1 week and a half to two whole weeks. During the period, approximately 14 teams consisting of 10–16 tutors each takes care of an individual house in which the new students have been allocated. There's normally 1 house of Natural Sciences, 4 of Social Studies and Economics, 4 houses of Arts and Language and 2 of technology and design. Each of the first 3 houses described has an International version as well, where the courses are taught in English instead of Danish.		Each tutor group spends roughly 14 days (and 3–5 days of preeducation in the spring semester) living on campus before the arrival of the new students (also called ruslings). These periods usually involve heavy amounts of drinking, partying and sexual activity among the tutors themselves. However most festive activities including alcohol only occurs until after 4 pm, due to the alcohol policies of the university. Because of this policy, most of the daily activity is spent on planning and preparing activities for the new students.		When the students arrive all tutor groups welcomes the ruslings with the infamous Marbjergmark show- usually a display of wacky sketches such as naked people playing chess, smashing rotten eggs at bystanders or themselves or guys chasing midgets with a butcher's knife (to name a few examples).		During the two-week period the tutor group teach and introduce the new students to life at campus. Both the social and educational aspects. As it is with the preparation period, festive activities take place after 4 PM, and educational activities are held during the day.		The two-week period ends in a four-day period in which the house will leave campus to varied destinations. During these days mostly social activities are held, including the more secret hazing rituals of the university.		The tutors uphold a strict set of rules to maintain a safe and pleasant tutorship to prevent harmful and humiliating hazing rituals. Examples are the presence of minimum two sober tutors at each party (In Danish Ædruvagter). Engaging in sexual relations with new students is also strongly discouraged. Also it is generally not seen as appropriate to force people to drink alcohol through various games and activities. Furthermore, the university dictates that each tutor must be taught basic first aid, as well as a couple of courses in conflict management and basic education psychology.		At DTU (Danish Faculty of Technology and Engineering), Copenhagen Business School and Copenhagen University similar periods are held. They however vary, and are significantly shorter than the overall orientation period spent on Roskilde University.		
An anarchistic free school (also anarchist free school and free skool) is a decentralized network in which skills, information, and knowledge are shared without hierarchy or the institutional environment of formal schooling. Free school students may be adults, children, or both. This organisational structure is distinct from ones used by democratic free schools which permit children's individual initiatives and learning endeavors within the context of a school democracy, and from free education where 'traditional' schooling is made available to pupils without charge. The open structure of free schools is intended to encourage self-reliance, critical consciousness, and personal development. Free schools often operate outside the market economy in favor of a gift economy. Nevertheless, the meaning of the "free" of free schools is not restricted to monetary cost, and can refer to an emphasis on free speech and student-centred education.		Free schools have their roots in the anarchist Escuela Moderna of Spain in the late 19th and early 20th centuries. They are, at heart, non-institutional, non-authoritarian, and counter-cultural. Generally, these are formed at a grassroots level by a group of individuals acting collectively and autonomously to create educational opportunities and promote skill-sharing within their communities. For example, the Anarchist Free School in Toronto was described as "a volunteer-run, autonomous collective offering free courses, workshops, and lectures."[1]						While the philosophy and intention of anarchist free school projects vary, they share a desire to offer free ongoing education without hierarchy and outside of institutional control.		Free Skool Santa Cruz in California is perhaps typical of the current batch of free schools that are explicitly rooted in an anarchist tradition of collectivism, self-reliance, and mutual support, and feature informal, non-authoritarian learning outside of the monetary economy. From the Free Skool Santa Cruz website: "More than just an opportunity to learn, we see Free Skool as a direct challenge to dominant institutions and hierarchical relationships. Part of creating a new world is resistance to the old one, to the relentless commodification of everything, including learning and the way we relate to each other."[2]		Spanish anarchist Francisco Ferrer (1859–1909) established "modern" or progressive schools in Spain in defiance of an educational system controlled by the church. Fiercely anti-clerical, he believed in "freedom in education," education free from the authority of church and state.[3] Murray Bookchin writes: "This period [1890s] was the heyday of libertarian schools and pedagogical projects in all areas of the country where Anarchists exercised some degree of influence. Perhaps the best-known effort in this field was Francisco Ferrer's Modern School (Escuela Moderna), a project which exercised a considerable influence on Catalan education and on experimental techniques of teaching generally."[4]		
Scholarism (Chinese: 學民思潮) was a Hong Kong pro-democracy[3] student activist group active in the fields of Hong Kong's education policy, political reform and youth policy.[4] It was reported to have 200 members in May 2015.[5]		The group was known for its stance on defending the autonomy of Hong Kong's education policy from Beijing's influence.[6][7][8] It was also the leading organisation during the 2014 Hong Kong protests, better known as the "Umbrella Revolution".		Founded by a number of secondary school students on 29 May 2011, the group first came to media attention when they organised a protest against the Pro-Communist “moral and national education” put forward by the Hong Kong government in 2012. At the height of the event, 120,000 students and members of the public attended the demonstration and forced the government to retract its plans to introduce “moral and national education” as a compulsory subject in schools.[7]		Scholarism ceased functioning in March 2016. Core members including Joshua Wong, Oscar Lai and Agnes Chow formed a new political party Demosisto in April.						Originally formed as "Scholarism – The Alliance Against Moral & National Education",[9] Scholarism was the first student pressure group that protested against the Pro-Communist "Moral and National Education" school curriculum[3] put forward by the Hong Kong Government in 2012. The controversial subject ignored the Tiananmen massacre and tried to present the Communist Party of China in favourable light.[7] The group was one of the few organisations that took part in the protest outside the Central Government Liaison Office after the 1 July March 2012.[10]		In August 2012 members of Scholarism launched an occupation protest at the Hong Kong government headquarters to force the government to retract its plans to introduce "Moral and National Education" as a compulsory subject. Fifty members occupied the public park beneath the government offices, and three of the protesters began a hunger strike.[11] The protest lasted until September 2012.		After rising to prominence during the movement against the "Moral and National Education" the pro-democracy students remained active in the social and democracy movement in Hong Kong.		On 23 June 2013, Scholarism issued a statement which stressed the necessity of civil nomination for the 2017 Chief Executive election.[12] By late August, Scholarism drew up a charter and began lobbying democratic Legislative Councillors to sign it, which would commit their parties to make civil nomination through universal suffrage the number one priority during the coming campaign for the 2017 Chief Executive election. The charter was signed by People Power, League of Social Democrats, Neo Democrats, and Neighbourhood and Worker's Service Centre.[13] The Civic Party signed on with reservations, and the Democratic Party, Labour Party and Association for Democracy and People's Livelihood refused to sign it as they disagreed that public nomination should be the only way to put forward candidates.[14]		In September 2014, Scholarism launched a class boycott with the Hong Kong Federation of Students (HKFS), protesting against the National People's Congress Standing Committee's (NPCSC) decision on the restricting nomination procedure of the election of the Chief Executive of Hong Kong.[15]		On 26 September, the class boycott was held on Tim Mei Avenue and the square outside the Legislative Council Complex, as an organisation applied to hold a celebration event of 65th anniversary of the People's Republic China at the Tamar Park. It was also the day on which Scholarism hosted the class boycott movement for secondary school students. Joshua Wong, leading activist of Scholarism, pronounced the boycott statement. At 10:30 p.m. when the assembly came to the end, Joshua Wong, all of a sudden, called for the crowd to "retake" the Civic Square, around which fences were built two months earlier. Led by members of HKFS, hundred of protesters climbed across fences and tore down the barriers around the flag stage. The police surrounded hundreds of protesters, and then further mobilised towards the Civic Square and clashed with the protesters. The police pepper-sprayed them and displayed their batons. In chaos, it was reported a protester was having a heart attack. The police force originally refused to let the medics enter, and later permitted under the protesters pressure. At 10:52 p.m. police handcuffed and arrested Joshua Wong for forcible entry to government premises, disorderly conduct in public place and unlawful assembly. As visible wounds were found, he was sent to Ruttonjee Hospital for medical inspections before sent to the Central Police station. Many people on site were sent to the hospital because of injuries or feeling unwell. The police and protesters fell into stalemate later on.		The raid on the Civic Square triggered the massive occupy protest in the following days, as more protesters were called in support of the students being carried away from the Civic Square. At the night of 27 September, HKFS and Scholarism organised another assembly. Having declared the assembly unlawful, police blockaded Exit A of Admiralty MTR Station and bridges outside the Central Office Complex until the size of the crowd caused the police to lift the blockade. At 1:30 am, 28 September, Benny Tai, co-initiator of the Occupy Central with Love and Peace movement, declared the official launch of the "Occupy Central" campaign. In the afternoon, as more and more people flooded to Admiralty in support of the students, the police completely blockaded the access to the government headquarters. Later, the protesters began to occupy Harcourt Road which police responded by using tear gas, triggering widespread occupations at Admiralty, Mong Kok and Causeway Bay.		Although the protests were fruitless in the end, Scholarism remained its leading role in the following 79-day occupy movement.		In February 2016, Oscar Lai Man-lok, a core member of Scholarism, revealed that he, Joshua Wong and Agnes Chow Ting planned to form a political party and field at least two candidates to run in the Legislative Council elections in September. Lai subsequently quit Scholarism to support Civic Party’s Alvin Yeung Ngok-kiu in the Legislative Council by-election for New Territories East, as Scholarism refused to officially endorse a candidate between the pro-democratic Civic Party and the localist Hong Kong Indigenous's Edward Leung Tin-kei in the election.[16]		In March, an online news outlet IBHK reported that the group, would be disbanded. Scholarism’s Facebook page said on 16 March that it was at present working on its future direction and would announce if it reached any decisions.[16] Scholarism officially ceased functioning on 20 March 2016. By the day, the group had a total number of HK$1,450,000 at its bank account. HK$700,000 of it will be transferred to a new student activist group and the other HK$750,000 will be transferred to a legal assistance fund to assist the lawsuits the group members face. The group will disband itself when it has no money left. Core members including Joshua Wong, Oscar Lai and Agnes Chow announced that they will form a new political party Demosisto in April.[17]		
A tutor is an instructor who gives private lessons. Shadow education is a name for private supplementary tutoring that is offered outside the mainstream education system.		Normally, a tutor will help a student who is struggling in a subject of some sort. Also, a tutor may be provided for a student who wants to learn at home.		In the United States, the term "tutor" is generally associated with one who gives professional instruction (sometimes within a school setting but often independently) in a given topic or field.						In British and Irish secondary schools, form tutors are given the responsibilities of a form or class of students in a particular year group (up to 30 students). They usually work in year teams headed by a year leader, year head, or guidance teacher.[citation needed]		Form tutors will provide parents with most of the information about their child's progress and any problems they might be experiencing. Ordinarily, the form tutor is the person who contacts a parent if there is a problem at school; however, the year leader or guidance teacher may contact the parents, since the form tutor has full-time responsibility as a specialist subject teacher.		A 2012 study by the Asian Development Bank and the Comparative Education Research Centre at the University of Hong Kong pointed out that private tutoring can dominate the lives of young people and their families, maintain and exacerbate social inequalities, divert needed household income into an unregulated industry, and create inefficiencies in education systems. It can also undermine official statements about fee-free education and create threats to social cohesion.[1]		In South Korea, nearly 90% of elementary students receive some sort of shadow education.[2] In Hong Kong, about 85% of senior secondary students do so.[3] 60% of primary students in West Bengal, India,[4] and 60% of secondary students in Kazakhstan receive private tutoring.[5]		Demand for tutoring in Asia is exploding; by comparison globally, shadow education is most extensive in Asia. This is partly due to the stratification of education systems, cultural factors, perceptions of shortcomings in regular school systems, and the combination of growing wealth and smaller family sizes.[1] Therefore, the education sector has become a profitable industry which businesses have created different kinds of products and advertisement such us "the king/queen of tutorial", a usual advertisement tactic of Hong Kong tutorial centers that has spread to South Korea, Thailand, Sri Lanka and India where tutors achieve "celebrity-like status".[6] In some cases, successful Southeast Asian tutors will even embrace the title of "tutor". Online private tutor matching platform and online learning platform offering online learning materials are other creations.		In Cambodia, most tutoring is provided by teachers,[7] whereas in Hong Kong, it is provided by individuals, small companies or large companies.[8] In Mongolia, most tutoring is labor-intensive,[9] while entrepreneurs in South Korea make use of computers and other forms of technology.[1]		Some studies have estimated costs associated with "shadow education". In Pakistan, expenditures on tutoring per child averaged $3.40 a month in 2011. In India, average spending was lower, but still equated to about $2 per month.[10]		In Georgia, household expenditures for private tutoring at the secondary school level was $48 million in 2011.[11] In Hong Kong, the business of providing private tutoring to secondary schools reached $255 million in 2011.[12]		In India, a 2008 survey estimated the size of the private tutoring sector to be $6.4 billion.[13] In Japan, families spent a whopping $12 billion in 2010 on private tutoring.[7]		In the Republic of Korea, where the government has attempted to cool down the private tutoring market, shadow education costs have continually grown, reaching a staggering $17.3 billion in 2010. Household expenditures on private tutoring are equivalent to about 80% of government expenditures on public education for primary and secondary students.[14]		In the United States, the tutoring market is fragmented. Some online tutoring marketplaces, however, have managed to aggregate a large number of private tutors on their platform and also tutoring data. For example, one such site has over 34,000 registered tutors in California and made tutoring hourly rate data for California public.[15]		In many countries, individuals can become tutors without training. In some countries, including Cambodia, Georgia, Kazakhstan, Lao PDR, and Tajikistan, the pattern of classroom teachers supplementing their incomes by tutoring students after school hours is more a necessity than a choice, as many teachers’ salaries hover close to the poverty line.[1]		In the Republic of Korea, the number of private tutors expanded roughly 7.1% annually on average from 2001 to 2006, and by 2009 the sector was the largest employer of graduates from the humanities and social sciences.[16]		Private tutoring is not always effective in raising academic achievement; and in some schools students commonly skip classes or sleep through lessons because they are tired after excessive external study. This means that the shadow system can make regular schooling less efficient.[1]		Teachers who spend more time focusing on private lessons than regular classes can cause greater inefficiencies in the mainstream school system. Situations in which teachers provide extra private lessons for pupils for whom they are already responsible in the public system can lead to corruption, particularly when teachers deliberately teach less in their regular classes in order to promote the market for private lessons.[17]		When private tutoring is provided by well trained tutor however the effects can be dramatic, with pupils improving performance by two standard deviations.[18] See also Bloom's 2 Sigma Problem.		A 2012 study by the Asian Development Bank and the Comparative Education Research Centre at the University of Hong Kong recommended policymakers across the region take a closer look at how ‘shadow education’ affects family budgets, children’s time, and national education systems. It suggested that in order to reduce the need for private lessons, improvements in mainstream schools should be made. Regulations are also needed to protect consumers.[1]		A private tutor is a private instructor who teaches a specific subject or skill to an individual student or small group of students. Such attention ideally allows the student to improve knowledge or skills more rapidly than in a classroom setting. Tutors are often privately hired and paid by the student, the student's family or an agency. Some are used for remedial students or others needing special attention; some provide more advanced material for exceptionally capable and highly motivated students, or in the context of homeschooling. Tutoring can also occur when one adult helps another adult student to study a specific course or subject that he/she is taking to get a better result. The adult can also let the student work on his/her own, and can be there if the student has any questions. The ultimate goal of a private tutor is to foster independence. When a student no longer requires private tutoring, the tutor successfully puts him/herself out of a job. Ironic as it may seem, a tutor's professional reputation is often measured in terms of the degree of self-imposed obsolescence.		Academic coaching is an evolution of mentoring applied to academics. Coaching involves a collaborative approach. Coaches try to help students learn how they best learn and how to operate in an academic environment. Tutors help students learn the material in individual courses while coaches help students learn how to be successful in school. In college, that includes such topics as: study skills, time management, stress management, effective reading, note-taking, test-taking, and understanding how to use a syllabus. Academic coaches meet with the student regularly throughout the semester. Coaches work with students in all kinds of situations, not just those who are struggling academically. Some highly motivated, high-achieving students will have a coach to improve their learning efficiency. Academic coaching also occurs to help students prepare for entrance exams to gain entry to schools or universities. Tutoring may even be used for the whole application process to university. Academic coaching is a huge industry in Asia. For example, in India, a majority of students, be it of any class or stream, visit a coaching center or a "study circle."[19]		Sometimes, current students act as tutors to other students. Sometimes, a classroom setting is not enough for a student to learn all of the material that they need to know in order to pass the test or to go on to harder classes. Academic tutoring from students at a higher grade level or experience in an academic setting can help to encourage and strengthen a student so that they do not fall behind.		For students, helping other students will be beneficial because the students can check themselves while they teach the lesson(s).[20]		Online tutoring is a new way for a student to receive help, either scheduled or on-demand. Sessions are done through an application where a student and tutor can communicate. Common tools include chat, whiteboard, web conferencing, teleconferencing and other specialized applets which make it easier to convey information back and forth. For example, there are specialized applets designed specifically for mathematics which allow the use of mathematical symbols. There is also an example-tracing tutor program that uses a behavior graph. The tutor is able to create an outline program that works a specific problem step by step. The process is activated once the student selects that problem. This is helpful for those who need help but are not able to meet face to face with someone.[21]		Online tutoring has been gaining popularity over the past couple of years due to the ease of being able to connect to a tutor at moment's notice when help is required.[citation needed] This is especially effective when a student is studying for a test that is scheduled for the next day at school and is stumped on a particular problem. Not all online tutoring companies offer an on-demand tutoring service.		In-home tutoring is a form of tutoring that occurs in the home. Most often the tutoring relates to an academic subject or test preparation. This is in contrast to tutoring centers or tutoring provided through after-school programs. The service most often involves one-on-one attention provided to the pupil.		Solution assistance is a growing trend in the field of mathematics tutoring. This method of checking the accuracy of answers is particularly helpful for students without a computer or those students that live in remote areas.		In Canada and the United States, writing tutor is the common term used for individuals working one-on-one with students in college and university writing centers.[22][23] The terms tutor and consultant are often used interchangeably, and both terms are used with deliberation as they are seen to represent a specific relationship, role, or activity between tutor and tutee. For example, Griffin, Keller, Pandey, Pedersen, and Skinner[24] in their 2003-2004 survey of North American writing centers describe a tutor as an expert providing a less expert learner with knowledge, implying a transmission approach. In contrast, the consultant, also expert, collaborates with the tutee in addressing the writing task, implying a social constructivist approach. The focus of the social constructionism paradigm is to get rid of the idea that tutors are an authoritarian figure instead of someone who the student can collaborate with. Social constructionism is the dominant approach used in writing centers today versus the expressivism approach that was favored in the 1970s and 1980s.[25] Others who use the term writing tutor describe the tutor as facilitating learning through active listening, responding, as well as using silence and wait time.[26] Taking the cue from the student, these writing tutors function much like the consultants described by Griffin et al., offering suggestions and working together on a given writing task. Regardless of the title, the intent and actions of the tutor are important to writing center practitioners. A tutor may say he/she is acting collaboratively with the student and unknowingly be enforcing her or his own agenda.[27]		
The Junior Certificate (Irish: Teastas Sóisearach) or "Junior Cert" for short, is an educational qualification awarded in Ireland by the Department of Education and Skills to students who have successfully completed the junior cycle of secondary education and achieved a minimum standard in their Junior Certificate Examination (Irish: Scrúdú an Teastais Shóisearaigh). These exams, like those for the Leaving Certificate, are supervised by the State Examinations Commission. A "recognised pupil"[3] who commences the Junior Cycle must reach at least 12 years of age on 1 January of the school year of admission and must have completed primary education; the examination is normally taken after three years' study in a secondary school. Typically a student takes 9 to 13 subjects – including English, Irish and Mathematics – as part of the Junior Cycle. The examination does not reach the standards for college or university entrance; instead, a school leaver in Ireland will typically take the Leaving Certificate Examination two or three years after completion of the Junior Certificate to reach that standard.		The objective[4] of the Junior Cycle is:		...to provide a well-balanced, general education suitable for pupils who leave full-time education at the end of compulsory schooling or, alternatively, who wish to enter on more advanced courses of study.						The Junior Certificate officially replaced the Day Vocational (Group) Certificate ("Day Cert" or "Group Cert") and the Intermediate Certificate ("Inter Cert") in 1992, when the first Junior Cert examinations were held; instruction in the new course had commenced in September 1989[5] The new, modern course was acclaimed as it was much more flexible than its predecessors. The Junior Certificate quickly became the minimum requirement for getting a job in Ireland.		Near the end of the decade, the Department of Education and Science began to replace many subject curricula, particularly those that were dated, such as History and Geography. In 1999, Civic, Social, and Political Education was introduced as a subject and made mandatory from 2000, when Religious Education was also brought in. Religion was phased in with just a few schools adopting it in its first year, but now nearly all do the exam for Junior Cert, whilst CSPE was implemented nationwide. In 2002 a new Science course was introduced. The new course emphasised greater class participation and introduced the awarding of a percentage of marks for class practicals throughout the three years. However, many teachers complained about a lack of information from the Department about this change. Sample papers were not released until early 2006, the year when the new exam would be sat for the first time. Also, some schools complained that they did not have the laboratory facilities to do the new course but were forced to teach it anyway.		In 2004, results were made available on the Internet for the first time thus allowing students who, for instance, had moved school or left school to get their results without having to return to their old school. The lack of students taking honours Maths has been a consistent issue throughout the history of the Junior Certificate. However, in recent years the trend of taking honours Maths has increased positively.[6]		The Junior Cycle is the first three years of second-level education. In the final year of the course, teachers allocate a substantial amount of time for revision of key topics. Candidates also practice answering questions which appeared on previous examination papers. Courses are quite broad – for example, the Business Studies course covers business organisation, marketing, economics, accounting and several other areas. The same is also true of the Science course, which covers basic physics, chemistry and biology. The Leaving Certificate exam, by comparison, is much more specific.		A "recognised junior pupil"[4] must undertake all the mandatory subjects and at least two of the optional subjects, except insofar as exemptions or exclusions apply. In certain types of schools, subjects in the optional grouping (or a selection from combinations of them) may, in fact, be mandatory, for instance, History and Geography are mandatory in certain types of schools.[7] Most schools do not offer all the optional subjects but must offer all the mandatory and certain optional subjects.		Each subject is examined at one of three levels, Higher Level (informally Honours), Ordinary Level (informally Pass), or Foundation Level. Foundation Level may only be taken in two subjects: Irish and Mathematics. All other subjects may be taken at either Ordinary or Higher Level. In general, a Higher Level grade is worth 40 points more than the equivalent Ordinary Level grade (e.g. a Higher C1 is 70 points, while an Ordinary C1 is 30). No points are awarded for a grade below D3 (40%). However, a D3 may be awarded with a tolerance of up to 2%; a practice in place since the introduction of the grading system in 1969.[4]		†An exemption from taking Irish may be awarded in some cases, for students with a specific learning difficulty such as dyslexia or Autism Spectrum Disorder, or those who did not attend school in the country before their twelfth birthday.		All optional subjects are offered at Ordinary and Higher Level.		‡Classical Studies can not be taken by a student who is also taking Greek or Latin.		The final examination takes place after three years of the course, in early June. The exams always start with English, then the other core subjects and finish with the subjects that have the fewest candidates. They usually last two and a half weeks. The exams can take the form of written papers, aural exams (which are usually included at the start of the written paper), practical exams (for example, in Music, 25% of the final result is based on a performance and skills test in front of an examiner) and marks from course work assignments (such as in CSPE, where 60% of the exam rests on an action project completed during the school year). Exams normally range from two to two and a half hours long; most subjects are one paper only (i.e. they are taken in a single session), however, three subjects have two papers at higher level – Irish, Mathematics and Business Studies. Until 2017, the English examination also had two papers at higher level.		Schools with students taking the examinations will have one or more examination centres (individual enclosed rooms in which examinations take place), and almost always at least two, because the Leaving Certificate and Junior Certificate examinations cannot take place in the same centre. Smaller centres can be used for students with reasonable accommodations because of a learning or writing difficulty. Each exam centre is supervised by an external invigilator, usually a teacher from another school or an employee of the SEC. A staff member of the school is hired as an examination aide by the SEC to act as a liaison between the SEC and the school officials during the examination period. Candidates may not enter the exam centre after the first 30 minutes, and are permitted to leave the centre after 30 minutes have passed, up until the last 15 minutes of the examination, although this practice has been abolished in some schools, and is discouraged in many others.		The Irish Times published an article where teachers expressed their concern that some syllabi for certain subjects (e.g. Business Studies) were not "up-to-date" with current events and would therefore not encourage students enough to think independently and apply theory to real-world scenarios.[10]		At the Junior Certificate, students can take an examination subject at one of four levels. These are:		The level taken at Junior Certificate may have bearing on the level taken in the Leaving Certificate; thus, for instance, a student could take an Ordinary level in the Junior Certificate and then could not take a Higher level in the corresponding Leaving Certificate subject, later.		A mark below 10% receives no grade. Above this, there are six ranges of 15%, from F up to A. Grades A, B, C and D are passing grades, E and F are failing grades; therefore, the pass mark for the Junior Cert is 40%.		In the Junior Certificate candidates have the option of answering either in Irish(only if they have been in the Irish stream) or in English, except in the case of the subjects Irish and English and questions in other language subjects.[11] Certain subjects and components are not available for bonus marks, marks awarded also vary depending on the written nature of the subject.		Students who face disadvantages (e.g. suffer spelling problems caused by dyslexia, dyspraxia, dysgraphia, or other disorders such as Autism Spectrum Disorder or ADHD) can not be penalised for bad spellings in exams such as English and Irish. These candidates will then be marked easier on all topics (e.g. if a student has a spelling problem in English he/she will be marked out of 50 for their mechanics).		In 2017, English gets a new grading system, as part of the new Junior Cycle.[12] The grading is as follows:		It is not possible to fail the Junior Cert overall: all students continue to their next year of education no matter what their results, but most schools will not permit a student to take a Leaving Cert subject at Higher Level if they did not receive at least a Higher Level "C" grade at Junior Cert[citation needed]. The Junior Certificate (and more so, the Leaving Certificate) results take centre place in the Irish media during the week surrounding their release. The newspapers publish various statistics about the exam and cover high achievers (some receive ten or more "A" grades).		If a student is unhappy with a grade they received on any of the exam results, they may appeal the decision made by the SEC. They need to pay a fee (in 2010 the fee was set at €32 per exam) and the principal of the school writes a letter of appeal application to the State Examinations Commission, stating the candidate's name, exam number and the exam they would like to appeal. There is a deadline to appeal, usually 14–21 days after the results are published, in which the student's application must be made. The appeal results are usually handed out mid-November. The grade that is received this time is final, and no more appeals can be made. If the candidate's grade did not change, no further action will be taken. However, if a change did occur, then the candidate will be refunded the appeal fee via a Cheque made out to the principal of the school. These refunds take time to be issued, but in an appeal made in September of one year, the refund was issued as late as March in the following year.		Although school attendance in Ireland is very high, some students drop out of the education system after completion of the Junior Certificate. Those who stay in the education system sit the Leaving Certificate – the requirement for college entry in Ireland. A new type of Leaving Certificate, the Leaving Certificate Applied has been designed to discourage people from dropping out. This is all practical work and students may work after school or do an apprenticeship, respectively.		The vast majority of students continue from lower level to senior level, with only 12.3% leaving after the Junior Certificate. This is lower than the EU average of 15.2%.[13]		Transition Year (TY) (Irish: Idirbhlian) is an optional one-year programme that can be taken in the year after the Junior Certificate in Ireland and is intended to make the senior cycle a three-year programme encompassing both Transition Year and Leaving Certificate.[14] The idea of such a year is strange in other countries, as they don't have the same year. Transition Year was created as a result of the Programme for Economic and Social Progress which called for a six-year cycle of post-primary education.[15]		In late 2009 the Irish Government considered for a short period of time to completely scrap all Junior Cert examinations permanently. The move was met with criticism and outrage from the Teachers' Union (ASTI), but the Government said that scrapping the annual examinations and replacing them with continuous assessment would save the country €30 million.		However, later on, the government agreed to scrap the Junior Certificate and instead, introduce a brand new syllabus in English for students starting First Year of secondary school in September 2014, with only 90% of the test going for a written exam. The other 10% is based on continuous assessment over two years. The first assessment takes place at the end of 2nd Year, which is an oral exam. Students have 3 to choose a topic and have to present it. The second assessment takes place at Christmas of 3rd Year. It is Collection of Texts project which a student will choose 4 written pieces throughout the 3 years from 4 different genres and will re-draft them. These are then sent off to be corrected by the State Examination Commission. The final written exam at higher and ordinary levels now only consists of a two-hour paper.		Pilots of the new system have been underway for three years, with the principal of St Joseph’s College, Lucan, in particular noting that the "engagement in learning" proved to be a panacea for the school's discipline problems.[16]		Schools started the new course September 2014, as soon as the Junior Certificate Examinations were abolished with the aim of a soft transition. English was reformed in 2014. Irish, Science and Business were reformed in 2016. History and Geography will be reformed in 2019. It is expected that the full Junior Cert will be revised into the Junior Cycle Student Award by 2022.		On 15 January 2014, the Department of Education and Skills announced that the new name for the Junior Certificate will be called the "Junior Cycle Student Award".[17]		
– in Europe  (green & dark grey) – in the United Kingdom  (green)		Northern Ireland (Irish: Tuaisceart Éireann [ˈt̪ˠuəʃcəɾˠt̪ˠ ˈeːɾʲən̪ˠ] ( listen);[8] Ulster-Scots: Norlin Airlann) is a country of the United Kingdom[9] located in the north-east of the island of Ireland.[10] It has been variously described as a country, province, region, or "part" of the United Kingdom, amongst other terms.[11][12][13] Northern Ireland shares a border to the south and west with the Republic of Ireland. In 2011, its population was 1,810,863,[4] constituting about 30% of the island's total population and about 3% of the UK's population. Established by the Northern Ireland Act 1998 as part of the Good Friday Agreement, the Northern Ireland Assembly holds responsibility for a range of devolved policy matters, while other areas are reserved for the British government. Northern Ireland co-operates with the Republic of Ireland in some areas, and the Agreement granted the Republic the ability to "put forward views and proposals" with "determined efforts to resolve disagreements between the two governments".[14]		Northern Ireland was created in 1921, when Ireland was partitioned between Northern Ireland and Southern Ireland by an act of the British parliament. Unlike Southern Ireland, which would become the Irish Free State in 1922, the majority of Northern Ireland's population were unionists, who wanted to remain within the United Kingdom,.[15] Most of these were the Protestant descendants of colonists from Great Britain. However, a significant minority, mostly Catholics, were nationalists who wanted a united Ireland independent of British rule.[16][17][18][19] Today, the former generally see themselves as British and the latter generally see themselves as Irish, while a distinct Northern Irish or Ulster identity is claimed both by a large minority of Catholics and Protestants and by many of those who are non-aligned.[20]		For most of the 20th century, when it came into existence, Northern Ireland was marked by discrimination and hostility between these two sides in what First Minister of Northern Ireland David Trimble called a "cold house" for Catholics. In the late 1960s, conflict between state forces and chiefly Protestant unionists on the one hand, and chiefly Catholic nationalists on the other, erupted into three decades of violence known as the Troubles, which claimed over 3,500 lives and caused over 50,000 casualties.[21][22] The 1998 Good Friday Agreement was a major step in the peace process, including the decommissioning of weapons, although sectarianism and religious segregation still remain major social problems, and sporadic violence has continued.[23]		Northern Ireland has historically been the most industrialised region of Ireland. After declining as a result of the political and social turmoil of the Troubles,[24] its economy has grown significantly since the late 1990s. The initial growth came from the "peace dividend" and the links which increased trade with the Republic of Ireland, continuing with a significant increase in tourism, investment and business from around the world. Unemployment in Northern Ireland peaked at 17.2% in 1986, dropping to 6.1% for June–August 2014[update] and down by 1.2 percentage points over the year,[25] similar to the UK figure of 6.2%.[26] 58.2% of those unemployed had been unemployed for over a year.		Prominent artists and sportspeople from Northern Ireland include Van Morrison, Rory McIlroy, Joey Dunlop, Wayne McCullough and George Best. Some people from Northern Ireland prefer to identify as Irish (e.g., poet Seamus Heaney and actor Liam Neeson) while others prefer to identify as British (e.g. actor Kenneth Branagh). Cultural links between Northern Ireland, the rest of Ireland, and the rest of the UK are complex, with Northern Ireland sharing both the culture of Ireland and the culture of the United Kingdom. In many sports, the island of Ireland fields a single team, a notable exception being association football. Northern Ireland competes separately at the Commonwealth Games, and people from Northern Ireland may compete for either Great Britain or Ireland at the Olympic Games.						The region that is now Northern Ireland was the bedrock of the Irish war of resistance against English programmes of colonialism in the late 16th century. The English-controlled Kingdom of Ireland had been declared by the English king Henry VIII in 1542, but Irish resistance made English control fragmentary. Following Irish defeat at the Battle of Kinsale, though, the region's Gaelic, Roman Catholic aristocracy fled to continental Europe in 1607 and the region became subject to major programmes of colonialism by Protestant English (mainly Anglican) and Scottish (mainly Presbyterian) settlers. A rebellion in 1641 by Irish aristocrats against English rule resulted in a massacre of settlers in Ulster in the context of a war breaking out between England, Scotland and Ireland fuelled by religious intolerance in government. Victories by English forces in that war and further Protestant victories in the Williamite War in Ireland toward the close of the 17th century solidified Anglican rule in Ireland. In Northern Ireland, the victories of the Siege of Derry (1689) and the Battle of the Boyne (1690) in this latter war are still celebrated by some Protestants (both Anglican and Presbyterian).[27][28]		Following the victory of 1691, and contrary to the terms of the Treaty of Limerick, after the Pope who had been allied to William of Orange recognised James II as continuing king of Great Britain and Ireland in place of William, a series of penal laws was passed by the Anglican ruling class in Ireland. Their intention was to materially disadvantage the Catholic community and, to a lesser extent, the Presbyterian community. In the context of open institutional discrimination, the 18th century saw secret, militant societies develop in communities in the region and act on sectarian tensions in violent attacks. These events escalated at the end of the century following an event known as the Battle of the Diamond, which saw the supremacy of the Anglican and Presbyterian Peep o'Day Boys over the Catholic Defenders and leading to the formation of the Anglican Orange Order. A rebellion in 1798 led by the cross-community Belfast-based Society of the United Irishmen and inspired by the French Revolution sought to break the constitutional ties between Ireland and Britain and unite Irish people of all religions. Following this, in an attempt to quell sectarianism and force the removal of discriminatory laws (and to prevent the spread of French-style republicanism to Ireland), the government of the Kingdom of Great Britain pushed for the two kingdoms to be merged. The new state, formed in 1801, the United Kingdom of Great Britain and Ireland, was governed from a single government and parliament based in London.		Between 1717 and 1775 some 250,000 people from Ulster emigrated to the British North American colonies.[29] It is estimated that there are more than 27 million Scotch-Irish Americans now living in the US.[30]		During the 19th century, legal reforms started in the late 18th century continued to remove statutory discrimination against Catholics, and progressive programmes enabled tenant farmers to buy land from landlords. By the close of the century, autonomy for Ireland within the United Kingdom, known as Home Rule, was regarded as highly likely. In 1912, after decades of obstruction from the House of Lords, Home Rule became a near-certainty. A clash between the House of Commons and House of Lords over a controversial budget produced the Parliament Act 1911, which enabled the veto of the Lords to be overturned. The House of Lords veto had been the unionists' main guarantee that Home Rule would not be enacted, because the majority of members of the House of Lords were unionists. In response, opponents to Home Rule, from Conservative and Unionist Party leaders such as Bonar Law and Dublin-based barrister Sir Edward Carson to militant working class unionists in Ireland, threatened the use of violence. In 1914, they smuggled thousands of rifles and rounds of ammunition from Imperial Germany for use by the Ulster Volunteers (UVF), a paramilitary organisation opposed to the implementation of Home Rule.		Unionists were in a minority in Ireland as a whole, but in the northern province of Ulster they were a very large majority in County Antrim and County Down, small majorities in County Armagh and County Londonderry and a substantial minority in the rest of the province.[31] These four counties, as well as County Fermanagh and County Tyrone, would later constitute Northern Ireland. Most of the remaining 26 counties which later became the Republic of Ireland were overwhelmingly majority-nationalist.		During the Home Rule Crisis the possibility was discussed of a "temporary" partition of these six counties from the rest of Ireland. In 1914, the Third Home Rule Bill received Royal Assent as the Government of Ireland Act 1914. However, its implementation was suspended before it came into effect because of the outbreak of the First World War, and the Amending Bill to partition Ireland was abandoned. The war was expected to last only a few weeks but in fact lasted four years. By the end of the war (during which the 1916 Easter Rising had taken place), the Act was seen as unimplementable. Public opinion among nationalists had shifted during the war from a demand for home rule to one for full independence. In 1919, David Lloyd George proposed a new bill be established by the cabinet's Walter Long Committee on Ireland, which by adopting findings of his (Lloyd George's) inconclusive 1917-18 Irish Convention would divide Ireland into two Home Rule areas: twenty-six counties being ruled from Dublin and six being ruled from Belfast. Straddling these two areas would be a shared Lord Lieutenant of Ireland who would appoint both governments and a Council of Ireland, which Lloyd George believed would evolve into an all-Ireland parliament.[32]		Events overtook the government. In the general election of 1918, the pro-independence Sinn Féin won 73 of the 105 parliamentary seats in Ireland and unilaterally established the First Dáil, an extrajudicial parliament in Ireland. Ireland was partitioned between Northern Ireland and Southern Ireland in 1921 under the terms of Lloyd George's Government of Ireland Act 1920[33] during the Anglo-Irish War between Irish republican and British forces. The war ended on 6 December 1921, with the signing of the Anglo-Irish Treaty, which created the Irish Free State. Under the terms of the treaty, Northern Ireland would become part of the Free State unless the government opted out by presenting an address to the king, although in practice partition remained in place.[34]		As expected, the Houses of the Parliament of Northern Ireland resolved on 7 December 1922 (the day after the establishment of the Irish Free State) to exercise its right to opt out of the Free State by making an address to the King.[35] The text of the address was:		Most Gracious Sovereign, We, your Majesty's most dutiful and loyal subjects, the Senators and Commons of Northern Ireland in Parliament assembled, having learnt of the passing of the Irish Free State Constitution Act 1922, being the Act of Parliament for the ratification of the Articles of Agreement for a Treaty between Great Britain and Ireland, do, by this humble Address, pray your Majesty that the powers of the Parliament and Government of the Irish Free State shall no longer extend to Northern Ireland.[36][37]		Shortly afterwards, the Boundary Commission was established to decide on the territorial boundaries between the Irish Free State and Northern Ireland. Owing to the outbreak of civil war in the Free State, the work of the commission was delayed until 1925. Leaders in Dublin expected a substantial reduction in the territory of Northern Ireland, with nationalist areas moving to the Free State. However the commission's report recommended only that some small portions of land should be ceded from Northern Ireland to the Free State and even that a small amount of land should be ceded from the Free State to Northern Ireland. To prevent argument, this report was suppressed and, in exchange for a waiver to the Free State's obligations to the UK's public debt and the dissolution of the Council of Ireland (sought by the Government of Northern Ireland), the initial six-county border was maintained with no changes.		In June 1940, to encourage the neutral Irish state to join with the Allies, British Prime Minister Winston Churchill indicated to the Taoiseach Éamon de Valera that the United Kingdom would push for Irish unity, but believing that Churchill could not deliver, de Valera declined the offer.[38] The British did not inform the Government of Northern Ireland that they had made the offer to the Dublin government, and De Valera's rejection was not publicised until 1970.		The Ireland Act 1949 gave the first legal guarantee that the region would not cease to be part of the United Kingdom without the consent of the Parliament of Northern Ireland.		The Troubles, which started in the late 1960s, consisted of about thirty years of recurring acts of intense violence during which 3,254 people were killed[39] with over 50,000 casualties.[40] From 1969 to 2003 there were over 36,900 shooting incidents and over 16,200 bombings or attempted bombings associated with The Troubles.[41] The conflict was caused by the disputed status of Northern Ireland within the United Kingdom and the discrimination against the Irish nationalist minority by the dominant unionist majority.[42] From 1967 to 1972 the Northern Ireland Civil Rights Association (NICRA), which modelled itself on the US civil rights movement, led a campaign of civil resistance to anti-Catholic discrimination in housing, employment, policing, and electoral procedures. The franchise for local government elections included only rate-payers and their spouses, and so excluded over a quarter of the electorate. While the majority of disenfranchised electors were Protestant, but Catholics were over-represented since they were poorer and had more adults still living in the family home.[43]		NICRA's campaign, seen by many unionists as an Irish republican front, and the violent reaction to it, proved to be a precursor to a more violent period.[44] As early as 1969, armed campaigns of paramilitary groups began, including the Provisional IRA campaign of 1969–1997 which was aimed at the end of British rule in Northern Ireland and the creation of a United Ireland, and the Ulster Volunteer Force, formed in 1966 in response to the perceived erosion of both the British character and unionist domination of Northern Ireland. The state security forces – the British Army and the police (the Royal Ulster Constabulary) – were also involved in the violence. The British government's position is that its forces were neutral in the conflict, trying to uphold law and order in Northern Ireland and the right of the people of Northern Ireland to democratic self-determination. Republicans regarded the state forces as combatants in the conflict, pointing to the collusion between the state forces and the loyalist paramilitaries as proof of this. The "Ballast" investigation by the Police Ombudsman has confirmed that British forces, and in particular the RUC, did collude with loyalist paramilitaries, were involved in murder, and did obstruct the course of justice when such claims had been investigated,[45] although the extent to which such collusion occurred is still hotly disputed.		As a consequence of the worsening security situation, autonomous regional government for Northern Ireland was suspended in 1972. Alongside the violence, there was a political deadlock between the major political parties in Northern Ireland, including those who condemned violence, over the future status of Northern Ireland and the form of government there should be within Northern Ireland. In 1973, Northern Ireland held a referendum to determine if it should remain in the United Kingdom, or be part of a united Ireland. The vote went heavily in favour (98.9%) of maintaining the status quo. Approximately 57.5% of the total electorate voted in support, but only 1% of Catholics voted following a boycott organised by the Social Democratic and Labour Party (SDLP).[46]		The Troubles were brought to an uneasy end by a peace process which included the declaration of ceasefires by most paramilitary organisations and the complete decommissioning of their weapons, the reform of the police, and the corresponding withdrawal of army troops from the streets and from sensitive border areas such as South Armagh and Fermanagh, as agreed by the signatories to the Belfast Agreement (commonly known as the "Good Friday Agreement"). This reiterated the long-held British position, which had never before been fully acknowledged by successive Irish governments, that Northern Ireland will remain within the United Kingdom until a majority of voters in Northern Ireland decides otherwise. The Constitution of Ireland was amended in 1999 to remove a claim of the "Irish nation" to sovereignty over the entire island (in Article 2), a claim qualified by an acknowledgement that Ireland could only exercise legal control over the territory formerly known as the Irish Free State.[citation needed]		The new Articles 2 and 3, added to the Constitution to replace the earlier articles, implicitly acknowledge that the status of Northern Ireland, and its relationships within the rest of the United Kingdom and with the Republic of Ireland, would only be changed with the agreement of a majority of voters in each jurisdiction. This aspect was also central to the Belfast Agreement which was signed in 1998 and ratified by referendums held simultaneously in both Northern Ireland and the Republic. At the same time, the British Government recognised for the first time, as part of the prospective, the so-called "Irish dimension": the principle that the people of the island of Ireland as a whole have the right, without any outside interference, to solve the issues between North and South by mutual consent.[47] The latter statement was key to winning support for the agreement from nationalists. It established a devolved power-sharing government within Northern Ireland, which must consist of both unionist and nationalist parties. These institutions were suspended by the British Government in 2002 after Police Service of Northern Ireland (PSNI) allegations of spying by people working for Sinn Féin at the Assembly (Stormontgate). The resulting case against the accused Sinn Féin member collapsed.[48][49]		On 28 July 2005, the Provisional IRA declared an end to its campaign and has since decommissioned what is thought to be all of its arsenal. This final act of decommissioning was performed in accordance with the Belfast Agreement of 1998, and under the watch of the Independent International Commission on Decommissioning and two external church witnesses. Many unionists, however, remain sceptical. The International Commission later confirmed that the main loyalist paramilitary groups, the UDA, UVF and the Red Hand Commando, had decommissioned what is thought to be all of their arsenals, witnessed by a former archbishop and a former top civil servant.[50]		Politicians elected to the Assembly at the 2003 Assembly election were called together on 15 May 2006 under the Northern Ireland Act 2006[51] for the purpose of electing a First Minister and deputy First Minister of Northern Ireland and choosing the members of an Executive (before 25 November 2006) as a preliminary step to the restoration of devolved government.		Following the election held on 7 March 2007, devolved government returned on 8 May 2007 with Democratic Unionist Party (DUP) leader Ian Paisley and Sinn Féin deputy leader Martin McGuinness taking office as First Minister and deputy First Minister, respectively.[52] In its white paper on Brexit the United Kingdom government reiterated its commitment to the Belfast Agreement. With regard to Northern Ireland's status, it said that the UK Government's "clearly-stated preference is to retain Northern Ireland’s current constitutional position: as part of the UK, but with strong links to Ireland".[53]		The main political divide in Northern Ireland is between unionists, who wish to see Northern Ireland continue as part of the United Kingdom, and nationalists, who wish to see Northern Ireland unified with the Republic of Ireland, independent from the United Kingdom. These two opposing views are linked to deeper cultural divisions. Unionists are predominantly Ulster Protestant, descendants of mainly Scottish, English, and Huguenot settlers as well as Gaels who converted to one of the Protestant denominations. Nationalists are overwhelmingly Catholic and descend from the population predating the settlement, with a minority from the Scottish Highlands as well as some converts from Protestantism. Discrimination against nationalists under the Stormont government (1921–1972) gave rise to the civil rights movement in the 1960s.[54]		While some unionists argue that discrimination was not just due to religious or political bigotry, but also the result of more complex socio-economic, socio-political and geographical factors,[55] its existence, and the manner in which nationalist anger at it was handled, were a major contributing factor to the Troubles. The political unrest went through its most violent phase between 1968 and 1994.[56]		In 2007, 36% of the population defined themselves as unionist, 24% as nationalist and 40% defined themselves as neither.[57] According to a 2015 opinion poll, 70% express a long-term preference of the maintenance of Northern Ireland's membership of the United Kingdom (either directly ruled or with devolved government), while 14% express a preference for membership of a united Ireland.[58] This discrepancy can be explained by the overwhelming preference among Protestants to remain a part of the UK (93%), while Catholic preferences are spread across a number of solutions to the constitutional question including remaining a part of the UK (47%), a united Ireland (32%), Northern Ireland becoming an independent state (4%), and those who "don't know" (16%).[59]		Official voting figures, which reflect views on the "national question" along with issues of candidate, geography, personal loyalty and historic voting patterns, show 54% of Northern Ireland voters vote for unionist parties, 42% vote for nationalist parties and 4% vote "other". Opinion polls consistently show that the election results are not necessarily an indication of the electorate's stance regarding the constitutional status of Northern Ireland. Most of the population of Northern Ireland are at least nominally Christian, mostly Roman Catholic and Protestant denominations. Many voters (regardless of religious affiliation) are attracted to unionism's conservative policies, while other voters are instead attracted to the traditionally leftist Sinn Féin and SDLP and their respective party platforms for democratic socialism and social democracy.[60]		For the most part, Protestants feel a strong connection with Great Britain and wish for Northern Ireland to remain part of the United Kingdom. Many Catholics however, generally aspire to a United Ireland or are less certain about how to solve the constitutional question. In the 2015 survey by Northern Ireland Life and Times, 47% of Northern Irish Catholics supported Northern Ireland remaining a part of the United Kingdom, either by direct rule (6%) or devolved government (41%).		Protestants have a slight majority in Northern Ireland, according to the latest Northern Ireland Census. The make-up of the Northern Ireland Assembly reflects the appeals of the various parties within the population. Of the 108 Members of the Legislative Assembly (MLAs), 56 are unionists and 40 are nationalists (the remaining 12 are classified as "other").[citation needed]		Since 1998, Northern Ireland has had devolved government within the United Kingdom. The UK Government and UK Parliament are responsible for reserved and excepted matters. Reserved matters comprise listed policy areas (such as civil aviation, units of measurement, and human genetics) that Parliament may devolve to the Northern Ireland Assembly some time in the future. Excepted matters (such as international relations, taxation and elections) are never expected to be considered for devolution. On all other governmental matters, the Northern Ireland Executive together with the 108-member Northern Ireland Assembly may legislate for and govern Northern Ireland. Devolution in Northern Ireland is dependent upon participation by members of the Northern Ireland executive in the North/South Ministerial Council, which co-ordinates areas of co-operation (such as agriculture, education and health) between Northern Ireland and the Republic of Ireland. Additionally, "in recognition of the Irish Government's special interest in Northern Ireland", the Government of Ireland and Government of the United Kingdom co-operate closely on non-devolved matters through the British-Irish Intergovernmental Conference.		Elections to the Northern Ireland Assembly are by single transferable vote with six representatives (Member of the Legislative Assembly, MLAs) elected from 18 parliamentary constituencies. Eighteen representatives to the lower house of the UK parliament (Members of Parliament, MPs) are elected from the same constituencies using the first-past-the-post system. However, not all of these take their seats. Sinn Féin MPs, currently five, refuse to take the oath to serve the Queen that is required before MPs are allowed to take their seats. In addition, the upper house of the UK parliament, the House of Lords, currently has some 25 appointed members from Northern Ireland. Northern Ireland itself forms a single constituency for elections to the European Parliament.		The Northern Ireland Office represents the UK government in Northern Ireland on reserved matters and represents Northern Ireland's interests within the UK Government. Additionally, the Republic's government also has the right to "put forward views and proposals" on non-devolved matters in relation to Northern Ireland. The Northern Ireland Office is led by the Secretary of State for Northern Ireland, who sits in the Cabinet of the United Kingdom.		Northern Ireland is a distinct legal jurisdiction, separate from the two other jurisdictions in the United Kingdom (England and Wales, and Scotland). Northern Ireland law developed from Irish law that existed before the partition of Ireland in 1921. Northern Ireland is a common law jurisdiction and its common law is similar to that in England and Wales. However, there are important differences in law and procedure between Northern Ireland and England and Wales. The body of statute law affecting Northern Ireland reflects the history of Northern Ireland, including Acts of the Parliament of the United Kingdom, the Northern Ireland Assembly, the former Parliament of Northern Ireland and the Parliament of Ireland, along with some Acts of the Parliament of England and of the Parliament of Great Britain that were extended to Ireland under Poynings' Law between 1494 and 1782.		There is no generally accepted term to describe what Northern Ireland is: province, region, country or something else.[11][12][13] The choice of term can be controversial and can reveal the writer's political preferences.[12] This has been noted as a problem by several writers on Northern Ireland, with no generally recommended solution.[11][12][13]		Owing in part to the way in which the United Kingdom, and Northern Ireland, came into being, there is no legally defined term to describe what Northern Ireland 'is'. There is also no uniform or guiding way to refer to Northern Ireland amongst the agencies of the UK government. For example, the websites of the Office of the Prime Minister of the United Kingdom[61] and the UK Statistics Authority describe the United Kingdom as being made up of four countries, one of these being Northern Ireland.[62] Other pages on the same websites refer to Northern Ireland specifically as a "province" as do publications of the UK Statistics Authority.[63][64] The website of the Northern Ireland Statistics and Research Agency also refers to Northern Ireland as being a province[65] as does the website of the Office of Public Sector Information[66] and other agencies within Northern Ireland.[67] Publications of HM Treasury[68] and the Department of Finance and Personnel of the Northern Ireland Executive,[69] on the other hand, describe Northern Ireland as being a "region of the UK". The UK's submission to the 2007 United Nations Conference on the Standardization of Geographical Names defines the UK as being made up of two countries (England and Scotland), one principality (Wales) and one province (Northern Ireland).[70]		Unlike England, Scotland and Wales, Northern Ireland has no history of being an independent country or of being a nation in its own right.[71] Some writers describe the United Kingdom as being made up of three countries and one province[72] or point out the difficulties with calling Northern Ireland a country.[73] Authors writing specifically about Northern Ireland dismiss the idea that Northern Ireland is a "country" in general terms,[11][13][74][75] and draw contrasts in this respect with England, Scotland and Wales.[76] Even for the period covering the first 50 years of Northern Ireland's existence, the term country is considered inappropriate by some political scientists on the basis that many decisions were still made in London.[71] The absence of a distinct nation of Northern Ireland, separate within the island of Ireland, is also pointed out as being a problem with using the term[13][77][78] and is in contrast to England, Scotland, and Wales.[79]		Many commentators prefer to use the term "province", although that is also not without problems. It can arouse irritation, particularly among nationalists, for whom the title province is properly reserved for the traditional province of Ulster, of which Northern Ireland comprises six out of nine counties.[12][73] The BBC style guide is to refer to Northern Ireland as a province, and use of the term is common in literature and newspaper reports on Northern Ireland and the United Kingdom. Some authors have described the meaning of this term as being equivocal: referring to Northern Ireland as being a province both of the United Kingdom and of the traditional country of Ireland.[77]		"Region" is used by several UK government agencies and the European Union. Some authors choose this word but note that it is "unsatisfactory".[12][13] Northern Ireland can also be simply described as "part of the UK", including by UK government offices.[61]		Many people inside and outside Northern Ireland use other names for Northern Ireland, depending on their point of view. Disagreement on names, and the reading of political symbolism into the use or non-use of a word, also attaches itself to some urban centres. The most notable example is whether Northern Ireland's second city should be called "Derry" or "Londonderry".		Choice of language and nomenclature in Northern Ireland often reveals the cultural, ethnic and religious identity of the speaker. The first Deputy First Minister of Northern Ireland, Seamus Mallon, was criticised by unionist politicians for calling the region the "North of Ireland" while Sinn Féin has been criticised in a Dublin newspaper for referring to the "Six Counties".[80]		Those who do not belong to any group but lean towards one side often tend to use the language of that group. Supporters of unionism in the British media (notably The Daily Telegraph and the Daily Express) regularly call Northern Ireland "Ulster".[81] Some media outlets in the Republic use "North of Ireland", "the North", or (less often) the "Six Counties".		Government and cultural organisations in Northern Ireland often use the word "Ulster" in their title; for example, the University of Ulster, the Ulster Museum, the Ulster Orchestra, and BBC Radio Ulster.		Although some news bulletins since the 1990s have opted to avoid all contentious terms and use the official name, Northern Ireland, the term "the North" remains commonly used by broadcast media in the Republic. For Northern Ireland's second largest city, broadcasting outlets which are unaligned to either community and broadcast to both use both names interchangeably, often starting a report with "Londonderry" and then using "Derry" in the rest of the report. However, within Northern Ireland, print media which are aligned to either ideology (the Belfast Telegraph and News Letter are unionist in outlook while the Irish News is nationalist) generally use their preferred term. British newspapers with unionist leanings, such as The Daily Telegraph, usually use unionist language. However the more left-wing Guardian recommends in its style guide using "Derry" and "County Derry", and "not Londonderry".[82]		The division in nomenclature is sometimes seen in the names of organisations associated with either side of the political divide, but there are exceptions. In Gaelic games, followed mainly by nationalists, the GAA county is "Derry", but in sports followed mainly by unionists, clubs tend to avoid the use of "Londonderry" in favour of more precise locales (Glendermott Cricket Club) or neutral terms (Foyle Hockey Club). "Derry" is also used in the names of both the Church of Ireland and Roman Catholic dioceses, and by one of the largest Protestant fraternal societies, the Apprentice Boys of Derry. There is no agreement on how to decide on a name. When the nationalist-controlled local council voted to rename the city "Derry", unionists objected, stating that as it owed its city status to a Royal Charter, only a charter issued by the Queen could change the name. The Queen has not intervened on the matter and thus the council is now called the Derry City Council while the city is still officially Londonderry. Nevertheless, the council has printed two sets of stationery—one for each term—and its policy is to reply to correspondence using whichever term the original sender used.		Northern Ireland was covered by an ice sheet for most of the last ice age and on numerous previous occasions, the legacy of which can be seen in the extensive coverage of drumlins in Counties Fermanagh, Armagh, Antrim and particularly Down.		The centrepiece of Northern Ireland's geography is Lough Neagh, at 151 square miles (391 km2) the largest freshwater lake both on the island of Ireland and in the British Isles. A second extensive lake system is centred on Lower and Upper Lough Erne in Fermanagh. The largest island of Northern Ireland is Rathlin, off the north Antrim coast. Strangford Lough is the largest inlet in the British Isles, covering 150 km2 (58 sq mi).		There are substantial uplands in the Sperrin Mountains (an extension of the Caledonian mountain belt) with extensive gold deposits, granite Mourne Mountains and basalt Antrim Plateau, as well as smaller ranges in South Armagh and along the Fermanagh–Tyrone border. None of the hills are especially high, with Slieve Donard in the dramatic Mournes reaching 850 metres (2,789 ft), Northern Ireland's highest point. Belfast's most prominent peak is Cavehill.		The volcanic activity which created the Antrim Plateau also formed the eerily geometric pillars of the Giant's Causeway on the north Antrim coast. Also in north Antrim are the Carrick-a-Rede Rope Bridge, Mussenden Temple and the Glens of Antrim.		The Lower and Upper River Bann, River Foyle and River Blackwater form extensive fertile lowlands, with excellent arable land also found in North and East Down, although much of the hill country is marginal and suitable largely for animal husbandry.		The valley of the River Lagan is dominated by Belfast, whose metropolitan area includes over a third of the population of Northern Ireland, with heavy urbanisation and industrialisation along the Lagan Valley and both shores of Belfast Lough.		The whole of Northern Ireland has a temperate maritime climate, rather wetter in the west than the east, although cloud cover is persistent across the region. The weather is unpredictable at all times of the year, and although the seasons are distinct, they are considerably less pronounced than in interior Europe or the eastern seaboard of North America. Average daytime maximums in Belfast are 6.5 °C (43.7 °F) in January and 17.5 °C (63.5 °F) in July. The highest maximum temperature recorded was 30.8 °C (87.4 °F) at Knockarevan, near Garrison, County Fermanagh on 30 June 1976 and at Belfast on 12 July 1983.[94] The lowest minimum temperature recorded was −18.7 °C (−1.7 °F) at Castlederg, County Tyrone on 23 December 2010.[95]		Northern Ireland consists of six historic counties: County Antrim, County Armagh, County Down, County Fermanagh, County Londonderry,[96] County Tyrone.		These counties are no longer used for local government purposes; instead there are eleven districts of Northern Ireland which have different geographical extents. These were created in 2015, replacing the twenty-six districts which previously existed.[97]		Although counties are no longer used for local governmental purposes, they remain a popular means of describing where places are. They are officially used while applying for an Irish passport, which requires one to state one's county of birth. The name of that county then appears in both Irish and English on the passport's information page, as opposed to the town or city of birth on the United Kingdom passport. The Gaelic Athletic Association still uses the counties as its primary means of organisation and fields representative teams of each GAA county. The original system of car registration numbers largely based on counties still remains in use. In 2000, the telephone numbering system was restructured into an 8 digit scheme with (except for Belfast) the first digit approximately reflecting the county.		The county boundaries still appear on Ordnance Survey of Northern Ireland Maps and the Phillips Street Atlases, among others. With their decline in official use, there is often confusion surrounding towns and cities which lie near county boundaries, such as Belfast and Lisburn, which are split between counties Down and Antrim (the majorities of both cities, however, are in Antrim).		Northern Ireland has traditionally had an industrial economy, most notably in shipbuilding, rope manufacture and textiles, but most heavy industry has since been replaced by services, primarily the public sector. Government subsidies account for 20% of the economy's revenue.[citation needed]		Seventy percent of the economy's revenue comes from the service sector. Apart from the public sector, another important service sector is tourism, which rose to account for over 1% of the economy's revenue in 2004. Tourism has been a major growth area since the end of the Troubles. Key tourism attractions include the historic cities of Derry, Belfast and Armagh and the many castles in Northern Ireland. More recently, the economy has benefited from major investment by many large multi-national corporations into high tech industry.[citation needed] These large firms are attracted by government subsidies and the skilled workforce in Northern Ireland.		The local economy has seen contraction during the Great Recession. In response, the Northern Ireland Assembly has sent trade missions abroad. The Executive wishes to gain taxation powers from London, to align Northern Ireland's corporation tax rate with the unusually low rate of the Republic of Ireland.		Northern Ireland has underdeveloped transport infrastructure, with most infrastructure concentrated around Greater Belfast, Greater Derry and Craigavon. Northern Ireland is served by three airports – Belfast International near Antrim, George Best Belfast City integrated into the railway network at Sydenham in East Belfast, and City of Derry in County Londonderry.		Major sea ports at Larne and Belfast carry passengers and freight between Great Britain and Northern Ireland.		Passenger railways are operated by Northern Ireland Railways. With Iarnród Éireann (Irish Rail), Northern Ireland Railways co-operates in providing the joint Enterprise service between Dublin Connolly and Belfast Central. The whole of Ireland has a mainline railway network with a gauge of 5 ft 3 in (1,600 mm), which is unique in Europe and has resulted in distinct rolling stock designs. Main railway lines linking to and from Belfast Great Victoria Street railway station and Belfast Central are:		Main motorways are:		The cross-border road connecting the ports of Larne in Northern Ireland and Rosslare Harbour in the Republic of Ireland is being upgraded as part of an EU-funded scheme. European route E01 runs from Larne through the island of Ireland, Spain and Portugal to Seville.		The population of Northern Ireland has risen yearly since 1978. The population in 2011 was 1.8 million, having grown 7.5% over the previous decade[98] from just under 1.7 million in 2001. This constitutes just under 3% of the population of the UK (62 million) and just over 28% of the population of the island of Ireland (6.3 million).		The population of Northern Ireland is almost entirely white (98.2%).[98] In 2011, 88.8% of the population were born in Northern Ireland, with 4.5% born in Britain, and 2.9% born in the Republic of Ireland. 4.3% were born elsewhere; triple the amount there were in 2001.[99] Most are from Eastern Europe and Lithuania and Latvia. The largest non-white ethnic groups were Chinese (6,300) and Indian (6,200). Black people of various origins made up 0.2% of the 2011 population and people of mixed ethnicity made up 0.2%.[100]		In the 2011 census, 41.5% of the population identified as belonging to Protestant or other non-Roman Catholic Christian denominations. The biggest of these denominations were the Presbyterian Church (19%), the Church of Ireland (14%) and the Methodist Church (3%). The largest single denomination is the Roman Catholic Church, to which 41% of the population belonged. 0.8% identified with non-Christian religions or philosophies, while 17% identified with no religion or did not state one.[100] In terms of community background (i.e. religion or religion brought up in), 48% of the population came from a Protestant background, 45% from a Catholic background, 0.9% from non-Christian backgrounds, and 5.6% from non-religious backgrounds.[100]		Belfast Derry		Lisburn Newry		In the 2011 census in Northern Ireland respondents gave their national identity as follows.[103]		Several studies and surveys carried out between 1971 and 2006 have indicated that, in general, most Protestants in Northern Ireland see themselves primarily as British, whereas a majority of Roman Catholics regard themselves primarily as Irish.[104][105][106][107][108][109][110][111] This does not however account for the complex identities within Northern Ireland, given that many of the population regard themselves as "Ulster" or "Northern Irish", either as a primary or secondary identity. Overall, the Catholic population is somewhat more ethnically diverse than the more homogeneous Protestant population. 83.1% of Protestants identified as "British" or with a British ethnic group (English, Scottish, or Welsh) in the 2011 Census, whereas only 3.9% identified as "Irish". Meanwhile, 13.7% of Catholics identified as "British" or with a British ethnic group. A further 4.4% identified as "all other", which are largely immigrants, for example from Poland.		A 2008 survey found that 57% of Protestants described themselves as British, while 32% identified as Northern Irish, 6% as Ulster and 4% as Irish. Compared to a similar survey carried out in 1998, this shows a fall in the percentage of Protestants identifying as British and Ulster, and a rise in those identifying as Northern Irish. The 2008 survey found that 61% of Catholics described themselves as Irish, with 25% identifying as Northern Irish, 8% as British and 1% as Ulster. These figures were largely unchanged from the 1998 results.[112][113]		People born in Northern Ireland are, with some exceptions, deemed by UK law to be citizens of the United Kingdom. They are also, with similar exceptions, entitled to be citizens of Ireland. This entitlement was reaffirmed in the 1998 Good Friday Agreement between the British and Irish governments, which provides that:		...it is the birthright of all the people of Northern Ireland to identify themselves and be accepted as Irish or British, or both, as they may so choose, and accordingly [the two governments] confirm that their right to hold both British and Irish citizenship is accepted by both Governments and would not be affected by any future change in the status of Northern Ireland.		As a result of the Agreement, the Constitution of the Republic of Ireland was amended. The current wording provides that people born in Northern Ireland are entitled to be Irish citizens on the same basis as people from any other part of the island.[114]		Neither government, however, extends its citizenship to all persons born in Northern Ireland. Both governments exclude some people born in Northern Ireland, in particular persons born without one parent who is a British or Irish citizen. The Irish restriction was given effect by the twenty-seventh amendment to the Irish Constitution in 2004. The position in UK nationality law is that most of those born in Northern Ireland are UK nationals, whether or not they so choose. Renunciation of British citizenship requires the payment of a fee, currently £229.[115]		In the 2011 census in Northern Ireland respondents stated that they held the following passports.[116]		English is spoken as a first language by almost all of the Northern Ireland population. It is the de facto official language and the Administration of Justice (Language) Act (Ireland) 1737 prohibits the use of languages other than English in legal proceedings.		Under the Good Friday Agreement, Irish and Ulster Scots (an Ulster dialect of the Scots language, sometimes known as Ullans), are recognised as "part of the cultural wealth of Northern Ireland".[117] Two all-island bodies for the promotion of these were created under the Agreement: Foras na Gaeilge, which promotes the Irish language, and the Ulster Scots Agency, which promotes the Ulster Scots dialect and culture. These operate separately under the aegis of the North/South Language Body, which reports to the North/South Ministerial Council.		The British government in 2001 ratified the European Charter for Regional or Minority Languages. Irish (in Northern Ireland) was specified under Part III of the Charter, with a range of specific undertakings in relation to education, translation of statutes, interaction with public authorities, the use of placenames, media access, support for cultural activities and other matters. A lower level of recognition was accorded to Ulster Scots, under Part II of the Charter.[118]		The dialect of English spoken in Northern Ireland shows influence from the lowland Scots language.[119] There are supposedly some minute differences in pronunciation between Protestants and Catholics, the best known of which is the name of the letter h, which Protestants tend to pronounce as "aitch", as in British English, and Catholics tend to pronounce as "haitch", as in Hiberno-English. However, geography is a much more important determinant of dialect than religious background.		The Irish language (Irish: an Ghaeilge), or Gaelic, is a native language of Ireland.[120] It was spoken predominantly throughout what is now Northern Ireland before the Ulster Plantations in the 17th century and most place names in Northern Ireland are anglicised versions of a Gaelic name. Today, the language is often associated with Irish nationalism (and thus with Catholics). However, in the 19th century, the language was seen as a common heritage, with Ulster Protestants playing a leading role in the Gaelic revival.		In the 2011 census, 11% of the population of Northern Ireland claimed "some knowledge of Irish"[98] and 3.7% reported being able to "speak, read, write and understand" Irish.[98] In another survey, from 1999, 1% of respondents said they spoke it as their main language at home.[121]		The dialect spoken in Northern Ireland, Ulster Irish, has two main types, East Ulster Irish and Donegal Irish (or West Ulster Irish),[122] is the one closest to Scottish Gaelic (which developed into a separate language from Irish Gaelic in the 17th century). Some words and phrases are shared with Scots Gaelic, and the dialects of east Ulster – those of Rathlin Island and the Glens of Antrim – were very similar to the dialect of Argyll, the part of Scotland nearest to Ireland. And those dialects of Armagh and Down were also very similar to the dialects of Galloway.		Use of the Irish language in Northern Ireland today is politically sensitive. The erection by some district councils of bilingual street names in both English and Irish,[123] invariably in predominantly nationalist districts, is resisted by unionists who claim that it creates a "chill factor" and thus harms community relationships. Efforts by members of the Northern Ireland Assembly to legislate for some official uses of the language have failed to achieve the required cross-community support, and the UK government has declined to legislate. There has recently been an increase in interest in the language among unionists in East Belfast.[124]		Ulster Scots comprises varieties of the Scots language spoken in Northern Ireland. For a native English speaker, "[Ulster Scots] is comparatively accessible, and even at its most intense can be understood fairly easily with the help of a glossary."[125]		Along with the Irish language, the Good Friday Agreement recognised the dialect as part of Northern Ireland's unique culture and the St Andrews Agreement recognised the need to "enhance and develop the Ulster Scots language, heritage and culture".[126]		Approximately 2% of the population claim to speak Ulster Scots.[127] However, the number speaking it as their main language in their home is negligible,[121] with only 0.9% of 2011 census respondents claiming to be able to speak, read, write and understand Ulster-Scots. 8.1% professed to have "some ability" however.[98]		The most common sign language in Northern Ireland is Northern Ireland Sign Language (NISL). However, because in the past Catholic families tended to send their deaf children to schools in Dublin[citation needed] where Irish Sign Language (ISL) is commonly used, ISL is still common among many older deaf people from Catholic families.		Irish Sign Language (ISL) has some influence from the French family of sign language, which includes American Sign Language (ASL). NISL takes a large component from the British family of sign language (which also includes Auslan) with many borrowings from ASL. It is described as being related to Irish Sign Language at the syntactic level while much of the lexicon is based on British Sign Language (BSL)[128] and American Sign Language.[citation needed]		As of March 2004[update] the British Government recognises only British Sign Language and Irish Sign Language as the official sign languages used in Northern Ireland.[129][130]		Northern Ireland shares both the culture of Ireland and the culture of the United Kingdom. Those of Catholic background tend to identity more with Irish culture, and those of Protestant background more with British culture. This has caused the two communities to become pillarised.		Parades are a prominent feature of Northern Ireland society,[131] more so than in the rest of Ireland or in Britain. Most are held by Protestant fraternities such as the Orange Order, and Ulster loyalist marching bands. Each summer, during the "marching season", these groups have hundreds of parades, deck streets with British flags, bunting and specially-made arches, and light large towering bonfires.[132] The biggest parades are held on 12 July (The Twelfth). There is often tension when these activities take place near Catholic neighbourhoods, which sometimes leads to violence.[133]		Since the end of the Troubles, Northern Ireland has witnessed rising numbers of tourists. Attractions include cultural festivals, musical and artistic traditions, countryside and geographical sites of interest, public houses, welcoming hospitality and sports (especially golf and fishing). Since 1987 public houses have been allowed to open on Sundays, despite some opposition.		The Ulster Cycle is a large body of prose and verse centring on the traditional heroes of the Ulaid in what is now eastern Ulster. This is one of the four major cycles of Irish mythology. The cycle centres on the reign of Conchobar mac Nessa, who is said to have been king of Ulster around the 1st century. He ruled from Emain Macha (now Navan Fort near Armagh), and had a fierce rivalry with queen Medb and king Ailill of Connacht and their ally, Fergus mac Róich, former king of Ulster. The foremost hero of the cycle is Conchobar's nephew Cúchulainn.		Northern Ireland comprises a patchwork of communities whose national loyalties are represented in some areas by flags flown from flagpoles or lamp posts. The Union Jack and the former Northern Ireland flag are flown in many loyalist areas, and the Tricolour, adopted by republicans as the flag of Ireland in 1916,[135] is flown in some republican areas. Even kerbstones in some areas are painted red-white-blue or green-white-orange, depending on whether local people express unionist/loyalist or nationalist/republican sympathies.[136]		The official flag is that of the state having sovereignty over the territory, i.e. the Union Flag.[137] The former Northern Ireland flag, also known as the "Ulster Banner" or "Red Hand Flag", is a banner derived from the coat of arms of the Government of Northern Ireland until 1972. Since 1972, it has had no official status. The Union Flag and the Ulster Banner are used exclusively by unionists. UK flags policy states that in Northern Ireland, "The Ulster flag and the Cross of St Patrick have no official status and, under the Flags Regulations, are not permitted to be flown from Government Buildings."[138][139]		The Irish Rugby Football Union and the Church of Ireland have used the Saint Patrick's Saltire or "Cross of St Patrick". This red saltire on a white field was used to represent Ireland in the flag of the United Kingdom. It is still used by some British army regiments. Foreign flags are also found, such as the Palestinian flags in some nationalist areas and Israeli flags in some unionist areas.[140]		The United Kingdom national anthem of "God Save the Queen" is often played at state events in Northern Ireland. At the Commonwealth Games and some other sporting events, the Northern Ireland team uses the Ulster Banner as its flag—notwithstanding its lack of official status—and the Londonderry Air (usually set to lyrics as Danny Boy), which also has no official status, as its national anthem.[141][142] The national football team also uses the Ulster Banner as its flag but uses "God Save The Queen" as its anthem.[143] Major Gaelic Athletic Association matches are opened by the Irish national anthem, "Amhrán na bhFiann (The Soldier's Song)", which is also used by most other all-Ireland sporting organisations.[144] Since 1995, the Ireland rugby union team has used a specially commissioned song, "Ireland's Call" as the team's anthem. The Irish national anthem is also played at Dublin home matches, being the anthem of the host country.[145]		Northern Irish murals have become well-known features of Northern Ireland, depicting past and present events and documenting peace and cultural diversity. Almost 2,000 murals have been documented in Northern Ireland since the 1970s.		In Northern Ireland, sport is popular and important in the lives of many people. Sports tend to be organised on an all-Ireland basis, with a single team for the whole island.[146] The most notable exception is association football, which has separate governing bodies for each jurisdiction.[146]		The Irish Football Association (IFA) serves as the organising body for association football in Northern Ireland, with the Northern Ireland Football League (NIFL) responsible for the independent administration of the three divisions of national domestic football, as well as the Northern Ireland Football League Cup.		The highest level of competition within Northern Ireland are the NIFL Premiership and the NIFL Championship. However, many players from Northern Ireland compete with clubs in England and Scotland.		NIFL clubs are semi-professional or Intermediate.NIFL Premiership clubs are also eligible to compete in the UEFA Champions League and UEFA Europa League with the league champions entering the Champions league second qualifying round and the 2nd placed league finisher, the European play-off winners and the Irish Cup winners entering the Europa League second qualifying round.No clubs have ever reached the group stage.		Despite Northern Ireland's small population, the national team qualified for the World Cup in 1958, 1982 and 1986, making it to the quarter-finals in 1958 and 1982 and made it the first knockout round in the European Championships in 2016.		The six counties of Northern Ireland are among the nine governed by the Ulster branch of the Irish Rugby Football Union, the governing body of rugby union in Ireland. Ulster is one of the four professional provincial teams in Ireland and competes in the Celtic League and European Cup. It won the European Cup in 1999.		In international competitions, the Ireland national rugby union team's recent successes include four Triple Crowns between 2004 and 2009 and a Grand Slam in 2009 in the Six Nations Championship.		Northern Ireland plays as the Ireland cricket team which represents both Northern Ireland and Republic of Ireland. The Ireland Cricket team is a full member of the International Cricket Council, having been granted Test status and full membership (along with Afghanistan) by the ICC in June, 2017. They are currently able to compete in Test cricket, the highest level of competitive cricket in the international arena and they are one of the twelve full-member countries under the ICC.		Ireland is the current champion of the ICC Intercontinental Cup. One of Ireland's regular international venues is Stormont in Belfast.		Gaelic games include Gaelic football, hurling (and camogie), handball and rounders. Of the four, football is the most popular in Northern Ireland. Players play for local clubs with the best being selected for their county teams. The Ulster GAA is the branch of the Gaelic Athletic Association that is responsible for the nine counties of Ulster, which include the six of Northern Ireland.		These nine county teams participate in the Ulster Senior Football Championship, Ulster Senior Hurling Championship, All-Ireland Senior Football Championship and All-Ireland Senior Hurling Championship.		Recent successes for Northern Ireland teams include Armagh's 2002 All-Ireland Senior Football Championship win and Tyrone's wins in 2003, 2005 and 2008.		Perhaps Northern Ireland's most notable successes in professional sport have come in golf. Northern Ireland has contributed more major champions in the modern era than any other European country, with three in the space of just 14 months from the US Open in 2010 to The Open Championship in 2011. Notable golfers include Fred Daly (winner of The Open in 1947), Ryder Cup players Ronan Rafferty and David Feherty, leading European Tour professionals David Jones, Michael Hoey (a winner on Tour in 2011) and Gareth Maybin, as well as three recent major winners Graeme McDowell (winner of the US Open in 2010, the first European to do so since 1970), Rory McIlroy (winner of four majors) and Darren Clarke (winner of The Open in 2011).[147][148] Northern Ireland has also contributed several players to the Great Britain and Ireland Walker Cup team, including Alan Dunbar and Paul Cutler who played on the victorious 2011 team in Scotland.		The Golfing Union of Ireland, the governing body for men's and boy's amateur golf throughout Ireland and the oldest golfing union in the world, was founded in Belfast in 1891. Northern Ireland's golf courses include the Royal Belfast Golf Club (the earliest, formed in 1881), Royal Portrush Golf Club, which is the only course outside Great Britain to have hosted The Open Championship, and Royal County Down Golf Club (Golf Digest magazine's top-rated course outside the United States).[149][150]		Northern Ireland has produced two world snooker champions; Alex Higgins, who won the title in 1972 and 1982, and Dennis Taylor, who won in 1985. The highest-ranked Northern Ireland professional on the world circuit presently is Mark Allen from Antrim. The sport is governed locally by the Northern Ireland Billiards and Snooker Association who run regular ranking tournaments and competitions.		Although Northern Ireland lacks an international automobile racecourse, two Northern Irish drivers have finished inside the top two of Formula One, with John Watson achieving the feat in 1982 and Eddie Irvine doing the same in 1999. The largest course and the only MSA-licensed track for UK-wide competition is Kirkistown.[151]		The Ireland national rugby league team has participated in the Emerging Nations Tournament (1995), the Super League World Nines (1996), the World Cup (2000 and 2008), European Nations Cup (since 2003) and Victory Cup (2004).		The Ireland A rugby league team compete annually in the Amateur Four Nations competition (since 2002) and the St Patrick's Day Challenge (since 1995).		In 2007, after the closure of UCW (Ulster Championship Wrestling) which was a wrestling promotion, PWU formed, standing for Pro Wrestling Ulster. The wrestling promotion features championships, former WWE superstars and local independent wrestlers. Events and IPPV's throughout Northern Ireland.[152]		Unlike most areas of the United Kingdom, in the last year of primary school many children sit entrance examinations for grammar schools.		Integrated schools, which attempt to ensure a balance in enrolment between pupils of Protestant, Roman Catholic and other faiths (or none), are becoming increasingly popular, although Northern Ireland still has a primarily de facto religiously segregated education system. In the primary school sector, forty schools (8.9% of the total number) are integrated schools and thirty-two (7.2% of the total number) are Irish language-medium schools.		The main universities in Northern Ireland are Queen's University Belfast and Ulster University, and the distance learning Open University which has a regional office in Belfast.		The BBC has a division called BBC Northern Ireland with headquarters in Belfast. As well as broadcasting standard UK-wide programmes, BBC NI produces local content, including a news break-out called BBC Newsline. The ITV franchise in Northern Ireland is Ulster Television (UTV). The state-owned Channel 4 and the privately owned Channel 5 also broadcast in Northern Ireland. Access is available to satellite and cable services.[153] All Northern Ireland viewers must obtain a UK TV licence to watch live television transmissions.		RTÉ, the national broadcaster of the Republic of Ireland, is available over the air to most parts of Northern Ireland via reception overspill[154] and via satellite and cable. Since the digital TV switchover, RTÉ One, RTÉ2 and the Irish-language channel TG4, are now available over the air on the UK's Freeview system from transmitters within Northern Ireland.[155] Although they are transmitted in standard definition, a Freeview HD box or television is required for reception.		As well as the standard UK-wide radio stations from the BBC, Northern Ireland is home to many local radio stations, such as Cool FM, CityBeat, and Q102.9. The BBC has two regional radio stations which broadcast in Northern Ireland, BBC Radio Ulster and BBC Radio Foyle.		The Belfast Telegraph is the leading newspaper, and UK and Irish national newspapers are also available. There is a range of local newspapers such as the News Letter and the Irish News.[156]		Northern Ireland uses the same telecommunications and postal services as the rest of the United Kingdom at standard domestic rates and there are no mobile roaming charges between Great Britain and Northern Ireland.[157][158] People in Northern Ireland who live close to the border with the Republic of Ireland may inadvertently switch over to the Irish mobile networks, causing international roaming fees to be applied.[159] Calls from landlines in Northern Ireland to numbers in the Republic of Ireland are charged at the same rate as those to numbers in Great Britain, while landline numbers in Northern Ireland can similarly be called from the Republic of Ireland at domestic rates, using the 048 prefix.[160]		Both the national flag and the national anthem of present-day Ireland drive origins directly from the Rising. At first it still appeared as if the revolutionaries would take over the old symbols because on the roof of their headquarters, the Dublin General Post Office, a green flag with the harp was hoisted next to the republican tricolour although with the inscription 'Irish Republic'. Even 'Got save Ireland' was sung by the revolutionaries during Easter week. But after the failure of the Rising and the subsequent executions of the leading revolutionaries the tricolour and 'The Soldier's Song' became more and more popular as symbols of the rebellion.		Click on a coloured area to see an article about English in that country or region		
Established in 1944, the RMIT University Student Union or RUSU, is the peak representative body for all students enrolled at RMIT University. The Student Union is independent of the university and operates under the direction of annually elected student representatives. According to the constitution, all students are automatic members of the Student Union but may choose to become a financial member.[3] RUSU works in collaboration with its sister organisation the RMIT Vietnam Student Council to achieve common aims and objectives for all students.		The Student Union offers a range of services, including student rights advocacy, campus activities and events, funding student media including RMITV & Catalyst as well as hosting Women's, Queer and Postgraduate student lounges. RUSU is also responsible for funding and supporting over clubs & societies that are either Academic, Cultural, Political, Social or Spiritual based. RMIT Link, which is run by the university (not the Student Union) funds and manages all Arts and Sports clubs. RUSU has offices at the three major Melbourne campuses and sites of RMIT University. RUSU is an affiliated body to the National Union of Students and the Council of Australian Postgraduate Associations.						John Storey Junior helped found the Student Representative Council in 1944, acted as its first President, and lobbied for the establishment of a central library. His studies were cut short when he was diagnosed with leukaemia and died in 1947, aged just 22. His recognition of service to the RMIT community lives on with one RMIT's most striking buildings – Storey Hall – in tribute to John Storey Junior and his father Sir John.[4] Over the years since its founding, the student union has continued to grow and expand into more areas to become an integral part of the student experience on campus.		In 2006, with the introduction of voluntary student unionism (VSU) legislation, the Student Union underwent a major re-organization. Most of the staff were made redundant, the organization's three separate campus councils were merged, and several services such as the second-hand bookshop were abandoned. While the organization suffered a drastic funding cut (from $3.9 million AUD to $1.3 million AUD) as a result of VSU, it managed to survive the cutbacks and continue providing services, advocacy and representation to students.		As of 2006, the Student Union Council has 25 voting members, who are elected by RMIT students at annual elections. Each Melbourne campus of RMIT (Brunswick, Bundoora and City) has a campus coordinator and a general campus representative as part of the 25 voting member structure.		Councillors are typically elected in September and hold November to October terms. Ex-officio (non-voting) members may be appointed to the Student Union Council at its discretion. All members of the Student Union Council must be financial members of the Student Union.		The Student Union Council meets regularly, and it is also responsible for electing the president and communication officer, as outlined in the Student Union Constitution. A smaller group of student office bearers, known as the Secretariat, meets more regularly to discuss day-to-day operational, staffing, and other urgent matters.		The Student Union in addition to having student representatives as board directors of the organisation, employ professional staff to help deliver key programs and services and assist in governance. All staff members are supervised by an elected student representative as determined by the Secretariat.		The RMIT Student Union funds the student-run magazine Catalyst & student television on-campus production studios RMITV. It continues to have strong ties with SYN radio station located within RMIT, however there is no formal or funding relationship between the separate organisations.		Catalyst Magazine was first published in 1944, the same year the Student Union was established. It continues today as the only official student magazine and news source through its website.		
The Worker Student Alliance (WSA) in the United States was the section of Students for a Democratic Society led by the Progressive Labor Party. The WSA argued that the best way to build a movement in the working class, like SDS wanted, was for students to become involved in workers' struggles both on and off the campuses. In practice, that usually meant students enrolled in school would get jobs as cafeteria hands and other manual labor jobs at those schools.		The WSA explicitly rejected the rest of the New Left's insistence that it would be various combinations of 'progressive' nationalism and popular rebellion that would jump-start the revolution; rather, the WSA said the catalyst would be organized workers in various industries and the service sector, and that students could best help spread and deepen workers' class consciousness by really being among the workers themselves, rather than just using their class designation in rhetoric to appear more Marxist.		The WSA faction took about 900 of the approximately 1400 representatives in the split at the 1969 SDS convention in Chicago. The other 500, who had been the Revolutionary Youth Movement, left to form a myriad of other groups. SDS chapters around the country then split along these same lines, or disbanded entirely.		Both the RYM and the WSA kept the SDS name, but the Weatherman organization continued to hold the SDS National Office and all the SDS membership lists; thus it was able to assume effective "command" of the name and public face of SDS despite its inferior size. Nearly immediately post-conference, Weatherman led their "Days of Rage" Chicago riots of 1969 and other sporadic acts of violence — all under the SDS name — until 1970, when Mark Rudd and a few other Weathermen decided to close the SDS National Office and drop the SDS name. PL, however, continued to keep its SDS for several more years. Since all active SDS chapters after 1970 were SDS-WSA, the "WSA" initials were dropped.		In 1974 PL's SDS voted to dissolve itself and join the Committee Against Racism which PL had helped to form at a conference at New York University in November 1973. The CAR eventually expanded to several other countries and added "International" to its name to become InCAR, but InCAR was in many respects another "mass organization" led and directed by the PLP, with separate publications and staff, but always with a goal of winning InCAR members to support PLP. By 1996, this strategy was too much to maintain, and PLP elected to pursue pure and open communist activity again, using only its own party as an organizational structure.				
Vocational education is education that prepares people to work in a trade, a craft, as a technician, or in professional vocations such as engineering, accountancy, nursing, medicine, architecture, or law. Craft vocations are usually based on manual or practical activities and are traditionally non-academic but related to a specific trade or occupation. Vocational education is sometimes referred to as career education or technical education.[1]		Vocational education can take place at the secondary, post-secondary, further education, and higher education level; and can interact with the apprenticeship system. At the post-secondary level, vocational education is often provided by highly specialized trade, Technical schools, community colleges, colleges of further education UK, universities, Institutes of technology / Polytechnic Institutes.		Until recently, almost all vocational education took place in the classroom, or on the job site, with students learning trade skills and trade theory from accredited professors or established professionals. However, online vocational education has grown in popularity, and made it easier than ever for students to learn various trade skills and soft skills from established professionals in the industry.		Wilhelm von Humboldt's educational model goes beyond vocational training. In a letter to the Prussian king, he wrote: "There are undeniably certain kinds of knowledge that must be of a general nature and, more importantly, a certain cultivation of the mind and character that nobody can afford to be without. People obviously cannot be good craftworkers, merchants, soldiers or businessmen unless, regardless of their occupation, they are good, upstanding and – according to their condition – well-informed human beings and citizens. If this basis is laid through schooling, vocational skills are easily acquired later on, and a person is always free to move from one occupation to another, as so often happens in life."[2] The philosopher Julian Nida-Rümelin criticized discrepancies between Humboldt's ideals and the contemporary European education policy, which narrowly understands education as a preparation for the labor market, and argued that we need to decide between "McKinsey", to describe vocational training, and Humboldt.[3]						In Australia vocational education and training is mostly post-secondary and provided through the vocational education and training (VET) system by registered training organisations. However some senior schools do offer school-based apprenticeships and traineeships for students in years 10, 11 and 12. There were 24 Technical Colleges in Australia but now only 5 independent Trade Colleges remain with three in Queensland; one in Townsville (Tec-NQ), one in Brisbane (Australian Trade College) and one on the Gold Coast (Australian Industry Trade College) and one in Adelaide and Perth. This system encompasses both public, TAFE, and private providers in a national training framework consisting of the Australian Quality Training Framework, Australian Qualifications Framework and Industry Training Packages which define the assessment standards for the different vocational qualifications.		Australia’s apprenticeship system includes both apprenticeships in "traditional" trades and "traineeships" in other more service-oriented occupations. Both involve a legal contract between the employer and the apprentice or trainee and provide a combination of school-based and workplace training. Apprenticeships typically last three to four years, traineeships only one to two years. Apprentices and trainees receive a wage which increases as they progress through the training scheme.[4]		Since the states and territories are responsible for most public delivery and all regulation of providers, a central concept of the VET system is "national recognition", whereby the assessments and awards of any one registered training organisation must be recognised by all others, and the decisions of any state or territory training authority must be recognised by the other states and territories. This allows national portability of qualifications and units of competency.		A crucial feature of the training package (which accounts for about 60% of publicly funded training and almost all apprenticeship training) is that the content of the vocational qualifications is theoretically defined by industry and not by government or training providers. A Training Package is "owned" by one of 11 Industry Skills Councils which are responsible for developing and reviewing the qualifications.		The National Centre for Vocational Education Research or NCVER is a not-for-profit company owned by the federal, state and territory ministries responsible for training. It is responsible for collecting, managing, analysing, evaluating and communicating research and statistics about vocational education and training (VET).		The boundaries between vocational education and tertiary education are becoming more blurred. A number of vocational training providers such as Melbourne Polytechnic, BHI and WAI are now offering specialised bachelor's degrees in specific areas not being adequately provided by universities. Such applied courses include equine studies, winemaking and viticulture, aquaculture, information technology, music, illustration, culinary management and many more.[5]		The largest and the most unified system of vocational education was created in the Soviet Union with the professional`no-tehnicheskoye uchilische and Tehnikum. But it became less effective with the transition of the economies of post-Soviet countries to a market economy.		Education and training is the responsibility of member states, but the single European labour market makes some cooperation on education imperative, including on vocational education and training. The 'Copenhagen process', based on the open method of cooperation between Member States, was launched in 2002 in order to help make vocational education and training better and more attractive to learners throughout Europe. The process is based on mutually agreed priorities that are reviewed periodically. Much of the activity is monitored by Cedefop, the European Centre for the Development of Vocational Training.		There is strong support, particularly in northern Europe, for a shift of resources from university education to vocational training. This is due to the perception that an oversupply of university graduates in many fields of study has aggravated graduate unemployment and underemployment. At the same time, employers are experiencing a shortage of skilled tradespeople.[6]		In Finland, vocational education belongs to secondary education. After the nine-year comprehensive school, almost all students choose to go to either a lukio (high school), which is an institution preparing students for tertiary education, or to a vocational school. Both forms of secondary education last three years, and give a formal qualification to enter university or ammattikorkeakoulu, i.e., Finnish polytechnics. In certain fields (e.g., the police school, air traffic control personnel training), the entrance requirements of vocational schools include completion of the lukio, thus causing the students to complete their secondary education twice.		The education in vocational school is free, and students from low-income families are eligible for a state student grant. The curriculum is primarily vocational, and the academic part of the curriculum is adapted to the needs of a given course. The vocational schools are mostly maintained by municipalities.		After completing secondary education, one can enter higher vocational schools (ammattikorkeakoulu, or AMK) or universities.		It is also possible for a student to choose both lukio and vocational schooling. The education in such cases lasts usually from three to four years.		Vocational education is an important part of the education systems in Austria, Germany, Liechtenstein, Belgium (Deutsche Sprachgemeinschaft Ostbelgien) and Switzerland (including French- and Italian-speaking regions) and one element of the German model.		For example, in Germany a law (the Berufsausbildungsgesetz) was passed in 1969 which regulated and unified the vocational training system and codified the shared responsibility of the state, the unions, associations and Industrie- und Handelskammer (chambers of trade and industry). The system is very popular in modern Germany: in 2001, two-thirds of young people aged under 22 began an apprenticeship, and 78% of them completed it, meaning that approximately 51% of all young people under 22 have completed an apprenticeship. One in three companies offered apprenticeships in 2003; in 2004 the government signed a pledge with industrial unions that all companies except very small ones must take on apprentices.		The vocational education systems in the other German-speaking countries are very similar to the German system and a vocational qualification from one country is generally also recognized in the other states within this area.		In Hong Kong, vocational education is usually for post-secondary 6 students. The Hong Kong Institute of Vocational Education (IVE) provides training in nine different vocational fields, namely: applied science, business administration, child education and community services, construction, design, printing, textiles and clothing, hotel service and tourism studies, information technology, electrical and electronic engineering, and mechanical, manufacturing and industrial engineering.		Normally at the end of elementary school (at age 14) students are directed to one of three types of upper secondary education: one academic track (gymnasium) and two vocational tracks. Vocational secondary schools (szakközépiskola) provide four years of general education and also prepare students for the maturata (school leaving certificate). These schools combine general education with some specific subjects, referred to as pre-vocational education and career orientation. At that point many students enrol in a post-secondary VET programme often at the same institution, to obtain a vocational qualification, although they may also seek entry to tertiary education.		Vocational training schools (szakiskola) initially provide two years of general education, combined with some pre-vocational education and career orientation, they then choose an occupation, and then receive two or three years of vocational education and training focusing on that occupation—such as bricklayer. Students do not obtain the maturata but a vocational qualification at the end of a successfully completed programme. Demand for vocational training schools, both from the labour market and among students, has declined while it has increased for upper secondary schools delivering the maturata.[7]		Vocational training historically has been a subject handled by the Ministry of Labour, other central ministries and various state-level organizations. To harmonize the variations and multiplicity in terms of standards and costs, the National Skills Qualification Framework was launched in December 2013.		The National Skills Qualifications Framework (NSQF) is a competency-based framework that organizes all qualifications according to a series of levels of knowledge, skills and aptitude. These levels, graded from one to ten, are defined in terms of learning outcomes which the learner must possess regardless of whether they are obtained through formal, non-formal or informal learning. NSQF in India was notified on 27 December 2013. All other frameworks, including the NVEQF (National Vocational Educational Qualification Framework) released by the Ministry of HRD, stand superseded by the NSQF.		In November 2014 the new Government in India formed the Ministry of Skill Development & Entrepreneurship. Articulating the need for such a Ministry, the Prime Minister said, [1], "A separate Ministry, which will look after promoting entrepreneurship and skill development, would be created. Even developed countries have accorded priority to promoting skilled manpower".		As a continuation of its efforts to harmonize and consolidate skill development activities across the country, the Government launched the 1st Skill India Development Mission (NSDM) on 15 July 2015. Also launched on the day was the National Policy for Skill Development & Entrepreneurship.		Today all skill development efforts through the Government (Directorate General of Training) and through the Public Private Partnership arm (National Skill Development Corporation) are carried out under the Ministry, through the Skill India Mission.		The Ministry works with various central ministries and departments and the State government in implementing the NSQF across all Government funded projects, based on a five-year implementation schedule for complete convergence.		The involvement of the private sector in various aspects of skill development has enhanced access, quality, and innovative financing models leading to sustainable skill development organizations on the ground.[citation needed] The short-term skill development programs (largely offered by private organizations) combined with the long-term programs offered by the Indian technical institutes (ITIs) complement each other under the larger framework. Credit equivalency, transnational standards, quality assurance and standards are being managed by the Ministry through the National Skill Development Agency (an autonomous body under the Ministry) in close partnership with industry-led sector-specific bodies (Sector Skill Councils) and various line ministries.		India has bilateral collaboration with governments including those of the UK, Australia, Germany, Canada, and the UAE, with the intention of implementing globally acceptable standards and providing the Indian workforce with overseas job mobility.[citation needed]		Japanese vocational schools are known as senmon gakkō. They are part of Japan's higher education system. They are two-year schools that many students study at after finishing high school (although it is not always required that students graduate from high school). Some have a wide range of majors, others only a few majors. Some examples are computer technology, fashion, and English.		Vocational high schools offer programmes in five fields: agriculture, technology/engineering, commerce/business, maritime/fishery, and home economics. In principle, all students in the first year of high school (10th grade) follow a common national curriculum, In the second and third years (11th and 12th grades) students are offered courses relevant to their specialisation. In some programmes, students may participate in workplace training through co-operation between schools and local employers. The government is now piloting Vocational Meister Schools in which workplace training is an important part of the programme. Around half of all vocational high schools are private. Private and public schools operate according to similar rules; for example, they charge the same fees for high school education, with an exemption for poorer families.		The number of students in vocational high schools has decreased, from about half of students in 1995 down to about one-quarter today. To make vocational high schools more attractive, in April 2007 the Korean government changed the name of vocational high schools into professional high schools. With the change of the name the government also facilitated the entry of vocational high school graduates to colleges and universities.		Most vocational high school students continue into tertiary education; in 2007 43% transferred to junior colleges and 25% to university. At tertiary level, vocational education and training is provided in junior colleges (two- and three-year programmes) and at polytechnic colleges. Education at junior colleges and in two-year programmes in polytechnic colleges leads to an Industrial associate degree. Polytechnics also provide one-year programmes for craftsmen and master craftsmen and short programmes for employed workers. The requirements for admission to these institutions are in principle the same as those in the rest of tertiary sector (on the basis of the College Scholastic Aptitude Test) but candidates with vocational qualifications are given priority in the admission process. Junior colleges have expanded rapidly in response to demand and in 2006 enrolled around 27% of all tertiary students.		95% of junior college students are in private institutions. Fees charged by private colleges are approximately twice those of public institutions. Polytechnic colleges are state-run institutions under the responsibility of the Ministry of Labour; government funding keeps student fees much lower than those charged by other tertiary institutions. Around 5% of students are enrolled in polytechnic colleges.[8]		Skills training are no longer depicted as second-class education in Malaysia. There are numerous vocational education centres here including vocational schools (high schools to train skilled students), technic schools (high schools to train future engineers) and vocational colleges all of them under the Ministry of Education. Then there are 33 polytechnics and 86 community colleges under the Ministry of Higher Education; 10 MARA Advanced Skills Colleges, 13 MARA Skills Institutes, 286 GIATMARAs under Majlis Amanah Rakyat (MARA) and 15 National Youth Skills Institutes under Ministry of Youth and Sports. The first vocational institute in Malaysia is the Industrial Training Institute of Kuala Lumpur established in 1964 under the Manpower Department. Other institutes under the same department including 8 Advanced Technology Training Centres, one Centre for Instructor and Advanced Skill Training, one Japan-Malaysia Technical Institute and the other 21 ITIs.		In Mexico, both federal and state governments are responsible for the administration of vocational education. Federal schools are funded by the federal budget, in addition to their own funding sources. The state governments are responsible for the management of decentralised institutions, such as the State Centres for Scientific and Technological Studies (CECyTE) and Institutes of Training for Work (ICAT). These institutions are funded 50% from the federal budget and 50% from the state budget. The state governments also manage and fund "decentralised institutions of the federation", such as CONALEP schools.		Compulsory education (including primary and lower secondary education) finishes at the age of 15 and about half of those aged 15-to-19 are enrolled full-time or part-time in education. All programmes at upper secondary level require the payment of a tuition fee.		The upper secondary vocational education system in Mexico includes over a dozen subsystems (administrative units within the Upper Secondary Education Undersecretariat of the Ministry of Public Education, responsible for vocational programmes) which differ from each other to varying degrees in content, administration, and target group. The large number of school types and corresponding administrative units within the Ministry of Public Education makes the institutional landscape of vocational education and training complex by international standards.		Vocational education and training provided under the Upper Secondary Education Undersecretariat includes three main types of programme:		Nearly all of those leaving lower secondary school enter upper secondary education, and around 50% of them follow one of four vocational programmes; technology, economics, agricultural, personal/social services & health care. These programmes vary from 1 to 4 years (by level; only level 2, 3 and 4 diplomas are considered formal ‘start qualifications’ for successfully entering the labour market). The programmes can be attended in either of two pathways. One either involving a minimum of 20% of school time (apprenticeship pathway; BBL-BeroepsBegeleidende Leerweg) or the other, involving a maximum of 80% schooltime (BOL -BeroepsOpleidende Leerweg). The remaining time in both cases is apprenticeship/work in a company. So in effect, students have a choice out of 32 trajectories, leading to over 600 professional qualifications. BBL-Apprentices usually receive a wage negotiated in collective agreements. Employers taking on these apprentices receive a subsidy in the form of a tax reduction on the wages of the apprentice. (WVA-Wet vermindering afdracht). Level 4 graduates of senior secondary VET may go directly to institutes for Higher Profession Education and Training (HBO-Hoger beroepsonderwijs), after which entering university is a possibility. The social partners participate actively in the development of policy. As of January 1, 2012 they formed a foundation for Co operation Vocational Education and Entrepreneurship (St. SBB – stichting Samenwerking Beroepsonderwijs Bedrijfsleven; www.s-bb.nl). Its responsibility is to advise the Minister on the development of the national vocational education and training system, based on the full consensus of the constituent members (the representative organisations of schools and of entrepreneurship and their centres of expertise). Special topics are Qualification & Examination, Apprenticeships (BPV-Beroepspraktijkvorming) and (labourmarket) Efficiency of VET. The Centres of Expertices are linked to the four vocational education programmes provided in senior secondary VET on the content of VET programmes and on trends and future skill needs. The Local County Vocational Training (MBO Raad www.mboraad.nl) represents the VET schools in this foundation and advise on the quality, operations and provision of VET.[9]		New Zealand is served by 11 Industry Training Organisations (ITO). The unique element is that ITOs purchase training as well as set standards and aggregate industry opinion about skills in the labour market. Industry Training, as organised by ITOs, has expanded from apprenticeships to a more true lifelong learning situation with, for example, over 10% of trainees aged 50 or over. Moreover, much of the training is generic. This challenges the prevailing idea of vocational education and the standard layperson view that it focuses on apprenticeships.		One source for information in New Zealand is the Industry Training Federation.[10] Another is the Ministry of Education.[11]		Polytechnics, Private Training Establishments, Wananga and others also deliver vocational training, amongst other areas.		Nearly all those leaving lower secondary school enter upper secondary education, and around half follow one of nine vocational programmes. These programmes typically involve two years in school followed by two years of apprenticeship in a company. The first year provides general education alongside introductory knowledge of the vocational area. During the second year, courses become more trade-specific.		Apprentices receive a wage negotiated in collective agreements ranging between 30% and 80% of the wage of a qualified worker; the percentage increase over the apprenticeship period. Employers taking on apprentices receive a subsidy, equivalent to the cost of one year in school. After the two years vocational school programme some students opt for a third year in the ‘general’ programme as an alternative to an apprenticeship. Both apprenticeship and a third year of practical training in school lead to the same vocational qualifications. Upper secondary VET graduates may go directly to Vocational Technical Colleges, while those who wish to enter university need to take a supplementary year of education.		The social partners participate actively in the development of policy. The National Council for Vocational Education and Training advises the Minister on the development of the national vocational education and training system. The Advisory Councils for Vocational Education and Training are linked to the nine vocational education programmes provided in upper secondary education and advise on the content of VET programmes and on trends and future skill needs. The National Curriculum groups assist in deciding the contents of the vocational training within the specific occupations. The Local County Vocational Training Committees advise on the quality, provision of VET and career guidance.[12]		In Paraguay, vocational education is known as Bachillerato Técnico and is part of the secondary education system. These schools combine general education with some specific subjects, referred to as pre-vocational education and career orientation. After nine years of Educación Escolar Básica (Primary School), the student can choose to go to either a Bachillerato Técnico (Vocational School) or a Bachillerato Científico (High School). Both forms of secondary education last three years, and are usually located in the same campus called Colegio.		After completing secondary education, one can enter to the universities. It is also possible for a student to choose both Técnico and Científico schooling.		Vocational training from Agricultural subjects to ICT related subjects are available in Sri Lanka. In 2005 the Ministry of Vocational and Technical Training (MVTT) introduced the National Vocational Qualifications (NVQ) framework which was an important milestone for the education, economic and social development of Sri Lanka. The NVQ framework consists of seven levels of instruction. NVQ levels 1 to 4 are for craftsmen designation and successful candidates are issued with National certificates. NVQ levels 5 and 6 are Diploma level, whereas Level 7 is for degree equivalent qualification.		Training courses are provided by many institutions island wide. All training providers (public and private) must obtain institutional registration and course accreditation from the Tertiary and Vocational Education Commission (TVEC).In order to obtain registration institutions must satisfy specific criteria: infrastructure, basic services, tools and equipment, quality of instruction and staff, based on curriculum and syllabus, and quality of management and monitoring systems.		Government Ministries and Agencies involved in Vocational Training are The Ministry of Vocational and Technical Training (MVTT), The Tertiary and Vocational Education Commission (TVEC), The National Apprentice and Industrial Training Authority (NAITA), The Department of Technical Education and Training (DTET), The Vocational Training Authority (VTA) and the National Youth Services Council (NYSC).[13]		Nearly all of those leaving compulsory schooling immediately enter upper secondary schools, and most complete their upper secondary education in three years. Upper secondary education is divided into 13 vocationally oriented and 4 academic national programmes. Slightly more than half of all students follow vocational programmes. All programmes offer broad general education and basic eligibility to continue studies at the post-secondary level. In addition, there are local programmes specially designed to meet local needs and ‘individual’ programmes.		A 1992 school reform extended vocational upper secondary programmes by one year, aligning them with three years of general upper secondary education, increasing their general education content, and making core subjects compulsory in all programmes. The core subjects (which occupy around one-third of total teaching time in both vocational and academic programmes) include English, artistic activities, physical education and health, mathematics, natural science, social studies, Swedish or Swedish as a second language, and religious studies. In addition to the core subjects, students pursue optional courses, subjects which are specific to each programme and a special project.		Vocational programmes include 15 weeks of workplace training (Arbetsplatsförlagd utbildning – APU) over the three-year period. Schools are responsible for arranging workplace training and verifying its quality. Most municipalities have advisory bodies: programme councils (programmråd) and vocational councils (yrkesråd) composed of employers’ and employees’ representatives from the locality. The councils advise schools on matters such as provision of workplace training courses, equipment purchase and training of supervisors in APU.[8]		Nearly two thirds of those entering upper secondary education enter the vocational education and training system. At this level, vocational education and training is mainly provided through the ‘dual system’. Students spend some of their time in a vocational school; some of their time doing an apprenticeship at a host company; and for most programmes, students attend industry courses at an industry training centre to develop complementary practical skills relating to the occupation at hand. Common patterns are for students to spend one- two days per week at the vocational school and three-four days doing the apprenticeship at the host company; alternatively they alternate between some weeks attending classes at the vocational school and some weeks attending industry courses at an industry training centre. A different pattern is to begin the programme with most of the time devoted to in-school education and gradually diminishing the amount of in-school education in favour of more in-company training.		Switzerland draws a distinction between vocational education and training (VET) programmes at upper-secondary level, and professional education and training (PET) programmes, which take place at tertiary B level. In 2007, more than half of the population aged 25–64 had a VET or PET qualification as their highest level of education. In addition, universities of applied sciences (Fachhochschulen) offer vocational education at tertiary A level. Pathways enable people to shift from one part of the education system to another.[14]		Students in Turkey may choose vocational high schools after completing the 8-year-long compulsory primary and secondary education. Vocational high school graduates may pursue two year-long polytechnics or may continue with a related tertiary degree.		According to a survey by OECD, 38% of 15-year-old students attend vocational study programmes that are offered by Anatolian vocational, Anatolian technical, and technical high schools.[15]		Municipalities in Turkey also offer vocational training. The metropolitan municipality of Istanbul, the most populous city in Turkey, offers year long free vocational programs in a wide range of topics through ISMEK,[16] an umbrella organization formed under the municipality.		The first "Trades School" in the UK was Stanley Technical Trades School (now Harris Academy South Norwood) which was designed, built and set up by William Stanley. The initial idea was thought of in 1901, and the school opened in 1907.[17]		The system of vocational education in the UK initially developed independently of the state, with bodies such as the RSA and City & Guilds setting examinations for technical subjects. The Education Act 1944 made provision for a Tripartite System of grammar schools, secondary technical schools and secondary modern schools, but by 1975 only 0.5% of British senior pupils were in technical schools, compared to two-thirds of the equivalent German age group.[18]		Successive recent British Governments have made attempts to promote and expand vocational education. In the 1970s, the Business And Technology Education Council was founded to confer further and higher education awards, particularly to further education colleges in the United Kingdom. In the 1980s and 1990s, the Conservative Government promoted the Youth Training Scheme, National Vocational Qualifications and General National Vocational Qualifications. However, youth training was marginalised as the proportion of young people staying on in full-time education increased.[18]		In 1994, publicly funded Modern Apprenticeships were introduced to provide "quality training on a work-based (educational) route".[19] Numbers of apprentices have grown in recent years and the Department for Children, Schools and Families has stated its intention to make apprenticeships a "mainstream" part of England's education system.[20]		In the UK some higher engineering-technician positions that require 4–5 years' apprenticeship require academic study to HNC / HND or higher City & Guilds level. Apprenticeships are increasingly recognised as the gold standard for work-based training. There are four levels of apprenticeship available for those aged 16 and over:		Apprentices work towards work-based learning qualifications such as a Level 2 Competence Qualification, Functional Skills and, in most cases, a relevant knowledge-based qualification.		Apprentices work towards work-based learning such as a Level 3 Competence Qualification, Functional Skills and, in most cases, a relevant knowledgebased qualification. They can take four years to complete.		Apprentices work towards work-based learning qualifications such as a Level 4 and 5 Competence Qualification, Functional Skills and, in some cases, a knowledge-based qualification such as a Foundation Degree. They can take between four and five years to complete, depending on the level at which an apprentice enrolls.		They are similar to higher apprenticeships, but differ in that they provide an opportunity to gain a full bachelor’s (Level 6) or master's degree (Level 7). The courses are designed in partnership with employers, with part-time study taking place at a university. They can take between four and six years to complete, depending on the level of the course, and the level of entry.		Main article: TVET (Technical and Vocational Education and Training)		TVET (Technical and Vocational Education and Training) is education and training that provides the necessary knowledge and skills for employment.[21] It uses many forms of education including formal, non-formal and informal learning,[22] and is considered to be a crucial vehicle for social equity and inclusion, as well as for the sustainability of development. TVET, together with literacy and higher education, is one of three priority subsectors for UNESCO in its work to foster inclusive and equitable quality education and lifelong learning opportunities for all.[23]		The development and definition of TVET is one that parallels other types of education and training, such as Vocational Education; however, TVET was officiated on an international level as a better term to describe the field, and therefore is likewise used as an umbrella term to encompass education and training activities such as Vocational Education.[21]		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		
The Helsinki University of Technology (TKK; Finnish: Teknillinen korkeakoulu; Swedish: Tekniska högskolan) was a technical university in Finland. It was located in Otaniemi, Espoo in the metropolitan area of Greater Helsinki. The university was founded in 1849 by Grand Duke Nicholas I and received university status in 1908. It moved from Helsinki to Otaniemi campus area in 1966. It was merged into Aalto University in 2010 and briefly had the name Aalto University School of Science and Technology before being split into four schools in 2011.		Much of the university's Otaniemi campus was designed by Alvar Aalto.						In 1849, TKK was established in Helsinki by the decree of the Russian Emperor Nicholas I, Grand Duke of Finland as a "manufacture and handicraft school", with the name Helsingin teknillinen reaalikoulu/Helsingfors tekniska realskola, along with two other similar schools, situated in Vaasa and Turku. In 1872, the school's name was changed to Polyteknillinen koulu/Polytekniska skolan ("Polytechnical School") and in 1878, to Polyteknillinen opisto/Polytekniska institutet ("Polytechnical Institute"), while the two other manufacture and handiwork schools were demoted to institutions of lower level. As the proportion of matriculation diploma holders in the student intake gradually increased, the school gained more social respectability. In 1908, TKK was given university status along with its present name, thus becoming the second university to be founded in Finland. In 1955, building of the new campus area started with the housing village. In 1966, TKK moved from Helsinki to the new campus in Otaniemi, Espoo.		In the past, the university was also known by the abbreviations HUT and TH, from its English language and Swedish language names, but in 2005 a decision was made to officially solely use the abbreviation TKK for branding reasons.		In 2010, TKK was merged with Helsinki School of Economics and University of Art and Design Helsinki into Aalto University. After brief existence in the new university as own institution, Aalto University School of Science and Technology, it was split into four schools, corresponding to four old faculties, School of Engineering, School of Science, School of Electrical Engineering, and Aalto University School of Chemical Technology. In 2012, the Department of Architecture of the School of Engineering, formerly of Faculty of Engineering and Architecture, was merged with Aalto University School of Art and Design into Aalto University School of Arts, Design and Architecture.		All engineering programmes offered by TKK led to the degree of diplomi-insinööri ("engineer with university diploma"), a five-year master's degree. The only exceptions to this were the architecture programmes that lead to the master's degrees of architecture and landscape architecture. From 2005, according to the Bologna process, all students might also complete an intermediate degree (tekniikan kandidaatti, TkK) before the DI or architect's degree. This degree is considered a bachelor's degree and enables enrollment in foreign universities where a bachelor's degree is required. TKK did not offer programs terminating in a bachelor's degree; a student might only be accepted to study for the Master's level degree. TKK required a bachelor's degree from foreign students studying in English, because only Master's studies were offered completely in English.		Apart from numerous programs in Finnish language, various international Master's programs were offered exclusively for studies in English.		The university was organized in four faculties, each consisting of departments and separate laboratories, and separate units not operating under any faculty.		Additionally, TKK participated in various joint units with other Finnish universities and the VTT Technical Research Centre of Finland:		TKK participated in 12 Centres of Excellence (huippuyksikkö), selected by the Academy of Finland to represent the top research in the country and receiving separate, fixed-period funding from the Academy.		Researchers at TKK have achieved notability in, among other things, low temperature physics (holding the current world record for the lowest temperature achieved), the development of devices and methods for magnetoencephalography, mobile communications, wood processing, and neural networks, with professor Teuvo Kohonen initiating research in self-organizing maps. Additionally, the first commercialised total synthesis, the synthesis of camphor, was invented by Gustaf Komppa, the first professor of chemistry at TKK[37] and the Nobel laureate (chemistry, 1945) Artturi Virtanen held a professorship in biochemistry at TKK. More recently, the university has notably invested in the research of nanotechnology, operating the largest cleanroom facility in Northern Europe[38] and of the largest microscopy clusters in Europe.[39]		The Nokia Research Center has operated a "lablet" on university premises since 2008, in order to establish joint research programs and daily interaction between Nokia and university researchers, who would share the same facilities.		TKK was located in Otaniemi, Espoo. Several high-tech companies, the Finnish forest industry's joint experimental laboratory KCL, and business incubators Innopoli and Technopolis are also situated there. It is also directly adjacent to Keilaniemi, with Life Science Center and the headquarters of several notable Finnish companies, such as Nokia and Fortum. The area is connected by a 15-minute bus ride to the center of Helsinki.		TKK was known for its active student community and technology students (teekkaris) are highly noticeable, as they wear a distinctive hat and often brightly colored overalls to many of their public events. The community has also organised important charity events (tempaus in local language). TKK students are also famous for, and Finland's leading practitioners of, student pranks (jäynä), similar in principle to MIT hacks. Their most widely publicised stunt took place in 1961, when a team of students smuggled a statue of Paavo Nurmi onto the 300-year-old wreck of Regalskeppet Vasa just days before its lifting from the bottom of the sea.[40]		The Student Union of Helsinki University of Technology (TKY, Finnish: Teknillisen korkeakoulun ylioppilaskunta, Swedish: Tekniska högskolans studentkår) was the interest group for the students of the university. In 2006 it had 11,187 members,[41] which included all the students of the university, as is stipulated by Finnish law.[42] It was founded in 1872.		TKK was also one of the two universities in Finland to host one or more nations, a Finnish type of student corporation. The only nation at TKK was Teknologföreningen (TF) and its goal was to unite Swedish-speaking students at TKK. Teknologföreningen was founded in 1872, that is, prior to the student union. Teknologföreningen also has its own building opposite to Dipoli called Urdsgjallar, completed in 1966. The Finnish-speaking student nation Tekniikan Ylioppilaat was disbanded in 1972 and its functions given to the university student union, since a separate Finnish-speaking nation in a university with an overwhelming Finnish-speaking majority was considered unnecessary. The regional Finnish-speaking nations at the University of Helsinki also accepted TKK students as members.[43][44]		The housing area of Otaniemi campus, known as Teekkarikylä (technology student village), was owned mostly by the student union and partly by HOAS (Helsinki Student Housing Fund). The housing was characterised by the presence of foreign students of many nationalities. As of 2005, the village offered housing for approximately 2,600 students.[45]		Construction of the Otaniemi campus was started in 1950, in order for the first buildings to host the athletes of the 1952 Summer Olympics in Helsinki. Some of the building material originally used for the campus was acquired from the former Soviet Union embassy, which had been destroyed during World War II,[46] as a result of bombings by Soviet Union itself. Later the student housing has been used for housing athletes again in a number of athletics events, sometimes to the dismay of the students that have to move out during the events. The quality of the Otaniemi student housing holds a high standard in international comparison.		The campus contains the former student union building and convention centre Dipoli, named as the second Poli, the second building of the polytechnic students. The original first building being located formerly in the Helsinki centre. Dipoli was designed by Reima and Raili Pietilä and was completed in 1966. However, in 1993 the building was transformed into a training centre of the university. The ownership of the property was later transferred from the student union to the university itself, due to high maintenance costs. It is regularly used for conventions, congresses and student parties.		In addition to the student union TKK students have formed numerous associations for studies, cultural activity and sports. In 2007, there were some 150 associations maintained by university students. In 2006, two-thirds of the student union members were members of "the guilds",[41] which are student associations uniting students inside their department, e.g. the Guild of Electrical Engineers.		Currently this list includes only the associations known to have English Wikipedia articles.		Aalto University		Coordinates: 60°11′9″N 024°49′40″E﻿ / ﻿60.18583°N 24.82778°E﻿ / 60.18583; 24.82778		
Phonological history		The Organisation internationale de la Francophonie (OIF), generally known as the Francophonie (French: La Francophonie [la fʁɑ̃kɔfɔni]),[3][4] but also called International Organisation of La Francophonie in English language context,[5] is an international organization representing countries and regions where French is the first ("mother") or customary language, where a significant proportion of the population are francophones (French speakers) or where there is a notable affiliation with French culture.		The organization comprises 57 member states and governments, three associate members and twenty observers. The term francophonie (with a lowercase "f"), or francosphere (often capitalised in English) also refers to the global community of French-speaking peoples,[6] comprising a network of private and public organizations promoting equal ties among countries where French people or France played a significant historical role, culturally, militarily or politically.		French geographer Onésime Reclus, brother of Élisée Reclus, coined the word Francophonie in 1880 to refer to the community of people and countries using the French language. Francophonie was then coined a second time by Léopold Sédar Senghor, founder of the Négritude movement, in the review Esprit in 1962, who assimilated it into Humanism.[7][8]		The modern organisation was created in 1970. Its motto is égalité, complémentarité, solidarité ("equality, complementarity, and solidarity"),[1] a deliberate allusion to France's motto liberté, égalité, fraternité. Started as a small club of northern French-speaking countries, the Francophonie has since evolved into a global organization whose numerous branches cooperate with its member states in the fields of culture, science, economy, justice, and peace.						The convention which created the Agency for Cultural and Technical Co-operation (Agence de Coopération Culturelle et Technique) was signed on 20 March 1970 by the representatives of the 21 states and governments under the influence of African Heads of State, Léopold Sédar Senghor of Senegal, Habib Bourguiba of Tunisia, Hamani Diori of Niger and Prince Norodom Sihanouk of Cambodia.		The missions of this new intergovernmental organization, based on the sharing of the French language, are the promotion of the cultures of its members and the intensification of the cultural and technical cooperation between them, as well as the solidarity and the connection between them through dialogue.		The Francophonie project ceaselessly evolved since the creation of the Agency for Cultural and Technical Co-operation, it became the intergovernmental Agency of the Francophonie (Agence intergouvernementale de la Francophonie) in 1998 to remind its intergovernmental status. Finally in 2005, the adoption of a new Charter of the Francophonie (la Charte de la Francophonie) gives the name to the Agency of international Organization of the Francophonie (Organisation internationale de la Francophonie).[9]		The position of Secretary-General was created in 1997 at the seventh leaders' summit held in Hanoi. Canadian Jean-Louis Roy was secretary of the Agence de coopération culturelle et technique from 1989 until the formal creation of the Agence intergouvernementale de la Francophonie in 1997 with former Secretary-General of the United Nations Boutros Boutros-Ghali as the first secretary-general of La Francophonie. Abdou Diouf, the former president of the Republic of Senegal, became Secretary General in January 1, 2003. He was reelected on 29 September 2006, for a second mandate during the Summit of the Francophonie of Bucharest, and elected again in 2010 at the Summit of the Francophonie of Montreux for another mandate which ran until 31 December 2014. At the 2014 summit in Dakar, former Governor General of Canada Michaëlle Jean was chosen to lead the organization starting in January 2015.[10][11]		The Secretary General of the Francophonie is elected during the Summit. He/she is the keystone of the institutional device and of the Francophonie and leads the organization. He/she is the spokesperson and the official representative internationally of the political actions of the Francophonie. The Secretary General is responsible for proposing priority areas for multilateral Francophonie actions. His/her job is to facilitate Francophone multilateral cooperation and to ensure that programs and activities of all operating agencies work in harmony. The Secretary General carries out his/her four-year mandate under the authority of the three main institutions of the Francophonie: the Summits, the Ministerial Conference and the Permanent Council.[12]		The Summit, the highest authority in the Francophonie, is held every two years and gathers the Heads of states and governments of all member countries of the International Organization of the Francophonie around themes of discussion. It is chaired by the Head of state and government of the host country, and this person assumes that responsibility until the next Summit. By enabling the Heads of state and government to hold a dialogue on all of the international issues of the day, the Summit serves to develop strategies and goals of the Francophonie so as to ensure the organization's influence on the world scene.[13]		Armenia is to play host to the next summit in 2018 and Tunisia is to host in 2020.[19]		The Ministerial Conference of the Francophonie gathers the foreign or francophone affairs ministers of member states and governments every year to ensure the political continuity of the Summit. This conference ensures that the decisions made during the previous Summits are carried out and to plan the next Summit. It also recommends new members and observers to the Summit.[12]		The Permanent Council of the Francophonie gathers the Ambassadors of the member countries, chaired by the General Secretary of the Francophonie and under the authority of the Ministerial Conference, its main task is to plan Summits. This conference also supervises the execution of the Summit decisions made by the ministerial conferences on a day-to-day basis, about the examination of the propositions of the budget distribution.[12]		The objectives of the Parliamentary Assembly of the Francophonie are to represent to the French-speaking authorities, the interests of the French-speaking communities, to promote the democracy, the rule of law and the respect of human rights. Furthermore, it follows the execution by the operators of the Francophonie of action plans elaborated by the Conference of the members using French as a common language It also favours the cooperation and strengthens the solidarity within the French-speaking communities, mainly towards the parliaments of the South. The Parliamentary Assembly of the Francophonie is constituted by member sections representing 77 parliaments or interparliamentary organizations. The Secretary General is the French senator Jacques Legendre.[12]		The Agency of the Francophonie is the main operator of the cultural, scientific, technical, economic and legal cooperation programs decided at the Summits. It is also the legal seat of the Secretary General and is used by him as an administrative support. The agency also contributes to the development of the French language and to the promotion of the diverse languages and cultures of its members, while encouraging mutual understanding between them and the Francophonie. For this reason, it is a place of exchange and dialogue and its simultaneous in Francophone countries. The Agency's headquarters are in Paris and it has three regional branches in Libreville, Gabon; Lomé, Togo; and Hanoi, Vietnam.[20]		Mauritania's membership was suspended on August 26, 2008, pending democratic elections, after a military coup d'état.[21] Madagascar's membership was suspended in April 2009 due to unconstitutional transfer of power on 17 March 2009.[22] Mali's membership was also suspended in March 2012[23] due to a coup d'état, and then the Central African Republic was suspended for instances of la Francophonie at the 88th session of the CPF (March 2012), as well as Guinea-Bissau on April 18, 2012[24] for the same reason. Thailand, an observer nation, was suspended in 2014 following the 2013–14 political crisis.[25]		The International Organization of the Francophonie relies on five operating agencies to carry out its mandate: l’Agence Universitaire de la Francophonie (AUF); TV5Monde; l’Association Internationale des Maires Francophones (AIMF); l'Association des Fonctionnaires Francophones des Organisations Internationales (AFFOI); and l’Université Senghor d’Alexandrie.[26]		Established in 1961 in Montreal, the Association of Francophone Universities gathers institutions of higher education and research among the Francophone countries of Africa, the Arab world, Southeast Asia, Central and Eastern Europe, and the Caribbean.		Its mission is to contribute to the construction and consolidation of a scientific space in French. It supports the French language, cultural and linguistic diversity, law and democracy, and the environment and sustainable development. It also provides an important mobility program for the students, the researchers and the professors.[27]		Established in 2008 in The Hague, the Assemblée des francophones fonctionnaires des organisations internationales (AFFOI) gathers international civil servants from all international organisations of the world—such as United Nations, the European Commission of the African Union—and coming from the member countries of the Francophonie.[28]		Its mission is to support the French language and the linguistic diversity within International Organisations. Every year the association coordinates the day of French language within International Organisations.[29] It also organizes seminaries to increase awareness about the importance of linguistic, cultural and conceptual diversity. The president is the French international civil servant Dominique Hoppe.		TV5Monde is the first international French language television network, available in many countries. On television and online the audience of TV5Monde has grown rapidly. TV5 is one of the three largest television networks in the world (along with the BBC and CNN), and is considered one of the greatest achievements of the Francophonie.[30] It provides wide access to original television programmes in French, and contributes to the development of the language and French-speaking cultures. It broadcasts the different forms of the French language spoken around the world, with all their accents. It reaches beyond native speakers of French; the majority of those who can receive it and part of its audience comprise viewers for whom French is not the mother tongue. Thanks to subtitles in various languages, it provides access to the Francophonie to non-French speakers - it is translated into 12 languages.[31]		The International Association of French-speaking Mayors was created in Quebec City in 1979 on the initiative of Jean Pelletier and Jacques Chirac, then the respective mayors of Quebec City and Paris. It is an operating agency for urban development gathering 48 countries or governments. The goal is to establish close cooperation in all areas of municipal activities. Its missions are to strengthen local democracy, building municipal capacities, and to support the populations. The association pursues its actions in the domains of health, culture, youth and education, urban development, training, and municipal infrastructures.[32][33]		The project of creation of a French-speaking university in the service of the African development was presented and adopted following the Dakar Summit in 1989. The Senghor University is a private postgraduate institution that trains managers and high-level trainers in areas that are a priority for development in Francophone Africa.[34] It directs the capacities of the managers and trainers to the action and the exercise of responsibilities in certain domains for the development: the project management, the financial institutions, the environment, the nutrition-health and of the cultural heritage. The Senghor University organizes regularly seminaries to help its students and of the public specialized in the domains of its action, by collaborating with the other operators and the institutions of the Francophonie.[35]		The International Organization of the Francophonie leads political actions and multilateral cooperation according to the missions drawn by the Summits of the Francophonie. The Summits gather the Heads of states and governments of the member countries of the International Organization of the Francophonie where they discuss international politics, world economy, French-speaking cooperation, human rights, education, culture and democracy. Actions of the International Organization of the Francophonie are scheduled over a period of four years and funded by contributions from its members.[36]		The Charte de la Francophonie defines the role and missions of the organization. The current charter was adopted in Antananarivo, on 23 November 2005. The summit held in Ouagadougou, Burkina Faso on 26–27 November 2004 saw the adoption of a strategic framework for the period 2004–2014.		The four missions drawn by the Summit of the Francophonie are:		The primary mission of the organization is the promotion of the French language as an international language and the promotion of worldwide cultural and linguistic diversity in the era of economic globalization. In this regard, countries that are members of the Francophonie have contributed largely to the adoption by the UNESCO of the Convention on the Protection and Promotion of the Diversity of Cultural Expressions (20 October 2005).		At the national level, there is the problem of promoting the French language within the context of its co-existence with other partner or international languages in most member countries, especially in Africa. Maintaining the relative importance of the status of French is an imperative that requires solidarity and the pooling of means and resources among countries committed to the French language within their respective societies.		The Francophonie has been a pioneer in terms of the recognition of cultural diversity and dialogue of cultures. It must find ways of confronting the trend towards uniformity that accompanies globalization and fostering the preservation and development of cultural diversity.[37]		Similar to the Commonwealth of Nations, the Francophonie has as its stated aims the promotion of democracy and human rights. Following the 3 November 2000 Déclaration de Bamako,[38] the Francophonie has given itself the financial means to attain a number of set objectives in that regard.		The Francophonie intends to contribute significantly to promoting peace, democracy and support for the rule of law and human rights by focusing on prevention. Political stability and full rights for all, the subject of the Bamako declaration, are considered key to sustainable development.		The Francophonie has chosen to provide its member countries with access to the expertise of its extensive intergovernmental, institutional, academic and non-governmental network with a view to building national capacities, resolving conflict and providing support for ending crises.[39]		In recent years, some participating governments, notably the governments of Canada and Quebec, pushed for the adoption of a Charter in order for the organization to sanction member States that are known to have poor records when it comes to the protection of human rights and the practice of democracy. Such a measure was debated at least twice but was never approved.		The International Organization of the Francophonie aims at connecting the various peoples using French as a common language through their knowledge. Education, like access to autonomy and information for all, begins with all children having access to a full primary education free of any inequality. It involves an integrated approach of teaching and training from primary to secondary school that will lead to employment. Education policies must also give French an integral place alongside the partner languages. Last, the research potential of French-language academic streams must be promoted.[39]		The Francophonie is committed to working towards sustainable development by supporting the improvement of economic governance, capacity building, cooperation and the search for common positions in major international negotiations. It's necessary to manage durably the natural resources, particularly the energy and the water, and politics are established to make sure of the conservation of these resources with effective anti-poverty campaigns.[40]		In 2013, the United Nations Volunteers programme received a financial contribution from the Federal Public Service (FPS) Foreign Affairs, Foreign Trade and Development Cooperation of the Kingdom of Belgium for the years 2013 and 2014 to support the outreach to the francophone world and the promotion of volunteerism via its Online Volunteering service.[41]		Coordinates: 48°51′36″N 2°18′12″E﻿ / ﻿48.86000°N 2.30333°E﻿ / 48.86000; 2.30333		
School bullying is a type of bullying that occurs in an educational setting.		Bullying without comprehensive definition, can be physical, sexual, verbal or emotional in nature. This can involve physical, emotional, verbal, and cyberbullying. For an act to be considered bullying it must meet certain criteria. This includes hostile intent, imbalance of power, repetition, distress, and provocation. Bullying can have a wide spectrum of effects on a student including anger, depression, stress and suicide. The person who is bullied is affected, and the bully can also grow up to develop different social disorders or have higher chances of engaging in criminal activity.		If there is suspicion that a child is being bullied or is a bully, there are warning signs in their behavior. There are many programs and organizations worldwide which provide bullying prevention services or information on how to cope if a child has been bullied.						There is no universal definition of school bullying; however, it is widely agreed that bullying is a subcategory of aggressive behavior characterized by the following three minimum criteria:[2]		The following two additional criteria have been proposed to complement the above-mentioned criteria:		Some of these characteristics have been disputed (e.g., for power imbalance: bullies and victims often report that conflicts occur between two equals); nevertheless, they remain widely established in the scientific literature.[2]		The underlying causes of school violence and bullying include gender and social norms and wider contextual and structural factors.[4]		Discriminatory gender norms that shape the dominance of men and the subservience of women and the perpetuation of these norms through violence are found in some form in almost every culture. Gender inequality and the prevalence of violence against women in society exacerbate the problem. Similarly, social norms that support the authority of teachers over children may legitimise the use of violence to maintain discipline and control.[4]		The pressure to conform to dominant gender norms is also high.[5] Young people who cannot or who choose not to conform to these norms are often punished for this through violence and bullying at school.[4]		Schools themselves can ‘teach’ children to be violent through discriminatory practices, curricula and textbooks. If unchecked, gender discrimination and power imbalances in schools can encourage attitudes and practices that subjugate children, uphold unequal gender norms and tolerate violence, including corporal punishment.[4]		Schools and the education system also operate within the context of wider social and structural factors and may reflect and reproduce environments that do not protect children and adolescents from violence and bullying. For example, physical and sexual violence may be more prevalent in schools in contexts where it is also more prevalent in wider society. Studies suggest that sexual violence and harassment of girls is worse in schools where other forms of violence are prevalent, and in conflict and emergency contexts,[6] and that gang violence is more common in schools where gangs, weapons and drugs are part of the local culture.[4] 		A victim, in the short term, may feel depressed, anxious, angry, have excessive stress, learned helplessness, feel as though their life has fallen apart, have a significant drop in school performance, or may commit suicide (bullycide). In the long term, they may feel insecure, lack trust, exhibit extreme sensitivity (hypervigilant), or develop a mental illness such as psychopathy, avoidant personality disorder or PTSD. They may also desire vengeance, sometimes leading them to torment others in return.[7]		Anxiety, depression and psychosomatic symptoms are common among both bullies and their victims. Among these participants alcohol and substance abuse is commonly seen later on in life.[8]		In the short term, being a bystander "can produce feelings of anger, fear, guilt, and sadness.... Bystanders who witness repeated victimization of peers can experience negative effects similar to the victimized children themselves."[9]		While most bullies, in the long term, grow to be emotionally functional adults, many have an increased risk of developing antisocial personality disorder, which is linked with increased risk of committing criminal acts (including domestic violence).[10]		The educational effects on victims of school violence and bullying are significant. Violence and bullying at the hands of teachers or other students may make children and adolescents afraid to go to school and interfere with their ability to concentrate in class or participate in school activities. It can also have similar effects on bystanders.[4]		The consequences include missing classes, avoiding school activities, playing truant or dropping out of school altogether. This in turn has an adverse impact on academic achievement and attainment and on future education and employment prospects. Children and adolescents who are victims of violence may achieve lower grades and may be less likely to anticipate going on to higher education. Analyses of international learning assessments highlight the impact of bullying on learning outcomes. These analyses clearly show that bullying reduces students’ achievement in key subjects such as mathematics and other studies have also documented the negative impact of school violence and bullying on educational performance.[11][4]		Bystanders and the school climate as a whole are also affected by school violence and bullying. Unsafe learning environments create a climate of fear and insecurity and a perception that teachers do not have control or do not care about students’ well-being, and this reduces the quality of education for all students.[4]		The 2006 UN World Report on Violence against Children shows that victims of corporal punishment, both at school and at home, may develop into adults who are passive and over-cautious or who are aggressive themselves. Involvement in school bullying can be a predictor of future antisocial and criminal behaviour. Being bullied is also linked to heightened risk of eating disorders and social and relationship difficulties.[12][4]		Other studies have shown the longer-term effects of bullying at school. For example, in a study of all children born in England, Scotland and Wales during one week in 1958, data on 7,771 children who had been bullied at ages 7 and 11 was studied. At age 50, those who had been bullied as children were less likely to have obtained school qualifications and less likely to live with a spouse or partner or to have adequate social support. They also had lower scores on word memory tests designed to measure cognitive IQ even when their childhood intelligence levels were taken into account and more often reported that they had poor health. The effects of bullying were visible nearly four decades later, with health, social and economic consequences lasting well into adulthood. For children, “peers are a much more important influence than has been realised. It is a terrible thing to be excluded by your peers”.[13][4]		The economic impact of violence against children and adolescents is substantial.[14] Youth violence in Brazil alone is estimated to cost nearly US$19 billion every year, of which US$943 million can be linked to violence in schools. The estimated cost to the economy in the USA of violence associated with schools is US$7.9 billion a year.[15][4]		Analytic work supported by the United States Agency for International Development (USAID) shows that school-related gender-based violence alone can be associated with the loss of one primary grade of schooling, which translates to an annual cost of around US$17 billion to low- and middle-income countries.[16][4]		In the East Asia and Pacific region, it is estimated that the economic costs of just some of the health consequences of child maltreatment were equivalent to between 1.4% and 2.5% of the region’s annual GDP.[4]		In Argentina, the forgone benefit to society from overall early school dropout is 11.4% of GDP, and in Egypt, nearly 7% of potential earnings is lost as a result of the number of children dropping out of school.[4]		A study has shown that each year Cameroon, Democratic Republic of Congo and Nigeria lose US$974 million, US$301 million and US$1,662 million respectively for failing to educate girls to the same standard as boys, and violence in school is one of the key factors contributing to the under-representation of girls in education.[17][4]		According to the American Psychological Association, "40% to 80% of school-age children experience bullying at some point during their school careers."[18] Various studies show that students from lower socioeconomic backgrounds experience bullying more often than other students.[19][dead link] The following statistics help illustrate the severity of bullying within classrooms:[18]		Statistics referencing the prevalence of bullying in schools may be inaccurate and tend to fluctuate. In a U.S. study of 5,621 students ages 12–18, 64% of the students had experienced bullying and did not report it.[29]		Proactive aggression is a behavior that expects a reward. With bullying each individual has a role to defend.[clarification needed] Some children act proactively but will show aggression to defend themselves if provoked. These children will react aggressively but tend to never be the ones to attack first.		There have been two subtypes created in bully classification; popular aggressive and unpopular aggressive. Popular aggressive bullies are social and do not encounter a great deal of social stigma from their aggression. Unpopular aggressive bullies, however, are most often rejected by other students and use aggression to seek attention.[20]		In a survey by the Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD), students were asked to complete a questionnaire.		A total of 10.6% of the children replied that they had sometimes bullied other children, a response category defined as moderate bullying. An additional 8.8% said they had bullied others once a week or more, defined as frequent bullying. Similarly, 8.5% said they had been targets of moderate bullying, and 8.4% said they were bullied frequently. Out of all the students, 13% said they had engaged in moderate or frequent bullying of others, while 10.6% said they had been bullied either moderately or frequently. Some students — 6.3% — had both bullied others and been bullied themselves. In all, 29% of the students who responded to the survey had been involved in some aspect of bullying, either as a bully, as the target of bullying or both.[37]		According to Tara Kuther, an associate professor of psychology at Western Connecticut State University, "...bullying gets so much more sophisticated and subtle in high school. It's more relational. It becomes more difficult for teens to know when to intervene; whereas with younger kids, bullying is more physical and, therefore, more clear-cut."[35]		There are four basic types of bullying: verbal, physical, psychological, and cyber. Cyberbullying is becoming one of the most common types. While victims can experience bullying at any age, it is witnessed most often in school-aged children.		Direct bullying is a relatively open attack on a victim that is physical and/or verbal in nature.[9] Indirect bullying is more subtle and harder to detect, but involves one or more forms of relational aggression, including social isolation via intentional exclusion, spreading rumors to defame one's character or reputation, making faces or obscene gestures behind someone's back, and manipulating friendships or other relationships.[9]		Pack bullying is bullying undertaken by a group. The 2009 Wesley Report on bullying found that pack bullying was more prominent in high schools and lasted longer than bullying undertaken by individuals.[38]		Physical bullying is any unwanted physical contact between the bully and the victim. This is one of the most easily identifiable forms of bullying. Examples include:[39][40]		Emotional bullying is any form of bullying that causes damage to a victim’s psyche and/or emotional well-being. Examples include:[39][40]		Verbal bullying is any slanderous statements or accusations that cause the victim undue emotional distress. Examples include:[40]		According to the website Stop Cyberbullying, "Cyberbullying is when anyone is tormented, threatened, harassed, humiliated, embarrassed or otherwise targeted by another child, preteen or teen using the Internet, interactive and digital technologies or mobile phones."[43] This form of bullying can easily go undetected because of the lack of parental or authoritative supervision. Because bullies can pose as someone else, it is the most anonymous form of bullying. Cyberbullying includes abuse using email, blogs, instant messaging, text messaging, or websites. Many who are bullied in school are likely to be bullied over the Internet and vice versa.[40]		Cyberbullying can happen 24 hours a day and seven days a week and reach a child even when they are alone. Deleting inappropriate or harassing messages,texts or pictures is extremely difficult after being posted or sent[45]		According to the website Stop Cyberbullying, "When schools try and get involved by disciplining the student for cyberbullying actions that took place off campus and outside of school hours, they are often sued for exceeding their authority and violating the student's free speech right." [43]		Cyberbullying has become extremely prevalent; 95% of teens who use social media reported having witnessed malicious behavior on social media from 2009 to 2013.[46] As sites like Facebook or Twitter offer no routine monitoring, children from a young age must learn proper internet behavior, say Abraham Foxman and Cyndi Silverman. "This is a call for parents and educators to teach these modern skills... through awareness and advocacy."[47] Per Scott Eidler, "Parents and educators need to make children aware at a young age of the life-changing effects cyberbullying can have on the victim. The next step for prevention is advocacy. For example, three high school students from Melville, New York organized a Bullying Awareness Walk, where several hundred people turned out to show their support."[48]		Clara Wajngurt writes, "Other than organizing events, calling for social media sites to take charge could make the difference between life and death. Cyberbullying is making it increasingly difficult to enforce any form of prevention."[49] Joanna Wojcik concludes, "The rapid growth of social media is aiding the spread of cyberbullying, and prevention policies are struggling to keep up. In order for prevention policies to be put in place, the definition of cyberbullying must be stated, others must be educated on how to recognize and prevent bullying, and policies that have already attempted to be enacted need to be reviewed and learned from."[50]		Researcher Charisse Nixon found that students do not reach out for help with cyberbullying for four main reasons: they do not feel connected to the adults around them; the students do not see the cyberbullying as an issue that is worth bringing forward; they do not feel the surrounding adults have the ability to properly deal with the cyberbullying; and the teenagers have increased feelings of shame and humiliation regarding the cyberbullying.[51] Nixon also found that when bystanders took action in helping end the cyberbullying in adolescents, the results were more positive than when the adolescents attempted to resolve the situation without outside help.[51]		Sexual bullying is "any bullying behavior, whether physical or non-physical, that is based on a person’s sexuality or gender. It is when sexuality or gender is used as a weapon by boys or girls towards other boys or girls—although it is more commonly directed at girls. It can be carried out to a person’s face, behind their back or through the use of technology."[52]		As part of its research into sexual bullying in schools, the BBC TV series Panorama commissioned a questionnaire aimed at people aged 11 to 19 in schools and youth clubs across five regions of England.[53] The survey revealed that of the 273 respondents, 28 had been forced to do something sexual, and 31 had seen it happen to someone else. Of the 273 respondents, 40 had experienced unwanted touching.[54] U.K. government figures show that in the 2007–2008 school year, there were 3,450 fixed-period exclusions and 120 expulsions from schools in England due to sexual misconduct.[55] This included incidents such as groping and using sexually insulting language. From April 2008 to March 2009, ChildLine counselled a total of 156,729 children, 26,134 of whom spoke about bullying as a main concern and 300 of whom spoke specifically about sexual bullying.[56]		The U.K. charity Beatbullying has claimed that as gang culture enters, children are being bullied into providing sexual favours in exchange for protection.[57] However, other anti-bullying groups and teachers' unions, including the National Union of Teachers, challenged the charity to provide evidence of this.[57]		Sexting cases are also on the rise and have become a major source of bullying. The circulation of explicit photos of those involved either around school or the internet put the originators in a position to be scorned and bullied.[58] There have been reports of some cases in which the bullying has been so extensive that the victim has taken their life.[59]		According to HealthDay News, 15 percent of college students claim to have been victims of bullying while at college.[60] In the article, "Bullying not a thing of the past for college students," Kaitlyn Krasselt writes, "Bullying comes in all forms but is usually thought of as a K-12 issue that ceases to exist once students head off to college."[61] The misconception that bullying does not occur in higher education began to receive attention after the death of college student Tyler Clementi.		Bullying is usually associated with an imbalance of power.[62] A bully has a perceived authority over another due to factors such as size, gender, or age.[63] Boys tend to bully peers based on the peer's physical weakness, short temper, friend group, and clothing. Bullying among girls, on the other hand, results from factors such as facial appearance, emotional factors, being overweight, and academic status.[64] Both sexes tend to target people with speech impediments of some sort (such as stutter).		Bullies often come from families that use physical forms of discipline.[65]		Bullying locations vary by context. Most bullying in elementary school happens in the playground. In middle school and high school, it occurs most in the hallways, which have little supervision. According to the U.S Department of Education's National Center for Education Statistics, more than 47% of kids reported getting bullied in hallways and stairway.[66] Bus stops and bus rides to and from school tend to be hostile environments as well; children tend to view the driver as someone with no disciplinary authority.[67]		Bullying may also follows people into adult life and university. Bullying can take over the lives of both lecturers and students, and can lead to supervisors putting pressure on students.[68] Bullying can happen in any place at any time.		Victims of bullying typically are physically smaller, more sensitive, unhappy, cautious, anxious, quiet, and withdrawn. They are often described as passive or submissive. Possessing these qualities make these individuals vulnerable, as they are seen as being less likely to retaliate.[65]		Signs that a child is being bullied include:[69][70]		Signs that a child is bullying others include:[69][70]		Signs that a child has witnessed bullying include:[69][70]		McNamee and Mercurio state that there is a "bullying triangle", consisting of the person doing the bullying, the person getting bullied, and the bystander.[71]		The US Department of Health and Human Services divides the people involved in bullying into several roles:[69]		In her book, The Bully, the Bullied, and the Bystander, Barbara Coloroso divides bullies into several types:[72]		Parsons identifies school bullying cultures as typically having a web of dynamics which are much more complex than just considering bullying amongst students. These dynamics include:[73]		Researchers have identified many misconceptions regarding bullying:[75][76]		Studies have shown that bullying programs set up in schools with the help and engagements of staff and faculty have been shown to reduce peer victimization and bullying.[78] Incidences of bullying are noticeably reduced when the students themselves disapprove of bullying.[79]		Measures such as increasing awareness,[contradictory] instituting zero tolerance for fighting, or placing troubled students in the same group or classroom are actually ineffective in reducing bullying; methods that are effective include increasing empathy for victims; adopting a program that includes teachers, students, and parents; and having students lead anti-bullying efforts.[80][pages needed] Success is most associated with beginning interventions at an early age, constantly evaluating programs for effectiveness, and having some students simply take online classes to avoid bullies at school.[81]		One possible prevention and intervention for bullying is PBIS/Positive behavioral interventions and supports it is defined as a "framework for enhancing adoption of a continuum of evidence based interventions to achieve academically and behaviorally important outcomes for all students. PBIS seeks to improve school climate, reduce discipline issues, and support academic achievement."[82]		Section 89 of the Education and Inspections Act 2006 provides for an anti-bullying policy for all state schools to be made available to parents.		The victims of some school shootings have sued both the shooters' families and the schools.[83] At one point only 23 states had Anti-Bullying laws. In 2015 Montana became the last state to have an anti-bullying law and at that point all 50 states had an anti-bullying law. These laws are not going to abolish bullying but it does bring attention to the behavior and it lets the aggressors know it will not be tolerated.[84]		In 2016, a legal precedent was set by a mother and her son, after the son was bullied at his public school. The mother and son won a court case against the Ottawa-Carleton District School Board, making this the first case in North America where a school board has been found negligent in a bullying case for failing to meet the standard of care (the "duty of care" that the school board owes to its students). A similar bullying case was won in Australia in 2013 (Oyston v. St. Patricks College).[85]		School bullying is associated with school shootings; the vast majority of students (87%) believe that shootings occur in direct retaliation to bullying.[87] School shooters who left behind evidence that they were bullied include Eric Harris and Dylan Klebold (perpetrators of the Columbine school shooting), Nathan Ferris, Edmar Aparecido Freitas, Brian Head, Seung-Hui Cho, Wellington Menezes Oliveira, Kimveer Gill, Karl Pierson, and Jeff Weise.[88][unreliable source?]		Events and organizations which address bullying in schools include:		To learn how to add open-license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia.		
The Student Press Law Center (SPLC) is a non-profit organization in the United States that aims at protecting the freedom of the press for student journalists, usually from high school and university student newspapers. It describes itself as "an advocate for student free-press rights [that] provides information, advice and legal assistance at no charge to students and the educators who work with them."		The SPLC was founded in 1974. It is the only legal assistance agency in the United States with the primary mission of educating high school and college journalists about the rights and responsibilities embodied in the First Amendment and supporting the freedom of expression of student news media to address issues and express themselves free from censorship.[1][2][3]		The SPLC is a non-partisan 501(c)(3) corporation. It is headquartered in Washington, D.C. It was previously headquartered in Arlington, Virginia, where it shared a suite of offices with the Reporters Committee for Freedom of the Press.[4]						The SPLC:		The organization is run by an executive director and a corporate board of directors composed primarily of journalism educators, professional journalists, and attorneys.[13] The current executive director is Frank LoMonte,[14] who began at the Center in January 2008.[15][16] He was preceded by Mark Goodman, who led the organization from 1985 to 2007 before accepting a position as professor and Knight Chair in Scholastic Journalism at Kent State University.[17][18]		In addition to the executive director, the SPLC permanent staff consists of a full-time attorney advocate and an office manager.[19] A full-time publications fellow serves as managing editor of the Center's news content, much of which is produced by journalism student interns, who work at the Center for a semester.[20] In addition, the SPLC works with a development/communications consultant and a West Coast consulting attorney based in Ferndale, Washington.[21]		The SPLC is supported by contributions from student journalists, journalism educators, and other individuals, as well as by donations from foundations and corporations. On January 23, 2007, the center announced it had successfully completed a three-year $3.75 million endowment campaign, spurred by a challenge grant from the John S. and James L. Knight Foundation.[22][23]		Student Press Law Center		
International Students' Day is an international observance of student community, held annually on November 17. Originally commemorating the Nazi German storming of Czech universities in 1939 and the subsequent killing and sending of students to concentration camps, a number of universities now mark it, sometimes on a day other than November 17, for a nonpolitical celebration of the multiculturalism of their international students.						The date commemorates the anniversary of the 1939 German Nazi storming of the University of Prague after demonstrations against the killing of Jan Opletal and worker Václav Sedláček as well as against the German occupation of Czechoslovakia. The German Nazis rounded up the students, murdered nine student leaders and sent over 1,200 students to concentration camps (mainly Sachsenhausen concentration camp). Subsequently, they closed all Czech universities and colleges. By this time Czechoslovakia no longer existed, as it was divided by the Nazi Germany into Protectorate of Bohemia and Moravia and Slovak Republic with a puppet fascist government.		During late 1939 the Nazi authorities in the Protectorate of Bohemia and Moravia suppressed a demonstration in Prague held by students of the Medical Faculty of Charles University. The demonstration was held on the 28th of October to commemorate the anniversary of the independence of the Czechoslovak Republic (1918).		During this demonstration the student Jan Opletal was shot and died from wounds on the 11th of November. On the 15th of November his body was meant to be transported from Prague back to his home in Moravia. His funeral procession consisted of thousands of students, who turned this event into an anti-Nazi demonstration. However, this resulted in drastic measures being taken by the Nazis. All Czech higher education institutions were closed down, more than 1,200 students were arrested and sent to concentration camps, and nine students and professors were executed without trial on the 17th of November. Due to this, 17 November was chosen as International Students’ Day.		The nine students and professors executed on the 17th of November in Prague were:		The 17th of November was first marked as International Students' Day in 1941 in London by the International Students' Council (which had many refugee members) in agreement with the Allies, and the tradition has been kept up by the successor International Union of Students, which together with the National Unions of Students in Europe and other groups has been lobbying to make the day an official United Nations observance.		In 1989 independent student leaders together with the Socialist Union of Youth (SSM/SZM) organized a mass demonstration to commemorate International Students’ Day. This fiftieth-anniversary event gave students an opportunity to voice their displeasure with the communist party of Czechoslovakia. What began as a peaceful commemorative event turned into a violent one, by nightfall, with many participants being brutally beaten by riot police, red berets, and other members of the law enforcement agencies. About 15,000 people took part in this demonstration. The only person to left lying where the beatings took place was an alleged body of a student who in fact was an undercover agent. The rumor of a fellow student that died due to the police brutality triggered events that the secret police probably had not envisaged. That same night, students and theater actors agreed to go on strike. The events linked to International Students' Day of 17 November 1989 helped spark the Velvet Revolution in Czechoslovakia. Struggle for Freedom and Democracy Day is today marked among both the official holidays in the Czech Republic (since 2000, thanks to the efforts of the Czech Student Chamber of the Council of Higher Education Institutions) and the holidays in Slovakia.		After the collapse of the Berlin Wall and the progressive crisis within the International Union of Students, celebrations for the 17th of November were held only in few countries without any coordination worldwide. During the World Social Forum held in Mumbai, India, in 2004, some international union of students such as OCLAE and some national unions such as the Italian Unione degli Studenti decided to re-launch the date and to call for global demonstration on 17 November 2004. Student movements in many countries mobilized again that year and kept on observing international students' day the following years with the active support of the European platform representing student and school student organizations OBESSU and the ESU.		In 2009, on the seventieth anniversary of 17 November 1939, OBESSU and ESU promoted a number of initiatives throughout Europe to commemorate the date. An event in Brussels was held from the 16th to the 18th of November at the University of Brussels. The event focused on the history of the students' movement and its role in promoting active citizenship towards authoritarian regimes and it was followed by an assembly discussing the role of student unions today and the need for the recognition of a European Student Rights Charter. The conference gathered around 100 students representing national students and student unions from over 30 European countries as well as some international delegations.[citation needed]		
The European Students' Union (ESU) is the umbrella organisation of 45 national unions of students from 38 countries, representing over 15 million students. The aim of ESU is to represent and promote the educational, social, economic and cultural interests of students at a European level towards all relevant bodies and in particular those of the European Union, Council of Europe and UNESCO. ESU is representing the voice of the students in Europe by being a consultative member to the Bologna Process. ESU is also a full member of the European Youth Forum (YFJ), the ESU nominee for 2006 was elected as YFJ president.						On 17 October 1982, seven National Unions of Students (NUSes) from The United Kingdom, Sweden, Iceland, France, Denmark, Norway and Austria gathered in Stockholm to create WESIB, the Western European Students Information Bureau. In February 1990, WESIB dropped the "W" to become the European Student Information Bureau following the political upheaval in Europe at the time. In 1992 the name was changed into the National Unions of Students in Europe (ESIB). This reflected the recognition of the changing mission of ESIB from just an information sharing organisation to political organisation that represented the views of students to European institutions. In May 2007 the name changed to European Students' Union (ESU).		Over the years the office of ESU has moved around Europe and was first hosted by the member NUSes. Following the establishment of WESIB in Stockholm the office was based in the SFS Office in Sweden from 1982 until 1985, funded by a grant by the Swedish Government. By 1985 the grant was running low and so NUS UK offered to host WESIB in their London headquarters. In 1988 the office moved to the ÖH offices in Vienna and remained there until 2000, when it was decided that for reasons of being near the European institutions the office should move to Brussels and was hosted by VVS.		The highest structure of ESU is the Board Meeting, bringing together representatives from all the National Unions of Students it represents. The Board Meeting sets the policy direction of the organisation, and elects members to the Executive Committee to run the organisation.		The Executive Committee (EC) is elected for a one-year term at the annual Board Meeting by representatives of the member organisations, with each country (not organisation) given two votes. The President and Vice-Presidents together make up the Presidency of ESU, and are responsible for the day-to-day operations of the organisation along with the seven General Members of the EC. [1]		Candidate members are NUSes that have submitted an application of membership to ESU, but have not yet been granted member status by the Board Meeting. Candidate members retain their status for one year and are subject to a "study visit" by ESU to ensure they meet membership criteria. As of September 2016, the candidate members are:[2]		Associate members of ESU are pan-European and international student organisations that have similar goals to ESU. The criteria for associate membership require the organisation to be democratic and student-run, have either students or NUSes as members, and represent students from at least 8 countries that are parties to the European Cultural Convention. Associate organizations can attend and speak at all ESU meetings, but cannot vote at Board Meetings.[3]		
A college (Latin: collegium) is an educational institution or a constituent part of one. A college may be a degree-awarding tertiary educational institution, a part of a collegiate or federal university, or an institution offering vocational education.		In the United States, "college" often refers to a constituent part of a university or to a degree-awarding tertiary educational institution, but generally "college" and "university" are used interchangeably,[1] whereas in the United Kingdom, Oceania, South Asia and Southern Africa, "college" may refer to a secondary or high school, a college of further education, a training institution that awards trade qualifications, a higher education provider that does not have university status (often without its own degree-awarding powers), or a constituent part of a university (See this comparison of British and American English educational terminology for further information).						In ancient Rome a collegium was a club or society, a group of people living together under a common set of rules (con- = "together" + leg- = "law" or lego = "I choose" or "I read").		Aside from the modern educational context - nowadays the most common use of "college" - there are various other meanings also derived from the original Latin term, such as Electoral college.		Within higher education, the term can be used to refer to:[2]		A sixth form college or college of further education is an educational institution in England, Wales, Northern Ireland, Belize, The Caribbean, Malta, Norway, Brunei, or Southern Africa, among others, where students aged 16 to 19 typically study for advanced school-level qualifications, such as A-levels, BTEC, HND or its equivalent and the International Baccalaureate Diploma, or school-level qualifications such as GCSEs. In Singapore and India, this is known as a junior college. The municipal government of the city of Paris uses the phrase "sixth form college" as the English name for a lycée.[3]		In some national education systems, secondary schools may be called "colleges" or have "college" as part of their title.		In Australia the term "college" is applied to any private or independent (non-government) primary and, especially, secondary school as distinct from a state school. Melbourne Grammar School, Cranbrook School, Sydney and The King's School, Parramatta are considered colleges.		There has also been a recent trend to rename or create government secondary schools as "colleges". In the state of Victoria, some state high schools are referred to as secondary colleges, although the pre-eminent government secondary school for boys in Melbourne is still named Melbourne High School. In Western Australia, South Australia and the Northern Territory, "college" is used in the name of all state high schools built since the late 1990s, and also some older ones. In New South Wales, some high schools, especially multi-campus schools resulting from mergers, are known as "secondary colleges". In Queensland some newer schools which accept primary and high school students are styled state college, but state schools offering only secondary education are called "State High School". In Tasmania and the Australian Capital Territory, "college" refers to the final two years of high school (years 11 and 12), and the institutions which provide this. In this context, "college" is a system independent of the other years of high school. Here, the expression is a shorter version of matriculation college.		In a number of Canadian cities, many government-run secondary schools are called "collegiates" or "collegiate institutes" (C.I.), a complicated form of the word "college" which avoids the usual "post-secondary" connotation. This is because these secondary schools have traditionally focused on academic, rather than vocational, subjects and ability levels (for example, collegiates offered Latin while vocational schools offered technical courses). Some private secondary schools (such as Upper Canada College, Vancouver College) choose to use the word "college" in their names nevertheless.[4] Some secondary schools elsewhere in the country, particularly ones within the separate school system, may also use the word "college" or "collegiate" in their names.[5]		In New Zealand the word "college" normally refers to a secondary school for ages 13 to 17 and "college" appears as part of the name especially of private or integrated schools. "Colleges" most frequently appear in the North Island, whereas "high schools" are more common in the South Island.		In South Africa, some secondary schools, especially private schools on the English public school model, have "college" in their title. Thus no less than six of South Africa's Elite Seven high schools call themselves "college" and fit this description. A typical example of this category would be St John's College.		Private schools that specialize in improving children's marks through intensive focus on examination needs are informally called "cram-colleges".		In Sri Lanka the word "college" (known as Vidyalaya in Sinhala) normally refers to a secondary school, which usually signifies above the 5th standard. During the British colonial period a limited number of exclusive secondary schools were established based on English public school model (Royal College Colombo, S. Thomas' College, Mount Lavinia, Trinity College, Kandy) these along with several Catholic schools (St. Joseph's College, Colombo, St Anthony's College) traditionally carry their name as colleges. Following the start of free education in 1931 large group of central colleges were established to educate the rural masses. Since Sri Lanka gained Independence in 1948, many schools that have been established have been named as "college".[citation needed]		As well as an educational institution, the term can also refer, following its etymology, to any formal group of colleagues set up under statute or regulation; often under a Royal Charter. Examples are an electoral college, the College of Arms, a college of canons, and the College of Cardinals. Other collegiate bodies include professional associations, particularly in medicine and allied professions. In the UK these include the Royal College of Nursing and the Royal College of Physicians. Examples in the United States include the American College of Physicians, the American College of Surgeons, and the American College of Dentists. An example in Australia is the Royal Australian College of General Practitioners.		In Australia a college may be an institution of tertiary education that is smaller than a university, run independently or as part of a university. Following a reform in the 1980s many of the formerly independent colleges now belong to a larger universities.		Referring to parts of a university, there are residential colleges which provide residence for students, both undergraduate and postgraduate, called university colleges. These colleges often provide additional tutorial assistance, and some host theological study. Many colleges have strong traditions and rituals, so are a combination of dormitory style accommodation and fraternity or sorority culture.		Most technical and further education institutions (TAFEs), which offer certificate and diploma vocational courses, are styled "TAFE colleges" or "Colleges of TAFE".		Some senior high schools are also referred to as colleges.		In Canada, the term "college" usually refers to a trades school, applied arts/science/technology/business/health school or community college. These are post-secondary institutions granting certificates, diplomas, associate's degree, and in some cases bachelor's degrees. In Quebec, the term is seldom used; the French acronym for public colleges, CEGEP (Collège d'enseignement général et professionnel, "college of general and professional education"), is colloquially all collegiate level institutions specific to the Quebec education system, a step that is required to continue onto university (unless one applies as a "mature" student, meaning 21 years of age or over, and out of the educational system for at least 2 years), or to learn a trade. In Ontario and Alberta, there are also institutions which are designated university colleges, as they only grant undergraduate degrees. This is to differentiate between universities, which have both undergraduate and graduate programs and those that do not. In contrast to usage in the United States, there is a strong distinction between "college" and "university" in Canada. In conversation, one specifically would say either "They are going to university" (i.e., studying for a three- or four-year degree at a university) or "They are going to college" (suggesting technical/career training or university transfer courses).		The Royal Military College of Canada, a full-fledged degree-granting university, does not follow the naming convention used by the rest of the country, nor does its sister school Royal Military College Saint-Jean or the now closed Royal Roads Military College.		The term "college" also applies to distinct entities within a university (usually referred to as "federated colleges" or "affiliated colleges"), similar to the residential colleges in the United Kingdom. These colleges act independently, but in affiliation or federation with the university that actually grants the degrees. For example, Trinity College was once an independent institution, but later became federated with the University of Toronto, and is now one of its residential colleges (though it remains a degree-granting institution through its Faculty of Divinity). In the case of Memorial University of Newfoundland, located in St. John's, the Corner Brook campus is called Sir Wilfred Grenfell College. Occasionally, "college" refers to a subject specific faculty within a university that, while distinct, are neither federated nor affiliated—College of Education, College of Medicine, College of Dentistry, College of Biological Science[6] among others.		There are also universities referred to as art colleges, empowered to grant academic degrees of BFA, Bdes, MFA, Mdes and sometimes collaborative PhD degrees. Some of them have "university" in their name (NSCAD University, OCAD University and Emily Carr University of Art and Design)and others do not. In some Canadian provinces, the word "college" may also be seen in the proper name of a high school, especially one with a history as a private school, but these institutions would not actually be considered colleges in the more general sense of the term.		Online and distance education (E-learning) use "college" in the name in the British sense, for example : Canada Capstone College.[citation needed]		One use of the term "college" in the American sense is by the Canadian Football League (CFL), which calls its annual entry draft the Canadian College Draft. The draft is restricted to players who qualify under CFL rules as "non-imports"—essentially, players who were raised in Canada (see the main CFL article for a more detailed definition). Because a player's designation as "non-import" is not affected by where he plays post-secondary football, the category includes former players at U.S. college football programs ("universities" in the Canadian sense) as well as CIS football programs at Canadian universities.		In Chile, the term "college" is usually used in the name of some bilingual schools, like Santiago College, Saint George's College etc.		International Association of "Tourists and Travelers" College. International association "tourists and travelers" is a non-commercial, non political and non industrial organization, which is created to develop tourism in Georgia.[7]		Kollegio (in Greek Κολλέγιο) refers to the Centers of Post-Lyceum Education (in Greek Κέντρο Μεταλυκειακής Εκπαίδευσης, abbreviated as KEME), which are principally private and belong to the Greek post-secondary education system. Some of them have links to EU or US higher education institutions or accreditation organizations, such as the NEASC.[8] Kollegio (or Kollegia in plural) may also refer to private non-tertiary schools, such as the Athens College.		In Hong Kong, the term 'college' is used by tertiary institutions as either part of their names or to refer to a constituent part of the university, such as the colleges in the collegiate The Chinese University of Hong Kong; or to a residence hall of a university, such as St. John's College, University of Hong Kong. Many older secondary schools have the term 'college' as part of their names.		The modern system of education was heavily influenced by the British starting in 1835.[9]		In India, the term "college" is commonly reserved for institutions that offer degrees at year 12 ("Junior College", similar to American high schools), and those that offer the bachelor's degree; some colleges, however, offer programmes up to PhD level. Generally, colleges are located in different parts of a state and all of them are affiliated to a regional university. The colleges offer programmes leading to degrees of that university. Colleges may be either Autonomous or non-autonomous. Autonomous Colleges are empowered to establish their own syllabus, and conduct and assess their own examinations; in non-autonomous colleges, examinations are conducted by the university, at the same time for all colleges under its affiliation. There are several hundred universities and each university has affiliated colleges, often a large number.		The first liberal arts and sciences college in India was C. M. S. College Kottayam, Kerala, established in 1817, and the Presidency College, Kolkata, also 1817, initially known as Hindu College. The first college for the study of Christian theology and ecumenical enquiry was Serampore College (1818). The first Missionary institution to impart Western style education in India was the Scottish Church College, Calcutta (1830). The first commerce and economics college in India was Sydenham College, Mumbai (1913).		In Ireland the term "college" is normally used to describe an institution of tertiary education. University students often say they attend "college" rather than "university". Until 1989, no university provided teaching or research directly; they were formally offered by a constituent college of the university.		There are number of secondary education institutions that traditionally used the word "college" in their names: these are either older, private schools (such as Belvedere College, Gonzaga College and St. Michael's College) or what were formerly a particular kind of secondary school. These secondary schools, formerly known as "technical colleges," were renamed "community colleges," but remain secondary schools.		The country's only ancient university is the University of Dublin. Created during the reign of Elizabeth I, it is modelled on the collegiate universities of Cambridge and Oxford. However, only one constituent college was ever founded, hence the curious position of Trinity College, Dublin today; although both are usually considered one and the same, the University and College are completely distinct corporate entities with separate and parallel governing structures.		Among more modern foundations, the National University of Ireland, founded in 1908, consisted of constituent colleges and recognised colleges until 1997. The former are now referred to as constituent universities – institutions that are essentially universities in their own right. The National University can trace its existence back to 1850 and the creation of the Queen's University of Ireland and the creation of the Catholic University of Ireland in 1854. From 1880, the degree awarding roles of these two universities was taken over by the Royal University of Ireland, which remained until the creation of the National University in 1908 and Queen's University Belfast.		The state's two new universities Dublin City University and University of Limerick were initially National Institute for Higher Education institutions. These institutions offered university level academic degrees and research from the start of their existence and were awarded university status in 1989 in recognition of this.		Third level technical education in the state has been carried out in the Institutes of Technology, which were established from the 1970s as Regional Technical Colleges. These institutions have delegated authority which entitles them to give degrees and diplomas from the Higher Education and Training Awards Council in their own name.		A number of Private Colleges exist such as DBS, providing undergraduate and postgraduate courses validated by HETAC and in some cases by other Universities.		Other types of college include Colleges of Education, such as the Church of Ireland College of Education. These are specialist institutions, often linked to a university, which provide both undergraduate and postgraduate academic degrees for people who want to train as teachers.		A number of state funded further education colleges exist - which offer vocational education and training in a range of areas from business studies, I.C.T to sports injury therapy. These courses are usually 1, 2 or less often 3 three years in duration and are validated by FETAC at levels 5 or 6 or for the BTEC Higher National Diploma award - validated by Edexcel which is a level 6/7 qualification. There are numerous private colleges (particularly in Dublin and Limerick)[citation needed] which offer both further and higher education qualifications. These degrees and diplomas are often certified by foreign universities/international awarding bodies and are aligned to the National Framework of Qualifications at level 6, 7 and 8.		In Israel, any non university higher-learning facility is called a college. Institutions accredited by the Council for Higher Education in Israel (CHE) to confer a bachelor's degree are called "Academic Colleges."[10] These colleges (at least 4 for 2012) may also offer master's degrees and act as Research facilities. There are also over twenty teacher training colleges or seminaries, most of which may award only a Bachelor of Education (B.Ed.) degree.		Following the Portuguese usage, the term "college" (colégio) in Macau has traditionally been used in the names for private (and non-governmental) pre-university educational institutions, which correspond to form one to form six level tiers. Such schools are usually run by the Roman Catholic church or missionaries in Macau. Examples include Chan Sui Ki Perpetual Help College, Yuet Wah College, and Sacred Heart Canossian College.		The constituent colleges of the former University of New Zealand (such as Canterbury University College) have become independent universities. Some halls of residence associated with New Zealand universities retain the name of "college", particularly at the University of Otago (which although brought under the umbrella of the University of New Zealand, already possessed university status and degree awarding powers). The institutions formerly known as "Teacher-training colleges" now style themselves "College of education".		Some universities, such as the University of Canterbury, have divided their University into constituent administrative "Colleges" – the College of Arts containing departments that teach Arts, Humanities and Social Sciences, College of Science containing Science departments, and so on. This is largely modelled on the Cambridge model, discussed above.		Like the United Kingdom some professional bodies in New Zealand style themselves as "colleges", for example, the Royal Australasian College of Surgeons, the Royal Australasian College of Physicians.		Secondary school is often referred to as college and the term is used interchangeably with high school. This is reflected in the names of many secondary schools such as Rangitoto College, New Zealand's largest secondary.		In the Philippines, colleges usually refer to institutions of learning that grant degrees but whose scholastic fields are not as diverse as that of a university (University of Santo Tomas, University of the Philippines, Ateneo de Manila University, De La Salle University, Far Eastern University, and AMA University), such as the San Beda College which specializes in law, AMA Computer College whose campuses are spread all over the Philippines which specializes in information and computing technologies, and the Mapúa Institute of Technology which specializes in engineering, or to component units within universities that do not grant degrees but rather facilitate the instruction of a particular field, such as a College of Science and College of Engineering, among many other colleges of the University of the Philippines.		A state college may not have the word "college" on its name, but may have several component colleges, or departments. Thus, the Eulogio Amang Rodriguez Institute of Science and Technology is a state college by classification.		Usually, the term "college" is also thought of as a hierarchical demarcation between the term "university", and quite a number of colleges seek to be recognized as universities as a sign of improvement in academic standards (Colegio de San Juan de Letran, San Beda College), and increase in the diversity of the offered degree programs (called "courses"). For private colleges, this may be done through a survey and evaluation by the Commission on Higher Education and accrediting organizations, as was the case of Urios College which is now the Fr. Saturnino Urios University. For state colleges, it is usually done by a legislation by the Congress or Senate. In common usage, "going to college" simply means attending school for an undergraduate degree, whether it's from an institution recognized as a college or a university.		When it comes to referring to the level of education, college is the term more used to be synonymous to tertiary or higher education. A student who is or has studied his/her undergraduate degree at either an institution with college or university in its name is considered to be going to or have gone to college.		Presently in Portugal, the term colégio (college) is normally used as a generic reference to a private (non-government) school that provides from basic to secondary education. Many of the private schools include the term colégio in their name. Some special public schools - usually of the boarding school type - also include the term in their name, with a notable example being the Colégio Militar (Military College). The term colégio interno (literally "internal college") is used specifically as a generic reference to a boarding school.		Until the 19th century, a colégio was usually a secondary or pre-university school, of public or religious nature, where the students usually lived together. A model for these colleges was the Royal College of Arts and Humanities, founded in Coimbra by King John III of Portugal in 1542.		The term "college" in Singapore is generally only used for pre-university educational institutions called "Junior Colleges", which provide the final two years of secondary education (equivalent to sixth form in British terms or grades 11–12 in the American system). Since 1 January 2005, the term also refers to the three campuses of the Institute of Technical Education with the introduction of the "collegiate system", in which the three institutions are called ITE College East, ITE College Central, and ITE College West respectively.		The term "university" is used to describe higher-education institutions offering locally conferred degrees. Institutions offering diplomas are called "polytechnics", while other institutions are often referred to as "institutes" and so forth.		Although the term "college" is hardly used in any context at any university in South Africa, some non-university tertiary institutions call themselves colleges. These include teacher training colleges, business colleges and wildlife management colleges. See: List of universities in South Africa#Private colleges and universities; List of post secondary institutions in South Africa.		There are several professional and vocational institutions that offer post-secondary education without granting degrees that are referred to as "colleges". This includes the Sri Lanka Law College, the many Technical Colleges and Teaching Colleges.		Further education (FE) colleges and sixth form colleges are institutions providing further education to students over 16. Some of these also provide higher education courses (see below).[16] In the context of secondary education, 'college' is used in the names of some private schools, e.g. Eton College and Winchester College.		In higher education, a college is normally a provider that does not hold university status, although it can also refer to a constituent part of a collegiate or federal university or a grouping of academic faculties or departments within a university. Traditionally the distinction between colleges and universities was that colleges did not award degrees while universities did, but this is no longer the case with NCG having gained taught degree awarding powers (the same as some universities) on behalf of its colleges,[17] and many of the colleges of the University of London holding full degree awarding powers and being effectively universities. Most colleges, however, do not hold their own degree awarding powers and continue to offer higher education courses that are validated by universities or other institutions that can award degrees.		In England, as of August 2016, over 60% of the higher education providers directly funded by HEFCE (208/340) are sixth-form or further education colleges, often termed colleges of further and higher education, along with 17 colleges of the University of London, one university college, 100 universities, and 14 other providers (six of which use 'college' in their name). Overall, this means over two thirds of state-supported higher education providers in England are colleges of one form or another.[18][19] Many private providers are also called colleges, e.g. the New College of the Humanities and St Patrick's College, London.		Colleges within universities vary immensely in their responsibilities. The large constituent colleges of the University of London are effectively universities in their own right; colleges in some universities, including those of the University of the Arts London and smaller colleges of the University of London, run their own degree courses but do not award degrees; those at the University of Roehampton provide accommodation and pastoral care as well as delivering the teaching on university courses; those at Oxford and Cambridge deliver some teaching on university courses as well as providing accommodation and pastoral care; and those in Durham, Kent, Lancaster and York provide accommodation and pastoral care but do not normally participate in formal teaching. The legal status of these colleges also varies widely, with University of London colleges being independent corporations and recognised bodies, Oxbridge colleges, colleges of the University of the Highlands and Islands (UHI) and some Durham colleges being independent corporations and listed bodies, most Durham colleges being owned by the university but still listed bodies, and those of other collegiate universities not having formal recognition. When applying for undergraduate courses through UCAS, University of London colleges are treated as independent providers, colleges of Oxford, Cambridge, Durham and UHI are treated as locations within the universities that can be selected by specifying a 'campus code' in addition to selecting the university, and colleges of other universities are not recognised.[20][21][22][23][24]		The UHI and the University of Wales Trinity Saint David (UWTSD) both include further education colleges. However, while the UHI colleges integrate FE and HE provision, UWTSD maintains a separation between the university campuses (Lampeter, Carmarthen and Swansea) and the two colleges (Coleg Sir Gâr and Coleg Ceredigion; n.b. coleg is Welsh for college), which although part of the same group are treated as separate institutions rather than colleges within the university.[25][26]		A university college is an independent institution with the power to award taught degrees, but which has not been granted university status. University College is a protected title that can only be used with permission, although note that University College London, University College, Oxford and University College, Durham are colleges within their respective universities and not university colleges (in the case of UCL holding full degree awarding powers that set it above a university college), while University College Birmingham is a university in its own right and also not a university college.		In the United States, there are over 7,021 colleges and universities.[27] A "college" in the US formally denotes a constituent part of a university, but in popular usage, the word "college" is the generic term for any post-secondary undergraduate education. Americans "go to college" after high school, regardless of whether the specific institution is formally a college or a university. Some students choose to dual-enroll, by taking college classes while still in high school. The word and its derivatives are the standard terms used to describe the institutions and experiences associated with American post-secondary undergraduate education.		Students must pay for college before taking classes. Some borrow the money via loans, and some students fund their educations with cash, scholarships, or grants, or some combination of any two or more of those payment methods. In 2011, the state or federal government subsidized $8,000 to $100,000 for each undergraduate degree. For state-owned schools (called "public" universities), the subsidy was given to the college, with the student benefiting from lower tuition.[28][29] The state subsidized on average 50% of public university tuition.[30]		Colleges vary in terms of size, degree, and length of stay. Two-year colleges, also known as junior or community colleges, usually offer an associate's degree, and four-year colleges usually offer a bachelor's degree. Often, these are entirely undergraduate institutions, although some have graduate school programs.		Four-year institutions in the U.S. that emphasize a liberal arts curriculum are known as liberal arts colleges. Until the 20th century, liberal arts, law, medicine, theology, and divinity were about the only form of higher education available in the United States.[31][32] These schools have traditionally emphasized instruction at the undergraduate level, although advanced research may still occur at these institutions.		While there is no national standard in the United States, the term "university" primarily designates institutions that provide undergraduate and graduate education. A university typically has as its core and its largest internal division an undergraduate college teaching a liberal arts curriculum, also culminating in a bachelor's degree. What often distinguishes a university is having, in addition, one or more graduate schools engaged in both teaching graduate classes and in research. Often these would be called a School of Law or School of Medicine, (but may also be called a college of law, or a faculty of law). An exception is Vincennes University, Indiana, which is styled and chartered as a "university" even though almost all of its academic programs lead only to two-year associate degrees. Some institutions, such as Dartmouth College and The College of William & Mary, have retained the term "college" in their names for historical reasons. In one unique case, Boston College and Boston University, both located in Boston, Massachusetts, are completely separate institutions.		Usage of the terms varies among the states. In 1996 for example, Georgia changed all of its four-year institutions previously designated as colleges to universities, and all of its vocational technology schools to technical colleges.		The terms "university" and "college" do not exhaust all possible titles for an American institution of higher education. Other options include "Polytechnic" (Rensselaer Polytechnic Institute), "Institute of Technology" (Massachusetts Institute of Technology), "academy" (United States Military Academy), "union" (Cooper Union), "conservatory" (New England Conservatory), and "school" (Juilliard School). In colloquial use, they are still referred to as "college" when referring to their undergraduate studies.		The term college is also, as in the United Kingdom, used for a constituent semi-autonomous part of a larger university but generally organized on academic rather than residential lines. For example, at many institutions, the undergraduate portion of the university can be briefly referred to as the college (such as The College of the University of Chicago, Harvard College at Harvard, or Columbia College at Columbia) while at others, such as the University of California, Berkeley, each of the faculties may be called a "college" (the "college of engineering", the "college of nursing", and so forth). There exist other variants for historical reasons; for example, Duke University, which was called Trinity College until the 1920s, still calls its main undergraduate subdivision Trinity College of Arts and Sciences.		Some American universities, such as Princeton, Rice, and Yale have established residential colleges (sometimes, as at Harvard, the first to establish such a system in the 1930s, known as houses) along the lines of Oxford or Cambridge.[33] Unlike the Oxbridge colleges, but similarly to Durham, these residential colleges are not autonomous legal entities nor are they typically much involved in education itself, being primarily concerned with room, board, and social life.[34] At the University of Michigan, University of California, San Diego and the University of California, Santa Cruz, however, each of the residential colleges does teach its own core writing courses and has its own distinctive set of graduation requirements.		Many U.S. universities have placed increased emphasis on their residential colleges in recent years. This is exemplified by the creation of new colleges at Ivy League schools such as Yale University[35] and Princeton University,[36] and efforts to strengthen the contribution of the residential colleges to student education, including through a 2016 taskforce at Princeton on residential colleges.[37]		The founders of the first institutions of higher education in the United States were graduates of the University of Oxford and the University of Cambridge. The small institutions they founded would not have seemed to them like universities – they were tiny and did not offer the higher degrees in medicine and theology. Furthermore, they were not composed of several small colleges. Instead, the new institutions felt like the Oxford and Cambridge colleges they were used to – small communities, housing and feeding their students, with instruction from residential tutors (as in the United Kingdom, described above). When the first students graduated, these "colleges" assumed the right to confer degrees upon them, usually with authority—for example, The College of William & Mary has a Royal Charter from the British monarchy allowing it to confer degrees while Dartmouth College has a charter permitting it to award degrees "as are usually granted in either of the universities, or any other college in our realm of Great Britain."		The leaders of Harvard College (which granted America's first degrees in 1642) might have thought of their college as the first of many residential colleges that would grow up into a New Cambridge university. However, over time, few new colleges were founded there, and Harvard grew and added higher faculties. Eventually, it changed its title to university, but the term "college" had stuck and "colleges" have arisen across the United States.		In U.S. usage, the word "college" embodies not only a particular type of school, but has historically been used to refer to the general concept of higher education when it is not necessary to specify a school, as in "going to college" or "college savings accounts" offered by banks.		In a survey of more than 2,000 college students in 33 states and 156 different campuses, the U.S. Public Interest Research Group found the average student spends as much as $1,200 each year on textbooks and supplies alone. By comparison, the group says that's the equivalent of 39 percent of tuition and fees at a community college, and 14 percent of tuition and fees at a four-year public university.[38]		In addition to private colleges and universities, the U.S. also has a system of government funded, public universities. Many were founded under the Morrill Land-Grant Colleges Act of 1862. A movement had arisen to bring a form of more practical higher education to the masses, as "...many politicians and educators wanted to make it possible for all young Americans to receive some sort of advanced education."[39] The Morrill Act "...made it possible for the new western states to establish colleges for the citizens."[39] Its goal was to make higher education more easily accessible to the citizenry of the country, specifically to improve agricultural systems by providing training and scholarship in the production and sales of agricultural products,[40] and to provide formal education in "...agriculture, home economics, mechanical arts, and other professions that seemed practical at the time."[39]		The act was eventually extended to allow all states that had remained with the Union during the American Civil War, and eventually all states, to establish such institutions. Most of the colleges established under the Morrill Act have since become full universities, and some are among the elite of the world.		Selection of a four-year college as compared to a two-year junior college, even by marginal students such as those with a C+ grade average in high school and SAT scores in the mid 800s, increases the probability of graduation and confers substantial economic and social benefits.[41][42][43]		The term college is mainly used by private or independent secondary schools with Advanced Level (Upper 6th formers) and also Polytechnic Colleges which confer diplomas only. A student can complete secondary education (International General Certificate of Secondary Education, IGCSE) at 16 years and proceed straight to a poly-technical college or they can proceed to Advanced level (16 to 19 years) and obtain a General Certificate of Education (GCE) certificate which enables them to enrol at a University, provided they have good grades. Alternatively, with lower grades the GCE certificate holders will have an added advantage over their GCSE counterparts if they choose to enrol at a Poly-technical College. Some schools in Zimbabwe choose to offer the International Baccalaureate studies as an alternative to the IGCSE and GCE.		
Kindergarten ( listen (help·info); from German [ˈkɪndɐˌɡaːɐ̯tn̩] ( listen), which literally means "garden for the children"[1]) is a preschool educational approach traditionally based on playing, singing, practical activities such as drawing, and social interaction as part of the transition from home to school. At first such institutions were created in the late 18th century in Bavaria and Strasbourg to serve children whose parents both worked out of the home. The term was coined by the German Friedrich Fröbel, whose approach globally influenced early-years education. Today, the term is used in many countries to describe a variety of educational institutions and learning spaces for children ranging from two to seven years of age, based on a variety of teaching methods.						In 1779, Johann Friedrich Oberlin and Louise Scheppler founded in Strasbourg an early establishment for caring for and educating pre-school children whose parents were absent during the day.[2] At about the same time, in 1780, similar infant establishments were established in Bavaria.[3] In 1802, Princess Pauline zur Lippe established a preschool center in Detmold, the capital of the then principality of Lippe, Germany (now in the State of North Rhine-Westphalia).[4]		In 1816, Robert Owen, a philosopher and pedagogue, opened the first British and probably globally the first infants school in New Lanark, Scotland.[5][6][7] In conjunction with his venture for cooperative mills Owen wanted the children to be given a good moral education so that they would be fit for work. His system was successful in producing obedient children with basic literacy and numeracy.[8]		Samuel Wilderspin opened his first infant school in London in 1819,[9] and went on to establish hundreds more. He published many works on the subject, and his work became the model for infant schools throughout England and further afield. Play was an important part of Wilderspin's system of education. He is credited with inventing the playground. In 1823, Wilderspin published On the Importance of Educating the Infant Poor, based on the school. He began working for the Infant School Society the next year, informing others about his views. He also wrote The Infant System, for developing the physical, intellectual, and moral powers of all children from 1 to seven years of age.		Countess Theresa Brunszvik (1775–1861), who had known and been influenced by Johann Heinrich Pestalozzi, was influenced by this example to open an Angyalkert ("angel garden" in Hungarian) on May 27, 1828, in her residence in Buda, the first of eleven care centers that she founded for young children.[10][11] In 1836 she established an institute for the foundation of preschool centers. The idea became popular among the nobility and the middle class and was copied throughout the Kingdom of Hungary.		Friedrich Fröbel (1782–1852) opened a "play and activity" institute in 1837 in the village of Bad Blankenburg in the principality of Schwarzburg-Rudolstadt, Thuringia, as an experimental social experience for children entering school. He renamed his institute Kindergarten on June 28, 1840, reflecting his belief that children should be nurtured and nourished "like plants in a garden".[12]		Women trained by Fröbel opened kindergartens throughout Europe and around the world. The first kindergarten in the US was founded in Watertown, Wisconsin in 1856 and was conducted in German by Margaretha Meyer-Schurz.[13]		Elizabeth Peabody founded the first English-language kindergarten in the US in 1860. The first free kindergarten in the US was founded in 1870 by Conrad Poppenhusen, a German industrialist and philanthropist, who also established the Poppenhusen Institute. The first publicly financed kindergarten in the US was established in St. Louis in 1873 by Susan Blow.		Canada's first private kindergarten was opened by the Wesleyan Methodist Church in Charlottetown, Prince Edward Island, in 1870. By the end of the decade, they were common in large Canadian towns and cities.[14][15] The country's first public-school kindergartens were established in Berlin, Ontario (modern Kitchener), in 1882 T Central School).[16] In 1885, the Toronto Normal School (teacher training) opened a department for kindergarten teaching.[16]		Elizabeth Harrison wrote extensively on the theory of early childhood education and worked to enhance educational standards for kindergarten teachers by establishing what became the National College of Education in 1886.		In Afghanistan, children between the ages of 3 and 6 attend kindergartens (Dari: کودکستان‎‎; Pashto: وړکتون‎). Although kindergartens in Afghanistan are not part of the school system, they are often run by the government.		Early Childhood Development programs were first introduced during the Soviet occupation with the establishment in 1980 of 27 urban preschools, or kodakistan. The number of preschools grew steadily during the 1980s, peaking in 1990 with more than 270 in Afghanistan. At this peak, there were 2,300 teachers caring for more than 21,000 children in the country. These facilities were an urban phenomenon, mostly in Kabul, and were attached to schools, government offices, or factories. Based on the Soviet model, these Early Childhood Development programs provided nursery care, preschool, and kindergarten for children from 3 months to 6 years of age under the direction of the Department of Labor and Social Welfare.		The vast majority of Afghan families were never exposed to this system, and many of these families were in opposition to these programs due to the belief that it diminishes the central role of the family and inculcates children with Soviet values. With the onset of civil war after the Soviet withdrawal, the number of kindergartens dropped rapidly. By 1995, only 88 functioning facilities serving 2,110 children survived, and the Taliban restrictions on female employment eliminated all of the remaining centers in areas under their control. In 2007, there were about 260 kindergarten/pre-school centers serving over 25,000 children. Though every government center is required to have an early childhood center,[citation needed] at present, no governmental policies deal with early childhood and no institutions have either the responsibility or the capacity to provide such services.[citation needed]		In each state of Australia, kindergarten (frequently referred to as "kinder" or "kindy") means something slightly different. In Tasmania, New South Wales and the Australian Capital Territory, it is the first year of primary school. In Victoria, kindergarten is a form of preschool and may be referred to interchangeably as preschool or kindergarten. In Victoria and Tasmania the phrase for the first year of primary school is called Prep (short for "preparatory"), which is followed by grade 1.		In Queensland, kindergarten is usually an institution for children around the age of 4 and thus it is the precursor to preschool and primary education. In recent years though, Kindergartens have been taking children as young as 6 months old.		The year preceding the first year of primary school education in Western Australia, South Australia or the Northern Territory is referred to respectively as pre-primary, reception or transition.[17]		In New Zealand, kindergarten can refer to education in the 2 years preceding primary school, from age 3 to 4. Primary Education starts at age 5.		In Bangladesh, the term "kindergarten", or "KG School" (Kindergarten School), is used to refer to the schooling children attend from 3 to 6 years of age. The names of the levels are nursery, shishu (children), etc. But the view of kindergarten education has changed much from previous years. Almost every rural area now has at least one Kindergarten School, with most being run in the Bengali language. They also follow the text books published by the National Curriculum and Textbook Board (NCTB) with a light modification, adding some extra books in syllabus. The grades generally start from Nursery (sometimes "Play"), "KG" afterwards, and ends with the 5th grade. Separate from the National Education System, kindergarten is contributing greatly toward achieving the Millennium Development Goal of universal primary education in Bangladesh.		In Bulgaria, the term detska gradina (деτска градина) refers to the schooling children attend from 3 to 7 (in some cases 6) years of age. Since 2012, two years of pre-school education are compulsory. These two years of mandatory pre-school education may be attended either at kindergarten or in preparatory groups at primary schools.[18]		Schools outside of Ontario and the Northwest Territories generally provide one year of kindergarten, except some private schools offer junior kindergarten for 4-year-olds (school before kindergarten is most commonly referred to as pre-school). After kindergarten, the child begins grade one. The province of Nova Scotia refers to Kindergarten as Grade Primary.		The province of Ontario and the Northwest Territories provide two years of kindergarten. Within the province of Quebec, junior kindergarten is called prématernelle (which is not mandatory), is attended by 4-year-olds, and senior kindergarten is called maternelle, which is also not mandatory by the age of 5, this class is integrated into primary schools. Within the French school system in the province of Ontario, junior kindergarten is called maternelle and senior kindergarten is called jardin d'enfants, which is a calque of the German word Kindergarten.		In Chile, the term equivalent to Kindergarten is "Educación parvularia", sometimes also called "Educación Preescolar". It is the first level of the Chilean educational system. It meets the needs of boys and girls integrally from their birth until their entry to the Educación Básica (Primary education), without being considered as compulsory. Generally, schools imparting this level, the JUNJI (National Council of Kindergarten Schools) and other private institutions have the following organization of groups or sub categories of levels:		In China, the equivalent term to kindergarten is 幼儿园 (yòu ér yuán). The children start attending kindergarten at age 3 until they are at least 6 years old. The kindergartens in China generally have the following grades:		The public kindergartens only accept children older than 3 years, while private ones do not have such limitations.		Kindergarten is a day-care service offered to children from age three until the child starts attending school. Kindergarten classes (grade 0) are voluntary and are offered by primary schools before a child enters 1st grade.		Two-thirds of established day-care institutions in Denmark are municipal day-care centres while the other third are privately owned and are run by associations of parents or businesses in agreement with local authorities. In terms of both finances and subject-matter, municipal and private institutions function according to the same principles.		Denmark is credited with pioneering (although not inventing) forest kindergartens, in which children spend most of every day outside in a natural environment.		In Egypt, children may go to kindergartens for two years (KG1 and KG2) between the ages of four and six.		In France, pre-school is known as école maternelle (French for "nursery school", literally "maternal school"). Free maternelle schools are available throughout the country, welcoming children aged from 3 to 6 (although in many places, children under three may not be granted a place). The ages are divided into grande section (GS: 5-year-olds), moyenne section (MS: 4-year-olds), petite section (PS: 3-year-olds) and toute petite section (TPS: 2-year-olds). It is not compulsory, yet almost all children aged 3 to 5 attend. It is regulated by the Ministry of National Education.		In Germany, a Kindergarten (masculine: der Kindergarten, plural Kindergärten) is a facility for the care of pre-school children who are typically at least three years old. By contrast, Kinderkrippe or Krippe refers to a crèche for the care of children before they enter Kindergarten (9 weeks to about three years), Kindertagesstätte—literally "children's day site", usually shortened to Kita—is an umbrella term for any day care facility for pre-schoolers.		Attendance is voluntary, and usually not free of charge. Pre-school children over the age of one are entitled to receive local and affordable day care.[20] Within the federal system, Kindergärten fall under the responsibility of the states,[21] which usually delegate a large share of the responsibility to the municipalities. Due to the subsidiarity principle stipulated by §4 SGB VIII, there are a multitude of operators, from municipalities, churches and welfare societies to parents' initiatives and profit-based corporations. Many Kindergärten follow a certain educational approach, such as Montessori, Reggio Emilia, "Berliner Bildungsprogramm" or Waldorf; forest kindergartens are well established. Most Kindergärten are subsidised by the community councils, with the fees depending on the income of the parents.		Even in smaller townships, there are often both Roman Catholic and Lutheran kindergartens available. Places in crèches and kindergarten are often difficult to secure, and must be 'reserved' in advance, although the situation has improved with a new law in effect August 2013. The availability of childcare, however, varies greatly by region. It is usually better in eastern regions, and in big cities in the north, such as Berlin[22] or Hamburg,[23] and poorest in parts of Southern Germany.[24]		All caretakers in Kita or Kindergarten must have a three-year qualified education, or are under special supervision during training.		Kindergärten can be open from 7am to 5pm or longer and may also house a crèche (Kinderkrippe) for children between the ages of eight weeks and three years, and possibly an afternoon Hort (often associated with a primary school) for school-age children aged 6 to 10 who spend the time after their lessons there. Alongside nurseries, there are day-care nurses (Tagesmütter or Tagespflegepersonen) working independently of any pre-school institution in individual homes and looking after only three to five children, typically up to the age of three. These nurses are supported and supervised by local authorities.		The term Vorschule ("pre-school") is used both for educational efforts in Kindergärten and for a mandatory class that is usually connected to a primary school. Both systems are handled differently in each German state. The Schulkindergarten is a type of Vorschule.		Pre-primary Services in Hong Kong refers to provision of education and care to young children by kindergartens and child care centres. Kindergartens, registered with the Education Bureau, provide services for children from three to six years old. Child care centres, on the other hand, are registered with the Social Welfare Department and include nurseries, catering for children aged two to three, and creches, looking after infants from birth to two.		At present, most of the kindergartens operate on half-day basis offering upper, lower kindergarten classes and nursery classes. Some kindergartens operate full-day kindergarten classes too. Child care centres also provide full-day and half-day services with most centres providing full-day services.		The aim of pre-primary education in Hong Kong is to provide children with a relaxing and pleasurable learning environment to promote a balanced development of different aspects necessary to a child's development such as the physical, intellectual, language, social, emotional and aesthetic aspects.		To help establish the culture of self-evaluation in kindergartens and to provide reference for the public in assessing the quality and standard of pre-primary education, the Education Bureau has developed Performance Indicators for pre-primary institutions in Hong Kong. Commencing in the 2000/01 school year, Quality Assurance Inspection was launched to further promote the development of quality Early Childhood Education.		In Hungary a kindergarten is called an óvoda ("place for caring"). Children attend kindergarten between ages 3–6/7 (they go to school in the year in which they have their 7th birthday). Attendance in kindergarten is compulsory from the age of 3 years, though exceptions are made for developmental reasons.[25] Though kindergartens may include programs in subjects such as foreign languages and music, children spend most of their time playing. In their last year children begin to be prepared to attend elementary school.		Most kindergartens are state-funded. Kindergarten teachers are required to have a diploma.		In India, there are only informal directives pertaining to pre-primary education, for which pre-primary schools and sections need no affiliation. Directives state that children who are three years old on 30 May in the given academic year are eligible to attend Nursery and Kindergarten classes. Typically, children spend 3 to 4 years of their time in pre-primary school after which they are eligible to attend 1st Standard in Primary School which falls under HRD ministry norms. Primary education is now compulsory in India, and accompanied with mid-day meals, in most parts of the country run by the government. Pre-primary is not mandatory, however preferred. All government schools and affiliated private schools allow children who are 5 years of age as of 30 May to enroll to standard 1 of a primary school.		In Italy, pre-school education refers to two different grades:		Italian asili-nido were officially instituted in a 1971 State Law (L. 1044/1971), and may be ruled by either private or public institutions. They were originally established to allow mothers a chance to work out of their homes, and were therefore seen as a social service. Today, they mostly serve the purpose of general education and social interaction. In Italy, much effort has been spent on developing a pedagogical approach to children's care: well known is the so-called Reggio Emilia approach, named after the city of Reggio Emilia, in Emilia-Romagna.		Asili-nido normally occupy small one-story buildings, surrounded by gardens; usually suitable for no more than 60 or 70 children. The heart of the asili-nido are the classrooms, split into playroom and restroom; the playroom always has windows and doors leading to the outside playground and garden.		Maternal schools (scuola materna) were established in 1968 after State Law n. 444 and are a full part of the official Italian education system, though attendance is not compulsory. Like asili-nido (nursery schools), maternal schools may be held either by public or private institutions.		Early childhood education begins at home, and there are numerous books and television shows aimed at helping mothers & fathers of preschool children to educate their children and to parent more effectively. Much of the home training is devoted to teaching manners, proper social behavior, and structured play, although verbal and number skills are also popular themes. Parents are strongly committed to early education and frequently enroll their children in preschools.		Kindergartens (幼稚園 yōchien), predominantly staffed by young female junior college graduates, are supervised by the Ministry of Education, but are not part of the official education system. The 58% of kindergartens that are private accounted for 77% of all children enrolled. In addition to kindergartens, there exists a well-developed system of government-supervised day-care centers (保育園 hoikuen), supervised by the Ministry of Labor. Whereas kindergartens follow educational aims, preschools are predominantly concerned with providing care for infants and toddlers. Just as there are public and private kindergartens, there are both public and privately run preschools. Together, these two kinds of institutions enroll well over 90 percent of all preschool-age children prior to their entrance into the formal system at first grade. The Ministry of Education's 1990 Course of Study for Preschools, which applies to both kinds of institutions, covers such areas as human relationships, health, environment, language, and expression. Starting from March 2008 the new revision of curriculum guidelines for kindergartens as well as for preschools came into effect.		In South Korea, children normally attend kindergarten (Korean: 유치원 yuchi won) between the ages of three or four and six or seven in the Western age system. (Korean ages are calculated differently from Western ages: when they are born they are considered one-year-olds, rather than one day old. Additionally, every January 1, everyone's age increases by one year regardless of when their birthday is. Hence in Korea, kindergarten children are called five-, six- and seven-year-olds.) The school year begins in March. It is followed by primary school. Normally the kindergartens are graded on a three-tier basis.		Korean kindergartens are private schools, and monthly costs vary. Korean parents often send their children to English kindergartens to give them a head start in English. Such specialized kindergartens can be mostly taught in Korean with some English lessons, mostly taught in English with some Korean lessons, or completely taught in English. Almost all middle-class parents send their children to kindergarten.		Kindergarten programs in South Korea attempt to incorporate much academic instruction alongside more playful activities. Korean kindergartners learn to read, write (often in English as well as Korean) and do simple arithmetic. Classes are conducted in a traditional classroom setting, with the children focused on the teacher and one lesson or activity at a time. The goal of the teacher is to overcome weak points in each child's knowledge or skills.		Because the education system in Korea is very competitive, kindergartens are becoming more intensely academic. Children are pushed to read and write at a very young age. They also become accustomed to regular and considerable amounts of homework. These very young children may also attend other specialized afternoon schools, taking lessons in art, piano or violin, taekwondo, ballet, soccer or mathematics.		In Kosovo, kindergarten is known as Çerdhe or Kopshti i fëmijëve, and they serve as Day Care Centers. There are public and private kindergartens, and they are for children under the age of 3. Children between 3–6 years old go to Institucione parashkollore, which are different from the Day Care Centers, because here children start the basic learning process, and they serve as preparatory institutions for the Primary School. After the age of 6, children continue in Primary School. However, neither the Day Care Centers nor the Preparatory Institutions are mandatory.		In Kuwait, Kuwaiti children may go to free government kindergartens for two years (KG1 and KG2) between the ages of four and six.		In Luxembourg, a Kindergarten is called Spillschoul (literally "Playschool", plural Spillschoulen). It is a public education facility which is attended by children between the age of 4 (or 5) and 6 when they advance to the Grondschoul (elementary school).		The Macedonian equivalent of kindergarten is detska gradinka (детска градинка), sometimes called zabavishte (забавиште) when the kids are younger than 4 years. Detska gradinka is not part of the state's mandatory education, because the educational process in the country begins at the age of 5 or 6, i.e. first grade.		In Malaysia, kindergarten is known as tadika. Most kindergartens are available to children of ages five and six (and some are available to children as young as four). For children up to the age of three (or four), there are pre-school playgroups. There are no fixed rules for when a child needs to go to a kindergarten, but the majority will when the child turns 5 years old. The child will usually attend kindergarten for two years, before proceeding to primary school at age 7.[26]		In Mexico, kindergarten is called kínder, with the last year sometimes referred to as preprimaria (primaria is the name given to grades 1 through 6, so the name literally means "prior to elementary school"). The kindergarten system in Mexico was developed by professor Rosaura Zapata, who received the country's highest honor for her contribution. It consists of three years of pre-school education, which are mandatory before elementary school. Previous nursery is optional, and may be offered in either private schools or public schools.		At private schools, kinders usually consist of three grades, and a fourth one may be added for nursery. The fourth one is called maternal. It goes before the other three years and is not obligatory. While the first grade is a playgroup, the other two are of classroom education.		In 2002, the Congress of the Union approved the Law of Obligatory Pre-schooling, which made pre-school education for three to six-year-olds obligatory, and placed it under the auspices of the federal and state ministries of education.[27][28]		In Mongolia, kindergarten is known as "цэцэрлэг" or tsetserleg. As of September 2013, there are approximately 152 kindergartens registered in the country. From those 152 kindergartens, 142 are state owned. Children begin kindergarten at the age of 2 and finish it by 5. The education system before kindergarten in Mongolia is called "ясль", which accepts children between 0 and 2 years of age.		In Morocco, pre-school is known as école maternelle, kuttab, or ar-rawd. State-run, free maternelle schools are available throughout the kingdom, welcoming children aged from 2 to 5 (although in many places, children under 3 may not be granted a place). It is not compulsory, yet almost 80% of children aged 3 to 5 attend. It is regulated by the Moroccan department of education.		In Nepal, kindergartens are run as private institutions, with their lessons conducted in English. The kindergarten education in Nepal is most similar to that of Hong Kong and India. Children start attending kindergarten from the age of 2 until they are at least 5 years old.		The kindergartens in Nepal have the following grades:		In the Netherlands, the equivalent term to kindergarten was kleuterschool. From the mid-19th century to the mid-20th century the term Fröbelschool was also common, after Friedrich Fröbel. However this term gradually faded in use as the verb Fröbelen gained a slightly derogatory meaning in everyday language. Until 1985, it used to be a separate non-compulsory form of education (for children aged 4–6 years), after which children (aged 6–12 years) attended the primary school (lagere school). After 1985, both forms were integrated into one, called basisonderwijs (Dutch for primary education). For children under 4, the country offers private, subsidized daycares (kinderdagverblijf), which are non compulsory but nevertheless very popular.		In Norway, barnehage (children's garden) is the term equivalent to kindergarten, used for children in the ages between 10 months and 6 years. The first barnehager were founded in Norway in the late 19th century. Although they have existed for 120 years, they are not considered part of the education system. They are both publicly and privately owned and operated. The staff, at minimum the manager, should be educated as førskolelærer (pre-school teachers). The children spend most of the time outdoors. There is also an institution called barnepark (children's park), which does not have to have certified staff.		In Peru, the term nido refers to the schooling children attend from 3 to 6 years of age. It is followed by primary school classes, which last for six years. Some families choose to send their children to primary school at the age of 6. In 1902 the teacher Elvira Garcia and Garcia co-founder of the Society cited above, organized the first kindergarten for children 2 to 8 years old, Fanning annex to the Lyceum for ladies. Her studies and concern for children led her to spread through conferences and numerous documents, the importance of protecting children early and to respond to the formation of a personality based on justice and understanding, as well as the use of methods Fröbel and from Montessori and participation of parents in this educational task.		In the Philippines, education officially starts at the Elementary level and placing children into early childhood education through kindergarten is optional to parents. Early childhood education in the Philippines is classified into:		Early childhood education was strengthened through the creation of the Early Childhood Care and Development Act of 2000 (Republic Act No. 8980).[29] In 2011, the Department of Education disseminated copies of the Kindergarten Education Act through Republic Act No. 10157 making it compulsory and mandatory in the entire nation. As a provision in this law, children under five years old are required to enroll in a kindergarten in any public elementary school in the country. This goes with the implementation of the K-12 system in the Basic Education Curriculum.		In Romania, grădiniţă, which means "little garden", is the favored form of education for preschool children usually aged 3–6. The children are divided in three groups: "little group" (grupa mică, age 3–4), "medium group" (grupa mijlocie, age 4-5) and "big group" (grupa mare, age 5-6). In the last few years, private kindergartens have become popular, supplementing the state preschool education system. Kindergarten is optional. The "preparatory school year" (clasa pregătitoare) is for children aged 6–7, and since it became compulsory in 2012,[30] it usually takes place at school.		In the Russian Federation, Детский сад (dyetskiy sad, literal translation of "children's garden") is a preschool educational institution for children, usually 3 to 6 years of age.		Kindergartens in South Africa provide up to three years of preschool programs for children aged between three and six. The three-year program, known as nursery, kindergarten 1 (K1), and kindergarten 2 (K2), prepares children for their first year in primary school education. Some kindergartens further divide nursery into N1 and N2.		In Spain, kindergarten is called infantil, ciclo infantil or guardería, and serves children from 3 to 6 years of age. It's commonly known by some people as parvulitos.		1º ciclo de Educación Infantil (from 0 to 3 years of age) 2º ciclo de Educación Infantil (from 3 to 6 years of age)		Kindergarten in Sudan is divided into private and public kindergarten. Preschool is compulsory in Sudan. The proper kindergarten age spans from 3–6 years. The curriculum covers Arabic, English, religion, mathematics and more.		In Sweden, kindergarten activities were established in the 19th century, and have been widely expanded since the 1970s.[31][32] The first Swedish kindergarten teachers were trained by Henriette Schrader-Breymann at the Pestalozzi-Fröbel Haus, which she founded in 1882.[31][32]		While many public kindergartens and preschools exist in Taiwan, private kindergartens and preschools are also quite popular. Many private preschools offer accelerated courses in various subjects to compete with public preschools and capitalize on public demand for academic achievement. Curriculum at such preschools often encompasses subject material such as science, art, physical education and even mathematics classes. The majority of these schools are part of large school chains, which operate under franchise arrangements. In return for annual fees, the chain enterprises may supply advertising, curriculum, books, materials, training, and even staff for each individual school.		There has been a huge growth in the number of privately owned and operated English immersion preschools in Taiwan since 1999. These English immersion preschools generally employ native English speaking teachers to teach the whole preschool curriculum in an "English only" environment. The legality of these types of schools has been called into question on many occasions, yet they continue to prosper. Some members of Taiwanese society have raised concerns as to whether local children should be placed in English immersion environments at such a young age, and have raised fears that the students abilities in their mother language may suffer as a result. The debate continues, but at the present time, the market for English Immersion Preschools continues to grow.		In 2010, a total of 56% of children aged one to six years old had the opportunity to attend preschool education, the Education and Science Ministry of Ukraine reported in August 2010.[33] Many preschools and kindergartens were closed previously in light of economic and demographic considerations.[34]		The term kindergarten is never used in the UK to describe modern pre-school education; pre-schools are usually known as creche, nursery schools or playgroups. However, the word "kindergarten" is used for more specialist organisations such as forest kindergartens, and is sometimes used in the naming of private nurseries that provide full-day child care for working parents. Historically the word was used during the nineteenth century when activists like Adelaide Manning were introducing educators to the work of Friedrich Fröbel.[35]		In the UK, parents have the option of nursery for their children at the ages of three or four years, before compulsory education begins. Before that, less structured childcare is available privately. The details vary between England, Northern Ireland, Scotland and Wales.		Some nurseries are attached to state infant or primary schools, but many are provided by the private sector. The Scottish government provides funding[36] so that all children from the age of three until they start compulsory school can attend five sessions per week of two and a half hours each, either in state-run or private nurseries. Working parents can also receive from their employers child care worth £55 per week free of income tax,[37] which is typically enough to pay for one or two days per week.		The Scottish Government defines its requirements for nursery schools in the Early Years Framework[38] and the Curriculum for Excellence.[39] Each school interprets these with more or less independence (depending on their management structure) but must satisfy the Care Commission[40] in order to retain their licence to operate. The curriculum aims to develop:		Nursery forms part of the Foundation Stage of education. In the 1980s, England and Wales officially adopted the Northern Irish system whereby children start school either in the term or year in which they will become five depending on the policy of the local education authority. In Scotland, schooling becomes compulsory between the ages of 4½ and 5½ years, depending on their birthday (school starts in August for children who were 4 by the end of the preceding February). The first year of compulsory schooling is known as Reception in England, Dosbarth Derbyn in Welsh ("reception class") and Primary One in Scotland and Northern Ireland.		In the US, kindergarten is usually part of the K-12 educational system. In most state and private schools, children begin kindergarten at age 5 to 6 and attend for one year. They do activities such as addition (+), subtraction) (-), and playing outside on the playground. [41] Forty-three states require their school districts to offer a kindergarten year.[42]		In a typical US kindergarten classroom, resources like toys, picture books, and crayons are available for children's use. The daily schedule varies from town to town, but there are some similarities. In the morning, the children usually do circle time. This includes saying the pledge of allegiance, looking at the calendar, and discussing the weather and season that day. Next, the children work on different subjects:		In math, kindergartners usually do single digit addition and subtraction, learn to count with "more or less" games, become acquainted with a clock, and learn skip counting to prepare them for one digit multiplication.		In language arts (English), children learn sight words, ( cat, fun), rhyming, blends, and silent e.		In social studies, kindergartners learn about the months, U.S. states, the continents, and sometimes about people performing community functions (e.g. doctor, barber, teacher) and places (e.g."You go to a hospital when you're sick or to have surgery." "You go to a park to play.")		After a few lessons, there is a break for lunch. Children either get their lunchboxes or fetch a lunch from the cafeteria and eat it (or talk) there or in the classroom. Sometimes after lunch the children have recess, some students' favorite part of the day, when they can go outside to play on swings, slides, play basketball, or socialize. After recess kindergartners go back inside to do more learning. Some schools let children take a nap or do free choice (blocks, tic tac toe, play doh, etc.) When kindergarten is over for the day, parents come to fetch their child or the children ride a bus home.		The following reading list relates specifically to kindergarten in North America, where it is the first year of formal schooling and not part of the pre-school system as it is in the rest of the world:		
A Student Bill of Rights or Charter of Student Rights and Freedoms is a document adopted by a student group, university or college or government at a local, state or national level. It outlines a population's basic beliefs regarding student rights. These statements of belief are often the foundation for future legislative efforts or collaborative efforts to create joint statements between organizations. The European Student Union, for example, uses their Student Rights Charter when lobbying for student rights in the European Union Higher Education Area as a document representing the student will.[1] The historic National Student Association in the United States used their Student Bill of Rights to help create dialogue between the American Association of University Professors and to initiate the creation of a joint statement on student rights.[2] This collaborative effort gave credence to the demands of students and helped normalize student rights on campuses across North America. While the United States Student Association does not have a student bill of rights of its own, it upholds the 1947 student bill of rights put forth by the National Student Association.[3]		These documents tend to do several things. They can be statements of belief, policy or law. When put forth by a student organization or third party organization they tend to be statements of belief because these organizations do not often have the ability to enforce their beliefs. Even though they are not legally binding they are important because they help policy and law makers understand what students expect and believe to be ethical treatment. At the institutional level they tend to be policy statements. These are legally binding as a promise from the institution to the students who attend and have been found in court to be considered to be part of the educational contract.[4] At the state or federal level student rights documents are legal and binding upon either the state or the country.		At any level these documents provide students with an understanding of their civil or legal rights which are already contained in legislation. These include rights pertinent to all citizens and also to students in the educational setting and provide procedural rights to inform students how institutions should be respecting their legal rights. When used as a statement of belief, however, they often include rights an organization feels students should have and the procedural rights institutions must follow to ensure these rights are fulfilled. There are currently a number of student petitions calling for the creation of national student bills in various countries. While there have been some attempts to create subject specific student bills in the United States, like the Academic Bill of Rights, these have not been successful due primarily for demand for a document which is wider in scope and deals with students constitutional, civil, contractual and consumer rights.						No country in North America has yet adopted a National Student Bill of Rights, Charter of Rights and Freedoms or Code of Rights and Responsibilities.[5][6]		In the United States there have been several national student bill of rights drafted by student organizations including the historic National Student Association and the American Association of University Professors and other non-governmantal organizations but as yet none have been legally institutionalized.		In 1947 the National Student Association NSA in the United States adopted a student bill of rights. The text of this document is not easily accessible. In 1967, however, the NSA put forth a joint statement on the rights and freedoms of students with the American Association of University Professors AAUP (now the Association of American Colleges and Universities). This statement was endorsed by a number of professional organizations. This document included the following rights:		Constitutional Rights		Speech and Association Rights		Due Process Rights		Classroom Rights		The United States Student Association does not have an official student bill of rights. The USSA was formed when the historic NSA merged with the National Student Lobby NSL in 1978. They did not officially adopt the student bill of rights put forth in 1947. They text of this document is not on their website and cannot easily be found with a search of the internet.		In 2003, Georgia Congressman Kingston proposed the first national student bill of rights, House Bill #318. This bill, which was rejected in congress, was not an all encompassing student bill of rights but was narrowly defined to address academic freedom. Nevertheless, it was the first attempt at the creation of national legislation. The congressman found that "at almost every American university, conservative professors are drastically outnumbered." This bill was intended to secure the intellectual freedom of students and faculty. It did not address whether teachers would have the freedom to determine all course goals, curriculum, assignments, grading schemes and course timeline. These issues are of major importance because students in Europe are calling for rights which protect students from teachers having complete academic freedom in the classroom and which regulate them to ensure students have minimum educational quality standards.[8]		UP = Unproceduralized / P = Proceduralized (Proceduralized means there are bylaws instructing schools how to interpret and institute these laws)		Student Rights Document		Athlete's Bill of Rights Deaf Children's Bill of Rights		Deaf Child Bill of Rights		Deaf Child's Bill of Rights		Special Ed. Parent Rights Guide		Patient Bill of Rights (Uni hospitals)		Limited Student Rights Doc[9]		Inc only one federal and one state right		Student Rights and Responsibilities (post 18) in Special Ed.[10] Civil Rights Website[11]		In the United States individual institutions often have their own Student Bill of Rights, Charter of Rights and Freedoms or Code of Rights and Responsibilities. In elementary and secondary education these codes are often drafted by school board officials with perhaps the input of parent or parent teacher associations. In post secondary institutions these are often drafted by the academic senate with the contribution of student representatives. This process often includes varying degrees of student input from student surveys to no student voice and the academic senate may have as much as 50% student representation or as little as 0% student representation depending on the structure of the institution.		UP = Unproceduralized / P = Proceduralized (Proceduralized means the school has included in the document how it will institute these laws)		The Canadian Federation of Students has yet to accept a student bill or rights, code of rights and responsibilities or a charter of rights and freedoms. The Canadian Encyclopedia, which details issues of Canadian life and governance, however, states that in Canada "Basically 2 sorts of rights apply to students: substantive rights - the actual rights that students should enjoy - and procedural rights - methods by which students claim their rights. This article is concerned with students in public institutions, although those in private schools can claim rights under the common law and provincial education Acts."		In the 1960s, the Canadian Union of Students had adopted the Declaration on the Canadian Student, a declaration aimed defining the rights and role of a student. The Declaration was adopted at the September 1965 annual meeting of the CUS, and was initiated mainly by the Students' Union of the University of Ottawa President Jock Turcot.[13] A year later, the CUS re-affirmed the "fundamental philosophy" behind the Declaration.[14]		In 2008 the European Student Union adopted a Student Rights Charter [15] which is in essence their platform. This document includes those rights and freedoms which members of the European student Union are lobbying for in their own countries. Romania currently has the strongest student rights legislation in the European Union and this is in part to do with the efforts of the European Student Union to press for legislation.[15] The 2008 ESU Student Rights Charter includes the following rights:		Access to Higher Education		Student Involvement		Extracurricular Aspect of Study		Curricular Aspect of Study		Romania is the country which has the greatest student rights legislation currently in place. In 2010 the National Alliance of student organizations in Romania, which is also part of the European Student Union, and the Romanian Students' Union(USR) worked with the Romanian National Government to bring into law the Romanian National Student Code of Rights and Responsibilities. This document provides Romanian students with roughly a hundred which theoretical rights and procedural rights necessary to ensure theoretical rights are fulfilled.[8] This document includes the following rights:		Educational Package Rights		Contract Rights		Equitability Rights		Accountability & Quality Assurance Rights		Due Process Rights		Information Accessibility Rights		It does not seem that Asia has any national or continental student rights bills in place.		It does not seem that Africa has any national or continental student rights bills in place.		It does not seem that South or Central America has any national or continental student rights bills in place.		
River Valley High School (RVHS) (Chinese: 立化中学; pinyin: Lì Huà Zhōngxué) is an educational institution in Singapore offering the Integrated Programme in Singapore. Founded in 1956, it is one of the Special Assistance Plan schools designated by the Ministry of Education, Singapore.						River Valley High School was founded as the Singapore Government Chinese Middle School. It was the first Chinese middle school set up by the government, occupying the premises of Seng Poh Primary School, it was later renamed Queenstown Government Chinese Middle School and subsequently River Valley Government Chinese Middle School when it was moved to Strathmore Avenue.[1]		River Valley High School admitted its first batch of English stream pupils when it was selected as one of the nine Special Assistance Plan (SAP) schools in 1979. It shifted from River Valley to West Coast in December 1986. Due to the smaller school campus at the West Coast site, the school became a double-session school. It reverted to a single-session school in 1993 after adding new blocks to the West Coast site. It was among the first six schools in Singapore to assume the ascendance to autonomous status in 1994. For its achievements in its CCAs, it was also one of the first schools to receive all three available Sustained Achievement Awards when it was first offered by the Ministry of Education in 2001.[2]		In 2006 River Valley High School was officially designated a 6-Year Integrated Programme school. Under the programme, students follows a six-year Integrated Programme curriculum which allows students to sit for the GCE 'A' Level examinations at year 6 without taking the GCE 'O' Level examinations. The final batch of GCE 'O' Level students graduated from River Valley High School in 2007.[3][4]		River Valley High School retained its school song in mandarin Chinese. The song, with close links to the school motto, represents the heritage of the school and the aspirations of the founders' hopes in educating the next generation. Students are reminded to bring glory to the school through excellence in both academics and behaviour. The school song is written in literary Chinese, giving it a poetic touch. The lyrics are written in verses of four characters, a parallel to Chinese idioms, with the exception of the last two verses.[5][6]		River Valley High School's crest is made of the school's initials "RV". The colour of red symbolises radiance, progress and vitality, the colour of blue symbolises steadfastness, graciousness and serenity, while white symbolises purity and receptiveness to innovation.		The school uniform is unique to River Valley High School, one of the most outstanding and identifiable uniform designs in Singapore. For Year 1 to Year 3, the uniform is all white, with a school logo at the front left chest that reads RV in red and blue - the school For Year 3/4, the uniform is white blouse with blue skirt/white pants.		From 2006, River Valley High School offers the 6-year Integrated Programme, allowing its students to take the GCE 'A' level examinations at the end their sixth year in the school. It also offers the Bicultural Studies Programme (Chinese), joining Dunman High School, Hwa Chong Institution and Nanyang Girls' High School. The programmed is aimed to nurture independent and passionate individuals who uphold strong moral character, effectively bilingual in English and Chinese language, and have a global perspective that allows them to face challenges in the changing world.[7][8]		The Construct, Integrate, Differentiate (CID) Programme is an integrated curriculum under the RVIP that focuses on knowledge construction with differentiation features to cater to the diverse talents and interests of the students.		The CID Programme provides students with opportunities to construct their own learning by integrating knowledge from various disciplines, using modes of inquiry appropriate to the subject or project that they are working on. In the CID Programme, students spend their freed time constructively on inquiry-based and research-oriented project work. Teamwork is emphasised through collaborative and co-operative learning.[9]		The programme is designed with the objectives of providing students with the opportunities to engage in individualised research in an area of strength. All students will complete coursework in Research Methods, an in depth learning of the quantitative and qualitative methods appropriate to individualised researches. With the completion of the Research Methods coursework, students can opt to undertake research in one of the following programmes: the Junior Scientists Programme (scientific research work), Innovators Programme (engineering and design), Mathematics Talent Programme and the Explorers Programme (humanities). Students can also seek attachments at the local or overseas institutes for the CID Extended Learning. Opportunities for further learning are also provided through overseas exchange programmes.		CHAMPS is an acronym for "Character Education; Health Education; Active Citizenry; Moral Philosophy; Physical Education; & Student Leadership Development". It is a unique and integrated Pupil Development Programme designed by RV. It focuses on the development of values and character. It seeks to develop life skills, and the social, national, cultural and moral consciousness of the students. ASK will be infused in the teaching and learning of PDP. Besides the core modules in CHAMPS, students would complete at least 1 PDP elective module each year.		River Valley High School, being a Special Assistance Plan school, has a very strong Chinese environment. Chinese language, culture and history are often emphasised in the School. It is compulsory for all Year 1 to 4 students to take up Higher Chinese as a subject, Chinese culture lessons are also mandatory in lower secondary and students can opt for Chinese Literature in upper secondary.		
Feminism is a range of political movements, ideologies, and social movements that share a common goal: to define, establish, and achieve political, economic, personal, and social rights for women.[1][2] This includes seeking to establish educational and professional opportunities for women that are equal to such opportunities for men.		Feminist movements have campaigned and continue to campaign for women's rights, including the right to vote, to hold public office, to work, to earn fair wages or equal pay, to own property, to receive education, to enter contracts, to have equal rights within marriage, and to have maternity leave. Feminists have also worked to promote bodily autonomy and integrity, and to protect women and girls from rape, sexual harassment, and domestic violence.[3] Changes in dress and acceptable physical activity have often been part of feminist movements.[4]		Feminist campaigns are generally considered to be a main force behind major historical societal changes for women's rights, particularly in the West, where they are near-universally credited with achieving women's suffrage, gender neutrality in English, reproductive rights for women (including access to contraceptives and abortion), and the right to enter into contracts and own property.[5] Although feminist advocacy is, and has been, mainly focused on women's rights, some feminists, including bell hooks, argue for the inclusion of men's liberation within its aims because men are also harmed by traditional gender roles.[6] Feminist theory, which emerged from feminist movements, aims to understand the nature of gender inequality by examining women's social roles and lived experience; it has developed theories in a variety of disciplines in order to respond to issues concerning gender.[7][8]		Numerous feminist movements and ideologies have developed over the years and represent different viewpoints and aims. Some forms of feminism have been criticized for taking into account only white, middle class, and college-educated perspectives. This criticism led to the creation of ethnically specific or multicultural forms of feminism, including black feminism and intersectional feminism.[9]						Charles Fourier, a Utopian Socialist and French philosopher, is credited with having coined the word "féminisme" in 1837.[10] The words "féminisme" ("feminism") and "féminist" ("feminist") first appeared in France and the Netherlands in 1872,[11] Great Britain in the 1890s, and the United States in 1910,[12][13] and the Oxford English Dictionary lists 1852 as the year of the first appearance of "feminist"[14] and 1895 for "feminism".[15] Depending on the historical moment, culture and country, feminists around the world have had different causes and goals. Most western feminist historians contend that all movements working to obtain women's rights should be considered feminist movements, even when they did not (or do not) apply the term to themselves.[16][17][18][19][20][21] Other historians assert that the term should be limited to the modern feminist movement and its descendants. Those historians use the label "protofeminist" to describe earlier movements.[22]		The history of the modern western feminist movements is divided into three "waves".[23][24] Each wave dealt with different aspects of the same feminist issues. The first wave comprised women's suffrage movements of the nineteenth and early twentieth centuries, promoting women's right to vote. The second wave was associated with the ideas and actions of the women's liberation movement beginning in the 1960s. The second wave campaigned for legal and social equality for women. The third wave is a continuation of, and a reaction to, the perceived failures of second-wave feminism, which began in the 1990s.[25]		First-wave feminism was a period of activity during the 19th century and early twentieth century. In the UK and eventually the US, it focused on the promotion of equal contract, marriage, parenting, and property rights for women. By the end of the 19th century, a number of important steps had been made with the passing of legislation such as the UK Custody of Infants Act 1839 which introduced the Tender years doctrine for child custody arrangement and gave woman the right of custody of their children for the first time.[26][27][28] Other legislation such as the Married Women's Property Act 1870 in the UK and extended in the 1882 Act,[29] these became models for similar legislation in other British territories. For example, Victoria passed legislation in 1884, New South Wales in 1889, and the remaining Australian colonies passed similar legislation between 1890 and 1897. Therefore, with the turn of the 19th century activism had focused primarily on gaining political power, particularly the right of women's suffrage, though some feminists were active in campaigning for women's sexual, reproductive, and economic rights as well.[30]		Women's suffrage began in Britain's Australasian colonies at the close of the 19th century, with the self-governing colonies of New Zealand granting women the right to vote in 1893 and South Australia granting female suffrage (the right to vote and stand for parliamentary office) in 1895. This was followed by Australia granting female suffrage in 1902.[31][32]		In Britain the Suffragettes and the Suffragists campaigned for the women's vote, and in 1918 the Representation of the People Act was passed granting the vote to women over the age of 30 who owned property. In 1928 this was extended to all women over 21.[33] Emmeline Pankhurst was the most notable activist in England, with Time naming her one of the 100 Most Important People of the 20th Century stating: "she shaped an idea of women for our time; she shook society into a new pattern from which there could be no going back."[34] In the U.S., notable leaders of this movement included Lucretia Mott, Elizabeth Cady Stanton, and Susan B. Anthony, who each campaigned for the abolition of slavery prior to championing women's right to vote. These women were influenced by the Quaker theology of spiritual equality, which asserts that men and women are equal under God.[35] In the United States, first-wave feminism is considered to have ended with the passage of the Nineteenth Amendment to the United States Constitution (1919), granting women the right to vote in all states. The term first wave was coined retroactively to categorize these western movements after the term second-wave feminism began to be used to describe a newer feminist movement that focused on fighting social and cultural inequalities, as well political inequalities.[30][36][37][38][39]		During the late Qing period and reform movements such as the Hundred Days' Reform, Chinese feminists called for women's liberation from traditional roles and Neo-Confucian gender segregation.[40][41][42] Later, the Chinese Communist Party created projects aimed at integrating women into the workforce, and claimed that the revolution had successfully achieved women's liberation.[43]		According to Nawar al-Hassan Golley, Arab feminism was closely connected with Arab nationalism. In 1899, Qasim Amin, considered the "father" of Arab feminism, wrote The Liberation of Women, which argued for legal and social reforms for women.[44] He drew links between women's position in Egyptian society and nationalism, leading to the development of Cairo University and the National Movement.[45] In 1923 Hoda Shaarawi founded the Egyptian Feminist Union, became its president and a symbol of the Arab women's rights movement.[45]		The Iranian Constitutional Revolution in 1905 triggered the Iranian women's movement, which aimed to achieve women's equality in education, marriage, careers, and legal rights.[46] However, during the Iranian revolution of 1979, many of the rights that women had gained from the women's movement were systematically abolished, such as the Family Protection Law.[47]		In France, women obtained the right to vote only with the Provisional Government of the French Republic of 21 April 1944. The Consultative Assembly of Algiers of 1944 proposed on 24 March 1944 to grant eligibility to women but following an amendment by Fernand Grenier, they were given full citizenship, including the right to vote. Grenier's proposition was adopted 51 to 16. In May 1947, following the November 1946 elections, the sociologist Robert Verdier minimized the "gender gap", stating in Le Populaire that women had not voted in a consistent way, dividing themselves, as men, according to social classes. During the baby boom period, feminism waned in importance. Wars (both World War I and World War II) had seen the provisional emancipation of some women, but post-war periods signalled the return to conservative roles.[48]		By the mid 20th century, in some European countries, women still lacked some significant rights. Feminists in these countries continued to fight for voting rights. In Switzerland, women gained the right to vote in federal elections in 1971;[49] but in the canton of Appenzell Innerrhoden women obtained the right to vote on local issues only in 1991, when the canton was forced to do so by the Federal Supreme Court of Switzerland.[50] In Liechtenstein, women were given the right to vote by the women's suffrage referendum of 1984. Three prior referendums held in 1968, 1971 and 1973 had failed to secure women's right to vote.		Feminists continued to campaign for the reform of family laws which gave husbands control over their wives. Although by the 20th century coverture had been abolished in the UK and the US, in many continental European countries married women still had very few rights. For instance, in France married women did not receive the right to work without their husband's permission until 1965.[51][52] Feminists have also worked to abolish the "marital exemption" in rape laws which precluded the prosecution of husbands for the rape of their wives.[53] Earlier efforts by first-wave feminists such as Voltairine de Cleyre, Victoria Woodhull and Elizabeth Clarke Wolstenholme Elmy to criminalize marital rape in the late 19th century had failed;[54][55] this was only achieved a century later in most Western countries, but is still not achieved in many other parts of the world.[56]		French philosopher Simone de Beauvoir provided a Marxist solution and an existentialist view on many of the questions of feminism with the publication of Le Deuxième Sexe (The Second Sex) in 1949.[57] The book expressed feminists' sense of injustice. Second-wave feminism is a feminist movement beginning in the early 1960s[58] and continuing to the present; as such, it coexists with third-wave feminism. Second-wave feminism is largely concerned with issues of equality beyond suffrage, such as ending gender discrimination.[30]		Second-wave feminists see women's cultural and political inequalities as inextricably linked and encourage women to understand aspects of their personal lives as deeply politicized and as reflecting sexist power structures. The feminist activist and author Carol Hanisch coined the slogan "The Personal is Political", which became synonymous with the second wave.[3][59]		Second- and third-wave feminism in China has been characterized by a reexamination of women's roles during the communist revolution and other reform movements, and new discussions about whether women's equality has actually been fully achieved.[43]		In 1956, President Gamal Abdel Nasser of Egypt initiated "state feminism", which outlawed discrimination based on gender and granted women's suffrage, but also blocked political activism by feminist leaders.[60] During Sadat's presidency, his wife, Jehan Sadat, publicly advocated further women's rights, though Egyptian policy and society began to move away from women's equality with the new Islamist movement and growing conservatism.[61] However, some activists proposed a new feminist movement, Islamic feminism, which argues for women's equality within an Islamic framework.[62]		In Latin America, revolutions brought changes in women's status in countries such as Nicaragua, where feminist ideology during the Sandinista Revolution aided women's quality of life but fell short of achieving a social and ideological change.[63]		In 1969, Betty Friedan's book The Feminine Mystique was published and helped voice the discontent that American women felt. The book proved highly successful, almost becoming a bible for feminists and a spur for political activists. The book's success also meant that Friedan could lecture her views while she was on tour in 1970. Within ten years, after Friedan's successful publishing, women made up more than half of the total percentage in the First World workforce.[64]		In the early 1990s in the USA, third-wave feminism began as a response to perceived failures of the second wave and to the backlash against initiatives and movements created by the second wave. Third-wave feminism distinguished itself from the second wave around issues of sexuality, challenging female heterosexuality and celebrating sexuality as a means of female empowerment.[65] Third-wave feminism also seeks to challenge or avoid what it deems the second wave's essentialist definitions of femininity, which, they argue, over-emphasize the experiences of upper middle-class white women. Third-wave feminists often focus on "micro-politics" and challenge the second wave's paradigm as to what is, or is not, good for women, and tend to use a post-structuralist interpretation of gender and sexuality.[30][66][67][68] Feminist leaders rooted in the second wave, such as Gloria Anzaldúa, bell hooks, Chela Sandoval, Cherríe Moraga, Audre Lorde, Maxine Hong Kingston, and many other non-white feminists, sought to negotiate a space within feminist thought for consideration of race-related subjectivities.[67][69][70] Third-wave feminism also contains internal debates between difference feminists, who believe that there are important differences between the sexes, and those who believe that there are no inherent differences between the sexes and contend that gender roles are due to social conditioning.[71]		Standpoint theory is a feminist theoretical point of view that believes a persons' social position influences their knowledge. This perspective argues that research and theory treats women and the feminist movement as insignificant and refuses to see traditional science as unbiased.[72] Since the 1980s, standpoint feminists have argued that the feminist movement should address global issues (such as rape, incest, and prostitution) and culturally specific issues (such as female genital mutilation in some parts of Africa and the Middle East, as well as glass ceiling practices that impede women's advancement in developed economies) in order to understand how gender inequality interacts with racism, homophobia, classism and colonization in a "matrix of domination".[73][74]		The term post-feminism is used to describe a range of viewpoints reacting to feminism since the 1980s. While not being "anti-feminist", post-feminists believe that women have achieved second wave goals while being critical of third wave feminist goals. The term was first used to describe a backlash against second-wave feminism, but it is now a label for a wide range of theories that take critical approaches to previous feminist discourses and includes challenges to the second wave's ideas.[75] Other post-feminists say that feminism is no longer relevant to today's society.[76] Amelia Jones has written that the post-feminist texts which emerged in the 1980s and 1990s portrayed second-wave feminism as a monolithic entity.[77] Dorothy Chunn notes a "blaming narrative" under the post-feminist moniker, where feminists are undermined for continuing to make demands for gender equality in a "post-feminist" society, where "gender equality has (already) been achieved." According to Chunn, "many feminists have voiced disquiet about the ways in which rights and equality discourses are now used against them."[78]		Feminist theory is the extension of feminism into theoretical or philosophical fields. It encompasses work in a variety of disciplines, including anthropology, sociology, economics, women's studies, literary criticism,[79][80] art history,[81] psychoanalysis[82] and philosophy.[83][84] Feminist theory aims to understand gender inequality and focuses on gender politics, power relations, and sexuality. While providing a critique of these social and political relations, much of feminist theory also focuses on the promotion of women's rights and interests. Themes explored in feminist theory include discrimination, stereotyping, objectification (especially sexual objectification), oppression, and patriarchy.[7][8] In the field of literary criticism, Elaine Showalter describes the development of feminist theory as having three phases. The first she calls "feminist critique", in which the feminist reader examines the ideologies behind literary phenomena. The second Showalter calls "gynocriticism", in which the "woman is producer of textual meaning". The last phase she calls "gender theory", in which the "ideological inscription and the literary effects of the sex/gender system are explored".[85]		This was paralleled in the 1970s by French feminists, who developed the concept of écriture féminine (which translates as 'female or feminine writing').[75] Helene Cixous argues that writing and philosophy are phallocentric and along with other French feminists such as Luce Irigaray emphasize "writing from the body" as a subversive exercise.[75] The work of Julia Kristeva, a feminist psychoanalyst and philosopher, and Bracha Ettinger,[86] artist and psychoanalyst, has influenced feminist theory in general and feminist literary criticism in particular. However, as the scholar Elizabeth Wright points out, "none of these French feminists align themselves with the feminist movement as it appeared in the Anglophone world".[75][87] More recent feminist theory, such as that of Lisa Lucile Owens,[88] has concentrated on characterizing feminism as a universal emancipatory movement.		Many overlapping feminist movements and ideologies have developed over the years.		Some branches of feminism closely track the political leanings of the larger society, such as liberalism and conservatism, or focus on the environment. Liberal feminism seeks individualistic equality of men and women through political and legal reform without altering the structure of society. Catherine Rottenberg has argued that the neoliberal shirt in Liberal feminism has led to that form of feminism being individualized rather than collectivized and becoming detached from social inequality.[89] Due to this she argues that Liberal Feminism cannot offer any sustained analysis of the structures of male dominance, power, or privilege.[89]		Radical feminism considers the male-controlled capitalist hierarchy as the defining feature of women's oppression and the total uprooting and reconstruction of society as necessary.[3] Conservative feminism is conservative relative to the society in which it resides. Libertarian feminism conceives of people as self-owners and therefore as entitled to freedom from coercive interference.[90] Separatist feminism does not support heterosexual relationships. Lesbian feminism is thus closely related. Other feminists criticize separatist feminism as sexist.[6] Ecofeminists see men's control of land as responsible for the oppression of women and destruction of the natural environment; ecofeminism has been criticized for focusing too much on a mystical connection between women and nature.[91]		Rosemary Hennessy and Chrys Ingraham say that materialist forms of feminism grew out of Western Marxist thought and have inspired a number of different (but overlapping) movements, all of which are involved in a critique of capitalism and are focused on ideology's relationship to women.[92] Marxist feminism argues that capitalism is the root cause of women's oppression, and that discrimination against women in domestic life and employment is an effect of capitalist ideologies.[93] Socialist feminism distinguishes itself from Marxist feminism by arguing that women's liberation can only be achieved by working to end both the economic and cultural sources of women's oppression.[94] Anarcha-feminists believe that class struggle and anarchy against the state[95] require struggling against patriarchy, which comes from involuntary hierarchy.		Sara Ahmed argues that Black and Postcolonial feminisms pose a challenge "to some of the organizing premises of Western feminist thought."[96] During much of its history, feminist movements and theoretical developments were led predominantly by middle-class white women from Western Europe and North America.[69][73][97] However women of other races have proposed alternative feminisms.[73] This trend accelerated in the 1960s with the civil rights movement in the United States and the collapse of European colonialism in Africa, the Caribbean, parts of Latin America, and Southeast Asia. Since that time, women in developing nations and former colonies and who are of colour or various ethnicities or living in poverty have proposed additional feminisms.[97] Womanism[98][99] emerged after early feminist movements were largely white and middle-class.[69] Postcolonial feminists argue that colonial oppression and Western feminism marginalized postcolonial women but did not turn them passive or voiceless.[9] Third-world feminism and Indigenous feminism are closely related to postcolonial feminism.[97] These ideas also correspond with ideas in African feminism, motherism,[100] Stiwanism,[101] negofeminism,[102] femalism, transnational feminism, and Africana womanism.[103]		In the late twentieth century various feminists began to argue that gender roles are socially constructed,[104][105] and that it is impossible to generalize women's experiences across cultures and histories.[106] Post-structural feminism draws on the philosophies of post-structuralism and deconstruction in order to argue that the concept of gender is created socially and culturally through discourse.[107] Postmodern feminists also emphasize the social construction of gender and the discursive nature of reality;[104] however, as Pamela Abbott et al. note, a postmodern approach to feminism highlights "the existence of multiple truths (rather than simply men and women's standpoints)".[108]		Riot grrls took an anti-corporate stance of self-sufficiency and self-reliance.[109] Riot grrrl's emphasis on universal female identity and separatism often appears more closely allied with second-wave feminism than with the third wave.[110] The movement encouraged and made "adolescent girls' standpoints central", allowing them to express themselves fully.[111] Lipstick feminism is a cultural feminist movement that attempts to respond to the backlash of second-wave radical feminism of the 1960s and 1970s by reclaiming symbols of "feminine" identity such as make-up, suggestive clothing and having a sexual allure as valid and empowering personal choices.[112][113]		According to 2015 poll, 18 percent of Americans consider themselves feminists, while 85 percent reported they believe in "equality for women". Despite the popular belief in equal rights, 52 percent did not identify as feminist, 26 percent were unsure, and four percent provided no response.[114]		According to 2014 Ipsos poll covering 15 developed countries, 53 percent of respondents identified as feminists, and 87% agreed that "women should be treated equally to men in all areas based on their competency, not their gender". However, only 55% of women agreed that they have "full equality with men and the freedom to reach their full dreams and aspirations".[115]		Among women, some of the strongest support for feminism was found in Sweden, where one in three (36%) agreed very much that they defined themselves as feminists. They were followed by women in Italy (31%) and Argentina (29%). Those in the middle of the ranking were from Great Britain (22%), Spain (22%), United States (20%), Australia (18%), Belgium (18%), France (18%), Canada (17%), Poland (17%), and Hungary (15%). Women least likely to agree very much were from Japan (8%), Germany (7%) and South Korea (7%).[115]		One quarter of men in Italy (25%) and Argentina (25%), and two in ten of those in Poland (21%) and France (19%), agreed very much they defined themselves as feminist. They were followed by those from Sweden (17%), Spain (16%), the United States (16%), Canada (15%), Great Britain (14%), Hungary (12%), Belgium (11%) and Australia (10%). Men least likely to identify this way were from South Korea (7%), Germany (3%) and Japan (3%).[115]		Women were more likely to self-identify as being feminists than men in every country except Poland, where men (21%) were four points more likely than women (17%) to agree very much with the statement. In South Korea, there was no difference between men and women (7%) on this measure.[115]		Feminist views on sexuality vary, and have differed by historical period and by cultural context. Feminist attitudes to female sexuality have taken a few different directions. Matters such as the sex industry, sexual representation in the media, and issues regarding consent to sex under conditions of male dominance have been particularly controversial among feminists. This debate has culminated in the late 1970s and the 1980s, in what came to be known as the feminist sex wars, which pitted anti-pornography feminism against sex-positive feminism, and parts of the feminist movement were deeply divided by these debates.[116][117][118][119][120] Feminists have taken a variety of positions on different aspects of the sexual revolution from the 1960s and 70s. Over the course of the 1970s, a large number of influential women accepted lesbian and bisexual women as part of feminism.[121]		Opinions on the sex industry are diverse. Feminists critical of the sex industry generally see it as the exploitative result of patriarchal social structures which reinforce sexual and cultural attitudes complicit in rape and sexual harassment. Alternately, feminists who support at least part of the sex industry argue that it can be a medium of feminist expression and a means for women to take control of their sexuality.		Feminist views of pornography range from condemnation of pornography as a form of violence against women, to an embracing of some forms of pornography as a medium of feminist expression.[116][117][118][119][120] Feminists' views on prostitution vary, but many of these perspectives can be loosely arranged into an overarching standpoint that is generally either critical or supportive of prostitution and sex work.[122]		For feminists, a woman's right to control her own sexuality is a key issue. Feminists such as Catharine MacKinnon argue that women have very little control over their own bodies, with female sexuality being largely controlled and defined by men in patriarchal societies. Feminists argue that sexual violence committed by men is often rooted in ideologies of male sexual entitlement, and that these systems grant women very few legitimate options to refuse sexual advances.[123][124] In many cultures, men do not believe that a woman has the right to reject a man's sexual advances or to make an autonomous decision about participating in sex. Feminists argue that all cultures are, in one way or another, dominated by ideologies that largely deny women the right to decide how to express their sexuality, because men under patriarchy feel entitled to define sex on their own terms. This entitlement can take different forms, depending on the culture. In many parts of the world, especially in conservative and religious cultures, marriage is regarded as an institution which requires a wife to be sexually available at all times, virtually without limit; thus, forcing or coercing sex on a wife is not considered a crime or even an abusive behaviour.[125][126] In more liberal cultures, this entitlement takes the form of a general sexualization of the whole culture. This is played out in the sexual objectification of women, with pornography and other forms of sexual entertainment creating the fantasy that all women exist solely for men's sexual pleasure, and that women are readily available and desiring to engage in sex at any time, with any man, on a man's terms.[127]		Sandra Harding says that the "moral and political insights of the women's movement have inspired social scientists and biologists to raise critical questions about the ways traditional researchers have explained gender, sex and relations within and between the social and natural worlds."[128] Some feminists, such as Ruth Hubbard and Evelyn Fox Keller, criticize traditional scientific discourse as being historically biased towards a male perspective.[129] A part of the feminist research agenda is the examination of the ways in which power inequities are created or reinforced in scientific and academic institutions.[130] Physicist Lisa Randall, appointed to a task force at Harvard by then-president Lawrence Summers after his controversial discussion of why women may be underrepresented in science and engineering, said, "I just want to see a whole bunch more women enter the field so these issues don't have to come up anymore."[131]		Lynn Hankinson Nelson notes that feminist empiricists find fundamental differences between the experiences of men and women. Thus, they seek to obtain knowledge through the examination of the experiences of women, and to "uncover the consequences of omitting, misdescribing, or devaluing them" to account for a range of human experience.[132] Another part of the feminist research agenda is the uncovering of ways in which power inequities are created or reinforced in society and in scientific and academic institutions.[130] Furthermore, despite calls for greater attention to be paid to structures of gender inequity in the academic literature, structural analyses of gender bias rarely appear in highly cited psychological journals, especially in the commonly studied areas of psychology and personality.[133]		One criticism of feminist epistemology is that it allows social and political values to influence its findings.[134] Susan Haack also points out that feminist epistemology reinforces traditional stereotypes about women's thinking (as intuitive and emotional, etc.); Meera Nanda further cautions that this may in fact trap women within "traditional gender roles and help justify patriarchy".[135]		Modern feminism challenges the essentialist view of gender as biologically intrinsic.[136][137] For example, Anne Fausto-Sterling's book, Myths of Gender, explores the assumptions embodied in scientific research that support a biologically essentialist view of gender.[138] In Delusions of Gender, Cordelia Fine disputes scientific evidence that suggests that there is an innate biological difference between men's and women's minds, asserting instead that cultural and societal beliefs are the reason for differences between individuals that are commonly perceived as sex differences.[139]		Feminism in psychology emerged as a critique of the dominant male outlook on psychological research where only male perspectives were studied with all male subjects. As women earned doctorates in psychology, females and their issues were introduced as legitimate topics of study. Feminist psychology emphasizes social context, lived experience, and qualitative analysis.[140] Projects such as Psychology's Feminist Voices have emerged to catalogue the influence of feminist psychologists on the discipline.[141]		Gender-based inquiries into and conceptualization of architecture have also come about, leading to feminism in modern architecture. Piyush Mathur coined the term "archigenderic". Claiming that "architectural planning has an inextricable link with the defining and regulation of gender roles, responsibilities, rights, and limitations", Mathur came up with that term "to explore ... the meaning of 'architecture' in terms of gender" and "to explore the meaning of 'gender' in terms of architecture".[142]		Feminist activists have established a range of feminist businesses, including women's bookstores, feminist credit unions, feminist presses, feminist mail-order catalogs, and feminist restaurants. These businesses flourished as part of the second and third-waves of feminism in the 1970s, 1980s, and 1990s.[143][144]		Corresponding with general developments within feminism, and often including such self-organizing tactics as the consciousness-raising group, the movement began in the 1960s and flourished throughout the 1970s.[145] Jeremy Strick, director of the Museum of Contemporary Art in Los Angeles, described the feminist art movement as "the most influential international movement of any during the postwar period", and Peggy Phelan says that it "brought about the most far-reaching transformations in both artmaking and art writing over the past four decades".[145] Feminist artist Judy Chicago, who created The Dinner Party, a set of vulva-themed ceramic plates in the 1970s, said in 2009 to ARTnews, "There is still an institutional lag and an insistence on a male Eurocentric narrative. We are trying to change the future: to get girls and boys to realize that women's art is not an exception—it's a normal part of art history."[146] A feminist approach to the visual arts has most recently developed through Cyberfeminism and the posthuman turn, giving voice to the ways "contemporary female artists are dealing with gender, social media and the notion of embodiment".[147]		The feminist movement produced both feminist fiction and non-fiction, and created new interest in women's writing. It also prompted a general reevaluation of women's historical and academic contributions in response to the belief that women's lives and contributions have been underrepresented as areas of scholarly interest.[148] Much of the early period of feminist literary scholarship was given over to the rediscovery and reclamation of texts written by women. Studies like Dale Spender's Mothers of the Novel (1986) and Jane Spencer's The Rise of the Woman Novelist (1986) were ground-breaking in their insistence that women have always been writing. Commensurate with this growth in scholarly interest, various presses began the task of reissuing long-out-of-print texts. Virago Press began to publish its large list of 19th and early-20th-century novels in 1975 and became one of the first commercial presses to join in the project of reclamation. In the 1980s Pandora Press, responsible for publishing Spender's study, issued a companion line of 18th-century novels written by women.[149] More recently, Broadview Press continues to issue 18th- and 19th-century novels, many hitherto out of print, and the University of Kentucky has a series of republications of early women's novels. A Vindication of the Rights of Woman (1792) by Mary Wollstonecraft, is one of the earliest works of feminist philosophy. A Room of One's Own (1929) by Virginia Woolf, is noted in its argument for both a literal and figural space for women writers within a literary tradition dominated by patriarchy.		The widespread interest in women's writing is related to a general reassessment and expansion of the literary canon. Interest in post-colonial literatures, gay and lesbian literature, writing by people of colour, working people's writing, and the cultural productions of other historically marginalized groups has resulted in a whole scale expansion of what is considered "literature", and genres hitherto not regarded as "literary", such as children's writing, journals, letters, travel writing, and many others are now the subjects of scholarly interest.[148][150][151] Most genres and subgenres have undergone a similar analysis, so that one now sees work on the "female gothic"[152] or women's science fiction.		According to Elyce Rae Helford, "Science fiction and fantasy serve as important vehicles for feminist thought, particularly as bridges between theory and practice."[153] Feminist science fiction is sometimes taught at the university level to explore the role of social constructs in understanding gender.[154] Notable texts of this kind are Ursula K. Le Guin's The Left Hand of Darkness (1969), Joanna Russ' The Female Man (1970), Octavia Butler's Kindred (1979) and Margaret Atwood's Handmaid's Tale (1985).		Women's music (or womyn's music or wimmin's music) is the music by women, for women, and about women.[155] The genre emerged as a musical expression of the second-wave feminist movement[156] as well as the labour, civil rights, and peace movements.[157] The movement was started by lesbians such as Cris Williamson, Meg Christian, and Margie Adam, African-American women activists such as Bernice Johnson Reagon and her group Sweet Honey in the Rock, and peace activist Holly Near.[157] Women's music also refers to the wider industry of women's music that goes beyond the performing artists to include studio musicians, producers, sound engineers, technicians, cover artists, distributors, promoters, and festival organizers who are also women.[155] Riot grrrl is an underground feminist hardcore punk movement described in the cultural movements section of this article.		Feminism became a principal concern of musicologists in the 1980s[158] as part of the New Musicology. Prior to this, in the 1970s, musicologists were beginning to discover women composers and performers, and had begun to review concepts of canon, genius, genre and periodization from a feminist perspective. In other words, the question of how women musicians fit into traditional music history was now being asked.[158] Through the 1980s and 1990s, this trend continued as musicologists like Susan McClary, Marcia Citron and Ruth Solie began to consider the cultural reasons for the marginalizing of women from the received body of work. Concepts such as music as gendered discourse; professionalism; reception of women's music; examination of the sites of music production; relative wealth and education of women; popular music studies in relation to women's identity; patriarchal ideas in music analysis; and notions of gender and difference are among the themes examined during this time.[158]		While the music industry has long been open to having women in performance or entertainment roles, women are much less likely to have positions of authority, such as being the leader of an orchestra.[159] In popular music, while there are many women singers recording songs, there are very few women behind the audio console acting as music producers, the individuals who direct and manage the recording process.[160]		Feminist cinema, advocating or illustrating feminist perspectives, arose largely with the development of feminist film theory in the late '60s and early '70s. Women who were radicalized during the 1960s by political debate and sexual liberation; but the failure of radicalism to produce substantive change for women galvanized them to form consciousness-raising groups and set about analysing, from different perspectives, dominant cinema's construction of women.[161] Differences were particularly marked between feminists on either side of the Atlantic. 1972 saw the first feminist film festivals in the U.S. and U.K. as well as the first feminist film journal, Women and Film. Trailblazers from this period included Claire Johnston and Laura Mulvey, who also organized the Women's Event at the Edinburgh Film Festival.[162] Other theorists making a powerful impact on feminist film include Teresa de Lauretis, Anneke Smelik and Kaja Silverman. Approaches in philosophy and psychoanalysis fuelled feminist film criticism, feminist independent film and feminist distribution.		It has been argued that there are two distinct approaches to independent, theoretically inspired feminist filmmaking. 'Deconstruction' concerns itself with analysing and breaking down codes of mainstream cinema, aiming to create a different relationship between the spectator and dominant cinema. The second approach, a feminist counterculture, embodies feminine writing to investigate a specifically feminine cinematic language.[163] Some recent criticism[164] of "feminist film" approaches has centred around a Swedish rating system called the Bechdel test.		During the 1930s–1950s heyday of the big Hollywood studios, the status of women in the industry was abysmal[165] and, while much has improved, many would argue that there is still much to be done. From art films by Sally Potter, Catherine Breillat, Claire Denis and Jane Campion to action movies by Kathryn Bigelow, women now have a stronger voice, but are only too aware of the still lingering gender gap.[166]		Feminism had complex interactions with the major political movements of the twentieth century.		Since the late nineteenth century some feminists have allied with socialism, whereas others have criticized socialist ideology for being insufficiently concerned about women's rights. August Bebel, an early activist of the German Social Democratic Party (SPD), published his work Die Frau und der Sozialismus, juxtaposing the struggle for equal rights between sexes with social equality in general. In 1907 there was an International Conference of Socialist Women in Stuttgart where suffrage was described as a tool of class struggle. Clara Zetkin of the SPD called for women's suffrage to build a "socialist order, the only one that allows for a radical solution to the women's question".[167][168]		In Britain, the women's movement was allied with the Labour party. In the U.S., Betty Friedan emerged from a radical background to take leadership. Radical Women is the oldest socialist feminist organization in the U.S. and is still active.[169] During the Spanish Civil War, Dolores Ibárruri (La Pasionaria) led the Communist Party of Spain. Although she supported equal rights for women, she opposed women fighting on the front and clashed with the anarcha-feminist Mujeres Libres.[170]		Fascism has been prescribed dubious stances on feminism by its practitioners and by women's groups. Amongst other demands concerning social reform presented in the Fascist manifesto in 1919 was expanding the suffrage to all Italian citizens of age 18 and above, including women (accomplished only in 1946, after the defeat of fascism) and eligibility for all to stand for office from age 25. This demand was particularly championed by special Fascist women's auxiliary groups such as the fasci femminilli and only partly realized in 1925, under pressure from Prime Minister Benito Mussolini's more conservative coalition partners.[171][172]		Cyprian Blamires states that although feminists were among those who opposed the rise of Adolf Hitler, feminism has a complicated relationship with the Nazi movement as well. While Nazis glorified traditional notions of patriarchal society and its role for women, they claimed to recognize women's equality in employment.[173] However, Hitler and Mussolini declared themselves as opposed to feminism,[173] and after the rise of Nazism in Germany in 1933, there was a rapid dissolution of the political rights and economic opportunities that feminists had fought for during the pre-war period and to some extent during the 1920s.[168] Georges Duby et al. note that in practice fascist society was hierarchical and emphasized male virility, with women maintaining a largely subordinate position.[168] Blamires also notes that Neofascism has since the 1960s been hostile towards feminism and advocates that women accept "their traditional roles".[173]		The civil rights movement has influenced and informed the feminist movement and vice versa. Many Western feminists adapted the language and theories of black equality activism and drew parallels between women's rights and the rights of non-white people.[174] Despite the connections between the women's and civil rights movements, some tension arose during the late 1960s and early 1970s as non-white women argued that feminism was predominantly white and middle class, and did not understand and was not concerned with race issues.[175] Similarly, some women argued that the civil rights movement had sexist elements and did not adequately address minority women's concerns.[174] These criticisms created new feminist social theories about the intersections of racism, classism, and sexism, and new feminisms, such as black feminism and Chicana feminism.[176][177]		Neoliberalism has been criticized by feminist theory for having a negative effect on the female workforce population across the globe, especially in the global south. Masculinist assumptions and objectives continue to dominate economic and geopolitical thinking.[178]:177 Women's experiences in non-industrialized countries reveal often deleterious effects of modernization policies and undercut orthodox claims that development benefits everyone.[178]:175		Proponents of neoliberalism have theorized that by increasing women's participation in the workforce, there will be heightened economic progress, but feminist critics have noted that this participation alone does not further equality in gender relations.[179]:186–98 Neoliberalism has failed to address significant problems such as the devaluation of feminized labour, the structural privileging of men and masculinity, and the politicization of women's subordination in the family and the workplace.[178]:176 The "feminization of employment" refers to a conceptual characterization of deteriorated and devalorized labour conditions that are less desirable, meaningful, safe and secure.[178]:179 Employers in the global south have perceptions about feminine labour and seek workers who are perceived to be undemanding, docile and willing to accept low wages.[178]:180 Social constructs about feminized labour have played a big part in this, for instance, employers often perpetuate ideas about women as 'secondary income earners to justify their lower rates of pay and not deserving of training or promotion.[179]:189		The feminist movement has effected change in Western society, including women's suffrage; greater access to education; more nearly equitable pay with men; the right to initiate divorce proceedings; the right of women to make individual decisions regarding pregnancy (including access to contraceptives and abortion); and the right to own property.[5]		From the 1960s on, the campaign for women's rights[180] was met with mixed results[181] in the U.S. and the U.K. Other countries of the EEC agreed to ensure that discriminatory laws would be phased out across the European Community.		Some feminist campaigning also helped reform attitudes to child sexual abuse. The view that young girls cause men to have sexual intercourse with them was replaced by that of men's responsibility for their own conduct, the men being adults.[182]		In the U.S., the National Organization for Women (NOW) began in 1966 to seek women's equality, including through the Equal Rights Amendment (ERA),[183] which did not pass, although some states enacted their own. Reproductive rights in the U.S. centred on the court decision in Roe v. Wade enunciating a woman's right to choose whether to carry a pregnancy to term. Western women gained more reliable birth control, allowing family planning and careers. The movement started in the 1910s in the U.S. under Margaret Sanger and elsewhere under Marie Stopes. In the final three decades of the 20th century, Western women knew a new freedom through birth control, which enabled women to plan their adult lives, often making way for both career and family.[184]		The division of labour within households was affected by the increased entry of women into workplaces in the 20th century. Sociologist Arlie Russell Hochschild found that, in two-career couples, men and women, on average, spend about equal amounts of time working, but women still spend more time on housework,[185][186] although Cathy Young responded by arguing that women may prevent equal participation by men in housework and parenting.[187] Judith K. Brown writes, "Women are most likely to make a substantial contribution when subsistence activities have the following characteristics: the participant is not obliged to be far from home; the tasks are relatively monotonous and do not require rapt concentration; and the work is not dangerous, can be performed in spite of interruptions, and is easily resumed once interrupted."[188]		In international law, the Convention on the Elimination of All Forms of Discrimination Against Women (CEDAW) is an international convention adopted by the United Nations General Assembly and described as an international bill of rights for women. It came into force in those nations ratifying it.[189]		Feminist jurisprudence is a branch of jurisprudence that examines the relationship between women and law. It addresses questions about the history of legal and social biases against women and about the enhancement of their legal rights.[190]		Feminist jurisprudence signifies a reaction to the philosophical approach of modern legal scholars, who typically see law as a process for interpreting and perpetuating a society's universal, gender-neutral ideals. Feminist legal scholars claim that this fails to acknowledge women's values or legal interests or the harms that they may anticipate or experience.[191]		Proponents of gender-neutral language argue that the use of gender-specific language often implies male superiority or reflects an unequal state of society.[192] According to The Handbook of English Linguistics, generic masculine pronouns and gender-specific job titles are instances "where English linguistic convention has historically treated men as prototypical of the human species."[193]		Feminist theology is a movement that reconsiders the traditions, practices, scriptures, and theologies of religions from a feminist perspective. Some of the goals of feminist theology include increasing the role of women among the clergy and religious authorities, reinterpreting male-dominated imagery and language about God, determining women's place in relation to career and motherhood, and studying images of women in the religion's sacred texts.[194]		Christian feminism is a branch of feminist theology which seeks to interpret and understand Christianity in light of the equality of women and men, and that this interpretation is necessary for a complete understanding of Christianity. While there is no standard set of beliefs among Christian feminists, most agree that God does not discriminate on the basis of sex, and are involved in issues such as the ordination of women, male dominance and the balance of parenting in Christian marriage, claims of moral deficiency and inferiority of women compared to men, and the overall treatment of women in the church.[195][196] The Christian Bible refers to women in positions of authority in Judges 4:4 and Kings 22:14.[197][198][non-primary source needed]		Islamic feminists advocate women's rights, gender equality, and social justice grounded within an Islamic framework. Advocates seek to highlight the deeply rooted teachings of equality in the Quran and encourage a questioning of the patriarchal interpretation of Islamic teaching through the Quran, hadith (sayings of Muhammad), and sharia (law) towards the creation of a more equal and just society.[199] Although rooted in Islam, the movement's pioneers have also utilized secular and Western feminist discourses and recognize the role of Islamic feminism as part of an integrated global feminist movement.[200]		Buddhist feminism is a movement that seeks to improve the religious, legal, and social status of women within Buddhism. It is an aspect of feminist theology which seeks to advance and understand the equality of men and women morally, socially, spiritually, and in leadership from a Buddhist perspective. The Buddhist feminist Rita Gross describes Buddhist feminism as "the radical practice of the co-humanity of women and men."[201]		Jewish feminism is a movement that seeks to improve the religious, legal, and social status of women within Judaism and to open up new opportunities for religious experience and leadership for Jewish women. The main issues for early Jewish feminists in these movements were the exclusion from the all-male prayer group or minyan, the exemption from positive time-bound mitzvot, and women's inability to function as witnesses and to initiate divorce.[202] Many Jewish women have become leaders of feminist movements throughout their history.[203]		Dianic Wicca is a feminist-centred thealogy.[204]		Secular or atheist feminists have engaged in feminist criticism of religion, arguing that many religions have oppressive rules towards women and misogynistic themes and elements in religious texts.[205][206][207]		Patriarchy is a social system in which society is organized around male authority figures. In this system fathers have authority over women, children, and property. It implies the institutions of male rule and privilege, and is dependent on female subordination.[208] Most forms of feminism characterize patriarchy as an unjust social system that is oppressive to women. Carole Pateman argues that the patriarchal distinction "between masculinity and femininity is the political difference between freedom and subjection."[209] In feminist theory the concept of patriarchy often includes all the social mechanisms that reproduce and exert male dominance over women. Feminist theory typically characterizes patriarchy as a social construction, which can be overcome by revealing and critically analyzing its manifestations.[210] Some radical feminists have proposed that because patriarchy is too deeply rooted in society, separatism is the only viable solution.[211] Other feminists have criticized these views as being anti-men.[212][213][214]		Feminist theory has explored the social construction of masculinity and its implications for the goal of gender equality. The social construct of masculinity is seen by feminism as problematic because it associates males with aggression and competition, and reinforces patriarchal and unequal gender relations.[68][215] Patriarchal cultures are criticized for "limiting forms of masculinity" available to men and thus narrowing their life choices.[216] Some feminists are engaged with men's issues activism, such as bringing attention to male rape and spousal battery and addressing negative social expectations for men.[217][218][219]		Male participation in feminism is encouraged by feminists and is seen as an important strategy for achieving full societal commitment to gender equality.[6][220][221] Many male feminists and pro-feminists are active in both women's rights activism, feminist theory, and masculinity studies. However, some argue that while male engagement with feminism is necessary, it is problematic because of the ingrained social influences of patriarchy in gender relations.[222] The consensus today in feminist and masculinity theories is that both genders can and should cooperate to achieve the larger goals of feminism.[216] It has been proposed that, in large part, this can be achieved through considerations of women's agency.[223]		Different groups of people have responded to feminism, and both men and women have been among its supporters and critics. Among American university students, for both men and women, support for feminist ideas is more common than self-identification as a feminist.[224][225][226] The US media tends to portray feminism negatively and feminists "are less often associated with day-to-day work/leisure activities of regular women."[227][228] However, as recent research has demonstrated, as people are exposed to self-identified feminists and to discussions relating to various forms of feminism, their own self-identification with feminism increases.[229] Roy Baumeister has criticized feminists who "look only at the top of society and draw conclusions about society as a whole. Yes, there are mostly men at the top. But if you look at the bottom, really at the bottom, you'll find mostly men there, too."[230]		Pro-feminism is the support of feminism without implying that the supporter is a member of the feminist movement. The term is most often used in reference to men who are actively supportive of feminism. The activities of pro-feminist men's groups include anti-violence work with boys and young men in schools, offering sexual harassment workshops in workplaces, running community education campaigns, and counselling male perpetrators of violence. Pro-feminist men also may be involved in men's health, activism against pornography including anti-pornography legislation, men's studies, and the development of gender equity curricula in schools. This work is sometimes in collaboration with feminists and women's services, such as domestic violence and rape crisis centres.[231][232]				Anti-feminism is opposition to feminism in some or all of its forms.[233]		In the nineteenth century, anti-feminism was mainly focused on opposition to women's suffrage. Later, opponents of women's entry into institutions of higher learning argued that education was too great a physical burden on women. Other anti-feminists opposed women's entry into the labour force, or their right to join unions, to sit on juries, or to obtain birth control and control of their sexuality.[234]		Some people have opposed feminism on the grounds that they believe it is contrary to traditional values or religious beliefs. These anti-feminists argue, for example, that social acceptance of divorce and non-married women is wrong and harmful, and that men and women are fundamentally different and thus their different traditional roles in society should be maintained.[235][236][237] Other anti-feminists oppose women's entry into the workforce, political office, and the voting process, as well as the lessening of male authority in families.[238][239]		Writers such as Camille Paglia, Christina Hoff Sommers, Jean Bethke Elshtain, Elizabeth Fox-Genovese, Lisa Lucile Owens[240] and Daphne Patai oppose some forms of feminism, though they identify as feminists. They argue, for example, that feminism often promotes misandry and the elevation of women's interests above men's, and criticize radical feminist positions as harmful to both men and women.[241] Daphne Patai and Noretta Koertge argue that the term "anti-feminist" is used to silence academic debate about feminism.[242][243] Lisa Lucile Owens argues that certain rights extended exclusively to women are patriarchal because they relieve women from exercising a crucial aspect of their moral agency.[223]		
An academic major is the academic discipline to which an undergraduate student formally commits. A student who successfully completes all courses required for the major qualifies for an undergraduate degree. The word "major" is also sometimes used administratively to refer to the academic discipline pursued by a graduate student or postgraduate student in a master's or doctoral program.		An academic major typically requires completion of a combination of prescribed and elective courses in the chosen discipline. In addition, most colleges and universities require that all students take a general core curriculum in the liberal arts. The latitude a student has in choosing courses varies from program to program.[1] An academic major is administered by select faculty in an academic department. A major administered by more than one academic department is called an interdisciplinary major. In some settings, students may be permitted to design their own major, subject to faculty approval.		In the US, students are usually not required to choose their major discipline when first enrolling as an undergraduate. Normally students are required to commit by the end of their second academic year at latest, and some schools even disallow students from declaring a major until this time.		A student who declares two academic majors is said to have a double major. A coordinate major is an ancillary major designed to complement the primary one. A coordinate major requires fewer course credits to complete.						US universities began to encourage concentrated foci at the undergraduate level in the second half of the 19th century. The term "major" first appeared in the 1877 Johns Hopkins University course catalogue. At that time,he major generally required two years of study, while a minor concentration required only one.[2] Abbott Lawrence Lowell introduced the academic major system to Harvard University in 1910, during his presidency there. It required students to complete courses not only in a specialized discipline, but also in other subjects.[1] Variations of this system are now definitive among the great majority of tertiary education institutions in the United States and Canada.		Many labor economics studies report that employment and earnings vary by college major and this appears to be caused by differences in the labor market value of the skills taught in different majors.[3] Majors also have different labor market value even after students complete graduate degrees such as law degrees or business degrees.[4]		The roots of the academic major as we now know it first surfaced in the 19th century as "alternative components of the undergraduate degree".[5] Before that, all students receiving an undergraduate degree would be required to study the same slate of courses geared at a comprehensive "liberal education".[5]		In 1825, the University of Virginia initiated an educational approach that would allow students to choose, from eight options, an area of focus (ex: ancient languages, anatomy, medicine) and higher educational systems in Europe after the American civil war developed further into a stricter specialization approach to studies.[5]		In the United States, in the second half of the 19th century, concentrated foci at the undergraduate level began to prosper and popularize, but the familiar term "major" did not appear until 1877 in a Johns Hopkins University catalogue. The major generally required 2 years of study. The minor, required one.		From 1880 to 1910, Baccalaureate granting American institutions vastly embraced a free-elective system, where students were endowed with a greater freedom to explore intellectual curiosities.		The 1930s witnessed the appearance of first interdisciplinary major: American studies. Culture was the grounding concept and orchestrating principle for its courses.[5] 1960s to 1970s experienced a new tide of interdisciplinary majors and a relaxation of curriculum and graduation requirements. (Civil Rights Movement spawned Women’s studies and Black Studies, for example.) [6] In the 1980s and 1990s, "interdisciplinary studies, multiculturalism, feminist pedagogy, and a renewed concern for the coherence and direction of the undergraduate program began to assail the Baccalaureate degree dominated by the academic major."[5]		The academic major is considered a defining and dominant characteristic of the undergraduate degree. "The ascendancy of the disciplines in the late nineteenth century and their continuing dominance throughout the twentieth century have left an indelible imprint on the shape and direction of the academic major" and research affirms that the academic major is the strongest and clearest curricular link to gains in student learning.[5] While general education is considered to be the breadth component of an undergraduate education, the major is commonly deemed as the depth aspect.[5]		Through its development, scholars, academics, and educators have disagreed on the purpose and nature of the undergraduate major. Generally, proponents of the major and departmental system "argue that they enable an academic community to foster the development, conservation and diffusion of knowledge." While critics "claim that they promote intellectual tribalism, where specialization receives favor over the mastery of multiple epistemologies, where broader values of liberal learning and of campus unity are lost, and where innovation is inhibited due to parochial opposition to new subspecialties and research methods."[5]		In many universities, an academic concentration is a focus within a specific academic major, that is a field of study within a specific academic major. For example, interdisciplinary programs in humanities or social sciences will require a student to pick a specific academic concentration as a focus within his academic major, such as an academic major in Interdisciplinary Humanities with an academic concentration in Film or an academic major in Interdisciplinary Social Sciences with an academic concentration in Geography. At several art schools and liberal arts colleges, an academic concentration serves a similar function to an academic minor at other universities, that is an academic discipline outside of the student's academic major in which he or she takes a small number of classes. At Brown University and Harvard University, concentrations serve the same function as minors at other institutions.		At the doctoral studies level, an academic major or major field refers to a student's primary focus within their degree program while a minor or minor field refers to his or her secondary focus. For example, a doctoral student studying History might pursue their degree in History with a major field in War and Society and a minor field in Postcolonial Studies.		An impacted major is a major for which more students apply for than the school can accommodate. It is a classic example of when Demand exceeds Supply. When this occurs, the major becomes impacted and therefore is susceptible to higher standards of admission.		For example: Lets assume there's a school whose minimum requirements are SATs of 1100 and a GPA of 3.0.		If a person applies to an impacted major, the school can raise the minimum requirements as much as needed in order to weed out the students it is unable to accommodate.		In some cases, it is a better idea to apply to a school as "Undeclared".		For example:		If the school implements requirements of SATs of 1300 and a GPA of 3.4 for the impacted major, it would be better to apply as "Undeclared" if the student only meets the minimum requirements. The student will then have a better chance of being accepted and generally has the option of changing his/her major at a later date.		
Censorship of student media pertains to the suppression of free speech by school administrative bodies of student-run news operations. Typically this involves interfering with the operation and final publishing authority of a school newspaper, radio, television or other electronic online content generated by students.		While this has typically consisted of schools enforcing their authority to control the funding and distribution of publications, sometimes forms of censorship extend to expression not funded by or under the official auspices of the school system or college (for example, confiscating independently produced "underground" publications or imposing discipline for material posted on off-campus websites).		The states of Arkansas, California, Colorado, Illinois, Iowa, Kansas, Massachusetts and Oregon have all passed legislation fortifying student journalists' right to free expression.[1] The nonprofit Student Press Law Center tracks and provides pro-bono legal aid to student-run media organizations in the U.S.		Some notable cases in the United States include:				
The United States service academies, also known as the United States military academies, are federal academies for the undergraduate education and training of commissioned officers for the United States Armed Forces.		There are five U.S. service academies:						Service academies can be used to refer to all of the academies collectively. However, in popular usage, this term is more often used for the academies of three branches of the military: those of the Army, Navy, and Air Force (under the Department of Defense); and that of the Coast Guard (under the Department of Homeland Security). These are the only four academies whose students are on active duty in the Armed Forces of the United States from the day they enter the Academy, with the rank of officer cadet or midshipman, and subject to the Uniform Code of Military Justice. Students at these academies cannot, however, count this active duty time towards military active/retired pay and allowances nor can they claim the time for years of service for retirement. In the case of the Merchant Marine Academy, midshipmen may elect to receive an active duty or reserves commission in any branch of the uniformed services, including NOAA and the U.S. Public Health Service, most are commissioned into the Navy Reserve, Strategic Sealift Officer Program.		In the context of college football, the term "service academies" most often refers specifically to the grouping of Army, Navy, and Air Force, the three academies whose football teams compete in the top-level NCAA Division I FBS. The three schools compete annually for the Commander-in-Chief's Trophy. Coast Guard and Merchant Marine compete at the NCAA Division III level and play each other annually for the Secretaries Cup (formerly Secretary's Cup when both academies were under the Department of Transportation).		The United States Coast Guard, and therefore the Coast Guard Academy, is part of the United States Armed Forces,[1] albeit under the Department of Homeland Security, but in time of war it can be placed under the Department of the Navy.		Applicants to all service academies, except the United States Coast Guard Academy, are required to obtain a nomination to the schools. Nominations may be made by Congressional Representatives, Senators, the Vice President and the President. Applicants to the Coast Guard Academy compete in a direct nationwide competitive process that has no by-state quotas.		The admissions process to the US service academies is an extensive and very competitive process. The US Military Academy at West Point, the US Naval Academy at Annapolis, and the US Air Force Academy at Colorado Springs all require an applicant to submit an on-line file and proceed through pre-candidate qualification before an application is provided. The US Merchant Marine Academy at Kings Point New York requires an applicant to submit part 1 of the 3 part application prior to receiving a nomination. All these schools have an extremely competitive application process and are ranked annually by U.S. News & World Report and Forbes.com as some of the most selective colleges and universities in America. The average acceptance rate is between 8-17% for each of the schools.		Students at the United States Military Academy, the United States Air Force Academy, and the United States Coast Guard Academy are cadets. Students at the United States Naval Academy and the United States Merchant Marine Academy are midshipmen. All cadets and midshipmen receive taxable pay at a rate of 35% of O1 under 2 years of service (which can be used to pay for textbooks and uniforms), free room and board, and pay no tuition or fees, with the exception of USMMA who receive taxable pay at $1006.80 a month only during their required 300+ days at sea during their 4-year studies.		Upon graduation and the receipt of a Bachelor of Science degree, the former students become second lieutenants or ensigns and must serve a minimum term of duty, usually five years plus another three years in the Reserves. If the student's chosen occupation requires particularly extensive training (such as aviation or Special Operations), the service commitment may be longer.		With respect to the Merchant Marine Academy, midshipmen repay their service obligations through a variety of methods depending on their selected career path. On average, about one third of the graduating class each year will actively sail on their Coast Guard License as either Unlimited Third Mates or Third Assistant Engineers in the United States Merchant Marine, about one third will go to work in the civilian maritime industry ashore, and the remaining one third will enter active duty military service. A midshipman who enters active duty military service will typically assume a service obligation similar to those of cadets and midshipmen entering the military services from their respective service academies (i.e. a Merchant Marine midshipman entering the US Marine Corps would assume a similar obligation to a midshipman from the Naval Academy entering the Marine Corps). Merchant Marine midshipmen not entering active duty typically assume an eight-year obligation to the Naval Reserve Strategic Sealift Officer Program, unless they have elected to enter another reserve branch of the armed forces. In addition, midshipmen who do not see service on active duty are restricted from working outside the maritime industry or merchant marine for a period of five years following graduation and must seek annual MARAD approval for their employment.		Senior Military Colleges must maintain military standards similar to those of the federal service academies. These are:		There are also Military Junior Colleges which must maintain military standards similar to those of the federal service academies. These are:		The United States federal government also runs the Uniformed Services University of the Health Sciences, a post-graduate institution for the training of doctors, nurses, and other medical professionals for the military and uniformed services. Additionally each of the services also operate a number of other graduate schools, granting master's and in some cases doctoral degrees. These schools include:		Every commissioned officer in the United States armed forces is expected to have a post-graduate degree and Joint Professional Military Education prior to promotion to lieutenant colonel or commander. One more institution that does not fit neatly is the National Defense Intelligence College which is run by the Defense Intelligence Agency (DIA) for the benefit of the United States Intelligence Community. It grants both bachelors and masters degrees.		The DoD also runs three schools for the training of lawyers within the military services (i.e., judge advocates). The Judge Advocate General's Legal Center and School serves the Army; the Naval Justice School collectively serves the Navy, Marine Corps, and Coast Guard; and the Air Force Judge Advocate General School serves the Air Force. Of these, only the Army school actually awards a degree. It operates a special graduate course of study for lawyers in all of the services, known as the Judge Advocate Officer Graduate Course. This program is accredited by the American Bar Association to grant the Master of Laws to its graduates.		These schools provide for strengthening of academic potential of candidates to each of the above-described United States service academies. Admission is restricted to those students who have applied to an academy, failed initially to qualify, either academically or physically, but who have demonstrated an ability to qualify during the initial admission selection process:		
Animal rights is the idea in which some, or all, non-human animals are entitled to the possession of their own lives and that their most basic interests—such as the need to avoid suffering—should be afforded the same consideration as similar interests of human beings.[2]		Advocates oppose the assignment of moral value and fundamental protections on the basis of species membership alone—an idea known since 1970 as speciesism, when the term was coined by Richard D. Ryder—arguing that it is a prejudice as irrational as any other.[3] They maintain that animals should no longer be viewed as property or used as food, clothing, research subjects, entertainment, or beasts of burden.[4] Multiple cultural traditions around the world—such as Animism, Taoism, Hinduism, Buddhism, and Jainism—also espouse some forms of animal rights.		In parallel to the debate about moral rights, animal law is now widely taught in law schools in North America, and several prominent legal scholars[who?] support the extension of basic legal rights and personhood to at least some animals. The animals most often considered in arguments for personhood are bonobos and chimpanzees. This is supported by some animal rights academics because it would break through the species barrier, but opposed by others because it predicates moral value on mental complexity, rather than on sentience alone.[5]		Critics of animal rights argue that animals are unable to enter into a social contract, and thus cannot be possessors of rights, a view summed up by the philosopher Roger Scruton, who writes that only humans have duties, and therefore only humans have rights.[6] A parallel argument, known as the utilitarian position, is that animals may be used as resources so long as there is no unnecessary suffering; they may have some moral standing, but they are inferior in status to human beings, and insofar as they have interests, those interests may be overridden, though what counts as necessary suffering or a legitimate sacrifice of interests varies considerably.[7] Certain forms of animal rights activism, such as the destruction of fur farms and animal laboratories by the Animal Liberation Front, have also attracted criticism, including from within the animal rights movement itself,[8] as well as prompted reaction from the U.S. Congress with the enactment of the "Animal Enterprise Protection Act" (amended in 2006 by the Animal Enterprise Terrorism Act).[9]						Aristotle argued that animals lacked reason (logos), and placed humans at the top of the natural world, yet the respect for animals in ancient Greece was very high. Some animals were considered divine e.g.:dolphins. The 21st-century debates about animals can be traced back to the ancient world, and the idea of a divine hierarchy. In the Book of Genesis 1:26 (5th or 6th century BCE), Adam is given "dominion over the fish of the sea, and over the fowl of the air, and over the cattle, and over all the earth, and over every creeping thing that creepeth upon the earth." Dominion need not entail property rights, but it has been interpreted, by some, over the centuries to imply ownership.[11]		Contemporary philosopher Bernard Rollin writes that "dominion does not entail or allow abuse any more than does dominion a parent enjoys over a child."[12] Rollin further states that the Biblical Sabbath requirement promulgated in the Ten Commandments "required that animals be granted a day of rest along with humans. Correlatively, the Bible forbids 'plowing with an ox and an ass together' (Deut. 22:10–11). According to the rabbinical tradition, this prohibition stems from the hardship that an ass would suffer by being compelled to keep up with an ox, which is, of course, far more powerful. Similarly, one finds the prohibition against 'muzzling an ox when it treads out the grain' (Deut. 25:4–5), and even an environmental prohibition against destroying trees when besieging a city (Deut. 20:19–20). These ancient regulations, virtually forgotten, bespeak of an eloquent awareness of the status of animals as ends in themselves", a point also corroborated by Norm Phelps.[12][13]		The philosopher and mathematician, Pythagoras (c. 580–c. 500 BCE), urged respect for animals, believing that human and nonhuman souls were reincarnated from human to animal, and vice versa.[14] Against this, Aristotle (384–322 BCE), student to the philosopher Plato, argued that nonhuman animals had no interests of their own, ranking them far below humans in the Great Chain of Being. He was the first to create a taxonomy of animals; he perceived some similarities between humans and other species, but argued for the most part that animals lacked reason (logos), reasoning (logismos), thought (dianoia, nous), and belief (doxa).[10]		Theophrastus (c. 371 – c. 287 BCE), one of Aristotle's pupils, argued that animals also had reasoning (logismos), he opposed eating meat on the grounds that it robbed them of life and was therefore unjust.[15][16] Theophrastus did not prevail; Richard Sorabji writes that current attitudes to animals can be traced to the heirs of the Western Christian tradition selecting the hierarchy that Aristotle sought to preserve.[10]		Plutarch (1 C. A.D.) in his Life of Cato the Elder comments that while law and justice are applicable strictly to men only, beneficence and charity towards beasts is characteristic of a gentle heart. This is intended as a correction and advance over the merely utilitarian treatment of animals and slaves by Cato himself.[17]		Tom Beauchamp (2011) writes that the most extensive account in antiquity of how animals should be treated was written by the Neoplatonist philosopher Porphyry (234–c. 305 CE), in his On Abstinence from Animal Food, and On Abstinence from Killing Animals.[18]		According to Richard D. Ryder, the first known animal protection legislation in Europe was passed in Ireland in 1635. It prohibited pulling wool off sheep, and the attaching of ploughs to horses' tails, referring to "the cruelty used to beasts."[19] In 1641 the first legal code to protect domestic animals in North America was passed by the Massachusetts Bay Colony.[20] The colony's constitution was based on The Body of Liberties by the Reverend Nathaniel Ward (1578–1652), an English lawyer, Puritan clergyman, and University of Cambridge graduate. Ward's list of "rites" included rite 92: "No man shall exercise any Tirrany or Crueltie toward any brute Creature which are usually kept for man's use." Historian Roderick Nash (1989) writes that, at the height of René Descartes' influence in Europe—and his view that animals were simply automata—it is significant that the New Englanders created a law that implied animals were not unfeeling machines.[21]		The Puritans passed animal protection legislation in England too. Kathleen Kete writes that animal welfare laws were passed in 1654 as part of the ordinances of the Protectorate—the government under Oliver Cromwell (1599–1658), which lasted from 1653 to 1659, following the English Civil War. Cromwell disliked blood sports, which included cockfighting, cock throwing, dog fighting, bull baiting and bull running, said to tenderize the meat. These could be seen in villages and fairgrounds, and became associated with idleness, drunkenness, and gambling. Kete writes that the Puritans interpreted the biblical dominion of man over animals to mean responsible stewardship, rather than ownership. The opposition to blood sports became part of what was seen as Puritan interference in people's lives, and the animal protection laws were overturned during the Restoration, when Charles II was returned to the throne in 1660.[22]		The great influence of the 17th century was the French philosopher, René Descartes (1596–1650), whose Meditations (1641) informed attitudes about animals well into the 20th century.[24] Writing during the scientific revolution, Descartes proposed a mechanistic theory of the universe, the aim of which was to show that the world could be mapped out without allusion to subjective experience.[25]		There are barbarians who seize this dog, who so greatly surpasses man in fidelity and friendship, and nail him down to a table and dissect him alive, to show you the mesaraic veins! You discover in him all the same organs of feeling as in yourself. Answer me, mechanist, has Nature arranged all the springs of feeling in this animal to the end that he might not feel? — Voltaire (1694–1778)[26]		His mechanistic approach was extended to the issue of animal consciousness. Mind, for Descartes, was a thing apart from the physical universe, a separate substance, linking human beings to the mind of God. The nonhuman, on the other hand, were for Descartes nothing but complex automata, with no souls, minds, or reason.[24]		Against Descartes, the British philosopher John Locke (1632–1704) argued, in Some Thoughts Concerning Education (1693), that animals did have feelings, and that unnecessary cruelty toward them was morally wrong, but that the right not to be harmed adhered either to the animal's owner, or to the human being who was being damaged by being cruel. Discussing the importance of preventing children from tormenting animals, he wrote: "For the custom of tormenting and killing of beasts will, by degrees, harden their minds even towards men."[27]		Locke's position echoed that of Thomas Aquinas (1225–1274). Paul Waldau writes that the argument can be found at 1 Corinthians (9:9–10), when Paul asks: "Is it for oxen that God is concerned? Does he not speak entirely for our sake? It was written for our sake." Christian philosophers interpreted this to mean that humans had no direct duty to nonhuman animals, but had a duty only to protect them from the effects of engaging in cruelty.[28]		The German philosopher Immanuel Kant (1724–1804), following Aquinas, opposed the idea that humans have direct duties toward nonhumans. For Kant, cruelty to animals was wrong only because it was bad for humankind. He argued in 1785 that "cruelty to animals is contrary to man's duty to himself, because it deadens in him the feeling of sympathy for their sufferings, and thus a natural tendency that is very useful to morality in relation to other human beings is weakened."[29]		Jean-Jacques Rousseau (1712–1778) argued in Discourse on Inequality (1754) for the inclusion of animals in natural law on the grounds of sentience: "By this method also we put an end to the time-honored disputes concerning the participation of animals in natural law: for it is clear that, being destitute of intelligence and liberty, they cannot recognize that law; as they partake, however, in some measure of our nature, in consequence of the sensibility with which they are endowed, they ought to partake of natural right; so that mankind is subjected to a kind of obligation even toward the brutes. It appears, in fact, that if I am bound to do no injury to my fellow-creatures, this is less because they are rational than because they are sentient beings: and this quality, being common both to men and beasts, ought to entitle the latter at least to the privilege of not being wantonly ill-treated by the former."[30]		In his treatise on education, Emile, or On Education (1762), he encouraged parents to raise their children on a vegetarian diet. He believed that the food of the culture a child was raised eating, played an important role in the character and disposition they would develop as adults. "For however one tries to explain the practice, it is certain that great meat-eaters are usually more cruel and ferocious than other men. This has been recognized at all times and in all places. The English are noted for their cruelty while the Gaures are the gentlest of men. All savages are cruel, and it is not their customs that tend in this direction; their cruelty is the result of their food."		Four years later, one of the founders of modern utilitarianism, the English philosopher Jeremy Bentham (1748–1832), although opposed to the concept of natural rights, argued that it was the ability to suffer that should be the benchmark of how we treat other beings. If rationality were the criterion, he argued, many humans, including infants and the disabled, would also have to be treated as though they were things.[32] He did not conclude that humans and nonhumans had equal moral significance, but argued that the latter's interests should be taken into account. He wrote in 1789, just as African slaves were being freed by the French:		The French have already discovered that the blackness of the skin is no reason a human being should be abandoned without redress to the caprice of a tormentor. It may one day come to be recognized that the number of the legs, the villosity of the skin, or the termination of the os sacrum are reasons equally insufficient for abandoning a sensitive being to the same fate. What else is it that should trace the insuperable line? Is it the faculty of reason or perhaps the faculty of discourse? But a full-grown horse or dog, is beyond comparison a more rational, as well as a more conversable animal, than an infant of a day or a week or even a month, old. But suppose the case were otherwise, what would it avail? the question is not, Can they reason?, nor Can they talk? but, Can they suffer?[33]		The 19th century saw an explosion of interest in animal protection, particularly in England. Debbie Legge and Simon Brooman write that the educated classes became concerned about attitudes toward the old, the needy, children, and the insane, and that this concern was extended to nonhumans. Before the 19th century, there had been prosecutions for poor treatment of animals, but only because of the damage to the animal as property. In 1793, for example, John Cornish was found not guilty of maiming a horse after pulling the animal's tongue out; the judge ruled that Cornish could be found guilty only if there was evidence of malice toward the owner.[34]		From 1800 onwards, there were several attempts in England to introduce animal protection legislation. The first was a bill against bull baiting, introduced in April 1800 by a Scottish MP, Sir William Pulteney (1729–1805). It was opposed inter alia on the grounds that it was anti-working class, and was defeated by two votes. Another attempt was made in 1802, this time opposed by the Secretary at War, William Windham (1750–1810), who said the Bill was supported by Methodists and Jacobins who wished to "destroy the Old English character, by the abolition of all rural sports."[35]		In 1809, Lord Erskine (1750-1823) introduced a bill to protect cattle and horses from malicious wounding, wanton cruelty, and beating. He told the House of Lords that animals had protection only as property: "The animals themselves are without protection--the law regards them not substantively--they have no rights!"[36] Erskine in his parliamentary speech combined the vocabulary of animal rights and trusteeship with a theological appeal included in the Bill's preamble to opposing cruelty.[37] The Bill was passed by the Lords, but was opposed in the Commons by Windham, who said it would be used against the "lower orders" when the real culprits would be their employers.[35]		D' ye think I'd wollop him? No, no, no! But gentle means I'd try, d' ye see, Because I hate all cruelty. If all had been like me, in fact, There'd ha' been no occasion for Martin's Act. — Music hall song inspired by the prosecution of Bill Burns for cruelty to a donkey.[38]		In 1821, the Treatment of Horses bill was introduced by Colonel Richard Martin (1754–1834), MP for Galway in Ireland, but it was lost among laughter in the House of Commons that the next thing would be rights for asses, dogs, and cats.[39] Nicknamed "Humanity Dick" by George IV, Martin finally succeeded in 1822 with his "Ill Treatment of Horses and Cattle Bill"—or "Martin's Act", as it became known—which was the world's first major piece of animal protection legislation. It was given royal assent on June 22 that year as An Act to prevent the cruel and improper Treatment of Cattle, and made it an offence, punishable by fines up to five pounds or two months imprisonment, to "beat, abuse, or ill-treat any horse, mare, gelding, mule, ass, ox, cow, heifer, steer, sheep or other cattle."[34]		Legge and Brooman argue that the success of the Bill lay in the personality of "Humanity Dick", who was able to shrug off the ridicule from the House of Commons, and whose sense of humour managed to capture the House's attention.[34] It was Martin himself who brought the first prosecution under the Act, when he had Bill Burns, a costermonger—a street seller of fruit—arrested for beating a donkey, and paraded the animal's injuries before a reportedly astonished court. Burns was fined, and newspapers and music halls were full of jokes about how Martin had relied on the testimony of a donkey.[40]		Other countries followed suit in passing legislation or making decisions that favoured animals. In 1822, the courts in New York ruled that wanton cruelty to animals was a misdemeanor at common law.[20] In France in 1850, Jacques Philippe Delmas de Grammont succeeded in having the Loi Grammont passed, outlawing cruelty against domestic animals, and leading to years of arguments about whether bulls could be classed as domestic in order to ban bullfighting.[41] The state of Washington followed in 1859, New York in 1866, California in 1868, and Florida in 1889.[42] In England, a series of amendments extended the reach of the 1822 Act, which became the Cruelty to Animals Act 1835, outlawing cockfighting, baiting, and dog fighting, followed by another amendment in 1849, and again in 1876.		It was resolved:		That a committee be appointed to superintend the Publication of Tracts, Sermons, and similar modes of influencing public opinion, to consist of the following Gentlemen:		Sir Jas. Mackintosh MP, A Warre Esqr. MP, Wm. Wilberforce Esqr. MP, Basil Montagu Esqr., Revd. A Broome, Revd. G Bonner, Revd G A Hatch, A E Kendal Esqr., Lewis Gompertz Esqr., Wm. Mudford Esqr., Dr. Henderson.		Resolved also:		That a Committee be appointed to adopt measures for Inspecting the Markets and Streets of the Metropolis, the Slaughter Houses, the conduct of Coachmen, etc.- etc, consisting of the following Gentlemen:		T F Buxton Esqr. MP, Richard Martin Esqr., MP, Sir James Graham, L B Allen Esqr., C C Wilson Esqr., Jno. Brogden Esqr., Alderman Brydges, A E Kendal Esqr., E Lodge Esqr., J Martin Esqr. T G Meymott Esqr.		A. Broome,		Honorary Secretary[40]		Richard Martin soon realized that magistrates did not take the Martin Act seriously, and that it was not being reliably enforced. Martin's Act was supported by various social reformers who were not parliamentarians and an informal network had gathered around the efforts of Reverend Arthur Broome (1779-1837) to create a voluntary organisation that would promote kindness toward animals. Broome canvassed opinions in letters that were published or summarised in various periodicals in 1821.[43] After the passage of Richard Martin's anti-cruelty to cattle bill in 1822, Broome attempted to form a Society for the Prevention of Cruelty to Animals that would bring together the patronage of persons who were of social rank and committed to social reforms. Broome did organise and chair a meeting of sympathisers in November 1822 where it was agreed that a Society should be created and at which Broome was named its Secretary but the attempt was short-lived.[44] In 1824 Broome arranged a new meeting in Old Slaughter's Coffee House in St. Martin's Lane, a London café frequented by artists and actors. The group met on June 16, 1824, and included a number of MPs: Richard Martin, Sir James Mackintosh (1765–1832), Sir Thomas Buxton (1786–1845), William Wilberforce (1759–1833), and Sir James Graham (1792–1861), who had been an MP, and who became one again in 1826.[38]		They decided to form a "Society instituted for the purpose of preventing cruelty to animals"; the Society for the Prevention of Cruelty to Animals, as it became known. It determined to send men to inspect slaughterhouses, Smithfield Market, where livestock had been sold since the 10th century, and to look into the treatment of horses by coachmen.[38] The Society became the Royal Society in 1840, when it was granted a royal charter by Queen Victoria, herself strongly opposed to vivisection.[45]		From 1824 onwards, several books were published, analyzing animal rights issues, rather than protection alone. Lewis Gompertz (1783/4–1865), one of the men who attended the first meeting of the SPCA, published Moral Inquiries on the Situation of Man and of Brutes (1824), arguing that every living creature, human and nonhuman, has more right to the use of its own body than anyone else has to use it, and that our duty to promote happiness applies equally to all beings. Edward Nicholson (1849–1912), head of the Bodleian Library at the University of Oxford, argued in Rights of an Animal (1879) that animals have the same natural right to life and liberty that human beings do, disregarding Descartes' mechanistic view—or what he called the "Neo-Cartesian snake"—that they lack consciousness.[46] Other writers of the time who explored whether animals might have natural (or moral) rights were Edward Payson Evans (1831–1917), John Muir (1838–1914), and J. Howard Moore (1862–1916), an American zoologist and author of The Universal Kinship (1906) and The New Ethics (1907).[47]		The development in England of the concept of animal rights was strongly supported by the German philosopher, Arthur Schopenhauer (1788–1860). He wrote that Europeans were "awakening more and more to a sense that beasts have rights, in proportion as the strange notion is being gradually overcome and outgrown, that the animal kingdom came into existence solely for the benefit and pleasure of man."[49]		He stopped short of advocating vegetarianism, arguing that, so long as an animal's death was quick, men would suffer more by not eating meat than animals would suffer by being eaten. He applauded the animal protection movement in England—"To the honor, then, of the English, be it said that they are the first people who have, in downright earnest, extended the protecting arm of the law to animals."[49] He also argued against the dominant Kantian idea that animal cruelty is wrong only insofar as it brutalizes humans:		Thus, because Christian morality leaves animals out of account ... they are at once outlawed in philosophical morals; they are mere "things," mere means to any ends whatsoever. They can therefore be used for vivisection, hunting, coursing, bullfights, and horse racing, and can be whipped to death as they struggle along with heavy carts of stone. Shame on such a morality that is worthy of pariahs, chandalas, and mlechchhas, and that fails to recognize the eternal essence that exists in every living thing ...[48]		The English poet and dramatist Percy Bysshe Shelley (1792–1822) wrote two essays advocating a vegetarian diet, for ethical and health reasons: A Vindication of Natural Diet (1813) and On the Vegetable System of Diet (1929, posth.).		John Stuart Mill (1806–1873), the English philosopher, also argued that utilitarianism must take animals into account, writing in 1864:[year verification needed] "Nothing is more natural to human beings, nor, up to a certain point in cultivation, more universal, than to estimate the pleasures and pains of others as deserving of regard exactly in proportion to their likeness to ourselves. ... Granted that any practice causes more pain to animals than it gives pleasure to man; is that practice moral or immoral? And if, exactly in proportion as human beings raise their heads out of the slough of selfishness, they do not with one voice answer 'immoral,' let the morality of the principle of utility be for ever condemned."[50]		James Rachels writes that Charles Darwin's (1809–1882) On the Origin of Species (1859)—which presented the theory of evolution by natural selection—revolutionized the way humans viewed their relationship with other species. Not only did human beings have a direct kinship with other animals, but the latter had social, mental and moral lives too, Darwin argued.[51] He wrote in his Notebooks (1837): "Animals – whom we have made our slaves we do not like to consider our equals. – Do not slave holders wish to make the black man other kind?"[52] Later, in The Descent of Man (1871), he argued that "There is no fundamental difference between man and the higher mammals in their mental faculties", attributing to animals the power of reason, decision making, memory, sympathy, and imagination.[51]		Rachels writes that Darwin noted the moral implications of the cognitive similarities, arguing that "humanity to the lower animals" was one of the "noblest virtues with which man is endowed." He was strongly opposed to any kind of cruelty to animals, including setting traps. He wrote in a letter that he supported vivisection for "real investigations on physiology; but not for mere damnable and detestable curiosity. It is a subject which makes me sick with horror ..." In 1875, he testified before a Royal Commission on Vivisection, lobbying for a bill to protect both the animals used in vivisection, and the study of physiology. Rachels writes that the animal rights advocates of the day, such as Frances Power Cobbe, did not see Darwin as an ally.[51]		An early proposal for legal rights for animals came from a group of citizens in Ashtabula County, Ohio. Around 1844, the group proposed an amendment to the U.S. Constitution stating that if slaves from slave states were receiving representation as 3/5 of a person on the basis that they were animal property, all the animal property of the free states should receive representation also.[53]		The first animal protection group in the United States, the American Society for the Prevention of Cruelty to Animals (ASPCA), was founded by Henry Bergh in April 1866. Bergh had been appointed by President Abraham Lincoln to a diplomatic post in Russia, and had been disturbed by the mistreatment of animals he witnessed there. He consulted with the president of the RSPCA in London, and returned to the United States to speak out against bullfights, cockfights, and the beating of horses. He created a "Declaration of the Rights of Animals", and in 1866 persuaded the New York state legislature to pass anti-cruelty legislation and to grant the ASPCA the authority to enforce it.[54]		In 1875, the Irish social reformer Frances Power Cobbe (1822–1904) founded the Society for the Protection of Animals Liable to Vivisection, the world's first organization opposed to animal research, which became the National Anti-Vivisection Society. In 1880, the English feminist Anna Kingsford (1846–1888) became one of the first English women to graduate in medicine, after studying for her degree in Paris, and the only student at the time to do so without having experimented on animals. She published The Perfect Way in Diet (1881), advocating vegetarianism, and in the same year founded the Food Reform Society. She was also vocal in her opposition to experimentation on animals.[55] In 1898, Cobbe set up the British Union for the Abolition of Vivisection, with which she campaigned against the use of dogs in research, coming close to success with the 1919 Dogs (Protection) Bill, which almost became law.		Ryder writes that, as the interest in animal protection grew in the late 1890s, attitudes toward animals among scientists began to harden. They embraced the idea that what they saw as anthropomorphism—the attribution of human qualities to nonhumans—was unscientific. Animals had to be approached as physiological entities only, as Ivan Pavlov wrote in 1927, "without any need to resort to fantastic speculations as to the existence of any possible subjective states." It was a position that hearkened back to Descartes in the 17th century, that nonhumans were purely mechanical, with no rationality and perhaps even no consciousness.[56]		Avoiding utilitarianism, Friedrich Nietzsche (1844–1900) found other reasons to defend animals. He argued that "The sight of blind suffering is the spring of the deepest emotion."[57] He once wrote: "For man is the cruelest animal. At tragedies, bull-fights, and crucifixions hath he hitherto been happiest on earth; and when he invented his hell, behold, that was his heaven on earth."[58] Throughout his writings, he speaks of the human being as an animal.[59]		In 1894, Henry Salt (1851–1939), a former master at Eton, who had set up the Humanitarian League to lobby for a ban on hunting the year before, published Animals' Rights: Considered in Relation to Social Progress.[60] He wrote that the object of the essay was to "set the principle of animals' rights on a consistent and intelligible footing."[61] Concessions to the demands for jus animalium had been made grudgingly to date, he wrote, with an eye on the interests of animals qua property, rather than as rights bearers:		Even the leading advocates of animal rights seem to have shrunk from basing their claim on the only argument which can ultimately be held to be a really sufficient one—the assertion that animals, as well as men, though, of course, to a far less extent than men, are possessed of a distinctive individuality, and, therefore, are in justice entitled to live their lives with a due measure of that "restricted freedom" to which Herbert Spencer alludes.[61]		He argued that there was no point in claiming rights for animals if those rights were subordinated to human desire, and took issue with the idea that the life of a human might have more moral worth. "[The] notion of the life of an animal having 'no moral purpose,' belongs to a class of ideas which cannot possibly be accepted by the advanced humanitarian thought of the present day—it is a purely arbitrary assumption, at variance with our best instincts, at variance with our best science, and absolutely fatal (if the subject be clearly thought out) to any full realization of animals' rights. If we are ever going to do justice to the lower races, we must get rid of the antiquated notion of a 'great gulf' fixed between them and mankind, and must recognize the common bond of humanity that unites all living beings in one universal brotherhood."[61]		In 1902, Lizzy Lind af Hageby (1878–1963), a Swedish feminist, and a friend, Lisa Shartau, traveled to England to study medicine at the London School of Medicine for Women, intending to learn enough to become authoritative anti-vivisection campaigners. In the course of their studies, they witnessed several animal experiments, and published the details as The Shambles of Science: Extracts from the Diary of Two Students of Physiology (1903). Their allegations included that they had seen a brown terrier dog dissected while conscious, which prompted angry denials from the researcher, William Bayliss, and his colleagues. After Stephen Coleridge of the National Anti-Vivisection Society accused Bayliss of having violated the Cruelty to Animals Act 1876, Bayliss sued and won, convincing a court that the animal had been anesthetized as required by the Act.[62]		In response, anti-vivisection campaigners commissioned a statue of the dog to be erected in Battersea Park in 1906, with the plaque: "Men and Women of England, how long shall these Things be?" The statue caused uproar among medical students, leading to frequent vandalism of the statue and the need for a 24-hour police guard. The affair culminated in riots in 1907 when 1,000 medical students clashed with police, suffragettes and trade unionists in Trafalgar Square. Battersea Council removed the statue from the park under cover of darkness two years later.[62]		Coral Lansbury (1985) and Hilda Kean (1998) write that the significance of the affair lay in the relationships that formed in support of the "Brown Dog Done to Death", which became a symbol of the oppression the women's suffrage movement felt at the hands of the male political and medical establishment. Kean argues that both sides saw themselves as heirs to the future. The students saw the women and trade unionists as representatives of anti-science sentimentality, while the women saw themselves as progressive, with the students and their teachers belonging to a previous age.[63]		Members of the English Vegetarian Society who avoided the use of eggs and animal milk in the 19th and early 20th century were known as strict vegetarians. The International Vegetarian Union cites an article informing readers of alternatives to shoe leather in the Vegetarian Society's magazine in 1851 as evidence of the existence of a group that sought to avoid animal products entirely. There was increasing unease within the Society from the start of the 20th century onwards with regards to egg and milk consumption, and in 1923 its magazine wrote that the "ideal position for vegetarians is [complete] abstinence from animal products."[64]		Mahatma Gandhi (1869–1948) argued in 1931 before a meeting of the Society in London that vegetarianism should be pursued in the interests of animals, and not only as a human health issue. He met both Henry Salt and Anna Kingsford, and read Salt's A Plea for Vegetarianism (1880). Salt wrote in the pamphlet that "a Vegetarian is still regarded, in ordinary society, as little better than a madman."[64] In 1944, several members, led by Donald Watson (1910–2016), decided to break from the Vegetarian Society over the issue of egg and milk use. Watson coined the term "vegan" for those whose diet included no animal products, and they formed the British Vegan Society on November 1 that year.[65]		On coming to power in January 1933, the Nazi Party passed a comprehensive set of animal protection laws. The laws were similar to those that already existed in England, though more detailed and with severe penalties for breaking them. Arnold Arluke and Boria Sax write that the Nazis tried to abolish the distinction between humans and animals, to the point where many people were regarded as less valuable than animals.[66]		In April 1933 they passed laws regulating the slaughter of animals; one of their targets was kosher slaughter. In November the Tierschutzgesetz, or animal protection law, was introduced, with Adolf Hitler announcing an end to animal cruelty: "Im neuen Reich darf es keine Tierquälerei mehr geben." ("In the new Reich, no more animal cruelty will be allowed.") It was followed in July 1934 by the Reichsjagdgesetz, prohibiting hunting; in July 1935 by the Naturschutzgesetz, environmental legislation; in November 1937 by a law regulating animal transport in cars; and in September 1938 by a similar law dealing with animals on trains.[67] Hitler was a vegetarian in the later years of his life; several members of his inner circle, including Rudolf Hess, Joseph Goebbels, and Heinrich Himmler, adopted some form of vegetarianism, though by most accounts their vegetarianism was not as strict as Hitler's.[68]		Despite the proliferation of animal protection legislation, animals still had no legal rights. Debbie Legge writes that existing legislation was very much tied to the idea of human interests, whether protecting human sensibilities by outlawing cruelty, or protecting property rights by making sure animals were not damaged. The over-exploitation of fishing stocks, for example, is viewed as harming the environment for people; the hunting of animals to extinction means that humans in the future will derive no enjoyment from them; poaching results in financial loss to the owner, and so on.[42]		Notwithstanding the interest in animal welfare of the previous century, the situation for animals arguably deteriorated in the 20th century, particularly after the Second World War. This was in part because of the increase in the numbers used in animal research—300 in the UK in 1875, 19,084 in 1903, and 2.8 million in 2005 (50–100 million worldwide), and a modern annual estimated range of 10 million to upwards of 100 million in the US[69]—but mostly because of the industrialization of farming, which saw billions of animals raised and killed for food on a scale considered impossible before the war.[70]		In the early 1960s in England, support for animal rights began to coalesce around the issue of blood sports, particularly hunting deer, foxes, and otters using dogs, an aristocratic and middle-class English practice, stoutly defended in the name of protecting rural traditions. The psychologist Richard D. Ryder – who became involved with the animal rights movement in the late 1960s – writes that the new chair of the League Against Cruel Sports tried in 1963 to steer it away from confronting members of the hunt, which triggered the formation that year of a direct action breakaway group, the Hunt Saboteurs Association. This was set up by a journalist, John Prestige, who had witnessed a pregnant deer being chased into a village and killed by the Devon and Somerset Staghounds. The practice of sabotaging hunts (for example, by misleading the dogs with scents or horns) spread throughout south-east England, particularly around university towns, leading to violent confrontations when the huntsmen attacked the "sabs".[71]		The controversy spread to the RSPCA, which had arguably grown away from its radical roots to become a conservative group with charity status and royal patronage. It had failed to speak out against hunting, and indeed counted huntsmen among its members. As with the League Against Cruel Sports, this position gave rise to a splinter group, the RSPCA Reform Group, which sought to radicalize the organization, leading to chaotic meetings of the group's ruling Council, and successful (though short-lived) efforts to change it from within by electing to the Council members who would argue from an animal rights perspective, and force the RSPCA to address issues such as hunting, factory farming, and animal experimentation. Ryder himself was elected to the Council in 1971, and served as its chair from 1977 to 1979.[71]		The same period saw writers and academics begin to speak out again in favor of animal rights. Ruth Harrison published Animal Machines (1964), an influential critique of factory farming, and on October 10, 1965, the novelist Brigid Brophy had an article, "The Rights of Animals", published in The Sunday Times.[56] She wrote:		The relationship of homo sapiens to the other animals is one of unremitting exploitation. We employ their work; we eat and wear them. We exploit them to serve our superstitions: whereas we used to sacrifice them to our gods and tear out their entrails in order to foresee the future, we now sacrifice them to science, and experiment on their entail in the hope—or on the mere off chance—that we might thereby see a little more clearly into the present ... To us it seems incredible that the Greek philosophers should have scanned so deeply into right and wrong and yet never noticed the immorality of slavery. Perhaps 3000 years from now it will seem equally incredible that we do not notice the immorality of our own oppression of animals.[56]		Robert Garner writes that Harrison's book and Brophy's article led to an explosion of interest in the relationship between humans and nonhumans.[72] In particular, Brophy's article was discovered in or around 1969 by a group of postgraduate philosophy students at the University of Oxford, Roslind and Stanley Godlovitch (husband and wife from Canada), John Harris, and David Wood, now known as the Oxford Group. They decided to put together a symposium to discuss the theory of animal rights.[56]		Around the same time, Richard Ryder wrote several letters to The Daily Telegraph criticizing animal experimentation, based on incidents he had witnessed in laboratories. The letters, published in April and May 1969, were seen by Brigid Brophy, who put Ryder in touch with the Godlovitches and Harris. Ryder also started distributing pamphlets in Oxford protesting against experiments on animals; it was in one of these pamphlets in 1970 that he coined the term "speciesism" to describe the exclusion of nonhuman animals from the protections offered to humans.[73] He subsequently became a contributor to the Godlovitches' symposium, as did Harrison and Brophy, and it was published in 1971 as Animals, Men and Morals: An Inquiry into the Maltreatment of Non-humans.[74]		In 1970, over lunch in Oxford with fellow student Richard Keshen, a vegetarian, Australian philosopher Peter Singer came to believe that, by eating animals, he was engaging in the oppression of other species. Keshen introduced Singer to the Godlovitches, and in 1973 Singer reviewed their book for The New York Review of Books. In the review, he used the term "animal liberation", writing:		We are familiar with Black Liberation, Gay Liberation, and a variety of other movements. With Women's Liberation some thought we had come to the end of the road. Discrimination on the basis of sex, it has been said, is the last form of discrimination that is universally accepted and practiced without pretense ... But one should always be wary of talking of "the last remaining form of discrimination." ... Animals, Men and Morals is a manifesto for an Animal Liberation movement.[75]		On the strength of his review, The New York Review of Books took the unusual step of commissioning a book from Singer on the subject, published in 1975 as Animal Liberation, now one of the animal rights movement's canonical texts. Singer based his arguments on the principle of utilitarianism – the view, in its simplest form, that an act is right if it leads to the "greatest happiness of the greatest number", a phrase first used in 1776 by Jeremy Bentham.[75] He argued in favor of the equal consideration of interests, the position that there are no grounds to suppose that a violation of the basic interests of a human—for example, an interest in not suffering—is different in any morally significant way from a violation of the basic interests of a nonhuman.[76] Singer used the term "speciesism" in the book, citing Ryder, and it stuck, becoming an entry in the Oxford English Dictionary in 1989.[77]		The book's publication triggered a groundswell of scholarly interest in animal rights. Richard Ryder's Victims of Science: The Use of Animals in Research (1975) appeared, followed by Andrew Linzey's Animal Rights: A Christian Perspective (1976), and Stephen R. L. Clark's The Moral Status of Animals (1977). A Conference on Animal Rights was organized by Ryder and Linzey at Trinity College, Cambridge, in August 1977. This was followed by Mary Midgley's Beast And Man: The Roots of Human Nature (1978), then Animal Rights–A Symposium (1979), which included the papers delivered to the Cambridge conference.[72]		From 1982 onwards, a series of articles by Tom Regan led to his The Case for Animal Rights (1984), in which he argues that nonhuman animals are "subjects-of-a-life", and therefore possessors of moral rights, a work regarded as a key text in animal rights theory.[72] Regan wrote in 2001 that philosophers had written more about animal rights in the previous 20 years than in the 2,000 years before that.[78] Garner writes that Charles Magel's bibliography, Keyguide to Information Sources in Animal Rights (1989), contains 10 pages of philosophical material on animals up to 1970, but 13 pages between 1970 and 1989 alone.[79]		In 1971, a law student, Ronnie Lee, formed a branch of the Hunt Saboteurs Association in Luton, later calling it the Band of Mercy after a 19th-century RSPCA youth group. The Band attacked hunters' vehicles by slashing tires and breaking windows, calling it "active compassion". In November 1973, they engaged in their first act of arson when they set fire to a Hoechst Pharmaceuticals research laboratory, claiming responsibility as a "nonviolent guerilla organization dedicated to the liberation of animals from all forms of cruelty and persecution at the hands of mankind."[80]		Lee and another activist were sentenced to three years in prison in 1974, paroled after 12 months. In 1976 Lee brought together the remaining Band of Mercy activists along with some fresh faces to start a leaderless resistance movement, calling it the Animal Liberation Front (ALF).[80] ALF activists see themselves as a modern Underground Railroad, passing animals removed from farms and laboratories to sympathetic veterinarians, safe houses and sanctuaries.[81] Some activists also engage in threats, intimidation, and arson, acts that have lost the movement sympathy in mainstream public opinion.[82]		The decentralized model of activism is frustrating for law enforcement organizations, who find the networks difficult to infiltrate, because they tend to be organized around friends.[83] In 2005, the US Department of Homeland Security indicated how seriously it takes the ALF when it included them in a list of domestic terrorist threats.[84] The tactics of some of the more determined ALF activists are anathema to many animal rights advocates, such as Singer, who regard the movement as something that should occupy the moral high ground. ALF activists respond to the criticism with the argument that, as Ingrid Newkirk puts it, "Thinkers may prepare revolutions, but bandits must carry them out."[85]		From the 1980s through to the early 2000s there was an increased level of violence by animal rights extremist groups directed at individuals and institutions associated with animal research. Activist groups involved included the Justice Department, the Animal Rights Militia and SHAC.[86]		Henry Spira (1927–1998), a former seaman and civil rights activist, became the most notable of the new animal advocates in the United States. A proponent of gradual change, he formed Animal Rights International in 1974, and introduced the idea of "reintegrative shaming", whereby a relationship is formed between a group of animal rights advocates and a corporation they see as misusing animals, with a view to obtaining concessions or halting a practice. It is a strategy that has been widely adopted, most notably by People for the Ethical Treatment of Animals.[87]		Spira's first campaign was in opposition to the American Museum of Natural History in 1976, where cats were being experimented on, research that he persuaded them to stop. His most notable achievement was in 1980, when he convinced the cosmetics company Revlon to stop using the Draize test, which involves toxicity tests on the skin or in the eyes of animals. He took out a full-page ad in several newspapers, featuring a rabbit with sticking plaster over the eyes, and the caption, "How many rabbits does Revlon blind for beauty's sake?" Revlon stopped using animals for cosmetics testing, donated money to help set up the Center for Alternatives to Animal Testing, and was followed by other leading cosmetics companies.[88] Revlon has since renewed testing on animals as it is unwilling to give up revenue from sales in China, where animal testing is required for cosmetics and other items. [[89]]		In 1999, New Zealand passed a new Animal Welfare Act that had the effect of banning experiments on "non-human hominids".[90]		Also in 1999, Public Law 106-152 (Title 18, Section 48) was put into action in the United States. This law makes it a felony to create, sell, or possess videos showing animal cruelty with the intention of profiting financially from them.[91]		In 2005, the Austrian parliament banned experiments on apes, unless they are performed in the interests of the individual ape.[90] Also in Austria, the Supreme Court ruled in January 2008 that a chimpanzee (called Matthew Hiasl Pan by those advocating for his personhood) was not a person, after the Association Against Animal Factories sought personhood status for him because his custodians had gone bankrupt. The chimpanzee had been captured as a baby in Sierra Leone in 1982, then smuggled to Austria to be used in pharmaceutical experiments, but was discovered by customs officials when he arrived in the country, and was taken to a shelter instead. He was kept there for 25 years, until the group that ran the shelter went bankrupt in 2007. Donors offered to help him, but under Austrian law only a person can receive personal gifts, so any money sent to support him would be lost to the shelter's bankruptcy. The Association appealed the ruling to the European Court of Human Rights. The lawyer proposing the chimpanzee's personhood asked the court to appoint a legal guardian for him and to grant him four rights: the right to life, limited freedom of movement, personal safety, and the right to claim property.[92]		In June 2008, a committee of Spain's national legislature became the first to vote for a resolution to extend limited rights to nonhuman primates. The parliamentary Environment Committee recommended giving chimpanzees, bonobos, gorillas, and orangutans the right not to be used in medical experiments or in circuses, and recommended making it illegal to kill apes, except in self-defense, based upon the rights recommended by the Great Ape Project.[93] The committee's proposal has not yet been enacted into law.[94]		From 2009 onwards, several countries outlawed the use of some or all animals in circuses, starting with Bolivia, and followed by several countries in Europe, Scandinavia, the Middle East, and Singapore.[95]		In 2010, the regional government in Catalonia passed a motion to outlaw bull fighting, the first such ban in Spain.[96] In 2011, PETA sued SeaWorld over the captivity of five orcas in San Diego and Orlando, arguing that the whales were being treated as slaves. It was the first time the Thirteenth Amendment to the United States Constitution, which outlaws slavery and involuntary servitude, was cited in court to protect nonhuman rights. A federal judge dismissed the case in February 2012.[97]		In 2015, the Nonhuman Rights Project (NhPR) filed three lawsuits in New York State on behalf of four captive chimpanzees, demanding that the courts grant them the right to bodily liberty via the writ of Habeas Corpus and to immediately send them to a sanctuary affiliated with the North American Primate Sanctuary Alliance.[98] All of the petitions were denied. In the case involving the chimpanzees Hercules and Leo, Justice Barbra Jaffe did not immediately dismiss the filing and instead ordered a hearing requiring the chimpanzee owner to show why the chimpanzees should be not be released and transferred to the sanctuary.[99] Following the hearing, Justice Jaffe issued an order denying Hercules and Leo's petition.		Even though the petition was denied, NhRP interpreted Justice Jeffe's decision as a victory. In its press release it emphasized the fact that Justice Jeffe agreed with NhRP when finding that "'persons' are not restricted to human beings, and that who is a 'person' is not a question of biology, but of public policy and principle" and also stating that "Efforts to extend legal rights to chimpanzees are thus understandable; some day they may even succeed."[100]		Robert Garner writes that both Hindu and Buddhist societies abandoned animal sacrifice and embraced vegetarianism from the 3rd century BCE. Several kings in India built hospitals for animals, and the emperor Ashoka (304–232 BCE) issued orders against hunting and animal slaughter, in line with ahimsa, the doctrine of non-violence. Garner writes that Jainism took this idea further. Jains believe that no living creature should be harmed, and they are known to clear paths in front of them by sweeping them to protect any insect life that may be present.[101]		Paul Waldau writes that, in 2000, the High Court in Kerala used the language of "rights" in relation to circus animals, ruling that they are "beings entitled to dignified existence" under Article 21 of the Indian Constitution. The ruling said that if human beings are entitled to these rights, animals should be too. The court went beyond the requirements of the Constitution that all living beings should be shown compassion, and said: "It is not only our fundamental duty to show compassion to our animal friends, but also to recognize and protect their rights." Waldau writes that other courts in India and one court in Sri Lanka have used similar language.[90]		In 2012, the Indian government issued an extensive ban on vivisection in education and research.[102]		For some the basis of animal rights is oin religion or animal worship (or in general nature worship), with some religions banning killing of any animal; One of the most important sanctions of the Buddhist faith is the concept of ahimsa, or refraining from the destruction of life. According to Buddhist belief, humans do not deserve preferential treatment over other living beings. See also in section above.		In contrast, in other religions, animals can be unclean, with in general allowing eating all except for these; "The Torah allows eating certain kinds of "winged swarming things" (i.e. insects) while prohibiting others".		Animal rights were recognized early by the Sharia (Islamic law). This recognition is based on both the Qur'an and the Hadith. In the Qur'an, there are many references to animals, detailing that they have souls, form communities, communicate with God and worship Him in their own way. Muhammad forbade his followers to harm any animal and asked them to respect the rights of animals.[103] It is a distinctive characteristic of the Shariah that all animals have legal rights. Othman Llewellyn even argues that Shariah has mechanisms for the full repair of injuries suffered by non-human creatures including their representation in court, assessment of injuries and awarding of relief to them.[citation needed] The classical Muslim jurist 'Izz ad-Din ibn 'Abd as-Salam, who flourished during the thirteenth century, formulated the following statement of animal rights:		The rights of livestock and animals upon man: these are that he spend on them the provision that their kinds require, even if they have aged or sickened such that no benefit comes from them; that he not burden them beyond what they can bear; that he not put them together with anything by which they would be injured, whether of their own kind or other species, and whether by breaking their bones or butting or wounding; that he slaughters them with kindness when he slaughters them, and neither flay their skins nor break their bones until their bodies have become cold and their lives have passed away; that he not slaughter their young within their sight, but that he isolate them; that he makes comfortable their resting places and watering places; that he puts their males and females together during their mating seasons; that he not discard those which he takes as game; and neither shoots them with anything that breaks their bones nor brings about their destruction by any means that renders their meat unlawful to eat.[104]		On the other hand, animal sacrifice is a prominent feature of Eid al-Adha observances.[105]		The two main philosophical approaches to animal rights are utilitarian and rights-based. The former is exemplified by Peter Singer, and the latter by Tom Regan and Gary Francione. Their differences reflect a distinction philosophers draw between ethical theories that judge the rightness of an act by its consequences (consequentialism/teleological ethics, or utilitarianism), and those that focus on the principle behind the act, almost regardless of consequences (deontological ethics). Deontologists argue that there are acts we should never perform, even if failing to do so entails a worse outcome.[106]		There are a number of positions that can be defended from a consequentalist or deontologist perspective, including the capabilities approach, represented by Martha Nussbaum, and the egalitarian approach, which has been examined by Ingmar Persson and Peter Vallentyne. The capabilities approach focuses on what individuals require to fulfill their capabilities: Nussbaum (2006) argues that animals need a right to life, some control over their environment, company, play, and physical health.[107]		Stephen R. L. Clark, Mary Midgley, and Bernard Rollin also discuss animal rights in terms of animals being permitted to lead a life appropriate for their kind.[108] Egalitarianism favors an equal distribution of happiness among all individuals, which makes the interests of the worse off more important than those of the better off.[109] Another approach, virtue ethics, holds that in considering how to act we should consider the character of the actor, and what kind of moral agents we should be. Rosalind Hursthouse has suggested an approach to animal rights based on virtue ethics.[110] Mark Rowlands has proposed a contractarian approach.[111]		Nussbaum (2004) writes that utilitarianism, starting with Jeremy Bentham and John Stuart Mill, has contributed more to the recognition of the moral status of animals than any other ethical theory.[113] The utilitarian philosopher most associated with animal rights is Peter Singer, professor of bioethics at Princeton University. Singer is not a rights theorist, but uses the language of rights to discuss how we ought to treat individuals. He is a preference utilitarian, meaning that he judges the rightness of an act by the extent to which it satisfies the preferences (interests) of those affected.[114]		His position is that there is no reason not to give equal consideration to the interests of human and nonhumans, though his principle of equality does not require identical treatment. A mouse and a man both have an interest in not being kicked, and there are no moral or logical grounds for failing to accord those interests equal weight. Interests are predicated on the ability to suffer, nothing more, and once it is established that a being has interests, those interests must be given equal consideration.[115] Singer quotes the English philosopher Henry Sidgwick (1838–1900): "The good of any one individual is of no more importance, from the point of view ... of the Universe, than the good of any other."[76]		Singer argues that equality of consideration is a prescription, not an assertion of fact: if the equality of the sexes were based only on the idea that men and women were equally intelligent, we would have to abandon the practice of equal consideration if this were later found to be false. But the moral idea of equality does not depend on matters of fact such as intelligence, physical strength, or moral capacity. Equality therefore cannot be grounded on the outcome of scientific investigations into the intelligence of nonhumans. All that matters is whether they can suffer.[116]		Commentators on all sides of the debate now accept that animals suffer and feel pain, although it was not always so. Bernard Rollin, professor of philosophy, animal sciences, and biomedical sciences at Colorado State University, writes that Descartes' influence continued to be felt until the 1980s. Veterinarians trained in the US before 1989 were taught to ignore pain, he writes, and at least one major veterinary hospital in the 1960s did not stock narcotic analgesics for animal pain control. In his interactions with scientists, he was often asked to "prove" that animals are conscious, and to provide "scientifically acceptable" evidence that they could feel pain.[117]		Scientific publications have made it clear since the 1980s that the majority of researchers do believe animals suffer and feel pain, though it continues to be argued that their suffering may be reduced by an inability to experience the same dread of anticipation as humans, or to remember the suffering as vividly.[118] The problem of animal suffering, and animal consciousness in general, arose primarily because it was argued that animals have no language. Singer writes that, if language were needed to communicate pain, it would often be impossible to know when humans are in pain, though we can observe pain behavior and make a calculated guess based on it. He argues that there is no reason to suppose that the pain behavior of nonhumans would have a different meaning from the pain behavior of humans.[119]		Tom Regan, professor emeritus of philosophy at North Carolina State University, argues in The Case for Animal Rights (1983) that nonhuman animals are what he calls "subjects-of-a-life", and as such are bearers of rights.[120] He writes that, because the moral rights of humans are based on their possession of certain cognitive abilities, and because these abilities are also possessed by at least some nonhuman animals, such animals must have the same moral rights as humans. Although only humans act as moral agents, both marginal-case humans, such as infants, and at least some nonhumans must have the status of "moral patients".[120]		Moral patients are unable to formulate moral principles, and as such are unable to do right or wrong, even though what they do may be beneficial or harmful. Only moral agents are able to engage in moral action. Animals for Regan have "intrinsic value" as subjects-of-a-life, and cannot be regarded as a means to an end, a view that places him firmly in the abolitionist camp. His theory does not extend to all animals, but only to those that can be regarded as subjects-of-a-life.[120] He argues that all normal mammals of at least one year of age would qualify:		... individuals are subjects-of-a-life if they have beliefs and desires; perception, memory, and a sense of the future, including their own future; an emotional life together with feelings of pleasure and pain; preference- and welfare-interests; the ability to initiate action in pursuit of their desires and goals; a psychophysical identity over time; and an individual welfare in the sense that their experiential life fares well or ill for them, logically independently of their utility for others and logically independently of their being the object of anyone else's interests.[120]		Whereas Singer is primarily concerned with improving the treatment of animals and accepts that, in some hypothetical scenarios, individual animals might be used legitimately to further human or nonhuman ends, Regan believes we ought to treat nonhuman animals as we would humans. He applies the strict Kantian ideal (which Kant himself applied only to humans) that they ought never to be sacrificed as a means to an end, and must be treated as ends in themselves.[121]		Gary Francione, professor of law and philosophy at Rutgers Law School in Newark, is a leading abolitionist writer, arguing that animals need only one right, the right not to be owned. Everything else would follow from that paradigm shift. He writes that, although most people would condemn the mistreatment of animals, and in many countries there are laws that seem to reflect those concerns, "in practice the legal system allows any use of animals, however abhorrent." The law only requires that any suffering not be "unnecessary". In deciding what counts as "unnecessary", an animal's interests are weighed against the interests of human beings, and the latter almost always prevail.[122]		Francione's Animals, Property, and the Law (1995) was the first extensive jurisprudential treatment of animal rights. In it, Francione compares the situation of animals to the treatment of slaves in the United States, where legislation existed that appeared to protect them, while the courts ignored that the institution of slavery itself rendered the protection unenforceable.[123] He offers as an example the United States Animal Welfare Act, which he describes as an example of symbolic legislation, intended to assuage public concern about the treatment of animals, but difficult to implement.[124]		He argues that a focus on animal welfare, rather than animal rights, may worsen the position of animals by making the public feel comfortable about using them and entrenching the view of them as property. He calls animal rights groups who pursue animal welfare issues, such as People for the Ethical Treatment of Animals, the "new welfarists", arguing that they have more in common with 19th-century animal protectionists than with the animal rights movement; indeed, the terms "animal protection" and "protectionism" are increasingly favored. His position in 1996 was that there is no animal rights movement in the United States.[125]		Mark Rowlands, professor of philosophy at the University of Florida, has proposed a contractarian approach, based on the original position and the veil of ignorance—a "state of nature" thought experiment that tests intuitions about justice and fairness—in John Rawls's A Theory of Justice (1971). In the original position, individuals choose principles of justice (what kind of society to form, and how primary social goods will be distributed), unaware of their individual characteristics—their race, sex, class, or intelligence, whether they are able-bodied or disabled, rich or poor—and therefore unaware of which role they will assume in the society they are about to form.[111]		The idea is that, operating behind the veil of ignorance, they will choose a social contract in which there is basic fairness and justice for them no matter the position they occupy. Rawls did not include species membership as one of the attributes hidden from the decision makers in the original position. Rowlands proposes extending the veil of ignorance to include rationality, which he argues is an undeserved property similar to characteristics including race, sex and intelligence.[111]		American philosopher Timothy Garry has proposed an approach that deems nonhuman animals worthy of prima facie rights. In a philosophical context, a prima facie (Latin for "on the face of it" or "at first glance") right is one that appears to be applicable at first glance, but upon closer examination may be outweighed by other considerations. In his book Ethics: A Pluralistic Approach to Moral Theory, Lawrence Hinman characterizes such rights as "the right is real but leaves open the question of whether it is applicable and overriding in a particular situation".[126] The idea that nonhuman animals are worthy of prima facie rights is to say that, in a sense, animals do have rights. However, these rights can be overridden by many other considerations, especially those conflicting a human's right to life, liberty, property, and the pursuit of happiness. Garry supports his view arguing:		... if a nonhuman animal were to kill a human being in the U.S., it would have broken the laws of the land and would probably get rougher sanctions than if it were a human. My point is that like laws govern all who interact within a society, rights are to be applied to all beings who interact within that society. This is not to say these rights endowed by humans are equivalent to those held by nonhuman animals, but rather that if humans possess rights then so must all those who interact with humans.[127]		In sum, Garry suggests that humans have obligations to nonhuman animals; however, animals do not, and ought not to, have uninfringible rights against humans.		Women have played a central role in animal advocacy since the 19th century.[128] The anti-vivisection movement in the 19th and early 20th century in England and the United States was largely run by women, including Francis Power Cobbe, Anna Kingsford, Lizzy Lind af Hageby and Caroline Earle White (1833–1916).[129] Garner writes that 70 per cent of the membership of the Victoria Street Society (one of the anti-vivisection groups founded by Cobbe) were women, as were 70 per cent of the membership of the British RSPCA in 1900.[130]		The modern animal advocacy movement has a similar representation of women, though Garner (2005) writes that they are not invariably in leadership positions: during the March for Animals in Washington, D.C., in 1990—the largest animal rights demonstration held until then in the United States—most of the participants were women, but most of the platform speakers were men.[131] Nevertheless, several influential animal advocacy groups have been founded by women, including the British Union for the Abolition of Vivisection by Cobbe in London in 1898; the Animal Welfare Board of India by Rukmini Devi Arundale in 1962; and People for the Ethical Treatment of Animals, co-founded by Ingrid Newkirk in 1980. In the Netherlands, Marianne Thieme and Esther Ouwehand were elected to parliament in 2006 representing the Parliamentary group for Animals.		The preponderance of women in the movement has led to a body of academic literature exploring feminism and animal rights; feminism and vegetarianism or veganism, the oppression of women and animals, and the male association of women and animals with nature and emotion, rather than reason—an association that several feminist writers have embraced.[128] Lori Gruen writes that women and animals serve the same symbolic function in a patriarchal society: both are "the used"; the dominated, submissive "Other".[132] When the British feminist Mary Wollstonecraft (1759–1797) published A Vindication of the Rights of Woman (1792), Thomas Taylor (1758–1835), a Cambridge philosopher, responded with an anonymous parody, A Vindication of the Rights of Brutes (1792), claiming that Wollstonecraft's arguments for women's rights could be applied equally to animals, a position he intended as reductio ad absurdum.[133]		Some transhumanists argue for animal rights, liberation, and "uplift" of animal consciousness into machines.[134] Transhumanism also understands animal rights on a gradation or spectrum with other types of sentient rights, including human rights and the rights of conscious artificial intelligences (posthuman rights).[135]		R. G. Frey, professor of philosophy at Bowling Green State University, is a preference utilitarian, as is Singer, but reaches a very different conclusion, arguing in Interests and Rights (1980) that animals have no interests for the utilitarian to take into account. Frey argues that interests are dependent on desire, and that no desire can exist without a corresponding belief. Animals have no beliefs, because a belief state requires the ability to hold a second-order belief—a belief about the belief—which he argues requires language: "If someone were to say, e.g. 'The cat believes that the door is locked,' then that person is holding, as I see it, that the cat holds the declarative sentence 'The door is locked' to be true; and I can see no reason whatever for crediting the cat or any other creature which lacks language, including human infants, with entertaining declarative sentences."[136]		Carl Cohen, professor of philosophy at the University of Michigan, argues that rights holders must be able to distinguish between their own interests and what is right. "The holders of rights must have the capacity to comprehend rules of duty governing all, including themselves. In applying such rules, [they] ... must recognize possible conflicts between what is in their own interest and what is just. Only in a community of beings capable of self-restricting moral judgments can the concept of a right be correctly invoked." Cohen rejects Singer's argument that, since a brain-damaged human could not make moral judgments, moral judgments cannot be used as the distinguishing characteristic for determining who is awarded rights. Cohen writes that the test for moral judgment "is not a test to be administered to humans one by one", but should be applied to the capacity of members of the species in general.[137]		Judge Richard Posner of the United States Court of Appeals for the Seventh Circuit debated the issue of animal rights in 2001 with Peter Singer.[139] Posner posits that his moral intuition tells him "that human beings prefer their own. If a dog threatens a human infant, even if it requires causing more pain to the dog to stop it, than the dog would have caused to the infant, then we favour the child. It would be monstrous to spare the dog."[138]		Singer challenges this by arguing that formerly unequal rights for gays, women, and certain races were justified using the same set of intuitions. Posner replies that equality in civil rights did not occur because of ethical arguments, but because facts mounted that there were no morally significant differences between humans based on race, sex, or sexual orientation that would support inequality. If and when similar facts emerge about humans and animals, the differences in rights will erode too. But facts will drive equality, not ethical arguments that run contrary to instinct, he argues. Posner calls his approach "soft utilitarianism", in contrast to Singer's "hard utilitarianism". He argues:		The "soft" utilitarian position on animal rights is a moral intuition of many, probably most, Americans. We realize that animals feel pain, and we think that to inflict pain without a reason is bad. Nothing of practical value is added by dressing up this intuition in the language of philosophy; much is lost when the intuition is made a stage in a logical argument. When kindness toward animals is levered into a duty of weighting the pains of animals and of people equally, bizarre vistas of social engineering are opened up.[138]		Roger Scruton, the British philosopher, argues that rights imply obligations. Every legal privilege, he writes, imposes a burden on the one who does not possess that privilege: that is, "your right may be my duty." Scruton therefore regards the emergence of the animal rights movement as "the strangest cultural shift within the liberal worldview", because the idea of rights and responsibilities is, he argues, distinctive to the human condition, and it makes no sense to spread them beyond our own species.[6]		He accuses animal rights advocates of "pre-scientific" anthropomorphism, attributing traits to animals that are, he says, Beatrix Potter-like, where "only man is vile." It is within this fiction that the appeal of animal rights lies, he argues. The world of animals is non-judgmental, filled with dogs who return our affection almost no matter what we do to them, and cats who pretend to be affectionate when, in fact, they care only about themselves. It is, he argues, a fantasy, a world of escape.[6]		Evolutionary studies have provided explanations of altruistic behaviours in humans and nonhuman animals, and suggest similarities between humans and some nonhumans.[140] Scientists such as Jane Goodall and Richard Dawkins believe in the capacity of nonhuman great apes, humans' closest relatives, to possess rationality and self-awareness.[141]		In 2010, research led by psychologist Diana Reiss and zoologist Lori Marino was presented to a conference in San Diego, suggesting that dolphins are second in intelligence only to human beings, and concluded that they should be regarded as nonhuman persons. Marino used MRI scans to compare the dolphin and primate brain; she said the scans indicated there was "psychological continuity" between dolphins and humans. Reiss's research suggested that dolphins are able to solve complex problems, use tools, and pass the mirror test, using a mirror to inspect parts of their bodies.[142][143]		Studies have established links between interpersonal violence and animal cruelty.[144][145]		In Christian theology, the founder of the Methodist movement, John Wesley, was a Christian vegetarian and maintained "that animals had immortal souls and that there were considerable similarities between human and non-human animals."[146]		According to a paper published in 2000 by Harold Herzog and Lorna Dorr, previous academic surveys of attitudes towards animal rights have tended to suffer from small sample sizes and non-representative groups.[147] However, a number of factors appear to correlate with the attitude of individuals regarding the treatment of animals and animal rights. These include gender, age, occupation, religion, and level of education. There has also been evidence to suggest that prior experience with companion animals may be a factor in people's attitudes.[148]		Women are more likely to empathize with the cause of animal rights than men.[148][149] A 1996 study of adolescents by Linda Pifer suggested that factors that may partially explain this discrepancy include attitudes towards feminism and science, scientific literacy, and the presence of a greater emphasis on "nurturance or compassion" amongst women.[150]		A 2007 survey to examine whether or not people who believed in evolution were more likely to support animal rights than creationists and believers in intelligent design found that this was largely the case – according to the researchers, the respondents who were strong Christian fundamentalists and believers in creationism were less likely to advocate for animal rights than those who were less fundamentalist in their beliefs. The findings extended previous research, such as a 1992 study which found that 48% of animal rights activists were atheists or agnostic.[151][152]		Two surveys found that attitudes towards animal rights tactics, such as direct action, are very diverse within the animal rights communities. Near half (50% and 39% in two surveys) of activists do not support direct action. One survey concluded "it would be a mistake to portray animal rights activists as homogeneous."[148][153]		
A school counselor is a counselor and an educator who works in primary (elementary and middle) schools and/or secondary schools to provide academic, career, college access/affordability/admission, and social-emotional competencies to all students through a school counseling program. School counselors in most countries have at least a master's degree in school counseling and state and/or national certification.		The four main school counseling program interventions include school counseling curriculum classroom lessons and annual academic, career/college access/affordability/admission, and social-emotional planning for every student; and group and individual counseling for some students.[1] School counseling is an integral part of the education system in countries representing over half of the world's population and in other countries it is emerging as a critical support for elementary, middle, and high school learning and/or student health concerns.[2]		An outdated term for the profession was guidance counselor; school counselor is preferred due to school counselors' role in advocating for every child's academic, career, college readiness, and personal/social success in every elementary, middle, and high school.[3] In the Americas, Africa, Asia, Europe, and the Pacific, some countries with no formal school counseling programs use teachers or psychologists to do school counseling with a primary emphasis on career development.[4]		Countries vary in how a school counseling program and services are provided based on economics (funding for schools and school counseling programs), social capital (independent versus public schools), and school counselor certification and credentialing movements in education departments, professional associations, and national and local legislation.[4] In 2013, school counseling is established in 62 countries and emerging in another seven.[2]		An international scoping project on school-based counseling showed school counseling is mandatory in 39 countries, 32 USA states, one Australian state, 3 German states, 2 countries in the United Kingdom, and three provinces in Canada.[2] The largest accreditation body for Counselor Education/School Counseling programs is the Council for the Accreditation of Counseling and Related Educational Programs (CACREP).[5] International Counselor Education programs are accredited through a CACREP affiliate, the International Registry of Counselor Education Programs (IRCEP).		In some countries, school counseling is provided by school counseling specialists (for example, Botswana, China, Finland, Israel, Malta, Nigeria, Romania, Taiwan, Turkey, United States). In other cases, school counseling is provided by classroom teachers who either have such duties added to their typical teaching load or teach only a limited load that also includes school counseling activities (for example- India, Japan, Mexico, South Korea, Zambia).[4] The IAEVG focuses primarily on career development with some international school counseling articles and conference presentations.[4]		Both the IAEVG and the Vanguard of Counsellors have promoted school counseling internationally.[4]		After the collapse of the Soviet Union, the post-Soviet Psychologists of Armenia together with the Government started developing the institution of School Counselor within Armenian Schools.		While the national policy is supportive of school counseling, only one Australian state requires it. The school counselor to student ratio ranges from 1: 850 in the Australian Capital Territory to 1:18,000 in the state of Tasmania.[2] School counselors play an integral part in the Australian schooling system, they provide support to teachers, parents and students. Part of their care includes; counseling students, assisting parents/guardians to make informed decisions about their child's education, both learning and behavioral. Assist schools and parents in assessing disabilities and liaise with outside agencies to provide the best support for schools, teachers, students and parents.[6]		Austria mandates school counseling at the high school level.[2]		The Bahamas mandate school counseling.[2]		Although not mandated, there is some school counseling done in schools and in community centers in the three regions of the country.[2]		Botswana mandates school counseling.[2]		In Canada, most provinces[7] have adapted K-12 comprehensive school counseling programs similar to those initiated by[8] and adapted in the ASCA National Model.[9] School counselors reported in 2004 at a conference in Winnipeg on issues such as budget cuts, lack of clarity about school counselor roles, high student to school counselor ratios, especially in elementary schools, and how using a comprehensive school counseling model helped to clarify school counselor roles with teachers and administrators and strengthen the profession.[10] In 2009, The Canadian Counselling Association (CCA) became the Canadian Counselling and Psychotherapy Association (CCPA).[11] Three Canadian provinces require school counseling.[2]		CCPA established a page dedicated to the specific needs of Parenting, Children, and the Classroom called Counselling Connect located at http://www.ccpa-accp.ca/blog/?cat=9 [12][13]		China has put substantial financial resources into school counseling with strong growth in urban areas but less than 1% of rural students receiving it; China does not mandate school counseling.[2]		In China,[14] discussed the main influences on school counseling as being Chinese philosophers Confucius and Lao-Tsu, who provided early models of child and adult development[15] that later influenced the work of Abraham Maslow and Carl Rogers.[16]		Only 15% of high school students are admitted to college in China, so the entrance exams are fiercely competitive and those who do enter university graduate at a rate of 99%.[17] Much pressure is put on children and adolescents to study and be able to attend college and this pressure is a central school counseling focus in China. An additional stressor is that there are not enough places for students to attend college, and over 1/3 of college graduates cannot find jobs,[18] so career and employment counseling and development are central in school counseling.		There is a stigma related to personal or emotional problems and even though most universities and many schools now have counselors, there is a reluctance by many students to seek counseling for issues such as anxiety and depression. There is no national system of certifying school counselors. Most are trained in Western-developed cognitive methods including REBT, Rogerian, Family Systems, Behavior Modification, and Object Relations[19] and also recommend Chinese methods such as qi-gong (deep breathing), acupuncture, and music therapy.[14][20] shared that Chinese school counselors always work within a traditional Chinese world view of a community and family-based system that lessens the primacy of focus on the individual. In Hong Kong, Hui (2000) discussed work on moving toward comprehensive whole-school counseling programs and away from a remediation-style model.[21]		Middle school students are the priority for school counseling services in China.[2]		Costa Rica mandates school counseling.[2]		School counseling is only available in certain schools.[2]		In 1991 Cyprus mandated school counseling with a goal of 60 students to every school counselor and one full-time school counselor for every high school although neither of these goals has been accomplished fully.[2]		The Czech Republic mandates school counseling.[2]		Denmark mandates school counseling.[2]		School counseling services are delivered by school psychologists with a ratio of 1 school psychologist to every 3,080 students.[2]		School counseling is only available in certain schools.[2]		In Finland, legislation has been passed in terms of the school counseling system. The Basic Education Act of 1998 states that every student must receive school counseling services.[2] All Finnish school counselors must have a teaching certificate as well as master's degree in a specific subject and a specialized certificate in school counseling.[citation needed] Finland has a school counselor to student ratio of 1:245.[2]		France mandates school counseling in high schools.[2]		Gambia mandates school counseling.[2]		The school counselor to student ratio in Georgia is 1:615.[2]		One German state requires school counseling at all levels but high school counseling is established in all states.[2]		Ghana mandates school counseling [2]		There are provisions for academic and career counseling in middle and high schools but school counseling is not mandated and emotional/mental health counseling is done in community agencies.[2] The National Guidance Resources Center in Greece was first established by a team of researchers at the Athens University of Economics & Business (ASOEE) in 1993 under the leadership of Professor Emmanuel J. Yannakoudakis, Professor of Computer Science. The team received funding under the European Union (PETRA II Programme): The establishment of a national occupational guidance resources centre 1993-1994. The team organised a series of seminars and lectures to train the first career counsellors in Greece between 1993 – 1994. Further research projects at the Athens University of Economics & Business were carried as part of the European Union (LEONARDO Programme): a) A pilot project on the use of multimedia for career analysis, 1995-1999, b) Guidance towards the future, 1995-1999, c) On the move guidance system, 1996-2001 (this project was praised as «one of 200 outstanding projects from the first three years of the Leonardo da Vinci projects»), d) Eurostage for guidance systems, 1996-1999.		School counseling exists at the high school level.[2]		Hong Kong mandates school counseling.[2]		Iceland mandates school counseling.[2]		In India, Central Board of Secondary Education guidelines expect one school counselor will be appointed for every affiliated school,[22] but this is less than 3% of all Indian students attending public schools.[23] Confluence Educational Services Private Limited is into School Counselling services.		Indonesia mandates school counseling only in middle and high school.[2]		Middle school students are the priority for school counseling in Iran and it is mandated in high schools but there are not enough school counselors particularly in rural areas.[2]		In Ireland, school counseling began in County Dublin in the 1960s and went countrywide in the 1970s. However, legislation in the early 1980s severely curtailed the movement due to budget constraints. The main organization for school counseling profession is the IGC or Institute of Guidance Counsellors, which has a code of ethics.[24]		In Israel, a 2005 study by Erhard & Harel of 600 elementary, middle, and high school counselors found that a third of school counselors were delivering primarily traditional individual counseling services, about a third were delivering preventive classroom counseling curriculum lessons, and a third were delivering both individual counseling services and school counseling curriculum lessons in a more balanced or comprehensive developmental school counseling program; school counselor roles varied due to three elements: the school counselor's personal preferences, school level, and the principal's expectations.[25] Erhard & Harel stated that the profession in Israel, like many other countries, is transforming from various marginal and ancillary services to a comprehensive school counseling approach integral in the total school's education program.[25] in 2011-12, Israel had a school counselor to student ratio of 1:570 [2]		School counseling is not well developed in Italy.[2]		In Japan, school counseling is a very recent phenomenon with school counselors being introduced only in the mid-1990s and then often only part-time with a strong emphasis on assisting with behavioral issues.[26] Middle school students are the priority for school counseling in Japan and it is mandated.[2]		Jordan mandates school counseling having 1,950 school counselors working in 2011-12.[2]		School counseling was introduced in Latvia in 1929 but disappeared in World War II.[2]		In Lebanon, the government sponsored the first training of school counselors for public elementary and middle schools in 1996. There are now school counselors in about 1/5 of the elementary and middle schools in Lebanon and none in the high schools.[27] They have been trained in delivering preventive, developmental, and remedial services. Private schools have some school counselors serving all grade levels but the focus is exclusively individual counseling and primarily remedial.[28] Challenges include regular violence and wartime strife and not enough resources and a lack of a professional school counseling organization, assignment of school counselors to cover more than one school at a time, and only two school counseling graduate programs in the country. Last, for persons trained in Western models of school counseling there are dangers of overlooking unique cultural and family aspects of Lebanese society.[29]		School counseling was introduced in 1931 but disappeared during World War II.[2]		Macau mandates school counseling.[2]		Malaysia mandates school counseling only in middle/high school.[2]		In Malta, school counseling services began in 1968 in the Department of Education based on recommendations from a UNESCO consultant and used these titles: Education Officer, School Counsellor, and Guidance Teacher. Through the 1990s they included school counselor positions in primary and trade schools in addition to secondary schools. Guidance teachers are mandated at a 1:300 teacher to student ratio.[citation needed] Malta mandates school counseling.[2]		Nepal mandates school counseling.[2]		New Zealand mandates school counseling but since 1988 when education was decentralized, there has been a perceived decline in the prevalence of school counselors and the quality and service delivery of school counseling.[2]		In Nigeria, school counseling began in 1959 and exists in some high schools. It rarely exists at the elementary school level. Where there are federally funded secondary schools, there are some professionally trained school counselors. However, in many cases, there are only teachers who function as career masters/mistresses. School counselors often have teaching and other responsibilities that take time away from their school counseling tasks. The Counseling Association of Nigeria (CASSON) was formed in 1976 to promote the profession, but there is no code of ethics. However, a certification/licensure board has been formed. Aluede, Adomeh, & Afen-Akpaida (2004) discussed the overreliance on textbooks from the USA and the need for school counselors in Nigeria to take a whole-school approach and lessen the focus on individual approaches and honor the traditional African world view that values the family and community's roles in decision-making as paramount for effective decision-making in schools.[30]		Norway mandates school counseling.[2]		There is some evidence of school counseling services at the high school level.[2]		The Philippines mandates school counseling in middle and high school.[2] The Congress of the Philippines passed the Guidance and Counseling Act of 2004, with a specific focus on Professional Practice, Ethics, National Certification, and the creation of a Regulatory Body, and specialists in school counseling are subject to this law.[31]		School counseling was introduced in 1918 but disappeared during World War II.[2]		Portugal mandates school counseling at the high school level.[2]		Romania mandates school counseling.[2]		School counseling has focused primarily on trauma-based counseling [2]		School counseling is developing in Saudi Arabia. In 2010, 90% of high schools had some type of school counseling service.[2]		School counseling is available only in certain schools.[2]		Singapore mandates school counseling.[2]		Slovakia mandates school counseling.[2]		In South Korea, school counselors must teach a subject besides counseling, and not all school counselors are appointed to counseling positions, even though Korean law requires school counselors in all middle and high schools.[2][32]		Spain provides school counseling at the high school level although it is unclear if it is mandated.[2]		St. Kitts mandates school counseling.[2]		Sweden mandates school counseling.[2]		In Sweden, school counselor work was divided into two work-groups in the 1970s. The workgroups are called "kurator" and "studie- och yrkesvägledare." They both work with communication methodology but the kurator's work is more therapeutic, often psychological and social problems in schools, and the studie- och yrkesvägledare's work is future-focused with educational and vocational guidance. Studie- och yrkesvägledaren work in primary, secondary, adult education, higher education and various training centers and most have a Bachelor of Arts degree in Study and Career Guidance.		School counseling is found at the high school level.[2]		School counseling has focused primarily on trauma-based counseling of students that prior to the war was done in schools but is now found in either in a school club or refugee camp sponsored and staffed by UNICEF.[2]		In Taiwan, school counseling traditionally was done by "guidance teachers." Recent advocacy by the Chinese Guidance and Counseling Association pushed for licensure for school counselors in Taiwan's public schools. Prior to this time, the focus had been primarily individual and group counseling, play therapy,[33] career counseling and development,[34] and stress related to national university examinations.		Tanzania mandates school counseling [2]		The Thai government has put substantial funding into school counseling but does not mandate it.[2]		Trinidad and Tobago mandate school counseling.[2]		Turkey mandates school counseling and is well-established in all schools.[2]		Uganda mandates school counseling.[2]		There is some evidence of school counseling at the high school level in the United Arab Emirates.[2]		School counseling originated in the UK to support underachieving students and involved specialist training for teachers.[2] Two UK countries require school counseling.[2]		In the United States, the school counseling profession began with the vocational guidance movement at the beginning of the 20th century now known as career development. Jesse B. Davis was the first to provide a systematic school guidance program. In 1907, he became the principal of a high school and encouraged the school English teachers to use compositions and lessons to relate career interests, develop character, and avoid behavioral problems. Many others during this time also focused on what is now called career development. For example, in 1908, Frank Parsons, "Father of Vocational Guidance" established the Bureau of Vocational Guidance to assist young people in making the transition from school to work.		From the 1920s to the 1930s, school counseling grew because of the rise of progressive education in schools. This movement emphasized personal, social, moral development. Many schools reacted to this movement as anti-educational, saying that schools should teach only the fundamentals of education. This, combined with the economic hardship of the Great Depression, led to a decline in school counseling. In the 1940s, psychologists and counselors selected, recruited, and trained military personnel. This propelled the school counseling movement in schools by providing ways to test students and meet their needs. Schools accepted these military tests openly. Also, Carl Rogers' emphasis on helping relationships and a move away from directive "guidance" to nondirective or person-centered "counseling" influenced the profession of school counseling.		In the 1950s the government established the Guidance and Personnel Services Section in the Division of State and Local School Systems. In 1957, the Soviet Union launched Sputnik I. Out of concern that the Russians were winning the space race and that there were not enough scientists and mathematicians, the government passed the National Defense Education Act, spurring growth in vocational counseling through larger funding. In the 1960s, new legislation and professional developments refined the school counseling profession (Schmidt,[35] 2003).		The 1960s was also a time of great federal funding for land grant colleges and universities in establishing Counselor Education programs.[36] School counseling shifted from an exclusive focus on career development and added personal and social issues paralleling the rise of social justice and civil rights movements. In the early 1970s, Dr. Norm Gysbers began shifting the profession from school counselors as solitary professionals into having a comprehensive developmental school counseling program for all students K-12.[37] He and his colleagues' research evidenced strong correlations between fully implemented school counseling programs and student academic success; a critical part of the evidence base for the profession based on their work in the state of Missouri.[38] Dr. Chris Sink & associates showed similar evidence-based success for school counseling programs at the elementary and middle school levels in Washington State.		But school counseling in the 1980s and early 1990s was absent from educational reform efforts.[39] The profession was facing irrelevance as the standards-based educational movement gained strength with little evidence of systemic effectiveness for school counselors. In response,[40] consulted with elementary, middle, and high school counselors and created the ASCA Student Standards with three core domains (Academic, Career, Personal/Social), nine standards, and specific competencies and indicators for K-12 students.[41] A year later, the first systemic meta-analysis of school counseling was published focused on outcome research in academic, career, and personal/social domains.[42]		In the late 1990s, a former mathematics teacher, school counselor, and administrator, Pat Martin, was hired by The Education Trust to focus the school counseling profession on closing the achievement gap that harmed children and adolescents of color, poor and working class children and adolescents, bilingual children and adolescents and children and adolescents with disabilities. Martin developed focus groups of K-12 students, parents, guardians, teachers, building leaders, and superintendents, and interviewed professors of School Counselor Education. She hired a school counselor educator from Oregon State University, Dr. Reese House, and they co-created what emerged in 2003 as the National Center for Transforming School Counseling (NCTSC).		The NCTSC focused on both changing school counselor education at the graduate level and changing school counselor practice in local districts to teach school counselors how to prevent, intervene with, and close achievement and opportunity gaps. In the focus groups, they found what Hart & Jacobi[43] had indicated—-too many school counselors were gatekeepers for the status quo instead of advocates for the academic success of every child and adolescent. Too many school counselors used inequitable practices, supported inequitable school policies, and were unwilling to change.		This professional behavior kept many students from non-dominant backgrounds (i.e., students of color, poor and working class students, students with disabilities, and bilingual students) from getting the rigorous coursework and academic, career, and college access skills needed to successfully graduate from high school and pursue post-secondary options including college. They funded six $500,000 grants for six Counselor Education/School Counseling programs, with a special focus on rural and urban settings, to transform their school counseling programs to include a focus on teaching school counselor candidates advocacy, leadership, teaming and collaboration, equity assessment using data, and culturally competent program counseling and coordination in 1998 (Indiana State University, University of Georgia, University of West Georgia, University of California-Northridge, University of North Florida, and Ohio State University) and then over 25 other Counselor Education/School Counseling programs joined as companion institutions in the following decade. By 2008, NCTSC consultants had worked in over 100 school districts and major cities and rural areas to transform the work of school counselors.		In 2002, the American School Counselor Association released the first edition of the ASCA National Model: A framework for school counseling programs[44] comprising key school counseling components: ASCA National Standards, and the skill-based focus for closing achievement and opportunity gaps from the Education Trust's into one document. In 2003, the Center for School Counseling Outcome Research and Evaluation (CSCORE)[45] was developed as a clearinghouse for evidence-based practice with regular research briefs disseminated and original research projects.		In 2004, the ASCA Ethical Standards for School Counselors was revised to focus on issues of equity, closing achievement and opportunity gaps, and ensuring all K-12 students received access to a school counseling program.[46] Also in 2004, an equity-focused entity on school counselors' role in college readiness and admission counseling, the National Office for School Counselor Advocacy (NOSCA) emerged.[47] NOSCA developed research scholarships for research on college counseling by K-12 school counselors and how it is taught in School Counselor Education programs.		In 2008, the first NOSCA study was released by Dr. Jay Carey and colleagues focused on innovations in selected College Board "Inspiration Award" schools where school counselors collaborated inside and outside their schools for high college-going rates and strong college-going cultures in schools with large numbers of students of non-dominant backgrounds.[48] In 2008, ASCA released School Counseling Competencies focused on assisting school counseling programs to effectively implement the ASCA Model.[46][49]		In 2010, the Center for Excellence in School Counseling and Leadership (CESCAL) co-sponsored the first school counselor and educator conference devoted to the needs of lesbian, bisexual, gay, and transgender students in San Diego, California.[50]		In 2011, Counseling at the Crossroads: The perspectives and promise of school counselors in American education, the largest survey of high school and middle school counselors in the United States (over 5,300 interviews), was released by the College Board's National Office for School Counselor Advocacy, the National Association of Secondary School Principals, and the American School Counselor Association. The study shared school counselors' views on educational policies, practices, and reform, and how many of them, especially in urban and rural school settings, are not given the chance to focus on what they were trained to do, especially career and college access counseling and readiness for all students, in part due to high caseloads and inappropriate tasks that take up too much of their time. School counselors made strong suggestions about their crucial role in accountability and success for all students and how school systems need to change so that school counselors can be key players in student success. Implications for public policy and district and school-wide change are addressed.[51] The National Center for Transforming School Counseling at The Education Trust released a brief, Poised to Lead: How School Counselors Can Drive Career and College Readiness, challenging all schools to utilize school counselors for equity and access for rigorous courses for all students and ensuring college and career access skills and competencies be a major focus of the work of school counselors K-12.[52]		In 2012, the CSCORE assisted in evaluating and publishing six statewide research studies assessing the effectiveness of school counseling programs based on statewide systemic use of school counseling programs such as the ASCA National Model and their outcomes in Professional School Counseling.[53] Research indicated strong correlational evidence between lower school counseling ratios and better student success academically, in terms of career and college access/readiness/admission, and for various personal/social issues including school safety, reduced disciplinary issues, and better attendance in schools with fully implemented school counseling programs.[53]		Also in 2012, the American School Counselor Association released the third edition of the ASCA National Model.[54] Also, the National Center for Transforming School Counseling (NCTSC) created a School Counselor Educator Coalition to further transform graduate School Counselor Education programs in the new vision of school counseling for K-12 school counselors. Twenty universities were represented and four School Counselor Educator faculty mentors were named.		In 2013, Northern Kentucky University and the Center for School Counseling Outcome Research and Evaluation (CSCORE) launched an annual Evidence-Based School Counseling Conference with specific strands for school counselors, building and district leaders, and school counselor educators [55]		From 2014-16, the White House, under the leadership of the Office of the First Lady, Michelle Obama, partnered with key school counselor educators and college access professionals nationwide to bring national focus on the key roles of school counselors and college access professionals. Their collaboration resulted in a series of national Reach Higher/School Counseling and College Access convenings at Harvard University, San Diego State University, the University of North Florida, and American University. The First Lady also began the Better Make Room program to focus on college access for underrepresented students, and she began hosting the American School Counselor Association's School Counselor of the Year awards ceremony at the White House. The initiatives culminated in an unprecedented collaboration among multiple major professional associations focused on school counseling and college access including the American Counseling Association, the American School Counselor Association, the National Association for College Admission Counseling, the College Board, and ACT raising the profile and prominence of the role of school counselors collaborating on college access, affordability, and admission for all students. [56]		School counseling is mandated in Venezuela and it has focused on cultural competency.[2]		School counseling is mandated in Vietnam.[2]		Professional school counselors ideally implement a school counseling program that promotes and enhances student achievement (Hatch & Bowers, 2003, 2005; ASCA, 2012).[57] A framework for appropriate and inappropriate school counselor responsibilities and roles is outlined in the ASCA National Model (Hatch & Bowers, 2003, 2005; ASCA, 2012).[9] School counselors, in most USA states, usually have a master's degree in school counseling from a Counselor Education graduate program. In Canada, they must be licensed teachers with additional school counseling training and focus on academic, career, and personal/social issues. China requires at least three years of college experience. In Japan, school counselors were added in the mid-1990s, part-time, primarily focused on behavioral issues. In Taiwan, they are often teachers with recent legislation requiring school counseling licensure focused on individual and group counseling for academic, career, and personal issues. In Korea, school counselors are mandated in middle and high schools.		School counselors are employed in elementary, middle, and high schools, and in district supervisory settings and in counselor education faculty positions (usually with an earned Ph.D. in Counselor Education in the USA or related graduate doctorates abroad), and post-secondary settings doing academic, career, college readiness, and personal/social counseling, consultation, and program coordination. Their work includes a focus on developmental stages of student growth, including the needs, tasks, and student interests related to those stages(Schmidt,[35] 2003).		Professional school counselors meet the needs of student in three basic domains: academic development, career development, and personal/social development (Dahir & Campbell, 1997; Hatch & Bowers, 2003, 2005; ASCA, 2012) with an increased emphasis on college access.[58] Knowledge, understanding and skill in these domains are developed through classroom instruction, appraisal, consultation, counseling, coordination, and collaboration. For example, in appraisal, school counselors may use a variety of personality and career assessment methods (such as the[59] or[60] (based on the[61]) to help students explore career and college needs and interests.		School counselor interventions include individual and group counseling for some students. For example, if a student's behavior is interfering with his or her achievement, the school counselor may observe that student in a class, provide consultation to teachers and other stakeholders to develop (with the student) a plan to address the behavioral issue(s), and then collaborate to implement and evaluate the plan. They also provide consultation services to family members such as college access, career development, parenting skills, study skills, child and adolescent development, and help with school-home transitions.		School counselor interventions for all students include annual academic/career/college access planning K-12 and leading classroom developmental lessons on academic, career/college, and personal/social topics. The topics of character education, diversity and multiculturalism (Portman, 2009), and school safety are important areas of focus for school counselors. Often school counselors will coordinate outside groups that wish to help with student needs such as academics, or coordinate a program that teaches about child abuse or drugs, through on-stage drama (Schmidt,[35] 2003).		School counselors develop, implement, and evaluate school counseling programs that deliver academic, career, college access, and personal/social competencies to all students in their schools. For example, the ASCA National Model (Hatch & Bowers, 2003, 2005; ASCA, 2012)[57] includes the following four main areas:		The model (ASCA, 2012) is implemented using key skills from the Education Trust's Transforming School Counseling Initiative: Advocacy, Leadership, Teaming and Collaboration, and Systemic Change.		School Counselors are expected to follow a professional code of ethics in many countries. For example, In the USA, they are the American School Counselor Association (ASCA) School Counselor Ethical Code, the American Counseling Association (ACA) Code of Ethics., and the National Association for College Admission Counseling (NACAC) Statement of Principles of Good Practice (SPGP).[62]		School Counselors around the world are affiliated with various national and regional school counseling associations, and must abide by their guideline. These associations include:		Elementary school counselors provide[39] academic, career, college access, and personal and social competencies and planning to all students, and individual and group counseling for some students and their families to meet the developmental needs of young children K-6.[63] Transitions from pre-school to elementary school and from elementary school to middle school are an important focus for elementary school counselors. Increased emphasis is placed on accountability for closing achievement and opportunity gaps at the elementary level as more school counseling programs move to evidence-based work with data and specific results.[64]		School counseling programs that deliver specific competencies to all students help to close achievement and opportunity gaps.[65] To facilitate individual and group school counseling interventions, school counselors use developmental, cognitive-behavioral, person-centered (Rogerian) listening and influencing skills, systemic, family, multicultural,[66] narrative, and play therapy theories and techniques.[67][68] released a research study showing the effectiveness of elementary school counseling programs in Washington state.		Middle school counselors provide school counseling curriculum lessons[39] on academic, career, college access, and personal and social competencies, advising and academic/career/college access planning to all students and individual and group counseling for some students and their families to meet the needs of older children/early adolescents in grades 7 and 8.[9]		Middle School College Access curricula have been developed by The College Board to assist students and their families well before reaching high school. To facilitate the school counseling process, school counselors use theories and techniques including developmental, cognitive-behavioral, person-centered (Rogerian) listening and influencing skills, systemic, family, multicultural,[66] narrative, and play therapy. Transitional issues to ensure successful transitions to high school are a key area including career exploration and assessment with seventh and eighth grade students.[69] Sink, Akos, Turnbull, & Mvududu released a study in 2008 confirming the effectiveness of middle school comprehensive school counseling programs in Washington state.[70]		High school counselors provide[39] academic, career, college access, and personal and social competencies with developmental classroom lessons and planning to all students, and individual and group counseling for some students and their families to meet the developmental needs of adolescents (Hatch & Bowers, 2003, 2005, 2012).[49] Emphasis is on college access counseling at the early high school level as more school counseling programs move to evidence-based work with data and specific results[64] that show how school counseling programs help to close achievement, opportunity, and attainment gaps ensuring all students have access to school counseling programs and early college access activities.[71] The breadth of demands high school counselors face, from educational attainment (high school graduation and some students' preparation for careers and college) to student social and mental health, has led to ambiguous role definition.[72] Summarizing a 2011 national survey of more than 5,300 middle school and high school counselors, researchers argued: "Despite the aspirations of counselors to effectively help students succeed in school and fulfill their dreams, the mission and roles of counselors in the education system must be more clearly defined; schools must create measures of accountability to track their effectiveness; and policymakers and key stakeholders must integrate counselors into reform efforts to maximize their impact in schools across America".[73]		Transitional issues to ensure successful transitions to college, other post-secondary educational options, and careers are a key area.[74] The high school counselor helps students and their families prepare for post-secondary education including college and careers (e.g. college, careers) by engaging students and their families in accessing and evaluating accurate information on what the National Office for School Counselor Advocacy calls the 8 essential elements of college and career counseling: (1) College Aspirations, (2) Academic Planning for Career and College Readiness, (3) Enrichment and Extracurricular Engagement, (4) College and Career Exploration and Selection Processes, (5) College and Career Assessments, (6) College Affordability Planning, (7) College and Career Admission Processes, and (8) Transition from High School Graduation to College Enrollment.[75] Some students turn to private college admissions advisors but there is no research evidence that private college admissions advisors have any effectiveness in assisting students attain selective college admissions.		Lapan, Gysbers & Sun showed correlational evidence of the effectiveness of fully implemented school counseling programs on high school students' academic success.[76] Carey et al.'s 2008 study showed specific best practices from high school counselors raising college-going rates within a strong college-going environment in multiple USA-based high schools with large numbers of students of nondominant cultural identities.		The education of school counselors (school counsellors) around the world varies based on the laws and cultures of countries and the historical influences of their educational and credentialing systems and professional identities related to who delivers academic, career, college readiness, and personal/social information, advising, curriculum, and counseling and related services.[4]		In Canada, school counselors must be certified teachers with additional school counseling training.		In China, there is no national certification or licensure system for school counselors.		Korea requires school counselors in all middle and high schools.[77]		In the Philippines, school counselors must be licensed with a master's degree in counseling.[78]		Taiwan instituted school counselor licensure for public schools (2006) through advocacy from the[79]		In the USA, a school counselor is a certified educator with a master's degree in school counseling (usually from a Counselor Education graduate program) with school counseling graduate training including qualifications and skills to address all students’ academic, career, college access and personal/social needs. Once you have completed your master's degree you can take one of 2 certification options in order to become fully licensed as a professional school counselor.[80]		About half of all Counselor Education programs that offer school counseling are accredited by the Council on the Accreditation of Counseling and Related Educational Programs (CACREP) and all are in the USA with one in Canada. In 2010 one was under review in Mexico. CACREP maintains a current list of accredited programs and programs in the accreditation process on their website.[81] CACREP desires to accredit more international counseling university programs.[81]		According to CACREP, an accredited school counseling program offers coursework in Professional Identity and Ethics, Human Development, Counseling Theories, Group Work, Career Counseling, Multicultural Counseling, Assessment, Research and Program Evaluation, and Clinical Coursework—a 100-hour practicum and a 600-hour internship under supervision of a school counseling faculty member and a certified school counselor site supervisor (CACREP,[82] 2001).		When CACREP released the 2009 Standards, the accreditation process became performance-based including evidence of school counselor candidate learning outcomes. In addition, CACREP tightened the school counseling standards with specific evidence needed for how school counseling students receive education in foundations; counseling prevention and intervention; diversity and advocacy; assessment; research and evaluation; academic development; collaboration and consultation; and leadership in K-12 school counseling contexts.[83]		Certification practices for school counselors vary around the world. School counselors in the USA may opt for national certification through two different boards. The National Board for Professional Teaching Standards (NBPTS) requires a two-to-three year process of performance based assessment, and demonstrate (in writing) content knowledge in human growth/development, diverse populations, school counseling programs, theories, data, and change and collaboration.[84] In February 2005, 30 states offered financial incentives for this certification.		Also in the USA, The National Board for Certified Counselors (NBCC) requires passing the National Certified School Counselor Examination (NCSC), including 40 multiple choice questions and seven simulated cases assessing school counselors' abilities to make critical decisions. Additionally, a master's degree and three years of supervised experience are required. NBPTS also requires three years of experience, however state certification is required (41 of 50 states require a master's degree). At least four states offer financial incentives for the NCSC certification.[85][86][87][88][89]		The rate of job growth and earnings for school counselors depends on the country that one is employed in and how the school is funded—public or independent. School counselors working in international schools or "American" schools globally may find similar work environments and expectations to the USA. School counselor pay varies based on school counselor roles, identity, expectations, and legal and certification requirements and expectations of each country. According to the Occupational Outlook Handbook (OOH), the median salary for school counselors in the USA in 2010 was (USD) $53,380 or $25.67 hourly. According to an infographic designed by Wake Forest University, the median salary of school counselors in the US was $43,690.[90] The USA has 267,000 employees in titles such as school counselor or related titles in education and advising and college and career counseling. The projected growth for school counselors is 14-19% or faster than average than other occupations in the USA with a predicted 94,000 job openings from 2008-2018.[91][92] In Australia, a survey by the Australian Guidance and Counseling Association found that school counselor salary ranged from (AUD) the high 50,000s to the mid 80,000s.		Among all counseling specialty areas, public elementary, middle and high school counselors are (2009) paid the highest salary on average of all counselors. Budget cuts, however, have affected placement of public school counselors in Canada, Ireland, the United States, and other countries due to the global recession in recent years. In the United States, rural areas and urban areas traditionally have been under-served by school counselors in public schools due to both funding shortages and often a lack of best practice models. With the advent of No Child Left Behind legislation in the USA and a mandate for school counselors to be working with data and showing evidence-based practice, school counselors able to show and share results in assisting to close gaps are in the best position to argue for increased school counseling resources and positions for their programs (Hatch & Bowers, 2003, 2005; ASCA, 2012).[57]		American school counseling for developing a Korean school counseling model. Korean Journal of Counseling Psychology, 19, 539-567.		
The Leningrad première of Shostakovich's Symphony No. 7 took place on 9 August 1942 during the Second World War, while the city (now Saint Petersburg) was under siege by Nazi German forces. Dmitri Shostakovich (pictured) had intended for the piece to be premièred by the Leningrad Philharmonic Orchestra, but they had been evacuated because of the siege, along with the composer, and the world première was instead held in Kuybyshev. The Leningrad première was performed by the surviving musicians of the Leningrad Radio Orchestra, supplemented with military performers. Most of the musicians were starving, and three died during rehearsals. Supported by a Soviet military offensive intended to silence German forces, the performance was a success, prompting an hour-long ovation. The symphony was broadcast to the German lines by loudspeaker as a form of psychological warfare. The Leningrad première was considered by music critics to be one of the most important artistic performances of the war because of its psychological and political effects. Reunion concerts featuring surviving musicians were convened in 1964 and 1992 to commemorate the event. (Full article...)		August 9: International Day of the World's Indigenous Peoples; National Women's Day in South Africa		Hieronymus Bosch (d. 1516) · Elizabeth Schuyler Hamilton (b. 1757) · Gillian Anderson (b. 1968)		Marina City is a mixed-use residential/commercial building complex in Chicago, Illinois. It occupies almost an entire city block on State Street and sits on the north bank of the Chicago River in downtown Chicago, directly across from the Loop. The complex consists of two corncob-shaped, 587-foot (179 m), 65-story towers, as well as a saddle-shaped auditorium building and a mid-rise hotel building. Designed by Bertrand Goldberg, Marina City was the first building in the United States to be constructed with tower cranes.		Photograph: Diego Delso		Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:		This Wikipedia is written in English. Started in 2001 (2001), it currently contains 5,456,578 articles. Many other Wikipedias are available; some of the largest are listed below.		
New Zealand /njuːˈziːlənd/ ( listen) (Māori: Aotearoa [aɔˈtɛaɾɔa]) is an island nation in the southwestern Pacific Ocean. The country geographically comprises two main landmasses—the North Island (or Te Ika-a-Māui), and the South Island (or Te Waipounamu)—and around 600 smaller islands. New Zealand is situated some 1,500 kilometres (900 mi) east of Australia across the Tasman Sea and roughly 1,000 kilometres (600 mi) south of the Pacific island areas of New Caledonia, Fiji, and Tonga. Because of its remoteness, it was one of the last lands to be settled by humans. During its long period of isolation, New Zealand developed a distinct biodiversity of animal, fungal and plant life. The country's varied topography and its sharp mountain peaks, such as the Southern Alps, owe much to the tectonic uplift of land and volcanic eruptions. New Zealand's capital city is Wellington, while its most populous city is Auckland.		Sometime between 1250 and 1300 CE, Polynesians settled in the islands that later were named New Zealand and developed a distinctive Māori culture. In 1642, Dutch explorer Abel Tasman became the first European to sight New Zealand. In 1840, representatives of Britain and Māori chiefs signed the Treaty of Waitangi, which declared British sovereignty over the islands. In 1841, New Zealand became a colony within the British Empire and in 1907 it became a Dominion. Today, the majority of New Zealand's population of 4.7 million is of European descent; the indigenous Māori are the largest minority, followed by Asians and Pacific Islanders. Reflecting this, New Zealand's culture is mainly derived from Māori and early British settlers, with recent broadening arising from increased immigration. The official languages are English, Māori and New Zealand Sign Language, with English predominant.		New Zealand is a developed country and ranks highly in international comparisons of national performance, such as health, education, economic freedom and quality of life. Since the 1980s, New Zealand has transformed from an agrarian, regulated economy to a market economy. Nationally, legislative authority is vested in an elected, unicameral Parliament, while executive political power is exercised by the Cabinet, led by the Prime Minister, who is currently Bill English. Queen Elizabeth II is the country's head of state and is represented by a governor-general, currently Dame Patsy Reddy. In addition, New Zealand is organised into 11 regional councils and 67 territorial authorities for local government purposes. The Realm of New Zealand also includes Tokelau (a dependent territory); the Cook Islands and Niue (self-governing states in free association with New Zealand); and the Ross Dependency, which is New Zealand's territorial claim in Antarctica. New Zealand is a member of the United Nations, Commonwealth of Nations, ANZUS, Organisation for Economic Co-operation and Development, Pacific Islands Forum, and Asia-Pacific Economic Cooperation.		Dutch explorer Abel Tasman sighted New Zealand in 1642 and called it Staten Landt, supposing it was connected to a landmass of the same name at the southern tip of South America.[12] In 1645, Dutch cartographers renamed the land Nova Zeelandia after the Dutch province of Zeeland.[13][14] British explorer James Cook subsequently anglicised the name to New Zealand.[15][16]		Aotearoa (often translated as "land of the long white cloud")[17] is the current Māori name for New Zealand. It is unknown whether Māori had a name for the whole country before the arrival of Europeans, with Aotearoa originally referring to just the North Island.[18] Māori had several traditional names for the two main islands, including Te Ika-a-Māui (the fish of Māui) for the North Island and Te Waipounamu (the waters of greenstone) or Te Waka o Aoraki (the canoe of Aoraki) for the South Island.[19] Early European maps labelled the islands North (North Island), Middle (South Island) and South (Stewart Island / Rakiura).[20] In 1830, maps began to use North and South to distinguish the two largest islands and by 1907, this was the accepted norm.[16] The New Zealand Geographic Board discovered in 2009 that the names of the North Island and South Island had never been formalised, and names and alternative names were formalised in 2013. This set the names as North Island or Te Ika-a-Māui, and South Island or Te Waipounamu.[21] For each island, either its English or Māori name can be used, or both can be used together.[21]		New Zealand was one of the last major landmasses settled by humans. Radiocarbon dating, evidence of deforestation[23] and mitochondrial DNA variability within Māori populations[24] suggest New Zealand was first settled by Eastern Polynesians between 1250 and 1300,[19][25] concluding a long series of voyages through the southern Pacific islands.[26] Over the centuries that followed, these settlers developed a distinct culture now known as Māori. The population was divided into iwi (tribes) and hapū (subtribes) who would sometimes cooperate, sometimes compete and sometimes fight against each other.[27] At some point a group of Māori migrated to Rēkohu, now known as the Chatham Islands, where they developed their distinct Moriori culture.[28][29] The Moriori population was all but wiped out between 1835 and 1862, largely because of Taranaki Māori invasion and enslavement in the 1830s, although European diseases also contributed. In 1862 only 101 survived, and the last known full-blooded Moriori died in 1933.[30]		The first Europeans known to have reached New Zealand were Dutch explorer Abel Tasman and his crew in 1642.[31] In a hostile encounter, four crew members were killed and at least one Māori was hit by canister shot.[32] Europeans did not revisit New Zealand until 1769 when British explorer James Cook mapped almost the entire coastline.[31] Following Cook, New Zealand was visited by numerous European and North American whaling, sealing and trading ships. They traded European food, metal tools, weapons and other goods for timber, Māori food, artefacts and water.[33] The introduction of the potato and the musket transformed Māori agriculture and warfare. Potatoes provided a reliable food surplus, which enabled longer and more sustained military campaigns.[34] The resulting intertribal Musket Wars encompassed over 600 battles between 1801 and 1840, killing 30,000–40,000 Māori.[35] From the early 19th century, Christian missionaries began to settle New Zealand, eventually converting most of the Māori population.[36] The Māori population declined to around 40% of its pre-contact level during the 19th century; introduced diseases were the major factor.[37]		In 1788 Captain Arthur Phillip assumed the position of Governor of the new British colony of New South Wales which according to his commission included New Zealand.[38] The British Government appointed James Busby as British Resident to New Zealand in 1832 following a petition from northern Māori.[39] In 1835, following an announcement of impending French settlement by Charles de Thierry, the nebulous United Tribes of New Zealand sent a Declaration of Independence to King William IV of the United Kingdom asking for protection.[39] Ongoing unrest, the proposed settlement of New Zealand by the New Zealand Company (which had already sent its first ship of surveyors to buy land from Māori) and the dubious legal standing of the Declaration of Independence prompted the Colonial Office to send Captain William Hobson to claim sovereignty for the United Kingdom and negotiate a treaty with the Māori.[40] The Treaty of Waitangi was first signed in the Bay of Islands on 6 February 1840.[41] In response to the New Zealand Company's attempts to establish an independent settlement in Wellington[42] and French settlers purchasing land in Akaroa,[43] Hobson declared British sovereignty over all of New Zealand on 21 May 1840, even though copies of the Treaty were still circulating throughout the country for Māori to sign.[44] With the signing of the Treaty and declaration of sovereignty the number of immigrants, particularly from the United Kingdom, began to increase.[45]		New Zealand, still part of the colony of New South Wales, became a separate Colony of New Zealand on 1 July 1841.[46] The colony gained a representative government in 1852 and the first Parliament met in 1854.[47] In 1856 the colony effectively became self-governing, gaining responsibility over all domestic matters other than native policy.[47] (Control over native policy was granted in the mid-1860s.[47]) Following concerns that the South Island might form a separate colony, premier Alfred Domett moved a resolution to transfer the capital from Auckland to a locality near the Cook Strait.[48] Wellington was chosen for its central location, with Parliament officially sitting there for the first time in 1865.[49] As immigrant numbers increased, conflicts over land led to the New Zealand Wars of the 1860s and 1870s, resulting in the loss and confiscation of much Māori land.[50]		In 1891 the Liberal Party, led by John Ballance,[51] came to power as the first organised political party.[52] The Liberal Government, later led by Richard Seddon,[53] passed many important social and economic measures. In 1893 New Zealand was the first nation in the world to grant all women the right to vote[52] and in 1894 pioneered the adoption of compulsory arbitration between employers and unions.[54]		In 1907, at the request of the New Zealand Parliament, King Edward VII proclaimed New Zealand a Dominion within the British Empire, reflecting its self-governing status.[55] In 1947 the country adopted the Statute of Westminster, confirming that the British Parliament could no longer legislate for New Zealand without the consent of New Zealand.[47]		Early in the 20th century, New Zealand was involved in world affairs, fighting in the First and Second World Wars[56] and suffering through the Great Depression.[57] The depression led to the election of the First Labour Government and the establishment of a comprehensive welfare state and a protectionist economy.[58] New Zealand experienced increasing prosperity following the Second World War[59] and Māori began to leave their traditional rural life and move to the cities in search of work.[60] A Māori protest movement developed, which criticised Eurocentrism and worked for greater recognition of Māori culture and of the Treaty of Waitangi.[61] In 1975, a Waitangi Tribunal was set up to investigate alleged breaches of the Treaty, and it was enabled to investigate historic grievances in 1985.[41] The government has negotiated settlements of these grievances with many iwi,[62] although Māori claims to the foreshore and seabed have proved controversial in the 2000s.[63][64]		New Zealand is a constitutional monarchy with a parliamentary democracy,[65] although its constitution is not codified.[66] Elizabeth II is the Queen of New Zealand and the head of state.[67] The Queen is represented by the Governor-General, whom she appoints on the advice of the Prime Minister.[68] The Governor-General can exercise the Crown's prerogative powers, such as reviewing cases of injustice and making appointments of ministers, ambassadors and other key public officials,[69] and in rare situations, the reserve powers (e.g. the power to dissolve Parliament or refuse the Royal Assent of a bill into law).[70] The powers of the Queen and the Governor-General are limited by constitutional constraints and they cannot normally be exercised without the advice of ministers.[70]		The New Zealand Parliament holds legislative power and consists of the Queen and the House of Representatives.[71] It also included an upper house, the Legislative Council, until this was abolished in 1950.[71] The supremacy of Parliament, over the Crown and other government institutions, was established in England by the Bill of Rights 1689 and has been ratified as law in New Zealand.[71] The House of Representatives is democratically elected and a government is formed from the party or coalition with the majority of seats. If no majority is formed, a minority government can be formed if support from other parties during confidence and supply votes is assured.[71] The Governor-General appoints ministers under advice from the Prime Minister, who is by convention the parliamentary leader of the governing party or coalition.[72] Cabinet, formed by ministers and led by the Prime Minister, is the highest policy-making body in government and responsible for deciding significant government actions.[73] Members of Cabinet make major decisions collectively, and are therefore collectively responsible for the consequences of these decisions.[74]		A parliamentary general election must be called no later than three years after the previous election.[75] Almost all general elections between 1853 and 1993 were held under the first-past-the-post voting system.[76] Since the 1996 election, a form of proportional representation called mixed-member proportional (MMP) has been used.[66] Under the MMP system, each person has two votes; one is for electorate seats (including some reserved for Māori),[77] and the other is for a party. Since the 2014 election, there have been 71 electorates (which includes 7 Māori electorates), and the remaining 49 seats are assigned so that representation in parliament reflects the party vote, although a party has to win one electorate or 5% of the total party vote before it is eligible for these seats.[78]		Elections since the 1930s have been dominated by two political parties, National and Labour.[76] Between March 2005 and August 2006, New Zealand became the first country in the world in which all the highest offices in the land—Head of State, Governor-General, Prime Minister, Speaker and Chief Justice—were occupied simultaneously by women.[79] The current Prime Minister is Bill English, since December 2016.[80] The incumbent National Government won a third term in office following the 2014 election.[81]		New Zealand's judiciary, headed by the Chief Justice,[82] includes the Supreme Court, Court of Appeal, the High Court, and subordinate courts.[66] Judges and judicial officers are appointed non-politically and under strict rules regarding tenure to help maintain judicial independence.[66] This theoretically allows the judiciary to interpret the law based solely on the legislation enacted by Parliament without other influences on their decisions.[83]		New Zealand is identified as one of the world's most stable and well-governed states,[84][85] with high government transparency and among the lowest perceived levels of corruption.[86] The country rates highly for civic participation in the political process, with 77% voter turnout during the most recent elections, compared to an OECD average of 68%.[87]		Early colonial New Zealand allowed the British Government to determine external trade and be responsible for foreign policy.[88] The 1923 and 1926 Imperial Conferences decided that New Zealand should be allowed to negotiate its own political treaties and the first commercial treaty was ratified in 1928 with Japan. On 3 September 1939 New Zealand allied itself with Britain and declared war on Germany with Prime Minister Michael Joseph Savage proclaiming, "Where she goes, we go; where she stands, we stand."[89]		In 1951 the United Kingdom became increasingly focused on its European interests,[90] while New Zealand joined Australia and the United States in the ANZUS security treaty.[91] The influence of the United States on New Zealand weakened following protests over the Vietnam War,[92] the refusal of the United States to admonish France after the sinking of the Rainbow Warrior,[93] disagreements over environmental and agricultural trade issues and New Zealand's nuclear-free policy.[94][95] Despite the United States' suspension of ANZUS obligations the treaty remained in effect between New Zealand and Australia, whose foreign policy has followed a similar historical trend.[96] Close political contact is maintained between the two countries, with free trade agreements and travel arrangements that allow citizens to visit, live and work in both countries without restrictions.[97] In 2013[update] there were about 650,000 New Zealand citizens living in Australia, which is equivalent to 15% of the population of New Zealand.[98]		New Zealand has a strong presence among the Pacific Island countries. A large proportion of New Zealand's aid goes to these countries and many Pacific people migrate to New Zealand for employment.[99] Permanent migration is regulated under the 1970 Samoan Quota Scheme and the 2002 Pacific Access Category, which allow up to 1,100 Samoan nationals and up to 750 other Pacific Islanders respectively to become permanent New Zealand residents each year. A seasonal workers scheme for temporary migration was introduced in 2007 and in 2009 about 8,000 Pacific Islanders were employed under it.[100] New Zealand is involved in the Pacific Islands Forum, Asia-Pacific Economic Cooperation and the Association of Southeast Asian Nations Regional Forum (including the East Asia Summit).[97] New Zealand is a member of the United Nations,[101] the Commonwealth of Nations[102] and the Organisation for Economic Co-operation and Development (OECD),[103] and participates in the Five Power Defence Arrangements.[104]		The New Zealand Defence Force consists of three services: the New Zealand Army; the Royal New Zealand Air Force; and the Royal New Zealand Navy.[105] New Zealand's national defence needs are modest because of the unlikelihood of direct attack,[106] although it does have a global presence. The country fought in both world wars, with notable campaigns in Gallipoli, Crete,[107] El Alamein[108] and Cassino.[109] The Gallipoli campaign played an important part in fostering New Zealand's national identity[110][111] and strengthened the ANZAC tradition it shares with Australia.[112]		In addition to Vietnam and the two world wars, New Zealand fought in the Second Boer War,[113] the Korean War,[114] the Malayan Emergency,[115] the Gulf War and the Afghanistan War. It has contributed forces to several regional and global peacekeeping missions, such as those in Cyprus, Somalia, Bosnia and Herzegovina, the Sinai, Angola, Cambodia, the Iran–Iraq border, Bougainville, East Timor, and the Solomon Islands.[116]		New Zealand ranks eighth in the Center for Global Development's 2015 Commitment to Development Index, which ranks the world's most developed countries on their dedication to policies that benefit poorer nations.[117] New Zealand is considered the fourth most peaceful country in the world according to the 2016 Global Peace Index.[118]		The early European settlers divided New Zealand into provinces, which had a degree of autonomy.[119] Because of financial pressures and the desire to consolidate railways, education, land sales and other policies, government was centralised and the provinces were abolished in 1876.[120] The provinces are remembered in regional public holidays[121] and sporting rivalries.[122]		Since 1876, various councils have administered local areas under legislation determined by the central government.[119][123] In 1989, the government reorganised local government into the current two-tier structure of regional councils and territorial authorities.[124] The 249 municipalities[124] that existed in 1975 have now been consolidated into 67 territorial authorities and 11 regional councils.[125] The regional councils' role is to regulate "the natural environment with particular emphasis on resource management",[124] while territorial authorities are responsible for sewage, water, local roads, building consents and other local matters.[126][127] Five of the territorial councils are unitary authorities and also act as regional councils.[127] The territorial authorities consist of 13 city councils, 53 district councils, and the Chatham Islands Council. While officially the Chatham Islands Council is not a unitary authority, it undertakes many functions of a regional council.[128]		New Zealand is one of 16 realms within the Commonwealth.[129][65] The Realm of New Zealand is the entire area over which the Queen of New Zealand is sovereign, and comprises New Zealand, Tokelau, the Ross Dependency, the Cook Islands and Niue.[65] The Cook Islands and Niue are self-governing states in free association with New Zealand.[130][131] The New Zealand Parliament cannot pass legislation for these countries, but with their consent can act on behalf of them in foreign affairs and defence. Tokelau is a non-self-governing territory, but is administered by a council of three elders (one from each Tokelauan atoll).[132] The Ross Dependency is New Zealand's territorial claim in Antarctica, where it operates the Scott Base research facility.[133] New Zealand nationality law treats all parts of the realm equally, so most people born in New Zealand, the Cook Islands, Niue, Tokelau and the Ross Dependency are New Zealand citizens.[134][n 7]		New Zealand is located near the centre of the water hemisphere and is made up of two main islands and a number of smaller islands. The two main islands (the North Island, or Te Ika-a-Māui, and the South Island, or Te Waipounamu) are separated by Cook Strait, 22 kilometres (14 mi) wide at its narrowest point.[136] Besides the North and South Islands, the five largest inhabited islands are Stewart Island, Chatham Island, Great Barrier Island (in the Hauraki Gulf),[137] d'Urville Island (in the Marlborough Sounds)[138] and Waiheke Island (about 22 km (14 mi) from central Auckland).[139]		New Zealand is long and narrow (over 1,600 kilometres (990 mi) along its north-north-east axis with a maximum width of 400 kilometres (250 mi)),[140] with about 15,000 km (9,300 mi) of coastline[141] and a total land area of 268,000 square kilometres (103,500 sq mi).[142] Because of its far-flung outlying islands and long coastline, the country has extensive marine resources. Its exclusive economic zone is one of the largest in the world, covering more than 15 times its land area.[143]		The South Island is the largest landmass of New Zealand and is the 12th largest island in the world. It is divided along its length by the Southern Alps.[144] There are 18 peaks over 3,000 metres (9,800 ft), the highest of which is Aoraki / Mount Cook at 3,754 metres (12,316 ft).[145] Fiordland's steep mountains and deep fiords record the extensive ice age glaciation of this south-western corner of the South Island.[146] The North Island is the 14th largest island in the world and is less mountainous but is marked by volcanism.[147] The highly active Taupo Volcanic Zone has formed a large volcanic plateau, punctuated by the North Island's highest mountain, Mount Ruapehu (2,797 metres (9,177 ft)). The plateau also hosts the country's largest lake, Lake Taupo,[148] nestled in the caldera of one of the world's most active supervolcanoes.[149]		The country owes its varied topography, and perhaps even its emergence above the waves, to the dynamic boundary it straddles between the Pacific and Indo-Australian Plates.[150] New Zealand is part of Zealandia, a microcontinent nearly half the size of Australia that gradually submerged after breaking away from the Gondwanan supercontinent.[151] About 25 million years ago, a shift in plate tectonic movements began to contort and crumple the region. This is now most evident in the Southern Alps, formed by compression of the crust beside the Alpine Fault. Elsewhere the plate boundary involves the subduction of one plate under the other, producing the Puysegur Trench to the south, the Hikurangi Trench east of the North Island, and the Kermadec and Tonga Trenches[152] further north.[150]		New Zealand is part of Australasia, and also forms the south-western extremity of Polynesia.[153] The term Oceania is often used to denote the region encompassing the Australian continent, New Zealand and various islands in the Pacific Ocean that are not included in the seven-continent model.[154]		Rural scene near Queenstown		The Emerald Lakes, Mt Tongariro		Lake Gunn		Pencarrow Head, Wellington		New Zealand's climate is predominantly temperate maritime (Köppen: Cfb) with mean annual temperatures ranging from 10 °C (50 °F) in the south to 16 °C (61 °F) in the north.[155] Historical maxima and minima are 42.4 °C (108.32 °F) in Rangiora, Canterbury and −25.6 °C (−14.08 °F) in Ranfurly, Otago.[156] Conditions vary sharply across regions from extremely wet on the West Coast of the South Island to almost semi-arid in Central Otago and the Mackenzie Basin of inland Canterbury and subtropical in Northland.[157] Of the seven largest cities, Christchurch is the driest, receiving on average only 640 millimetres (25 in) of rain per year and Wellington the wettest, receiving almost twice that amount.[158] Auckland, Wellington and Christchurch all receive a yearly average of more than 2,000 hours of sunshine. The southern and south-western parts of the South Island have a cooler and cloudier climate, with around 1,400–1,600 hours; the northern and north-eastern parts of the South Island are the sunniest areas of the country and receive about 2,400–2,500 hours.[159] The general snow season is early June until early October, though cold snaps can occur outside this season.[160] Snowfall is common in the eastern and southern parts of the South Island and mountain areas across the country.[155]		Autumn in Wellington		Central Plateau in winter		Spring in Hagley Park, Christchurch		Scorching Bay beach in summer		The table below lists climate normals for the warmest and coldest months in New Zealand's six largest cities. North Island cities are generally warmest in February. South Island cities are warmest in January.		New Zealand's geographic isolation for 80 million years[162] and island biogeography has influenced evolution of the country's species of animals, fungi and plants. Physical isolation has not caused biological isolation, and this has resulted in a dynamic evolutionary ecology with examples of very distinctive plants and animals as well as populations of widespread species.[163][164] About 82% of New Zealand's indigenous vascular plants are endemic, covering 1,944 species across 65 genera and includes a single endemic family.[165][166] The number of fungi recorded from New Zealand, including lichen-forming species, is not known, nor is the proportion of those fungi which are endemic, but one estimate suggests there are about 2,300 species of lichen-forming fungi in New Zealand[165] and 40% of these are endemic.[167] The two main types of forest are those dominated by broadleaf trees with emergent podocarps, or by southern beech in cooler climates.[168] The remaining vegetation types consist of grasslands, the majority of which are tussock.[169]		Before the arrival of humans, an estimated 80% of the land was covered in forest, with only high alpine, wet, infertile and volcanic areas without trees.[170] Massive deforestation occurred after humans arrived, with around half the forest cover lost to fire after Polynesian settlement.[171] Much of the remaining forest fell after European settlement, being logged or cleared to make room for pastoral farming, leaving forest occupying only 23% of the land.[172]		The forests were dominated by birds, and the lack of mammalian predators led to some like the kiwi, kakapo, weka and takahē evolving flightlessness.[173] The arrival of humans, associated changes to habitat, and the introduction of rats, ferrets and other mammals led to the extinction of many bird species, including large birds like the moa and Haast's eagle.[174][175]		Other indigenous animals are represented by reptiles (tuatara, skinks and geckos),[176] frogs, spiders,[177] insects (weta)[178] and snails.[179] Some, such as the tuatara, are so unique that they have been called living fossils.[180] Three species of bats (one since extinct) were the only sign of native land mammals in New Zealand until the 2006 discovery of bones from a unique, mouse-sized land mammal at least 16 million years old.[181][182] Marine mammals however are abundant, with almost half the world's cetaceans (whales, dolphins, and porpoises) and large numbers of fur seals reported in New Zealand waters.[183] Many seabirds breed in New Zealand, a third of them unique to the country.[184] More penguin species are found in New Zealand than in any other country.[185]		Since human arrival, almost half of the country's vertebrate species have become extinct, including at least fifty-one birds, three frogs, three lizards, one freshwater fish, and one bat. Others are endangered or have had their range severely reduced.[174] However, New Zealand conservationists have pioneered several methods to help threatened wildlife recover, including island sanctuaries, pest control, wildlife translocation, fostering, and ecological restoration of islands and other selected areas.[186][187][188][189]		New Zealand has a high-income economy with a nominal gross domestic product (GDP) per capita of US$36,254.[8] The currency is the New Zealand dollar, informally known as the "Kiwi dollar"; it also circulates in the Cook Islands (see Cook Islands dollar), Niue, Tokelau, and the Pitcairn Islands.[190] New Zealand was ranked 13th in the 2016 Human Development Index[10] and third in the 2016 Index of Economic Freedom.[191]		Historically, extractive industries have contributed strongly to New Zealand's economy, focussing at different times on sealing, whaling, flax, gold, kauri gum, and native timber.[192] With the development of refrigerated shipping in the 1880s meat and dairy products were exported to Britain, a trade which provided the basis for strong economic growth in New Zealand.[193] High demand for agricultural products from the United Kingdom and the United States helped New Zealanders achieve higher living standards than both Australia and Western Europe in the 1950s and 1960s.[194] In 1973, New Zealand's export market was reduced when the United Kingdom joined the European Community[195] and other compounding factors, such as the 1973 oil and 1979 energy crisis, led to a severe economic depression.[196] Living standards in New Zealand fell behind those of Australia and Western Europe, and by 1982 New Zealand had the lowest per-capita income of all the developed nations surveyed by the World Bank.[197] In the mid-1980s New Zealand deregulated its agricultural sector by phasing out subsidies over a three-year period.[198][199] Since 1984, successive governments engaged in major macroeconomic restructuring (known first as Rogernomics and then Ruthanasia), rapidly transforming New Zealand from a highly protectionist economy to a liberalised free trade economy.[200][201]		Unemployment peaked above 10% in 1991 and 1992,[203] following the 1987 share market crash, but eventually fell to a record low (since 1986)[204] of 3.4% in 2007 (ranking fifth from twenty-seven comparable OECD nations).[205] However, the global financial crisis that followed had a major impact on New Zealand, with the GDP shrinking for five consecutive quarters, the longest recession in over thirty years,[206][207] and unemployment rising back to 7% in late 2009.[208] At May 2012, the general unemployment rate was around 6.7%, while the unemployment rate for youth aged 15 to 21 was 13.6%.[209] In the September 2014 quarter, unemployment was 5.4%.[210] New Zealand has experienced a series of "brain drains" since the 1970s[211] that still continue today.[212] Nearly one quarter of highly skilled workers live overseas, mostly in Australia and Britain, which is the largest proportion from any developed nation.[213] In recent years, however, a "brain gain" has brought in educated professionals from Europe and less developed countries.[214][215]		New Zealand is heavily dependent on international trade,[216] particularly in agricultural products.[217] Exports account for 24% of its output,[141] making New Zealand vulnerable to international commodity prices and global economic slowdowns. Food products made up 55% of the value of all the country's exports in 2014; wood was the second largest earner (7%).[218] Its major export partners are Australia, United States, Japan, China, and the United Kingdom.[141] On 7 April 2008, New Zealand and China signed the New Zealand–China Free Trade Agreement, the first such agreement China has signed with a developed country.[219][220] The service sector is the largest sector in the economy, followed by manufacturing and construction and then farming and raw material extraction.[141] Tourism plays a significant role in the economy, contributing $12.9 billion (or 5.6%) to New Zealand's total GDP and supporting 7.5% of the total workforce in 2016.[221] International visitor arrivals are expected to increase at a rate of 5.4% annually up to 2022.[221]		Wool was New Zealand's major agricultural export during the late 19th century.[192] Even as late as the 1960s it made up over a third of all export revenues,[192] but since then its price has steadily dropped relative to other commodities[222] and wool is no longer profitable for many farmers.[223] In contrast dairy farming increased, with the number of dairy cows doubling between 1990 and 2007,[224] to become New Zealand's largest export earner.[225] In the year to June 2009, dairy products accounted for 21% ($9.1 billion) of total merchandise exports,[226] and the country's largest company, Fonterra, controls almost one-third of the international dairy trade.[227] Other agricultural exports in 2009 were meat 13.2%, wool 6.3%, fruit 3.5% and fishing 3.3%. New Zealand's wine industry has followed a similar trend to dairy, the number of vineyards doubling over the same period,[228] overtaking wool exports for the first time in 2007.[229][230]		In 2015, renewable energy, primarily geothermal and hydroelectric power, generated 40.1% of New Zealand's gross energy supply.[231] Geothermal power alone accounted for 22% of New Zealand's energy in 2015.[231]		The provision of water supply and sanitation is generally of good quality. Regional authorities provide water abstraction, treatment and distribution infrastructure to most developed areas.[232][233]		New Zealand's transport network comprises 94,000 kilometres (58,410 mi) of roads, including 199 kilometres (124 mi) of motorways,[234] and 4,128 kilometres (2,565 mi) of railway lines.[141] Most major cities and towns are linked by bus services, although the private car is the predominant mode of transport.[235] The railways were privatised in 1993, but were re-nationalised by the government in stages between 2004 and 2008. The state-owned enterprise KiwiRail now operates the railways, with the exception of Auckland and Wellington commuter services which are operated by Transdev[236] and Metlink.[237] Railways run the length of the country, although most lines now carry freight rather than passengers.[238] Most international visitors arrive via air[239] and New Zealand has six international airports, but currently[update] only the Auckland and Christchurch airports connect directly with countries other than Australia or Fiji.[240]		The New Zealand Post Office had a monopoly over telecommunications until 1987 when Telecom New Zealand was formed, initially as a state-owned enterprise and then privatised in 1990.[241] Chorus, which was split from Telecom in 2011, still owns the majority of the telecommunications infrastructure, but competition from other providers has increased. As of 2016[update], the United Nations International Telecommunication Union ranks New Zealand 13th in the development of information and communications infrastructure.[242]		As of June 2016, the population of New Zealand is estimated at 4.69 million and is increasing at a rate of approximately 2.1% per year.[243] New Zealand is a predominantly urban country, with 73.0% of the population living in the seventeen main urban areas (i.e. population 30,000 or greater) and 53.7% living in the four largest cities of Auckland, Christchurch, Wellington, and Hamilton.[244] New Zealand cities generally rank highly on international livability measures. For instance, in 2016 Auckland was ranked the world's third most liveable city and Wellington the twelfth by the Mercer Quality of Living Survey.[245]		Life expectancy for New Zealanders in 2012 was 84 years for females, and 80.2 years for males.[246] Life expectancy at birth is forecast to increase from 80 years to 85 years in 2050 and infant mortality is expected to decline.[247] New Zealand's fertility rate of 2.1 is relatively high for a developed country, and natural births account for a significant proportion of population growth. Consequently, the country has a young population compared to most industrialised nations, with 20% of New Zealanders being 14 years old or younger.[141] By 2050 the population is forecast to reach 5.3 million, the median age to rise from 36 years to 43 years and the percentage of people 60 years of age and older to rise from 18% to 29%.[247] In 2008, the leading cause of premature death was cancer, at 29.8%, followed by ischaemic heart disease, 19.7%, and then cerebrovascular disease, 9.2%.[248]		In the 2013 census, 74.0% of New Zealand residents identified ethnically as European, and 14.9% as Māori. Other major ethnic groups include Asian (11.8%) and Pacific peoples (7.4%), of which two-thirds live in the Auckland Region.[5][n 8] The population has become more diverse in recent decades: in 1961, the census reported that the population of New Zealand was 92% European and 7% Māori, with Asian and Pacific minorities sharing the remaining 1%.[250]		While the demonym for a New Zealand citizen is New Zealander, the informal "Kiwi" is commonly used both internationally[251] and by locals.[252] The Māori loanword Pākehā has been used to refer to New Zealanders of European descent, although others reject this appellation.[253][254] The word Pākehā today is increasingly used to refer to all non-Polynesian New Zealanders.[255]		The Māori were the first people to reach New Zealand, followed by the early European settlers. Following colonisation, immigrants were predominantly from Britain, Ireland and Australia because of restrictive policies similar to the White Australia policy.[256] There was also significant Dutch, Dalmatian,[257] German, and Italian immigration, together with indirect European immigration through Australia, North America, South America and South Africa.[258][259] Net migration increased after the Second World War; in the 1970s and 1980s policies were relaxed and immigration from Asia was promoted.[259][260] In 2009–10, an annual target of 45,000–50,000 permanent residence approvals was set by the New Zealand Immigration Service—more than one new migrant for every 100 New Zealand residents.[261] Just over 25% of New Zealand's population was born overseas, with the majority (52%) living in the Auckland Region. The United Kingdom remains the largest source of New Zealand's overseas population, with a quarter of all overseas-born New Zealanders born there; other major sources of New Zealand's overseas-born population are China, India, Australia, South Africa, Fiji and Samoa.[262] The number of fee-paying international students increased sharply in the late 1990s, with more than 20,000 studying in public tertiary institutions in 2002.[263]		English is the predominant language in New Zealand, spoken by 96.1% of the population.[4] New Zealand English is similar to Australian English and many speakers from the Northern Hemisphere are unable to tell the accents apart.[265] The most prominent differences between the New Zealand English dialect and other English dialects are the shifts in the short front vowels: the short-"i" sound (as in "kit") has centralised towards the schwa sound (the "a" in "comma" and "about"); the short-"e" sound (as in "dress") has moved towards the short-"i" sound; and the short-"a" sound (as in "trap") has moved to the short-"e" sound.[266]		After the Second World War, Māori were discouraged from speaking their own language (te reo Māori) in schools and workplaces and it existed as a community language only in a few remote areas.[267] It has recently undergone a process of revitalisation,[268] being declared one of New Zealand's official languages in 1987,[269] and is spoken by 3.7% of the population.[4][n 9] There are now Māori language immersion schools and two television channels that broadcast predominantly in Māori.[271] Many places have both their Māori and English names officially recognised.		As recorded in the 2013 census, Samoan is the most widely spoken non-official language (2.2%),[n 10] followed by Hindi (1.7%), "Northern Chinese" (including Mandarin, 1.3%) and French (1.2%).[4] About 20,000 people use New Zealand Sign Language.[272] It was declared one of New Zealand's official languages in 2006.[273]		Christianity is the predominant religion in New Zealand, although its society is among the most secular in the world.[274][275] In the 2013 census, 55.0% of the population identified with one or more religions, including 49.0% identifying as Christians. Another 41.9% indicated that they had no religion.[n 11][276] The main Christian denominations are, by number of adherents, Roman Catholicism (12.6%), Anglicanism (11.8%), Presbyterianism (8.5%) and "Christian not further defined" (i.e. people identifying as Christian but not stating the denomination, 5.5%).[276] The Māori-based Ringatū and Rātana religions (1.4%) are also Christian in origin.[277][278] Immigration and demographic change in recent decades has contributed to the growth of minority religions,[279] such as Hinduism (2.1%), Buddhism (1.5%), Islam (1.2%) and Sikhism (0.5%).[277] The Auckland Region exhibited the greatest religious diversity.[277]		Primary and secondary schooling is compulsory for children aged 6 to 16, with the majority attending from the age of 5.[280] There are 13 school years and attending state (public) schools is free to New Zealand citizens and permanent residents from a person's 5th birthday to the end of the calendar year following their 19th birthday.[281] New Zealand has an adult literacy rate of 99%,[141] and over half of the population aged 15 to 29 hold a tertiary qualification.[280] There are five types of government-owned tertiary institutions: universities, colleges of education, polytechnics, specialist colleges, and wānanga,[282] in addition to private training establishments.[283] In the adult population 14.2% have a bachelor's degree or higher, 30.4% have some form of secondary qualification as their highest qualification and 22.4% have no formal qualification.[284] The OECD's Programme for International Student Assessment ranks New Zealand's education system as the seventh best in the world, with students performing exceptionally well in reading, mathematics and science.[285]		Early Māori adapted the tropically based east Polynesian culture in line with the challenges associated with a larger and more diverse environment, eventually developing their own distinctive culture. Social organisation was largely communal with families (whānau), subtribes (hapū) and tribes (iwi) ruled by a chief (rangatira), whose position was subject to the community's approval.[286] The British and Irish immigrants brought aspects of their own culture to New Zealand and also influenced Māori culture,[287][288] particularly with the introduction of Christianity.[289] However, Māori still regard their allegiance to tribal groups as a vital part of their identity, and Māori kinship roles resemble those of other Polynesian peoples.[290] More recently American, Australian, Asian and other European cultures have exerted influence on New Zealand. Non-Māori Polynesian cultures are also apparent, with Pasifika, the world's largest Polynesian festival, now an annual event in Auckland.[291]		The largely rural life in early New Zealand led to the image of New Zealanders being rugged, industrious problem solvers.[292] Modesty was expected and enforced through the "tall poppy syndrome", where high achievers received harsh criticism.[293] At the time New Zealand was not known as an intellectual country.[294] From the early 20th century until the late 1960s, Māori culture was suppressed by the attempted assimilation of Māori into British New Zealanders.[267] In the 1960s, as tertiary education became more available and cities expanded[295] urban culture began to dominate.[296] However, rural imagery and themes have been pervasive in New Zealand's art, literature and media.[297]		New Zealand's national symbols are influenced by natural, historical, and Māori sources. The silver fern is an emblem appearing on army insignia and sporting team uniforms.[298] Certain items of popular culture thought to be unique to New Zealand are called "Kiwiana".[298]		As part of the resurgence of Māori culture, the traditional crafts of carving and weaving are now more widely practised and Māori artists are increasing in number and influence.[299] Most Māori carvings feature human figures, generally with three fingers and either a natural-looking, detailed head or a grotesque head.[300] Surface patterns consisting of spirals, ridges, notches and fish scales decorate most carvings.[301] The pre-eminent Māori architecture consisted of carved meeting houses (wharenui) decorated with symbolic carvings and illustrations. These buildings were originally designed to be constantly rebuilt, changing and adapting to different whims or needs.[302]		Māori decorated the white wood of buildings, canoes and cenotaphs using red (a mixture of red ochre and shark fat) and black (made from soot) paint and painted pictures of birds, reptiles and other designs on cave walls.[303] Māori tattoos (moko) consisting of coloured soot mixed with gum were cut into the flesh with a bone chisel.[304] Since European arrival paintings and photographs have been dominated by landscapes, originally not as works of art but as factual portrayals of New Zealand.[305] Portraits of Māori were also common, with early painters often portraying them as "noble savages", exotic beauties or friendly natives.[305] The country's isolation delayed the influence of European artistic trends allowing local artists to developed their own distinctive style of regionalism.[306] During the 1960s and 70s many artists combined traditional Māori and Western techniques, creating unique art forms.[307] New Zealand art and craft has gradually achieved an international audience, with exhibitions in the Venice Biennale in 2001 and the "Paradise Now" exhibition in New York in 2004.[299][308]		Māori cloaks are made of fine flax fibre and patterned with black, red and white triangles, diamonds and other geometric shapes.[309] Greenstone was fashioned into earrings and necklaces, with the most well-known design being the hei-tiki, a distorted human figure sitting cross-legged with its head tilted to the side.[310] Europeans brought English fashion etiquette to New Zealand, and until the 1950s most people dressed up for social occasions.[311] Standards have since relaxed and New Zealand fashion has received a reputation for being casual, practical and lacklustre.[312][313] However, the local fashion industry has grown significantly since 2000, doubling exports and increasing from a handful to about 50 established labels, with some labels gaining international recognition.[313]		Māori quickly adopted writing as a means of sharing ideas, and many of their oral stories and poems were converted to the written form.[314] Most early English literature was obtained from Britain and it was not until the 1950s when local publishing outlets increased that New Zealand literature started to become widely known.[315] Although still largely influenced by global trends (modernism) and events (the Great Depression), writers in the 1930s began to develop stories increasingly focused on their experiences in New Zealand. During this period literature changed from a journalistic activity to a more academic pursuit.[316] Participation in the world wars gave some New Zealand writers a new perspective on New Zealand culture and with the post-war expansion of universities local literature flourished.[317] Dunedin is a UNESCO City of Literature.[318]		New Zealand music has been influenced by blues, jazz, country, rock and roll and hip hop, with many of these genres given a unique New Zealand interpretation.[319] Māori developed traditional chants and songs from their ancient South-East Asian origins, and after centuries of isolation created a unique "monotonous" and "doleful" sound.[320] Flutes and trumpets were used as musical instruments[321] or as signalling devices during war or special occasions.[322] Early settlers brought over their ethnic music, with brass bands and choral music being popular, and musicians began touring New Zealand in the 1860s.[323][324] Pipe bands became widespread during the early 20th century.[325] The New Zealand recording industry began to develop from 1940 onwards and many New Zealand musicians have obtained success in Britain and the United States.[319] Some artists release Māori language songs and the Māori tradition-based art of kapa haka (song and dance) has made a resurgence.[326] The New Zealand Music Awards are held annually by Recorded Music NZ; the awards were first held in 1965 by Reckitt & Colman as the Loxene Golden Disc awards.[327] Recorded Music NZ also publishes the country's official weekly record charts.[328]		Public radio was introduced in New Zealand in 1922.[330] A state-owned television service began in 1960.[331] The number of New Zealand films significantly increased during the 1970s.[332] In 1978 the New Zealand Film Commission started assisting local film-makers and many films attained a world audience, some receiving international acknowledgement. The highest grossing New Zealand movies include: Hunt for the Wilderpeople, Boy, The World's Fastest Indian, Once Were Warriors, and Whale Rider.[333] Deregulation in the 1980s saw a sudden increase in the numbers of radio and television stations.[332] New Zealand television primarily broadcasts American and British programming, along with a large number of Australian and local shows. The country's diverse scenery and compact size, plus government incentives,[334] have encouraged some producers to film big budget movies in New Zealand, including Avatar, The Lord of the Rings, The Hobbit, The Chronicles of Narnia, King Kong and The Last Samurai.[335] The New Zealand media industry is dominated by a small number of companies, most of which are foreign-owned, although the state retains ownership of some television and radio stations. Since 1994, Freedom House has consistently ranked New Zealand's press freedom in the top twenty, with the 19th freest media in 2014[update].[336]		Most of the major sporting codes played in New Zealand have British origins.[337] Rugby union is considered the national sport[338] and attracts the most spectators.[339] Golf, netball, tennis and cricket have the highest rates of adult participation, while netball, rugby union and football (soccer) is popular among young people.[339][340] Around 54% of New Zealand adolescents participate in sports for their school.[340] Victorious rugby tours to Australia and the United Kingdom in the late 1880s and the early 1900s played an early role in instilling a national identity.[341] Horseracing was also a popular spectator sport and became part of the "Rugby, Racing and Beer" culture during the 1960s.[342] Māori participation in European sports was particularly evident in rugby and the country's team performs a haka, a traditional Māori challenge, before international matches.[343]		New Zealand has competitive international teams in rugby union, netball, cricket, rugby league and softball, and has traditionally done well in triathlons, rowing, yachting and cycling. New Zealand participated at the Summer Olympics in 1908 and 1912 as a joint team with Australia, before first participating on its own in 1920.[344] The country has ranked highly on a medals-to-population ratio at recent Games.[345][346] The "All Blacks", the national men's rugby union team, are the most successful in the history of international rugby[347] and the reigning World Cup champions.[348] New Zealand is known for its extreme sports, adventure tourism[349] and strong mountaineering tradition, as seen in the success of notable New Zealander Sir Edmund Hillary.[350] Other outdoor pursuits such as cycling, fishing, swimming, running, tramping, canoeing, hunting, snowsports and surfing are also popular.[351] The Polynesian sport of waka ama racing has increased in popularity and is now an international sport involving teams from all over the Pacific.[352]		The national cuisine has been described as Pacific Rim, incorporating the native Māori cuisine and diverse culinary traditions introduced by settlers and immigrants from Europe, Polynesia and Asia.[353] New Zealand yields produce from land and sea—most crops and livestock, such as maize, potatoes and pigs, were gradually introduced by the early European settlers.[354] Distinctive ingredients or dishes include lamb, salmon, kōura (crayfish),[355] dredge oysters, whitebait, pāua (abalone), mussels, scallops, pipis and tuatua (both are types of New Zealand shellfish),[356] kūmara (sweet potato), kiwifruit, tamarillo and pavlova (considered a national dish).[357][353] A hāngi is a traditional Māori method of cooking food using heated rocks buried in a pit oven. After European colonisation, Māori began cooking with pots and ovens and the hāngi was used less frequently, although it is still used for formal occasions such as tangihanga.[358]		Click on a coloured area to see an article about English in that country or region		Coordinates: 42°S 174°E﻿ / ﻿42°S 174°E﻿ / -42; 174		
Literal and figurative language is a distinction within some fields of language analysis, in particular stylistics, rhetoric, and semantics.		Literal usage confers meaning to words, in the sense of the meaning they have by themselves, outside any figure of speech.[2] It maintains a consistent meaning regardless of the context,[3] with the intended meaning corresponding exactly to the meaning of the individual words.[4] Figurative use of language is the use of words or phrases that implies a non-literal meaning which does make sense or that could [also] be true.[5]		Aristotle and later the Roman Quintilian were among the early analysts of rhetoric who expounded on the differences between literal and figurative language.[6]		In 1769, Frances Brooke's novel The History of Emily Montague was used in the earliest Oxford English Dictionary citation for the figurative sense of literally; the sentence from the novel used was, "He is a fortunate man to be introduced to such a party of fine women at his arrival; it is literally to feed among the lilies."[7] This citation was also used in the OED's 2011 revision.[7]		Within literary analysis, such terms are still used; but within the fields of cognition and linguistics, the basis for identifying such a distinction is no longer used.[8]						Figurative language can take multiple forms, such as simile or metaphor.[9] Merriam-Webster's Encyclopedia Of Literature says that figurative language can be classified in five categories: resemblance or relationship, emphasis or understatement, figures of sound, verbal games, and errors.[10]		A simile[11] is a comparison of two things, indicated by some connective, usually "like", "as", "than", or a verb such as "resembles" to show how they are similar.[12]		A metaphor[14] is a figure of speech in which two "essentially unlike things" are shown to have a type of resemblance or create a new image.[15] The similarities between the objects being compared may be implied rather than directly stated.[15]		An extended metaphor is metaphor that is continued over multiple sentences.[17][18]		Onomatopoeia is a word designed to be an imitation of a sound.[20]		Personification[21] is the attribution of a personal nature or character to inanimate objects or abstract notions,[22] especially as a rhetorical figure.		An oxymoron is a figure of speech in which a pair of opposite or contradictory terms is used together for emphasis.[23]		A paradox is a statement or proposition which is self-contradictory, unreasonable, or illogical.[24]		Hyperbole is a figure of speech which uses an extravagant or exaggerated statement to express strong feelings.[25]		Allusion is a reference to a famous character or event.		An idiom is an expression that has a figurative meaning unrelated to the literal meaning of the phrase.		A pun is an expression intended for a humorous or rhetorical effect by exploiting different meanings of words.		Prior to the 1980s, the "standard pragmatic" model of comprehension was widely believed. In that model, it was thought the recipient would first attempt to comprehend the meaning as if literal, but when an appropriate literal inference could not be made, the recipient would shift to look for a figurative interpretation that would allow comprehension.[26] Since then, research has cast doubt on the model. In tests, figurative language was found to be comprehended at the same speed as literal language; and so the premise that the recipient was first attempting to process a literal meaning and discarding it before attempting to process a figurative meaning appears to be false.[27]		Beginning with the work of Michael Reddy in his 1979 work "The Conduit Metaphor", many linguists now reject that there is a valid way to distinguish between a "literal" and "figurative" mode of language.[28]		
A mascot is any person, animal, or object thought to bring luck, or anything used to represent a group with a common public identity, such as a school, professional sports team, society, military unit, or brand name. Mascots are also used as fictional, representative spokespeople for consumer products, such as the rabbit used in advertising and marketing for the General Mills brand of breakfast cereal, Trix.		In the world of sports, mascots are also used for merchandising. Team mascots are often confused with team nicknames.[1] While the two can be interchangeable, they are not always the same. For example, the athletic teams of the University of Alabama are nicknamed the Crimson Tide, while their mascot is an elephant named Big Al. Team mascots may take the form of a logo, person, live animal, inanimate object, or a costumed character, and often appear at team matches and other related events, sports mascots are often used as marketing tools for their teams to children. Since the mid-20th century, costumed characters have provided teams with an opportunity to choose a fantasy creature as their mascot, as is the case with the Philadelphia Phillies' mascot, the Phillie Phanatic.		Costumed mascots are commonplace, and are regularly used as goodwill ambassadors in the community for their team, company, or organization such as the U.S. Forest Service's Smokey Bear.						It was originally sporting organisations that first thought of using animals as a form of mascot to bring entertainment and excitement for their spectators. Before mascots were fictional icons or people in suits, animals were mostly used in order to bring a somewhat different feel to the game and to strike fear upon the rivalry teams.		As the new era was changing and time went on, mascots evolved from predatory animals, to two-dimensional fantasy mascots, to finally what we know today, three-dimensional mascots. The event that prompted these changes was the invention of the Muppets in the late 1960s.[dubious – discuss] The puppets offered something different to what everyone was used to. It allowed people to not only have visual enjoyment but also interact physically with the mascots.		Marketers quickly realized the great potential in three-dimensional mascots and took on board the Muppet-like idea. This change encouraged other companies to start creating their own mascots, resulting in mascots being a necessity amongst not only the sporting industry but for other organisations[2]		The word 'mascot' originates from the French term 'mascotte' which means lucky charm. This was used to describe anything that brought luck to a household. The word was first recorded in 1867 and popularised by a French composer Edmond Audran who wrote the opera La mascotte, performed in December 1880. [3] The word entered the English language in 1881. However, before this, the terms were familiar to the people of France as a slang word used by gamblers. The term is a derivative of the word 'masco' meaning sorceress or witch. Before the 19th century, the word 'mascot' was associated with inanimate objects that would be commonly seen such as a lock of hair or a figurehead on a sailing ship. But from then on until the present day, the term was then seen to be associated with good luck animals, objects etc.[2][3]		Often the choice of mascot reflects a desired quality; a common example of this is the "fighting spirit," in which a competitive nature is personified by warriors or predatory animals.		Mascots may also symbolize a local or regional trait, such as the Nebraska Cornhuskers' mascot, Herbie Husker: a stylized version of a farmer, owing to the agricultural traditions of the area in which the university is located. Similarly, Pittsburg State University uses Gus the Gorilla as its mascot, "gorilla" being an old colloquial term for coal miners in the Southeast Kansas area in which the university was established.[4]		In the United States, controversy[5] surrounds some mascot choices, especially those using human likenesses. Mascots based on Native American tribes are particularly contentious, as many argue that they constitute offensive exploitations of an oppressed culture.[6] However, several Indian tribes have come out in support of keeping the names. For example, the Utah Utes and the Central Michigan Chippewas are sanctioned by local tribes, and the Florida State Seminoles are supported by the Seminole Tribe of Florida in their use of Osceola and Renegade as symbols. FSU chooses not to refer to them as mascots because of the offensive connotation.[7] This has not, however, prevented fans from engaging in "Redface"—dressing up in stereotypical, Plains Indian outfits during games, or creating offensive banners saying "Scalp 'em" as was seen at the 2014 Rose Bowl.[8]		Some sports teams have "unofficial" mascots: individual supporters or fans that have become identified with the team. The New York Yankees have such an individual in fan Freddy Sez. Former Toronto Blue Jays mascot BJ Birdie was a costumed character created by a Blue Jays fan, ultimately hired by the team to perform at their home games. USC Trojans mascot is Tommy Trojan who rides on his horse (and the official mascot of the school) Traveler.		Many sports teams in the United States (U.S.) have official mascots, sometimes enacted by costumed humans or even live animals. One of the earliest was a taxidermy mount for the Chicago Cubs, in 1908, and later a live animal used in 1916 by the same team. They abandoned the concept shortly thereafter and remained without an official "cub" until 2014, when they introduced a version that was a person wearing a costume.[9]		In the United Kingdom some teams have young fans become "mascots". These representatives sometimes have medical issues, and the appearance is a wish grant,[10] the winner of a contest,[11] or under other circumstances. Unlike the anonymous performers of costumed characters, however, their actions can be associated with the club later on.[12] Mascots also include older people such as Mr England, who are invited by national sports associations to be mascots for the representative teams.[13]		Mascots or advertising characters are very common in the corporate world. Recognizable mascots include Chester Cheetah, Keebler Elf, the Fruit of the Loom Guys, Pizza Pizza Guy for Little Caesars, Rocky the Elf, the Coca-Cola Bear, the NBC Peacock, and the NRA's Eddie Eagle. These characters are typically known without even having to refer to the company or brand. This is an example of corporate branding, and soft selling a company. Mascots are able to act as brand ambassadors where advertising is not allowed. For example, many corporate mascots can attend non-profit events, or sports and promote their brand while entertaining the crowd. Some mascots are simply cartoons or virtual mascots, others are characters in commercials, and others are actually created as costumes and will appear in person in front of the public at tradeshows or events.[14]		Most American schools have a mascot. High schools, colleges, and even middle and elementary schools typically have mascots. Most of them have their mascot created as a costume, and use this costume at sports or social events. Examples of School mascots include UNC's Rameses the Ram, the University of Kansas' Big Jay and Baby Jay, University of Tennessee at Chattanooga's Scrappy the Mocking Bird, Western Michigan's Buster the Bronco, Temple's Hooter the Owl, Villanova's Will D. Cat the Wildcat, MIT's Tim the Beaver, Boston University's Rhett the Boston Terrier, and St. Joe's "The Hawk".		The mascots that are used for the Summer and Winter Olympic games are fictional characters, typically a human figure or an animal native to the country to which is holding that year's Olympic Games. The mascots are used to entice an audience and bring joy and excitement to the Olympics festivities.		Sam and Seymore D. Fair from 1984 are examples of some of the first mascots used in the Summer Olympic Games and the Louisiana World Exposition, respectively. Dating from 1968, the city which holds the Olympic games every two years has the job of designing a mascot that corresponds with the culture of the country and is an icon symbol to that of the nations values. Recent Winter/Summer Olympic games mascots include Miga, Quatchi, Mukmuk (Vancouver, 2010), Wenlock and Mandeville (London, 2012), Bely Mishka, Snow Leopard, Zaika (Sochi, 2014) and Vinicius and Tom (Rio, 2016) have all gone on to become iconic symbols in their respective countries.[15]		Camilla Corona SDO is the mission mascot for NASA's Solar Dynamics Observatory (SDO) and assists the mission with Education and Public Outreach (EPO).[16]		Mascots are also popular in military units. For example, the United States Marine Corps uses the English Bulldog as its mascot, while the United States Army uses the mule, the United States Navy uses the goat, and the United States Air Force uses the Gyrfalcon.		The goat in the Royal Welsh is officially not a mascot but a ranking soldier. Lance Corporal William Windsor retired on 20 May 2009, and a replacement is expected in June.[17] Several regiments of the British Army have a live animal mascot which appear on parades. The Parachute Regiment and the Argyll and Sutherland Highlanders have a Shetland pony as their mascot, a ram for The Mercian Regiment; an Irish Wolfhound for the Irish Guards and the Royal Irish Regiment; a drum horse for the Queen's Royal Hussars and the Royal Scots Dragoon Guards; an antelope for the Royal Regiment of Fusiliers; and a goat for the Royal Welsh. Other British military mascots include a Staffordshire Bull Terrier and a pair of ferrets.		The Norwegian Royal Guard adopted a king penguin named Nils Olav as its mascot on the occasion of a visit to Edinburgh by its regimental band. The (very large) penguin remains resident at Edinburgh Zoo and has been formally promoted by one rank on the occasion of each subsequent visit to Britain by the band or other detachments of the Guard. Regimental Sergeant Major Olav was awarded the Norwegian Army's Long Service and Good Conduct medal at a ceremony in 2005.		Some bands, particularly in the heavy metal genre use band mascots to promote their music. The mascots are usually found on album covers or merchandise such as band T-shirts, but can also make appearances in live shows or music videos. A famous example of a band mascot is Eddie the Head of the English heavy metal band Iron Maiden. Eddie is a zombie-like creature which is personified in different forms on all of the band's albums, most of its singles and some of its promotional merchandise. Eddie is also known to make live appearances, especially during the song "Iron Maiden". Another notable example of a mascot in music is Skeleton Sam of The Grateful Dead. South Korean hip hop band B.A.P uses rabbits named Matoki as their mascot, each bunny a different color representing each member. Although rabbits have an innocent image, BAP gives off a tough image. Hip hop artist Kanye West used to use a teddy bear named Dropout Bear as his mascot; Dropout Bear has appeared on the cover of West's first three studio albums, and served as the main character of West's music video, "Good Morning".		
Hong Kong:		Umbrella Movement		A series of sit-in street protests, often called the Umbrella Revolution (Chinese: 雨傘革命; Jyutping: Jyu5saan3 gaak3meng6; pinyin: Yǔsǎn gémìng) and sometimes used interchangeably with Umbrella Movement (Chinese: 雨傘運動; Jyutping: Jyu5saan3 wan6dung6; pinyin: Yǔsǎn yùndòng), occurred in Hong Kong from 26 September to 15 December 2014.[10][11] The protests began after the Standing Committee of the National People's Congress (NPCSC) issued a decision regarding proposed reforms to the Hong Kong electoral system. The decision was widely[12] seen to be highly restrictive, and tantamount to the Chinese Communist Party's pre-screening of the candidates for the leader of Hong Kong.[12]		Students led a strike against the NPCSC's decision beginning on 22 September 2014, and the Hong Kong Federation of Students and Scholarism started protesting outside the government headquarters on 26 September 2014.[13] On 28 September, the Occupy Central with Love and Peace movement announced the beginning of their civil disobedience campaign.[14] Students and other members of the public demonstrated outside government headquarters, and some began to occupy several major city intersections.[15] Protesters blocked both east–west arterial routes in northern Hong Kong Island near Admiralty. Police tactics – including the use of tear gas – and triad attacks on protesters led more citizens to join the protests and to occupy Causeway Bay and Mong Kok.[16][17][18] The number of protesters peaked at more than 100,000 at any given time, overwhelming the police thus causing containment errors.[19][20][21]		Government officials in Hong Kong and in Beijing denounced the occupation as "illegal" and a "violation of the rule of law", and Chinese state media and officials claimed repeatedly that the West had played an "instigating" role in the protests, and warned of "deaths and injuries and other grave consequences."[22] In an opinion poll carried out by Chinese University of Hong Kong, only 36.1% of 802 people surveyed between 8–15 October accept NPCSC's decision but 55.6% are willing to accept if HKSAR Government would democratise the nominating committee during the second phase of public consultation period.[23] The protests precipitated a rift in Hong Kong society, and galvanised youth – a previously apolitical section of society – into political activism or heightened awareness of their civil rights and responsibilities. Not only were there fist fights at occupation sites and flame wars on social media, family members found themselves on different sides of the conflict.[24]		Key areas in Admiralty, Causeway Bay and Mong Kok were occupied and remained closed to traffic for 79 days. Despite numerous incidents of intimidation and violence by triads and thugs, particularly in Mong Kok, and several attempts at clearance by the police, suffragists held their ground for over two months. After the Mong Kok occupation site was cleared with some scuffles on 25 November, Admiralty and Causeway Bay were cleared with no opposition on 11 and 14 December respectively.		Throughout the protests the Hong Kong government's use of the police and courts to resolve political issues led to accusations that these institutions had been turned into a political tools, thereby compromising the police and judicial system in the territory and eroding the rule of law in favour of "rule by law".[25][26][27][28] Police inactions and violent actions throughout the occupation were widely perceived to have severely damaged the reputation of what was once recognised as the most efficient, honest and impartial police forces in the Asia Pacific region.[29] The protests ended without any political concessions from the government, but instead triggered rhetoric from Chief Executive of Hong Kong CY Leung and mainland officials about rule of law and patriotism, and an assault on academic freedoms and civil liberties of activists.[26][30][31][32]						As a result of negotiations and the 1984 agreement between China and Britain, Hong Kong was returned to China and became its first Special Administrative Region on 1 July 1997, under the principle of "one country, two systems". Hong Kong has a different political system from mainland China. Hong Kong's independent judiciary functions under the common law framework.[33][34] The Hong Kong Basic Law, the constitutional document drafted by the Chinese side before the handover based on the terms enshrined in the Joint Declaration,[35] governs its political system, and stipulates that Hong Kong shall have a high degree of autonomy in all matters except foreign relations and military defence.[36] The declaration stipulates that the region maintain its capitalist economic system and guarantees the rights and freedoms of its people for at least 50 years after the 1997 handover. The guarantees over the territory's autonomy and the individual rights and freedoms are enshrined in the Hong Kong Basic Law, which outlines the system of governance of the Hong Kong Special Administrative Region, but which is subject to the interpretation of the Standing Committee of the National People's Congress (NPCSC).[37][38]		The leader of Hong Kong, the Chief Executive, is currently elected by a 1200-member Election Committee, though Article 45 of the Basic Law states that "the ultimate aim is the selection of the Chief Executive by universal suffrage upon nomination by a broadly representative nominating committee in accordance with democratic procedures."[39] A 2007 decision by the Standing Committee opened the possibility of selecting the Chief Executive via universal suffrage in the 2017 Chief Executive election,[40] and the first round of consultations to implement the needed electoral reforms ran for five months in early 2014. Chief Executive CY Leung then, per procedure, submitted a report to the Standing Committee inviting them to deliberate whether it is necessary to amend the method of selection of the Chief Executive.[41]		On 31 August 2014, the tenth session of the Standing Committee in the twelfth National People's Congress set limits for the 2016 Legislative Council election and 2017 Chief Executive election. While notionally allowing for universal suffrage, the decision imposes the standard that "the Chief Executive shall be a person who loves the country and loves Hong Kong," and stipulates "the method for selecting the Chief Executive by universal suffrage must provide corresponding institutional safeguards for this purpose". The decision states that for the 2017 Chief Executive election, a nominating committee, mirroring the present 1200-member Election Committee be formed to nominate two to three candidates, each of whom must receive the support of more than half of the members of the nominating committee. After popular election of one of the nominated candidates, the Chief Executive-elect "will have to be appointed by the Central People's Government." The process of forming the 2016 Legislative Council would be unchanged, but following the new process for the election of the Chief Executive, a new system to elect the Legislative Council via universal suffrage would be developed with the approval of Beijing.[15]		The Standing Committee decision is set to be the basis for electoral reform crafted by the Legislative Council. Hundreds of suffragists gathered on the night of the Beijing announcement near the government offices to protest the decision.[42][43]		In an opinion poll carried out by Chinese University of Hong Kong, only 36.1% of 802 people surveyed between 8–15 October accept NPCSC's decision but 55.6% are willing to accept if the HKSAR Government would democratise the nominating committee during the second phase of public consultation period.[23]		At a gathering in Hong Kong on 1 September to explain the NPCSC decision, deputy secretary general Li Fei said that the procedure would protect the broad stability of Hong Kong now and in the future.[42] Pro-democracy advocates viewed the decision as a betrayal of the principle of "one person, one vote," in that candidates deemed unsuitable by the Beijing authorities would have been pre-emptively screened out by the mechanism.[42] About 100 suffragists attended the gathering, and some were ejected for heckling and protesting.[42] Police broke up a group of demonstrators protesting outside the hotel where Li was staying, arresting 19 people for illegal assembly.[44]		In response to the NPCSC decision, the Democratic Party legislators promised to veto the framework for both elections as being inherently undemocratic; Occupy Central with Love and Peace (OCLP) announced that it would organise civil disobedience protests.[42] The Hong Kong Federation of Students (representing tertiary students) and Scholarism mobilised students and staged a coordinated class boycott. They organised public rallies and street assemblies.[45][46] Tertiary students would commence a one-week boycott from 22 September. At the same time, Scholarism organised a demonstration outside of the Central Government Offices barricade on 13 September 2014 where they declared a class-boycott on 26 September.[47]		Having received a "notice of no objection" to the assembly on 26 September 2014 between 00:01 to 23:59, protesters gathered in Tim Mei Avenue near the eastern entrance of the Central Government Offices.[48] At around 22:30, up to 100 protesters led by Joshua Wong, the Convenor of Scholarism, went to "reclaim" the privatised Civic Square for the public by clambering over the fence of the square.[49] The police mobilised on Civic Square, surrounded protesters at the centre and prepared to physically remove the protesters overnight.[50][51] Protesters who chose to depart were allowed to do so; each of the remaining ones was carried away by four or more police officers. At 1:20 am (of 27 September), the police used pepper spray on a crowd that had gathered near the Legislative Council, and some students were injured. By the following midnight, 13 people had been arrested including Joshua Wong, who was released after more than 40 hours upon being granted a writ of habeas corpus.[52]		At 1:30 pm, the police carried out the second round of clearances, and 48 men and 13 women were arrested for forcible entry into government premises and unlawful assembly.[53] A man was also arrested for possession of an offensive weapon. A police spokesman declared the assembly outside the Central Government Complex at Tim Mei Avenue illegal, and advised citizens to avoid the area. The arrested demonstrators, including Legislative Councillor Leung Kwok-hung and some HKFS members, were released around 9 pm. However, HKFS representatives Alex Chow and Lester Shum were detained for 30 hours.[54] The police eventually cleared the assembly, arresting a total of 78 people.[55][56]		Occupy Central with Love and Peace had been expected to start their occupation on 1 October, but this was accelerated to capitalise on the mass student presence.[57] At 1:40 am on 28 September, Benny Tai, one of the founders of OCLP, announced its commencement at a rally near the Central Government Complex.[57][58]		Later that morning, protests escalated as police blocked roads and bridges entering Tim Mei Avenue. Protest leaders urged citizens to come to Admiralty to encircle the police.[59] Tensions rose at the junction of Tim Mei Avenue and Harcourt Road after the police used pepper spray. As night fell, armed riot police advanced from Wan Chai towards Admiralty and unfurled a banner that stated "Warning, Tear Smoke". Seconds later, at around 6 pm, shots of tear gas were fired.[60][61][62] The heavy-handed policing, including the use of tear gas on peaceful protesters, inspired tens of thousands of citizens to join the protests in Admiralty that night.[18][19][63][64][65][66] Containment errors by the police – the closure of Tamar Park and Admiralty Station – caused a spill-over to other parts of the city, including Wan Chai, Causeway Bay and Mong Kok.[19][67][68] 3,000 protesters occupied a road in Mong Kok and 1,000 went to Causeway Bay.[65] The total number of protesters on the streets swelled to 80,000,[68] at times considerably exceeding 100,000.[20][21]		The police confirmed that they fired tear gas 87 times.[69] The media recalled that last time Hong Kong police had used tear gas was on Korean protesters during the 2005 World Trade Organization conference.[62][70] At least 34 people were injured in that day's protests.[71] According to police spokesmen, officers exercised "maximum tolerance," and tear gas was used only after protesters refused to disperse and "violently charged".[70][72] However, the South China Morning Post (SCMP) reported that police were seen to charge the suffragists.[73]		On 29 September, police adopted a less aggressive approach, sometimes employing negotiators to urge protesters to leave. 89 protesters were arrested; there were 41 casualties, including 12 police.[18] Chief Secretary for Administration, Carrie Lam announced that the second round of public consultations on political reform, originally planned to be completed by the end of the year, would be postponed.[74]		Joshua Wong and several Scholarism members attended the National Day flag raising ceremony on 1 October at the Golden Bauhinia Square, having undertaken not to shout slogans or make any gestures during the flag raising. Instead, the students faced away from the flag to show their discontent. District councillor Paul Zimmerman opened a yellow umbrella in protest inside the reception after the ceremony.[75][76][77] Protesters set up a short-lived fourth occupation site at a section of Canton Road in Tsim Sha Tsui.[78]		On 2 October, activists lay siege to the Central Government Headquarters.[19][79] Shortly before midnight, the Hong Kong Government responded to an ultimatum demanding universal suffrage with unscreened nominees: Carrie Lam agreed to hold talks with student leaders about political reform at an unspecified date.[80]		On 3 October, violence erupted in Mong Kok and Causeway Bay when groups of anti-Occupy Central activists including triad members and locals attacked suffragists while tearing down their tents and barricades.[16][17][81][82] A student suffered head injuries. Journalists were also attacked.[16][83][84] The Foreign Correspondents' Club accused the police of appearing to arrest alleged attackers but releasing them shortly after.[85] One legislator accused the government of orchestrating triads to clear the protest sites.[17] It was also reported that triads, as proprietors of many businesses in Mong Kok, had their own motivations to attack the protesters.[66] There were 20 arrests, and 18 people injured, including 6 police officers. Eight of the people arrested had triad backgrounds, but were released on bail.[17][86] Student leaders blamed the government for the attacks, and halted plans to hold talks with the government.[87]		On 4 October, counter-protesters wearing blue ribbons marched in support of the police.[88] Patrick Ko of the Voice of Loving Hong Kong group accused the suffragists of having double standards, and said that if the police had enforced the law, protesters would have already been evicted.[89] The anti-Occupy group Caring Hong Kong Power staged their own rally, at which they announced their support for the use of fire-arms by police and the deployment of the People's Liberation Army.[90]		In the afternoon, Chief Executive CY Leung insisted that government operations and schools affected by the occupation must resume on Monday. Former Democratic Party lawmaker Cheung Man-Kwong claimed the occupy campaign was in a "very dangerous situation," and urged them to "sit down and talk, in order to avoid tragedy".[91] The Federation of Students demanded the government explain the previous night's events and said they would continue their occupation of streets.[92] Secretary for Security Lai Tung-kwok denied accusations against the police, and explained that the reason for not using tear gas was due to the difference in geographical environment. Police claimed that protesters' barricades had prevented reinforcements from arriving on the scene.[93]		Pan-democracy legislator James To said that "the government has used organised, orchestrated forces and even triad gangs in [an] attempt to disperse citizens."[17][94] Violent attacks on journalists were strongly condemned by The Foreign Correspondents' Club, the Hong Kong Journalists' Association and local broadcaster RTHK.[95] Three former US consuls general to HK wrote a letter to the Chief Executive asking him to solve the disputes peacefully.[96]		On 5 October, leading establishment figures sympathetic to the liberal cause, including university heads and politicians, urged the suffragists to leave the streets for their own safety.[97] The rumoured clearance operation by the police did not occur.[98] At lunchtime the government offered to hold talks if the protesters cleared the roads. Later that night, the government agreed to guarantee the protesters' safety, and Alex Chow Yong-kang, leader of the Federation of Students (HKFS), announced that he had agreed to begin preparations for talks with Carrie Lam.[98]		On 9 October, the government cancelled the meeting with student leaders that had been scheduled for 10 October.[99] Carrie Lam, explained at a news conference that "We cannot accept the linking of illegal activities to whether or not to talk."[100] Alex Chow said "I feel like the government is saying that if there are fewer people on the streets, they can cancel the meeting. Students urge people who took part in the civil disobedience to go out on the streets again to occupy."[100] Pan-democrat legislators threatened to veto non-essential funding applications, potentially disrupting government operations, in support of the suffragists.[101]		On 10 October, in defiance of police warnings, thousands of protesters, many with tents, returned to the streets.[101] Over a hundred tents were pitched across the eight-lane Harcourt Road thoroughfare in Admiralty, alongside dozens of food and first-aid marquees. The ranks of protesters continued to swell on the 11th.[102]		On 11 October, the student leaders issued an open letter to Xi Jinping saying that CY Leung's report to NPCSC disregarded public opinion and ignored "Hong Kong people's genuine wishes."[103]		At 5.30 am on 12 October, police started an operation to remove unmanned barricades in Harcourt Road (Admiralty site) to "reduce the chance of traffic accidents".[103] In a pre-recorded TV interview[104] CY Leung declared that his resignation "would not solve anything".[105] He said the decision to use tear gas was made by the police without any political interference.[106] Several press organisations including the Hong Kong Journalists Association objected to the exclusion of other journalists, and said that Leung was deliberately avoiding questions about the issues surrounding the electoral framework.[107][108]		On 13 October, hundreds of men, many wearing surgical masks and carrying crowbars and cutting tools, began removing barricades at various sites and attacking suffragists. Police made attempts to separate the groups. Suffragists repaired and reinforced some barricades using bamboo and concrete.[109][110][111] Protesters again claimed that the attacks were organised and involved triad groups.[112] Police made three arrests for assault and possession of weapons. Although police cautioned against reinforcing the existing obstacles or setting up new obstacles to enlarge the occupied area, suffragists later reinstated the barriers overnight.[109] Anti-occupy protesters began to besiege the headquarters of Next Media, publisher of Apple Daily. They accused the paper of biased reporting.[113] Masked men among the protesters prevented the loading of copies of Apple Daily as well as The New York Times onto delivery vans.[114] Apple Daily sought a court injunction and a High Court judge issued a temporary order to prevent any blocking of the entrance.[115] Five press unions made a statement condemning the harassment of journalists by anti-occupy protesters.[116]		In the early morning of 14 October, police conducted a dawn raid to dismantle barricades in Yee Wo Street (Causeway Bay site), opening one lane to westbound traffic.[117] They also dismantled barricades at Queensway, Admiralty, and reopened it to traffic.[118]		Before midnight on 15 October, protesters stopped traffic on Lung Wo Road, the arterial road north of the Central Government Complex at Admiralty, and began erecting barricades. The police was unable to hold their cordon at Lung Wo Road Tunnel and had to retreat for reinforcement and organised redemption. Around 3 am, police began to clear the road using batons and pepper spray. By dawn, traffic on the road resumed and the protesters retreated into Tamar Park, while 45 arrests were made.		Local television channel TVB broadcast footage of Civic Party member Ken Tsang being assaulted by police. He was carried off with his hands tied behind his back; then, while one officer kept watch, a group of about six officers punched, kicked and stamped on him for about four minutes.[119][120][121][122] Journalists complained that they too had been assaulted.[123][124] The video provoked outrage; Amnesty International joined others in calling for the officers to be prosecuted. In response, Secretary for security Lai Tung-kwok said that "the officers involved will be temporarily removed from their current duties."[119][120]		At 5 am on 17 October, police cleared the barricades and tents at the Mong Kok site and opened the northbound side of Nathan Road to traffic for the first time in three weeks. In the early evening, at least 9000 protesters tried to retake the northbound lanes of the road. The police claimed that 15 officers sustained injuries. There were at least 26 arrests, including photojournalist Paula Bronstein.[125] Around midnight, the police retreated and the suffragists re-erected barricades across the road.[126][127]		On Sunday, 19 October, police used pepper spray and riot gear to contain the protesters in Mong Kok. Martin Lee, who was at the scene, said that "triad elements" had initiated scuffles with police "for reasons best known to themselves".[128] The police had arrested 37 protesters that weekend; the government said that nearly 70 people had been injured. At night, two pro-democracy lawmakers, Fernando Cheung and Claudia Mo, appeared at Mong Kok to mediate between the suffragists and the police, leading to a lowering of tensions as the police and suffragists each stepped back and widened the buffer zone. No clashes were reported for the night.[129]		On 20 October, a taxi drivers' union and the owner of CITIC Tower were granted a court injunction against the occupiers of sections of several roads.[130] In his first interview to international journalists since the start of the protests, CY Leung said that Hong Kong had been "lucky" that Beijing had not yet intervened in the protests, and repeated Chinese claims that "foreign forces" were involved.[131] He defended Beijing's stance on screening candidates. He said that open elections would result in pressure on candidates to create a welfare state, arguing that "If it's entirely a numbers game – numeric representation – then obviously you'd be talking to half the people in Hong Kong [that] earn less than US$1,800 a month [the median wage in HK]. You would end up with that kind of politics and policies."[132][133] A SCMP comment by columnist Alex Lo said of this interview: "Leung has set the gold standard on how not to do a media interview for generations of politicians to come."[134]		On 21 October, the government and the HKFS held the first round of talks in a televised open debate. HKFS secretary-general Alex Chow, vice secretary Lester Shum, general secretary Eason Chung, and standing members Nathan Law and Yvonne Leung met with HK government representatives Chief secretary Carrie Lam, secretary of justice Rimsky Yuen, undersecretary Raymond Tam, office director Edward Yau and undersecretary Lau Kong-wah. The discussion was moderated by Leonard Cheng, the president of Lingnan University.[135][136][137][138] During the talks, government representatives suggested the possibility of writing a new report on the students' concerns to supplement the government's last report on political reform to Beijing, but stressed that students' proposal of civil nomination falls outside of the framework imposed by the Basic Law and the NPCSC decision, which cannot be retracted.[139] The government described the talks as "candid and meaningful" in a press release, while the students expressed their disappointment at the lack of concrete results.[140]		On 22 October about 200 demonstrators marched to Government House, the official residence of the Chief Executive, in protest at his statement to journalists on 20 October about the need to deny political rights to the poor in Hong Kong.[141] At Mong Kok, members of the Taxi Drivers and Operators Association and a coalition of truck drivers attempted to enforce the court injunction granted two days earlier to remove barricades and clear the street. They were accompanied by their lawyer, who read out the court order to the demonstrators. Fist fights broke out during the afternoon and evening.[142]		On 23 October, a massive yellow banner which read "I want real universal suffrage" was hung on the Lion Rock, the iconic hill that overlooks the Kowloon Peninsula.[143] The location was chosen because Lion Rock represents Hong Kong's special spirit.[143][144][145] The banner was removed the following day.[146]		On 25 October, a group of anti-Occupy supporters wearing blue ribbons gathered at Tsim Sha Tsui to show their support of the police. Four journalists from RTHK and TVB tried to interview them and were attacked.[147] The police had to escort the journalists out.[147] A female reporter for RTHK, a male reporter and two photographers for TVB were taken to hospital.[148] A group of about 10 men wearing face masks attacked suffragists in Mong Kok.[149] Six people were arrested for common assault.[149] Alex Chow Yong-kang said that citizens deserved a chance to express their views over the constitutional reform proposal and the National People's Congress Standing Committee's decision of 31 August. He said that the protest would only end if the government offers a detailed timeline or roadmap to allow universal suffrage and withdrawal of the standing committee decision.[150][151]		On 28 October, the HKFS issued an open letter to the Chief Secretary Carrie Lam asking for a second round of talks. HKFS set out a prerequisite for the negotiation, that the government's report to the Chinese government must include a call for the retraction of the NPCSC's decision. The HKFS demanded direct talks with Chinese Premier Li Keqiang should the Hong Kong Government feel it cannot fulfil this and other terms.[152] The 30th day since the police fired tear gas was marked at 5.57 pm exactly, with 87 seconds of silence, one for each tear gas canister that was fired.[153]		On 29 October, after James Tien of the pro-Beijing Liberal Party urged Leung to consider resigning in a public interview on 24 October,[154] the Chinese People's Political Consultative Conference Standing Committee convened to discuss Tien's removal from the body as a move to whip the pro-establishment camp into supporting Leung and the country.[155] Tien, a long-time critic of Leung, said that Leung's position was no longer tenable as Hong Kong people no longer trusted his administration, and that his hanging onto office would only exacerbate the divisions in society.[156] Tien stepped down from his position as the leader of the Liberal Party after the removal.[157] Lester Shum refused bail extension based on conditions imposed after his arrest on 26 September, and was released unconditionally by police.[158] That day was also the day of the Umbrella Ultra Marathon event.		The anti-Occupy group Alliance for Peace and Democracy had run a petition throughout the end of October to the start of November, and at the end of their campaign claimed to have collected over 1.8 million signatures demanding the return of streets occupied by the protesters and restoration of law and order. Each signator are required to show a valid Hong Kong ID card and the final result is checked and verified to make sure there is no multiple voting by the same individual. The group's previous signature collection has been criticised as "lack of credibility" by its opponents.[159][160]		The High Court extended injunctions on 10 November that had been granted to taxi, mini-bus and bus operators authorising the clearance of protest sites. On the following day, Carrie Lam told reporters that there would be no further dialogue with protesters. She warned that "the police will give full assistance, including making arrests where necessary" in the clearance of the sites, and advised the protesters to leave "voluntarily and peacefully".[161] However, the granting of the court order and the conditions attached to the execution attracted controversy as some lawyers and a top judge questioned why the order was granted based on an ex parte hearing, the urgency of the matter, and the use of the police when the order was for a civil complaint.[162]		On 10 November, around 1,000 pro-democracy demonstrators, many wearing yellow ribbons and carrying yellow umbrellas, marched to the PRC Liaison Office in Sai Wan to protest the arrests of people expressing support for the protest.[163] The marchers included Alex Chow, who announced that the Federation of Students were writing to the 35 local delegates to the National People's Congress to enlist their help in setting up talks with Beijing.[164] On 30 October Chow and other student leaders had announced that they were considering plans to take their protest to the APEC summit to be held in Beijing on 10 and 11 November.[165] As observers had predicted, the student delegation led by Chow was prevented from travelling to China when they attempted to leave on 15 November.[166] Airline officials informed them that mainland authorities had revoked their Home Return Permits, effectively banning them from boarding the flight to speak to government officials in Beijing.[167]		On 12 November, media tycoon Jimmy Lai was the target of an offal attack at the Admiralty site by three men, who were detained by volunteer marshalls for the protest site.[168][169] Both the attackers and the two site marshalls who restrained them were arrested by the police, which led to condemnation by the pan-democracy camp, who organised an unauthorised protest march the next day. The two marshalls from the protest site were later released on bail.[170]		On the morning of 18 November, suffragists pre-emptively moved their tents and other affairs that were blocking access to Citic Tower, which was subject to a court injunction, avoiding confrontation with bailiffs and the police over the removal of barricades.[171]		In the early hours of 19 November, protesters broke into a side-entrance to the Legislative Council Complex, breaking glass panels with concrete tiles and metal barricades.[172] Legislator Fernando Cheung and other suffragists tried to stop the radical activists, but were pushed aside.[172][173] The break-in, which according to The Standard was instigated by Civic Passion,[174] was criticised by the three main activist groups of the protests, and legislators from both the pan-democracy and pro-Beijing camps.[172][173] Three police were injured and six men were arrested for criminal damage and assault.[173]		On 21 November, up to 100 people gathered outside the British consulate accusing the former colonial power of failing to pressure China to grant free elections in the city and protect freedoms guaranteed in the Sino-British Joint Declaration.[175]		Amidst declining support for the occupation, bailiffs and police cleared the tents and barriers in the most volatile of the three Occupy sites, Mong Kok, on 25 and early 26 November. Suffragists poured into Mong Kok after the first day's clearance, and there was a stand-off between protesters and police the next day. Scuffles were reported, and pepper spray was used. Police detained 116 people during the clearance, including student leaders Joshua Wong and Lester Shum.[176] Joshua Wong, Lester Shum and some 30 of those arrested were bailed but subject to an exclusion zone centred around Mong Kok Station.[177][178] Mong Kok remained the centre of focus for several days after the clearance of the occupied area when members of the public angry about heavy-handed policing.[179][180] Fearing re-occupation, in excess of 4,000 police were deployed to the area.[179][180] Large crowds, ostensibly heeding a call from C. Y. Leung to return to the shops affected by the occupation, appeared nightly in and around Sai Yeung Choi Street South (close to the former occupied site); hundreds of armed riot police charged demonstrators with shields, pepper spraying and wrestling them to the ground. Protesters intent on "Gau Wu" (shopping) remained until dawn.[179][180]		Overnight on 30 November, there were violent clashes between police and protesters in Admiralty after the Federation of Students and Scholarism called upon the crowd to surround the Central Government Offices. The police used a hose to splash protesters for the first time. The entrance to the Admiralty Centre was also blocked. Most of the violence occurred near Admiralty MTR station.[181] Also, Joshua Wong and two other Scholarism members began an indefinite hunger strike.[182]		On 3 December, the OCLP trio, along with 62 others, including lawmaker Wu Chi-wai and Cardinal Joseph Zen, turned themselves in to the police to bear the legal consequences of civil disobedience. However, they were set free without being arrested or charged.[183] They also urged occupiers to leave and transform the movement into a community campaign, citing concerns for their safety amidst the police's escalation of force in recent crackdowns.[184] Nonetheless, HKFS and Scholarism both continued the occupation. Nightly "Gau Wu" tours continued in Mong Kok for over a week after the clearance of the occupation site, tying up some 2500 police officers.[185] The minibus company that took out the Mong Kok injunction was in turn accused of having illegally occupied Tung Choi Street for years.[186]		On the morning of 11 December, many protesters left the Admiralty site before crews of the bus company that had applied for the Admiralty injunction dismantled roadblocks without resistance. Afterwards, the police set a deadline for protesters to leave the occupied areas and cordoned off the zone for the remainder of the day.[187] 209 protesters declined to leave and were arrested,[188][189][190] including several pan-democratic legislators and members of HKFS and Scholarism.[191] Meanwhile, the police set the bridge access to Citic Tower and Central Government Office only allowing media to access. The Independent Police Complaints Council was present to monitor the area for any "excessive use of force" along with fifty professors[192]		On 15 December, police cleared protesters and their camps at Causeway Bay with essentially no resistance, bringing the protests to an end.[193][194]		Surface traffic between Central and Admiralty, Causeway Bay, as well as in Mong Kok, was seriously affected by the blockades, with traffic jams stretching for miles on Hong Kong Island and across Victoria Harbour.[195][196] Major tailbacks were reported on Queensway, Gloucester Road and Connaught Road, which are feeder roads to the blockaded route in Admiralty.[57] Whilst in excess of 100 bus or tram routes have been suspended or re-routed,[197] queues for underground trains in the Admiralty district stretched out onto the street at times.[195] The MTR, the city's underground transport operator, has been a beneficiary.[198] The number of passenger trips recorded on two of its lines has increased by 20 percent.[199] Others have opted to walk instead of driving.[200] Taxi drivers have reported a fall in income as they have had to advise passengers to use the MTR when faced with jams, diversions or bloackaded roads.[201] Hong Kong Taxi Owners' Association claimed its members' incomes had declined by 30 percent since the protests started.[202] Levels of PM2.5 particulate matter at the three sites descended to within the recommended safety levels of the World Health Organization.[203][204] An editorial in the South China Morning Post noted that, on 29 September, the air quality in all three of the occupied areas had markedly improved. The health risk posed by airborne pollutants was "low" – it is usually "high" – and there was a steep fall in the concentration of NO2. It said: "without a policy shift, after the demonstrations have ended, we will have to rely on our memories of the protest days for what clean vehicles on our roads mean for air quality".[205]		Nursery, primary and secondary schools within the Central and Western catchment areas were suspended from 29 September onwards. Classes for 25,000 primary students and 30,000 secondary students resumed on 7 October.[206][207][208] Kindergartens and nursery schools resumed operations on 9 October, adding to the traffic burden.[198] The Hong Kong Retail Management Association reported that chain stores takings declined between 30 and 45 percent during the period 1–5 October in Admiralty, Central and Causeway Bay.[209] The media reported that some shops and banks in the protest areas were shuttered.[197]		According to the World Bank, the protests were damaging Hong Kong's economy while China remained largely unaffected.[195] Although the Hang Seng Index fell by 2.59% during the "Golden Week", it recovered and trading volume rose considerably.[210] Shanghai Daily published on 4 October estimated that the protests had cost Hong Kong HK$40 billion ($5.2 billion), with tourism and retail reportedly being hardest hit. However, tourist numbers for the "Golden Week" (beginning 1 October) were 4.83% higher than the previous year, according to the Hong Kong Tourism Board. While substantial losses by retailer were predicted, some stores reported a marked increase in sales.[210] Triad gangs, which had reportedly suffered a 40% decline in revenues, were implicated in the attacks in Mong Kok, where some of the worst violence had occurred.[86][101][211][212][213] Economic effects seemed either to be extremely localised or transient, and in any event much less than the dire predictions of business lobbies. One of the hardest hit may have been the Hong Kong Tramways Company, which reported a decline in revenues of US$1 million.[214][215] An economist said that the future stability will depend on political governance, namely if political issues such as income gaps and political reforms will be addressed.[212]		The protests are causing strong differences of opinion in Hong Kong society, with a "yellow (pro-occupy) vs. blue (anti-occupy)" war being fought, and unfriending on social media, such as Facebook.[24] The media have reported conflict within peer groups over values or what positions may be orthodox, and rifts have formed between mentor–mentees over the extent to which the movement should go. Parents have rowed with their children over their attending protests.[216] Hong Kong people who oppose the Occupy protests do so for a number of different reasons. A significant part of the population, refugees from Communist China in the 1950s and 1960s, lived through the turmoil of the Hong Kong 1967 Leftist riots. Others feel that the protesters are too idealistic, and fear upsetting the PRC leadership and the possibility of another repeat of the crackdown that ended the Tiananmen Square protests of 1989.[217] However, the overwhelming reason is the disruption to the lives of ordinary citizens caused by roads blocked, traffic jams, school closures, and financial loss to businesses (including in particular those run by the Triads in Mong Kok).[217] According to some reports, the police actions on the protesters has resulted in a breakdown of citizens' trust in the previously respected police force. The police deny accusations that they failed to act diligently.[64] The media have reported on individuals who have quit their jobs, or students abroad who have rushed home to become a part of history, and one protester saw this as "the best and last opportunity for Hong Kong people's voices to be heard, as Beijing's influence grows increasingly stronger".[66] Police officers have been working 18-hour shifts to the detriment of their family lives.[218] Front line police officers, in addition to working long hours, being attacked and abused on the streets, are under unprecedented stress at home. Psychologists working with police officers in the field report that some felt humiliated as they may have been unfriended on Facebook, and family may blame them for their perceived roles in suppressing the protests.[219][220][221] Although the media has often dubbed it "Asia's Finest", the reputation of the police has taken a serious drubbing following the heavy-handed treatment of protesters, as well as police brutality captured on camera and made viral.[29] Andy Tsang, the police commissioner appointed in 2011, is held responsible for the procedural escalation of police violence in the face of protesters, through deployment of riot police and 87 instances in which tear gas was released; dispersal of unarmed students also caused disquiet among senior police staffers.[29][222]		In an opinion poll of Hong Kong citizens carried out since 4 October by Hong Kong Polytechnic University, 59% of the 850 people surveyed supported the protesters in their refusal to accept the government plan for the 2017 election. 29% of those questioned, the largest proportion, blamed the violence that had occurred during the demonstrations on the chief executive CY Leung.[218]		The BBC showed video footage from a Hong Kong TV network which appeared to show 'anti-Occupy protesters' being hired and transported to an Occupy protest site. The 'protesters', many of whom were initially unaware of what they were being paid to do, were secretly filmed on the bus being handed money by the organiser. Anonymous police sources informed the BBC Newsnight investigation that "back-up was strangely unforthcoming" to scenes of violence. The South China Morning Post also reported claims that people from poor districts were being offered up to HK$800 per day, via WhatsApp messaging, to participate in anti-Occupy riots.[64][223]		The HK police has stated that up to 200 gangsters from two major triads may have infiltrated the camps of Occupy Central supporters, although their exact motives are as yet unknown. A police officer explained the police could not arrest the triad gangsters there "if they do nothing more than singing songs for democracy".[224] A 2013 editorial in the Taipei Times of Taiwan described the pro-Beijing "grass-roots" organisations in Hong Kong: "Since Leung has been in office, three organizations – Voice of Loving Hong Kong, Caring Hong Kong Power and the Hong Kong Youth Care Association – have appeared on the scene and have been playing the role of Leung's hired "thugs", using Cultural Revolution-style language and methods to oppose Hong Kong's pan-democratic parties and groups."[225] Both Apple Daily and the Taiwan Central News Agency, as well as some pan-democrat legislators in Hong Kong, have named the Ministry of State Security and Ministry of Public Security as being responsible for the attacks.[226][227][228]		Legislative Council member James To alleged that "The police is happy to let the triad elements to threaten the students, at least for several hours, to see whether they would disperse or not." He added, "Someone, with political motive, is utilising the triad to clear the crowd, so as to help the government to advance their cause."[229] Amnesty International condemned the police for "[failing] in their duty to protect protesters from attacks" and stating that women were attacked, threatened, and sexually assaulted while police watched and did nothing.[64] Commander Paul Edmiston of the police admitted officers had been working long hours and had received heavy criticism. Responding to accusations that police chose not to protect the protesters, he said: "No matter what we do, we’re criticized for doing too little or too much. We can't win."[70] An analysis in Harbour Times suggested that businesses that pay protection money to Triads in the neighbourhood stood to be affected by an occupation.[19] The journal criticised police response as being at first disorganised and slow onto the scene, but observed that its handling was within operating norms in triad-heavy neighbourhoods although it was affected by low levels of mutual trust, suspicion.[19]		Many of Hong Kong's media outlets are owned by local tycoons who have significant business ties in the mainland, so they all adopt self-censorship at some level and have mostly maintained a conservative editorial line in their coverage of the protests.[230] Next Media, being Hong Kong's only openly pro-democracy media conglomerate, has been the target of blockades by anti-Occupy protesters, cyberattacks, and hijacks of their delivery trucks. The uneven spread of viewpoints on traditional media has turned young people to social media for news, which The Guardian has described as making the protests "the best-documented social movement in history, with even its quieter moments generating a maelstrom of status updates, shares and likes."[231] People at protest sites now rely on alternative media whose launches were propelled by the protests, also called "umbrella revolution", or actively covered news from a perspective not found in traditional journals. Even the recently defunct House News resurrected itself, reformatted as The House News Bloggers. Radical viewpoints are catered for at Hong Kong Peanut, and Passion Times – run by Civic Passion.[230]		The prominent local station, TVB, originally broadcast footage of police officers beating a protester on 15 October, but the station experienced internal conflict during the broadcast.[232] The pre-dawn broadcasts soundtrack mentioning "punching and kicking" was re-recorded to say that the officers were "suspected of using excessive force".[233] Secret audio recordings from an internal meeting were uploaded onto YouTube that included the voice of TVB director Keith Yuen Chi-wai asking "On what grounds can we say officers dragged him to a dark corner, and punched and kicked him?"[233] The protester was later named as Civic Party member Ken Tsang, who was also a member of the Election Committee that returned CY Leung as the city's Chief Executive.[232] About 57 journalists expressed their dissatisfaction with the handling of the broadcast. A petition by TVB staff to management protesting the handling of the event was signed by news staff.[232] The list grew to 80+ people including employees from sports, economics and other departments.[234] In 2015, the video, entitled "Suspected Police Brutality Against Occupy Central Movement's Protester", was declared the Best TV news item at the 55th Monte Carlo TV Festival; it was praised for its "comprehensive, objective and professional" report. It also won a prize at the Edward E. Murrow Awards in the Hard News category.[235]		Internet security firm CloudFlare said that, like for the attacks on PopVote sponsored by OCLP earlier in the year, the volume of junk traffic aimed at paralysing Apple Daily servers was an unprecedented 500Gbit/s and involved at least five botnets. Servers were bombarded with in excess of 250 million DNS requests per second, equivalent to the average volume of DNS requests for the entire Internet. And where the attacks do not succeed directly, they have caused some internet service providers to pre-emptively block such sites under attack to protect their own servers and lines.[236]		Beijing is generally reported as being concerned about similar popular demands for political reform on the mainland that would erode the Communist Party's hold on power.[42] Reuters sources revealed that the decision to offer no concessions was made at a meeting of the National Security Commission of the Communist Party of China chaired by General secretary Xi Jinping in the first week of October. "[We] move back one step and the dam will burst," a source was reported as saying, referring to mainland provinces such as Xinjiang and Tibet making similar demands for democratic elections.[237][238] The New York Times China correspondents say that the strategy for dealing with the crisis in Hong Kong was being planned under supervision from the top-tier national leadership, which was being briefed on a daily basis. According to the report, Hong Kong officials are in meetings behind the scenes with mainland officials in neighbouring Shenzhen, at a resort owned by the central government liaison office.[239] Beijing's direct involvement was confirmed subsequently by pro-establishment figures in Hong Kong.[240] The HKFS, which had been hoping to send a delegation to meet with the leadership in Beijing, was rebuffed by Tung Chee-hwa, vice-chairman of the NPC, whom they asked to help set up the meetings.[241][242]		Xi Jinping stated his support for CY Leung on the 44th day of the occupation, saying the occupation was a "direct challenge not just to the SAR and its governance but also to Beijing". Xi also said that Leung's administration must govern to safeguard the rule of law and maintain social order.[243]		On 28 September it emerged that Chinese government authorities had issued the following censorship directive: "All websites must immediately clear away information about Hong Kong students violently assaulting the government and about 'Occupy Central.' Promptly report any issues. Strictly manage interactive channels, and resolutely delete harmful information. This [directive] must be followed precisely."[244][245][246] Censors rapidly deleted messages internet posts with words such as "Hong Kong," "barricades", "Occupy Central" and "umbrella".[247][248] Sections of the CNN reporting from Hong Kong was also disrupted.[247] Most Chinese newspapers have not covered the protests except for editorials critical of the protests and devoid of any context,[247][249] or articles mentioning the negative impact of the occupation.[250] The Chinese website of the BBC was completely blocked after a video showing the violent assault on a protester by police on 15 October hosted on the site went viral.[251] Amnesty International reported that dozens of Chinese people have been arrested for showing support for the protests.[252] Facebook and Twitter are already blocked on the mainland, and now as a result of the sharing of images of the protests, PRC censors have now blocked Instagram.[248][253] However, Reuters noted that searches for "Umbrella Revolution" up to 30 September escaped censors on Sina Weibo but not on Tencent Weibo.[254] Despite this, certain American-funded reporting by the Voice of America and Radio Free Asia was able to break through some of the internet censors and provide information on the protests to inhabitants of the Chinese mainland.[255]		Mainland Chinese officials and media have repeatedly alleged that outside forces formented the protests. Li Fei, the first Chinese official to address Hong Kong about the NPCSC decision, accused democracy advocates of being tools for subversion by Western forces who were set at undermining the authority of the Communist Party. Li alleged that they were "sowing confusion" and "misleading society".[42] The People's Daily claimed that organisers of the Hong Kong protests learned their tactics from supporters of the Sunflower Student Movement in Taiwan, having first sought support from the United Kingdom and the United States.[256][257] Scholarism has been labelled as extremists and a pro-Beijing journal in Hong Kong alleged that Joshua Wong had been cultivated by "US forces".[258] In one of numerous editorials condemning the occupation, the People's Daily said "The US may enjoy the sweet taste of interfering in other countries' internal affairs, but on the issue of Hong Kong it stands little chance of overcoming the determination of the Chinese government to maintain stability and prosperity".[259] It alleged that the US National Endowment for Democracy was behind the protests, and that a director of the organisation had met with protest leaders.[260] On 15 October, an unnamed Chinese government official stated that "interference certainly exists", citing "the statements and the rhetoric and the behaviour of the outside forces of political figures, of some parliamentarians and individual media".[261]		In a televised interview on 19 October, Chief Executive CY Leung echoed Chinese claims about foreign responsibility for the protests, but declined to give details until an "appropriate time".[259][262] Six months later, on 22 April 2015, a reporter asked Leung, "has that time come yet?" Leung simply responded, "Well, I stand by what I say."[263]		The US State Department has categorically rejected accusations of interference, calling the charges "an attempt to distract from...the people expressing their desire for universal suffrage."[264] The South China Morning Post characterised claims of foreign interference as "vastly exaggerated",[265] and longtime Hong Kong democracy advocate Martin Lee said such claims were a "'convenient excuse' for Beijing to cover its shame for not granting the territory true democracy as it once promised."[266]		The China Media Project of the University of Hong Kong noted that the phrase "hostile forces" (敌对势力) – a hardline Stalinist term – has been frequently used in a conspiracy theory alleging foreign sources of instigation.[267] Apart from being used as a straightforward means to avoid blame, analysts said that Chinese claims of foreign involvement, which may be rooted in Marxist ideology, or simply in an authoritarian belief that "spontaneity is impossible", are "a pre-emptive strike making it very difficult for the American and British governments" to support the protests.[22][268]		On 1 October, China News Service criticised the protesters for "bringing shame to the rule of law in Hong Kong";[269] the People's Daily said that the Beijing stance on Hong Kong's elections is "unshakeable" and legally valid. Stating that the illegal occupation was hurting Hong Kong, it warned of "unimaginable consequences"[270] Some observers remarked that the editorial was similar to the April 26 Editorial that foreshadowed the suppression of the Tiananmen Square protests of 1989.[271][272] A state television editorial urged authorities to "deploy police enforcement decisively" and "restore the social order in Hong Kong as soon as possible," and again warned of "unimaginable consequences",[273] and a front page commentary in People's Daily on 3 October repeated that the protests "could lead to deaths and injuries and other grave consequences."[16][274]		By 6 October, official Chinese media outlets called for "all the people to create an anti-Occupy Central atmosphere in the society". The protesters were described as "going against the principle of democracy". A commentary in the China Review News claimed that "the US is now hesitant in its support for the Occupy Central. If those campaign organisers suddenly soften their approach, it will show that their American masters are giving out a different order."[275][276]		Chinese government officials have routinely affirmed the Chinese government's firm support for the chief executive and for the continued "necessary, reasonable and lawful" actions by the police against the illegal protests.[131][261][269]		While the Western press noticed the apparent silence of Hong Kong's richest businessmen since the occupation began,[277][278][279] Xinhua News Agency posted an English-language article in the morning of 25 October criticising the absence of condemnation of the occupation from the city's tycoons in response to the protest, but the article was deleted several hours later.[280][281] A replacement article that appeared that evening, in Chinese, stated how tycoons strongly condemned the protest, and quoted a number of them with pre-occupation soundbites reiterating how the occupation would damage Hong Kong's international reputation, disrupt social disorder and cause other harmful problems to society.[280]		Deputy director of China's National People's Congress Internal and Judicial Affairs Committee, Li Shenming, stated: "In today's China, engaging in an election system of one-man-one-vote is bound to quickly lead to turmoil, unrest and even a situation of civil war."[282] The mainland media also contested the protesters demands for democracy by blaming the colonial rulers, saying Britain "gave our Hong Kong compatriots not one single day of it", notwithstanding the fact that de-classified British diplomatic documents indicate that the lack of democracy since at least late 1950s was largely attributable to the refusal of the PRC to allow it.[283]		The Chinese authorities are rumoured to have blacklisted 47 entertainers from Hong Kong who had openly supported the suffragists, and the list made the rounds on social media.[284] Denise Ho, Chapman To and actor Anthony Wong, who are among the highest profile supporters of the movement, were strongly criticised by the official Xinhua News Agency.[285] In response to the possible ban from the Chinese market, Chow Yun-fat, was quoted as saying "I'll just make less, then". Reporting of Chow's riposte was subject to Mainland Chinese internet censors.[286]		Beijing refused to grant a visa to Richard Graham, British member of parliament who had said in a parliamentary debate on Hong Kong that Britain had a duty to uphold the principles of the Sino-British joint declaration. This resulted in the cancellation of a visit by a cross-party parliament group due to visit China, led by Peter Mandelson. Graham had also asserted that "Stability for nations is not, in our eyes, about maintaining the status quo regardless, but about reaching out for greater involvement with the people – in this case, of Hong Kong – allowing them a greater say in choosing their leaders and, above all, trusting in the people".[287]		In urging students to set aside their protest, Bao Tong, the former political secretary of CPC general secretary Zhao Ziyang, said he could not predict what the leadership would do.[288] He believed Zhao meant universal suffrage where everyone had the right to vote freely, and not this "special election with Chinese characteristics".[288][289] Bao said today's PRC leaders should respect the principle that HK citizens rule themselves, or Deng Xiaoping's promises to Hong Kong would have been fake.[288][289] Hu Jia co-authored an opinion piece for the Wall Street Journal, in which he wrote "China has the potential to become an even more relentless, aggressive dictatorship than Russia... Only a strong, unambiguous warning from the US will cause either of those countries to carefully consider the costs of new violent acts of repression. Hong Kong and Ukraine are calling for the rebirth of American global leadership for freedom and democracy.[290]		Amnesty International said that at least 37 mainland Chinese have been detained for supporting Hong Kong protesters in different ways: some posted pictures and messages online, others had been planning to travel to Hong Kong to join protesters. A poetry reading planned for 2 October in Beijing's Songzhuang art colony to support Hong Kong protesters was disrupted, and a total of eight people were detained. A further 60 people have been taken in for questioning by police.[291][292] Amnesty reported in February 2015 that at least two of those arrested have been tortured, and nine denied legal representation; one was given access to a lawyer only after being sleep-deprived and tortured for five days. The whereabouts of four are unknown.[293]		Former Chief Secretary Anson Chan expressed disappointment at Britain's silence on the matter and urged Britain to assert its legal and moral responsibility towards Hong Kong and not just think about trade opportunities. Chan dismissed China's accusation of foreign interference, saying: "Nobody from outside could possibly stir up this sort of depth of anger and frustration."[294] Former Legco president Rita Fan said "to support the movement, some protesters background have resources that are supported by foreign forces using young people for a cause. To pursue democracy that effects other people's livelihood is a form of democratic dictatorship."[295]		Director of Hong Kong Human Rights Monitor, Law Yuk-kai, was dissatisfied with the unnecessary violence by the police. He said students only broke into the Civic Square to sit-in peacefully with no intentions of destroying government premises.[296] He questioned the mobilisation of riot police while protesters staged no conflict. Also, the overuse of batons was underestimated by the police because the weapon could severely harm protesters.[296] Legislative Council Chairman Jasper Tsang Yok-sing has disagreed that the police were excessively violent, saying they would not misuse pepper spray.[297] and contrary to the claims of other pro-establishment members, Tsang sees little evidence of "foreign forces" at play.[298] Member of Legislative Council Albert Ho of Democratic Party said, "[Attack on protesters] was one of the tactics used by the communists in mainland China from time to time. They use triads or pro-government mobs to try to attack you so the government will not have to assume responsibility."[299]		Former Chief Executive Tung Chee-hwa when urging the students to end the occupation, praised their "great sacrifice" in the pursuit of democracy, and said that "the rule of law and obeying the law form the cornerstone of democracy."[300]		On 29 October, chairman of the Financial Services Development Council and Executive Councillor, Laura Cha, created controversy for the government and for HSBC, of which she is a board member, when she said: "African-American slaves were liberated in 1861, but did not get voting rights until 107 years later. So why can't Hong Kong wait for a while?" An online petition called for her to apologise and withdraw her remarks. A spokesman for the Executive Council stated in an e-mail on 31 October that "She did not mean any disrespect and regrets that her comment has caused concerns".[20][301][302][303]		The Federation of Hong Kong Industries, whose 3,000 manufacturer members are largely unaffected as manufacturing in Hong Kong has been largely de-localised to the mainland, oppose the protests, due to concerns for the effects on investor confidence.[281] While the business groups have expressed concern at the disruption caused to their members,[304][305] the city's wealthiest individuals have kept a relatively low-profile as they faced the dilemma of losing the patronage of CPC leadership while trying to avoid further escalation with overt condemnations of the movement.[281] On the 19th day, Li Ka-Shing recognised that students' voices had been noted by Beijing, and urged them to go home "to avoid any regret".[306] Li was, however, criticised by Xinhua for not being unambiguous in his opposition for the movement and his support for Leung.[281] Lui Che Woo, one of the richest men in Asia, appeared to hold a more pro-Beijing stance by stating that "citizens should be thankful to the police".[307] Lui was opposed to "any activity that has a negative impact on the Hong Kong economy".[281]		On 23 October, the UN Human Rights Committee, which monitors compliance with the International Covenant on Civil and Political Rights, urged China to allow free elections in Hong Kong.[308][309] The committee emphasised specifically that 'universal suffrage' includes the right to stand for office as well as the right to vote. Describing China's actions as "not satisfactory", the committee's chairman Konstantine Vardzelashvili announced that "The main concerns of Committee members were focused on the right to stand for elections without unreasonable restrictions."[310]		A spokesman for China's Foreign Ministry confirmed on the following day that the Covenant, signed by China in 1998, did apply to Hong Kong, but said that, nonetheless, "The covenant is not a measure for Hong Kong's political reform", and that China's policy on Hong Kong's elections had "unshakable legal status and effect". Reuters observed that "It was not immediately clear how, if the covenant applied to Hong Kong, it could have no bearing on its political reform."[311]		Leaders of countries, including Australia, Canada, France, Spain, Germany, Italy, Japan, Taiwan, Vatican City, United Kingdom, and the United States, supported the protesters' right to protest and their cause of universal suffrage and urged restraint on all sides, with the notable exception of Russia, whose state media claimed that the protests were another West-sponsored colour revolution similar to the Euromaidan.[22][312][313] German president Joachim Gauck, celebrating the 24th anniversary of German reunification, praised the spirit of Hong Kong's suffragists to their own of 24 years ago who overcame their fear of their oppressors;[314] Chancellor Angela Merkel said freedom of speech should remain guaranteed by law in Hong Kong.[315]		British Prime Minister David Cameron expressed deep concern about clashes in Hong Kong and said that he felt an obligation to the former colony.[316][317] Cameron said on 15 October that Britain should stand up for the rights set out in the Anglo-Chinese agreement.[318] The Foreign Office called on Hong Kong to uphold residents' rights to demonstrate, and said that the best way to guarantee these rights is through transition to universal suffrage.[319][320] Former Hong Kong Governor and current Chancellor of the University of Oxford Chris Patten expressed support for the protests[321] and denounced the Iranian-style democratic model for the city.[322] Citing China's obligation to Britain to adhere to the terms of Sino-British Joint Declaration,[323] he urged the British government to put greater pressure on the Chinese state, and to help China and Hong Kong find a solution to the impasse.[324] The Chinese Foreign Ministry said Patten should realise that "times have changed",[325] and that no party had the right to interfere in China's domestic affairs.[326]		British member of parliament and chairman of the Commons Parliamentary Committee on Foreign Affairs, Richard Ottaway, denounced China's declaration that the committee would be refused permission to enter Hong Kong on their planned visit in late December as part of their inquiry into progress of the implementation of the Sino-British Joint Declaration. Ottaway sought confirmation from the China's deputy ambassador after receiving a letter from the central government that his group's visit "would be perceived to be siding with the protesters involved in Occupy Central and other illegal activities", and was told that the group would be turned back.[327]		In Taiwan, the situation in Hong Kong is closely monitored since China aims to reunify the island with a "one country, two systems" model similar to one that is used in Hong Kong.[328] President Ma Ying-jeou expressed concern for the developments in Hong Kong and its future,[329] and said the realisation of universal suffrage will be a win-win scenario for both Hong Kong and mainland China.[330] On 10 October, Taiwan's National Day, President Ma urged China to introduce constitutional democracy, saying "now that the 1.3 billion people on the mainland have become moderately wealthy, they will of course wish to enjoy greater democracy and rule of law. Such a desire has never been a monopoly of the west, but is the right of all humankind."[331] In response to Ma's comments, China's Taiwan Affairs Office said Beijing was "firmly opposed to remarks on China's political system and Hong Kong's political reforms .... Taiwan should refrain from commenting on the issue."[332]		The protests captured the attention of the world and gained extensive global media coverage.[333] Student leader Joshua Wong featured on the cover of Time magazine during the week of his 18th birthday,[334] and the movement was written about, also as a cover story, the following week.[335] While the local pan-democrats and the majority of the Western press supported the protesters' aspirations for universal suffrage,[333] Martin Jacques, writing for The Guardian, argued that the PRC had "overwhelmingly honoured its commitment to the principle of one country, two systems". He believed that the reason for the unrest is "the growing sense of dislocation among a section of Hong Kong's population" since 1997.[336] Tim Summers, in an op-ed for CNN, said that the protests were fuelled by dissatisfaction with the Hong Kong government, but the catalyst was the decision of the NPCSC. Criticising politicians' and the media's interpretation of the agreements and undertakings of the PRC, Summers said "all the Joint Declaration said is that the chief executive will be 'appointed by the central people's government on the basis of the results of elections or consultations to be held locally [in Hong Kong].' Britain's role as co-signatory of that agreement gives it no legal basis for complaint on this particular point, and the lack of democracy for the executive branch before 1997 leaves it little moral high ground either."[337]		Once traffic resumed, roadside PM2.5 readings rose back up to levels in excess of WHO recommended safe levels of 25 µg/m³. According to the Clean Air Network, PM2.5 levels at Admiralty stood at 33 µg/m³, an increase of 83% since during the occupation; Causeway Bay measured 31 µg/m³, an increase of 55%, and Mong Kok's reading of 37 µg/m3 represents an increase of 42%.[203][204] The former director of the government archives, Simon Chu, expressed concern about preservation of official documents pertaining to the protest movement, and was seeking a proxy to file an injunction on the government. He feared that the absence of a law on official archives in Hong Kong meant that senior government officials may seek to destroy all documents involving deliberations, decisions and actions taken while the protests were ongoing.[338]		Chief Executive CY Leung said that protesters need to carefully consider what sort of democracy they are pursuing.[339] He welcomed the end of the occupation, saying: "Other than economic losses, I believe the greatest loss Hong Kong society has suffered is the damage to the rule of law by a small group of people... If we just talk about democracy without talking about the rule of law, it's not real democracy but a state of no government".[340] Leung saw his popularity ratings slump to a new low following the occupation protests, down to 39.7 percent, with a net of minus 37%. This was attributed to public perception of Leung's unwillingness to heal the wounds, and his unwarranted[according to whom?] shifting of the blame for the wrongs in society onto opponents. Leung also claimed negative effects on the economy without providing evidence, and his assertions were contradicted by official figures.[341] On, 19 December 2014, the eve of the 15th anniversary of Macau's handover, authorities in Macau banned journalists covering the arrival of Chinese president Xi Jinping from holding umbrellas in the rain.[342]		Commissioner of the Police Andy Tsang confirmed the unprecedented challenges to the police posed by the occupations, and that as at 15 December a total of 955 individuals had been arrested,[339][343] 221 activists had been hurt, and that 130 police officers had received light injuries.[343] At the same time, Tsang anticipated further arrests, pending a three-month investigation into the occupation movement.[343] Most activists call in under arrest by appointment remain to be formally charged, and although police said that they reserved the right to prosecute, pro-democracy legislators complained that the uncertain impending prosecution hangs over the interviewees constituted an act of intimidation.[344]		Although the occupations had ended, aggressive policing that became a hallmark of the official antipathy towards peaceful protests continued – as illustrated by police application for Care and Protection Orders (CPO) for two young suffragists in December 2014.[345] Typically, CPOs are only used in severe cases of juvenile delinquency, and could lead to the minor being sent to a children's home and removed from his parents' custody.[345] Police arrested one 14-year-old male for contempt of court during the clearance of Mong Kok and applied for a CPO.[345][346] The CPO was cancelled four weeks later when the Department of Justice decided that they would not prosecute.[345] In a second case, a 14-year-old female who drew a chalk flower onto the Lennon Wall on 23 December 2014 was arrested on suspicion of criminal damage, but was not charged. A magistrate decided in favour of a CPO pursuant to a police application, deeming it "safer". The incident created uproar as she was taken away from her hearing-impaired father, and was unable to go to school.[347][348][349] On 19 January, another magistrate rescinded the protection order for "Chalk Girl".[350] The handling of the situation by the police raised concerns, as there was no explanation as to why the police failed to locate and consult a social worker before applying for the order in accordance with proper procedures.[351] Usage of the protection orders against minors involved in the Umbrella movement was seen as "white terror" to deter young people from protesting.[345]		The Economic Journal predicts a rout as a result of growing alienation and disaffection with the system and with traditional politics. It criticised the means the government employed to deal with the problem, and said that: "[the SAR government's] legitimacy to govern has been deeply damaged. Officials may be made scapegoats for the mass protests, and the police may have forfeited much of their hard-earned reputation and sound relationship with citizens following charges of brutality and links with triads. The judiciary has also taken a beating after it issued injunctions against the occupation of roads in Mong Kok and Admiralty. This has left many people with the perception that it has colluded with the government and the checks and balances between the two powers are now gone. The government's ill-conceived plan to crack down hard on the protesters under the guise of assisting bailiffs sets a dangerous precedent."[352]		An editorial in The Wall Street Journal said that despite the establishment attempting to portray the occupy movement as a threat to Hong Kong, "it's clear that the real threat to Hong Kong comes from those who bend to Beijing's whims. China and its local proxies ... have mounted a violent march through the institutions that have sustained Hong Kong's stability and prosperity—independent courts, free press, honest law enforcement and more".[353] An editorial in the Washington Post predicted that "Political unrest is likely to become a chronic condition in a place that until now had mostly accepted the authority of the Communist regime since 1997... China's inflexible response to the democracy movement may yield exactly the results it wishes to avoid: an unmanageable political situation in Hong Kong and the spread of the demand for political freedom".[354]		A Guardian editorial wrote: "What China has done in Hong Kong will preserve control but deepen alienation... outside China, where it is seen as yet another indication that compromise and the Chinese communist party are strangers to each other, whether in dealing with non-Han minorities, in territorial issues with neighbours or in relations with other major states." It said that the one country, two systems formula "has been almost completely discredited by events in Hong Kong". It added that "The Chinese are prisoners of another narrative, in which China's rise is a phenomenon benefiting its neighbours as much as itself, in which opponents are seen as a tiny minority manipulated by hostile powers, and in which democracy is a flawed western concept that has no relevance for China".[355]		Business Spectator described the mixed legacy of the resilient protest community that has been created and galvanisation of youth – a previously apolitical section of society – into political activism or heightened awareness of their civil rights and responsibilities. Against that, the resulting divisions in Hong Kong society will leave a void for Beijing to strengthen its role and influence.[26] The journal also agreed that Hong Kong's institutions had been damaged through government actions as well as inaction. Citing the government and police choice to employ civil injunctions to justify clearing the protest sites, it said "In so doing, the government has called the court system into service as a political tool. This politicisation of the court system is known as 'rule by law', a phrase frequently applied to the PRC and which could now be used for the first time with some justification in the context of Hong Kong." It further predicts further erosion of press and academic freedoms, as universities have been shown to be seed beds of political activism and potential subversion.[26]		In the Journal of Chinese Political Science, Dylan Loh of Nanyang Technological University noted how the Umbrella Revolution was an instance of "defensive soft power' and that "through the mounting of this defence, certain state-sanctioned images of China are reinforced and promoted. Specifically, these include promoting images that depict China as a victim; portraying China as a reasonable and restraint power; constructing the image of wide support for China’s handling of the event and of its of governance; and promoting the perception of strong relations between Macau/Taiwan and the mainland. It contends that these state-aligned images were released defensively to counter the attacks by the western media and this process allowed the Central authorities an opportunity to reinforce and articulate its national images. "[356]		
An alternative school is an educational establishment with a curriculum and methods that are nontraditional.[1][2] Such schools offer a wide range of philosophies and teaching methods; some have strong political, scholarly, or philosophical orientations, while others are more ad hoc assemblies of teachers and students dissatisfied with some aspect of mainstream or traditional education.		Some schools are based on pedagogical approaches differing from that of the mainstream pedagogy employed in a culture, while other schools are for gifted students, children with special needs, children who have fallen off the track educationally, children who wish to explore unstructured or less rigid system of learning, etc.						There are many models of alternative schools but the features of promising alternative programs seem to converge more or less on the following characteristics:		In the United Kingdom, 'alternative school' refers to a school that provides a learner centered informal education as an alternative to the regimen of traditional education in the United Kingdom.[5] There's a long tradition of such schools in the United Kingdom, going back to Summerhill, whose founder, A. S. Neill, greatly influenced the spread of similar democratic type schools such as the famous Dartington Hall School, and Kilquhanity School,[6] both now closed. Currently there are two democratic primary schools, Park School and Small Acres, and two democratic secondary schools, Summerhill and Sands School.[7] There are also a range of schools based on the ideas of Maria Montessori and Rudolf Steiner.[8]		In the United States, there has been tremendous growth in the number of alternative schools in operation since the 1970s, when relatively few existed.[9][10] Some alternative schools are for students of all academic levels and abilities who are better served by a non-traditional program. Others are specifically intended for students with special educational needs, address social problems that affect students, such as teenage parenthood or homelessness, or accommodate students who are considered at risk of failing academically.		In Canada, local school boards choose whether or not they wish to have alternative schools and how they are operated. The alternative schools may include multi-age groupings, integrated curriculum or holistic learning, parental involvement, and descriptive reports rather than grades. Some school systems provide alternative education streams within the state schools.[11]		In Canada, schools for children who are having difficulty in a traditional secondary school setting are known as alternate schools.[12]		Germany has over 200 Waldorf schools, including the first such school in the world (founded 1919), and a large number of Montessori schools. Each of these has its own national association, whereas most other alternative schools are organized in the National Association of Independent Alternative Schools (). Funding for private schools in Germany differs from Bundesland to Bundesland.		Full public funding is given to laboratory schools researching school concepts for public education. The Laborschule Bielefeld had a great influence on many alternative schools, including the renewal of the democratic school concept.		In South Korea, alternative schools serve three big groups of youth in South Korea. The first group is students who could not succeed in formative Korean education. Many of these schools serve students who dropped out during their earlier school years, either voluntarily or by disciplinary action. The second group is young immigrants. As the population of immigrants from Southeast Asia and North Korea is increasing, several educators started to see the necessity of the adaptive education, specially designed for these young immigrants. Because South Korea has been a monoethnic society throughout its history, there is not enough system and awareness to protect these students from bullying, social isolation, or academic failure.For instance, the drop-out rate for North Korean immigrant students is ten times higher than that of students from South Korean students because their major challenge is initially to adapt to South Korean society, not to get a higher test score. The other group is students who choose an alternative education because of its philosophy. Korean education, as in many other Asian countries, is based on testing and memorizing. Some students and parents believe this kind of education cannot nurture a student thoroughly and choose to go to an alternative school, that suggests a different way to learn for students. These schools usually stress the importance of interaction between other people and nature over written test results.		The major struggle in alternative schools in South Korea are recognition, lack of financial support, and quality gap between alternative schools. Although South Korean public's recognition to alternative education has deliberately changed, the progressive education still is not widely accepted. To enter a college, regular education is often preferred because of the nation's rigid educational taste on test result and record. For the same reason, South Korean government is not actively supporting alternative schools financially.		Hence, many alternative schools are at risk of bankruptcy, especially the schools that do not or cannot collect tuition from their students. Most Southeast Asian and North Korean immigrant families are financially in need, so they need assist from government's welfare system for their everyday life. It is clear that affording private education is a mere fantasy for these families. That phenomenon, at last, causes a gap among alternative schools themselves. Some schools are richly supported by upper-class parents and provide variety of in-school and after-school programs, and others rarely have resource to build few academic and extracurricular programs as such.		India has a long history of alternative schools. Vedic and Gurukul systems of education during 1500 BC to 500 BC emphasized on acquisition of occupational skills, cultural and spiritual enlightenment in an atmosphere which encouraged rational thinking, reasoning among the students. Hence the aim of education was to develop the pupil in various aspects of life as well as ensure social service.[13] However, with the decline of the local economies and the advent of the colonial rulers this system went into decline. Some notable reforms like English as the medium of instruction, were introduced as recommended in Macaulay's Minute in the year 1835. The mainstream schools of today still follow the system developed in the colonial era. In the years since independence, Government has focused on expansion of school network, designing of curriculum according to educational needs,local language as the medium of instruction, etc. By the end of nineteenth century, many social reformers began to explore alternatives to contemporary education system. Vivekananda, Dayanand Saraswati, Jyotiba Phule, Savitribai Phule, Syed Ahmed Khan were the pioneers who took up the cause of social regeneration, removal of social inequalities, promotion of girl's education through alternate schools.[14] In the early twentieth century educationists create models of alternative schools as a response to the drawbacks to mainstream schools which are still viable. Rabindranath Tagore's Shanti Niketan, Jiddu Krishnamurthy's Rishi Valley School, Sri Aurobindo and Mother's Sri Aurobindo International Center for Education popularly known as Ashram Schools, and Walden's Path Magnet School are some of the examples. An upsurge in alternative schools was seen in 1970's onward. But most of the alternate schools are the result of individual efforts rather than government. The establishment of National Institute Open Schooling (NIOS)in 1989 by Ministry of Human Resource Development was one of the steps taken by the government which took all such schools under its wings. NIOS provide platform for the alternate school children to take government prescribed examination.		
– in Europe  (green & dark grey) – in the European Union  (green)		Ireland (/ˈaɪərlənd/ ( listen); Irish: Éire [ˈeːɾʲə] ( listen)), also described as the Republic of Ireland (Poblacht na hÉireann), is a sovereign state in north-western Europe occupying 26 of 32 counties of the island of Ireland. The capital and largest city is Dublin, which is located on the eastern part of the island, and whose metropolitan area is home to around a third of the country's 4.75 million inhabitants. The state shares its only land border with Northern Ireland, a part of the United Kingdom. It is otherwise surrounded by the Atlantic Ocean, with the Celtic Sea to the south, Saint George's Channel to the south-east, and the Irish Sea to the east. It is a unitary, parliamentary republic.[8] The legislature, the Oireachtas, consists of a lower house, Dáil Éireann, an upper house, Seanad Éireann, and an elected President (Uachtarán) who serves as the largely ceremonial head of state, but with some important powers and duties. The head of government is the Taoiseach (Prime Minister, literally 'Chief', a title not used in English), who is elected by the Dáil and appointed by the President; the Taoiseach in turn appoints other government ministers.		The state was created as the Irish Free State in 1922 as a result of the Anglo-Irish Treaty. It had the status of dominion until 1937 when a new constitution was adopted, in which the state was named "Ireland" and effectively became a republic, with an elected non-executive president as head of state. It was officially declared a republic in 1949, following the Republic of Ireland Act 1948. Ireland became a member of the United Nations in December 1955. It joined the European Economic Community (EEC), the predecessor of the European Union, in 1973. The state had no formal relations with Northern Ireland for most of the twentieth century, but during the 1980s and 1990s the British and Irish governments worked with the Northern Ireland parties towards a resolution to "the Troubles". Since the signing of the Good Friday Agreement in 1998, the Irish government and Northern Ireland Executive have co-operated on a number of policy areas under the North-South Ministerial Council created by the Agreement.		Ireland ranks among the top twenty-five wealthiest countries in the world in terms of GDP per capita,[9] and as the tenth most prosperous country in the world according to The Legatum Prosperity Index 2015.[10] After joining the EEC, Ireland enacted a series of liberal economic policies that resulted in rapid economic growth. The country achieved considerable prosperity between the years of 1995 and 2007, which became known as the Celtic Tiger period. This was halted by an unprecedented financial crisis that began in 2008, in conjunction with the concurrent global economic crash.[11][12] However, as the Irish economy was the fastest growing in the EU in 2015,[13] Ireland is again quickly ascending league tables comparing wealth and prosperity internationally. For example, in 2015, Ireland was ranked as the joint sixth (with Germany) most developed country in the world by the United Nations Human Development Index.[14] It also performs well in several national performance metrics, including freedom of the press, economic freedom and civil liberties. Ireland is a member of the European Union and is a founding member of the Council of Europe and the OECD. The Irish government has followed a policy of military neutrality through non-alignment since immediately prior to World War II and the country is consequently not a member of NATO,[15] although it is a member of Partnership for Peace.						The 1922 state, comprising 26 of the 32 counties of Ireland, was "styled and known as the Irish Free State".[16] The Constitution of Ireland, adopted in 1937, provides that "the name of the State is Éire, or, in the English language, Ireland". Section 2 of the Republic of Ireland Act 1948 states, "It is hereby declared that the description of the State shall be the Republic of Ireland." The 1948 Act does not name the state as "Republic of Ireland", because to have done so would have put it in conflict with the Constitution.[17]		The government of the United Kingdom used the name "Eire" (without the diacritic) and, from 1949, "Republic of Ireland", for the state;[18] it was not until the 1998 Good Friday Agreement that it used the name "Ireland".[19]		As well as "Ireland", "Éire" or "the Republic of Ireland", the state is also referred to as "the Republic", "Southern Ireland" or "the South".[20] In an Irish republican context it is often referred to as "the Free State" or "the 26 Counties".[21]		From the Act of Union on 1 January 1801, until 6 December 1922, the island of Ireland was part of the United Kingdom of Great Britain and Ireland. During the Great Famine, from 1845 to 1849, the island's population of over 8 million fell by 30%. One million Irish died of starvation and/or disease and another 1.5 million emigrated, mostly to the United States.[22] This set the pattern of emigration for the century to come, resulting in a constant population decline up to the 1960s.		From 1874, and particularly under Charles Stewart Parnell from 1880, the Irish Parliamentary Party gained prominence. This was firstly through widespread agrarian agitation via the Irish Land League, that won land reforms for tenants in the form of the Irish Land Acts, and secondly through its attempts to achieve Home Rule, via two unsuccessful bills which would have granted Ireland limited national autonomy. These led to "grass-roots" control of national affairs, under the Local Government Act 1898, that had been in the hands of landlord-dominated grand juries of the Protestant Ascendancy.		Home Rule seemed certain when the Parliament Act 1911 abolished the veto of the House of Lords, and John Redmond secured the Third Home Rule Act in 1914. However, the Unionist movement had been growing since 1886 among Irish Protestants after the introduction of the first home rule bill, fearing discrimination and loss of economic and social privileges if Irish Catholics achieved real political power. In the late nineteenth and early twentieth century unionism was particularly strong in parts of Ulster, where industrialisation was more common in contrast to the more agrarian rest of the island. It was feared that any tariff barriers would heavily affect that region. In addition, the Protestant population was more prominent in Ulster, with a majority in four counties.[23] Under the leadership of the Dublin-born Sir Edward Carson of the Irish Unionist Party and the Ulsterman Sir James Craig of the Ulster Unionist Party, unionists became strongly militant in order to oppose "the Coercion of Ulster". After the Home Rule Bill passed parliament in May 1914, to avoid rebellion with Ulster, the British Prime Minister H. H. Asquith introduced an Amending Bill reluctantly conceded to by the Irish Party leadership. This provided for the temporary exclusion of Ulster from the workings of the bill for a trial period of six years, with an as yet undecided new set of measures to be introduced for the area to be temporarily excluded.		Though it received the Royal Assent and was placed on the statute books in 1914, the implementation of the Third Home Rule Act was suspended until after the First World War which defused the threat of civil war in Ireland. With hope of ensuring the implementation of the Act at the end of the war through Ireland's engagement in the war, Redmond and his Irish National Volunteers supported Britain and its Allies. 175,000 men joined Irish regiments of the 10th (Irish) and 16th (Irish) divisions of the New British Army, while Unionists joined the 36th (Ulster) divisions.[24]		The remainder of the Irish Volunteers, who opposed any support of Britain, launched an armed insurrection against British rule in the 1916 Easter Rising, together with the Irish Citizen Army. This commenced on 24 April 1916 with the declaration of independence. After a week of heavy fighting, primarily in Dublin, the surviving rebels were forced to surrender their positions. The majority were imprisoned but fifteen of the prisoners (including most of the leaders) were executed as traitors to Britain. This included Patrick Pearse, the spokesman for the rising and who provided the signal to the volunteers to start the rising, as well as James Connolly, socialist and founder of the Industrial Workers of the World union and both the Irish and Scottish Labour movements. These events, together with the Conscription Crisis of 1918, had a profound effect on changing public opinion in Ireland.		In January 1919, after the December 1918 general election, 73 of Ireland's 106 Members of Parliament (MPs) elected were Sinn Féin members who refused to take their seats in the British House of Commons. Instead, they set up an Irish parliament called Dáil Éireann. This first Dáil in January 1919 issued a Declaration of Independence and proclaimed an Irish Republic. The Declaration was mainly a restatement of the 1916 Proclamation with the additional provision that Ireland was no longer a part of the United Kingdom. The new Irish Republic was recognised internationally only by the Russian Soviet Republic.[25] The Irish Republic's Aireacht (Ministry) sent a delegation under Ceann Comhairle (Head of Council, or Speaker, of the Daíl) Seán T. O'Kelly to the Paris Peace Conference of 1919, but it was not admitted.		After the War of Independence and truce called in July 1921, representatives of the British government and the Irish treaty delegates, led by Arthur Griffith, Robert Barton and Michael Collins, negotiated the Anglo-Irish Treaty in London from 11 October to 6 December 1921. The Irish delegates set up headquarters at Hans Place in Knightsbridge and it was here in private discussions that the decision was taken on 5 December to recommend the treaty to Dáil Éireann. The Second Dáil Éireann narrowly ratified the Treaty.		In accordance with the treaty, on 6 December 1922 the entire island of Ireland became a self-governing dominion called the Irish Free State (Saorstát Éireann). Under the Constitution of the Irish Free State, the Parliament of Northern Ireland had the option to leave the Irish Free State one month later and return to the United Kingdom. During the intervening period, the powers of the Parliament of the Irish Free State and Executive Council of the Irish Free State did not extend to Northern Ireland. Northern Ireland exercised its right under the treaty to leave the new dominion and rejoined the United Kingdom on 8 December 1922. It did so by making an address to the King requesting, "that the powers of the Parliament and Government of the Irish Free State shall no longer extend to Northern Ireland."[26] The Irish Free State was a constitutional monarchy sharing a monarch with the United Kingdom and other dominions of the British Commonwealth. The country had a governor-general (representing the monarch), a bicameral parliament, a cabinet called the "Executive Council", and a prime minister called the President of the Executive Council.		The Irish Civil War was the consequence of the creation of the Irish Free State. Anti-treaty forces, led by Éamon de Valera, objected to the fact that acceptance of the treaty abolished the Irish Republic of 1919 to which they had sworn loyalty, arguing in the face of public support for the settlement that the "people have no right to do wrong".[27] They objected most to the fact that the state would remain part of the British Empire and that members of the Free State Parliament would have to swear what the Anti-treaty side saw as an oath of fidelity to the British King. Pro-treaty forces, led by Michael Collins, argued that the treaty gave "not the ultimate freedom that all nations aspire to and develop, but the freedom to achieve it".[28]		At the start of the war, the Irish Republican Army (IRA) split into two opposing camps: a pro-treaty IRA and an anti-treaty IRA. The pro-treaty IRA disbanded and joined the new National Army. However, because the anti-treaty IRA lacked an effective command structure and because of the pro-treaty forces' defensive tactics throughout the war, Michael Collins and his pro-treaty forces were able to build up an army with many tens of thousands of World War I veterans from the 1922 disbanded Irish regiments of the British Army, capable of overwhelming the anti-treatyists. British supplies of artillery, aircraft, machine-guns and ammunition boosted pro-treaty forces, and the threat of a return of Crown forces to the Free State removed any doubts about the necessity of enforcing the treaty. The lack of public support for the anti-treaty forces (often called the Irregulars) and the determination of the government to overcome the Irregulars contributed significantly to their defeat.		Following a national referendum, on 29 December 1937 the new Constitution of Ireland (Bunreacht na hÉireann) came into force. This replaced the Constitution of the Irish Free State and called the state Ireland, or Éire in Irish.[29] Articles 2 and 3 of the constitution asserted a nominal territorial claim over the whole island, considering the partition of Ireland under the 1922 Anglo-Irish Treaty illegitimate. The former Irish Free State government had taken steps to abolish the Office of Governor-General some months before the new Constitution came into force.[30] Although the constitution established the office of President of Ireland, the question over whether Ireland was a republic remained open. Diplomats were accredited to the king, but the president exercised the internal functions of a head of state.[31] For instance, the President gave assent to new laws with his own authority, without reference to King George VI who was only an "organ", that was provided for by statute law.		Ireland remained neutral during World War II, a period it described as the Emergency. Ireland's link with the Commonwealth was terminated with the passage of the Republic of Ireland Act 1948, which came into force on 18 April 1949 and declared that the state was a republic. At the time, a declaration of a republic terminated Commonwealth membership. This rule was changed 10 days after Ireland declared itself a republic, with the London Declaration of 28 April 1949. Ireland did not reapply when the rules were altered to permit republics to join. Later, the Crown of Ireland Act was repealed in Ireland by the Statute Law Revision (Pre-Union Irish Statutes) Act, 1962.		Ireland became a member of the United Nations in December 1955, after having been denied membership because of its neutral stance during the Second World War and not supporting the Allied cause.[32] At the time, joining the UN involved a commitment to using force to deter aggression by one state against another if the UN thought it was necessary.[33]		Interest towards membership of the European Economic Community (EEC) developed in Ireland during the 1950s, with consideration also given to membership of the European Free Trade Area. As the United Kingdom intended on EEC membership, Ireland applied for membership in July 1961 due to the substantial economic linkages with the United Kingdom. However, the founding EEC members remained skeptical regarding Ireland's economic capacity, neutrality, and unattractive protectionist policy.[34] Many Irish economists and politicians realised that economic policy reform was necessary. The prospect of EEC membership became doubtful in 1963 when French President General Charles de Gaulle stated that France opposed Britain's accession, which ceased negotiations with all other candidate countries. However, in 1969 his successor, Georges Pompidou, was not opposed to British and Irish membership. Negotiations began and in 1972 the Treaty of Accession was signed. A referendum held in 1972 confirmed Ireland's entry, and it finally joined the EEC in 1973.[35]		The economic crisis of the late 1970s was fuelled by the Fianna Fáil government's budget, the abolition of the car tax, excessive borrowing, and global economic instability including the 1979 oil crisis.[36] There were significant policy changes from 1989 onwards, with economic reform, tax cuts, welfare reform, an increase in competition, and a ban on borrowing to fund current spending. This policy began in 1989–1992 by the Fianna Fáil/Progressive Democrat government, and continued by the subsequent Fianna Fáil/Labour government and Fine Gael/Labour/Democratic Left government. Ireland became one of the world's fastest growing economies by the late 1990s in what was known as the Celtic Tiger period, which lasted until the global Financial crisis of 2007–08. However, since 2014, Ireland has experienced strong economic growth.		In the Northern Ireland question, the British and Irish governments started to seek a peaceful resolution to the violent conflict involving many paramilitaries and the British Army in Northern Ireland known as "The Troubles". A peace settlement for Northern Ireland, known as the Good Friday Agreement, was approved in 1998 in referendums north and south of the border. As part of the peace settlement, the territorial claim to Northern Ireland in Articles 2 and 3 of the Constitution of Ireland was removed by referendum. In its white paper on Brexit the United Kingdom government reiterated its commitment to the Good Friday Agreement. With regard to Northern Ireland's status, it said that the UK Government's "clearly-stated preference is to retain Northern Ireland’s current constitutional position: as part of the UK, but with strong links to Ireland".[37]		The state extends over an area of about five-sixths (70,273 km2 or 27,133 sq mi) of the island of Ireland (84,421 km2 or 32,595 sq mi), with Northern Ireland constituting the remainder. The island is bounded to the north and west by the Atlantic Ocean and to the northeast by the North Channel. To the east, the Irish Sea connects to the Atlantic Ocean via St George's Channel and the Celtic Sea to the southwest.		The western landscape mostly consists of rugged cliffs, hills and mountains. The central lowlands are extensively covered with glacial deposits of clay and sand, as well as significant areas of bogland and several lakes. The highest point is Carrauntoohil (1,038 m or 3,406 ft), located in the Macgillycuddy's Reeks mountain range in the southwest. River Shannon, which traverses the central lowlands, is the longest river in Ireland at 386 kilometres or 240 miles in length. The west coast is more rugged than the east, with numerous islands, peninsulas, headlands and bays.		Before the arrival of the first settlers in Ireland about 9,000 years ago, the land was largely covered by forests of oak, ash, elm, hazel, yew, and other native trees.[38] The growth of blanket bog and the extensive clearing of woodland to facilitate farming are believed to be the main causes of deforestation during the following centuries. Today, about 12% of Ireland is forested, of which a significant majority is composed of mainly non-native coniferous plantations for commercial use.[39] Ideal soil conditions, high rainfall and a mild climate give Ireland the highest growth rates for forests in Europe. Hedgerows, which are traditionally used to define land boundaries, are an important substitute for woodland habitat, providing refuge for native wild flora and a wide range of insect, bird and mammal species.[40]		Agriculture accounts for about 64% of the total land area.[41] This has resulted in limited land to preserve natural habitats, in particular for larger wild mammals with greater territorial requirements.[42] The long history of agricultural production coupled with modern agricultural methods, such as pesticide and fertiliser use, has placed pressure on biodiversity.[43]		The Atlantic Ocean and the warming influence of the Gulf Stream affect weather patterns in Ireland.[44] Temperatures differ regionally, with central and eastern areas tending to be more extreme. However, due to a temperate oceanic climate, temperatures are seldom lower than −5 °C (23 °F) in winter or higher than 26 °C (79 °F) in summer.[45] The highest temperature recorded in Ireland was 33.3 °C (91.9 °F) on 26 June 1887 at Kilkenny Castle in Kilkenny, while the lowest temperature recorded was −19.1 °C (−2.4 °F) at Markree Castle in Sligo.[46] Rainfall is more prevalent during winter months and less so during the early months of summer. Southwestern areas experience the most rainfall as a result of south westerly winds, while Dublin receives the least. Sunshine duration is highest in the southeast of the country.[44] The far north and west are two of the windiest regions in Europe, with great potential for wind energy generation.[47] Ireland normally gets between 1100 and 1600 hours of sunshine each year, most areas averaging between 3.25 and 3.75 hours a day. The sunniest months are May and June, which average between 5 and 6.5 hours per day over most of the country. The extreme southeast gets most sunshine, averaging over 7 hours a day in early summer. December is the dullest month, with an average daily sunshine ranging from about 1 hour in the north to almost 2 hours in the extreme southeast. The sunniest summer in the 100 years from 1881 to 1980 was 1887, according to measurements made at the Phoenix Park in Dublin; 1980 was the dullest.[48]		Ireland is a constitutional republic with a parliamentary system of government. The Oireachtas is the bicameral national parliament composed of the President of Ireland and the two Houses of the Oireachtas: Seanad Éireann (Senate) and Dáil Éireann (House of Representatives).[49] Áras an Uachtaráin is the official residence of the President of Ireland, while the houses of the Oireachtas meet at Leinster House in Dublin.		The President serves as head of state, and is elected for a seven-year term and may be re-elected once. The President is primarily a figurehead, but is entrusted with certain constitutional powers with the advice of the Council of State. The office has absolute discretion in some areas, such as referring a bill to the Supreme Court for a judgment on its constitutionality.[50] Michael D. Higgins became the ninth President of Ireland on 11 November 2011.[51]		The Taoiseach (Prime Minister) serves as the head of government and is appointed by the President upon the nomination of the Dáil. Most Taoisigh have served as the leader of the political party that gains the most seats in national elections. It has become customary for coalitions to form a government, as there has not been a single-party government since 1989.[52] Leo Varadkar succeeded Enda Kenny as Taoiseach on 14 June 2017.		The Seanad is composed of sixty members, with eleven nominated by the Taoiseach, six elected by two universities, and 43 elected by public representatives from panels of candidates established on a vocational basis. The Dáil has 158 members (Teachtaí Dála) elected to represent multi-seat constituencies under the system of proportional representation and by means of the single transferable vote.		The Government is constitutionally limited to fifteen members. No more than two members can be selected from the Seanad, and the Taoiseach, Tánaiste (Deputy Prime Minister) and Minister for Finance must be members of the Dáil. The Dáil must be dissolved within five years after its first meeting following the previous election,[53] and a general election for members of the Dáil must take place no later than thirty days after the dissolution. According to the Constitution of Ireland, parliamentary elections must be held at least every seven years, though a lower limit may be set by statute law. The current government is a Fine Gael led minority government led by Leo Varadkar as Taoiseach and Frances Fitzgerald as Tánaiste. It is supported by a number of Independents including Shane Ross and former Senator Katherine Zappone. The minority government is held in place by a confidence and supply deal with Fianna Fáil. Opposition parties in the current Dáil are Fianna Fáil, Sinn Féin, the Socialist Party, the PBPA and Anti-Austerity Alliance, the Labour Party, Social Democrats, the WUAG, the Green Party as well as a number of Independents.		Ireland has been a member state of the European Union since 1973, but has chosen to remain outside the Schengen Area. Citizens of the United Kingdom can freely enter the country without a passport due to the Common Travel Area, which is a passport-free zone comprising the islands of Ireland, Great Britain, the Isle of Man and the Channel Islands. However, some identification is required at airports and seaports.		The Local Government Act 1898[54] is the founding document of the present system of local government, while the Twentieth Amendment to the constitution of 1999 provided for its constitutional recognition. The twenty-six traditional counties of Ireland are not always coterminous with administrative divisions although they are generally used as a geographical frame of reference by the population of Ireland. The Local Government Reform Act 2014 provides for a system of thirty-one local authorities - twenty-six county councils, two city and county councils and three city councils.[54] Below this (with the exception of the Dublin Region and the three city councils) are municipal districts, replacing a previous system of town councils.		Local authorities are responsible for matters such as planning, local roads, sanitation, and libraries. Dáil constituencies are required to follow county boundaries as much as possible. Counties with greater populations have multiple constituencies, some of more than one county, but generally do not cross county boundaries. The counties are grouped into eight regions, each with a Regional Authority composed of members delegated by the various county and city councils in the region. The regions do not have any direct administrative role as such, but they serve for planning, coordination and statistical purposes.		Ireland has a common law legal system with a written constitution that provides for a parliamentary democracy. The court system consists of the Supreme Court, the Court of Appeal, the High Court, the Circuit Court and the District Court, all of which apply the law of Ireland. Trials for serious offences must usually be held before a jury. The High Court and the Supreme Court have authority, by means of judicial review, to determine the compatibility of laws and activities of other institutions of the state with the constitution and the law. Except in exceptional circumstances, court hearings must occur in public. The Criminal Courts of Justice is the principal building for the criminal courts.[55][56] It includes the Dublin Metropolitan District Court, Court of Criminal Appeal, Dublin Circuit Criminal Court and Central Criminal Court.[55]		Garda Síochána na hÉireann (Guardians of the Peace of Ireland), more commonly referred to as the Gardaí, is the state's civilian police force. The force is responsible for all aspects of civil policing, both in terms of territory and infrastructure. It is headed by the Garda Commissioner, who is appointed by the Government. Most uniformed members do not routinely carry firearms. Standard policing is traditionally carried out by uniformed officers equipped only with a baton and pepper spray.[57]		The Military Police is the corps of the Irish Army responsible for the provision of policing service personnel and providing a military police presence to forces while on exercise and deployment. In wartime, additional tasks include the provision of a traffic control organisation to allow rapid movement of military formations to their mission areas. Other wartime roles include control of prisoners of war and refugees.[58]		Ireland's citizenship laws relate to "the island of Ireland", including islands and seas, thereby extending them to Northern Ireland, which is part of the United Kingdom. Therefore, anyone born in Northern Ireland who meets the requirements for being an Irish citizen, such as birth on the island of Ireland to an Irish or British citizen parent or a parent who is entitled to live in Northern Ireland or the Republic without restriction on their residency,[59] may exercise an entitlement to Irish citizenship, such as an Irish passport.[60]		Foreign relations are substantially influenced by membership of the European Union, although bilateral relations with the United Kingdom and United States are also important.[61] It held the Presidency of the Council of the European Union on six occasions, most recently from January to June 2013.[62]		Ireland tends towards independence in foreign policy, thus the country is not a member of NATO and has a longstanding policy of military neutrality. This policy has helped the Irish Defence Forces to be successful in their contributions to peace-keeping missions with the United Nations since 1960, during the Congo Crisis and subsequently in Cyprus, Lebanon and Bosnia and Herzegovina.[63][disputed – discuss]		Despite Irish neutrality during World War II, Ireland had more than 50,000 participants in the war through enlistment in the British armed forces. During the Cold War, Irish military policy, while ostensibly neutral, was biased towards NATO.[64] During the Cuban Missile Crisis, Seán Lemass authorised the search of Cuban and Czechoslovak aircraft passing through Shannon and passed the information to the CIA.[65] Ireland's air facilities were used by the United States military for the delivery of military personnel involved in the 2003 invasion of Iraq through Shannon Airport. The airport had previously been used for the invasion of Afghanistan in 2001, as well as the First Gulf War.[66]		Since 1999, Ireland has been a member of NATO's Partnership for Peace (PfP) program and NATO's Euro-Atlantic Partnership Council (EAPC), which is aimed at creating trust between NATO and other states in Europe and the former Soviet Union.[67][68]		The Defence Forces are made up of the Army, Naval Service, Air Corps and Reserve Defence Force. It is small but well equipped, with almost 10,000 full-time military personnel and over 2,000 in reserve.[69][70] Ireland is a neutral country,[71] and has "triple-lock" rules governing the participation of Irish troops in conflict zones, whereby approval must be given by the UN, the Dáil and Government.[72] Daily deployments of the Defence Forces cover aid to civil power operations, protection and patrol of Irish territorial waters and EEZ by the Irish Naval Service, and UN, EU and PfP peace-keeping missions. By 1996, over 40,000 Irish service personnel had served in international UN peacekeeping missions.[73]		The Irish Air Corps is the air component of the Defence Forces and operates sixteen fixed wing aircraft and eight helicopters. The Irish Naval Service is Ireland's navy, and operates eight patrol ships, and smaller numbers of inflatable boats and training vessels, and has armed boarding parties capable of seizing a ship and a special unit of frogmen. The military includes the Reserve Defence Forces (Army Reserve and Naval Service Reserve) for part-time reservists. Ireland's special forces include the Army Ranger Wing, which trains and operates with international special operations units. The President is the formal Supreme Commander of the Defence Forces, but in practice answers to the Government via the Minister for Defence.		The Irish economy has transformed since the 1980s from being predominantly agricultural to a modern knowledge economy focused on high technology industries and services. Ireland adopted the euro currency in 2002 along with eleven other EU member states.[43] The country is heavily reliant on Foreign Direct Investment and has attracted several multinational corporations due to a highly educated workforce and a low corporation tax rate.[74]		Companies such as Intel invested in Ireland during the late 1980s, later followed by Microsoft and Google. Ireland is ranked as the ninth most economically free economy in the world, according to the Index of Economic Freedom. In terms of GDP per capita, Ireland is one of the wealthiest countries in the OECD and EU. However, the country ranks below the OECD average in terms of GNP per capita. GDP is significantly greater than GNP due to the large number of multinational corporations based in Ireland.[74]		Beginning in the early 1990s, the country experienced unprecedented economic growth fuelled by a dramatic rise in consumer spending, construction and investment, which became known as the Celtic Tiger period. The pace of growth slowed during 2007 and led to the burst of a major property bubble which had developed over time.[75] The dramatic fall in property prices highlighted the over-exposure of the economy to construction and contributed to the Irish banking crisis. Ireland officially entered a recession in 2008 following consecutive months of economic contraction.[76] GNP contracted by 11.3% in 2009 alone, the largest annual decline in GNP since 1950.[77]		The country officially exited recession in 2010, assisted by a strong growth in exports.[78] However, due to a significant rise in the cost of public borrowing due to government guarantees of private banking debt, the Irish government accepted an €85 billion programme of assistance from the EU, International Monetary Fund (IMF) and bilateral loans from the United Kingdom, Sweden and Denmark.[79] Following three years of contraction, the economy grew by 0.7% in 2011 and 0.9% in 2012.[80] The unemployment rate was 14.7% in 2012, including 18.5% among recent immigrants.[81] In March 2016 the unemployment rate was reported by the Central Statistics Office (Ireland) to be 8.6%, down from a peak unemployment rate of 15.1% in February 2012.[82] In addition to unemployment, net emigration from Ireland between 2008 and 2013 totalled 120,100,[83] or some 2.6% of the total population according to the Census of Ireland 2011. One-third of the emigrants were aged between 15 and 24.[83]		In 2013, Ireland was named the "best country for business" by Forbes.[84] Ireland exited its EU-IMF bailout programme on 15 December 2013.[85] Having implemented budget cuts, reforms and sold assets, Ireland was again able to access debt markets. Since then, Ireland has been able to sell long term bonds at record rates.[86]		Although multinational corporations dominate Ireland's export sector, exports from other sources also contribute significantly to the national income. The activities of multinational companies based in Ireland have made it one of the largest exporters of pharmaceutical agents, medical devices and software-related goods and services in the world. Ireland's exports also relate to the activities of large Irish companies (such as Ryanair, Kerry Group and Smurfit Kappa Group) and exports of mineral resources: Ireland is the seventh largest producer of zinc concentrates, and the twelfth largest producer of lead concentrates. The country also has significant deposits of gypsum, limestone, and smaller quantities of copper, silver, gold, barite, and dolomite.[43] Tourism in Ireland contributes about 4% of GDP and is a significant source of employment.		Other goods exports include agri-food, cattle, beef, dairy products, and aluminum. Ireland's major imports include data processing equipment, chemicals, petroleum and petroleum products, textiles, and clothing. Financial services provided by multinational corporations based at the Irish Financial Services Centre also contribute to Irish exports. The difference between exports (€89.4 billion) and imports (€45.5 billion) resulted an annual trade surplus of €43.9 billion in 2010, which is the highest trade surplus relative to GDP achieved by any EU member state.[87]		The EU is by far the country's largest trading partner, accounting for 57.9% of exports and 60.7% of imports. The United Kingdom is the most important trading partner within the EU, accounting for 15.4% of exports and 32.1% of imports. Outside the EU, the United States accounted for 23.2% of exports and 14.1% of imports in 2010.[87]		ESB, Ervia and Airtricity are the three main electricity and gas suppliers in Ireland. There are 19.82 billion cubic metres of proven reserves of gas.[43][88] Natural gas extraction previously occurred at the Kinsale Head until its exhaustion. The Corrib gas field was due to come on stream in 2013/14. In 2012, the Barryroe field was confirmed to have up to 1.6 billion barrels of oil in reserve, with between 160 and 600 million recoverable.[89] That could provide for Ireland's entire energy needs for up to 13 years, when it is developed in 2015/16. There have been significant efforts to increase the use of renewable and sustainable forms of energy in Ireland, particularly in wind power, with 3,000 MegaWatts[90] of wind farms being constructed, some for the purpose of export.[91] The Sustainable Energy Authority of Ireland (SEAI) has estimated that 6.5% of Ireland's 2011 energy requirements were produced by renewable sources.[92] The SEAI has also reported an increase in energy efficiency in Ireland with a 28% reduction in carbon emissions per house from 2005 to 2013.[93]		The country's three main international airports at Dublin, Shannon and Cork serve many European and intercontinental routes with scheduled and chartered flights. The London and Dublin route is the second busiest international air route in Europe, with 3.6 million people flying between the two cities in 2013[94] down from the 4.4 million who flew in 2003.[95][96] Aer Lingus is the flag carrier of Ireland, although Ryanair is the country's largest airline. Ryanair is Europe's largest low-cost carrier,[97] the second largest in terms of passenger numbers, and the world's largest in terms of international passenger numbers.[98]		Railway services are provided by Iarnród Éireann (Irish Rail), which operates all internal intercity, commuter and freight railway services in the country. Dublin is the centre of the network with two main stations, Heuston station and Connolly station, linking to the country's cities and main towns. The Enterprise service, which runs jointly with Northern Ireland Railways, connects Dublin and Belfast. The whole of Ireland's mainline network operates on track with a gauge of 5 ft 3 in (1,600 mm), which is unique in Europe and has resulted in distinct rolling stock designs. Dublin has a steadily improving public transport network including the DART, Luas, Dublin Bus, and dublinbikes.		Motorways, national primary roads and national secondary roads are managed by the National Roads Authority, while regional roads and local roads are managed by the local authorities in each of their respective areas. The road network is primarily focused on the capital, but motorways have been extended to other cities as part of the Transport 21 capital investment programme, as a result motorways have been completed between Dublin and a number of other major Irish cities including Cork, Limerick, Waterford and Galway.[99]		Dublin has been the focus of major projects such as the East-Link and West-Link toll-bridges, as well as the Dublin Port Tunnel. The Jack Lynch Tunnel, under the River Lee in Cork, and the Limerick Tunnel, under the River Shannon, were two major projects outside Dublin. Several by-pass projects are underway in other urban areas.		Genetic research suggests that the earliest settlers migrated from Iberia following the most recent ice age.[100] After the Mesolithic, Neolithic and Bronze Age, migrants introduced a Celtic language and culture. Migrants from the two latter eras still represent the genetic heritage of most Irish people.[101][102] Gaelic tradition expanded and became the dominant form over time. Irish people are a combination of Gaelic, Norse, Anglo-Norman, French, and British ancestry.		The population of Ireland stood at 4,588,252 in 2011, an increase of 8.2% since 2006.[103] As of 2011[update], Ireland had the highest birth rate in the European Union (16 births per 1,000 of population).[104] In 2014, 36.3% of births were to unmarried women.[105] Annual population growth rates exceeded 2% during the 2002-2006 intercensal period, which was attributed to high rates of natural increase and immigration.[106] This rate declined somewhat during the subsequent 2006-2011 intercensal period, with an average annual percentage change of 1.6%.		At the time of the 2011 census, the number of non-Irish nationals was recorded at 544,357, comprising 12% of the total population. This is nearly 2.5 times the number of non-Irish nationals recorded in the 2002 census (224,261), when the question of nationality was asked for the first time. The five largest non-national cohorts were Polish (122,585), UK (112,259), Lithuanian (36,683), Latvian (20,593) and Nigerian (17,642) respectively.[107]		Dublin Limerick		Cork Galway		The following is a list of functional urban areas in Ireland and their population as of 2011.[118]		The Irish Constitution describes Irish as the "national language", but English is the dominant language. In the 2006 census, 39% of the population regarded themselves as competent in Irish. Irish is spoken as a community language only in a small number of rural areas mostly in the west and south of the country, collectively known as the Gaeltacht. Except in Gaeltacht regions, road signs are usually bilingual.[119] Most public notices and print media are in English only. While the state is officially bilingual, citizens can often struggle to access state services in Irish and most government publications are not available in both languages, even though citizens have the right to deal with the state in Irish. Irish language media include the TV channel TG4, the radio station RTÉ Raidió na Gaeltachta and online newspaper Tuairisc.ie. In the Irish Defence Forces, all foot and arms drill commands are given in the Irish language.		As a result of immigration, Polish is the most widely spoken language in Ireland after English, with Irish as the third most spoken.[120] Several other Central European languages (namely Czech, Hungarian and Slovak), as well as Baltic languages (Lithuanian and Latvian) are also spoken on a day-to-day basis. Other languages spoken in Ireland include Shelta, spoken by Irish Travellers, and a dialect of Scots is spoken by some Ulster Scots people in Donegal.[121] Most secondary school students choose to learn one or two foreign languages. Languages available for the Junior Certificate and the Leaving Certificate include French, German, Italian and Spanish; Leaving Certificate students can also study Arabic, Japanese and Russian. Some secondary schools also offer Ancient Greek, Hebrew and Latin. The study of Irish is compulsory for Leaving Certificate students, but some may qualify for an exemption in some circumstances, such as learning difficulties or entering the country after age 11.[122]		Although the Irish healthcare system comes under constant criticism from politicians and the public, Ireland has one of the most developed systems of healthcare in the world and healthcare professionals who are highly trained. Healthcare in Ireland is provided by both public and private healthcare providers.[123]		The Minister for Health has responsibility for setting overall health service policy. Every resident of Ireland is entitled to receive health care through the public health care system, which is managed by the Health Service Executive and funded by general taxation. A person may be required to pay a subsidised fee for certain health care received; this depends on income, age, illness or disability. All maternity services are provided free of charge and children up to the age of 6 months. Emergency care is provided to patients who present to a hospital emergency department. However, visitors to emergency departments in non-emergency situations who are not referred by their GP may incur a fee of €100. In some circumstances this fee is not payable or may be waived.[124]		Anyone holding a European Health Insurance Card is entitled to free maintenance and treatment in public beds in Health Service Executive and voluntary hospitals. Outpatient services are also provided for free. However, the majority of patients on median incomes or above are required to pay subsidised hospital charges. Private health insurance is available to the population for those who want to avail of it.		The average life expectancy in Ireland in 2012 is 81 years (OECD average life expectancy in 2012 was 80 years), with 78.2 years for men and 83.6 years for women.[125] It has the highest birth rate in the EU (16.8 births per 1,000 inhabitants, compared to an EU average of 10.7)[126] and a very low infant mortality rate (3.5 per 1,000 live births). The Irish healthcare system ranked 13th out of 34 European countries in 2012 according to the European Health Consumer Index produced by Health Consumer Powerhouse.[127] The same report ranked The Irish healthcare system as having the 8th best health outcomes but only the 21st most accessible system in Europe.		Ireland has three levels of education: primary, secondary and higher education. The education systems are largely under the direction of the Government via the Minister for Education and Skills. Recognised primary and secondary schools must adhere to the curriculum established by the relevant authorities. Education is compulsory between the ages of six and fifteen years, and all children up to the age of eighteen must complete the first three years of secondary, including one sitting of the Junior Certificate examination.[128]		There are approximately 3,300 primary schools in Ireland.[129] The vast majority (92%) are under the patronage of the Catholic Church. Schools run by religious organisations, but receiving public money and recognition, cannot discriminate against pupils based upon religion or lack thereof. A sanctioned system of preference does exist, where students of a particular religion may be accepted before those who do not share the ethos of the school, in a case where a school's quota has already been reached.		The Leaving Certificate, which is taken after two years of study, is the final examination in the secondary school system. Those intending to pursue higher education normally take this examination, with access to third-level courses generally depending on results obtained from the best six subjects taken, on a competitive basis.[130] Third-level education awards are conferred by at least 38 Higher Education Institutions - this includes the constituent or linked colleges of seven universities, plus other designated institutions of the Higher Education and Training Awards Council.		The Programme for International Student Assessment, coordinated by the OECD, currently ranks Ireland as having the fourth highest reading score, ninth highest science score and thirteenth highest mathematics score, among OECD countries, in its 2012 assessment.[131] In 2012, Irish students aged 15 years had the second highest levels of reading literacy in the EU.[132] Ireland also has 0.747 of the World's top 500 Universities per capita, which ranks the country in 8th place in the world.[133] Primary, secondary and higher (university/college) level education are all free in Ireland for all EU citizens.[134] There are charges to cover student services and examinations.		In addition, 37 percent of Ireland population has a university or college degree, which is among the highest percentages in the world.[135][136]		Religious freedom is constitutionally provided for in Ireland. Christianity is the predominant religion, and while Ireland remains a predominantly Catholic country, the percentage of the population who identified as Catholic on the census has fallen sharply from 84.2 percent in the 2011 census to 78.3 percent in the most recent 2016 census. Other results from the 2016 census are : 4.2% Protestant, 1.3% as Muslim, and 9.8% as having no religion.[3] According to a Georgetown University study, before 2000 the country had one of the highest rates of regular Mass attendance in the Western world.[137] While daily attendance was 13% in 2006, there was a reduction in weekly attendance from 81% in 1990 to 48% in 2006, although the decline was reported as stabilising.[138] In 2011, it was reported that weekly Mass attendance in Dublin was just 18%, with it being even lower among younger generations.[139]		The Church of Ireland is the second largest Christian denomination. Membership declined throughout the twentieth century, but experienced an increase early in the 21st century, as have other small Christian denominations. Significant Protestant denominations are the Presbyterian Church and Methodist Church. Immigration has contributed to a growth in Hindu and Muslim populations. In percentage terms, Orthodox Christianity and Islam were the fastest growing religions, with increases of 100% and 70% respectively.[140]		Ireland's patron saints are Saint Patrick, Saint Bridget and Saint Columba. Saint Patrick is the only one commonly recognised as the patron saint. Saint Patrick's Day is celebrated on 17 March in Ireland and abroad as the Irish national day, with parades and other celebrations.		As with other predominantly Catholic European states, Ireland underwent a period of legal secularisation in the late twentieth century. In 1972, the article of the Constitution naming specific religious groups was deleted by the Fifth Amendment in a referendum. Article 44 remains in the Constitution: "The State acknowledges that the homage of public worship is due to Almighty God. It shall hold His Name in reverence, and shall respect and honour religion." The article also establishes freedom of religion, prohibits endowment of any religion, prohibits the state from religious discrimination, and requires the state to treat religious and non-religious schools in a non-prejudicial manner.		Religious studies was introduced as an optional Junior Certificate subject in 2001. Although many schools are run by religious organisations, a secularist trend is occurring among younger generations.[141]		Ireland's culture was for centuries predominantly Gaelic, and it remains one of the six principal Celtic nations. Following the Anglo-Norman invasion in the 12th century, and gradual British conquest and colonisation beginning in the 16th century, Ireland became influenced by English and Scottish culture. Subsequently, Irish culture, though distinct in many aspects, shares characteristics with the Anglosphere, Catholic Europe, and other Celtic regions. The Irish diaspora, one of the world's largest and most dispersed, has contributed to the globalisation of Irish culture, producing many prominent figures in art, music, and science.		Ireland has made a significant contribution to world literature in both the English and Irish languages. Modern Irish fiction began with the publishing of the 1726 novel Gulliver's Travels by Jonathan Swift. Other writers of importance during the 18th century and their most notable works include Laurence Sterne with the publication of The Life and Opinions of Tristram Shandy, Gentleman and Oliver Goldsmith's The Vicar of Wakefield. Numerous Irish novelists emerged during the 19th century, including Maria Edgeworth, John Banim, Gerald Griffin, Charles Kickham, William Carleton, George Moore, and Somerville and Ross. Bram Stoker is best known as the author of the 1897 novel Dracula.		James Joyce (1882–1941) published his most famous work Ulysses in 1922, which is an interpretation of the Odyssey set in Dublin. Edith Somerville continued writing after the death of her partner Martin Ross in 1915. Dublin's Annie M. P. Smithson was one of several authors catering for fans of romantic fiction in the 1920s and 1930s. After the Second World War, popular novels were published by, among others, Brian O'Nolan, who published as Flann O'Brien, Elizabeth Bowen, and Kate O'Brien. During the final decades of the 20th century, Edna O'Brien, John McGahern, Maeve Binchy, Joseph O'Connor, Roddy Doyle, Colm Tóibín, and John Banville came to the fore as novelists.		Patricia Lynch (1898–1972) was a prolific children's author, while Eoin Colfer has been particularly successful in this genre in recent years. In the genre of the short story, which is a form favoured by many Irish writers, the most prominent figures include Seán Ó Faoláin, Frank O'Connor and William Trevor. Well known Irish poets include Patrick Kavanagh, Thomas McCarthy, Dermot Bolger, and Nobel Prize in Literature laureates William Butler Yeats and Seamus Heaney (born in Northern Ireland but resided in Dublin). Prominent writers in the Irish language are Pádraic Ó Conaire, Máirtín Ó Cadhain, Séamus Ó Grianna, and Nuala Ní Dhomhnaill.		The history of Irish theatre begins with the expansion of the English administration in Dublin during the early 17th century, and since then, Ireland has significantly contributed to English drama. In its early history, theatrical productions in Ireland tended to serve political purposes, but as more theatres opened and the popular audience grew, a more diverse range of entertainments were staged. Many Dublin-based theatres developed links with their London equivalents, and British productions frequently found their way to the Irish stage. However, most Irish playwrights went abroad to establish themselves. In the 18th century, Oliver Goldsmith and Richard Brinsley Sheridan were two of the most successful playwrights on the London stage at that time. At the beginning of the 20th century, theatre companies dedicated to the staging of Irish plays and the development of writers, directors and performers began to emerge, which allowed many Irish playwrights to learn their trade and establish their reputations in Ireland rather than in Britain or the United States. Following in the tradition of acclaimed practitioners, principally Oscar Wilde, Literature Nobel Prize laureates George Bernard Shaw (1925) and Samuel Beckett (1969), playwrights such as Seán O'Casey, Brian Friel, Sebastian Barry, Brendan Behan, Conor McPherson and Billy Roche have gained popular success.[142] Other Irish playwrights of the 20th century include Denis Johnston, Thomas Kilroy, Tom Murphy, Hugh Leonard, Frank McGuinness, and John B. Keane.		Irish traditional music has remained vibrant, despite globalising cultural forces, and retains many traditional aspects. It has influenced various music genres, such as American country and roots music, and to some extent modern rock. It has occasionally been blended with styles such as rock and roll and punk rock. Ireland has also produced many internationally known artists in other genres, such as rock, pop, jazz, and blues.		There are a number of classical music ensembles around the country, such as the RTÉ Performing Groups.[143] Ireland also has three opera organisations. Opera Ireland produces large-scale operas in Dublin, the Opera Theatre Company tours its chamber-style operas throughout the country, and the annual Wexford Opera Festival, which promotes lesser-known operas, takes place during October and November.		Ireland has participated in the Eurovision Song Contest since 1965.[144] Its first win was in 1970, when Dana won with All Kinds of Everything.[145] It has subsequently won the competition six more times,[146][147] the highest number of wins by any competing country. The phenomenon Riverdance originated as an interval performance during the 1994 contest.[148]		Irish dance can broadly be divided into social dance and performance dance. Irish social dance can be divided into céilí and set dancing. Irish set dances are quadrilles, danced by 4 couples arranged in a square, while céilí dances are danced by varied formations of couples of 2 to 16 people. There are also many stylistic differences between these two forms. Irish social dance is a living tradition, and variations in particular dances are found across the country. In some places dances are deliberately modified and new dances are choreographed. Performance dance is traditionally referred to as stepdance. Irish stepdance, popularised by the show Riverdance, is notable for its rapid leg movements, with the body and arms being kept largely stationary. The solo stepdance is generally characterised by a controlled but not rigid upper body, straight arms, and quick, precise movements of the feet. The solo dances can either be in "soft shoe" or "hard shoe".		Ireland has a wealth of structures,[149] surviving in various states of preservation, from the Neolithic period, such as Brú na Bóinne, Poulnabrone dolmen, Castlestrange stone, Turoe stone, and Drombeg stone circle.[150] As the Romans never conquered Ireland, architecture of Greco-Roman origin is extremely rare. The country instead had an extended period of Iron Age architecture.[151] The Irish round tower originated during the Early Medieval period.		Christianity introduced simple monastic houses, such as Clonmacnoise, Skellig Michael and Scattery Island. A stylistic similarity has been remarked between these double monasteries and those of the Copts of Egypt.[152] Gaelic kings and aristocrats occupied ringforts or crannógs.[153] Church reforms during the 12th century via the Cistercians stimulated continental influence, with the Romanesque styled Mellifont, Boyle and Tintern abbeys.[154] Gaelic settlement had been limited to the Monastic proto-towns, such as Kells, where the current street pattern preserves the original circular settlement outline to some extent.[155] Significant urban settlements only developed following the period of Viking invasions.[153] The major Hiberno-Norse Longphorts were located on the coast, but with minor inland fluvial settlements, such as the eponymous Longford.		Castles were built by the Anglo-Normans during the late 12th century, such as Dublin Castle and Kilkenny Castle,[156] and the concept of the planned walled trading town was introduced, which gained legal status and several rights by grant of a Charter under Feudalism. These charters specifically governed the design of these towns.[157] Two significant waves of planned town formation followed, the first being the 16th and 17th century plantation towns, which were used as a mechanism for the Tudor English kings to suppress local insurgency, followed by 18th century landlord towns.[158] Surviving Norman founded planned towns include Drogheda and Youghal; plantation towns include Portlaoise and Portarlington; well-preserved 18th century planned towns include Westport and Ballinasloe. These episodes of planned settlement account for the majority of present-day towns throughout the country.		Gothic cathedrals, such as St Patrick's, were also introduced by the Normans.[159] Franciscans were dominant in directing the abbeys by the Late Middle Ages, while elegant tower houses, such as Bunratty Castle, were built by the Gaelic and Norman aristocracy.[160] Many religious buildings were ruined with the Dissolution of the Monasteries.[161] Following the Restoration, palladianism and rococo, particularly country houses, swept through Ireland under the initiative of Edward Lovett Pearce, with the Houses of Parliament being the most significant.[162]		With the erection of buildings such as The Custom House, Four Courts, General Post Office and King's Inns, the neoclassical and Georgian styles flourished, especially in Dublin.[162] Georgian townhouses produced streets of singular distinction, particularly in Dublin, Limerick and Cork. Following Catholic Emancipation, cathedrals and churches influenced by the French Gothic Revival emerged, such as St Colman's and St Finbarre's.[162] Ireland has long been associated with thatched roof cottages, though these are nowadays considered quaint.[163]		Beginning with the American designed art deco church at Turner's Cross in 1927, Irish architecture followed the international trend towards modern and sleek building styles since the 20th century.[164] Recent developments include the regeneration of Ballymun and an urban extension of Dublin at Adamstown.[165] Since the establishment of the Dublin Docklands Development Authority in 1997, the Dublin Docklands area underwent large-scale redevelopment, which included the construction of the Convention Centre Dublin and Grand Canal Theatre.[166] Completed in 2008, the Elysian tower in Cork is the tallest storeyed building in the Republic of Ireland (the Obel Tower in Belfast, Northern Ireland being the tallest in Ireland), at a height of 71 metres (233 feet), surpassing Cork County Hall. The Royal Institute of the Architects of Ireland regulates the practice of architecture in the state.[167]		Raidió Teilifís Éireann (RTÉ) is Ireland's public service broadcaster, funded by a television licence fee and advertising.[168] RTÉ operates two national television channels, RTÉ One and RTÉ Two. The other independent national television channels are TV3, 3e, UTV Ireland and TG4, the latter of which is a public service broadcaster for speakers of the Irish language. All these channels are available on Saorview, the national free-to-air digital terrestrial television service.[169] Additional channels included in the service are RTÉ News Now, RTÉjr, and RTÉ One +1. Subscription-based television providers operating in Ireland include Virgin Media and Sky.		Supported by the Irish Film Board, the Irish film industry grew significantly since the 1990s, with the promotion of indigenous films as well as the attraction of international productions like Braveheart and Saving Private Ryan.[170]		A large number of regional and local radio stations are available countrywide. A survey showed that a consistent 85% of adults listen to a mixture of national, regional and local stations on a daily basis.[171] RTÉ Radio operates four national stations, Radio 1, 2fm, Lyric fm, and RnaG. It also operates four national DAB radio stations. There are two independent national stations: Today FM and Newstalk.		Ireland has a traditionally competitive print media, which is divided into daily national newspapers and weekly regional newspapers, as well as national Sunday editions. The strength of the British press is a unique feature of the Irish print media scene, with the availability of a wide selection of British published newspapers and magazines.[170]		Eurostat reported that 82% of Irish households had Internet access in 2013 compared to the EU average of 79% but only 67% had broadband access.[172]		Irish cuisine was traditionally based on meat and dairy products, supplemented with vegetables and seafood. Examples of popular Irish cuisine include boxty, colcannon, coddle, stew, and bacon and cabbage. Ireland is famous for the full Irish breakfast, which involves a fried or grilled meal generally consisting of bacon, egg, sausage, pudding, and fried tomato. Apart from the significant influence by European and international dishes, there has been a recent emergence of a new Irish cuisine based on traditional ingredients handled in new ways. This cuisine is based on fresh vegetables, fish, oysters, mussels and other shellfish, and the wide range of hand-made cheeses that are now being produced across the country. Shellfish have increased in popularity, especially due to the high quality shellfish available from the country's coastline. The most popular fish include salmon and cod. Traditional breads include soda bread and wheaten bread. Barmbrack is a yeasted bread with added sultanas and raisins.		Popular everyday beverages among the Irish include tea and coffee. Alcoholic drinks associated with Ireland include Poitín and the world-famous Guinness, which is a dry stout that originated in the brewery of Arthur Guinness at St. James's Gate in Dublin. Irish whiskey is also popular throughout the country, and comes in various forms, including single malt, single grain and blended whiskey.[173]		Gaelic football and hurling are the traditional sports of Ireland as well as most popular spectator sports.[174] They are administered by the Gaelic Athletics Association on an all-Ireland basis. Other Gaelic games organised by the association include Gaelic handball and rounders.[175]		Soccer is the third most popular spectator sport and has the highest level of participation.[176] Although the League of Ireland is the national league, the English Premier League is the most popular among the public.[177] The Republic of Ireland national football team plays at international level and is administered by the Football Association of Ireland.[178]		The Irish Rugby Football Union is the governing body of rugby union, which is played at local and international levels on an all-Ireland basis, and has produced players such as Brian O'Driscoll and Ronan O'Gara, who were on the team that won the Grand Slam in 2009.[179]		The success of the Irish Cricket Team in the 2007 Cricket World Cup has led to an increase in the popularity of cricket, which is also administered on an all-Ireland basis by Cricket Ireland.[180]		Netball is represented by the Ireland national netball team.		Golf is another popular sport in Ireland, with over 300 courses countrywide.[181] The country has produced several internationally successful golfers, such as Pádraig Harrington and Paul McGinley.		Horse Racing has a very large presence in Ireland, with one of the most influential breeding and racing operations based in the country. Racing takes place at courses at The Curragh Racecourse in County Kildare and at Leopardstown Racecourse, racing taking place since the 1860s, but racing taking place as early as the early 1700s. Popular race meetings also take place at Galway. Operations include Coolmore Stud and Ballydoyle, the base of Aidan O'Brien arguably one of the world's most successful horse trainers. Ireland has produced champion horses such as Galileo, Montjeu, and Sea the Stars.		Boxing is Ireland's most successful sport at an Olympic level. Administered by the Irish Athletic Boxing Association on an all-Ireland basis, it has gained in popularity as a result of the international success of boxers such as Bernard Dunne, Andy Lee and Katie Taylor.		Some of Ireland's highest performers in athletics have competed at the Olympic Games, such as Eamonn Coghlan and Sonia O'Sullivan. The annual Dublin Marathon and Dublin Women's Mini Marathon are two of the most popular athletics events in the country.[182]		Rugby league is represented by the Ireland national rugby league team and administered by Rugby League Ireland (who are full member of the Rugby League European Federation) on an all-Ireland basis. The team compete in the European Cup (rugby league) and the Rugby League World Cup. Ireland reached the quarter finals of the 2000 Rugby League World Cup as well as reaching the semi finals in the 2008 Rugby League World Cup.[183] The Irish Elite League is a domestic competition for rugby league teams in Ireland.[184]		The profile of Australian rules football has increased in Ireland due to the International rules series that take place annually between Australia and Ireland. Baseball and basketball are also emerging sports in Ireland, both of which have an international team representing the island of Ireland. Other sports which retain a strong following in Ireland include cycling, greyhound racing, horse riding, motorsport, and softball.		Ireland ranks fifth in the world in terms of gender equality.[185] In 2011, Ireland was ranked the most charitable country in Europe, and second most charitable in the world.[186] Contraception was controlled in Ireland until 1979, however, the receding influence of the Catholic Church has led to an increasingly secularised society.[187] In 1983, the Eighth Amendment recognised "the right to life of the unborn", subject to qualifications concerning the "equal right to life" of the mother. The case of Attorney General v. X subsequently prompted passage of the Thirteenth and Fourteenth Amendments, guaranteeing the right to have an abortion performed abroad, and the right to learn about "services" that are illegal in Ireland but legal abroad. The prohibition on divorce in the 1937 Constitution was repealed in 1995 under the Fifteenth Amendment. Divorce rates in Ireland are very low compared to European Union averages (0.7 divorced people per 1,000 population in 2011) while the marriage rate in Ireland is slightly above the European Union average (4.6 marriages per 1,000 population per year in 2012).		Capital punishment is constitutionally banned in Ireland, while discrimination based on age, gender, sexual orientation, marital or familial status, religion, race or membership of the travelling community is illegal. The legislation which outlawed homosexual acts was repealed in 1993.[188][189] In 2010, the Dáil and the Seanad passed the Civil Partnership and Certain Rights and Obligations of Cohabitants Act, which recognised civil partnerships between same-sex couples.[190] It permits same-sex couples to register their relationship before a registrar.[191] A Sunday Times poll carried out in March 2011 showed that 73% of people believe that same-sex couples should be allowed to marry, while 60% believe that same-sex couples should be allowed to adopt children.[192] In April 2012, the Constitutional Convention voted overwhelmingly in favour of extending marriage rights to same-sex couples.[193] On 23 May 2015, Ireland became the first country to legalise same-sex marriage by popular vote.[194]		Ireland became the first country in the world to introduce an environmental levy for plastic shopping bags in 2002 and a public smoking ban in 2004. Recycling in Ireland is carried out extensively and Ireland has the second highest rate of packaging recycling in the European Union. It was the first country in Europe to ban incandescent lightbulbs in 2008 and the first EU country to ban in-store tobacco advertising and product display in 2009.[195][196] In 2015 Ireland became the second country in the world to introduce plain cigarette packaging.[197] Despite the above measures to discourage tobacco use, smoking rates in Ireland remain above 20% of the adult population and above those in other developed countries.[198]		The state shares many symbols with the island of Ireland. These include the colours green and blue, animals such as the Irish wolfhound and stags, structures such as round towers and celtic crosses, and designs such as Celtic knots and spirals. The shamrock, a type of clover, has been a national symbol of Ireland since the 17th century when it became customary to wear it as a symbol on St. Patrick's Day. These symbols are used by state institutions as well as private bodies in the Republic of Ireland.		The flag of Ireland is a tricolour of green, white and orange. The flag originates with the Young Ireland movement of the mid-19th century but was not popularised until its use during the Easter Rising of 1916.[199] The colours represent the Gaelic tradition (green) and the followers of William of Orange in Ireland (orange), with white representing the aspiration for peace between them.[200] It was adopted as the flag of the Irish Free State in 1922 and continues to be used as the sole flag and ensign of the state. A naval jack, a green flag with a yellow harp, is set out in Defence Forces Regulations and flown from the mast head of ships in addition to the national flag in limited circumstances (e.g. when a ship is not underway). It is based on the unofficial green ensign of Ireland used in the 18th and 19th centuries and the traditional green flag of Ireland dating from the 16th century.[201]		Like the national flag, the national anthem, Amhrán na bhFiann (English: A Soldier's Song), has its roots in the Easter Rising, when the song was sung by the rebels. Although originally published in English in 1912,[202] the song was translated into Irish in 1923 and the Irish-language version is more commonly sung today.[202] The song was officially adopted as the anthem of the Irish Free State in 1926 and continues as the national anthem of the state.[203] The first four bars of the chorus followed by the last five comprise the presidential salute.		The arms of Ireland originate as the arms of the monarchs of Ireland and was recorded as the arms of the King of Ireland in the 12th century. From the union of the crowns of England, Scotland and Ireland in 1603, they have appeared quartered on the royal coat of arms of the United Kingdom. Today, they are the personal arms of the President of Ireland whilst he or she is in office and are flown as the presidential standard. The harp symbol is used extensively by the state to mark official documents, Irish coinage and on the seal of the President of Ireland.		1. All twenty-eight member states of the European Union are also members of the WTO in their own right:		2. Special administrative regions of the People's Republic of China, participates as "Hong Kong, China" and "Macao China". 3. Officially the Republic of China, participates as "Separate Customs Territory of Taiwan, Penghu, Kinmen and Matsu", and "Chinese Taipei" in short.		Click on a coloured area to see an article about English in that country or region		Coordinates: 53°N 8°W﻿ / ﻿53°N 8°W﻿ / 53; -8		
The Iranian University Entrance Exam also known as the Concours (from the French; Konkoor, Konkour, and Konkur are transliterations of the Persian) is a standardized test used as one of the means to gain admission to higher education in Iran. Generally, to get Ph.D. in non-medical majors, there are 3 exams, all of them called Concour. In recent years there was parliament bill to gradually eliminate this entrance exam to enter universities in Iran.						In June each year, high school graduates in Iran take a stringent, centralized nationwide university entrance exam, called the Concours, seeking a place in one of the public universities. The competition is fierce and the exam content rigorous since the seats at universities are limited. In recent years, although the government has responded to demands for improved access and to a rapid increase in the rising number of applicants by enlarging the capacity of universities and creating Azad University, public universities are still only able to accept 10 percent of all applicants.		In Iran, as in many other countries where a university entrance exam is a sole criterion for student selection. Concours is a comprehensive, 4.5-hour multiple-choice exam that covers all subjects taught in Iranian high schools—from math and science to Islamic studies and foreign language. The exam is so stringent that normally students spend a year preparing for it; those who fail are allowed to repeat the test in the following years until they pass it.		A very lucrative cram industry offers courses to enthusiastic students. The Ministry of Science, Research, and Technology has established the Education Evaluation Organization to oversee all aspects of the test.		As the sole criterion for student admissions into universities in Iran, Concours has gone through many phases. In prerevolutionary Iran, the exam was—as currently—a comprehensive test of knowledge and assessment of academic achievement for admissions. However, the problem in this era was that the selection methods provide advantages to candidates from urban areas, especially those from the upper and upper-middle classes with better education and preparation. Thus, almost 70 to 80 percent of university entrants came from large urban cities.		In the early years of postrevolutionary Iran, the purpose of testing shifted from being just a mere test of knowledge to an instrument to ensure the "Islamization of universities," aimed at admitting students committed to the ideology of the revolution. The university entrance exam judged admissions candidates not only by their academic test score but also by their social and political background and loyalty to the Islamic government.		In the early 1980s, a quota system was introduced to further democratize the selection criteria by allowing preferential treatments to underprivileged students. A year after the Iran-Iraq war ended, a law was passed to help handicapped and volunteer veterans to enter universities, reserving 40 percent of university seats for war veterans.		An additional criterion for student selection was introduced in the early 1990s to localize the student population, giving priority to candidates who applied to study in their native provinces. This policy was to prevent student migration into the larger cities. The requirement of service after graduation also was instrumental in providing education and health to the needy areas.		Despite attempts made in recent years to reform university selection criteria and to promote the equalization of educational opportunities, the Concours remains an impediment to equal education access. Both quantitatively and qualitatively, the quota criteria have worked against students whose academic performance is superior to those favored by the quota system. Another factor that contributes to the phenomenon of student elimination is the lack of infrastructure and facilities in spite of the expansion of infrastructure and establishment of an "open" university, Azad University. Azad University, a semi private university, favors its autonomy in governance, but its degrees and curriculum are overseen by the Ministry of Science, Research, and Technology.		The other drawback is the nature of the test itself. As in many other countries where only a long multiple-choice, mostly memory-based exam is used to select qualified applicants to enter universities, Iranian schools have been turned into factories for exam cramming.		Concours, especially in recent years, has further contributed to the massive brain drain from Iran and has created psychological and social problems such as anxiety, boredom, and hopelessness among the youth who fail the test.		In Iran, admission to university—especially prestigious public and highly selective universities such as Amirkabir University of Technology(AUT), Sharif University of Technology, University of Tehran,Tarbiat Modares University, Iran University of Science and Technology(IUST), K. N. Toosi University of Technology(KNTU), and Isfahan University of Technology —remains a means of achieving elevated status in a society where education is a major determinant of class mobility. Graduates of such universities have a better chance of securing the increasingly limited jobs in the prestigious professions in Iran—medicine, engineering, law—making success in the entrance exam the first and perhaps the most important hoop through which Iranian youths must jump.		As the Concours crisis persists, authorities are contemplating a replacement mechanism for student selection. One of the options being considered is to use the cumulative grade-point average (GPA) of the final three years of high school to admit students. While this policy seems more humanistic and fair than using a single exam to measure students' preparedness, it still cannot ensure fairness or reveal students' aptitude for further learning. Perhaps incorporating interviews, essay writing, and aptitude tests, in addition to GPA would be a more effective way of measuring students' qualifications to enter universities.		Another long-term approach to remedy the Concours crisis in Iran would be to rely on midcareer education and training in place of the precareer pattern of university education by introducing the community college concept into the education system of Iran. This approach could serve to divert less academically inclined students from participating in the university entrance exams and hopefully eventually alleviate the Concours crisis.		
The Leaving Certificate Examinations (Irish: Scrúduithe na hArdteistiméireachta), which is commonly referred to as the Leaving Cert (Irish: Ardteist) is the final examination in the Irish secondary school system. It takes a minimum of two years preparation, but an optional Transition Year means that for those students it takes place three years after the Junior Certificate Examination. These years are referred to collectively as "The Senior Cycle." Most students taking the examination are aged 16–20; in excess of eighty percent of this group undertake the exam. The Examination is overseen by the State Examinations Commission. The Leaving Certificate Examinations are taken annually by approximately 55,000 students.[1]						There are three distinct programmes that can be followed. While the outcomes of each programme are quite distinct, each is intended to reinforce the principles of secondary education; to prepare the student for education, society and work.		Each subject is examined at one of three levels, Higher Level (informally Honours), Ordinary Level (informally Pass), or Foundation Level. Foundation Level may only be taken in two subjects: Irish and Mathematics. All other subjects may be taken at either Ordinary or Higher Level.		The points awarded for a given percentage range are given in the table right.		Maths Bonus Points		25 bonus points will be awarded for Higher Level Mathematics for H6 grades and above. For example, if an applicant receives a H6 grade an additional 25 points will be added to the 46 points already awarded for a H6 grade i.e. Higher Level Mathematics now carries a points score of 71 for this applicant.		Since 2012, a pass (min H6) in higher level Mathematics is awarded 25 bonus points, making it possible to earn 125 points in this subject.[3] If a student gets a H2, instead of receiving 88 points, as they would in other subjects, they receive 113. This also means that, provided they pass, the minimum number of points a student can receive is 71, which is 15 points greater than a O1 at Ordinary Level.		The points allocations in the table right have been collectively agreed by the third-level institutions involved in the CAO scheme, and relativities that they imply have no official standing in the eyes of the State Examinations Commission or the Department of Education and Skills.		Below is the list of subjects available to Established Leaving Certificate students, though most schools only offer a limited number.		The following languages can only be taken if the student is from a member state of the European Union, speak the language in which they opt to be examined in as a mother tongue, has followed a programme of study leading to the Leaving Certificate and is taking Leaving Certificate English. Another condition is that candidates may undertake examination in one non-curricular language subject only.[4]		†Subject exclusions – candidates may not take any of following subject combinations:		The Leaving Certificate Vocational Programme is an additional Link Module which may be taken along with the other optional subjects. Students wishing to sit the LCVP Link Modules Exam must meet certain requirement. They must take an extra language subject and must have one or more of the following subject combinations:		Subjects are examined through a number of methods. These will include at least one written paper (English, Mathematics, Irish and some of the optional courses contain two written papers).		Language courses examine the students writing, conversation and listening skills. The spoken section of the exams ('oral') take place some months before the written exams, and the listening ('aurals') take place in the same weeks as the written.		A number of subjects in the sciences and arts include the keeping of records or creation of a physical object or project. This work is designed to provide tangible proof of the students abilities. However, not every book or project is examined, with inspectors being sent to a small few, randomly selected schools each year, or simply examining a small selection of projects from each class to check the standard. Some subjects such as Art and Technology involve a practical exam which is supervised by an external examiner. In the academic year of the written exam, all practical science subjects are partially examined by student assignments which involve stepping away from the books and getting students to put the theory they have learned into practice.		Each subjects paper at Leaving Certificate level may have as few as two variants, or as many as six. They are divided by level: Higher and Ordinary, and in the case of Irish and Mathematics, Foundation. Each subject level-variant will also have provisions for both English and Irish speakers, with the exception of English and Irish themselves (which are printed exclusively in the relevant language). Certain subjects are printed in a combined English/Irish format, such as French or German. This leaves such subjects with only two versions: a bilingual Higher, and a bilingual Ordinary. However Mathematics in contrast has a total of six: three levels: Higher, Ordinary and Foundation, each with both English and Irish versions.		Higher Level papers are printed on pink paper, while Ordinary Level papers are printed on powder blue paper. In the case of certain subjects, such as Geography, full-colour photographs need to be printed and as such, all pages but the cover are white.		On the first day of examinations on 3 June 2009, the second paper of the Leaving Certificate English examination, (initially scheduled for 4 June), was accidentally distributed instead of Paper 1 at an examination centre in St. Oliver's Community College, Drogheda, Co. Louth.[6] It was confirmed that a number of candidates had seen the paper before the mistake was acted upon. The examiner had failed to report the incident straight away and was immediately suspended. A State Examinations Commission official had visited the examination centre on the day in question as part of a routine inspection, and no report was made by the invigilator to the official.[7] Due to the time at which the SEC was informed, it was unable to distribute the contingency paper in time for the following morning. Details of the leaked paper had already circulated onto many message boards and social networking sites, many hours after the incident had taken place.[8] The exam was rescheduled for Saturday 6 June, from 09:30 to 12:50.[9] About 10 Jewish students, who could not sit the exam at the rescheduled time because it conflicted with Shabbat, were sent to an Orthodox household in Dublin, where they were sequestered from all electronic media (as is normal for Shabbat) and kept under supervision until they sat the exam on Sunday morning.[10]		On 12 June 1969, exam papers were stolen in a break in to a Dublin secondary school. Examination papers, including English, Mathematics and physics were circulated among students.[11] The repeat examinations for English and Mathematics paper 2 were rescheduled for 27 June 1969 and 28 June 1969 respectively.[12]		In 1957, papers in Latin, English and Mathematics became available to some students before the exams.[13] Supplemental examination were held later in June.[14]		Matriculation is administered by the Central Applications Office (CAO) following requirements laid down by the universities. Applicants must present English and usually Mathematics and Irish. Some courses require specific subjects to be taken at secondary level. For example, veterinary medicine applicants must present with a minimum grade of C3 in Chemistry at higher level. Most commonly, engineering and science programs require Mathematics and/or a physical science. Other courses, such as medicine, have similar matriculation requirements. From 2012 onwards, the greatest score that can be achieved in the Leaving Certificate is [625 points, equivalent to six higher level A1's and 25 bonus points for passing (scoring a D3 or higher) higher level mathematics. Every year approximately 150 students get the maximum grade with 10-20 students receiving 7 A1s or more.		Generally, students will be required to have pass grades in English, Irish and Mathematics to gain entry to university in Ireland. The concept of failing the leaving certificate is not applied. If demand exceeds supply for a course (which it usually does), the CAO will award candidates points based on their Leaving Certificate performance in six subjects. The majority of candidates take six to eight subjects, including English, Mathematics and Irish (exemptions available) and usually a foreign language, with the points from their six highest scoring subjects being considered. Once base criteria have been met, course places are offered to the applicants with the highest points.		Subjects taken at foundation level are rarely counted for matriculation to university.		The University of Limerick awards up to 40 bonus points for Mathematics (Higher Level) in an increasing scale of points starting at 5 bonus points for a C3 continuing up to 40 for an A1 grade. This is an attempt to correct the recent decline in demand for scientific subjects. In 2009, 16.2% of students attempted the higher level Mathematics paper.[15] It also reflects a return to earlier times, pre 1982,[citation needed] when the points scored for mathematics were doubled. This bonus for achievement in mathematics was removed mid 1980s[citation needed] because of a populist reaction against bias being shown towards persons who were supposedly "naturally" talented at mathematics, and being unfair to persons who were not mathematically inclined. Recent Minister for Education, Batt O'Keeffe, acknowledged that he sat mathematics at ordinary level in the Leaving Certificate because he was aware of the extensive study that would be required by this subject.		Some universities require a foreign language and Irish. Exemptions are available for learning difficulties, birth outside Ireland, not having taken Irish before the age of eleven years, and studying abroad for a period of at least two years after the age of eleven.[16]		The Department of Education and Skills will introduce a new Leaving Certificate grading scale in 2017. The new scale has 8 grades, the highest grade is a Grade 1, the lowest grade a Grade 8. The highest seven grades 1-7 divide the marks range 100% to 30% into seven equal grade bands 10% wide, with a grade 8 being awarded for percentage marks of less than 30%. The grades at higher level and ordinary level are distinguished by prefixing the grade with H or O respectively, giving H1-H8 at higher level, and O1-O8 at ordinary level. This new 8-point grading scale will replace the current 14-point scale at both Higher and Ordinary levels. Currently, the majority of students receiving a given grade are within 3 percentage marks of a higher grade, and 5 extra points, creating pressure towards rote learning and using the marking scheme to gain those few additional marks. The new broader grade bands will ease the pressure on students to achieve marginal gains in examinations and encourage more substantial engagement with each subject. The new grading system will also allow for greater flexibility, variety and innovation in Leaving Certificate assessments. The broader objective is to allow for an enhanced learner experience in senior cycle, with a greater focus on the achievement of broader learning objectives. The new 8-point scale moves the Irish Leaving Certificate closer to school leaving examinations in other countries, such as Scotland, England, Wales and Northern Ireland, Finland, and to the International Baccalaureate.[3]		Some Irish students go to university in the United Kingdom, particularly in Northern Ireland and larger British cities. Increasingly students from the Republic attend university in Northern Ireland, and vice versa.		In recognition of this, the Established Leaving Certificate underwent a process with UCAS to gain entry to the UCAS Tariff for direct entry to United Kingdom universities.[17] This introduced the examination directly onto the UCAS Tariff, allowing it to be compared more easily with other qualifications on the UCAS Tariff. After comparing syllabi of the Irish Leaving Certificate and British GCE A-Levels, it was decided that a Leaving Certificate (higher) subject will be worth two-thirds of an A-level (UK, except Scotland). This is because Leaving Certificate students undertake several more subjects (often a total of six to eight) than a typical A level student, but as a result study them in a slightly more narrow fashion.		The Irish Leaving Certificate can be used for university applications in Germany. For subjects that require the applicant to produce a certain minimum average grade, however, the Irish point score must first be converted into the German grading system, e.g. 520 points = 1.5 in the German Abitur, 450 points = 2.0 and so on.		The exams begin on the first Wednesday after the June Bank Holiday every year, traditionally commencing with English Paper One, followed by Paper Two on Thursday afternoon.[18] The exams typically last two and a half weeks, but for some students may last longer due to exams in uncommon subjects, such as Swedish, Polish, etc. The 2017 exams began on 7 June and ended on 23 June, with results released on 16 August. The 2018 exams commence on 6 June.		The exam timetable was reorganised in 2008 to reduce the intensity of exam period. Particular changes included the moving of English Paper Two to Thursday afternoon, as opposed to its usual time of Wednesday afternoon after English Paper One, to reduce the amount of writing candidates were required to do at the beginning of the exams.		Only one school outside Ireland offers the Leaving Certificate exam to their students. Since 1997 students at the ISM international school in Tripoli, Libya take the Leaving Cert, with Arabic being substituted for Irish. The School's principal said, "We have students from 42 countries studying at our school; the Irish Leaving Certificate programme offered us the kind of academic standard and subject spread that we were looking for".[citation needed]		
Education in Ontario comprises public and private primary and secondary schools and post-secondary institutions. By right of the constitution of Canada, Roman Catholics are entitled to their own school system.[6] Ontario's school boards are divided among four large separate publicly funded school systems: 31 English public, 29 English Catholic, four French public, and eight French Catholic.[1] One non-Roman Catholic separate school board, the Penetanguishene Protestant Separate School Board operates in Penetanguishene, Ontario.[7] Post-secondary education in Ontario consists of 20 public universities, 24 public colleges and over 400 registered private career colleges.[8]		In Canada, education falls entirely under provincial jurisdiction. There is no federal government department or agency involved in the formation or analysis of policy regarding education. Publicly funded elementary and secondary schools are administered by the Ontario Ministry of Education, while colleges and universities are administered by the Ontario Ministry of Advanced Education and Skills Development. The current Minister of Education is Mitzie Hunter, and the current minister of Advanced Education and Skills Development is Deb Matthews.						Upper Canada's Grammar School Act of 1807 provided the first public funds for schools in what would become Ontario. 8 schools were opened.[9]		1816: The Act of 1816 authorized local trustees to decide on hiring criteria for teachers.[10]		1823: A General Board of Education was established.[11]		1824: The Legislature supported "moral and religious instruction of the more indigent and remote settlements" by granting the Board of Education a budget to create Sunday schools.[12]		1824: The right to decide hiring criteria for teachers was transferred from trustees to the district Board.[12]		1841: With the union of Upper and Lower Canada into the Province of Canada, the position of General Superintendent of Education was created.[12]		1843: With the realization that Canada West (formerly Upper Canada) and Canada East (formerly Lower Canada) had vastly different educational needs, the Act of 1841 was repealed. The Act of 1843 created the position of Chief Superintendent of Education for Canada West (which would become the Province of Ontario in 1867).[13] Egerton Ryerson is Chief Superintendent from 1844 until his retirement in 1876.[14]		1847: Chief Superintendent Egerton Ryerson returns from a tour of European education systems and submits his "Report on a System of Public Elementary Instruction in Upper Canada".[15] On the religious issue, he writes that "religious differences and divisions should rather be healed than inflamed".[15]:p62		1876: The first Minister of Education was appointed,[12] after Ryerson retired after 22 years as Chief Superintendent of Education.		1968: Release of the Hall-Dennis Report, officially titled Living and Learning.		1984: Grade 13 is replaced by OAC (Ontario Academic Credits)		1997: Education funding moves to the provincial level.[16]		2003: High school becomes a 4-year program, with the phasing out of OAC.		2013: Release of the Fullan Report, officially titled Great to Excellent.[17]		Education in Ontario is governed by the Education Act.[18] Provincial, federal and international human rights codes and charters also have stipulations on children's learning experiences.		0.1(1) Public education is vital.		0.1(2) The purpose of education is to help students reach their full potential while becoming caring, contributing citizens.		0.1(3) All partners in the education sector have a role to play. Partners include the Minister, the Ministry, and the boards.		1(1) of the Education Act defines bullying as behaviour within a power imbalance that creates harm, fear, distress or a negative environment. The Act specifies that it only counts as bullying if it is done by a pupil.		The following sections of the Education Act are related to learning options:		1(1) defines the following terms related to learning options:		8(1)-3: The Minister of Education may approve, or allow boards to approve, courses of study that are not based on the curriculum guidelines, to be used in lieu of prescribed courses.		8(1)-3.0.1 Equivalent learning policies, guidelines and standards may be established by the Minister of Education. The Minister may require that boards offer equivalent learning opportunities. The Minister may designate entities who may offer equivalent learning within a board.[20]		8(1)-6 The Minister may decide which learning materials can or can't be used.		8(1)-14 The Minister may approve anyone as a teacher, regardless of official qualifications, if the candidate is considered to have equivalent experience.		8(2) Equivalent learning must not be of lesser educational value than traditional learning.[21]		8(3) Everyone qualifying for special education programs shall get them. Identification of special needs shall be early and ongoing.		10 The Minister may establish an advisory body or commission of inquiry at any time.		13(5) The Minister may establish demonstration schools.		21(2) Compulsory attendance is excused if the person is receiving satisfactory instruction at home or elsewhere.		21(5) Parents are legally liable for their children obeying compulsory schooling laws, unless the child has turned 16 and has withdrawn from parental controls.		41(2) A child may be admitted to secondary school if the principal believes the child is prepared, regardless of having attended elementary school.		41(5) Students who have demonstrated to their principal that they are not competent in a course or program of study must change program, or take a prerequisite.		55 The Minister may regulate having student trustees on school boards, representing students in grades 9-12. A student trustee's vote doesn't count.		57(3) A Special Education Tribunal is available if a parent wishes to appeal a decision regarding special education for their child.[22]		57.1(1) Every board shall have a special education advisory committee.		169.1(1)(c) Boards shall provide every student with an effective and appropriate education program.		170.1-7.3 Boards shall provide equivalent learning opportunities to their pupils.		218.1(d) Board members shall bring concerns of parents, students and supporters to the attention of the board.		218.1(g) Board members shall maintain focus on student achievement and well-being.		264(1)(c) Teachers (meaning members of the Ontario College of Teachers) must "inculcate by precept and example respect for religion and the principles of Judaeo-Christian morality and the highest regard for truth, justice, loyalty, love of country, humanity, benevolence, sobriety, industry, frugality, purity, temperance and all other virtues."		8(1)29.3 The Minister may regulate the nutritional content of school food.		8.1 regulates the collection and use of personal information. Education data is used to plan and evaluate programs.		A child's educational experience is informed by the following provisions of the Ontario Human Rights Code:		The Preamble says that "it is public policy in Ontario to recognize the dignity and worth of every person and to provide for equal rights and opportunities without discrimination that is contrary to law, and having as its aim the creation of a climate of understanding and mutual respect for the dignity and worth of each person so that each person feels a part of the community and able to contribute fully to the development and well-being of the community and the Province".[23]		Section 1 says that everyone has the right to equal treatment with respect to such things as services and facilities.		Section 19(2) says that the Code does not apply to the duties of teachers as they fulfill their requirements in the Education Act.		The Canadian Charter of Rights and Freedoms does not necessarily restrict how children can be treated under the Education Act, because Section 1 allows for restrictions on the Charter's application "as can be demonstrably justified in a free and democratic society."[24] For example, compulsory schooling laws can override a child's right to freedom of association.		The Convention on the Rights of the Child is a legally binding treaty with provisions on children's education. In particular, article 3(1) mandates that states act in the best interest of the child.[25]		Article 12(1) mandates that children be able to have input on all matters that effect them. Their input will have "due weight in accordance with the age and maturity of the child."		Article 28 and 29 mandate free, compulsory primary education, including:		Ontario operates four publicly funded school systems: An English-language public school system, a French-language public school system, an English- language separate school system and a French-language separate school system. The public school system was originally Protestant but is now secular. The Separate School system is Roman Catholic (open to students of all faiths at secondary level, they have the option of refusing non-Catholics at the elementary level[citation needed]) with the exception of the Penetanguishene Protestant Separate School Board which runs a single Protestant school.		School Board Funding Projections (excluding capital programs) estimates more than twenty billion dollars will be put toward the province's 73 school board districts in the 2012-2013 school-year.[26]		Preschool daycare is usually for youngsters under 4 years of age. Next comes Junior Kindergarten/J.K (for youngsters who turn 4 before Dec 31) and Senior Kindergarten/S.K (for youngsters who turn 5 before Dec 31)		Ministry of Education documents divide grades into two categories: Elementary (Grades 1-8) and Secondary (Grades 9-12, also called High School). Local variances exist in the division of grades (e.g. middle school).[27]		While children must begin school if they are 6 years old on the first day of the school year,[28] the cutoff is Dec 31st:[29] A 5-year-old may start Grade 1 if he/she will turn 6 by Dec 31st. Children typically start a grade if they will be the following age by Dec 31st:		There is no legal age or time constraint against attending secondary school longer than 4 years, although a limit on course credit exists. Victory lappers make up an average of 4% of all students enrolled in Ontario secondary schools each year.[30][31]		Split-grade classes are common.[32]		A pilot project groups math students by ability rather than age.[citation needed]		Montessori schools group students in 3-year age groups. Democratic schools have complete age-mixing.[33] Homeschooling families and unschoolers decide for themselves about age-mixing.		Roots of Empathy brings a baby into an elementary school classroom, regularly over the course of a year, to help students develop social/emotional competence and empathy.		Almost all Ontario public schools, and most private schools, follow the Ontario Curriculum.[34] It has specific requirements about knowledge and behaviours to be learned, while allowing flexibility in how the curriculum is delivered.		The secondary school curriculum offers three streams (or tracks): the Grades 9 and 10 curricula have the academic, applied and essentials streams. The academic courses are to prepare students for university-bound courses; the applied courses are to prepare students for college-bound courses; and the essentials courses are mainly for students, but not exclusively, with learning exceptionalities (e.g., learning disabilities, intellectually delayed, etc.). Recently, there have been parents and educational advocates pushing for de-streaming Grades 9 and 10, so students have more time to choose which pathway they would like to take.		Then the Grades 11 and 12 curricula have university-bound, college-bound and workplace-bound courses. All of the courses are to develop students' higher-order thinking skills, and every secondary school course must have a focus on skills-based learning no matter the stream; however, academic/university-bound courses tend to have a strong focus on abstract thinking and knowledge-based learning, and usually pushes students to become more independent in their learning compared with college- and workplace-bound courses. Once a student is taking a Grade 12 university-bound course, then the student is expected to be academically and intellectually challenged in order for them to build their knowledge and skills for a university post-secondary education. Even though college-bound courses are academically rigorous, there is more of a focus on skills-based learning and making the content of the course practical for everyday life. Workplace-bound courses are even more practical. Each stream has different learning expectations, so marks in each stream are not equal to one another. For example, an 85% average student in Grade 12 College English would not be considered for a university program if that student has not taken the Grade 12 University English credit.		Within the public boards, alternative schools have begun to emerge. In 2009 the Africentric School opened in Toronto and in 2011 the DSBN Academy opened its doors. The Africentric school was established in part to address the 40% dropout rate of black students in the Toronto District School Board.[35] the DSBN Academy is a new school aimed at providing additional supports for students who may lack access or the resources to attend post secondary education. These alternative schools are based on social contexts that the individual school boards deem necessary for their constituents and are funded within the publicly funded school systems.[36]		In addition, alternative schools in Ontario have also come to be for religious contexts as in the case of Eden High School in St. Catharines."Eden is a publicly funded secondary school that operates as an alternative secondary school within the District School Board of Niagara. The school offers the prescribed Ontario Ministry of Education’s Secondary School program delivered in the context of a community where the educational objectives of the Ministry of Education and those of Eden's own Spiritual Life Department are respected and regarded as complementary in the training of students."[37]		Ontario private schools must do the following:		A private institution is considered a school if "instruction is provided at any time between the hours of 9 a.m. and 4 p.m. on any school day for five or more pupils who are of or over compulsory school age in any of the subjects of the elementary or secondary school courses of study."[41]		Private schools that meet provincial standards may offer the Ontario Secondary School Diploma.		"There are approximately 700 private schools in Ontario, most represented by associations uniting schools of a common goal, view, or philosophy. Instructors are not required to be members of the Ontario College of Teachers, though many often are. More importantly, instructors are commissioned if their credentials satisfy the requirements outlined by their respective private school. As there are several types of private schools from elementary to the secondary school level, experience and training will differ for each."[42]		In 2012–2013, the total expenditure for both public and private primary and secondary education in Ontario was $26,975,997,000.[43]		The provincial government allocates funds to school boards based on factors including:[16]		Ontario is the only province in Canada that funds religious schools of the Catholic faith exclusively. This has been criticized by taxpayers,[44] the national LGBT community,[45] and even high profile celebrities like blogger Perez Hilton.[46]		This issue received media attention during the controversy surrounding gay-straight alliances, especially after Halton Catholic District School Board's Alice Anne Lemay compared gay-straight alliance groups to those of the Nazis in an effort to explain her board's decision to ban gay-straight alliances in their schools.[47] The ban on gay-straight alliances was repealed with board members voting 6-2 on the issue.[48]		The UN has cited Ontario for discrimination against non-Catholics, as UNESCO states that religion not be affiliated with any political powers.[49] They are suggesting that Ontario either fund no faith-based schools, or all of them. While Ontario's government maintains that its hands are tied in the matter (as the Constitution protects funding for the Catholic faith), many critics use the examples of Manitoba, Quebec, and Newfoundland (as provinces which have successfully navigated this obstacle) to counter their argument.[50]		A CBC poll suggested that 58.2% of Ontarians want a single publicly-funded school system.[51]		About half of Ontario's government-funded District School Boards are Catholic (37 out of 72).[52]		Contrary to the belief that Catholic Schools are funded exclusively, the Canadian Constitution of 1867 gave the Provinces the responsibility of education. Because schools were religious based at the time, they put in protections for the minority religion whether it be Catholic or Protestant. Over time the majority Protestant boards have become secular as they had not had protections of minority status. This has not always been true though as there is one Protestant school board and Protestant school left in Ontario, The Burkevale Protestant Separate School in the Penetanguishene Protestant Separate School Board. [53] [54]		There are many organizations in Ontario supporting education.[55]		People for Education provides "research, resources and connections for everyone who cares about public education." Executive Director Annie Kidder believes that "Our public schools belong to all of us. They could and should be thriving hubs of every community — hubs of learning, of support for families and of neighbourhood activity.”[56]		The Society for Quality Education wants every child to succeed. The problem is that "most public schools are not using the most effective teaching methods and materials available. And, as a result, the students are not learning nearly as much as they could."[57]		The Ontario Student Parent & Educator Survey is an annual survey regulated by the Ontario Student Trustees Association[58][59]		The Ministry of Education does a School Climate Survey of students at least every two years, to understand and prevent bullying, and to support a positive school climate.[60]		
A secondary school is both an organization that provides secondary education and the building where this takes place. Some secondary schools provide both lower secondary education and (upper) secondary education (levels 2 and 3 of the ISCED scale), but these are often provided in separate schools. When lower secondary education is provided in the same school as primary education or in a separate school, usually called a middle school, it is usually not called secondary education (except by some education experts) and is considered to be the second and final phase of basic education. Secondary schools typically follow on from primary schools and lead into vocational and tertiary education. Attendance is compulsory in most countries for students between the ages of 13 and 16. The organisations, buildings, and terminology are more or less unique in each country.[1][2]						Within the English speaking world, there are three widely used systems to describe the age of the child. The first is the 'equivalent ages', then countries that base their education systems on the 'British model' use one of two methods to identify the year group, while countries that base their systems on the 'American K-12 model' have refer to their year groups as 'grades'. This terminology extends into research literature. Below is a convenient comparison [3]		School building design does not happen in isolation. The building (or school plant US) needs to accommodate:		Each country will have a different education system and priorities. [4] Schools need to accommodate students, staff, storage, mechanical and electrical systems, storage, support staff, ancillary staff and administration. The number of rooms required can be determined from the predicted roll of the school and the area needed. A general classroom for 30 students needs to be 55 m², or more generously 62 m². A general art room for 30 students needs to be 83 m², but 104 m² for 3D textile work. A drama studio or a specialist science laboratory for 30 needs to be 90 m². Examples are given on how this can be configured for a 1,200 place secondary (practical specialism).,[5] and 1,850 place secondary school.[6]		The building providing the education has to fulfil the needs of: The students, the teachers, the non-teaching support staff, the administrators and the community. It has to meet general government building guidelines, health requirements, minimal functional requirements for classrooms, toilets and showers, electricity and services, preparation and storage of textbooks and basic teaching aids. [7] An optimum secondary school will meet the minimum conditions and will have :		Government accountants having read the advice then publish minimum guidelines on schools. These enable environmental modelling and establishing building costs. Future design plans are audited to ensure that these standards are met but not exceeded. Government ministries continue to press for the 'minimum' space and cost standards to be reduced.		The UK government published this downwardly revised space formula in 2014. It said the floor area should be 1050m² (+ 350m² if there is a sixth form) + 6.3m²/pupil place for 11- to 16-year-olds + 7m²/pupil place for post-16s. The external finishes were to be downgraded to meet a build cost of £1113/m². [8]		A secondary school, locally may be called high school or senior high school. In some countries there are two phases to secondary education (ISCED 2) and (ISCED 3), here the junior high school, intermediate school, lower secondary school or middle school occurs between the primary school(ISCED 1) and high school.		
Canada's geography is divided into administrative divisions known as provinces and territories that are responsible for delivery of sub-national governance. When Canada was formed in 1867, three provinces of British North America—New Brunswick, Nova Scotia and the Province of Canada (which, on the formation of Canada, was divided into Ontario and Quebec)—were united to form a federated colony, which eventually became a sovereign nation in the next century. Over its history, Canada's international borders have changed several times, and the country has grown from the original four provinces to the current ten provinces and three territories. The ten provinces are Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, and Saskatchewan. Several of the provinces were former British colonies, and Quebec was originally a French colony, while others were added as Canada grew. The three territories are Northwest Territories, Nunavut, and Yukon, which govern the rest of the area of the former British North America. Together, the provinces and territories make up the world's second-largest country by area.		The major difference between a Canadian province and a territory is that provinces receive their power and authority from the Constitution Act, 1867 (formerly called the British North America Act, 1867), whereas territorial governments have powers delegated to them by the Parliament of Canada. The powers flowing from the Constitution Act are divided between the federal government and the provincial governments to exercise exclusively. A change to the division of powers between the federal government and the provinces requires a constitutional amendment, whereas a similar change affecting the territories can be performed unilaterally by the Parliament of Canada or government.		In modern Canadian constitutional theory, the provinces are considered to be sovereign within certain areas based on the divisions of responsibility between the provincial and federal government within the Constitution Act 1867, and each province thus has its own representative of the Canadian "Crown", the lieutenant governor. The territories are not sovereign, but instead their authorities and responsibilities come directly from the federal level, and as a result have a commissioner instead of a lieutenant governor.								Notes:		British Columbia Parliament Buildings		Alberta Legislative Building		Saskatchewan Legislative Building		Manitoba Legislative Building		Ontario Legislative Building		Parliament Building (Quebec)		Confederation Building (Newfoundland and Labrador)		New Brunswick Legislative Building		Province House (Nova Scotia)		Province House (Prince Edward Island)		There are three territories in Canada. Unlike the provinces, the territories of Canada have no inherent sovereignty and have only those powers delegated to them by the federal government.[8][9][10] They include all of mainland Canada north of latitude 60° north and west of Hudson Bay, as well as most islands north of the Canadian mainland (from those in James Bay to the Canadian Arctic islands). The following table lists the territories in order of precedence (each province has precedence over all the territories, regardless of the date each territory was created).		Yukon Legislative Building		Northwest Territories Legislative Building		Legislative Building of Nunavut		Ontario, Quebec, New Brunswick, and Nova Scotia were the original provinces, formed when several British North American colonies federated on July 1, 1867, into the Dominion of Canada and by stages began accruing the indicia of sovereignty from the United Kingdom.[14] Prior to this, Ontario and Quebec were united as the Province of Canada. Over the following years, Manitoba (1870), British Columbia (1871), and Prince Edward Island (1873) were added as provinces.[14]		The Hudson's Bay Company controlled large swathes of western Canada referred to as Rupert's Land and the North-Western Territory until 1870, when it turned the land over to the Government of Canada.[15] Subsequently, the area was re-organized into the province of Manitoba and the Northwest Territories.[15] The Northwest Territories were vast at first, encompassing all of current northern and western Canada, except for the British holdings in the Arctic islands and the Colony of British Columbia; the Territories also included the northern two-thirds of Ontario and Quebec, and almost all of present Manitoba, with the 1870 province of Manitoba originally being confined to a small area in the south of today's province.[16] The remaining Arctic islands were transferred by Britain to Canada in 1880, adding to the size of the Northwest Territories. 1898 saw the Yukon Territory, later renamed simply as Yukon, carved from the parts of the Northwest Territories surrounding the Klondike gold fields. On September 1, 1905, a portion of the Northwest Territories south of the 60th parallel north became the provinces of Alberta and Saskatchewan.[16] In 1912, the boundaries of Quebec, Ontario and Manitoba were expanded northward: Manitoba's to the 60° parallel, Ontario's to Hudson Bay and Quebec's to encompass the District of Ungava.[17]		In 1869, the people of Newfoundland voted to remain a British colony over fears that taxes would increase with Confederation, and that the economic policy of the Canadian government would favour mainland industries.[18] In 1907, Newfoundland acquired dominion status.[19] In the middle of the Great Depression in Canada with Newfoundland facing a prolonged period of economic crisis, the legislature turned over political control to the Commission of Government in 1933.[20] Following Canada's participation in World War II, in a 1948 referendum, a narrow majority of Newfoundland citizens voted to join the Confederation, and on March 31, 1949, Newfoundland became Canada's tenth province.[21] In 2001, it was officially renamed Newfoundland and Labrador.[22]		In 1903, the Alaska Panhandle Dispute fixed British Columbia's northwestern boundary.[23] This was one of only two provinces in Canadian history to have its size reduced. The second reduction, in 1927, occurred when a boundary dispute between Canada and the Dominion of Newfoundland saw Labrador increased at Quebec's expense – this land returned to Canada, as part of the province of Newfoundland, in 1949.[24] In 1999, Nunavut was created from the eastern portion of the Northwest Territories.[25] Yukon lies in the western portion of The North, while Nunavut is in the east.[26]		All three territories combined are the most sparsely populated region in Canada, covering 3,921,739 km2 (1,514,192 sq mi) in land area.[5] They are often referred to as a single region, The North, for organisational and economic purposes.[27] For much of the Northwest Territories' early history it was divided into several districts for ease of administration.[28] The District of Keewatin was created as a separate territory from 1876 to 1905, after which, as the Keewatin Region, it became an administrative district of the Northwest Territories.[29] In 1999, it was dissolved when it became part of Nunavut.		Theoretically, provinces have a great deal of power relative to the federal government, with jurisdiction over many public goods such as health care, education, welfare, and intra-provincial transportation.[30] They receive "transfer payments" from the federal government to pay for these, as well as exacting their own taxes.[31] In practice, however, the federal government can use these transfer payments to influence these provincial areas. For instance, in order to receive healthcare funding under Medicare, provinces must agree to meet certain federal mandates, such as universal access to required medical treatment.[31]		Provincial and territorial legislatures have no second chamber like the Canadian Senate. Originally, most provinces did have such bodies, known as legislative councils, with members titled councillors. These upper houses were abolished one by one, Quebec's being the last in 1968.[32] In most provinces, the single house of the legislature is known as the Legislative Assembly; the exceptions are Nova Scotia and Newfoundland and Labrador, where the chamber is called the House of Assembly, and Quebec where it is called the National Assembly.[33] Ontario has a Legislative Assembly but its members are called Members of the Provincial Parliament or MPPs.[34] The legislative assemblies use a procedure similar to that of the Canadian House of Commons. The head of government of each province, called the premier, is generally the head of the party with the most seats.[35] This is also the case in Yukon, but the Northwest Territories and Nunavut have no political parties at the territorial level.[36] The Queen's representative to each province is the Lieutenant Governor.[37] In each of the territories there is an analogous Commissioner, but he or she represents the federal government rather than the monarch.[38]		Most provinces have rough provincial counterparts to major federal parties. However, these provincial parties are not usually formally linked to the federal parties that share the same name.[39] For example, no provincial Conservative or Progressive Conservative Party shares an organizational link to the federal Conservative Party of Canada, and neither do provincial Green Parties to the Green Party of Canada. Provincial New Democratic Parties, on the other hand, are fully integrated with the federal New Democratic Party – meaning that provincial parties effectively operate as sections, with common membership, of the federal party. The Liberal Party of Canada shares such an organizational integration with the provincial Liberals in New Brunswick, Newfoundland and Labrador, Nova Scotia, and Prince Edward Island. Other provincial Liberal Parties are unaffiliated with their federal counterpart.[39]		Some provinces have provincial political parties with no clear federal equivalent, such as the Alberta Party, Saskatchewan Party, and Wildrose Party.		The provincial political climate of Quebec is quite different: the main split is between sovereignty, represented by the Parti Québécois and Québec solidaire, and federalism, represented primarily by the Quebec Liberal Party.[40] The Coalition Avenir Québec, meanwhile, takes an abstentionist position on the question and does not support or oppose sovereignty.		Currently, the only minority provincial/territorial government is held by the British Columbia Liberal Party after receiving 43 out of 87 seats in the 2017 general election.		The Canadian National Vimy Memorial, near Vimy, Pas-de-Calais, and the Beaumont-Hamel Newfoundland Memorial, near Beaumont-Hamel, France are ceremonially considered Canadian territory.[43] In 1922, the French government donated the land used for the Vimy Memorial "freely, and for all time, to the Government of Canada the free use of the land exempt from all taxes".[44] The site of the Somme battlefield near Beaumont-Hamel site was purchased in 1921 by the people of the Dominion of Newfoundland.[43] These sites do not, however, enjoy extraterritorial status and are thus subject to French law.		Since Confederation in 1867, there have been several proposals for new Canadian provinces and territories. The Constitution of Canada requires an amendment for the creation of a new province[45] but the creation of a new territory requires only an act of Parliament;[46] therefore, it is easier legislatively to create a territory than a province.		In late 2004, Prime Minister Paul Martin surprised some observers by expressing his personal support for all three territories gaining provincial status "eventually". He cited their importance to the country as a whole and the ongoing need to assert sovereignty in the Arctic, particularly as global warming could make that region more open to exploitation leading to more complex international waters disputes.[47]		
The youth rights movement (also known as youth liberation) seeks to grant the rights to young people that are traditionally reserved for adults, due to having reached a specific age or sufficient maturity. This is closely akin to the notion of evolving capacities within the children's rights movement, but the youth rights movement differs from the children's rights movement in that the latter places emphasis on the welfare and protection of children through the actions and decisions of adults, while the youth rights movement seeks to grant youth the liberty to make their own decisions autonomously in the ways adults are permitted to, or to lower the legal minimum ages at which such rights are acquired, such as the age of majority and the voting age.		Youth rights have increased over the last century in many countries.[1] The youth rights movement seeks to further increase youth rights, with some advocating intergenerational equity.		Youth rights are one aspect of how youth are treated in society. Other aspects include how adults see and treat youth, and how open society is to youth participation.						Of primary importance to youth rights advocates are historical perceptions of young people, which they say are oppressive and informed by paternalism, adultism and ageism in general, as well as fears of children and youth. Several of these perceptions made by society include the assumption that young people are incapable of making crucial decisions and need protecting from their tendency to act impulsively.[2] Youth rights advocates believe those perceptions inform laws throughout society, including voting age, child labor laws, right-to-work laws, curfews, drinking/smoking age, gambling age, age of consent, driving age, youth suffrage, emancipation of minors, minors and abortion, closed adoption, corporal punishment, the age of majority, and military conscription. Restrictions on young people that would be considered unacceptable if applied to adults are viewed by youth rights advocates as a form of discrimination.		There are specific sets of issues addressing the rights of youth in schools, including zero tolerance, "gulag schools", In loco parentis, and student rights in general. Homeschooling, unschooling, and alternative schools are popular youth rights issues. A long-standing effort within the youth rights movements has focused on civic engagement. There have been a number of historical campaigns to increase youth voting rights by lowering the voting age and the age of candidacy. There are also efforts to get young people elected to prominent positions in local communities, including as members of city councils and as mayors. For example, in the 2011 Raleigh mayoral election 17-year-old Seth Keel launched a campaign for Mayor despite the age requirement of 21.[3] Strategies for gaining youth rights that are frequently utilized by their advocates include developing youth programs and organizations that promote youth activism, youth participation, youth empowerment, youth voice, youth/adult partnerships, intergenerational equity and civil disobedience between young people and adults.		First emerging as a distinct movement in the 1930s, youth rights have long been concerned with civil rights and intergenerational equity. Tracing its roots to youth activists during the Great Depression, youth rights has influenced the civil rights movement, opposition to the Vietnam War, and many other movements. Since the advent of the Internet youth rights is gaining predominance again.[citation needed]		Certain youth rights advocates use the argument of fallibility against the belief that others can know what is best or worst for an individual, and criticize the children's rights movement for assuming that exterior legislators, parents, authorities and so on can know what is for a "minor"'s own good. These thinkers argue that the ability to correct what others think about one's own welfare in a falsificationist (as opposed to postmodernist) manner constitutes a non-arbitrary mental threshold at which an individual can speak for him or herself independently of exterior assumptions, as opposed to arbitrary chronological minimum ages in legislation. They also criticize the carte blanche for arbitrary definitions of "maturity" implicit in children's rights laws such as "with rising age and maturity" for being part of the problem, and suggest the absolute threshold of conceptual after-correcture to remedy it.[4]		These views are often supported by people with experience of the belief in absolutely gradual mental development being abused as an argument for "necessity" of arbitrary distinctions such as age of majority which they perceive as oppressive (either currently oppressing or having formerly oppressed them, depending on age and jurisdiction), and instead cite types of connectionism that allows for critical phenomena that encompasses the entire brain. These thinkers tend to stress that different individuals reach the critical threshold at somewhat different ages with no more than one in 365 (one in 366 in the case of leap years) chance of coinciding with a birthday, and that the relevant difference that it is acceptable to base different treatment on is only between individuals and not between jurisdictions. Generally, the importance of judging each individual by observable relevant behaviors and not by birth date is stressed by advocates of these views.[5]		Children's rights cover all the rights that belong to children, when they grow up they are granted new rights (like voting, consent, driving, etc) and duties (criminal responsibility, etc). There are different minimum limits of age at which youth are not free, independent or deemed legally competent enough to make some decisions or take certain actions. Some of the rights and responsibilities that come with age are:		After youth reach these limits they are free to vote, have sexual intercourse, buy or consume alcohol beverages or to drive cars, etc.		The "youth rights movement", also described as "youth liberation", is a nascent grass-roots movement whose aim is to fight against ageism and for the civil rights of young people – those "under the age of majority", which is 18 in most countries. It is ostensibly an effort to combat pedophobia and ephebiphobia throughout society by promoting youth voice, youth empowerment and ultimately, intergenerational equity through youth/adult partnerships.[6] Advocates of youth rights distinguish their movement from the children's rights movement, which they argue advocates changes that are often restrictive towards children and youth, and which they accuse of paternalism, pedophobia, and adultism.[citation needed] They point out distinctions between 1970s youth liberation literature and child rights literature from groups such as the Children's Defense Fund.[7]		International Youth Rights (IYR) is a student-run youth rights organization in China, with regional chapters across the country and abroad. Its aim is to make voices of youth be heard across the world and give opportunities for youths to carry out their own creative solutions to world issues in real life.		The European Youth Forum (YFJ, from Youth Forum Jeunesse) is the platform of the National Youth Council and International Non-Governamental Youth Organisations in Europe. It strives for youth rights in International Institutions such as the European Union, the Council of Europe and the United Nations. The European Youth Forum works in the fields of youth policy and youth work development. It focuses its work on European youth policy matters, whilst through engagement on the global level it is enhancing the capacities of its members and promoting global interdependence. In its daily work the European Youth Forum represents the views and opinions of youth organisations in all relevant policy areas and promotes the cross-sectoral nature of youth policy towards a variety of institutional actors. The principles of equality and sustainable development are mainstreamed in the work of the European Youth Forum. Other International youth rights organizations include Article 12 in Scotland and K.R.A.T.Z.A. in Germany.		In Poland, Palikot's Movement campaigns for lowering the voting age to 16.		The European Youth Portal is the starting place for the European Union's youth policy, with Erasmus+ as one of its key initiatives.		The National Youth Rights Association is the primary youth rights organization for the youths in the United States, with local chapters across the country. The organization known as Americans for a Society Free from Age Restrictions is also an important organization. The Freechild Project has gained a reputation for interjecting youth rights issues into organizations historically focused on youth development and youth service through their consulting and training activities. The Global Youth Action Network engages young people around the world in advocating for youth rights, and Peacefire provides technology-specific support for youth rights activists. Choose Responsibility and their successor organization, the Amethyst Initiative, founded by Dr. John McCardell, Jr., exist to promote the discussion of the drinking age, specifically. Choose Responsibility focuses on promoting a legal drinking age of 18, but includes provisions such as education and licensing. The Amethyst Initiative, a collaboration of college presidents and other educators, focuses on discussion and examination of the drinking age, with specific attention paid to the culture of alcohol as it exists on college campuses and the negative impact of the drinking age on alcohol education and responsible drinking.		Youth rights, as a philosophy and as a movement, has been informed and is led by a variety of individuals and institutions across the United States and around the world. In the 1960s and 70s John Holt, Richard Farson, Paul Goodman and Neil Postman were regarded authors who spoke out about youth rights throughout society, including education, government, social services and popular citizenship. Shulamith Firestone also wrote about youth rights issues in the second-wave feminist classic "The Dialectic of Sex." Alex Koroknay-Palicz has become a vocal youth rights proponent, making regular appearances on television and in newspapers. Mike A. Males is a prominent sociologist and researcher who has published several books regarding the rights of young people across the United States. Robert Epstein is another prominent author who has called for greater rights and responsibilities for youth. Several organizational leaders, including Sarah Fitz-Claridge of Taking Children Seriously, Bennett Haselton of Peacefire and Adam Fletcher (activist) of The Freechild Project conduct local, national, and international outreach for youth and adults regarding youth rights.		Those opposed to the youth rights movement often argue that youth lack a necessary amount of life experience to make sensible decisions for themselves, that young people's physical brain anatomy renders them impulsive or immature, that young people have to be restricted in order to prevent them from being exploited by adults with more power and life experience, and that young people are given the privileges of free education and parental support in order to grow into healthy adults, but that these special privileges come with strings attached - in particular, that the obligation of parents to provide care and discipline for their children must carry with it the ability to make decisions on behalf of their children.		
A vocational school, sometimes called a trade school or vocational college, is a type of educational institution, which, depending on country, may refer to secondary or post-secondary education designed to provide vocational education, or technical skills required to perform the tasks of a particular and specific job. In the case of secondary education, these schools differ from academic high schools which usually prepare students who aim to pursue tertiary education, rather than enter directly into the workforce. With regard to post-secondary education, vocational schools are traditionally distinguished from four-year colleges by their focus on job-specific training to students who are typically bound for one of the skilled trades, rather than providing academic training for students pursuing careers in a professional discipline. While many schools have largely adhered to this convention, the purely vocational focus of other trade schools began to shift in the 1990s "toward a broader preparation that develops the academic" as well as technical skills of their students. [1]						Vocational schools were called "technical colleges" in Australia, and there were more than 20 schools specializing in vocational educational training (VET). Only four technical colleges remain, and these are now referred to as "trade colleges". At these colleges, students complete a modified year 12 certificate and commence a school-based apprenticeship in a trade of their choice. There are two trade colleges in Queensland; Brisbane, the Gold Coast, Australian Industry Trade College and one in Adelaide, Vergalargona rico, Technical College, and another in Perth, Australian Trades College.		In Queensland, students can also undertake VET at private and public high schools instead of studying for their overall position (OP), which is a tertiary entrance score. However these students usually undertake more limited vocational education of one day per week whereas in the trade colleges the training is longer.		Vocational schools are sometimes called "colleges" in Canada. However, a college may also refer to an institution that offers part of a university degree, or credits that may be transferred to a university.		In Ontario, secondary schools were separated into three streams: technical schools, commercial or business schools, and collegiates (the academic schools). Those schools still exist; however, the curriculum has changed that no matter which type of school one attends, they can still attend any post-secondary institution and still study a variety of subjects and others (either academic or practical).		In Ontario, the Ministry of Training, Colleges and Universities have divided postsecondary education into universities, community colleges and private career colleges.		In the Province of Quebec, there are some vocational programs offered at institutions called CEGEPs (collège d'enseignement général et professionnel), but these too may function as an introduction to university. Generally, students complete two years at a CEGEP directly out of high school, and then complete three years at a university (rather than the usual four), to earn an undergraduate degree. Alternatively, some CEGEPs offer vocational training, but it is more likely that vocational training will be found at institutions separate from the academic institutions, though they may still be called colleges.		In Central and Eastern Europe, a vocational education is represented in forms of (professional) vocational technical schools often abbreviated as PTU and technical colleges (technikum).		Vocational school or Vocational college ([vyshche] uchylyshche - study school) is considered a post-secondary education type school, but combines coursework of a high school and junior college stretching for six years. In Ukraine, the term is used mostly for sports schools sometimes interchangeably with the term college. Such college could be a separate entity or a branch of bigger university. Successful graduates receive a specialist degree.		PTUs are usually a preparatory vocational education and are equivalent to the general education of the third degree in the former Soviet education, providing a lower level of vocational education (apprenticeship). It could be compared to a trade high school. In the 1920-30s, such PTUs were called schools of factory and plant apprenticeship, and later 1940s - vocational schools. Sometime after 1959, the name PTU was established, however, with the reorganization of the Soviet educational system these vocational schools renamed into lyceums. There were several types of PTUs such as middle city PTU and rural PTU.		Technical college (technicum) is becoming an obsolete term for a college in different parts of Central and Eastern Europe. Technicums provided a middle level of vocational education. Aside of technicums and PTU there also were vocational schools (Russian: Профессиональные училища) that also provided a middle level of vocational education. In 1920-30s Ukraine, technicums were a (technical) vocational institutes, however, during the 1930-32s Soviet educational reform they were degraded in their accreditation.		Institutes were considered a higher level of education; however, unlike universities, they were more oriented to a particular trade. With the reorganization of the Soviet education system, most institutes have been renamed as technical universities.		The Finnish system is divided between vocational and academic paths. Currently about 47 percent of Finnish students at age 16 go to vocational school. The vocational school is a secondary school for ages 16–21, and prepares the students for entering the workforce. The curriculum includes little academic general education, while the practical skills of each trade are stressed. The education is divided into eight main categories with a total of about 50 trades. The basic categories of education are		In addition to these categories administered by the Ministry of Education, the Ministry of Interior provides vocational education in the security and rescue branch for policemen, prison guards and firefighters.		The vocational schools are usually owned by the municipalities, but in special cases, private or state vocational schools exist. The state grants aid to all vocational schools on the same basis, regardless of the owner. On the other hand, the vocational schools are not allowed to operate for profit. The Ministry of Education issues licences to provide vocational education. In the licence, the municipality or a private entity is given permission to train a yearly quota of students for specific trades. The licence also specifies the area where the school must be located and the languages used in the education.		The vocational school students are selected by the schools on the basis of criteria set by the Ministry of Education. The basic qualification for the study is completed nine-year comprehensive school. Anyone may seek admission in any vocational school regardless of their domicile. In certain trades, bad health or invalidity may be acceptable grounds for refusing admission. The students do not pay tuition and they must be provided with health care and a free daily school lunch. However, the students must pay for the books, although the tools and practice material are provided to the students for free.		In tertiary education, there are higher vocational schools (ammattikorkeakoulu which is translated to "polytechnic" or "university of applied sciences"), which give three- to four-year degrees in more involved fields, like engineering (see insinööri (amk)) and nursing.		In contrast to the vocational school, an academically orientated upper secondary school, or senior high school (Finnish: lukio) teaches no vocational skills. It prepares students for entering the university or a higher vocational school.		A vocational school in Ireland is a type of secondary education school which places a large emphasis on vocational and technical education; this led to some conflict in the 1960s when the Regional Technical College system was in development. Since 2013 the schools have been managed by Education and Training Boards, which replaced Vocational Education Committees which were largely based on city or county boundaries. Establishment of the schools is largely provided by the state; funding is through block grant system providing about 90% of necessary funding requirements.		Vocational schools typically have further education courses in addition to the traditional courses at secondary level. For instance, post leaving certificate courses which are intended for school leavers and pre-third level education students.		Until the 1970s the vocational schools were seen as inferior to the other schools then available in Ireland. This was mainly because traditional courses such as the leaving certificate were not available at the schools, however this changed with the Investment in Education (1962) report which resulted in an upgrade in their status. Currently about 25% of secondary education students attend these schools.		In Japan vocational schools are known as senmon gakkō (専門学校). They are a part of Japan's higher education system. There are two-year schools that many students study at after finishing high school (although it is not always required that students graduate from high school). Some have a wide range of majors, others only a few majors. Some examples are computer technology, fashion and English.		In the Middle Ages boys learned a vocation through an apprenticeship. They were usually 10 years old when they entered service, and were first called leerling (apprentice), then gezel (journeyman) and after an exam - sometimes with an example of workmanship called a meesterproef (masterpiece) - they were called meester (master craftsman). In 1795, all of the guilds in the Netherlands were disbanded by Napoleon, and with them the guild vocational schooling system. After the French occupation, in the 1820s, the need for quality education caused more and more cities to form day and evening schools for various trades. In 1854, the society Maatschappij tot verbetering van den werkenden stand (society to improve the working class) was founded in Amsterdam, that changed its name in 1861 to the Maatschappij voor de Werkende Stand (Society for the working class). This society started the first public vocational school (De Ambachtsschool) in Amsterdam, and many cities followed. At first only for boys, later the Huishoudschool (housekeeping) was introduced as vocational schooling for girls. Housekeeping education began in 1888 with the Haagsche Kookschool in The Hague.		In 1968 the law called the Mammoetwet changed all of this, effectively dissolving the Ambachtsschool and the Huishoudschool. The name was changed to LTS (lagere technische school, lower technical school), where mainly boys went because of its technical nature, and the other option, where most girls went, was LBO (lager beroepsonderwijs, lower vocational education). In 1992 both LTS and LBO changed to VBO (voorbereidend beroepsonderwijs, preparatory vocational education) and since 1999 VBO together with MAVO (middelbaar algemeen voortgezet onderwijs, intermediate general secondary education) changed to the current VMBO (voorbereidend middelbaar beroepsonderwijs, preparatory intermediate vocational education).		In the United States, there is a very large difference between career college and vocational college. The term career college is generally reserved for post-secondary for-profit institutions. Conversely, vocational schools are government-owned or at least government-supported institutions, occupy two full years of study, and their credits are by and large accepted elsewhere in the academic world and in some instances such as charter academies or magnet schools may take the place of the final years of high school.		Career colleges on the other hand are generally not government supported in any capacity, occupy periods of study less than a year, and their training and certifications are rarely recognized by the larger academic world. In addition, as most career colleges are private schools, this group may be further subdivided into non-profit schools and proprietary schools, operated for the sole economic benefit of their owners.		As a result of this emphasis on the commercialization of education, a widespread poor reputation for quality was retained by a great number of career colleges for over promising what the job prospects for their graduates would actually be in their field of study upon completion of their program, and for emphasizing the number of careers from which a student could choose.		Even though the popularity of career colleges has exploded in recent years, the number of government-sponsored vocational schools in the United States has decreased significantly.[citation needed].		The Association for Career and Technical Education (ACTE) is the largest American national education association dedicated to the advancement of career and technical education or vocational education that prepares youth and adults for careers.		Earlier vocational schools such as California Institute of Technology[2] and Carnegie Mellon University have gone on to become full degree-granting institutions.		
Underclassman is a 2005 action comedy film directed by Marcos Siega, and stars Nick Cannon, Shawn Ashmore, Roselyn Sánchez, Kelly Hu, Hugh Bonneville, and Cheech Marin. It was released on September 2, 2005, had been originally set for a release in 2004.						Tracy (Trey) Stokes (Cannon) is a 23-year-old immature bike cop working for the Los Angeles Police Department. Stokes is known for creating lots of problems for his superiors and instead of doing his assigned duties, he tries to do detective work, which always results in failure. While playing basketball with Captain Delgado, Delgado is approached by someone in the Mayor's office looking to put an undercover detective into the affluent and prestigious Westbury School after a student was killed at a party. Stokes pushes himself into the conversation and volunteers to go undercover. With much hesitation from Delgado, the Mayor's representative states that Stokes looks the part and is assigned to the undercover operation. When Stokes arrives at the school, he learns that he has been placed in the same classes as Rob Donovan (Ashmore). Stokes tries to befriend Rob and his group but they dislike Stokes. After Stokes arrives at the "Blacktop Battle" (a basketball game between two elite schools) and fills in for an injured player, helping them win the game and impressing Rob, they slowly take a shine to Stokes. Stokes later finds out at every high school party thrown, it seems as though a luxury car is stolen. Stokes cooks up a plan to throw his own party (at Captain Delgado's home) where Rob and his friends get two girls to distract Stokes in order to steal Captain Delgado's car. Stokes soon figures out that Donovan has been stealing the cars at the parties. Near the end, it is revealed that the headmaster (Bonneville) of the school had been blackmailing Donovan, thus he is the ringmaster of the crimes. After being kicked off the force by his father-figure police chief (Marin), Trey solves the case (with the help from Donovan) and is reinstated.		Underclassman received negative reviews from critics. On Rotten Tomatoes, only 6% of the critics gave it positive write-ups with the consensus: "Despite the appealing presence of Nick Cannon, Underclassman is a shopworn knockoff of both Beverly Hills Cop and 21 Jump Street."[2]		The movie did terribly at the box office, earning $5,655,459 against a budget of $25 million.				
Tertiary education, also referred to as third stage, third level, and post-secondary education, is the educational level following the completion of a school providing a secondary education. The World Bank, for example, defines tertiary education as including universities as well as institutions that teach specific capacities of higher learning such as colleges, technical training institutes, community colleges, nursing schools, research laboratories, centers of excellence, and distance learning centers.[1] Higher education is taken to include undergraduate and postgraduate education, while vocational education and training beyond secondary education is known as further education in the United Kingdom, or continuing education in the United States.		Tertiary education generally culminates in the receipt of certificates, diplomas, or academic degrees.						"Tertiary education" includes further education (FE), as well as higher education (HE). Since the 1970s specialized FE colleges called “tertiary colleges” have been set up to offer courses such as A Levels, that allow progression to HE, alongside vocational courses. An early example of this which expanded in September 1982 as part of a reorganization of education in the Halesowen area which also saw three-tier education axed after just 10 years in force.[2]		In some areas where schools do not universally offer sixth forms, tertiary colleges function as a sixth form college as well as a general FE college.		Unlike sixth form colleges, the staff join lecturers' rather than teachers' unions.		Within Australia "Tertiary Education" refers to continuing studies after a students Higher School Certificate. It also refers to any education a student receives after final compulsory schooling, which occurs at the age of 17 within Australia. Tertiary Education options include University, TAFE or private colleges.		
A Sudbury school is a type of school, usually for the K-12 age range, where students have complete responsibility for their own education, and the school is run by direct democracy in which students and staff are equals.[1] Students individually decide what to do with their time, and tend to learn as a by-product of ordinary experience rather than through coursework. There is no predetermined educational syllabus, prescriptive curriculum or standardized instruction. This is a form of democratic education. Daniel Greenberg, one of the founders of the original Sudbury Model school, writes that the two things that distinguish a Sudbury Model school are that everyone - adults and children - are treated equally and that there is no authority other than that granted by the consent of the governed.[2]		While each Sudbury Model school operates independently and determines their own policies and procedures, they share a common culture.[3] The intended culture within a Sudbury school has been described with such words as freedom, trust, respect, responsibility and democracy.		The name 'Sudbury' originates from the Sudbury Valley School, founded in 1968 in Framingham, Massachusetts, near Sudbury, Massachusetts. Though there is no formal or regulated definition of a Sudbury Model school, there are now more than 60 schools that identify themselves with Sudbury around the world. Some, though not all, include "Sudbury" in their name. These schools operate as independent entities and are not formally associated in any way.[1]						Sudbury schools are based on:[4]		"The fundamental premises of the school are simple: that all people are curious by nature; that the most efficient, long-lasting, and profound learning takes place when started and pursued by the learner; that all people are creative if they are allowed to develop their unique talents; that age-mixing among students promotes growth in all members of the group; and that freedom is essential to the development of personal responsibility."[5]		All aspects of governing a Sudbury School are determined by the weekly School Meeting, modeled after the traditional New England town meeting.[6] School Meeting passes, amends and repeals school rules, manages the school's budget, and decides on hiring and firing of staff. Each individual present — including students and staff — has an equal vote, and most decisions are made by simple majority[1][7]		School rules are normally compiled in a law book, updated repeatedly over time, which forms the school's code of law. Usually, there is a set procedure to handle complaints, and most of the schools follow guidelines that respect the idea of due process of law. There are usually rules requiring an investigation, a hearing, a trial, a sentence, and allowing for an appeal,[8] generally following the philosophy that students face the consequences of their own behavior.[9]		The Sudbury pedagogical philosophy may be summarised thus: Learning is a natural by-product of all human activity.[10] Learning is self-initiated and self-motivated.[11]		There are many ways to learn. Learning is a process you do, not a process that is done to you;[12] The presence and guidance of a teacher are not necessary.		The free exchange of ideas and free conversation and interplay between people provides broad exposure to areas that may prove relevant and interesting to students. Students of all ages mix; older students learn from younger students as well as vice versa. The presence of older students provides role models, both positive and negative, for younger students. The pervasiveness of play has led to a recurring observation by first-time visitors to a Sudbury school that the students appear to be in perpetual "recess".[10][13]		Implicitly and explicitly, students are given responsibility for their own education: The only person designing what a student will learn is the student. Exceptions are when a student asks for a particular class or arranges an apprenticeship. Sudbury schools do not compare or rank students — the school requires no tests, evaluations, or transcripts.		Reading is treated the same as any other subject: Students learn to read when they choose, or simply by going about their lives.[14]		"Only a few kids seek any help at all when they decide to learn. Each child seems to have their own method. Some learn from being read to, memorizing the stories and then ultimately reading them. Some learn from cereal boxes, others from game instructions, others from street signs. Some teach themselves letter sounds, others syllables, others whole words. To be honest, we rarely know how they do it, and they rarely tell us."[15]		Sudbury Valley School claims that all of their students have learned to read. While students learn to read at a wide variety of ages, there appears to be no drawback to learning to read later: No one who meets their older students could guess the age at which they first learned to read.[15][16]		The model differs in some ways from other types of democratic schools and free schools, but there are many similarities:		
Minimally invasive education (MIE) is a form of learning in which children operate in unsupervised environments. The methodology arose from an experiment done by Sugata Mitra while at NIIT in 1999, often called The Hole in the Wall,[1][2] which has since gone on to become a significant project with the formation of Hole in the Wall Education Limited (HiWEL), a cooperative effort between NIIT and the International Finance Corporation, employed in some 300 'learning stations', covering some 300,000 children in India and several African countries.		The programme has been feted with the digital opportunity award by WITSA,[2] and been extensively covered in the media.						Professor Mitra, Chief Scientist at NIIT, is credited with proposing and initiating the Hole-in-the-Wall programme. As early as 1982, he had been toying with the idea of unsupervised learning and computers. Finally, in 1999, he decided to test his ideas in the field.		On 26 January 1999, Mitra's team carved a "hole in the wall" that separated the NIIT premises from the adjoining slum in Kalkaji, New Delhi. Through this hole, a freely accessible computer was put up for use. This computer proved to be popular among the slum children. With no prior experience, the children learned to use the computer on their own. This prompted Mitra to propose the following hypothesis:[3] The acquisition of basic computing skills by any set of children can be achieved through incidental learning provided the learners are given access to a suitable computing facility, with entertaining and motivating content and some minimal (human) guidance.		In the following comment on the TED website Mitra explains how they saw to it that the computer in this experiment was accessible to children only:		Mitra has summarised the results of his experiment as follows. Given free and public access to computers and the Internet, a group of children can		The first adopter of the idea was the Government of National Capital Territory of Delhi. In 2000, the Government of Delhi set up 30 Learning Stations in a resettlement colony. This project is ongoing and said to be achieving significant results.		Encouraged by the initial success of the Kalkaji experiment, freely accessible computers were set up in Shivpuri (a town in Madhya Pradesh) and in Madantusi (a village in Uttar Pradesh). These experiments came to be known as Hole-in-the-Wall experiments. The findings from Shivpuri and Madantusi confirmed the results of Kalkaji experiments. It appeared that the children in these two places picked up computer skills on their own. Dr. Mitra defined this as a new way of learning "Minimally Invasive Education".		At this point in time, International Finance Corporation joined hands with NIIT to set up Hole-in-the-Wall Education Ltd (HiWEL). The idea was to broaden the scope of the experiments and conduct research to prove and streamline Hole-in-the-Wall. The results,[6] show that children learn to operate as well as play with the computer with minimum intervention. They picked up skills and tasks by constructing their own learning environment.		Today, more than 300,000 children have benefited from 300 Hole-in-the-Wall stations over last 8 years. In India Suhotra Banerjee (Head-Government Relations) has increased the reach of HiWEL learning stations in Nagaland, Jharkhand, Andhra Pradesh... and is slowly expanding their numbers.[7]		Besides India, HiWEL also has projects abroad. The first such project was established in Cambodia in 2004. The project currently operates in Botswana, Mozambique, Nigeria, Rwanda, Swaziland, Uganda, and Zambia, besides Cambodia.[7] The idea, also called Open learning, is even being applied in Britain, albeit inside the classroom.[8]		Hole-in-the-Wall Education Ltd. (HiWEL) is a joint venture between NIIT and the International Finance Corporation. Established in 2001, HiWEL was set up to research and propagate the idea of Hole-in-the-Wall, a path-breaking learning methodology created by Mitra, Chief Scientist of NIIT.[2]		The project has received extensive coverage from sources as diverse as UNESCO,[9] Business Week,[10] CNN, Reuters,[7] and The Christian Science Monitor,[11] besides being featured at the annual TED conference in 2007.		The project received international publicity, when it was found that it was the inspiration behind the book Q & A, itself the inspiration for the Academy Award winning film Slumdog Millionaire.[7]		HiWEL has been covered by the Indian Reader's Digest.[12]		Minimally Invasive Education in school adduces there are many reasons why children may have difficulty learning, especially when the learning is imposed and the subject is something the student is not interested in, a frequent occurrence in modern schools. Schools also label children as "learning disabled" and place them in special education even if the child does not have a learning disability, because the schools have failed to teach the children basic skills.[13]		Minimally Invasive Education in school asserts there are many ways to study and learn. It argues that learning is a process you do, not a process that is done to you.[14] The experience of schools holding this approach shows that there are many ways to learn without the intervention of teaching, to say, without the intervention of a teacher being imperative. In the case of reading for instance in these schools some children learn from being read to, memorizing the stories and then ultimately reading them. Others learn from cereal boxes, others from games instructions, others from street signs. Some teach themselves letter sounds, others syllables, others whole words. They adduce that in their schools no one child has ever been forced, pushed, urged, cajoled, or bribed into learning how to read or write, and they have had no dyslexia. None of their graduates are real or functional illiterates, and no one who meets their older students could ever guess the age at which they first learned to read or write.[15]		In a similar form students learn all the subjects, techniques and skills in these schools. Every person, children and youth included, has a different learning style and pace and each person, is unique, not only capable of learning but also capable of succeeding. These schools assert that applying the medical model of problem-solving to individual children who are pupils in the school system, and labeling these children as disabled—referring to a whole generation of non-standard children that have been labeled as dysfunctional, even though they suffer from nothing more than the disease of responding differently in the classroom than the average manageable student—systematically prevents the students' success and the improvement of the current educational system, thus requiring the prevention of academic failure through intervention. This, they clarify, does not refer to people who have a specific disability that affects their drives; nor is anything they say and write about education meant to apply to people who have specific mental impairments, which may need to be dealt with in special, clinical ways.		Describing current instructional methods as homogenization and lockstep standardization, alternative approaches are proposed, such as the Sudbury model schools, an alternative approach in which children, by enjoying personal freedom thus encouraged to exercise personal responsibility for their actions, learn at their own pace rather than following a chronologically-based curriculum.[16][17][18][19] These schools are organized to allow freedom from adult interference in the daily lives of students. As long as children do no harm to others, they can do whatever they want with their time in school. The adults in other schools plan a curriculum of study, teach the students the material and then test and grade their learning. The adults at Sudbury schools are "the guardians of the children's freedom to pursue their own interests and to learn what they wish," creating and maintaining a nurturing environment, in which children feel that they are cared for, and that does not rob children of their time to explore and discover their inner selves. They also are there to answer questions and to impart specific skills or knowledge when asked to by students.[20][21] As Sudbury schools, proponents of unschooling have also claimed that children raised in this method do not suffer from learning disabilities, thus not requiring the prevention of academic failure through intervention.				
In the United States, a sophomore (/ˈsɒfəmɔːr/ or /ˈsɒfmɔːr/)[1][2] is a student in the second year of study at high school or college.						The 10th grade is the second year of a student's high school period but is referred to as sophomore year.[3][4] The term is derived from Greek σόφισμα (sophisma), 'acquired skill, clever device, method',[5] altered to resemble a compound of the Greek words σοφός (sophos), 'wise' and μωρός (moros), 'foolish, dull'.[6][7] In How to Read a Book, the Aristotelean philosopher and founder of the "Great Books of the Western World" program Mortimer Adler says, "There have always been literate ignoramuses, who have read too widely, and not well. The Greeks had a name for such a mixture of learning and folly which might be applied to the bookish but poorly read of all ages. They are all 'sophomores.'"[8] High school sophomores are expected to begin preparing for the college application process, including increasing and focusing their extracurricular activities. Students at this level are also considered to be developing greater ability for abstract thinking.[9]		In the United States, college sophomores are advised to begin thinking of career options and to get involved in volunteering or social organisations on or near campus.[10]		
Freedom of speech is the right to articulate one's opinions and ideas without fear of government retaliation or censorship, or societal sanction.[1][2][3][4] The term freedom of expression is sometimes used synonymously, but includes any act of seeking, receiving and imparting information or ideas, regardless of the medium used.		The right to freedom of expression is recognized as a human right under article 19 of the Universal Declaration of Human Rights and recognized in international human rights law in the International Covenant on Civil and Political Rights (ICCPR). Article 19 of the UDHR states that "everyone shall have the right to hold opinions without interference" and "everyone shall have the right to freedom of expression; this right shall include freedom to seek, receive and impart information and ideas of all kinds, regardless of frontiers, either orally, in writing or in print, in the form of art, or through any other media of his choice". The version of Article 19 in the ICCPR later amends this by stating that the exercise of these rights carries "special duties and responsibilities" and may "therefore be subject to certain restrictions" when necessary "[f]or respect of the rights or reputation of others" or "[f]or the protection of national security or of public order (order public), or of public health or morals".[5] Therefore, freedom of speech and expression may not be recognized as being absolute, and common limitations to freedom of speech relate to libel, slander, obscenity, pornography, sedition, incitement, fighting words, classified information, copyright violation, trade secrets, non-disclosure agreements, the right to privacy, the right to be forgotten, public security, and perjury. Justifications for such include the harm principle, proposed by John Stuart Mill in On Liberty, which suggests that: "the only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others."[6] The idea of the "offense principle" is also used in the justification of speech limitations, describing the restriction on forms of expression deemed offensive to society, considering factors such as extent, duration, motives of the speaker, and ease with which it could be avoided.[6] With the evolution of the digital age, application of the freedom of speech becomes more controversial as new means of communication and restrictions arise, for example the Golden Shield Project, an initiative by Chinese government's Ministry of Public Security that filters potentially unfavorable data from foreign countries.		The right to freedom of expression has been interpreted to include the right to take and publish photographs of strangers in public areas without their permission or knowledge.[7][8] However, according to a legal case in the Netherlands, the right to freedom of expression does not include the right to use a photograph in a racist manner to incite racial hatred or ethnic discrimination if the photograph was taken without the knowledge of the subject.[9]						Freedom of speech and expression has a long history that predates modern international human rights instruments.[10] It is thought that ancient Athens' democratic ideology of free speech may have emerged in the late 6th or early 5th century BC.[11] The values of the Roman Republic included freedom of speech and freedom of religion.[12]		Concepts of freedom of speech can be found in early human rights documents.[10] England's Bill of Rights 1689 legally established the constitutional right of 'freedom of speech in Parliament' which is still in effect.[13] The Declaration of the Rights of Man and of the Citizen, adopted during the French Revolution in 1789, specifically affirmed freedom of speech as an inalienable right.[10] The Declaration provides for freedom of expression in Article 11, which states that:		The free communication of ideas and opinions is one of the most precious of the rights of man. Every citizen may, accordingly, speak, write, and print with freedom, but shall be responsible for such abuses of this freedom as shall be defined by law.[14]		Article 19 of the Universal Declaration of Human Rights, adopted in 1948, states that:		Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.[15]		Today, freedom of speech, or the freedom of expression, is recognized in international and regional human rights law. The right is enshrined in Article 19 of the International Covenant on Civil and Political Rights, Article 10 of the European Convention on Human Rights, Article 13 of the American Convention on Human Rights and Article 9 of the African Charter on Human and Peoples' Rights.[16] Based on John Milton's arguments, freedom of speech is understood as a multi-faceted right that includes not only the right to express, or disseminate, information and ideas, but three further distinct aspects:		International, regional and national standards also recognize that freedom of speech, as the freedom of expression, includes any medium, be it orally, in written, in print, through the Internet or through art forms. This means that the protection of freedom of speech as a right includes not only the content, but also the means of expression.[16]		The right to freedom of speech and expression is closely related to other rights, and may be limited when conflicting with other rights (see limitations on freedom of speech).[16] The right to freedom of expression is also related to the right to a fair trial and court proceeding which may limit access to the search for information, or determine the opportunity and means in which freedom of expression is manifested within court proceedings.[17] As a general principle freedom of expression may not limit the right to privacy, as well as the honor and reputation of others. However greater latitude is given when criticism of public figures is involved.[17]		The right to freedom of expression is particularly important for media, which plays a special role as the bearer of the general right to freedom of expression for all.[16] However, freedom of the press is not necessarily enabling freedom of speech. Judith Lichtenberg has outlined conditions in which freedom of the press may constrain freedom of speech, for example where the media suppresses information or stifles the diversity of voices inherent in freedom of speech. Lichtenberg argues that freedom of the press is simply a form of property right summed up by the principle "no money, no voice".[18]		Freedom of speech is understood to be fundamental in a democracy. The norms on limiting freedom of expression mean that public debate may not be completely suppressed even in times of emergency.[17] One of the most notable proponents of the link between freedom of speech and democracy is Alexander Meiklejohn. He argues that the concept of democracy is that of self-government by the people. For such a system to work an informed electorate is necessary. In order to be appropriately knowledgeable, there must be no constraints on the free flow of information and ideas. According to Meiklejohn, democracy will not be true to its essential ideal if those in power are able to manipulate the electorate by withholding information and stifling criticism. Meiklejohn acknowledges that the desire to manipulate opinion can stem from the motive of seeking to benefit society. However, he argues, choosing manipulation negates, in its means, the democratic ideal.[19]		Eric Barendt has called this defense of free speech on the grounds of democracy "probably the most attractive and certainly the most fashionable free speech theory in modern Western democracies".[20] Thomas I. Emerson expanded on this defense when he argued that freedom of speech helps to provide a balance between stability and change. Freedom of speech acts as a "safety valve" to let off steam when people might otherwise be bent on revolution. He argues that "The principle of open discussion is a method of achieving a more adaptable and at the same time more stable community, of maintaining the precarious balance between healthy cleavage and necessary consensus." Emerson furthermore maintains that "Opposition serves a vital social function in offsetting or ameliorating (the) normal process of bureaucratic decay."[21]		Research undertaken by the Worldwide Governance Indicators project at the World Bank, indicates that freedom of speech, and the process of accountability that follows it, have a significant impact in the quality of governance of a country. "Voice and Accountability" within a country, defined as "the extent to which a country's citizens are able to participate in selecting their government, as well as freedom of expression, freedom of association, and free media" is one of the six dimensions of governance that the Worldwide Governance Indicators measure for more than 200 countries.[22] Against this backdrop it is important that development agencies create grounds for effective support for a free press in developing countries.[23]		Richard Moon has developed the argument that the value of freedom of speech and freedom of expression lies with social interactions. Moon writes that "by communicating an individual forms relationships and associations with others – family, friends, co-workers, church congregation, and countrymen. By entering into discussion with others an individual participates in the development of knowledge and in the direction of the community."[24]		University campuses have historically been bastions of free speech. UC Berkeley was the birthplace of the Free Speech Movement.		Some organizations believe that universities have increasingly restricted free speech on campuses.[25] Young Americans for Liberty is one such organization.[26]		University of California, Los Angeles Chancellor Gene Block issued a statement concerning both the value of free speech and the responsibility for civil discourse. The statement was in favor of an environment in which people coming from different beliefs and backgrounds may engage in passionate dialogue without belittling one another. In Block's view, "just because speech is constitutionally protected doesn’t mean that it is wise, fair or productive."[27]		Legal systems sometimes recognize certain limits on the freedom of speech, particularly when freedom of speech conflicts with other rights and freedoms, such as in the cases of libel, slander, pornography, obscenity, fighting words, and intellectual property. Justifications for limitations to freedom of speech often reference the "harm principle" or the "offense principle". Limitations to freedom of speech may occur through legal sanction or social disapprobation, or both.[28] Certain public institutions may also enact policies restricting the freedom of speech, for example speech codes at state schools.		In On Liberty (1859), John Stuart Mill argued that "...there ought to exist the fullest liberty of professing and discussing, as a matter of ethical conviction, any doctrine, however immoral it may be considered."[28] Mill argues that the fullest liberty of expression is required to push arguments to their logical limits, rather than the limits of social embarrassment. However, Mill also introduced what is known as the harm principle, in placing the following limitation on free expression: "the only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others."[28]		In 1985, Joel Feinberg introduced what is known as the "offense principle", arguing that Mill's harm principle does not provide sufficient protection against the wrongful behaviors of others. Feinberg wrote "It is always a good reason in support of a proposed criminal prohibition that it would probably be an effective way of preventing serious offense (as opposed to injury or harm) to persons other than the actor, and that it is probably a necessary means to that end."[30] Hence Feinberg argues that the harm principle sets the bar too high and that some forms of expression can be legitimately prohibited by law because they are very offensive. But, as offending someone is less serious than harming someone, the penalties imposed should be higher for causing harm.[30] In contrast, Mill does not support legal penalties unless they are based on the harm principle.[28] Because the degree to which people may take offense varies, or may be the result of unjustified prejudice, Feinberg suggests that a number of factors need to be taken into account when applying the offense principle, including: the extent, duration and social value of the speech, the ease with which it can be avoided, the motives of the speaker, the number of people offended, the intensity of the offense, and the general interest of the community at large.[28]		Along similar lines as Mill, Jasper Doomen has argued that harm should be defined from the point of view of the individual citizen, not limiting harm to physical harm since nonphysical harm may also be involved; Feinberg's distinction between harm and offense is criticized as largely trivial.[31]		In 1999, Bernard Harcourt wrote of the collapse of the harm principle: "Today the debate is characterized by a cacophony of competing harm arguments without any way to resolve them. There is no longer an argument within the structure of the debate to resolve the competing claims of harm. The original harm principle was never equipped determine the relative importance of harms."[32]		Interpretations of both the harm and offense limitations to freedom of speech are culturally and politically relative. For instance, in Russia, the harm and offense principles have been used to justify the Russian LGBT propaganda law restricting speech (and action) in relation to LGBT issues. A number of European countries that take pride in freedom of speech nevertheless outlaw speech that might be interpreted as Holocaust denial. These include Austria, Belgium, Czech Republic, France, Germany, Hungary, Israel, Liechtenstein, Lithuania, Luxembourg, Netherlands, Poland, Portugal, Slovakia, and Switzerland.[33]		Kurt Westergaard, a Danish cartoonist, created the controversial cartoon of the Islamic prophet Muhammad wearing a bomb in his turban and was met with strong violent reactions worldwide.[34] Similarly, Noam Chomsky, the MIT professor and vociferous critic of Israeli and US policies, has received numerous death threats.[35]		Norman Finkelstein, a writer and professor of political science expressed the opinion that Charlie Hebdo's abrasive cartoons of Muhammad exceeded the boundaries of free speech, and compared those cartoons with the cartoons of Julius Streicher,[36] who was hanged by the Allies after World War II for the words and drawings he had published. In 2006, in response to a particularly abrasive issue of Charlie Hebdo, French President Jacques Chirac condemned "overt provocations" which could inflame passions. "Anything that can hurt the convictions of someone else, in particular religious convictions, should be avoided", Chirac said.[37]		In the US, the standing landmark opinion on political speech is Brandenburg v. Ohio (1969),[38] expressly overruling Whitney v. California.[39] In Brandenburg, the US Supreme Court referred to the right even to speak openly of violent action and revolution in broad terms:		[Our] decisions have fashioned the principle that the constitutional guarantees of free speech and free press do not allow a State to forbid or proscribe advocacy of the use of force or law violation except where such advocacy is directed to inciting or producing imminent lawless action and is likely to incite or cause such action.[40]		The opinion in Brandenburg discarded the previous test of "clear and present danger" and made the right to freedom of (political) speech's protections in the United States almost absolute.[41][42] Hate speech is also protected by the First Amendment in the United States, as decided in R.A.V. v. City of St. Paul, (1992) in which the Supreme Court ruled that hate speech is permissible, except in the case of imminent violence.[43] See the First Amendment to the United States Constitution for more detailed information on this decision and its historical background.		Jo Glanville, editor of the Index on Censorship, states that "the Internet has been a revolution for censorship as much as for free speech".[44] International, national and regional standards recognise that freedom of speech, as one form of freedom of expression, applies to any medium, including the Internet.[16] The Communications Decency Act (CDA) of 1996 was the first major attempt by the United States Congress to regulate pornographic material on the Internet. In 1997, in the landmark cyberlaw case of Reno v. ACLU, the US Supreme Court partially overturned the law.[45] Judge Stewart R. Dalzell, one of the three federal judges who in June 1996 declared parts of the CDA unconstitutional, in his opinion stated the following:[46]		The Internet is a far more speech-enhancing medium than print, the village green, or the mails. Because it would necessarily affect the Internet itself, the CDA would necessarily reduce the speech available for adults on the medium. This is a constitutionally intolerable result. Some of the dialogue on the Internet surely tests the limits of conventional discourse. Speech on the Internet can be unfiltered, unpolished, and unconventional, even emotionally charged, sexually explicit, and vulgar – in a word, "indecent" in many communities. But we should expect such speech to occur in a medium in which citizens from all walks of life have a voice. We should also protect the autonomy that such a medium confers to ordinary people as well as media magnates.[...] My analysis does not deprive the Government of all means of protecting children from the dangers of Internet communication. The Government can continue to protect children from pornography on the Internet through vigorous enforcement of existing laws criminalizing obscenity and child pornography. [...] As we learned at the hearing, there is also a compelling need for public educations about the benefits and dangers of this new medium, and the Government can fill that role as well. In my view, our action today should only mean that Government's permissible supervision of Internet contents stops at the traditional line of unprotected speech. [...] The absence of governmental regulation of Internet content has unquestionably produced a kind of chaos, but as one of the plaintiff's experts put it with such resonance at the hearing: "What achieved success was the very chaos that the Internet is. The strength of the Internet is chaos." Just as the strength of the Internet is chaos, so that strength of our liberty depends upon the chaos and cacophony of the unfettered speech the First Amendment protects.[46]		The World Summit on the Information Society (WSIS) Declaration of Principles adopted in 2003 makes specific reference to the importance of the right to freedom of expression for the "Information Society" in stating:		We reaffirm, as an essential foundation of the Information society, and as outlined in Article 19 of the Universal Declaration of Human Rights, that everyone has the right to freedom of opinion and expression; that this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers. Communication is a fundamental social process, a basic human need and the foundation of all social organisation. It is central to the Information Society. Everyone, everywhere should have the opportunity to participate and no one should be excluded from the benefits of the Information Society offers.[47]		According to Bernt Hugenholtz and Lucie Guibault the public domain is under pressure from the "commodification of information" as information with previously little or no economic value has acquired independent economic value in the information age. This includes factual data, personal data, genetic information and pure ideas. The commodification of information is taking place through intellectual property law, contract law, as well as broadcasting and telecommunications law.[48]		Freedom of information is an extension of freedom of speech where the medium of expression is the Internet. Freedom of information may also refer to the right to privacy in the context of the Internet and information technology. As with the right to freedom of expression, the right to privacy is a recognised human right and freedom of information acts as an extension to this right.[49] Freedom of information may also concern censorship in an information technology context, i.e. the ability to access Web content, without censorship or restrictions.[50]		Freedom of information is also explicitly protected by acts such as the Freedom of Information and Protection of Privacy Act of Ontario, in Canada.[51]		The concept of freedom of information has emerged in response to state sponsored censorship, monitoring and surveillance of the internet. Internet censorship includes the control or suppression of the publishing or accessing of information on the Internet.[52] The Global Internet Freedom Consortium claims to remove blocks to the "free flow of information" for what they term "closed societies".[53] According to the Reporters without Borders (RWB) "internet enemy list" the following states engage in pervasive internet censorship: China, Cuba, Iran, Myanmar/Burma, North Korea, Saudi Arabia, Syria, Turkmenistan, Uzbekistan, and Vietnam.[54]		A widely publicized example of internet censorship is the "Great Firewall of China" (in reference both to its role as a network firewall and to the ancient Great Wall of China). The system blocks content by preventing IP addresses from being routed through and consists of standard firewall and proxy servers at the Internet gateways. The system also selectively engages in DNS poisoning when particular sites are requested. The government does not appear to be systematically examining Internet content, as this appears to be technically impractical.[55] Internet censorship in the People's Republic of China is conducted under a wide variety of laws and administrative regulations, including more than sixty regulations directed at the Internet. Censorship systems are vigorously implemented by provincial branches of state-owned ISPs, business companies, and organizations.[56][57]		Before the invention of the printing press a written work, once created, could only be physically multiplied by highly laborious and error-prone manual copying. No elaborate system of censorship and control over scribes existed, who until the 14th century were restricted to religious institutions, and their works rarely caused wider controversy. In response to the printing press, and the heresies it allowed to spread, the Roman Catholic Church moved to impose censorship.[59] Printing allowed for multiple exact copies of a work, leading to a more rapid and widespread circulation of ideas and information (see print culture).[60] The origins of copyright law in most European countries lie in efforts by the Roman Catholic Church and governments to regulate and control the output of printers.[60]		In 1501 Pope Alexander VI issued a Bill against the unlicensed printing of books and in 1559 the Index Expurgatorius, or List of Prohibited Books, was issued for the first time.[59] The Index Expurgatorius is the most famous and long lasting example of "bad books" catalogues issued by the Roman Catholic Church, which presumed to be in authority over private thoughts and opinions, and suppressed views that went against its doctrines. The Index Expurgatorius was administered by the Roman Inquisition, but enforced by local government authorities, and went through 300 editions. Amongst others, it banned or censored books written by René Descartes, Giordano Bruno, Galileo Galilei, David Hume, John Locke, Daniel Defoe, Jean-Jacques Rousseau and Voltaire.[61] While governments and church encouraged printing in many ways because it allowed for the dissemination of Bibles and government information, works of dissent and criticism could also circulate rapidly. As a consequence, governments established controls over printers across Europe, requiring them to have official licenses to trade and produce books.[60]		The notion that the expression of dissent or subversive views should be tolerated, not censured or punished by law, developed alongside the rise of printing and the press. Areopagitica, published in 1644, was John Milton's response to the Parliament of England's re-introduction of government licensing of printers, hence publishers.[64] Church authorities had previously ensured that Milton's essay on the right to divorce was refused a license for publication. In Areopagitica, published without a license,[65] Milton made an impassioned plea for freedom of expression and toleration of falsehood,[64] stating:		Give me the liberty to know, to utter, and to argue freely according to conscience, above all liberties.[64]		Milton's defense of freedom of expression was grounded in a Protestant worldview and he thought that the English people had the mission to work out the truth of the Reformation, which would lead to the enlightenment of all people. But Milton also articulated the main strands of future discussions about freedom of expression. By defining the scope of freedom of expression and of "harmful" speech Milton argued against the principle of pre-censorship and in favor of tolerance for a wide range of views.[64]		As the "menace" of printing spread, more governments attempted to centralize control.[66] The French crown repressed printing and the printer Etienne Dolet was burned at the stake in 1546. In 1557 the British Crown thought to stem the flow of seditious and heretical books by chartering the Stationers' Company. The right to print was limited to the members of that guild, and thirty years later the Star Chamber was chartered to curtail the "greate enormities and abuses" of "dyvers contentyous and disorderlye persons professinge the arte or mystere of pryntinge or selling of books." The right to print was restricted to two universities and to the 21 existing printers in the city of London, which had 53 printing presses. As the British crown took control of type founding in 1637 printers fled to the Netherlands. Confrontation with authority made printers radical and rebellious, with 800 authors, printers and book dealers being incarcerated in the Bastille in Paris before it was stormed in 1789.[66]		A succession of English thinkers was at the forefront of early discussion on a right to freedom of expression, among them John Milton (1608–74) and John Locke (1632–1704). Locke established the individual as the unit of value and the bearer of rights to life, liberty, property and the pursuit of happiness. However Locke's ideas evolved primarily around the concept of the right to seek salvation for one's soul, and was thus primarily concerned with theological matters. Locke neither supported a universal toleration of peoples nor freedom of speech; according to his ideas, some groups, such as atheists, should not be allowed.[67]		By the second half of the 17th century philosophers on the European continent like Baruch Spinoza and Pierre Bayle developed ideas encompassing a more universal aspect freedom of speech and toleration than the early English philosophers.[67] By the 18th century the idea of freedom of speech was being discussed by thinkers all over the Western world, especially by French philosophes like Denis Diderot, Baron d'Holbach and Claude Adrien Helvétius.[68] The idea began to be incorporated in political theory both in theory as well as practice; the first state edict in history proclaiming complete freedom of speech was the one issued December 4, 1770 in Denmark-Norway during the regency of Johann Friedrich Struensee.[69] However Struensee himself imposed some minor limitations to this edict in October 7, 1771, and it was even further limited after the fall of Struensee with legislation introduced in 1773, although censorship was not reintroduced.[70]		John Stuart Mill (1806–1873) argued that without human freedom there can be no progress in science, law or politics, which according to Mill required free discussion of opinion. Mill's On Liberty, published in 1859 became a classic defence of the right to freedom of expression.[64] Mill argued that truth drives out falsity, therefore the free expression of ideas, true or false, should not be feared. Truth is not stable or fixed, but evolves with time. Mill argued that much of what we once considered true has turned out false. Therefore, views should not be prohibited for their apparent falsity. Mill also argued that free discussion is necessary to prevent the "deep slumber of a decided opinion". Discussion would drive the onwards march of truth and by considering false views the basis of true views could be re-affirmed.[71] Furthermore, Mill argued that an opinion only carries intrinsic value to the owner of that opinion, thus silencing the expression of that opinion is an injustice to a basic human right. For Mill, the only instance in which speech can be justifiably suppressed is in order to prevent harm from a clear and direct threat. Neither economic or moral implications, nor the speakers own well-being would justify suppression of speech.[72]		In Evelyn Beatrice Hall's biography of Voltaire, she coined the following sentence to illustrate Voltaire's beliefs: "I disapprove of what you say, but I will defend to the death your right to say it."[73] Hall's quote is frequently cited to describe the principle of freedom of speech.[73] In the 20th Century, Noam Chomsky states that: "If you believe in freedom of speech, you believe in freedom of speech for views you don't like. Stalin and Hitler, for example, were dictators in favor of freedom of speech for views they liked only. If you're in favor of freedom of speech, that means you're in favor of freedom of speech precisely for views you despise."[74] Lee Bollinger argues that "the free speech principle involves a special act of carving out one area of social interaction for extraordinary self-restraint, the purpose of which is to develop and demonstrate a social capacity to control feelings evoked by a host of social encounters." Bollinger argues that tolerance is a desirable value, if not essential. However, critics argue that society should be concerned by those who directly deny or advocate, for example, genocide (see limitations above).[75]		
In United States usage, the word dormitory means a building primarily providing sleeping and residential quarters for large numbers of people, often boarding school, college or university students. In the US it is common for residents (typically two) to share a bedroom. In the US these buildings are often single sex, or sexes are accommodated on separate floors.		In United Kingdom usage, the word dormitory means a room containing several beds accommodating unrelated people.[1] In the United Kingdom, this arrangement exists typically for pupils at a boarding school, travellers or military personnel, but is almost entirely unknown for university students.		In United Kingdom usage, a building providing sleeping and residential quarters for large numbers of people is called a hall of residence (university students), house (members of a religious community or pupils at a boarding school[2]), hostel (students, workers or travellers) or barracks (military personnel). In the United Kingdom, Halls of Residence almost entirely have single occupancy rooms. In the United Kingdom, halls of residence are almost always mixed sex, with residents being allocated to adjacent rooms regardless of sex.		The word dormitory (often abbreviated to dorm) comes originally from the Latin word dormitorium.[3]		Worldwide, it is unusual for unrelated mixed sex occupancy of a bedroom except temporarily (for example in a [travel] hostel or a railway sleeping car). Where this does occur, it is so remarkable as to be newsworthy (for example the mixed sex sharing of bedrooms in the Norwegian Army [4]).						Most colleges and universities provide single or multiple occupancy rooms for their students, usually at a cost. These buildings consist of many such rooms, like an apartment building, and the number of rooms varies quite widely from just a few to hundreds. The largest dormitory building is Bancroft Hall at the United States Naval Academy.		Many colleges and universities no longer use the word "dormitory" and staff are now using the term residence hall (analogous to the United Kingdom "hall of residence") or simply "hall" instead. Outside academia however, the word "dorm" or "dormitory" is commonly used without negative connotations. Indeed, the words are used regularly in the marketplace as well as routinely in advertising. College and university residential rooms vary in size, shape, facilities and number of occupants. Typically, a United States residence hall room holds two students with no toilet. This is usually referred to as a "double". Often, residence halls have communal bathroom facilities.		In the United States, residence halls are sometimes segregated by sex, with men living in one group of rooms, and women in another. Some dormitory complexes are single-sex with varying limits on visits by persons of each sex. For example, the University of Notre Dame in Indiana has a long history of Parietals, or mixed visiting hours. Most colleges and universities offer coeducational dorms, where either men or women reside on separate floors but in the same building or where both sexes share a floor but with individual rooms being single-sex. In the early 2000s, dorms that allowed people of opposite sexes to share a room became available in some public universities.[5] Some colleges and university coeducational dormitories also feature coeducational bathrooms.[6]		Most residence halls are much closer to campus than comparable private housing such as apartment buildings. This convenience is a major factor in the choice of where to live since living physically closer to classrooms is often preferred, particularly for first-year students who may not be permitted to park vehicles on campus. Universities may therefore provide priority to first-year students when allocating this accommodation.		Halls located away from university facilities sometimes have extra amenities such as a recreation room or bar. As with campus located residence halls, these off-campus halls commonly also have Internet facilities, either through a network connection in each student room, a central computer cluster room, or Wi-Fi. Catered halls may charge for food by the meal or through a termly subscription. They may also contain basic kitchen facilities for student use outside catering hours. Most halls contain a laundry room. As of 2015 there was an expanding market for private luxury off-campus student residences which offered substantial amenities in both the United States[7] and Britain, particularly in London.[8]		In UK universities these buildings are usually called "halls of residence" (commonly referred to as "halls"), except at Oxford, Cambridge, Durham, York, Lancaster and Kent where the residential accommodation is incorporated in each college's complex of buildings, and simply known as "rooms". Members of the college who live in its own buildings are usually said to be "living in", or "living in college".		The majority of bedrooms in UK halls are now single occupancy - offering the first chance at privacy for some young people who shared bedrooms with siblings at home. Kitchen facilities are usually shared, as are bathrooms in some halls, though more expensive en suite rooms are available in some universities.		Over the years, UK universities have been hit by considerable funding cuts as part of government austerity measures. This, in part, has led to an increase in the rental of student accommodation during the winter, spring, and summer vacation periods to house conference delegates and tourists, often at rates similar to those charged by upmarket hotels.[9] Unfortunately, this often means that students are forced to vacate their rooms up to three times per year. As a result, several student-focussed personal storage and shipping companies have come into existence that cater to this need.		At some institutes in the UK, each residence hall has its own hall council. Where they exist, such individual councils are usually part of a larger organization called, variously, Residence Hall Association, Resident Students Association, or Junior Common Room Committee which typically provides funds and oversees the individual building council. These student-led organizations are often connected at a national level by the National Association of College and University Residence Halls (NACURH). Collectively, these hall councils plan social and educational events, and voice student needs to their respective administration.		In Germany there are dormitories called "Studentenwohnheim" (plural: Studentenwohnheime). Most Studentenwohnheime are run by the Studentenwerk (an organisation providing social, financial and cultural support services to students in Germany, comparable to student unions in the UK). Some Studentenwohnheime are run by a Catholic or Protestant church. Church-run facilities are sometimes single-sex. Studentenwohnheime may be situated on or off campus. They are usually low cost and serve students with limited budget. Flats may be shared with other students or may be studio-type, with on-suite bathroom and kitchen facilities. The rooms themselves are always single occupancy.		In India the dormitories are called "PG housing"[10] or "Student Hostels". Even though most of the colleges/universities have hostels on-campus, however in most of the cases it is not enough for the total students enrolled.[11] Majority of the students prefer to stay off-campus in PGs and private hostels as they usually have better amenities and services[12]. For example in 2015 estimated 1.8 lakh students enrolled with Delhi University , there are only about 9,000 seats available in its hostels for both undergraduate and postgraduate students. The university admits an average of 54,000 students every year.[13] Which leaves a majority of students to find accommodation off-campus.[14] This is led to a lot of student hostel or student PG chains to be established near Delhi University[15].		In Spain the dormitories are called "Colegios Mayores" or "Residencias de Estudiantes". There are some being part of the local universities like RESA who builds on land of the universities and provide accommodation to their students and private ones like Melon District at Barcelona as the major residence hall in the city, Galileo Galilei at Valencia (part of the Victoria Hall Group) and the Residencia Universitaria Benito Pérez Galdós at Madrid; the three of them offering high standard services to the most demanding students.		In China, dormitories are called "宿舍" (pinyin: sùshè). Dorms for mainland Chinese students usually have four to six students of the same sex living together in one room, with buildings usually being entirely gender-segregated and sometimes intentionally placed at some distance from each other to make inappropriate fraternization between male and female students more difficult. Sleeping hours may be enforced by cutting electricity at a given time, for instance at midnight.		Chinese students from Hong Kong, Macau and Taiwan live separately in their own dorms, as do foreigners. The quality of these dorms is usually better than that of mainland student dorms, with rooms either shared between only two people or completely private for a single student. Sexual decency attitudes are laxer than in mainlander dorms, with males and females sharing the same buildings and sometimes hallways (though not rooms). Students are allowed to bring visitors—including mainlanders—of the opposite sex to their rooms, although guests are usually not allowed to stay overnight. Electricity is usually available at all hours of the day.		Most dormitories for foreigners are run by the Foreign Students' Education Office (a department providing support services to students in China). They may be on campus or off campus. They are usually low cost and serve students.		Universities in Hong Kong are modeled on the British education system, with halls consequently being similar to those in the United Kingdom.		The terms "residence hall" and "dorm" are often used interchangeably in the US. However, within the residence life community, the term "residence hall" is preferred. According to the University of Oregon, their facilities "provide not just a place to sleep, but also opportunities for personal and educational growth. Highly trained Residence Life staff and Hall Government officers support this objective by creating engaging activities and programs in each hall or complex."[16]		Michigan State University, Ohio University, the University of Wisconsin–Milwaukee, the University of Texas at Austin, the University of Copenhagen, and London are six diverse and relevant examples of notable residential campuses that each display different relevances to contemporary dormitories in higher education. Michigan State has the largest hall; Ohio possesses four residential greens built into the campus; Wisconsin-Milwaukee has four notable tower constructions to house students; Texas maintains a residence hall with several high-tech amenities; Copenhagen has one of the world's oldest residence halls; and London possesses one of the largest metropolitan living quarters for university students.		Rutgers University in New Brunswick, New Jersey has the largest residence hall system in the United States. 16,429 students live within a myriad of housing options, including apartments, suites and graduate housing. Freshmen are guaranteed on-campus housing to live on the 39,950+ student campus for at least their first year.[17] Watterson Towers at Illinois State University are among the tallest residence halls in the world. The 28-story complex, which was built in 1967 holds over 2,200 students and its buildings are 91 meters tall.		Like many national universities, Ohio University includes its residence halls as a part of its campus architecture, augmenting the dormitories within plans for large sections of the urban campus. Ohio University includes three primary quadrangle residential lawns, also known as "greens," that have dormitories surrounding the central area per each.[18] The greens, named for cardinal directions, include East Green,[19] South Green,[20] and West Green.[21] Despite the appearance of the map, Voigt Hall and Scott Quadrangle are grouped onto East Green. There are no residence halls on College Green.[22]		The Sandburg Halls at University of Wisconsin–Milwaukee consists of four high-rise towers, with the tallest being the northern most tower reaching 74 metres (243 ft) tall (building), and 146.8 metres (482 ft) (radio antenna).[23] The halls combined have a total housing capacity of 2,700 students.[24]		Dobie Center, an off-campus, 27-story private dormitory next to the University of Texas at Austin, stands at 112 metres (367 ft). In addition to being a private residence for students, Dobie also contains a small 2-story mall, restaurants, and specialty stores.		The Valkendorfs Kollegium at the University of Copenhagen was founded in 1589. Though not as old as some of the colleges of Oxford and Cambridge, it is among the oldest dormitories in the world. In Canada, student dormitories are more commonly called "residences" and students live "in residence".[25] The Stone Frigate at Royal Military College of Canada in Kingston, Ontario was constructed in 1820 to store part of the dismantled fleet from the War of 1812. The former warehouse was converted into a residence and classrooms when the college was established in 1874. The Stone Frigate, a designated heritage building, was closed for more than 18 months for major renovations to the interior and exterior of the residence. The Capstone House at University of South Carolina in Columbia, South Carolina completed in 1967, standing at 18 stories, has the only revolving restaurant on an American college campus located on the 18th floor known as Top of Carolina Dining Room.		Prodigy Living Spitalfields [26][24] in London is the world's tallest student accommodation building, standing at 105 metres (344 ft), with 33 floors.[27] It was completed in 2010 and claimed the title from the previous record holder, Sky Plaza in Leeds which stands just two metres lower.		At some institutes, each residence hall has its own hall council. Where they exist, such individual councils are usually part of a larger organization called, variously, Residence Hall Association, Resident Students Association, or Junior Common Room Committee which typically provides funds and oversees the individual building council. These student-led organizations are typically connected at a national level by the National Association of College and University Residence Halls (NACURH). Collectively, these hall councils plan social and educational events, and voice student needs to their respective administration.		In the United States, university residence halls are normally staffed by a combination of both students and professional residence life staff. Student staff members, Resident Assistants, or community advisers act as liaisons, counselors, mediators and policy enforcers. The student staff is supervised by a graduate student or a full-time residence life professional, sometimes known as the hall director. Staff members frequently arrange programming activities to help residents learn about social and academic life during their college life.		In the United Kingdom, halls often run a similar setup to that in the U.S, although the resident academic responsible for the hall is known by the term of "warden" and may be supported by a team of vice-wardens, sub-wardens or senior-members; forming the SCR (Senior Common Room). These are often students or academic staff at the relevant university/college. Many UK halls also have a JCR (Junior Common Room) committee, usually made up of second year students who stayed in that hall during their first year.		The facilities in the hall are often managed by an individual termed the Bursar. Residence Halls may have housekeeping staff to maintain the cleanliness of common rooms including lobbies, lounges, and bathrooms. Students are normally required to maintain the cleanliness of their own rooms and private or semi-private bathrooms, where offered.		Dormitories have replaced barracks at most U.S. military installations. Much new construction includes private bathrooms, but most unaccompanied housing as of 2007[update] still features bathrooms between pairs of rooms. Traditional communal shower facilities, typically one per floor, are now considered substandard and are being phased out.		U.S. military dormitory accommodations are generally intended for two junior enlisted single personnel per room, although in most cases this is slowly being phased out in favor of single occupancy in accordance with newer Department of Defense standards.		All branches of the U.S. military except the Air Force still refer to these dormitory-style accommodations as "barracks".[citation needed] The Air Force, in contrast, refers to all unaccompanied housing as "dormitories", including open-bay barracks used for basic training that house dozens per room, as well as unaccompanied housing for senior ranking personnel, which resemble apartments and are only found in a select number of overseas locations.		In the US, China, UK, Ireland and Canada, a dormitory may be a room containing more than one bed. Examples are found in British boarding schools and many rooming houses such as hostels but have nowadays completely vanished as a type of accommodation in university halls of residence. CADs, or cold-air dormitories, are found in multi-level rooming houses such as fraternities, sororities, and cooperative houses. In CADs and in hostels, the room typically has very few furnishings except for beds. Such rooms can contain anywhere from three to 50 beds (though such very large dormitories are rare except perhaps as military barracks). Such rooms provide little or no privacy for the residents, and very limited storage for personal items in or near the beds. Cold-air dorms get their names from the common practice of keeping the windows open year-round, even in winter. The practice emerged based on the theory that circulation and cold air minimizes the spread of disease. Some communal bedrooms keep the name cold-air dorms or cold dorms despite having modern heating or cooling.[28][29]		While the practice of housing employees in company-owned dormitories has dwindled, several companies continue this practice in the U.S. and other countries.		Cast members in the Disney College Program at the Walt Disney World Resort have the opportunity to meet and live with other cast members within their housing complexes in Lake Buena Vista, FL.[30] In the Netherlands, the law forbids companies to offer housing to their employees, because the government wants to prevent people who have just lost their job adding to their stressful situation by having to search for new housing. In Japan, many of the larger companies as well as some of the ministries still offer to their newly graduated freshmen a room in a dormitory. A room in such a dormitory often comes with a communal cook (for the men) or rooms with furnished kitchen blocks (for the women). Usually the employees pay a very small amount of money to enable the men (especially) to save money to buy a house when they get married.		Housing units in prisons that house more than the one or two inmates normally held in cells are referred to as "dormitories" as well. Housing arrangements can vary widely. In some cases, dormitories in low-security prisons may almost resemble their academic counterparts, with the obvious differences of being locked at night, being administered by jailers, and subject to stricter institutional rules and fewer amenities. In other institutions, dormitories may be large rooms, often converted from other purposes such as gymnasiums in response to overcrowding, in which hundreds of prisoners have bunks and lockers.		Boarding schools generally have dormitories as resident halls at least for junior or younger children around age 4 to 9 years of age. In classic British boarding schools these typically have bunk beds that have traditionally come to be associated with boarding schools. The Department for Children, Schools and Families, in conjunction with the Department of Health of the United Kingdom, has prescribed guidelines for dormitories in boarding schools. These regulations come under what is called as the National Boarding Standards.[31]		The National Boarding Standards has prescribed minimum floor area or living space required for each student and other aspects of basic facilities. The minimum floor area of a dormitory accommodating two or more students is defined as the number of students sleeping in the dormitory multiplied by 4.2 m², plus 1.2 m².[31] A minimum distance of 0.9 m should also be maintained between any two beds in a dormitory, bedroom or cubicle.[31] In case students are provided with a cubicle, then each student must be provided with a window and a floor area of 5.0 m² at the least.[31] A bedroom for a single student should be at least of floor area of 6.0 m².[31] Boarding schools must provide a total floor area of at least 2.3 m² living accommodation for every boarder.[31] This should also be incorporated with at least one bathtub or shower for every ten students.[31] These are some of the few guidelines set by the department amongst many others. It could probably be observed that not all boarding schools around the world meet these minimum basic standards, despite their apparent appeal.		A floating dormitory is a water-borne vessel that provides, as its primary function, living quarters for students enrolled at an educational institution. A floating dormitory functions as a conventional land-based dormitory in all respects except that the living quarters are aboard a floating vessel. A floating dormitory is most often moored in place near the host educational facility and is not used for water transport. Dormitory ships may also refer to vessels that provide water-borne housing in support of non-academic enterprises such as off-shore oil drilling operations. Other vessels containing living quarters for students as ancillary support to the vessel's primary function — such as for providing maritime or other training given aboard the vessel — are more appropriately categorized as training ships.		Notable among floating dormitories is SS Stevens, a 473-foot, 14,893-ton ship operated by Stevens Institute of Technology, a technological university, in Hoboken, New Jersey. From 1968 to 1975, Stevens served as the floating dormitory for as many as 150 students of the institute.		
– in Europe  (green & dark grey) – in the European Union  (green)		The United Kingdom of Great Britain and Northern Ireland, commonly known as the United Kingdom (UK) or Britain, is a sovereign country in western Europe. Lying off the north-western coast of the European mainland, the United Kingdom includes the island of Great Britain, the north-eastern part of the island of Ireland and many smaller islands.[11] Northern Ireland is the only part of the United Kingdom that shares a land border with another sovereign state‍—‌the Republic of Ireland.[note 9] Apart from this land border, the United Kingdom is surrounded by the Atlantic Ocean, with the North Sea to its east, the English Channel to its south and the Celtic Sea to its south-south-west, giving it the 12th-longest coastline in the world. The Irish Sea lies between Great Britain and Ireland. With an area of 242,500 square kilometres (93,600 sq mi), the United Kingdom is the 78th-largest sovereign state in the world and the 11th-largest in Europe. It is also the 21st-most populous country, with an estimated 65.1 million inhabitants.[12] Together, this makes it the fourth-most densely populated country in the European Union (EU).[note 10][13]		The United Kingdom is a constitutional monarchy with a parliamentary​ democracy.[14][15] The monarch is Queen Elizabeth II, who has reigned since 6 February 1952. The capital of the United Kingdom and its largest city is London, a global city and financial centre with an urban area population of 10.3 million, the fourth-largest in Europe and second-largest in the European Union.[16] Other major urban areas in the United Kingdom include the conurbations centred on Birmingham, Leeds, Glasgow, Liverpool and Manchester. The United Kingdom consists of four countries—England, Scotland, Wales and Northern Ireland.[17] The last three have devolved administrations,[18] each with varying powers,[19][20] based in their capitals, Edinburgh, Cardiff and Belfast, respectively. The nearby Isle of Man, Bailiwick of Guernsey and Bailiwick of Jersey are not part of the United Kingdom, being Crown dependencies with the British Government responsible for defence and international representation.[21]		The relationships among the countries of the UK have changed over time. Wales was annexed by the Kingdom of England under the Laws in Wales Acts 1535 and 1542. A treaty between England and Scotland resulted in 1707 in a unified Kingdom of Great Britain, which merged in 1801 with the Kingdom of Ireland to form the United Kingdom of Great Britain and Ireland. Five-sixths of Ireland seceded from the UK in 1922, leaving the present formulation of the United Kingdom of Great Britain and Northern Ireland.[note 11] There are fourteen British Overseas Territories.[22] These are the remnants of the British Empire which, at its height in the 1920s, encompassed almost a quarter of the world's land mass and was the largest empire in history. British influence can be observed in the language, culture and legal systems of many of its former colonies.		The United Kingdom is a developed country and has the world's fifth-largest economy by nominal GDP and ninth-largest economy by purchasing power parity. The UK is considered to have a high-income economy and is categorised as very high in the Human Development Index, ranking 16th in the world. It was the world's first industrialised country and the world's foremost power during the 19th and early 20th centuries.[23][24] The UK remains a great power with considerable economic, cultural, military, scientific and political influence internationally.[25][26] It is a recognised nuclear weapons state and is seventh in military expenditure in the world.[27] The UK has been a permanent member of the United Nations Security Council since its first session in 1946. It has been a leading member state of the EU and its predecessor, the European Economic Community (EEC), since 1973. However, on 23 June 2016, a national referendum on the UK's membership of the EU resulted in a decision to leave, and its exit from the EU is currently being negotiated. The UK is also a member of the Commonwealth of Nations, the Council of Europe, the G7 finance ministers, the G7 forum, the G20, NATO, the Organisation for Economic Co-operation and Development (OECD), and the World Trade Organization (WTO).								The 1707 Acts of Union declared that the kingdoms of England and Scotland were "United into One Kingdom by the Name of Great Britain", though the new state is also referred to in the Acts as the "Kingdom of Great Britain", "United Kingdom of Great Britain" and "United Kingdom".[28][29][note 12] However, the term "United Kingdom" is only found in informal use during the 18th century and the country was only occasionally referred to as the "United Kingdom of Great Britain"—its full official name, from 1707 to 1800, being merely "Great Britain", without a "long form".[30][31][32][33][34] The Acts of Union 1800 united the Kingdom of Great Britain and the Kingdom of Ireland in 1801, forming the United Kingdom of Great Britain and Ireland. Following the partition of Ireland and the independence of the Irish Free State in 1922, which left Northern Ireland as the only part of the island of Ireland within the United Kingdom, the name "United Kingdom of Great Britain and Northern Ireland"[note 13] was adopted.[35]		Although the United Kingdom, as a sovereign state, is a country, England, Scotland, Wales and, to a lesser degree, Northern Ireland, are also regarded as countries, though they are not sovereign states.[36][37] Scotland, Wales and Northern Ireland have devolved self-government.[38][39] The British Prime Minister's website has used the phrase "countries within a country" to describe the United Kingdom.[17] Some statistical summaries, such as those for the twelve NUTS 1 regions of the United Kingdom, also refer to Scotland, Wales and Northern Ireland as "regions".[40][41] Northern Ireland is also referred to as a "province".[42][43] With regard to Northern Ireland, the descriptive name used "can be controversial, with the choice often revealing one's political preferences".[44]		The term "Britain"[note 14] is often used as synonym for the United Kingdom. The term "Great Britain", by contrast, refers conventionally to the island of Great Britain, or politically to England, Scotland and Wales in combination.[45][46][47] However, it is sometimes used as a loose synonym for the United Kingdom as a whole.[48][49] GB and GBR are the standard country codes for the United Kingdom (see ISO 3166-2 and ISO 3166-1 alpha-3) and are consequently used by international organisations to refer to the United Kingdom. Additionally, the United Kingdom's Olympic team competes under the name "Great Britain" or "Team GB".[50][51]		The adjective "British" is commonly used to refer to matters relating to the United Kingdom. The term has no definite legal connotation, but is used in law to refer to United Kingdom citizenship and matters to do with nationality.[52] People of the United Kingdom use a number of different terms to describe their national identity and may identify themselves as being British; or as being English, Scottish, Welsh, Northern Irish, or Irish;[53] or as being both.[54]		In 2006, a new design of British passport was introduced. Its first page shows the long form name of the state in English, Welsh and Scottish Gaelic.[55] In Welsh, the long form name of the state is "Teyrnas Unedig Prydain Fawr a Gogledd Iwerddon", with "Teyrnas Unedig" being used as a short form name on government websites.[56] However, it is usually abbreviated to "DU" for the mutated form "Y Deyrnas Unedig". In Scottish Gaelic, the long form is "Rìoghachd Aonaichte Bhreatainn is Èireann a Tuath" and the short form "Rìoghachd Aonaichte".		Settlement by anatomically modern humans of what was to become the United Kingdom occurred in waves beginning by about 30,000 years ago.[57] By the end of the region's prehistoric period, the population is thought to have belonged, in the main, to a culture termed Insular Celtic, comprising Brythonic Britain and Gaelic Ireland.[58] The Roman conquest, beginning in 43 AD, and the 400-year rule of southern Britain, was followed by an invasion by Germanic Anglo-Saxon settlers, reducing the Brythonic area mainly to what was to become Wales and the historic Kingdom of Strathclyde.[59] Most of the region settled by the Anglo-Saxons became unified as the Kingdom of England in the 10th century.[60] Meanwhile, Gaelic-speakers in north-west Britain (with connections to the north-east of Ireland and traditionally supposed to have migrated from there in the 5th century)[61][62] united with the Picts to create the Kingdom of Scotland in the 9th century.[63]		In 1066, the Normans invaded England from France and after its conquest, seized large parts of Wales, conquered much of Ireland and were invited to settle in Scotland, bringing to each country feudalism on the Northern French model and Norman-French culture.[64] The Norman elites greatly influenced, but eventually assimilated with, each of the local cultures.[65] Subsequent medieval English kings completed the conquest of Wales and made an unsuccessful attempt to annex Scotland. Following the Declaration of Arbroath, Scotland maintained its independence, albeit in near-constant conflict with England. The English monarchs, through inheritance of substantial territories in France and claims to the French crown, were also heavily involved in conflicts in France, most notably the Hundred Years War, while the Kings of Scots were in an alliance with the French during this period.[66]		The early modern period saw religious conflict resulting from the Reformation and the introduction of Protestant state churches in each country.[67] Wales was fully incorporated into the Kingdom of England,[68] and Ireland was constituted as a kingdom in personal union with the English crown.[69] In what was to become Northern Ireland, the lands of the independent Catholic Gaelic nobility were confiscated and given to Protestant settlers from England and Scotland.[70]		In 1603, the kingdoms of England, Scotland and Ireland were united in a personal union when James VI, King of Scots, inherited the crowns of England and Ireland and moved his court from Edinburgh to London; each country nevertheless remained a separate political entity and retained its separate political, legal, and religious institutions.[71][72]		In the mid-17th century, all three kingdoms were involved in a series of connected wars (including the English Civil War) which led to the temporary overthrow of the monarchy and the establishment of the short-lived unitary republic of the Commonwealth of England, Scotland and Ireland.[73][74] During the 17th and 18th centuries, British sailors were involved in acts of piracy (privateering), attacking and stealing from ships off the coast of Europe and the Caribbean.[75]		Although the monarchy was restored, the Interregnum ensured (along with the Glorious Revolution of 1688 and the subsequent Bill of Rights 1689, and the Claim of Right Act 1689) that, unlike much of the rest of Europe, royal absolutism would not prevail, and a professed Catholic could never accede to the throne. The British constitution would develop on the basis of constitutional monarchy and the parliamentary system.[76] With the founding of the Royal Society in 1660, science was greatly encouraged. During this period, particularly in England, the development of naval power (and the interest in voyages of discovery) led to the acquisition and settlement of overseas colonies, particularly in North America.[77][78]		On 1 May 1707, the united Kingdom of Great Britain came into being, the result of Acts of Union being passed by the parliaments of England and Scotland to ratify the 1706 Treaty of Union and so unite the two kingdoms.[79][80][81]		In the 18th century, cabinet government developed under Robert Walpole, in practice the first prime minister (1721–1742). A series of Jacobite Uprisings sought to remove the Protestant House of Hanover from the British throne and restore the Catholic House of Stuart. The Jacobites were finally defeated at the Battle of Culloden in 1746, after which the Scottish Highlanders were brutally suppressed. The British colonies in North America that broke away from Britain in the American War of Independence became the United States of America, recognised by Britain in 1783. British imperial ambition turned elsewhere, particularly to India.[82]		During the 18th century, Britain was involved in the Atlantic slave trade. British ships transported an estimated two million slaves from Africa to the West Indies before banning the trade in 1807, banning slavery in 1833, and taking a leading role in the movement to abolish slavery worldwide by pressing other nations to end their trade with a series of treaties, and then formed the world's oldest international human rights organisation, Anti-Slavery International, in London in 1839.[83][84][85] The term "United Kingdom" became official in 1801 when the parliaments of Britain and Ireland each passed an Act of Union, uniting the two kingdoms and creating the United Kingdom of Great Britain and Ireland.[86]		In the early 19th century, the British-led Industrial Revolution began to transform the country. Gradually political power shifted away from the old Tory and Whig landowning classes towards the new industrialists. An alliance of merchants and industrialists with the Whigs would lead to a new party, the Liberals, with an ideology of free trade and laissez-faire. In 1832 Parliament passed the Great Reform Act, which began the transfer of political power from the aristocracy to the middle classes. In the countryside, enclosure of the land was driving small farmers out. Towns and cities began to swell with a new urban working class. Few ordinary workers had the vote, and they created their own organisations in the form of trade unions.[citation needed]		After the defeat of France at the end of the Revolutionary and Napoleonic Wars (1792–1815), Great Britain emerged as the principal naval and imperial power of the 19th century (with London the largest city in the world from about 1830).[87] Unchallenged at sea, British dominance was later described as Pax Britannica ("British Peace"), a period of relative peace in Europe and the world (1815–1914) during which the British Empire became the global hegemon and adopted the role of global policeman.[88][89][90][91] By the time of the Great Exhibition of 1851, Britain was described as the "workshop of the world".[92] The British Empire was expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many regions, such as Asia and Latin America.[93][94] Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During the century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses.[95] To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa, and elsewhere. Canada, Australia, and New Zealand became self-governing dominions.[96] After the turn of the century, Britain's industrial dominance was challenged by the United States and Germany.[97]		Social reform and home rule for Ireland were important domestic issues after 1900. The Labour Party emerged from an alliance of trade unions and small Socialist groups in 1900, and suffragettes campaigned for women's right to vote before 1914.[98]		Britain fought alongside France, Russia and (after 1917) the United States, against Germany and its allies in the First World War (1914–1918).[99] British armed forces were engaged across much of the British Empire and in several regions of Europe, particularly on the Western front.[100] The high fatalities of trench warfare caused the loss of much of a generation of men, with lasting social effects in the nation and a great disruption in the social order.[citation needed]		After the war, Britain received the League of Nations mandate over a number of former German and Ottoman colonies. The British Empire reached its greatest extent, covering a fifth of the world's land surface and a quarter of its population.[101] However, Britain had suffered 2.5 million casualties and finished the war with a huge national debt.[100]		The rise of Irish nationalism, and disputes within Ireland over the terms of Irish Home Rule, led eventually to the partition of the island in 1921.[102] The Irish Free State became independent with Dominion status in 1922. Northern Ireland remained part of the United Kingdom.[103] A wave of strikes in the mid-1920s culminated in the General Strike of 1926. Britain had still not recovered from the effects of the war when the Great Depression (1929–1932) occurred. This led to considerable unemployment and hardship in the old industrial areas, as well as political and social unrest in the 1930s, with rising membership in communist and socialist parties. A coalition government was formed in 1931.[104]		Britain entered the Second World War by declaring war on Nazi Germany in 1939, after it had invaded Poland. Winston Churchill became prime minister and head of a coalition government in 1940. Despite the defeat of its European allies in the first year of the war, Britain and its Empire continued the fight alone against Germany. In 1940, the Royal Air Force defeated the German Luftwaffe in a struggle for control of the skies in the Battle of Britain. Urban areas suffered heavy bombing during the Blitz. There were also eventual hard-fought victories in the Battle of the Atlantic, the North Africa campaign and the Burma campaign. British forces played an important role in the Normandy landings of 1944, achieved with its United States ally.		After the end of the Second World War in 1945, the UK was one of the Big Four powers (the Soviet Union, the United Kingdom, the US and China) who met to plan the post-war world;[105][106] it was an original signatory to the Declaration of the United Nations. The UK became one of the five permanent members of the United Nations Security Council. However, the war left the UK severely weakened and depending financially on the Marshall Plan.[107] In the immediate post-war years, the Labour government initiated a radical programme of reforms, which had a significant effect on British society in the following decades.[108] Major industries and public utilities were nationalised, a welfare state was established, and a comprehensive, publicly funded healthcare system, the National Health Service, was created.[109] The rise of nationalism in the colonies coincided with Britain's now much-diminished economic position, so that a policy of decolonisation was unavoidable. Independence was granted to India and Pakistan in 1947.[110] Over the next three decades, most colonies of the British Empire gained their independence. Many became members of the Commonwealth of Nations.[111]		Although the UK was the third country to develop a nuclear weapons arsenal (with its first atomic bomb test in 1952), the new post-war limits of Britain's international role were illustrated by the Suez Crisis of 1956. The international spread of the English language ensured the continuing international influence of its literature and culture.[112][113] As a result of a shortage of workers in the 1950s, the government encouraged immigration from Commonwealth countries. In the following decades, the UK became a more multi-ethnic society than before.[114] Despite rising living standards in the late 1950s and 1960s, the UK's economic performance was not as successful as many of its competitors, such as West Germany and Japan.[citation needed]		In the decade-long process of European integration, the UK was a founding member of the alliance called the Western European Union, established with the London and Paris Conferences in 1954. In 1960 the UK was one of the seven founding members of the European Free Trade Association (EFTA), but in 1973 it left to join the European Communities (EC). When the EC became the European Union (EU) in 1992, the UK was one of the 12 founding members. The Treaty of Lisbon was signed in 2007, which forms the constitutional basis of the European Union since then.		From the late 1960s, Northern Ireland suffered communal and paramilitary violence (sometimes affecting other parts of the UK) conventionally known as the Troubles. It is usually considered to have ended with the Belfast "Good Friday" Agreement of 1998.[117][118][119]		Following a period of widespread economic slowdown and industrial strife in the 1970s, the Conservative government of the 1980s under Margaret Thatcher initiated a radical policy of monetarism, deregulation, particularly of the financial sector (for example, Big Bang in 1986) and labour markets, the sale of state-owned companies (privatisation), and the withdrawal of subsidies to others.[120] This resulted in high unemployment and social unrest, but ultimately also economic growth, particularly in the services sector. From 1984, the economy was helped by the inflow of substantial North Sea oil revenues.[121]		Around the end of the 20th century there were major changes to the governance of the UK with the establishment of devolved administrations for Scotland, Wales and Northern Ireland.[122] The statutory incorporation followed acceptance of the European Convention on Human Rights. The UK is still a key global player diplomatically and militarily. It plays leading roles in the EU, UN and NATO. However, controversy surrounds some of Britain's overseas military deployments, particularly in Afghanistan and Iraq.[123]		The 2008 global financial crisis severely affected the UK economy. The coalition government of 2010 introduced austerity measures intended to tackle the substantial public deficits which resulted.[124] In 2014 the Scottish Government held a referendum on Scottish independence, with 55% of voters rejecting the independence proposal and opting to remain within the United Kingdom.[125] In 2016, the United Kingdom voted to leave the European Union.[126] The legal process of leaving the EU began on 29 March 2017, with the UK's invocation of Article 50 of the Treaty of Lisbon, formally notifying the EU of the UK's intention to leave. The article stipulates that the negotiations to leave will last at least two years. The UK remains a full member of the EU during this time.[127][128]		The total area of the United Kingdom is approximately 243,610 square kilometres (94,060 sq mi). The country occupies the major part of the British Isles[129] archipelago and includes the island of Great Britain, the north-eastern one-sixth of the island of Ireland and some smaller surrounding islands. It lies between the North Atlantic Ocean and the North Sea with the south-east coast coming within 22 miles (35 km) of the coast of northern France, from which it is separated by the English Channel.[130] In 1993 10% of the UK was forested, 46% used for pastures and 25% cultivated for agriculture.[131] The Royal Greenwich Observatory in London is the defining point of the Prime Meridian.[132]		The United Kingdom lies between latitudes 49° to 61° N, and longitudes 9° W to 2° E. Northern Ireland shares a 224-mile (360 km) land boundary with the Republic of Ireland.[130] The coastline of Great Britain is 11,073 miles (17,820 km) long.[133] It is connected to continental Europe by the Channel Tunnel, which at 31 miles (50 km) (24 miles (38 km) underwater) is the longest underwater tunnel in the world.[134]		England accounts for just over half of the total area of the UK, covering 130,395 square kilometres (50,350 sq mi).[135] Most of the country consists of lowland terrain,[131] with mountainous terrain north-west of the Tees-Exe line; including the Cumbrian Mountains of the Lake District, the Pennines, Exmoor and Dartmoor. The main rivers and estuaries are the Thames, Severn and the Humber. England's highest mountain is Scafell Pike (978 metres (3,209 ft)) in the Lake District. Its principal rivers are the Severn, Thames, Humber, Tees, Tyne, Tweed, Avon, Exe and Mersey.[131]		Scotland accounts for just under a third of the total area of the UK, covering 78,772 square kilometres (30,410 sq mi)[136] and including nearly eight hundred islands,[137] predominantly west and north of the mainland; notably the Hebrides, Orkney Islands and Shetland Islands. Scotland is the most mountainous country in the UK and its topography is distinguished by the Highland Boundary Fault—a geological rock fracture—which traverses Scotland from Arran in the west to Stonehaven in the east.[138] The fault separates two distinctively different regions; namely the Highlands to the north and west and the lowlands to the south and east. The more rugged Highland region contains the majority of Scotland's mountainous land, including Ben Nevis which at 1,343 metres (4,406 ft) is the highest point in the British Isles.[139] Lowland areas—especially the narrow waist of land between the Firth of Clyde and the Firth of Forth known as the Central Belt—are flatter and home to most of the population including Glasgow, Scotland's largest city, and Edinburgh, its capital and political centre, although upland and mountainous terrain lies within the Southern Uplands.		Wales accounts for less than a tenth of the total area of the UK, covering 20,779 square kilometres (8,020 sq mi).[140] Wales is mostly mountainous, though South Wales is less mountainous than North and mid Wales. The main population and industrial areas are in South Wales, consisting of the coastal cities of Cardiff, Swansea and Newport, and the South Wales Valleys to their north. The highest mountains in Wales are in Snowdonia and include Snowdon (Welsh: Yr Wyddfa) which, at 1,085 metres (3,560 ft), is the highest peak in Wales.[131] The 14, or possibly 15, Welsh mountains over 3,000 feet (910 metres) high are known collectively as the Welsh 3000s. Wales has over 2,704 kilometres (1,680 miles) of coastline.[133] Several islands lie off the Welsh mainland, the largest of which is Anglesey (Ynys Môn) in the north-west.		Northern Ireland, separated from Great Britain by the Irish Sea and North Channel, has an area of 14,160 square kilometres (5,470 sq mi) and is mostly hilly. It includes Lough Neagh which, at 388 square kilometres (150 sq mi), is the largest lake in the British Isles by area.[141] The highest peak in Northern Ireland is Slieve Donard in the Mourne Mountains at 852 metres (2,795 ft).[131]		The United Kingdom has a temperate climate, with plentiful rainfall all year round.[130] The temperature varies with the seasons seldom dropping below −11 °C (12 °F) or rising above 35 °C (95 °F).[142] The prevailing wind is from the south-west and bears frequent spells of mild and wet weather from the Atlantic Ocean,[130] although the eastern parts are mostly sheltered from this wind since the majority of the rain falls over the western regions the eastern parts are therefore the driest. Atlantic currents, warmed by the Gulf Stream, bring mild winters;[143] especially in the west where winters are wet and even more so over high ground. Summers are warmest in the south-east of England, being closest to the European mainland, and coolest in the north. Heavy snowfall can occur in winter and early spring on high ground, and occasionally settles to great depth away from the hills.		There is no consistent system of administrative or geographic demarcation across the United Kingdom.[144] Each country of the United Kingdom has its own arrangements, whose origins often pre-date the UK's formation. Until the 19th century there was little change to those arrangements, but there has since been a constant evolution of role and function,[145] most significantly the devolution of powers to Scotland, Wales and Northern Ireland.		The organisation of local government in England is complex, with the distribution of functions varying according to local arrangements. Legislation concerning local government in England is the responsibility of the UK's parliament and the government, as England has no devolved legislature. The upper-tier subdivisions of England are the nine regions, now used primarily for statistical purposes.[146] One region, Greater London, has had a directly elected assembly and mayor since 2000 following popular support for the proposal in a referendum.[147] It was intended that other regions would also be given their own elected regional assemblies, but a proposed assembly in the North East region was rejected by a referendum in 2004.[148] Below the regional tier, some parts of England have county councils and district councils and others have unitary authorities; while London consists of 32 London boroughs and the City of London. Councillors are elected by the first-past-the-post system in single-member wards or by the multi-member plurality system in multi-member wards.[149]		For local government purposes, Scotland is divided into 32 council areas, with wide variation in both size and population. The cities of Glasgow, Edinburgh, Aberdeen and Dundee are separate council areas, as is the Highland Council which includes a third of Scotland's area but only just over 200,000 people. Local councils are made up of elected councillors, of whom there are 1,223;[150] they are paid a part-time salary. Elections are conducted by single transferable vote in multi-member wards that elect either three or four councillors. Each council elects a Provost, or Convenor, to chair meetings of the council and to act as a figurehead for the area. Councillors are subject to a code of conduct enforced by the Standards Commission for Scotland.[151] The representative association of Scotland's local authorities is the Convention of Scottish Local Authorities (COSLA).[152]		Local government in Wales consists of 22 unitary authorities. These include the cities of Cardiff, Swansea and Newport which are unitary authorities in their own right.[153] Elections are held every four years under the first-past-the-post system.[153] The most recent elections were held in May 2012, except for the Isle of Anglesey. The Welsh Local Government Association represents the interests of local authorities in Wales.[154]		Local government in Northern Ireland has since 1973 been organised into 26 district councils, each elected by single transferable vote. Their powers are limited to services such as collecting waste, controlling dogs and maintaining parks and cemeteries.[155] On 13 March 2008 the executive agreed on proposals to create 11 new councils and replace the present system.[156] The next local elections were postponed until 2016 to facilitate this.[157]		The United Kingdom has sovereignty over seventeen territories which do not form part of the United Kingdom itself: fourteen British Overseas Territories[22] and three Crown dependencies.[22][158]		The fourteen British Overseas Territories are: Anguilla; Bermuda; the British Antarctic Territory; the British Indian Ocean Territory; the British Virgin Islands; the Cayman Islands; the Falkland Islands; Gibraltar; Montserrat; Saint Helena, Ascension and Tristan da Cunha; the Turks and Caicos Islands; the Pitcairn Islands; South Georgia and the South Sandwich Islands; and Akrotiri and Dhekelia on the island of Cyprus.[159] British claims in Antarctica are not universally recognised.[160] Collectively Britain's overseas territories encompass an approximate land area of 1,727,570 square kilometres (667,018 sq mi) and a population of approximately 260,000 people.[161]		They are the last remaining remnants of the British Empire and a 1999 UK government white paper stated that: "[The] Overseas Territories are British for as long as they wish to remain British. Britain has willingly granted independence where it has been requested; and we will continue to do so where this is an option."[162] Self-determination is also enshrined into the constitutions of several overseas territories and three have specifically voted to remain under British sovereignty (Bermuda in 1995,[163] Gibraltar in 2002[164] and the Falkland Islands in 2013).[165]		The Crown dependencies are possessions of the Crown, as opposed to overseas territories of the UK.[166] They comprise three independently administered jurisdictions: the Channel Islands of Jersey and Guernsey in the English Channel, and the Isle of Man in the Irish Sea. By mutual agreement, the British Government manages the islands' foreign affairs and defence and the UK Parliament has the authority to legislate on their behalf. However, internationally, they are regarded as "territories for which the United Kingdom is responsible".[167] The power to pass legislation affecting the islands ultimately rests with their own respective legislative assemblies, with the assent of the Crown (Privy Council or, in the case of the Isle of Man, in certain circumstances the Lieutenant-Governor).[168] Since 2005 each Crown dependency has had a Chief Minister as its head of government.[169]		The British dependencies use a varied assortment of currencies. These include the British pound, US dollar, New Zealand dollar, euro or their own currencies, which may be pegged to either.		The United Kingdom is a unitary state under a constitutional monarchy. Queen Elizabeth II is the monarch and head of state of the UK, as well as Queen of fifteen other independent Commonwealth countries. The monarch has "the right to be consulted, the right to encourage, and the right to warn".[170] The Constitution of the United Kingdom is uncodified and consists mostly of a collection of disparate written sources, including statutes, judge-made case law and international treaties, together with constitutional conventions.[171] As there is no technical difference between ordinary statutes and "constitutional law", the UK Parliament can perform "constitutional reform" simply by passing Acts of Parliament, and thus has the political power to change or abolish almost any written or unwritten element of the constitution. However, no Parliament can pass laws that future Parliaments cannot change.[172]		The UK has a parliamentary government based on the Westminster system that has been emulated around the world: a legacy of the British Empire. The parliament of the United Kingdom meets in the Palace of Westminster and has two houses: an elected House of Commons and an appointed House of Lords. All bills passed are given Royal Assent before becoming law.		The position of prime minister,[note 15] the UK's head of government,[173] belongs to the person most likely to command the confidence of the House of Commons; this individual is typically the leader of the political party or coalition of parties that holds the largest number of seats in that chamber. The prime minister chooses a cabinet and its members are formally appointed by the monarch to form Her Majesty's Government. By convention, the monarch respects the prime minister's decisions of government.[174]		The cabinet is traditionally drawn from members of the prime minister's party or coalition and mostly from the House of Commons but always from both legislative houses, the cabinet being responsible to both. Executive power is exercised by the prime minister and cabinet, all of whom are sworn into the Privy Council of the United Kingdom, and become Ministers of the Crown. The current Prime Minister is Theresa May, who has been in office since 13 July 2016. May is also the leader of the Conservative Party. For elections to the House of Commons, the UK is divided into 650 constituencies,[175] each electing a single member of parliament (MP) by simple plurality. General elections are called by the monarch when the prime minister so advises. Prior to the Fixed-term Parliaments Act 2011, the Parliament Acts 1911 and 1949 required that a new election must be called no later than five years after the previous general election.[176]		The Conservative Party, the Labour Party and the Liberal Democrats (formerly as the Liberal Party) have, in modern times, been considered the UK's three major political parties,[177] representing the British traditions of conservatism, socialism and liberalism, respectively.[178] However, in the 2017 general election the Scottish National Party was the third-largest party by number of seats won, ahead of the Liberal Democrats. Most of the remaining seats were won by parties that contest elections only in one part of the UK: Plaid Cymru (Wales only); and the Democratic Unionist Party and Sinn Féin (Northern Ireland only[note 16]). In accordance with party policy, no elected Sinn Féin members of parliament have ever attended the House of Commons to speak on behalf of their constituents because of the requirement to take an oath of allegiance to the monarch.[179]		Scotland, Wales and Northern Ireland each have their own government or executive, led by a First Minister (or, in the case of Northern Ireland, a diarchal First Minister and deputy First Minister), and a devolved unicameral legislature. England, the largest country of the United Kingdom, has no such devolved executive or legislature and is administered and legislated for directly by the UK's government and parliament on all issues. This situation has given rise to the so-called West Lothian question which concerns the fact that members of parliament from Scotland, Wales and Northern Ireland can vote, sometimes decisively,[180] on matters that only affect England.[181] The McKay Commission reported on this matter in March 2013 recommending that laws affecting only England should need support from a majority of English members of parliament.[182]		The Scottish Government and Parliament have wide-ranging powers over any matter that has not been specifically reserved to the UK Parliament, including education, healthcare, Scots law and local government.[183] At the 2011 elections the Scottish National Party won re-election and achieved an overall majority in the Scottish Parliament, with its leader, Alex Salmond, as First Minister of Scotland.[184][185] In 2012, the UK and Scottish governments signed the Edinburgh Agreement setting out the terms for a referendum on Scottish independence in 2014, which was defeated 55.3% to 44.7%.[186]		The Welsh Government and the National Assembly for Wales have more limited powers than those devolved to Scotland.[187] The Assembly is able to legislate on devolved matters through Acts of the Assembly, which require no prior consent from Westminster. The 2011 elections resulted in a minority Labour administration led by Carwyn Jones.[188]		The Northern Ireland Executive and Assembly have powers similar to those devolved to Scotland. The Executive is led by a diarchy representing unionist and nationalist members of the Assembly. Arlene Foster (Democratic Unionist Party) and Martin McGuinness (Sinn Féin) were First Minister and deputy First Minister respectively until 2017.[189] Devolution to Northern Ireland is contingent on participation by the Northern Ireland administration in the North-South Ministerial Council, where the Northern Ireland Executive cooperates and develops joint and shared policies with the Government of Ireland. The British and Irish governments co-operate on non-devolved matters affecting Northern Ireland through the British–Irish Intergovernmental Conference, which assumes the responsibilities of the Northern Ireland administration in the event of its non-operation.		The UK does not have a codified constitution and constitutional matters are not among the powers devolved to Scotland, Wales or Northern Ireland. Under the doctrine of parliamentary sovereignty, the UK Parliament could, in theory, therefore, abolish the Scottish Parliament, Welsh Assembly or Northern Ireland Assembly.[190][191] Indeed, in 1972, the UK Parliament unilaterally prorogued the Parliament of Northern Ireland, setting a precedent relevant to contemporary devolved institutions.[192] In practice, it would be politically difficult for the UK Parliament to abolish devolution to the Scottish Parliament and the Welsh Assembly, given the political entrenchment created by referendum decisions.[193] The political constraints placed upon the UK Parliament's power to interfere with devolution in Northern Ireland are even greater than in relation to Scotland and Wales, given that devolution in Northern Ireland rests upon an international agreement with the Government of Ireland.[194]		The United Kingdom does not have a single legal system, as Article 19 of the 1706 Treaty of Union provided for the continuation of Scotland's separate legal system.[195] Today the UK has three distinct systems of law: English law, Northern Ireland law and Scots law. A new Supreme Court of the United Kingdom came into being in October 2009 to replace the Appellate Committee of the House of Lords.[196][197] The Judicial Committee of the Privy Council, including the same members as the Supreme Court, is the highest court of appeal for several independent Commonwealth countries, the British Overseas Territories and the Crown Dependencies.[198]		Both English law, which applies in England and Wales, and Northern Ireland law are based on common-law principles.[199] The essence of common law is that, subject to statute, the law is developed by judges in courts, applying statute, precedent and common sense to the facts before them to give explanatory judgements of the relevant legal principles, which are reported and binding in future similar cases (stare decisis).[200] The courts of England and Wales are headed by the Senior Courts of England and Wales, consisting of the Court of Appeal, the High Court of Justice (for civil cases) and the Crown Court (for criminal cases). The Supreme Court is the highest court in the land for both criminal and civil appeal cases in England, Wales and Northern Ireland and any decision it makes is binding on every other court in the same jurisdiction, often having a persuasive effect in other jurisdictions.[201]		Scots law is a hybrid system based on both common-law and civil-law principles. The chief courts are the Court of Session, for civil cases,[202] and the High Court of Justiciary, for criminal cases.[203] The Supreme Court of the United Kingdom serves as the highest court of appeal for civil cases under Scots law.[204] Sheriff courts deal with most civil and criminal cases including conducting criminal trials with a jury, known as sheriff solemn court, or with a sheriff and no jury, known as sheriff summary Court.[205] The Scots legal system is unique in having three possible verdicts for a criminal trial: "guilty", "not guilty" and "not proven". Both "not guilty" and "not proven" result in an acquittal.[206]		Crime in England and Wales increased in the period between 1981 and 1995, though since that peak there has been an overall fall of 66% in recorded crime from 1995 to 2015,[207] according to crime statistics. The prison population of England and Wales has increased to 86,000, giving England and Wales the highest rate of incarceration in Western Europe at 148 per 100,000.[208][209] Her Majesty's Prison Service, which reports to the Ministry of Justice, manages most of the prisons within England and Wales. The murder rate in England and Wales has stabilised in the first half of the 2010s with a murder rate around 1 per 100,000 which is half the peak in 2002 and similar to the rate in the 1980s.[210][unreliable source] More sexual offences have been reported to the police since 2002.[211][212] Crime in Scotland fell slightly in 2014/2015 to its lowest level in 39 years in with 59 killings for a murder rate of 1.1 per 100,000. Scotland's prisons are overcrowded but the prison population is shrinking.[213]		The UK is a permanent member of the United Nations Security Council, a member of NATO, the Commonwealth of Nations, the G7 finance ministers, the G7 forum (previously the G8 forum), the G20, the OECD, the WTO, the Council of Europe, the OSCE. It is also a member state of the European Union in the process of withdrawal.[214] The UK is said to have a "Special Relationship" with the United States and a close partnership with France—the "Entente cordiale"—and shares nuclear weapons technology with both countries.[215][216] The UK is also closely linked with the Republic of Ireland; the two countries share a Common Travel Area and co-operate through the British-Irish Intergovernmental Conference and the British-Irish Council. Britain's global presence and influence is further amplified through its trading relations, foreign investments, official development assistance and military engagements.[217]		The armed forces of the United Kingdom—officially, Her Majesty's Armed Forces—consist of three professional service branches: the Royal Navy and Royal Marines (forming the Naval Service), the British Army and the Royal Air Force.[218] The forces are managed by the Ministry of Defence and controlled by the Defence Council, chaired by the Secretary of State for Defence. The Commander-in-Chief is the British monarch, Elizabeth II, to whom members of the forces swear an oath of allegiance.[219] The Armed Forces are charged with protecting the UK and its overseas territories, promoting the UK's global security interests and supporting international peacekeeping efforts. They are active and regular participants in NATO, including the Allied Rapid Reaction Corps, as well as the Five Power Defence Arrangements, RIMPAC and other worldwide coalition operations. Overseas garrisons and facilities are maintained in Ascension Island, Belize, Brunei, Canada, Cyprus, Diego Garcia, the Falkland Islands, Germany, Gibraltar, Kenya, Qatar and Singapore.[220][221]		The British armed forces played a key role in establishing the British Empire as the dominant world power in the 18th, 19th and early 20th centuries. Throughout its unique history the British forces have seen action in a number of major wars, such as the Seven Years' War, the Napoleonic Wars, the Crimean War, the First World War and the Second World War—as well as many colonial conflicts. By emerging victorious from such conflicts, Britain has often been able to decisively influence world events. Since the end of the British Empire, the UK has nonetheless remained a major military power. Following the end of the Cold War, defence policy has a stated assumption that "the most demanding operations" will be undertaken as part of a coalition.[222] Setting aside the intervention in Sierra Leone, recent UK military operations in Bosnia, Kosovo, Afghanistan, Iraq and, most recently, Libya, have followed this approach. The last occasion on which the British military fought alone was the Falklands War of 1982.		According to various sources, including the Stockholm International Peace Research Institute and the International Institute for Strategic Studies, the United Kingdom has the fourth- or fifth-highest military expenditure in the world. Total defence spending amounts to 2.0% of national GDP.[27]		The UK has a partially regulated market economy.[223] Based on market exchange rates, the UK is today the fifth-largest economy in the world and the second-largest in Europe after Germany. HM Treasury, led by the Chancellor of the Exchequer, is responsible for developing and executing the government's public finance policy and economic policy. The Bank of England is the UK's central bank and is responsible for issuing notes and coins in the nation's currency, the pound sterling. Banks in Scotland and Northern Ireland retain the right to issue their own notes, subject to retaining enough Bank of England notes in reserve to cover their issue. The pound sterling is the world's third-largest reserve currency (after the US dollar and the euro).[224] Since 1997 the Bank of England's Monetary Policy Committee, headed by the Governor of the Bank of England, has been responsible for setting interest rates at the level necessary to achieve the overall inflation target for the economy that is set by the Chancellor each year.[225]		The UK service sector makes up around 73% of GDP.[226] London is one of the three "command centres" of the global economy (alongside New York City and Tokyo),[227] it is the world's largest financial centre alongside New York,[228][229][230] and it has the largest city GDP in Europe.[231] Edinburgh is also one of the largest financial centres in Europe.[232] Tourism is very important to the British economy; with over 27 million tourists arriving in 2004, the United Kingdom is ranked as the sixth major tourist destination in the world and London has the most international visitors of any city in the world.[233][234] The creative industries accounted for 7% GVA in 2005 and grew at an average of 6% per annum between 1997 and 2005.[235]		The Industrial Revolution started in the UK with an initial concentration on the textile industry,[236] followed by other heavy industries such as shipbuilding, coal mining and steelmaking.[237][238] British merchants, shippers and bankers developed overwhelming advantage over those of other nations allowing the UK to dominate international trade in the 19th century.[239][240] As other nations industrialised, coupled with economic decline after two world wars, the United Kingdom began to lose its competitive advantage and heavy industry declined, by degrees, throughout the 20th century. Manufacturing remains a significant part of the economy but accounted for only 16.7% of national output in 2003.[241]		The automotive industry is a significant part of the UK manufacturing sector and employs around 800,000 people, with a turnover in 2015 of some £70 billion, generating £34.6 billion of exports (11.8% of the UK's total export goods). In 2015, the UK produced around 1.6 million passenger vehicles and 94,500 commercial vehicles. The UK is a major centre for engine manufacturing and in 2015 around 2.4 million engines were produced in the country. The UK has a significant presence in motor racing and the UK motorsport industry employs around 41,000 people, comprises around 4,500 companies and has an annual turnover of around £6 billion.[242]		The aerospace industry of the UK is the second- or third-largest national aerospace industry in the world depending upon the method of measurement and has an annual turnover of around £30 billion.[243] In 2016, the global market opportunity for UK aerospace manufacturers over the next two decades was estimated to be £3.5 trillion.[244] The wings for the Airbus A380 and the A350 XWB are designed and manufactured at Airbus UK's world-leading Broughton facility, whilst over a quarter of the value of the Boeing 787 comes from UK manufacturers including Eaton (fuel subsystem pumps), Messier-Bugatti-Dowty (the landing gear) and Rolls-Royce (the engines). Other key names include GKN Aerospace—an expert in metallic and composite aerostructures that's involved in almost every civil and military fixed and rotary wing aircraft in production and development today.[245][246][247][246][248][247][248]		BAE Systems plays a critical role in some of the world's biggest defence aerospace projects. The company makes large sections of the Typhoon Eurofighter at its sub-assembly plant in Samlesbury and assembles the aircraft for the Royal Air Force at its Warton Plant, near Preston. It is also a principal subcontractor on the F35 Joint Strike Fighter—the world's largest single defence project—for which it designs and manufactures a range of components including the aft fuselage, vertical and horizontal tail and wing tips and fuel system. As well as this it manufactures the Hawk, the world's most successful jet training aircraft.[248] Airbus UK also manufactures the wings for the A400 m military transporter. Rolls-Royce, is the world's second-largest aero-engine manufacturer. Its engines power more than 30 types of commercial aircraft and it has more than 30,000 engines in service in the civil and defence sectors. Rolls-Royce is forecast to have more than 50% of the widebody market share by 2016, ahead of General Electric.[249] Agusta Westland designs and manufactures complete helicopters in the UK.[248]		The UK space industry was worth £9.1bn in 2011 and employed 29,000 people. It is growing at a rate of 7.5% annually, according to its umbrella organisation, the UK Space Agency. Government strategy is for the space industry to be a £40bn business for the UK by 2030, capturing a 10% share of the $250bn world market for commercial space technology.[248] On 16 July 2013, the British Government pledged £60 m to the Skylon project: this investment will provide support at a "crucial stage" to allow a full-scale prototype of the SABRE engine to be built. On 2 November 2015, BAE Systems announced they have bought a 20% stake in Reaction Engines ltd. The working partnership will draw on BAE Systems' extensive aerospace technology development and project management expertise and will provide Reaction Engines with access to critical industrial, technical and capital resources to help progress the development of the SABRE engine.[250]		The pharmaceutical industry plays an important role in the UK economy and the country has the third-highest share of global pharmaceutical R&D expenditures (after the United States and Japan).[251][252]		Agriculture is intensive, highly mechanised and efficient by European standards, producing about 60% of food needs with less than 1.6% of the labour force (535,000 workers).[253] Around two-thirds of production is devoted to livestock, one-third to arable crops. Farmers are subsidised by the EU's Common Agricultural Policy. The UK retains a significant, though much reduced fishing industry. It is also rich in a number of natural resources including coal, petroleum, natural gas, tin, limestone, iron ore, salt, clay, chalk, gypsum, lead, silica and an abundance of arable land.[254]		In the final quarter of 2008, as a result of the Great Recession, the UK economy officially entered recession for the first time since 1991.[255] Unemployment increased from 5.2% in May 2008 to 7.6% in May 2009 and by January 2012 the unemployment rate among 18- to 24-year-olds had risen from 11.9% to 22.5%, the highest since current records began in 1992, although it had fallen to 14.2% by November 2015.[256][257][258] Total UK government debt rose quickly from 44.4% of GDP in 2007 to 82.9% of GDP in 2011, then increased more slowly to 87.5% of GDP in 2015.[259][260] Following the likes of the United States, France and many major economies, in February 2013, the UK lost its top AAA credit rating for the first time since 1978 with Moodys and Fitch credit agency while, unlike the other major economies retained their triple A rating with the largest agency Standard & Poor's.[261][262] However, by the end of 2014, UK growth was the fastest in both the G7 and in Europe,[263][264] and by September 2015, the unemployment rate was down to a seven-year low of 5.3%.[265]		As a direct result of the Great Recession between 2010 and the third quarter of 2012 wages in the UK fell by 3.2%,[266] but by 2015 real wages were growing by 3%, having grown faster than inflation since 2014.[267] Since the 1980s, UK economic inequality, like Canada, Australia and the United States has grown faster than in other developed countries.[268][269]		The poverty line in the UK is commonly defined as being 60% of the median household income.[note 17] In 2007–2008 13.5 million people, or 22% of the population, lived below this line. This is a higher level of relative poverty than all but four other EU members.[270] In the same year 4.0 million children, 31% of the total, lived in households below the poverty line after housing costs were taken into account. This is a decrease of 400,000 children since 1998–1999.[271] The UK imports 40% of its food supplies.[272] The Office for National Statistics has estimated that in 2011, 14 million people were at risk of poverty or social exclusion, and that one person in 20 (5.1%) was now experiencing "severe material depression",[273] up from 3 million people in 1977.[274][275]		The UK has an external debt of $9.6 trillion dollars which is second highest in the world after the US which has an external debt of 18.5 trillion dollars. As a percentage of GDP, external debt is 408% which is third highest in the world after Luxembourg and Iceland.[276][277][278][279][280]		The combination of the UK's relatively lax regulatory regime and London's financial institutions providing sophisticated methods to launder proceeds from criminal activity around the world, including those from drug trade, makes the City of London a global hub for illicit finance and the UK a safe haven for the world's major-league tax dodgers, according to research papers and reports published in the mid-2010s.[281][282][283][284][285] The reports on the Panama papers published in April 2016 singled out the UK as being "at the heart of super-rich tax-avoidance network."[286]		England and Scotland were leading centres of the Scientific Revolution from the 17th century[287] and the United Kingdom led the Industrial Revolution from the 18th century,[236] and has continued to produce scientists and engineers credited with important advances.[288] Major theorists from the 17th and 18th centuries include Isaac Newton, whose laws of motion and illumination of gravity have been seen as a keystone of modern science;[289] from the 19th century Charles Darwin, whose theory of evolution by natural selection was fundamental to the development of modern biology, and James Clerk Maxwell, who formulated classical electromagnetic theory; and more recently Stephen Hawking, who has advanced major theories in the fields of cosmology, quantum gravity and the investigation of black holes.[290]		Major scientific discoveries from the 18th century include hydrogen by Henry Cavendish;[291] from the 20th century penicillin by Alexander Fleming,[292] and the structure of DNA, by Francis Crick and others.[293] Famous British engineers and inventors of the Industrial Revolution include James Watt, George Stephenson, Richard Arkwright, Robert Stephenson and Isambard Kingdom Brunel.[294] Other major engineering projects and applications by people from the UK include the steam locomotive, developed by Richard Trevithick and Andrew Vivian;[295] from the 19th century the electric motor by Michael Faraday, the incandescent light bulb by Joseph Swan,[296] and the first practical telephone, patented by Alexander Graham Bell;[297] and in the 20th century the world's first working television system by John Logie Baird and others,[298] the jet engine by Frank Whittle, the basis of the modern computer by Alan Turing, and the World Wide Web by Tim Berners-Lee.[299]		Scientific research and development remains important in British universities, with many establishing science parks to facilitate production and co-operation with industry.[300] Between 2004 and 2008 the UK produced 7% of the world's scientific research papers and had an 8% share of scientific citations, the third and second highest in the world (after the United States and China, respectively).[301] Scientific journals produced in the UK include Nature, the British Medical Journal and The Lancet.[302]		A radial road network totals 29,145 miles (46,904 km) of main roads, 2,173 miles (3,497 km) of motorways and 213,750 miles (344,000 km) of paved roads.[130] The M25, encircling London, is the largest and busiest bypass in the world.[305] In 2009 there were a total of 34 million licensed vehicles in Great Britain.[306]		The UK has a railway network of 10,072 miles (16,209 km) in Great Britain and 189 miles (304 km) in Northern Ireland. Railways in Northern Ireland are operated by NI Railways, a subsidiary of state-owned Translink. In Great Britain, the British Rail network was privatised between 1994 and 1997, which was followed by a rapid rise in passenger numbers following years of decline, although the factors behind this are disputed. Network Rail owns and manages most of the fixed assets (tracks, signals etc.). About 20 privately owned Train Operating Companies operate passenger trains, which carried 1.68 billion passengers in 2015.[307][308] There are also some 1,000 freight trains in daily operation.[when?][130] The British Government is to spend £30 billion on a new high-speed railway line, HS2, to be operational by 2026.[309] Crossrail, under construction in London, is Europe's largest construction project with a £15 billion projected cost.[310][311]		In the year from October 2009 to September 2010 UK airports handled a total of 211.4 million passengers.[312] In that period the three largest airports were London Heathrow Airport (65.6 million passengers), Gatwick Airport (31.5 million passengers) and London Stansted Airport (18.9 million passengers).[312] London Heathrow Airport, located 15 miles (24 km) west of the capital, has the most international passenger traffic of any airport in the world[303][304] and is the hub for the UK flag carrier British Airways, as well as Virgin Atlantic.[313]		In 2006, the UK was the world's ninth-largest consumer of energy and the 15th-largest producer.[314] The UK is home to a number of large energy companies, including two of the six oil and gas "supermajors"—BP and Royal Dutch Shell—and BG Group.[315][316] In 2011, 40% of the UK's electricity was produced by gas, 30% by coal, 19% by nuclear power and 4.2% by wind, hydro, biofuels and wastes.[317]		In 2013, the UK produced 914 thousand barrels per day (bbl/d) of oil and consumed 1,507 thousand bbl/d.[318][319] Production is now in decline and the UK has been a net importer of oil since 2005.[320] In 2010[update] the UK had around 3.1 billion barrels of proven crude oil reserves, the largest of any EU member state.[320] In 2009, 66.5% of the UK's oil supply was imported.[321]		In 2009, the UK was the 13th-largest producer of natural gas in the world and the largest producer in the EU.[322] Production is now in decline and the UK has been a net importer of natural gas since 2004.[322] In 2009, half of British gas was supplied from imports as domestic reserves are depleted.[317]		Coal production played a key role in the UK economy in the 19th and 20th centuries. In the mid-1970s, 130 million tonnes of coal was being produced annually, not falling below 100 million tonnes until the early 1980s. During the 1980s and 1990s the industry was scaled back considerably. In 2011, the UK produced 18.3 million tonnes of coal.[323] In 2005 it had proven recoverable coal reserves of 171 million tons.[323] The UK Coal Authority has stated there is a potential to produce between 7 billion tonnes and 16 billion tonnes of coal through underground coal gasification (UCG) or 'fracking',[324] and that, based on current UK coal consumption, such reserves could last between 200 and 400 years.[325] However, environmental and social concerns have been raised over chemicals getting into the water table and minor earthquakes damaging homes.[326][327]		In the late 1990s, nuclear power plants contributed around 25% of total annual electricity generation in the UK, but this has gradually declined as old plants have been shut down and ageing-related problems affect plant availability. In 2012, the UK had 16 reactors normally generating about 19% of its electricity. All but one of the reactors will be retired by 2023. Unlike Germany and Japan, the UK intends to build a new generation of nuclear plants from about 2018.[317]		The total of all renewable electricity sources provided for 14.9% of the electricity generated in the United Kingdom in 2013,[328] reaching 53.7 TWh of electricity generated. The UK is one of the best sites in Europe for wind energy, and wind power production is its fastest growing supply, in 2014 it generated 9.3% of the UK's total electricity.[329][330][331]		Access to improved water supply and sanitation in the UK is universal. It is estimated that 96.7% of households are connected to the sewer network.[332] According to the Environment Agency, total water abstraction for public water supply in the UK was 16,406 megalitres per day in 2007.[333] In England and Wales the economic regulator of water companies is the Water Services Regulation Authority (Ofwat). The Environment Agency is responsible for environmental regulation, and the Drinking Water Inspectorate for regulating drinking water quality. The economic water industry regulator in Scotland is the Water Industry Commission for Scotland and the environmental regulator is the Scottish Environment Protection Agency. Drinking water standards and wastewater discharge standards in the UK, as in other countries of the European Union, are determined by the EU (see Water supply and sanitation in the European Union).		In England and Wales water and sewerage services are provided by 10 private regional water and sewerage companies and 13 mostly smaller private "water only" companies. In Scotland water and sewerage services are provided by a single public company, Scottish Water. In Northern Ireland water and sewerage services are also provided by a single public entity, Northern Ireland Water.		A census is taken simultaneously in all parts of the UK every ten years.[334] The Office for National Statistics is responsible for collecting data for England and Wales, the General Register Office for Scotland and the Northern Ireland Statistics and Research Agency each being responsible for censuses in their respective countries.[335] In the 2011 census the total population of the United Kingdom was 63,181,775.[336] It is the third-largest in the European Union, the fifth-largest in the Commonwealth and the 22nd-largest in the world. In mid-2014 and mid-2015 net long-term international migration contributed more to population growth. In mid-2012 and mid-2013 natural change contributed the most to population growth.[337] Between 2001 and 2011 the population increased by an average annual rate of approximately 0.7%.[336] This compares to 0.3% per year in the period 1991 to 2001 and 0.2% in the decade 1981 to 1991.[338] The 2011 census also confirmed that the proportion of the population aged 0–14 has nearly halved (31% in 1911 compared to 18 in 2011) and the proportion of older people aged 65 and over has more than tripled (from 5 to 16%).[336] It has been estimated that the number of people aged 100 or over will rise steeply to reach over 626,000 by 2080.[339]		England's population in 2011 was found to be 53 million.[340] It is one of the most densely populated countries in the world, with 420 people resident per square kilometre in mid-2015.[341] with a particular concentration in London and the south-east.[342] The 2011 census put Scotland's population at 5.3 million,[343] Wales at 3.06 million and Northern Ireland at 1.81 million.[340] In percentage terms England has had the fastest growing population of any country of the UK in the period from 2001 to 2011, with an increase of 7.9%.		In 2012 the average total fertility rate (TFR) across the UK was 1.92 children per woman.[344] While a rising birth rate is contributing to current population growth, it remains considerably below the 'baby boom' peak of 2.95 children per woman in 1964,[345] below the replacement rate of 2.1, but higher than the 2001 record low of 1.63.[344] In 2012, Scotland had the lowest TFR at only 1.67, followed by Wales at 1.88, England at 1.94, and Northern Ireland at 2.03.[344] In 2011, 47.3% of births in the UK were to unmarried women.[346] The Office for National Statistics published an "Experimental Official Statistics" bulletin in 2015 showing that, out of the UK population aged 16 and over, 1.7% identify as lesbian, gay, or bisexual (2.0% of males and 1.5% of females). 4.5% of respondents responded with "other", "I don't know", or did not respond.[347]				Greater Manchester Urban Area		West Yorkshire Urban Area		Historically, indigenous British people were thought to be descended from the various ethnic groups that settled there before the 11th century: the Celts, Romans, Anglo-Saxons, Norse and the Normans. Welsh people could be the oldest ethnic group in the UK.[351] A 2006 genetic study shows that more than 50% of England's gene pool contains Germanic Y chromosomes.[352] Another 2005 genetic analysis indicates that "about 75% of the traceable ancestors of the modern British population had arrived in the British isles by about 6,200 years ago, at the start of the British Neolithic or Stone Age", and that the British broadly share a common ancestry with the Basque people.[353][354][355]		The UK has a history of small-scale non-white immigration, with Liverpool having the oldest Black population in the country dating back to at least the 1730s during the period of the African slave trade. During this period it is estimated the Afro-Caribbean population of Great Britain was 10,000 which later declined due to the abolition of slavery .[356][357] The UK also has the oldest Chinese community in Europe, dating to the arrival of Chinese seamen in the 19th century.[358] In 1950 there were probably fewer than 20,000 non-white residents in Britain, almost all born overseas.[359]		Since 1948 substantial immigration from Africa, the Caribbean and South Asia has been a legacy of ties forged by the British Empire.[360] Migration from new EU member states in Central and Eastern Europe since 2004 has resulted in growth in these population groups, although some of this migration has been temporary.[361] Since the 1990s, there has been substantial diversification of the immigrant population, with migrants to the UK coming from a much wider range of countries than previous waves, which tended to involve larger numbers of migrants coming from a relatively small number of countries.[362][363][364]		Academics have argued that the ethnicity categories employed in British national statistics, which were first introduced in the 1991 census, involve confusion between the concepts of ethnicity and race.[365][366] In 2011[update], 87.2% of the UK population identified themselves as white, meaning 12.8% of the UK population identify themselves as of one of number of ethnic minority groups.[367] In the 2001 census, this figure was 7.9% of the UK population.[368]		Because of differences in the wording of the census forms used in England and Wales, Scotland and Northern Ireland, data on the Other White group is not available for the UK as a whole, but in England and Wales this was the fastest growing group between the 2001 and 2011 censuses, increasing by 1.1 million (1.8 percentage points).[369] Amongst groups for which comparable data is available for all parts of the UK level, there was considerable growth in the size of the Other Asian category, which increased from 0.4 to 1.4% of the population between 2001 and 2011.[367][368] There was also considerable growth in the Mixed category. In 2001, people in this category accounted for 1.2% of the UK population;[368] by 2011, the proportion was 2%.[367]		Ethnic diversity varies significantly across the UK. 30.4% of London's population and 37.4% of Leicester's was estimated to be non-white in 2005[update],[370][371] whereas less than 5% of the populations of North East England, Wales and the South West were from ethnic minorities, according to the 2001 census.[372] In 2016[update], 31.4% of primary and 27.9% of secondary pupils at state schools in England were members of an ethnic minority.[373]		The UK's de facto official language is English.[379][380] It is estimated that 95% of the UK's population are monolingual English speakers.[381] 5.5% of the population are estimated to speak languages brought to the UK as a result of relatively recent immigration.[381] South Asian languages, including Punjabi, Urdu, Hindi, Bengali, Tamil and Gujarati, are the largest grouping and are spoken by 2.7% of the UK population.[381] According to the 2011 census, Polish has become the second-largest language spoken in England and has 546,000 speakers.[382]		Four Celtic languages are spoken in the UK: Welsh, Irish, Scottish Gaelic and Cornish. All are recognised as regional or minority languages, subject to specific measures of protection and promotion under the European Charter for Regional or Minority Languages[2][383] and the Framework Convention for the Protection of National Minorities.[384] In the 2001 Census over a fifth (21%) of the population of Wales said they could speak Welsh,[385] an increase from the 1991 Census (18%).[386] In addition it is estimated that about 200,000 Welsh speakers live in England.[387] In the same census in Northern Ireland 167,487 people (10.4%) stated that they had "some knowledge of Irish" (see Irish language in Northern Ireland), almost exclusively in the nationalist (mainly Catholic) population. Over 92,000 people in Scotland (just under 2% of the population) had some Gaelic language ability, including 72% of those living in the Outer Hebrides.[388] The number of schoolchildren being taught through Welsh, Scottish Gaelic and Irish is increasing.[389] Among emigrant-descended populations some Scottish Gaelic is still spoken in Canada (principally Nova Scotia and Cape Breton Island),[390] and Welsh in Patagonia, Argentina.[391]		Scots, a language descended from early northern Middle English, has limited recognition alongside its regional variant, Ulster Scots in Northern Ireland, without specific commitments to protection and promotion.[2][392]		It is compulsory for pupils to study a second language up to the age of 14 in England,[393] and up to age 16 in Scotland. French and German are the two most commonly taught second languages in England and Scotland. All pupils in Wales are taught Welsh as a second language up to age 16, or are taught in Welsh.[394]		Forms of Christianity have dominated religious life in what is now the United Kingdom for over 1400 years.[395] Although a majority of citizens still identify with Christianity in many surveys, regular church attendance has fallen dramatically since the middle of the 20th century,[396] while immigration and demographic change have contributed to the growth of other faiths, most notably Islam.[397] This has led some commentators to variously describe the UK as a multi-faith,[398] secularised,[399] or post-Christian society.[400]		In the 2001 census 71.6% of all respondents indicated that they were Christians, with the next largest faiths being Islam (2.8%), Hinduism (1.0%), Sikhism (0.6%), Judaism (0.5%), Buddhism (0.3%) and all other religions (0.3%).[401] 15% of respondents stated that they had no religion, with a further 7% not stating a religious preference.[402] A Tearfund survey in 2007 showed only one in ten Britons actually attend church weekly.[403] Between the 2001 and 2011 census there was a decrease in the amount of people who identified as Christian by 12%, whilst the percentage of those reporting no religious affiliation doubled. This contrasted with growth in the other main religious group categories, with the number of Muslims increasing by the most substantial margin to a total of about 5%.[3] The Muslim population has increased from 1.6 million in 2001 to 2.7 million in 2011,[404] making it the second-largest religious group in the United Kingdom.[405]		In a 2015 survey conducted by BSA (British Social Attitudes) on religious affiliation; 49% of respondents indicated 'no religion', while 42% indicated they were Christians, followed by 8% who affiliated with other religions (e.g. Islam, Hinduism, Judaism, etc.).[406] Among Christians, adherents to the Church of England constituted 17%, Roman Catholic Church – 8%, other Christians (including Presbyterians, Methodists, other Protestants, as well as Eastern Orthodox) – 17%. Amid other religions, Islam accounted for 5%.[407][408]		The Church of England is the established church in England.[409] It retains a representation in the UK Parliament and the British monarch is its Supreme Governor.[410] In Scotland, the Church of Scotland is recognised as the national church. It is not subject to state control, and the British monarch is an ordinary member, required to swear an oath to "maintain and preserve the Protestant Religion and Presbyterian Church Government" upon his or her accession.[411][412] The Church in Wales was disestablished in 1920 and, as the Church of Ireland was disestablished in 1870 before the partition of Ireland, there is no established church in Northern Ireland.[413] Although there are no UK-wide data in the 2001 census on adherence to individual Christian denominations, it has been estimated that 62% of Christians are Anglican, 13.5% Catholic, 6% Presbyterian, 3.4% Methodist with small numbers of other Protestant denominations such as Open Brethren, and Orthodox churches.[414]		The United Kingdom has experienced successive waves of migration. The Great Famine in Ireland, then part of the United Kingdom, resulted in perhaps a million people migrating to Great Britain.[415] Throughout the 19th century a small population of German immigrants built up, numbering 28,644 in England and Wales in 1861. London held around half of this population, and other small communities existed in Manchester, Bradford and elsewhere. The German immigrant community was the largest group until 1891, when it became second only to Russian Jews.[416] England has had small Jewish communities for many centuries, subject to occasional expulsions, but British Jews numbered fewer than 10,000 at the start of the 19th century. After 1881 Russian Jews suffered bitter persecutions, and, out of some 2,000,000 who left Russia by 1914, around 120,000 settled permanently in Britain, overtaking the Germans to be the largest ethnic minority from outside the British Isles.[417][418] The population increasing to 370,000 in 1938.[419][420][421] Unable to return to Poland at the end of World War II, over 120,000 Polish veterans remained in the UK permanently.[422] After World War II, there was significant immigration from the colonies and newly independent former colonies, partly as a legacy of empire and partly driven by labour shortages. Many of these migrants came from the Caribbean and the Indian subcontinent.[423] In 1841, 0.25% of the population of England and Wales was born in a foreign country. In 1901, 1.5% of the population was foreign born.[424] By 1931, this figure had risen to 2.6%, and by 1951 it was 4.4%.[425]		In 2014 the net increase was 318,000: immigration was 641,000, up from 526,000 in 2013, while the number of people emigrating (for more than 12 months) was 323,000.[426] One of the more recent trends in migration has been the arrival of workers from the new EU member states in Eastern Europe, known as the A8 countries.[361] In 2010, there were 7.0 million foreign-born residents in the UK, corresponding to 11.3% of the total population. Of these, 4.76 million (7.7%) were born outside the EU and 2.24 million (3.6%) were born in another EU Member State.[427] The proportion of foreign-born people in the UK remains slightly below that of many other European countries.[428] However, immigration is now contributing to a rising population[429] with arrivals and UK-born children of migrants accounting for about half of the population increase between 1991 and 2001. Analysis of Office for National Statistics (ONS) data shows that a net total of 2.3 million migrants moved to the UK in the 15 years from 1991 to 2006.[430] In 2008 it was predicted that migration would add 7 million to the UK population by 2031,[431] though these figures are disputed.[432] The ONS reported that net migration rose from 2009 to 2010 by 21% to 239,000.[433]		In 2013, approximately 208,000 foreign citizens were naturalised as British citizens, the highest number since records began in 1962. This figure fell to around 125,800 in 2014. Between 2009 and 2013, the average number of people granted British citizenship per year was 195,800. The main countries of previous nationality of those naturalised in 2014 were India, Pakistan, the Philippines, Nigeria, Bangladesh, Nepal, China, South Africa, Poland and Somalia.[434] The total number of grants of settlement, which confers permanent residence in the UK without granting British citizenship,[435] was approximately 154,700 in 2013, compared to 241,200 in 2010 and 129,800 in 2012.[434]		Over a quarter (27.0%) of live births in 2014 were to mothers born outside the UK, according to official statistics released in 2015.[436]		Citizens of the European Union, including those of the UK, have the right to live and work in any EU member state.[442] The UK applied temporary restrictions to citizens of Romania and Bulgaria, which joined the EU in January 2007.[443] Research conducted by the Migration Policy Institute for the Equality and Human Rights Commission suggests that, between May 2004 and September 2009, 1.5 million workers migrated from the new EU member states to the UK, two-thirds of them Polish, but that many subsequently returned home, resulting in a net increase in the number of nationals of the new member states in the UK of some 700,000 over that period.[444][445] The late-2000s recession in the UK reduced the economic incentive for Poles to migrate to the UK,[446] the migration becoming temporary and circular.[447] In 2009, for the first time since enlargement, more nationals of the eight central and eastern European states that had joined the EU in 2004 left the UK than arrived.[448] In 2011, citizens of the new EU member states made up 13% of the immigrants entering the country.[449]		The British Government has introduced a points-based immigration system for immigration from outside the European Economic Area to replace former schemes, including the Scottish Government's Fresh Talent Initiative.[450] In June 2010 the government introduced a temporary limit of 24,000 on immigration from outside the EU, aiming to discourage applications before a permanent cap was imposed in April 2011.[451]		Emigration was an important feature of British society in the 19th century. Between 1815 and 1930 around 11.4 million people emigrated from Britain and 7.3 million from Ireland. Estimates show that by the end of the 20th century some 300 million people of British and Irish descent were permanently settled around the globe.[452] Today, at least 5.5 million UK-born people live abroad,[453][454][455] mainly in Australia, Spain, the United States and Canada.[453][456]		Education in the United Kingdom is a devolved matter, with each country having a separate education system. About 38 percent of the United Kingdom population has a university or college degree, which is the highest percentage in Europe, and among the highest percentages in the world.[457][458]		Whilst education in England is the responsibility of the Secretary of State for Education, the day-to-day administration and funding of state schools is the responsibility of local authorities.[459] Universally free of charge state education was introduced piecemeal between 1870 and 1944.[460][461] Education is now mandatory from ages five to sixteen, and in England youngsters must stay in education or training until they are 18.[462] In 2011, the Trends in International Mathematics and Science Study (TIMSS) rated 13–14-year-old pupils in England and Wales 10th in the world for maths and 9th for science.[463] The majority of children are educated in state-sector schools, a small proportion of which select on the grounds of academic ability. Two of the top ten performing schools in terms of GCSE results in 2006 were state-run grammar schools. In 2010, over half of places at the University of Oxford and the University of Cambridge were taken by students from state schools,[464] while the proportion of children in England attending private schools is around 7% which rises to 18% of those over 16.[465][466] England has the two oldest universities in English-speaking world, Universities of Oxford and Cambridge (jointly known as "Oxbridge") with history of over eight centuries. The United Kingdom trails only the United States in terms of representation on lists of top 100 universities.[467][468][469][470]		Education in Scotland is the responsibility of the Cabinet Secretary for Education and Lifelong Learning, with day-to-day administration and funding of state schools the responsibility of Local Authorities. Two non-departmental public bodies have key roles in Scottish education. The Scottish Qualifications Authority is responsible for the development, accreditation, assessment and certification of qualifications other than degrees which are delivered at secondary schools, post-secondary colleges of further education and other centres.[471] The Learning and Teaching Scotland provides advice, resources and staff development to education professionals.[472] Scotland first legislated for compulsory education in 1496.[473] The proportion of children in Scotland attending private schools is just over 4%, and it has been rising slowly in recent years.[474] Scottish students who attend Scottish universities pay neither tuition fees nor graduate endowment charges, as fees were abolished in 2001 and the graduate endowment scheme was abolished in 2008.[475]		The Welsh Government has responsibility for education in Wales. A significant number of Welsh students are taught either wholly or largely in the Welsh language; lessons in Welsh are compulsory for all until the age of 16.[476] There are plans to increase the provision of Welsh-medium schools as part of the policy of creating a fully bilingual Wales.		Education in Northern Ireland is the responsibility of the Minister of Education and the Minister for Employment and Learning, although responsibility at a local level is administered by five education and library boards covering different geographical areas. The Council for the Curriculum, Examinations & Assessment (CCEA) is the body responsible for advising the government on what should be taught in Northern Ireland's schools, monitoring standards and awarding qualifications.[477]		A government commission's report in 2014 found that privately educated people comprise 7% of the general population of the UK but much larger percentages of the top professions, the most extreme case quoted being 71% of senior judges.[478][479]		Healthcare in the United Kingdom is a devolved matter and each country has its own system of private and publicly funded health care, together with alternative, holistic and complementary treatments. Public healthcare is provided to all UK permanent residents and is mostly free at the point of need, being paid for from general taxation. The World Health Organization, in 2000, ranked the provision of healthcare in the United Kingdom as fifteenth best in Europe and eighteenth in the world.[480][481]		Regulatory bodies are organised on a UK-wide basis such as the General Medical Council, the Nursing and Midwifery Council and non-governmental-based, such as the Royal Colleges. However, political and operational responsibility for healthcare lies with four national executives; healthcare in England is the responsibility of the British Government; healthcare in Northern Ireland is the responsibility of the Northern Ireland Executive; healthcare in Scotland is the responsibility of the Scottish Government; and healthcare in Wales is the responsibility of the Welsh Government. Each National Health Service has different policies and priorities, resulting in contrasts.[482][483]		Since 1979 expenditure on healthcare has been increased significantly to bring it closer to the European Union average.[484] The UK spends around 8.4% of its gross domestic product on healthcare, which is 0.5 percentage points below the Organisation for Economic Co-operation and Development average and about one percentage point below the average of the European Union.[485]		The culture of the United Kingdom has been influenced by many factors including: the nation's island status; its history as a western liberal democracy and a major power; as well as being a political union of four countries with each preserving elements of distinctive traditions, customs and symbolism. As a result of the British Empire, British influence can be observed in the language, culture and legal systems of many of its former colonies including Australia, Canada, India, Ireland, New Zealand, Pakistan, South Africa and the United States. The substantial cultural influence of the United Kingdom has led it to be described as a "cultural superpower".[112][113]		'British literature' refers to literature associated with the United Kingdom, the Isle of Man and the Channel Islands. Most British literature is in the English language. In 2005, some 206,000 books were published in the United Kingdom and in 2006 it was the largest publisher of books in the world.[486]		The English playwright and poet William Shakespeare is widely regarded as the greatest dramatist of all time,[487][488][489] and his contemporaries Christopher Marlowe and Ben Jonson have also been held in continuous high esteem. More recently the playwrights Alan Ayckbourn, Harold Pinter, Michael Frayn, Tom Stoppard and David Edgar have combined elements of surrealism, realism and radicalism.		Notable pre-modern and early-modern English writers include Geoffrey Chaucer (14th century), Thomas Malory (15th century), Sir Thomas More (16th century), John Bunyan (17th century) and John Milton (17th century). In the 18th century Daniel Defoe (author of Robinson Crusoe) and Samuel Richardson were pioneers of the modern novel. In the 19th century there followed further innovation by Jane Austen, the gothic novelist Mary Shelley, the children's writer Lewis Carroll, the Brontë sisters, the social campaigner Charles Dickens, the naturalist Thomas Hardy, the realist George Eliot, the visionary poet William Blake and romantic poet William Wordsworth. 20th century English writers include the science-fiction novelist H. G. Wells; the writers of children's classics Rudyard Kipling, A. A. Milne (the creator of Winnie-the-Pooh), Roald Dahl and Enid Blyton; the controversial D. H. Lawrence; the modernist Virginia Woolf; the satirist Evelyn Waugh; the prophetic novelist George Orwell; the popular novelists W. Somerset Maugham and Graham Greene; the crime writer Agatha Christie (the best-selling novelist of all time);[490] Ian Fleming (the creator of James Bond); the poets T.S. Eliot, Philip Larkin and Ted Hughes; the fantasy writers J. R. R. Tolkien, C. S. Lewis and J. K. Rowling; the graphic novelists Alan Moore and Neil Gaiman.		Scotland's contributions include the detective writer Arthur Conan Doyle (the creator of Sherlock Holmes), romantic literature by Sir Walter Scott, the children's writer J. M. Barrie, the epic adventures of Robert Louis Stevenson and the celebrated poet Robert Burns. More recently the modernist and nationalist Hugh MacDiarmid and Neil M. Gunn contributed to the Scottish Renaissance. A more grim outlook is found in Ian Rankin's stories and the psychological horror-comedy of Iain Banks. Scotland's capital, Edinburgh, was UNESCO's first worldwide City of Literature.[491]		Britain's oldest known poem, Y Gododdin, was composed in Yr Hen Ogledd (The Old North), most likely in the late 6th century. It was written in Cumbric or Old Welsh and contains the earliest known reference to King Arthur.[492] From around the seventh century, the connection between Wales and the Old North was lost, and the focus of Welsh-language culture shifted to Wales, where Arthurian legend was further developed by Geoffrey of Monmouth.[493] Wales's most celebrated medieval poet, Dafydd ap Gwilym (fl.1320–1370), composed poetry on themes including nature, religion and especially love. He is widely regarded as one of the greatest European poets of his age.[494] Until the late 19th century the majority of Welsh literature was in Welsh and much of the prose was religious in character. Daniel Owen is credited as the first Welsh-language novelist, publishing Rhys Lewis in 1885. The best-known of the Anglo-Welsh poets are both Thomases. Dylan Thomas became famous on both sides of the Atlantic in the mid-20th century. He is remembered for his poetry—his "Do not go gentle into that good night; Rage, rage against the dying of the light" is one of the most quoted couplets of English language verse—and for his "play for voices", Under Milk Wood. The influential Church in Wales "poet-priest" and Welsh nationalist R. S. Thomas was nominated for the Nobel Prize in Literature in 1996. Leading Welsh novelists of the twentieth century include Richard Llewellyn and Kate Roberts.[495][496]		Authors of other nationalities, particularly from Commonwealth countries, the Republic of Ireland and the United States, have lived and worked in the UK. Significant examples through the centuries include Jonathan Swift, Oscar Wilde, Bram Stoker, George Bernard Shaw, Joseph Conrad, T.S. Eliot, Ezra Pound and more recently British authors born abroad such as Kazuo Ishiguro and Sir Salman Rushdie.[497][498]		Various styles of music are popular in the UK from the indigenous folk music of England, Wales, Scotland and Northern Ireland to heavy metal. Notable composers of classical music from the United Kingdom and the countries that preceded it include William Byrd, Henry Purcell, Sir Edward Elgar, Gustav Holst, Sir Arthur Sullivan (most famous for working with the librettist Sir W. S. Gilbert), Ralph Vaughan Williams and Benjamin Britten, pioneer of modern British opera. Sir Harrison Birtwistle is one of the foremost living composers. The UK is also home to world-renowned symphonic orchestras and choruses such as the BBC Symphony Orchestra and the London Symphony Chorus. Notable conductors include Sir Simon Rattle, Sir John Barbirolli and Sir Malcolm Sargent. Some of the notable film score composers include John Barry, Clint Mansell, Mike Oldfield, John Powell, Craig Armstrong, David Arnold, John Murphy, Monty Norman and Harry Gregson-Williams. George Frideric Handel became a naturalised British citizen and wrote the British coronation anthem, while some of his best works, such as Messiah, were written in the English language.[502][503] Andrew Lloyd Webber is a prolific composer of musical theatre. His works have dominated London's West End since the late 20th century and have also been a commercial success worldwide.[504]		The Beatles have international sales of over one billion units and are the biggest-selling and most influential band in the history of popular music.[499][500][501][505] Other prominent British contributors to have influenced popular music over the last 50 years include; The Rolling Stones, Pink Floyd, Queen, Led Zeppelin, the Bee Gees, and Elton John, all of whom have worldwide record sales of 200 million or more.[506][507][508][509][510][511] The Brit Awards are the BPI's annual music awards, and some of the British recipients of the Outstanding Contribution to Music award include; The Who, David Bowie, Eric Clapton, Rod Stewart and The Police.[512] More recent UK music acts that have had international success include Coldplay, Radiohead, Oasis, Spice Girls, Robbie Williams, Amy Winehouse and Adele.[513]		A number of UK cities are known for their music. Acts from Liverpool have had 54 UK chart number one hit singles, more per capita than any other city worldwide.[514] Glasgow's contribution to music was recognised in 2008 when it was named a UNESCO City of Music, one of only three cities in the world to have this honour.[515]		The history of British visual art forms part of western art history. Major British artists include: the Romantics William Blake, John Constable, Samuel Palmer and J.M.W. Turner; the portrait painters Sir Joshua Reynolds and Lucian Freud; the landscape artists Thomas Gainsborough and L. S. Lowry; the pioneer of the Arts and Crafts Movement William Morris; the figurative painter Francis Bacon; the Pop artists Peter Blake, Richard Hamilton and David Hockney; the collaborative duo Gilbert and George; the abstract artist Howard Hodgkin; and the sculptors Antony Gormley, Anish Kapoor and Henry Moore. During the late 1980s and 1990s the Saatchi Gallery in London helped to bring to public attention a group of multi-genre artists who would become known as the "Young British Artists": Damien Hirst, Chris Ofili, Rachel Whiteread, Tracey Emin, Mark Wallinger, Steve McQueen, Sam Taylor-Wood and the Chapman Brothers are among the better-known members of this loosely affiliated movement.		The Royal Academy in London is a key organisation for the promotion of the visual arts in the United Kingdom. Major schools of art in the UK include: the six-school University of the Arts London, which includes the Central Saint Martins College of Art and Design and Chelsea College of Art and Design; Goldsmiths, University of London; the Slade School of Fine Art (part of University College London); the Glasgow School of Art; the Royal College of Art; and The Ruskin School of Drawing and Fine Art (part of the University of Oxford). The Courtauld Institute of Art is a leading centre for the teaching of the history of art. Important art galleries in the United Kingdom include the National Gallery, National Portrait Gallery, Tate Britain and Tate Modern (the most-visited modern art gallery in the world, with around 4.7 million visitors per year).[516]		The United Kingdom has had a considerable influence on the history of the cinema. The British directors Alfred Hitchcock, whose film Vertigo is considered by some critics as the best film of all time,[518] and David Lean are among the most critically acclaimed of all-time.[519] Other important directors including Charlie Chaplin,[520] Michael Powell,[521] Carol Reed[522] Christopher Nolan[523] and Ridley Scott.[524] Many British actors have achieved international fame and critical success, including: Julie Andrews,[525] Richard Burton,[526] Michael Caine,[527] Charlie Chaplin,[528] Sean Connery,[529] Vivien Leigh,[530] David Niven,[531] Laurence Olivier,[532] Peter Sellers,[533] Kate Winslet,[534] Anthony Hopkins,[535] and Daniel Day-Lewis.[536] Some of the most commercially successful films of all time have been produced in the United Kingdom, including two of the highest-grossing film franchises (Harry Potter and James Bond).[537] Ealing Studios has a claim to being the oldest continuously working film studio in the world.[538]		Despite a history of important and successful productions, the industry has often been characterised by a debate about its identity and the level of American and European influence. British producers are active in international co-productions and British actors, directors and crew feature regularly in American films. Many successful Hollywood films have been based on British people, stories or events, including Titanic, The Lord of the Rings, Pirates of the Caribbean.		In 2009, British films grossed around $2 billion worldwide and achieved a market share of around 7% globally and 17% in the United Kingdom.[539] UK box-office takings totalled £944 million in 2009, with around 173 million admissions.[539] The British Film Institute has produced a poll ranking of what it considers to be the 100 greatest British films of all time, the BFI Top 100 British films.[540] The annual British Academy Film Awards are hosted by the British Academy of Film and Television Arts.[541]		The BBC, founded in 1922, is the UK's publicly funded radio, television and Internet broadcasting corporation, and is the oldest and largest broadcaster in the world.[542][543][544] It operates numerous television and radio stations in the UK and abroad and its domestic services are funded by the television licence.[545][546] Other major players in the UK media include ITV plc, which operates 11 of the 15 regional television broadcasters that make up the ITV Network,[547] and News Corporation, which owns a number of national newspapers through News International such as the most popular tabloid The Sun and the longest-established daily "broadsheet" The Times,[548] as well as holding a large stake in satellite broadcaster British Sky Broadcasting.[549] London dominates the media sector in the UK: national newspapers and television and radio are largely based there, although Manchester is also a significant national media centre. Edinburgh and Glasgow, and Cardiff, are important centres of newspaper and broadcasting production in Scotland and Wales respectively.[550] The UK publishing sector, including books, directories and databases, journals, magazines and business media, newspapers and news agencies, has a combined turnover of around £20 billion and employs around 167,000 people.[551]		In 2009, it was estimated that individuals viewed a mean of 3.75 hours of television per day and 2.81 hours of radio. In that year the main BBC public service broadcasting channels accounted for an estimated 28.4% of all television viewing; the three main independent channels accounted for 29.5% and the increasingly important other satellite and digital channels for the remaining 42.1%.[552] Sales of newspapers have fallen since the 1970s and in 2010 41% of people reported reading a daily national newspaper.[553] In 2010, 82.5% of the UK population were Internet users, the highest proportion amongst the 20 countries with the largest total number of users in that year.[554]		The United Kingdom is famous for the tradition of 'British Empiricism', a branch of the philosophy of knowledge that states that only knowledge verified by experience is valid, and 'Scottish Philosophy', sometimes referred to as the 'Scottish School of Common Sense'.[555] The most famous philosophers of British Empiricism are John Locke, George Berkeley[note 20] and David Hume; while Dugald Stewart, Thomas Reid and William Hamilton were major exponents of the Scottish "common sense" school. Two Britons are also notable for a theory of moral philosophy utilitarianism, first used by Jeremy Bentham and later by John Stuart Mill in his short work Utilitarianism.[556][557] Other eminent philosophers from the UK and the unions and countries that preceded it include Duns Scotus, John Lilburne, Mary Wollstonecraft, Sir Francis Bacon, Adam Smith, Thomas Hobbes, William of Ockham, Bertrand Russell and A.J. "Freddie" Ayer. Foreign-born philosophers who settled in the UK include Isaiah Berlin, Karl Marx, Karl Popper and Ludwig Wittgenstein.		Major sports, including association football, tennis, rugby union, rugby league, golf, boxing, netball, rowing and cricket, originated or were substantially developed in the UK and the states that preceded it. With the rules and codes of many modern sports invented and codified in late 19th century Victorian Britain, in 2012, the President of the IOC, Jacques Rogge, stated; "This great, sports-loving country is widely recognized as the birthplace of modern sport. It was here that the concepts of sportsmanship and fair play were first codified into clear rules and regulations. It was here that sport was included as an educational tool in the school curriculum".[559][560]		In most international competitions, separate teams represent England, Scotland and Wales. Northern Ireland and the Republic of Ireland usually field a single team representing all of Ireland, with notable exceptions being association football and the Commonwealth Games. In sporting contexts, the English, Scottish, Welsh and Irish / Northern Irish teams are often referred to collectively as the Home Nations. There are some sports in which a single team represents the whole of United Kingdom, including the Olympics, where the UK is represented by the Great Britain team. The 1908, 1948 and 2012 Summer Olympics were held in London, making it the first city to host the games three times. Britain has participated in every modern Olympic Games to date and is third in the medal count.		A 2003 poll found that football is the most popular sport in the United Kingdom.[561] England is recognised by FIFA as the birthplace of club football, and The Football Association is the oldest of its kind, with the rules of football first drafted in 1863 by Ebenezer Cobb Morley.[562][563] Each of the Home Nations has its own football association, national team and league system. The English top division, the Premier League, is the most watched football league in the world.[564] The first-ever international football match was contested by England and Scotland on 30 November 1872.[565] England, Scotland, Wales and Northern Ireland compete as separate countries in international competitions.[566] A Great Britain Olympic football team was assembled for the first time to compete in the London 2012 Olympic Games. However, the Scottish, Welsh and Northern Irish football associations declined to participate, fearing that it would undermine their independent status—a fear confirmed by FIFA.[567]		In 2003, rugby union was ranked the second most popular sport in the UK.[561] The sport was created in Rugby School, Warwickshire, and the first rugby international took place on 27 March 1871 between England and Scotland.[568][569] England, Scotland, Wales, Ireland, France and Italy compete in the Six Nations Championship; the premier international tournament in the northern hemisphere. Sport governing bodies in England, Scotland, Wales and Ireland organise and regulate the game separately.[570] If any of the British teams or the Irish team beat the other three in a tournament, then it is awarded the Triple Crown.[571]		Cricket was invented in England, and its laws were established by Marylebone Cricket Club in 1788.[572] The England cricket team, controlled by the England and Wales Cricket Board,[573] is the only national team in the UK with Test status. Team members are drawn from the main county sides, and include both English and Welsh players. Cricket is distinct from football and rugby where Wales and England field separate national teams, although Wales had fielded its own team in the past. Irish and Scottish players have played for England because neither Scotland nor Ireland have Test status and have only recently started to play in One Day Internationals.[574][575] Scotland, England (and Wales), and Ireland (including Northern Ireland) have competed at the Cricket World Cup, with England reaching the finals on three occasions. There is a professional league championship in which clubs representing 17 English counties and 1 Welsh county compete.[576]		The modern game of tennis originated in Birmingham, England, in the 1860s, before spreading around the world.[577] The world's oldest tennis tournament, the Wimbledon championships, first occurred in 1877, and today the event takes place over two weeks in late June and early July.[578]		Thoroughbred racing, which originated under Charles II of England as the "sport of kings", is popular throughout the UK with world-famous races including the Grand National, the Epsom Derby, Royal Ascot and the Cheltenham National Hunt Festival (including the Cheltenham Gold Cup). The UK has proved successful in the international sporting arena in rowing.		The UK is closely associated with motorsport. Many teams and drivers in Formula One (F1) are based in the UK, and the country has won more drivers' and constructors' titles than any other. The UK hosted the first F1 Grand Prix in 1950 at Silverstone, the current location of the British Grand Prix held each year in July.[579] The UK hosts legs of the Grand Prix motorcycle racing, World Rally Championship and FIA World Endurance Championship. The premier national auto racing event is the British Touring Car Championship. Motorcycle road racing has a long tradition with races such as the Isle of Man TT and the North West 200.		Golf is the sixth most popular sport, by participation, in the UK. Although The Royal and Ancient Golf Club of St Andrews in Scotland is the sport's home course,[581] the world's oldest golf course is actually Musselburgh Links' Old Golf Course.[582] In 1764, the standard 18-hole golf course was created at St Andrews when members modified the course from 22 to 18 holes.[580] The oldest golf tournament in the world, and the first major championship in golf, The Open Championship, is played annually on the weekend of the third Friday in July.[583]		Rugby league originated in Huddersfield, West Yorkshire in 1895 and is generally played in Northern England.[584] A single 'Great Britain Lions' team had competed in the Rugby League World Cup and Test match games, but this changed in 2008 when England, Scotland and Ireland competed as separate nations.[585] Great Britain is still retained as the full national team. Super League is the highest level of professional rugby league in the UK and Europe. It consists of 11 teams from Northern England, 1 from London, 1 from Wales and 1 from France.[586]		The 'Queensberry rules', the code of general rules in boxing, was named after John Douglas, 9th Marquess of Queensberry in 1867, that formed the basis of modern boxing.[587] Snooker is another of the UK's popular sporting exports, with the world championships held annually in Sheffield.[588] In Northern Ireland Gaelic football and hurling are popular team sports, both in terms of participation and spectating, and Irish expatriates in the UK and the US also play them.[589] Shinty (or camanachd) is popular in the Scottish Highlands.[590] Highland games are held in spring and summer in Scotland, celebrating Scottish and celtic culture and heritage, especially that of the Scottish Highlands.[591]		The flag of the United Kingdom is the Union Flag (also referred to as the Union Jack). It was created in 1606 by the superimposition of the Flag of England on the Flag of Scotland and updated in 1801 with the addition of Saint Patrick's Flag. Wales is not represented in the Union Flag, as Wales had been conquered and annexed to England prior to the formation of the United Kingdom. The possibility of redesigning the Union Flag to include representation of Wales has not been completely ruled out.[592] The national anthem of the United Kingdom is "God Save the King", with "King" replaced with "Queen" in the lyrics whenever the monarch is a woman.		Britannia is a national personification of the United Kingdom, originating from Roman Britain.[593] Britannia is symbolised as a young woman with brown or golden hair, wearing a Corinthian helmet and white robes. She holds Poseidon's three-pronged trident and a shield, bearing the Union Flag. Sometimes she is depicted as riding on the back of a lion. Since the height of the British Empire in the late 19th century, Britannia has often been associated with British maritime dominance, as in the patriotic song "Rule, Britannia!". Up until 2008, the lion symbol was depicted behind Britannia on the British fifty pence coin and on the back of the British ten pence coin. It is also used as a symbol on the non-ceremonial flag of the British Army.		A second, less used, personification of the nation is the character John Bull. The bulldog is sometimes used as a symbol of the United Kingdom and has been associated with Winston Churchill's defiance of Nazi Germany.[594]		The following are international rankings of the United Kingdom, including those measuring life quality, health care quality, stability, press freedom and income.		United Kingdom – Wikipedia book		The full title of this country is 'the United Kingdom of Great Britain and Northern Ireland'. Great Britain is made up of England, Scotland and Wales. The United Kingdom (UK) is made up of England, Scotland, Wales and Northern Ireland. 'Britain' is used informally, usually meaning the United Kingdom. The Channel Islands and the Isle of Man are not part of the UK.		 Germany		
The wise fool, or the wisdom of the fool is a theme that is an oxymoron in which the fool may have an attribute of wisdom. With probable beginnings early in the civilizing process, the concept developed during the Middle Ages when there was a rise of "civilizing" factors (such as the advent of certain practices of manners in Western Europe) and achieved its most pronounced state in the Renaissance. The wisdom of the fool occupies a place in opposition to that of learned knowledge.		Innocuous fools have often enjoyed special privileges in cultural and economic groups, whereas aggressive madmen have had to be restrained or incarcerated. A fool's powerlessness and helplessness may gain him or her protection of more fortunate people. Since the fool is only guided by his natural instincts, because he or she does not understand social conventions, he or she is not culpable for breaches of those rules. The fool is not expected to "know better" or "know" anything.		Because of this, the fool has often been given great relative freedom, particularly in speech. The advantage of speaking with exemption from punishment has made the fool attractive in the literary imagination, for example, The Fool in Shakespeare's King Lear. Lear's fool is one of only three people in the play who consistently speak to him wisely, and the other two, Cordelia and the Earl of Kent, are punished severely.		Though the fool is in a position separated from normal society which can cause him or her to be subjected to deriding acts and contemptuous treatment, it has also at times caused him or her to be regarded with respect and reverence — the holy fool. In the Middle Ages, and in some primitive societies, the fool was thought to be under the protection of God and in possession of "Godly imparted tongues".[citation needed]		Another aspect of the wise fool may be shown in his or her rejection of the norms of the culture in which he or she lives, if he or she deems those norms to be counterproductive. This might make him or her appear foolish. As Sam Keen says: "To call a man a fool is not necessarily an insult, for the authentic life has frequently been pictured under the metaphor of the fool. In figures such as Socrates, Christ, and the Idiot of Dostoyevsky we see that foolishness and wisdom are not always what they seem to be.[1]		Patchface from A Song of Ice and Fire is a fool to King Stannis Baratheon who was the only survivor of a shipwreck that killed Stannis' parents. As a result, he was apparently driven mad and makes seemingly nonsensical statements. However, his words seem to prophecy significant events in the series, such as the Red Wedding.		
The Bahá'í Faith (Persian: بهائی‎‎ Bahā'i) is an Abrahamic religion teaching the essential worth of all religions, and the unity and equality of all people.[1] Established by Bahá'u'lláh in 1863, it initially grew in the Middle East and now has between 5-7 million adherents, known as Bahá'ís, spread out into most of the world's countries and territories, with the highest concentrations in India and Iran.[2][3]		The religion was born in Iran, where it has faced ongoing persecutions since its inception.[4] It grew from the mid-19th century Bábí religion, whose founder reinterpreted Shia Islam and said that God would soon send a prophet in the manner of Jesus or Muhammad.[5] In 1863, after being banished from his native Iran, Bahá'u'lláh announced that he was this prophet. He was further exiled, spending over a decade in the prison city of Akka in the Ottoman province of Syria, in what is now Israel. Following Bahá'u'lláh's death in 1892, leadership of the religion fell to his son `Abdu'l-Bahá (1844-1921), and later his great-grandson Shoghi Effendi (1897-1957). Bahá'ís around the world annually elect local, regional, and national Spiritual Assemblies that govern the affairs of the religion, and every five years the members of all National Spiritual Assemblies elect the Universal House of Justice, the nine-member supreme governing institution of the worldwide Bahá'í community, which sits in Haifa, Israel near the shrine of Báb.		Bahá'í teachings are in some ways similar to other monotheistic faiths: God is considered single and all-powerful. However, Bahá'u'lláh taught that religion is orderly and progressively revealed by one God through Manifestations of God who are the founders of major world religions throughout history; Buddha, Jesus, and Muhammad being the most recent in the period before the Báb and Bahá'u'lláh. As such, Bahá'ís regard the major religions as fundamentally unified in purpose, though varied in social practices and interpretations. There is a similar emphasis on the unity of all people, openly rejecting notions of racism and nationalism. At the heart of Bahá'í teachings is the goal of a unified world order that ensures the prosperity of all nations, races, creeds, and classes.[6][7]		Letters written by Bahá'u'lláh to various individuals, including some heads of state, have been collected and canonized into a body of Bahá'í scripture that includes works by his son `Abdu'l-Bahá, and also the Báb, who is regarded as Bahá'u'lláh's forerunner. Prominent among Bahá'í literature are the Kitáb-i-Aqdas, Kitáb-i-Íqán, Some Answered Questions, and The Dawn-Breakers.						In English-language use, the word Bahá'í  is used either as an adjective to refer to the Bahá'í Faith or as a term for a follower of Bahá'u'lláh. The word is not a noun meaning the religion as a whole.[8] It is derived from the Arabic Bahá' (بهاء), meaning "glory" or "splendor", although the word Bahá'í  is actually derived from its use as a loan word in Persian, in particular the "'i" suffix is Persian rather than Arabic.[note 1] The term "Bahaism" (or "Baha'ism") is still used, mainly in a pejorative sense.[9][10]		Three core principles establish a basis for Bahá'í teachings and doctrine: the unity of God, the unity of religion, and the unity of humanity.[11] From these postulates stems the belief that God periodically reveals his will through divine messengers, whose purpose is to transform the character of humankind and to develop, within those who respond, moral and spiritual qualities. Religion is thus seen as orderly, unified, and progressive from age to age.[12]		The Bahá'í writings describe a single, personal, inaccessible, omniscient, omnipresent, imperishable, and almighty God who is the creator of all things in the universe.[13] The existence of God and the universe is thought to be eternal, without a beginning or end.[14] Though inaccessible directly, God is nevertheless seen as conscious of creation, with a will and purpose that is expressed through messengers termed Manifestations of God.[15][16]		Bahá'í teachings state that God is too great for humans to fully comprehend, or to create a complete and accurate image of, by themselves. Therefore, human understanding of God is achieved through his revelations via his Manifestations.[17][18] In the Bahá'í religion, God is often referred to by titles and attributes (for example, the All-Powerful, or the All-Loving), and there is a substantial emphasis on monotheism; such doctrines as the Trinity are seen as compromising, if not contradicting, the Bahá'í view that God is single and has no equal.[19] The Bahá'í teachings state that the attributes which are applied to God are used to translate Godliness into human terms and also to help individuals concentrate on their own attributes in worshipping God to develop their potentialities on their spiritual path.[17][18] According to the Bahá'í teachings the human purpose is to learn to know and love God through such methods as prayer, reflection, and being of service to others.[17]		Bahá'í notions of progressive religious revelation result in their accepting the validity of the well known religions of the world, whose founders and central figures are seen as Manifestations of God. Religious history is interpreted as a series of dispensations, where each manifestation brings a somewhat broader and more advanced revelation that is rendered as a text of scripture and passed on through history with greater or lesser reliability but at least true in substance,[20] suited for the time and place in which it was expressed.[14] Specific religious social teachings (for example, the direction of prayer, or dietary restrictions) may be revoked by a subsequent manifestation so that a more appropriate requirement for the time and place may be established. Conversely, certain general principles (for example, neighbourliness, or charity) are seen to be universal and consistent. In Bahá'í belief, this process of progressive revelation will not end; however, it is believed to be cyclical. Bahá'ís do not expect a new manifestation of God to appear within 1000 years of Bahá'u'lláh's revelation.[21]		Bahá'í beliefs are sometimes described as syncretic combinations of earlier religious beliefs.[22] Bahá'ís, however, assert that their religion is a distinct tradition with its own scriptures, teachings, laws, and history.[14][23] While the religion was initially seen as a sect of Islam, most religious specialists now see it as an independent religion, with its religious background in Shi'a Islam being seen as analogous to the Jewish context in which Christianity was established.[24] Muslim institutions and clergy, both Sunni and Shia, consider Bahá'ís to be deserters or apostates from Islam, which has led to Bahá'ís being persecuted.[25][26] Bahá'ís describe their faith as an independent world religion, differing from the other traditions in its relative age and in the appropriateness of Bahá'u'lláh's teachings to the modern context.[27] Bahá'u'lláh is believed to have fulfilled the messianic expectations of these precursor faiths.[28]		The Bahá'í writings state that human beings have a "rational soul", and that this provides the species with a unique capacity to recognize God's station and humanity's relationship with its creator. Every human is seen to have a duty to recognize God through His messengers, and to conform to their teachings.[29] Through recognition and obedience, service to humanity and regular prayer and spiritual practice, the Bahá'í writings state that the soul becomes closer to God, the spiritual ideal in Bahá'í belief. When a human dies, the soul passes into the next world, where its spiritual development in the physical world becomes a basis for judgment and advancement in the spiritual world. Heaven and Hell are taught to be spiritual states of nearness or distance from God that describe relationships in this world and the next, and not physical places of reward and punishment achieved after death.[30]		The Bahá'í writings emphasize the essential equality of human beings, and the abolition of prejudice. Humanity is seen as essentially one, though highly varied; its diversity of race and culture are seen as worthy of appreciation and acceptance. Doctrines of racism, nationalism, caste, social class, and gender-based hierarchy are seen as artificial impediments to unity.[11] The Bahá'í teachings state that the unification of humanity is the paramount issue in the religious and political conditions of the present world.[14]		Shoghi Effendi, the Guardian of the religion from 1921 to 1957, wrote the following summary of what he considered to be the distinguishing principles of Bahá'u'lláh's teachings, which, he said, together with the laws and ordinances of the Kitáb-i-Aqdas constitute the bedrock of the Bahá'í Faith:		The independent search after truth, unfettered by superstition or tradition; the oneness of the entire human race, the pivotal principle and fundamental doctrine of the Faith; the basic unity of all religions; the condemnation of all forms of prejudice, whether religious, racial, class or national; the harmony which must exist between religion and science; the equality of men and women, the two wings on which the bird of human kind is able to soar; the introduction of compulsory education; the adoption of a universal auxiliary language; the abolition of the extremes of wealth and poverty; the institution of a world tribunal for the adjudication of disputes between nations; the exaltation of work, performed in the spirit of service, to the rank of worship; the glorification of justice as the ruling principle in human society, and of religion as a bulwark for the protection of all peoples and nations; and the establishment of a permanent and universal peace as the supreme goal of all mankind—these stand out as the essential elements [which Bahá'u'lláh proclaimed].[31]		The following principles are frequently listed as a quick summary of the Bahá'í teachings. They are derived from transcripts of speeches given by `Abdu'l-Bahá during his tour of Europe and North America in 1912.[32][33] The list is not authoritative and a variety of such lists circulate.[23][34]		With specific regard to the pursuit of world peace, Bahá'u'lláh prescribed a world-embracing collective security arrangement for the establishment of a temporary era of peace referred to in the Baha'i teachings as the Lesser Peace. For the establishment of a lasting peace (The Most Great Peace) and the purging of the 'overwhelming Corruptions' it is necessary that all the people of the world universally unite under a universal Faith.[35]		The Bahá'í teachings speak of both a "Greater Covenant",[36] being universal and endless, and a "Lesser Covenant", being unique to each religious dispensation. The Lesser Covenant is viewed as an agreement between a Messenger of God and his followers and includes social practices and the continuation of authority in the religion. At this time Bahá'ís view Bahá'u'lláh's revelation as a binding lesser covenant for his followers; in the Bahá'í writings being firm in the covenant is considered a virtue to work toward.[37] The Greater Covenant is viewed as a more enduring agreement between God and humanity, where a Manifestation of God is expected to come to humanity about every thousand years, at times of turmoil and uncertainty. With unity as an essential teaching of the religion, Bahá'ís follow an administration they believe is divinely ordained, and therefore see attempts to create schisms and divisions as efforts that are contrary to the teachings of Bahá'u'lláh. Schisms have occurred over the succession of authority, but any Bahá'í divisions have had relatively little success and have failed to attract a sizeable following.[38] The followers of such divisions are regarded as Covenant-breakers and shunned, essentially excommunicated.[37][39]		The canonical texts are the writings of the Báb, Bahá'u'lláh, `Abdu'l-Bahá, Shoghi Effendi and the Universal House of Justice, and the authenticated talks of `Abdu'l-Bahá. The writings of the Báb and Bahá'u'lláh are considered as divine revelation, the writings and talks of `Abdu'l-Bahá and the writings of Shoghi Effendi as authoritative interpretation, and those of the Universal House of Justice as authoritative legislation and elucidation. Some measure of divine guidance is assumed for all of these texts.[40] Some of Bahá'u'lláh's most important writings include the Kitáb-i-Aqdas, literally the Most Holy Book, which defines many laws and practices for individuals and society,[41] the Kitáb-i-Íqán, literally the Book of Certitude, which became the foundation of much of Bahá'í belief,[42] the Gems of Divine Mysteries, which includes further doctrinal foundations, and the Seven Valleys and the Four Valleys which are mystical treatises.[43]		Although the Bahá'í teachings have a strong emphasis on social and ethical issues, there exist a number of foundational texts that have been described as mystical.[14] The Seven Valleys is considered Bahá'u'lláh's "greatest mystical composition." It was written to a follower of Sufism, in the style of  `Attar, The Persian Muslim poet,[44] and sets forth the stages of the soul's journey towards God. It was first translated into English in 1906, becoming one of the earliest available books of Bahá'u'lláh to the West. The Hidden Words is another book written by Bahá'u'lláh during the same period, containing 153 short passages in which Bahá'u'lláh claims to have taken the basic essence of certain spiritual truths and written them in brief form.[45]		The Bahá'í Faith formed from the Persian religion of the Báb, a merchant who began preaching a new interpretation of Shia Islam in 1844. The Báb's claim to divine revelation was rejected by the generality of Islamic clergy in Iran, ending in his public execution by authorities in 1850. The Báb taught that God would soon send a new messenger, and Bahá'ís consider Bahá'u'lláh to be that person.[5] Although they are distinct movements, the Báb is so interwoven into Bahá'í theology and history that Bahá'ís celebrate his birth, death, and declaration as holy days, consider him one of their three central figures (along with Bahá'u'lláh and `Abdu'l-Bahá), and a historical account of the Bábí movement (The Dawn-Breakers) is considered one of three books that every Bahá'í should "master" and read "over and over again".[46]		The Bahá'í community was mostly confined to the Persian and Ottoman empires until after the death of Bahá'u'lláh in 1892, at which time he had followers in 13 countries of Asia and Africa.[47] Under the leadership of his son, `Abdu'l-Bahá, the religion gained a footing in Europe and America, and was consolidated in Iran, where it still suffers intense persecution.[48] After the death of `Abdu'l-Bahá in 1921, the leadership of the Bahá'í community entered a new phase, evolving from a single individual to a administrative order with both elected bodies and appointed individuals.[49]		On the evening of 22 May 1844, Siyyid `Alí-Muhammad of Shiraz proclaimed that he was "the Báb" (الباب "the Gate"), referring to his later claim to the station of Mahdi, the Twelfth Imam of Shi`a Islam.[48] His followers were therefore known as Bábís. As the Báb's teachings spread, which the Islamic clergy saw as a threat, his followers came under increased persecution and torture.[14] The conflicts escalated in several places to military sieges by the Shah's army. The Báb himself was imprisoned and eventually executed in 1850.[50]		Bahá'ís see the Báb as the forerunner of the Bahá'í Faith, because the Báb's writings introduced the concept of "He whom God shall make manifest", a Messianic figure whose coming, according to Bahá'ís, was announced in the scriptures of all of the world's great religions, and whom Bahá'u'lláh, the founder of the Bahá'í Faith, claimed to be in 1863.[14] The Báb's tomb, located in Haifa, Israel, is an important place of pilgrimage for Bahá'ís. The remains of the Báb were brought secretly from Iran to the Holy Land and eventually interred in the tomb built for them in a spot specifically designated by Bahá'u'lláh.[51] The main written works translated into English of the Báb's are collected in Selections from the Writings of the Báb out of the estimated 135 works.[52]		Mírzá Husayn `Alí Núrí was one of the early followers of the Báb, and later took the title of Bahá'u'lláh. Bábís faced a period of persecution that peaked in 1852-53 after a few individuals made a failed attempt to assassinate the Shah. Although they acted alone, the government responded with collective punishment, killing many Bábís. Bahá'u'lláh was put in prison. He claimed that in 1853, while incarcerated in the dungeon of the Síyáh-Chál in Tehran, he received the first intimations that he was the one anticipated by the Báb when he received a visit from the Maid of Heaven.[11]		Shortly thereafter he was expelled from Tehran to Baghdad, in the Ottoman Empire;[11] then to Constantinople (now Istanbul); and then to Adrianople (now Edirne). In 1863, at the time of his banishment from Baghdad to Constantinople, Bahá'u'lláh declared his claim to a divine mission to his family and followers. Tensions then grew between him and Subh-i-Azal, the appointed leader of the Bábís who did not recognize Bahá'u'lláh's claim. Throughout the rest of his life Bahá'u'lláh gained the allegiance of most of the Bábís, who came to be known as Bahá'ís. Beginning in 1866, he began declaring his mission as a Messenger of God in letters to the world's religious and secular rulers, including Pope Pius IX, Napoleon III, and Queen Victoria.		Bahá'u'lláh was banished by Sultan Abdülâziz a final time in 1868 to the Ottoman penal colony of `Akká, in present-day Israel. Towards the end of his life, the strict and harsh confinement was gradually relaxed, and he was allowed to live in a home near `Akká, while still officially a prisoner of that city.[53] He died there in 1892. Bahá'ís regard his resting place at Bahjí as the Qiblih to which they turn in prayer each day.[54]		`Abbás Effendi was Bahá'u'lláh's eldest son, known by the title of `Abdu'l-Bahá (Servant of Bahá). His father left a Will that appointed `Abdu'l-Bahá as the leader of the Bahá'í community, and designated him as the "Centre of the Covenant", "Head of the Faith", and the sole authoritative interpreter of Bahá'u'lláh's writings.[51][55] `Abdu'l-Bahá had shared his father's long exile and imprisonment, which continued until `Abdu'l-Bahá's own release as a result of the Young Turk Revolution in 1908. Following his release he led a life of travelling, speaking, teaching, and maintaining correspondence with communities of believers and individuals, expounding the principles of the Bahá'í Faith.[11]		There are over 27,000 extant documents by `Abdu'l-Bahá, mostly letters, of which only a fraction have been translated into English.[52] Among the more well known are The Secret of Divine Civilization, the Tablet to Auguste-Henri Forel, and Some Answered Questions. Additionally notes taken of a number of his talks were published in various volumes like Paris Talks during his journeys to the West.		Bahá'u'lláh's Kitáb-i-Aqdas and The Will and Testament of `Abdu'l-Bahá are foundational documents of the Bahá'í administrative order. Bahá'u'lláh established the elected Universal House of Justice, and `Abdu'l-Bahá established the appointed hereditary Guardianship and clarified the relationship between the two institutions.[51][56] In his Will, `Abdu'l-Bahá appointed his eldest grandson, Shoghi Effendi, as the first Guardian of the Bahá'í Faith, serving as head of the religion until his death, for 36 years.[57]		Shoghi Effendi throughout his lifetime translated Bahá'í texts; developed global plans for the expansion of the Bahá'í community; developed the Bahá'í World Centre; carried on a voluminous correspondence with communities and individuals around the world; and built the administrative structure of the religion, preparing the community for the election of the Universal House of Justice.[11] He died in 1957 under conditions that did not allow for a successor to be appointed.[58][59]		At local, regional, and national levels, Bahá'ís elect members to nine-person Spiritual Assemblies, which run the affairs of the religion. There are also appointed individuals working at various levels, including locally and internationally, which perform the function of propagating the teachings and protecting the community. The latter do not serve as clergy, which the Bahá'í Faith does not have.[14][60] The Universal House of Justice, first elected in 1963, remains the successor and supreme governing body of the Bahá'í Faith, and its 9 members are elected every five years by the members of all National Spiritual Assemblies.[61] Any male Bahá'í, 21 years or older, is eligible to be elected to the Universal House of Justice; all other positions are open to male and female Bahá'ís.[62]		In 1937, Shoghi Effendi launched a seven-year plan for the Bahá'ís of North America, followed by another in 1946.[63] In 1953, he launched the first international plan, the Ten Year World Crusade. This plan included extremely ambitious goals for the expansion of Bahá'í communities and institutions, the translation of Bahá'í texts into several new languages, and the sending of Bahá'í pioneers into previously unreached nations.[64] He announced in letters during the Ten Year Crusade that it would be followed by other plans under the direction of the Universal House of Justice, which was elected in 1963 at the culmination of the Crusade. The House of Justice then launched a nine-year plan in 1964, and a series of subsequent multi-year plans of varying length and goals followed, guiding the direction of the international Bahá'í community.[65]		Annually, on 21 April, the Universal House of Justice sends a ‘Ridván’ message to the worldwide Bahá’í community,[66] which generally gives an update on the progress made concerning the current plan, and provides further guidance for the year to come.[note 4] The Bahá'ís around the world are currently being encouraged to focus on capacity building through children's classes, youth groups, devotional gatherings, and a systematic study of the religion known as study circles.[67][68] Further focuses are involvement in social action and participation in the prevalent discourses of society.[69][70] The years from 2001 until 2021 represent four successive five-year plans, culminating in the centennial anniversary of the passing of `Abdu'l-Bahá.[71]		A Bahá'í published document reported 4.74 million Bahá'ís in 1986 growing at a rate of 4.4%.[72] Bahá'í sources since 1991 usually estimate the worldwide Bahá'í population to be above 5 million.[73] The World Christian Encyclopedia estimated 7.1 million Bahá'ís in the world in 2000, representing 218 countries,[74] and 7.3 million in 2010[75] with the same source. They further state: "The Baha'i Faith is the only religion to have grown faster in every United Nations region over the past 100 years than the general population; Baha’i was thus the fastest-growing religion between 1910 and 2010, growing at least twice as fast as the population of almost every UN region."[76] This source's only systematic flaw was to consistently have a higher estimate of Christians than other cross-national data sets.[77]		From its origins in the Persian and Ottoman Empires, by the early 20th century there were a number of converts in South and South East Asia, Europe, and North America. During the 1950s and 1960s, vast travel teaching efforts brought the religion to almost every country and territory of the world. By the 1990s, Bahá'ís were developing programs for systematic consolidation on a large scale, and the early 21st century saw large influxes of new adherents around the world. The Bahá'í Faith is currently the largest religious minority in Iran,[78] Panama,[79] Belize,[80] and South Carolina;[81] the second largest international religion in Bolivia,[82] Zambia,[83] and Papua New Guinea;[84] and the third largest international religion in Chad[85][86] and Kenya.[87]		According to The World Almanac and Book of Facts 2004:		The majority of Bahá'ís live in Asia (3.6 million), Africa (1.8 million), and Latin America (900,000). According to some estimates, the largest Bahá'í community in the world is in India, with 2.2 million Bahá'ís, next is Iran, with 350,000, the US, with 150,000, and Brazil, with 60,000. Aside from these countries, numbers vary greatly. Currently, no country has a Bahá'í majority.[88]		The Bahá'í Faith is a medium-sized religion[89] and was listed in The Britannica Book of the Year (1992–present) as the second most widespread of the world's independent religions in terms of the number of countries represented. According to Britannica, the Bahá'í Faith (as of 2010) is established in 221 countries and territories and has an estimated seven million adherents worldwide.[2] Additionally, Bahá'ís have self-organized in most of the nations of the world.		The Bahá'í religion was ranked by the Foreign Policy magazine as the world's second fastest growing religion by percentage (1.7%) in 2007.[90]		The following are a few examples from Bahá'u'lláh's teachings on personal conduct that are required or encouraged of his followers.		The following are a few examples from Bahá'u'lláh's teachings on personal conduct that are prohibited or discouraged.		While some of the laws from the Kitáb-i-Aqdas are applicable at the present time, others are dependent upon the existence of a predominantly Bahá'í society, such as the punishments for arson or murder. The laws, when not in direct conflict with the civil laws of the country of residence, are binding on every Bahá'í,[91] and the observance of personal laws, such as prayer or fasting, is the sole responsibility of the individual.[92][93]		The purpose of marriage in the Bahá'i faith is mainly to foster spiritual harmony, fellowship and unity between a man and a woman and to provide a stable and loving environment for the rearing of children.[94] The Bahá'í teachings on marriage call it a fortress for well-being and salvation and place marriage and the family as the foundation of the structure of human society.[95] Bahá'u'lláh highly praised marriage, discouraged divorce, and required chastity outside of marriage; Bahá'u'lláh taught that a husband and wife should strive to improve the spiritual life of each other.[96] Interracial marriage is also highly praised throughout Bahá'í scripture.[95]		Bahá'ís intending to marry are asked to obtain a thorough understanding of the other's character before deciding to marry.[95] Although parents should not choose partners for their children, once two individuals decide to marry, they must receive the consent of all living biological parents, whether they are Bahá'í or not. The Bahá'í marriage ceremony is simple; the only compulsory part of the wedding is the reading of the wedding vows prescribed by Bahá'u'lláh which both the groom and the bride read, in the presence of two witnesses.[95] The vows are "We will all, verily, abide by the Will of God."[95]		Bahá'u'lláh prohibited a mendicant and ascetic lifestyle.[97] Monasticism is forbidden, and Bahá'ís are taught to practice spirituality while engaging in useful work.[14] The importance of self-exertion and service to humanity in one's spiritual life is emphasised further in Bahá'u'lláh's writings, where he states that work done in the spirit of service to humanity enjoys a rank equal to that of prayer and worship in the sight of God.[14]		Most Bahá'í meetings occur in individuals' homes, local Bahá'í centers, or rented facilities. Worldwide, there are currently seven Bahá'í Houses of Worship, with an eighth near completion [98] in Chile,[99] and a further seven planned as of April 2012.[100] Bahá'í writings refer to an institution called a "Mashriqu'l-Adhkár" (Dawning-place of the Mention of God), which is to form the center of a complex of institutions including a hospital, university, and so on.[101] The first ever Mashriqu'l-Adhkár in `Ishqábád, Turkmenistan, has been the most complete House of Worship.[102]		The Bahá'í calendar is based upon the calendar established by the Báb. The year consists of 19 months, each having 19 days, with four or five intercalary days, to make a full solar year.[11] The Bahá'í New Year corresponds to the traditional Persian New Year, called Naw Rúz, and occurs on the vernal equinox, near 21 March, at the end of the month of fasting. Bahá'í communities gather at the beginning of each month at a meeting called a Feast for worship, consultation and socializing.[14]		Each of the 19 months is given a name which is an attribute of God; some examples include Bahá’ (Splendour), ‘Ilm (Knowledge), and Jamál (Beauty).[103] The Bahá'í week is familiar in that it consists of seven days, with each day of the week also named after an attribute of God. Bahá'ís observe 11 Holy Days throughout the year, with work suspended on 9 of these. These days commemorate important anniversaries in the history of the religion.[104]		The symbols of the religion are derived from the Arabic word Bahá’ (بهاء "splendor" or "glory"), with a numerical value of 9, which is why the most common symbol is the nine-pointed star.[105] The ringstone symbol and calligraphy of the Greatest Name are also often encountered. The former consists of two five-pointed stars interspersed with a stylized Bahá’ whose shape is meant to recall the three onenesses,[106] while the latter is a calligraphic rendering of the phrase Yá Bahá'u'l-Abhá (يا بهاء الأبهى "O Glory of the Most Glorious!").		The five-pointed star is the symbol of the Bahá'í Faith.[107][108] In the Bahá'í Faith, the star is known as the Haykal (Arabic: "temple"‎‎), and it was initiated and established by the Báb. The Báb and Bahá'u'lláh wrote various works in the form of a pentagram.[109]		Since its inception the Bahá'í Faith has had involvement in socio-economic development beginning by giving greater freedom to women,[111] promulgating the promotion of female education as a priority concern,[112] and that involvement was given practical expression by creating schools, agricultural coops, and clinics.[111]		The religion entered a new phase of activity when a message of the Universal House of Justice dated 20 October 1983 was released. Bahá'ís were urged to seek out ways, compatible with the Bahá'í teachings, in which they could become involved in the social and economic development of the communities in which they lived. Worldwide in 1979 there were 129 officially recognized Bahá'í socio-economic development projects. By 1987, the number of officially recognized development projects had increased to 1482.[65]		Bahá'u'lláh wrote of the need for world government in this age of humanity's collective life. Because of this emphasis the international Bahá'í community has chosen to support efforts of improving international relations through organizations such as the League of Nations and the United Nations, with some reservations about the present structure and constitution of the UN.[113] The Bahá'í International Community is an agency under the direction of the Universal House of Justice in Haifa, and has consultative status with the following organizations:[114][115]		The Bahá'í International Community has offices at the United Nations in New York and Geneva and representations to United Nations regional commissions and other offices in Addis Ababa, Bangkok, Nairobi, Rome, Santiago, and Vienna.[115] In recent years an Office of the Environment and an Office for the Advancement of Women were established as part of its United Nations Office. The Bahá'í Faith has also undertaken joint development programs with various other United Nations agencies. In the 2000 Millennium Forum of the United Nations a Bahá'í was invited as the only non-governmental speaker during the summit.[116]		Bahá'ís continue to be persecuted in Islamic countries, as Islamic leaders do not recognize the Bahá'í Faith as an independent religion, but rather as apostasy from Islam. The most severe persecutions have occurred in Iran, where over 200 Bahá'ís were executed between 1978 and 1998,[78] and in Egypt. The rights of Bahá'ís have been restricted to greater or lesser extents in numerous other countries, including Afghanistan,[117] Indonesia,[118] Iraq,[119] Morocco,[120][121] and several countries in sub-Saharan Africa.[65]		The marginalization of the Iranian Bahá'ís by current governments is rooted in historical efforts by Muslim clergy to persecute the religious minority. When the Báb started attracting a large following, the clergy hoped to stop the movement from spreading by stating that its followers were enemies of God. These clerical directives led to mob attacks and public executions.[48] Starting in the twentieth century, in addition to repression that impacted individual Bahá'ís, centrally directed campaigns that targeted the entire Bahá'í community and its institutions were initiated.[122] In one case in Yazd in 1903 more than 100 Bahá'ís were killed.[123] Bahá'í schools, such as the Tarbiyat boys' and girls' schools in Tehran, were closed in the 1930s and 1940s, Bahá'í marriages were not recognized and Bahá'í texts were censored.[122][124]		During the reign of Mohammad Reza Pahlavi, to divert attention from economic difficulties in Iran and from a growing nationalist movement, a campaign of persecution against the Bahá'ís was instituted.[note 5] An approved and coordinated anti-Bahá'í campaign (to incite public passion against the Bahá'ís) started in 1955 and it included the spreading of anti-Bahá'í propaganda on national radio stations and in official newspapers.[122] In the late 1970s the Shah's regime consistently lost legitimacy due to criticism that it was pro-Western. As the anti-Shah movement gained ground and support, revolutionary propaganda was spread which alleged that some of the Shah's advisors were Bahá'ís.[125] Bahá'ís were portrayed as economic threats, and as supporters of Israel and the West, and societal hostility against the Bahá'ís increased.[122][126]		Since the Islamic Revolution of 1979 Iranian Bahá'ís have regularly had their homes ransacked or have been banned from attending university or from holding government jobs, and several hundred have received prison sentences for their religious beliefs, most recently for participating in study circles.[78] Bahá'í cemeteries have been desecrated and property has been seized and occasionally demolished, including the House of Mírzá Buzurg, Bahá'u'lláh's father.[48] The House of the Báb in Shiraz, one of three sites to which Bahá'ís perform pilgrimage, has been destroyed twice.[48][127][128]		According to a US panel, attacks on Bahá'ís in Iran increased under Mahmoud Ahmadinejad's presidency.[129][130] The United Nations Commission on Human Rights revealed an October 2005 confidential letter from Command Headquarters of the Armed Forces of Iran ordering its members to identify Bahá'ís and to monitor their activities. Due to these actions, the Special Rapporteur of the United Nations Commission on Human Rights stated on 20 March 2006, that she "also expresses concern that the information gained as a result of such monitoring will be used as a basis for the increased persecution of, and discrimination against, members of the Bahá'í faith, in violation of international standards. The Special Rapporteur is concerned that this latest development indicates that the situation with regard to religious minorities in Iran is, in fact, deteriorating.[131]		On 14 May 2008, members of an informal body known as the "Friends" that oversaw the needs of the Bahá'í community in Iran were arrested and taken to Evin prison.[129][132] The Friends court case has been postponed several times, but was finally underway on 12 January 2010.[133] Other observers were not allowed in the court. Even the defence lawyers, who for two years have had minimal access to the defendants, had difficulty entering the courtroom. The chairman of the U.S. Commission on International Religious Freedom said that it seems that the government has already predetermined the outcome of the case and is violating international human rights law.[133] Further sessions were held on 7 February 2010,[134] 12 April 2010[135] and 12 June 2010.[136] On 11 August 2010 it became known that the court sentence was 20 years imprisonment for each of the seven prisoners[137] which was later reduced to ten years.[138] After the sentence, they were transferred to Gohardasht prison.[139] In March 2011 the sentences were reinstated to the original 20 years.[140] On 3 January 2010, Iranian authorities detained ten more members of the Baha'i minority, reportedly including Leva Khanjani, granddaughter of Jamaloddin Khanjani, one of seven Baha'i leaders jailed since 2008 and in February, they arrested his son, Niki Khanjani.[141]		The Iranian government claims that the Bahá'í Faith is not a religion, but is instead a political organization, and hence refuses to recognize it as a minority religion.[142] However, the government has never produced convincing evidence supporting its characterization of the Bahá'í community.[143] Also, the government's statements that Bahá'ís who recanted their religion would have their rights restored, attest to the fact that Bahá'ís are persecuted solely for their religious affiliation.[144] The Iranian government also accuses the Bahá'í Faith of being associated with Zionism because the Bahá'í World Centre is located in Haifa, Israel.[note 6] These accusations against the Bahá'ís have no basis in historical fact,[145] [126][146] and the accusations are used by the Iranian government to use the Bahá'ís as "scapegoats".[147] In fact it was the Iranian leader Naser al-Din Shah Qajar who banished Bahá'u'lláh from Persia to the Ottoman Empire and Bahá'u'lláh was later exiled by the Ottoman Sultan, at the behest of the Persian Shah, to territories further away from Iran and finally to Acre in Syria, which only a century later was incorporated into the state of Israel.[148]		Bahá'í institutions and community activities have been illegal under Egyptian law since 1960. All Bahá'í community properties, including Bahá'í centers, libraries, and cemeteries, have been confiscated by the government and fatwas have been issued charging Bahá'ís with apostasy.[149]		The Egyptian identification card controversy began in the 1990s when the government modernized the electronic processing of identity documents, which introduced a de facto requirement that documents must list the person's religion as Muslim, Christian, or Jewish (the only three religions officially recognized by the government). Consequently, Bahá'ís were unable to obtain government identification documents (such as national identification cards, birth certificates, death certificates, marriage or divorce certificates, or passports) necessary to exercise their rights in their country unless they lied about their religion, which conflicts with Bahá'í religious principle. Without documents, they could not be employed, educated, treated in hospitals, travel outside of the country, or vote, among other hardships.[150] Following a protracted legal process culminating in a court ruling favorable to the Bahá'ís, the interior minister of Egypt released a decree on 14 April 2009, amending the law to allow Egyptians who are not Muslim, Christian, or Jewish to obtain identification documents that list a dash in place of one of the three recognized religions.[151] The first identification cards were issued to two Bahá'ís under the new decree on 8 August 2009.[152]		
Approximately 330 to 360 million people speak English as their first language.[1] More than half of these (231 million) live in the United States, followed by some 55 million in England, the first place where Modern English was spoken.		English is the third largest language by number of native speakers, after Mandarin and Spanish.[2]		Estimates that include second language speakers vary greatly, from 470 million to more than 1 billion. David Crystal calculates that non-native speakers as of 2003 outnumbered native speakers by a ratio of 3 to 1.[3] When combining native and non-native speakers, English is the second most widely spoken language worldwide.		Besides the major varieties of English, such as American English, British English, Indian English, Canadian English, Australian English, Irish English, New Zealand English and their sub-varieties, countries such as South Africa, the Philippines, Jamaica and Nigeria also have millions of native speakers of dialect continua ranging from English-based creole languages to Standard English.						There are six large countries with a majority of native English speakers that are sometimes grouped under the term Anglosphere. They are, in descending order of English speakers, the United States (at least 231 million),[4] the United Kingdom (60 million),[5][6][7] Canada (at least 20 million),[8] Australia (at least 17 million),[9] Ireland (4.2 million), and New Zealand (3.7 million).[10]		Pie chart showing the percentage of native English speakers living in "inner circle" English-speaking countries. Native speakers are now substantially outnumbered worldwide by second-language speakers of English (not counted in this chart[11]).		English is also the primary natively spoken language in the countries and territories of Anguilla, Antigua and Barbuda, the Bahamas, Barbados, Belize, Bermuda, the British Indian Ocean Territory, the British Virgin Islands, the Cayman Islands, Dominica, the Falkland Islands, Gibraltar, Grenada, Guam, Guernsey, Guyana, the Isle of Man, Jamaica, Jersey, Montserrat, Pitcairn Islands, Saint Helena, Ascension and Tristan da Cunha, Saint Kitts and Nevis, Saint Vincent and the Grenadines, South Georgia and the South Sandwich Islands, Trinidad and Tobago, and the Turks and Caicos Islands.		Other substantial communities of native speakers are found in South Africa (4.8 million)[12] and Nigeria (4 million, 5%).		In some countries where English is not the most spoken language, it is an official language; these countries include Botswana, Cameroon (co-official with French), the Federated States of Micronesia, Fiji, Ghana, Hong Kong, India, Kenya, Kiribati, Lesotho, Liberia, Malta, the Marshall Islands, Mauritius, Namibia, Nigeria, Pakistan, Palau, Papua New Guinea, the Philippines, Rwanda, Saint Lucia, Samoa, Seychelles, Sierra Leone, Singapore, the Solomon Islands, Sri Lanka, Sudan, South Africa, South Sudan, Swaziland, Tanzania, Uganda, Zambia and Zimbabwe. There also are countries where in a part of the territory English became a co-official language, e.g. Colombia's San Andrés y Providencia and Nicaragua's Mosquito Coast. This was a result of the influence of British colonization in the area.		India has the largest number of second-language speakers of English (see Indian English); Crystal (2004) claims that, combining native and non-native speakers, India has more people who speak or understand English than any other country in the world.[13]		English is one of the eleven official languages that are given equal status in South Africa (South African English). It is also the official language in current dependent territories of Australia (Norfolk Island, Christmas Island and Cocos Island) and of the United States (American Samoa, Guam, Northern Mariana Islands, Puerto Rico (in Puerto Rico, English is co-official with Spanish) and the US Virgin Islands),[14] and the former British colony of Hong Kong. (See List of countries where English is an official language for more details.)		Although the United States federal government has no official languages, English has been given official status by 32 of the 50 US state governments.[15][16] Although falling short of official status, English is also an important language in several former colonies and protectorates of the United Kingdom, such as Bahrain, Bangladesh, Brunei, Cyprus and the United Arab Emirates.		Because English is so widely spoken, it has often been referred to as a "world language", the lingua franca of the modern era,[17] and while it is not an official language in most countries, it is currently the language most often taught as a foreign language.[18] It is, by international treaty, the official language for aeronautical[19] and maritime[20] communications. English is one of the official languages of the United Nations and many other international organizations, including the International Olympic Committee.		English is studied most often in the European Union, and the perception of the usefulness of foreign languages among Europeans is 67 percent in favour of English ahead of 17 percent for German and 16 percent for French (as of 2012). Among some of the non-English-speaking EU countries, the following percentages of the adult population claimed to be able to converse in English in 2012: 90 percent in the Netherlands, 89 percent in Malta, 86 percent in Sweden and Denmark, 73 percent in Cyprus and Austria, 70 percent in Finland, and over 50 percent in Greece, Belgium, Luxembourg, Slovenia, and Germany. In 2012, excluding native speakers, 38 percent of Europeans consider that they can speak English.[21]		Books, magazines, and newspapers written in English are available in many countries around the world, and English is the most commonly used language in the sciences[17] with Science Citation Index reporting as early as 1997 that 95% of its articles were written in English, even though only half of them came from authors in English-speaking countries.		In publishing, English literature predominates considerably with 28 percent of all books published in the world [leclerc 2011] and 30 percent of web content in 2011 (from 50 percent in 2000).[22]		This increasing use of the English language globally has had a large impact on many other languages, leading to language shift and even language death,[23] and to claims of linguistic imperialism. English itself has become more open to language shift as multiple regional varieties feed back into the language as a whole.[24]		
The free school movement, also known as the new schools or alternative schools movement, was an American education reform movement during the 1960s and early 1970s that sought to change the aims of formal schooling through alternative, independent community schools.						As disenchantment with social institutions spread with the 1960s counterculture, alternative schools sprouted outside the local public school system. Funded by tuition and philanthropic grants,[1] they were created by parents, teachers, and students in opposition to contemporaneous schooling practices across the United States and organized without central organization, usually small and grassroots with alternative curricula.[1] Their philosophical influence stemmed from the counterculture, A. S. Neill and Summerhill, child-centered progressive education of the Progressive Era, the Modern Schools, and Freedom Schools.[1] Influential voices within the movement included Paul Goodman, Edgar Z. Friedenberg, Herb Kohl, Jonathan Kozol, and James Herndon, with titles such as A. S. Neill's 1960 Summerhill, George Dennison's 1969 The Lives of Children, and Jonathan Kozol's 1972 Free Schools.[1] The movement's transference of ideas was tracked through the New Schools Exchange and American Summerhill Society.[1]		The definition and scope of schools self-classified as "free schools" and their associated movement were never clearly delineated, and as such, there was a wide variation between schools.[2] The movement did not subscribe to a single ideology, but its "free schools" tended to fall into the binaries of either utopian cultural withdrawal from external concerns, or direct political address of social injustices.[1] Some schools practiced participatory democracies for self-governance.[1] The "free schools" movement was also known as the "new schools" or "alternative schools movement".[2] Author Ron Miller defined the free school movement's principles as letting families choose for their children, and letting children learn at their own pace.[3]		Allen Graubard charted the growth of the free schools from 25 in 1967 to around 600 in 1972, with estimates of 200 created between 1971 and 1972.[2] These schools had an average enrollment of 33 students.[2] Almost all of the first American free schools were based on Summerhill and its associated book.[4] Many of the schools were started in nontraditional locations, including parks, churches, and abandoned buildings.[3]		The movement peaked in 1972 with hundreds of schools opened and public interest in open education.[3]		The movement subsided with the rise of 1970s conservatism,[1] particularly due to the Nixon administration's education policies.[3]		The Huffington Post wrote in 2012 that "the movement is revving up again", citing Education Revolution's listing of over 100 free schools in America.[3] The schools are mostly private in America, and generally serve middle and upper-middle-class families.[3] Author Ron Miller credits the rise of standardization with grassroots interest in alternative schools.[3] CBS News reported in 2006 that the remaining free schools, while unknown in number, are "democratic", as the students share in the school's governance.[5]		Education historian Diane Ravitch said in 2004 that these schools function best for students from educated families due to the free schools' emphasis on individual contribution.[6] Victoria Goldman of The Manhattan Family Guide to Private Schools and Selective Public Schools and E. D. Hirsch, Jr. echoed similar thoughts, with Hirsch adding that "it doesn't work for children who haven't had those advantages."[7] Ravitch believed that the free schools' values would conflict with predominant student testing trends.[6]		A number of schools founded during the Free school movement period of the late-60s/early-70s are still in operation. These include:		
The Ontario Academic Credit or OAC (French: Cours préuniversitaire de l'Ontario or CPO) was a fifth year of secondary school education that previously existed in the province of Ontario, Canada, designed for students preparing for post-secondary education. The OAC curriculum was codified by the Ontario Ministry of Education in Ontario Schools: Intermediate and Senior (OS:IS) and its revisions. The Ontario education system had five years of secondary education, known as Grade 13 from 1921 to 1988; grade 13 was replaced by OAC for students starting high school (grade 9) in 1984. OAC continued to act as a fifth year of secondary education until it was phased out in 2003.[1]						The fifth year in the Ontario secondary school system had existed in Ontario for 82 years, from 1921 to 2003, first as Grade 13, and then as the Ontario Academic Credit.[1] The first attempt to reform the education system in Ontario was initiated in 1945, with the Royal Commission on Education, which proposed a three-tiered education system with six years of elementary education, followed by four years of secondary education, and culminating in three years of junior colleges. However, the commission's report was shelved after five years, in part due its potential to re-open the politically sensitive issue of separate school funding and in part due to the Minister of Education's prior interference in curriculum redesign a year earlier.[1]		The threat to the fifth year of secondary school education in Ontario grew significantly during the 1960s, with growing opposition to the grade 13 departmental examinations from parents. This led to the establishment of the Grade 13 Study Committee in 1964 by Minister of Education Bill Davis, which recommended the elimination of both the departmental examinations, and grade 13.[1] A subsequent recommendation in 1968, in the Hall-Dennis Report, also called for the elimination of grade 13. In spite of these recommendations, however, grade 13 was maintained by the Ontario government.[1]		A significant opposition amongst parents, businesses and universities regarding the education reforms had surfaced by the 1970s, in which they believed there was a decline in academic standards, a lack of focus in the curriculum, and lax discipline in schools. Combined with financially pressured school boards beginning to call for the abolition of grade 13 as a means of financial restraint, this resulted in the government reevaluating its secondary education system.[1] The resulting document was the Ontario Schools: Intermediate and Senior (OS:IS), which had called for the formal elimination of grade 13, without formally eliminating the fifth year of secondary education.[1] Acting upon the recommendations of the document, Ontario formally eliminated grade 13 in 1984, and introduced the Ontario Academic Credit system. The new system allowed for students to graduate from secondary schools in four years, while also maintaining the fifth year, known as OAC, which had courses catering for students planning to proceed with post-secondary education.[2] Despite the fact that students were able to graduate from the secondary school system in four years, a fifth year of secondary education continued to persist in Ontario, with fewer than 15 percent of students exercising the option to graduate in four years, with reports that between 20 percent and 25 percent of students choose to repeat one or more OAC years.		Another Royal Commission on Learning, set up in 1995 by the then-NDP provincial government, had recommended the elimination of OAC.[3] The incoming Progressive Conservative provincial government acted upon the recommendations of the commission in 1998, but students still in the five-year system would continue in the OAC system until they graduated.[2] The motivation for phasing out OAC was largely thought of as a cost-saving measure by the Progressive Conservatives, and to bring Ontario in line with the rest of the provinces.[4][5][6] The reforms led to a new, standardized curriculum documented in Ontario Secondary Schools, Grades 9 to 12: Program and Diploma Requirements (OSS).[7] The OAC year was replaced with an extra ten days of schooling in each lower grade, and the material was integrated into the earlier years of education.[5] The last graduating class of OAC was in 2003.[4]		There were two high school diplomas in Ontario, the Secondary School Graduation Diploma (SSGD) which was awarded after Grade 12 and the Secondary School Honours Graduation Diploma (SSHGD) awarded after Grade 13. This practice ended with the replacement of both diplomas in 1988 with the Ontario Secondary School Diploma (OSSD) under OS:IS.[8] OS:IS more formally allowed for the completion of schooling after only 12 grades. Under OS:IS, OAC year was the final year of high school in Ontario.		OAC courses were the highest level courses in Ontario high schools until the formal elimination of the Ontario Academic Credit. To enter university, students were required to complete 30 high school credits (courses can have different credit values, but most courses were worth 1 credit; some courses were compulsory and there were other restrictions), 6 of which had to be at the OAC level. Assuming that one had taken the necessary prerequisite courses, one could complete an OAC course before the OAC year, and so in many schools it was common for Grade 11 or Grade 12 students to have taken some OAC courses. Students who completed these requirements in 4 years of high school were permitted to graduate; this practice was known as fast-tracking, finishing Grade 12 in four years with 30 credits if the student was college bound.[1] Under the old Grade 13 system, the SSGD represented 27 credits and the SSHGD usually represented 33 credits (Little known fact – as long as you could pass the 6 credits required for an SSHGD, the Ontario school system was required to award the OSSHGD, even if you had less than 33 credits overall)[citation needed]. A minority of students completed the old program in four years by completed eight credits per year and one summer school credit (usually Grade 12 mathematics, as each maths course had the previous year as prerequisite).		Students with an average of 80 percent or higher in six OAC courses were named Ontario Scholar. The award continues to exist today, although it requires the student to have 80 percent or higher in six grade 12 courses.[9]		The elimination of the fifth year of high school education in Ontario had led to a number of consequences, most notably the double cohorts in 2003, in which an unusually high proportion of students graduated in Ontario. Since the elimination of OAC, some have noted that a greater proportion of students have entered post-secondary education.[4] However, in a paper published by Harry Krashinsky of the University of Toronto, the author found that the elimination of OAC had a large and negative impact on academic performance in university.[10]		The elimination of OAC had led to a spike in more than 100,000 students graduating in 2003, with the last OAC (OS:IS) class and the first Grade 12 (OSS) class graduating in the same year.[4] This had strained many Ontario post-secondary institutions, as the spike in students forced the institutions to either construct or rent new buildings for student housing.[11] With the increase in students entering post-secondary education, the provincial government had set aside additional funding for colleges and universities to build more infrastructure such as residences and classrooms.[5] They also had to provide more resources such as upgrading libraries, adding more study areas, creating new programs, and hiring additional professors and teaching assistants. For those who are unable to enter post-secondary institutions, the provincial government allocated more funding for the apprentice program. The spike in students graduating in 2003 had also led to more competitive admission standards in most Ontario universities[4] along with arbitrarily short-lived higher standards to graduate from universities. Some students under OS:IS who feared that they might not be able to gain admission to the university of their choice as a result of the double cohort decided to fast-track to graduate before 2003; as well, some students under OSS decided to take an extra year of high school to graduate in 2004 or delayed application to post-secondary institutions.[12] Double-cohort students who chose the latter options in their turn affected those in the year after them, creating a ripple effect. In June 2007, a cascade double-cohort effect occurred at universities and the job market, as double cohort students who were finishing their undergraduate studies in April competed for graduate spaces in universities or employment in the job market.[4]		The elimination of OAC resulted in the majority of incoming first-year students in Ontario universities to drop from 19 years of age to 18 years of age. This created a legal liability to universities as the majority of first-year students were now below the legal drinking age (it is 19 in Ontario). This has forced the universities to eliminate or police many frosh-week events and traditions that allegedly encouraged drinking and has banned the consumption of alcohol at most frosh-week events. Queen's University's Student Orientation Activities Review Board (SOARB) noted in 2005 that "first-year students seemed to show more desire to drink than those of the past few years. Student drinking, prior to attendance, compromised events at which no alcohol was available... The Board wonders if there is merit to making the evening hours busier to avoid allowing time to “pre-drink” before events."[13]		With a significant minority of students below the legal drinking age, 18-year-olds are legally excluded from campus events and social activities.[14] The temporal nature of this exclusion and the stress associated with establishing a social network in an unfamiliar environment creates intense pressure for underage students to either find ways to subvert the Ontario drinking laws (by purchasing fake IDs, using real IDs of other people, or drinking in private residences with ill-gotten alcohol) or sacrifice relationships with those of legal drinking age.[15]		Patrick Brady and Philip Allingham of Lakehead University have argued that the provincial government's attempt to bring Ontario in line with the rest of the continent's 12 grades system has only been partially successful. While the fifth year of secondary education was formally eliminated, both have noted that the fifth year in secondary schools is still a norm in Ontario, with students in Ontario still opting to take a fifth year in secondary school, colloquially known as the victory lap.[1]		In the first few years after OAC had been eliminated, more than 32 percent of students returned for a fifth year. The percentage of students who opt for a fifth year has since decreased between 15 percent to 20 percent, with some predicting it to level out around that level.[2] In the 2007–2008 year, students over the age of 19 made up 3.7 percent of all secondary day school enrolment in Ontario.[16]		Both Brady and Allingham note that the motivations behind the victory lap can be traced to the province's history of a fifth year of secondary school education, making it a basic assumption of secondary school life for students in Ontario. They also note that it may represent a form of transition anxiety, as students seek to prolong their secondary education, which can be seen as a safe environment, or to acquire further maturity before moving on to their post-secondary education.[1] They also note how the centrality of secondary school life can make a student prolong it. While they found it was not a universal phenomenon, they noted a number of students who returned for a fifth year primarily to continue their participation in the school's non-academic programming.[1] In Brady's and Allingham's study, they had also found differences between genders. While close to half of male participants in their study opted to spend a fifth year in secondary schools, only one in five females choose to do so. They also noted that the motivations of both genders differed, with males opting to victory lap in order to gain additional academic credits, while males primarily opted for a fifth year in order to participate in sports and to gain maturity.[1]		
Radio Prague (Czech: Český rozhlas 7 - Radio Praha) is the official international broadcasting station of the Czech Republic.		Radio Prague broadcasts in six languages: English, German, French, Spanish, Czech and Russian. It broadcasts programmes about the Czech Republic on satellite and on the Internet. Broadcasting first began on August 31, 1936 near the spa town of Poděbrady.		The station broadcasts a total of 24 hours' worth of programmes per day, 3 hours of which are new programmes (one new 30-minute programme in each of the six languages); the remaining 21 hours are rebroadcasts. Rebroadcast programmes have fresh news bulletins. All programmes last for 30 minutes and have a standard layout: news, current affairs magazine and a feature. The theme of the feature changes each day and each section tailors programmes to suit its audience. The weekend broadcasts have a slightly more relaxed structure, they contain less news and more features devoted to the arts, social affairs, and music.						Radio Prague produces a number of programmes in co-operation with other radio stations, and also for them. Radio Prague's Czech section produces programmes for Czech expatriates through SBS Radio in Australia, Radio Daruvar in Croatia, Radio Timisoara in Romania and several radio stations in the United States. These programmes are sent by cassette, via the Internet or down telephone lines. The Russian section uses the Internet to send its features to two radio stations in Russia. From 2001 to 2008, the English section worked with Radio Slovakia International, Radio Budapest and Radio Polonia to produce a programme called Insight Central Europe, which examined contemporary issues facing Central Europe. The programme was discontinued in August 2008. The English Section also participates in a weekly programme called Network Europe co-produced by Deutsche Welle, Radio France International, Radio Netherlands, Radio Polonia, Radio Prague, Radio Romania International, Radio Slovakia International, Radio Sweden and Radio Ukraine International. The English Section also contributes features to Radio Polonia's Europe East programme. Both the English and German sections co-operate with a number of European radio stations on the Radio E project. The German section works together with Radio Slovakia International to produce a Czech-Slovak magazine programme. The French section contributes towards the Accents d´Europe programme produced by Radio France Internationale. The Spanish section sends programmes to several stations in Latin America.		On December 8, 2010, Radio Prague announced via its Facebook page plans to end shortwave broadcasts on January 31, 2011. Part of the post read: " The station’s financing for next year has been drastically reduced by the Foreign Ministry in line with government austerity measures aimed at cutting the state deficit."		As of May 2015 however, Radio Prague buys an hour of time a day on Radio Miami International to relay its programs via shortwave on 9955 kHz in both English and Spanish, targeting the Caribbean.[2]		
School corporal punishment refers to causing deliberate pain or discomfort in response to undesired behaviour by students in schools. It often involves striking the student either across the buttocks[1] or on the hands,[2] with an implement such as a rattan cane, wooden paddle, slipper, leather strap or wooden yardstick. Less commonly, it could also include spanking or smacking the student with the open hand, especially at the elementary school level.		In the English-speaking world, the use by schools of corporal punishment has historically been justified by the common-law doctrine in loco parentis, whereby teachers are considered authority figures granted the same rights as parents to punish children in their care.		Advocates of school corporal punishment argue that it provides an immediate response to indiscipline and that the student is quickly back in the classroom learning, as opposed to suspension from school. Opponents, including a number of medical and psychological societies, along with human-rights groups, argue that physical punishment is ineffective in the long term, interferes with learning, leads to antisocial behaviour as well as various forms of mental distress, and is a form of violence that breaches the rights of children.		Poland in 1783 was the first nation to outlaw corporal punishment in schools. School corporal punishment is no longer practised in any European country. As of 2015, most developed countries have abolished the practice, with the exception of some parts of the United States, some Australian states, and Singapore. It is still in common use in a number of countries in Africa and Asia. It was banned in state funded schools, throughout the United Kingdom, in 1986. It was banned in UK Public and private schools, that received no state funding, in 1999 for England and Wales, 2000 in Scotland, and 2003 in Northern Ireland.						Corporal punishment in the context of schools in the late 20th and early 21st centuries has been variously defined as: causing deliberate pain to a child in response to the child's undesired behaviour and/or language,[3] "purposeful infliction of bodily pain or discomfort by an official in the educational system upon a student as a penalty for unacceptable behavior",[4] and "intentional application of physical pain as a means of changing behavior" (not the occasional use of physical restraint to protect student or others from immediate harm).[5]		Corporal punishment used to be prevalent in schools in many parts of the world, but in recent decades it has been outlawed in most of Europe, most of South America, as well as in Canada, Korea, South Africa, New Zealand and several other countries. It remains commonplace in a number of countries in Africa, Southeast Asia, and the Middle East (see list of countries, below).		While most U.S. states have outlawed corporal punishment in state schools, it continues to be allowed mainly in the Southern and Western United States.[6] According to the United States Department of Education, more than 216,000 students were subjected to corporal punishment during the 2008–09 school year.[7]		Much of the traditional culture that surrounds corporal punishment in school, at any rate in the English-speaking world, derives largely from British practice in the 19th and 20th centuries, particularly as regards the caning of teenage boys.[8] There is a vast amount of literature on this, in both popular and serious culture.[9][10] Britain itself outlawed the practice in 1987 for state schools[11][12][13] and more recently for all schools.[14][15]		The doctrine of in loco parentis lets school officials stand in for parents as comparable authority figures.[4] The doctrine has its origins in an English common-law precedent of 1770.[5]		Many schools in Singapore and Malaysia use caning (for boys) as a routine official punishment for misconduct, as also some African countries. In some Middle Eastern countries whipping is used. (See list of countries, below.)		In most of continental Europe, school corporal punishment has been banned for several decades or longer, depending on the country (see the list of countries below).		From the 1917 Russian revolution onwards, corporal punishment was outlawed in the Soviet Union, because it was deemed contrary to communist ideology.[16] Communists in other countries such as Britain took the lead in campaigning against school corporal punishment, which they viewed as a symptom of the decadence of capitalist education systems.[17] In the 1960s, Soviet visitors to western schools expressed shock at the caning of boys there.[18] Other communist regimes followed suit: for instance, corporal punishment was "unknown" by students in North Korea in 2007.[19] In mainland China, corporal punishment in schools was outlawed in 1986,[20] although the practice remains common, especially in rural areas.[21]		According to the American Academy of Pediatrics, there are three broad rationales for the use of corporal punishment in schools: beliefs, based in traditional religion, that adults have a right, if not a duty, to physically punish misbehaving children; a disciplinary philosophy that corporal punishment builds character, being necessary for the development of a child's conscience and their respect for adult authority figures; and beliefs concerning the needs and rights of teachers, specifically that corporal punishment is essential for maintaining order and control in the classroom.[4]		School officials and policymakers often rely on personal anecdotes to argue that school corporal punishment improves students' behavior and achievement.[22] However, there is a lack of empirical evidence showing that corporal punishment leads to better control in the classroom. In particular, evidence does not suggest that it enhances moral character development, increases students' respect for teachers or other authority figures, or offers greater security for teachers.[23]		A number of medical, pediatric or psychological societies have issued statements opposing all corporal punishment in schools, citing such outcomes as poorer academic achievement, increases in antisocial behaviour, injuries to students, and an unwelcoming learning environment. They include the American Medical Association,[24] the American Academy of Child and Adolescent Psychiatry,[3] the American Academy of Pediatrics,[4][25][26] the Society for Adolescent Medicine,[5][27] the American Psychological Association,[28] the Royal College of Paediatrics and Child Health,[29][30] the Royal College of Psychiatrists,[31] the Canadian Paediatric Society[32] and the Australian Psychological Society,[33] as well as the United States' National Association of Secondary School Principals.[34]		According to the American Academy of Pediatrics (AAP), research shows that corporal punishment is less effective than other methods of behaviour management in schools, and "praise, discussions regarding values, and positive role models do more to develop character, respect, and values than does corporal punishment".[4] They say that evidence links corporal punishment of students to a number of adverse outcomes, including: "increased aggressive and destructive behaviour, increased disruptive classroom behaviour, vandalism, poor school achievement, poor attention span, increased drop-out rate, school avoidance and school phobia, low self-esteem, anxiety, somatic complaints, depression, suicide and retaliation against teachers".[4] The AAP recommends a number of alternatives to corporal punishment including various nonviolent behaviour-management strategies, modifications to the school environment, and increased support for teachers.[4]		An estimated 1 to 2 percent of physically punished students in the United States are seriously injured, to the point of needing medical attention. According to the AAP and the Society for Adolescent Medicine, these injuries have included bruises, abrasions, broken bones, whiplash injury, muscle damage, brain injury, and even death.[4][5] Other reported injuries to students include "sciatic nerve damage"[4] "extensive hematomas", and "life-threatening fat hemorrhage".[5]		The AAP cautions that there is a risk of corporal punishment in schools fostering the impression among students that violence is an appropriate means for managing others' behaviour.[4] According to the American Academy of Child and Adolescent Psychiatry, "Corporal punishment signals to the child that a way to settle interpersonal conflicts is to use physical force and inflict pain".[3] And according to the Society for Adolescent Medicine, "The use of corporal punishment in schools promotes a very precarious message: that violence is an acceptable phenomenon in our society. It sanctions the notion that it is meritorious to be violent toward our children, thereby devaluing them in society's eyes. It encourages children to resort to violence because they see their authority figures or substitute parents doing it [...] Violence is not acceptable and we must not support it by sanctioning its use by such authority figures as school officials".[5]		The Society for Adolescent Medicine recommends developing "a milieu of effective communication, in which the teacher displays an attitude of respect for the students", as well as instruction that is stimulating and appropriate to student's abilities, various nonviolent behaviour modification techniques, and involving students and parents in making decisions about school matters such as rules and educational goals. They suggest that student self-governance can be an effective alternative for managing disruptive classroom behaviour, while stressing the importance of adequate training and support for teachers.[5]		The AAP remarks that there has been "no reported increase in disciplinary problems in schools following the elimination of corporal punishment" according to evidence.[4]		A number of international human-rights organizations including the UN Committee on the Rights of the Child, the Parliamentary Assembly of the Council of Europe, and the Inter-American Commission on Human Rights have stated that physical punishment of any kind is a violation of children's human rights.[35][36][37]		According to the Committee on the Rights of the Child, "Children do not lose their human rights by virtue of passing through the school gates [...] the use of corporal punishment does not respect the inherent dignity of the child nor the strict limits on school discipline".[38] The Committee interprets Article 19 of the Convention on the rights of the child, which obliges member states to "take all appropriate legislative, administrative, social and educational measures to protect the child from all forms of physical or mental violence, injury or abuse […] while in the care of parent(s), legal guardian(s) or any other person who has the care of the child", to imply a prohibition on all forms of corporal punishment.		Other international human-rights bodies supporting prohibition of corporal punishment of children in all settings, including schools, include the European Committee of Social Rights and the African Committee of Experts on the Rights and Welfare of the Child. In addition, the obligation of member states to prohibit corporal punishment in schools and elsewhere was affirmed in the 2009 Cairo Declaration on the Convention on the Rights of the Child and Islamic Jurisprudence.[39]		Corporal punishment of minors in the United States		According to the Global Initiative to End All Corporal Punishment of Children, all forms of corporal punishment in schools are outlawed in 125 countries as of May 2015 (46 of these countries also prohibit corporal punishment of children in the home).[39]		Banned in 1813, corporal punishment was re-legalised in 1817 and punishments by physical pain lasted until the 1980s. The instruments were rebenques, slappings in the face and others.[40][41] All corporal punishment has now been prohibited; the ban came into effect in 2016.[42]		In Australia, laws on corporal punishment in schools are determined at individual state or territory level.[43][44] While still legal in private schools in some states, in practice, very few schools impose corporal punishment.[45]		Corporal punishment in schools was banned in Austria in 1974.[67]		Corporal punishment in all settings, including schools, was prohibited in Bolivia in 2014. According to the Children and Adolescents Code, "The child and adolescent has the right to good treatment, comprising a non-violent upbringing and education... Any physical, violent and humiliating punishment is prohibited".[68]		Corporal punishment in all settings, including schools, was prohibited in Brazil in 2014. According to an amendment to the Code on Children and Adolescents 1990, "Children and Adolescents are entitled to be educated and cared for without the use of physical punishment or cruel or degrading treatment as forms of correction, discipline, education or any other pretext".[69]		Caning is commonly used by teachers as a punishment in schools.[70] The cane is applied on the students' buttocks, calves or palms of the hands in front of the class. Tramline cane marks could be left. Sit-ups with ears pulled and arms crossed, kneeling, and standing on the bench in the classroom are other forms of corporal punishments used in schools. Common reasons for punishment include talking in class, not finishing homework, mistakes made with classwork, fighting and truancy.[71][72]		In Canadian Foundation for Children, Youth and the Law v. Canada (2004) the Supreme Court outlawed school corporal punishment.[73] In public schools, the usual implement was a rubber/canvas strap applied to the hands,[74] while private schools often used a paddle or cane administered to the student's posterior.[75][76] In many parts of Canada, 'the strap' had not been used in public schools since the 1970s or even earlier: thus, it has been claimed that it had not been used in Quebec since the 1960s,[77] and in Toronto it was banned in 1971.[2] However, some schools in Alberta had been using the strap up until the ban in 2004.[78]		Some Canadian provinces banned corporal punishment in public schools prior to the national ban in 2004. They are, in chronological order by year of provincial ban:[citation needed]		Corporal punishment in China was officially banned after the communist revolution in 1949. The Compulsory Education Law of 1986 states: "It shall be forbidden to inflict physical punishment on students".[20] In practice, beatings by schoolteachers are common, especially in rural areas.[21][79]		All corporal punishment, both in school and in the home, has been banned since 2008.[80]		Corporal punishment is outlawed under Article 31 of the Education Act.[81]		Corporal punishment was prohibited in the public schools in Copenhagen Municipality in 1951 and by law in all schools of Denmark in 1967.[82][83][84]		A 1998 study found that random physical punishment (not proper formal corporal punishment) was being used extensively by teachers in Egypt to punish behavior they regarded as unacceptable. Around 80% of the boys and 60% of the girls were punished by teachers, using their hands, sticks, straps, shoes, punches and kicks as most common methods of administration. The most common reported injuries were bumps and contusions.[85]		Corporal punishment in public schools was banned in 1914, but remained de facto commonplace until 1984, when a law banning all corporal punishment of minors, even by parents, was introduced.[86][87]		The systematic use of corporal punishment has been absent from French schools since the 19th century.[88] There is no explicit legal ban on it,[89] but in 2008 a teacher was fined €500 for what some people describe as slapping a student.[90][91][92]		School corporal punishment, historically widespread, was outlawed in different states via their administrative law at different times. It was not completely abolished everywhere until 1983.[93] Since 1993, use of corporal punishment by a teacher has been a criminal offence. In that year a sentence by the Federal Court of Justice of Germany (Bundesgerichtshof, case number NStZ 1993,591) was published which overruled the previous powers enshrined in unofficial customary law (Gewohnheitsrecht) and upheld by some regional appeal courts (Oberlandesgericht, Superior State Court) even in the 1970s. They assumed a right of chastisement was a defense of justification against the accusation of "causing bodily harm" per Paragraph (=Section) 223 Strafgesetzbuch (Federal Penal Code).		Corporal punishment in Greek primary schools was banned in 1998, and in secondary schools in 2005.[94]		Corporal punishment is still used in most of India. The Delhi High Court banned its use in Delhi schools in 2000. 17 out of 29 states claim to apply the ban, though enforcement is lax.[95] A number of social and cultural groups, including Shankaracharya, are campaigning against corporal punishment in India. In many states, corporal punishment is still practised within most schools. Society for Prevention of Injuries & Corporal Punishment (SPIC) is actively running awareness campaigns to educate the teachers and students through conferences and scientific publications.[96]		In schools in Ireland, corporal punishment was banned by regulation in 1982, and its use became a criminal offence in 1996. [97]		Corporal punishment in Italian schools was banned in 1928.[98]		Although banned in 1947, corporal punishment is still commonly found in schools in the 2010s and particularly widespread in school sports clubs. In late 1987, about 60% of junior high school teachers felt it was necessary, with 7% believing it was necessary in all conditions, 59% believing it should be applied sometimes and 32% disapproving of it in all circumstances; while at elementary (primary) schools, 2% supported it unconditionally, 47% felt it was necessary and 49% disapproved.[99] As recent as December 2012, a high school student committed suicide after having been constantly beaten by his basketball coach.[100] An education ministry survey found that more than 10,000 students received illegal corporal punishment from more than 5,000 teachers across Japan in 2012 fiscal year alone.[101]		Corporal punishment in schools was banned in 1845 and became a criminal offence in 1974 (Aggravated Assault on Minors under Authority). [102][103]		Caning is a common form of discipline in many Malaysian schools. Legally it should be applied only to male students, but the idea of making the caning of girls lawful has recently been debated. This would be applied to the palm of the hand, whereas boys are typically caned across the seat of the trousers.[104]		Banned in 1920.[105]		Corporal punishment in New Zealand schools was abolished in 1987, but wasn't abolished legislatively until 23 July 1990, when Section 139A of the Education Act 1989 was inserted by the Education Amendment Act 1990. Section 139A prohibits anyone employed by a school or ECE provider, or anyone supervising or controlling students on the school's behalf, from using force by way of correction or punishment towards any student at or in relation to the school or the student under their supervision or control.[106] Teachers who administer corporal punishment can be found guilty of physical assault, resulting in termination and cancellation of teacher registration, and possibly criminal charges, with a maximum penalty of five years' imprisonment.[107]		As enacted, the law had a loophole: parents, provided they were not school staff, could still discipline their children on school grounds. In early 2007, a southern Auckland Christian school was found to be using this loophole to discipline students by corporal punishment, by making the student's parents administer the punishment.[108] This loophole was closed in May 2007 by the Crimes (Substituted Section 59) Amendment Act 2007, which enacted a blanket ban on parents administering corporal punishment to their children.		Corporal punishment in Norwegian schools was strongly restricted in 1889, and was prohibited outright in 1936.[citation needed]		School corporal punishment in Pakistan is not very common in modern educational institutions although it is still used in schools across the rural parts of the country as a means of enforcing student discipline. The method has been criticised by some children's rights activists who claim that many cases of corporal punishment in schools have resulted in physical and mental abuse of schoolchildren. According to one report, corporal punishment is a key reason for school dropouts and subsequently, street children, in Pakistan; as many as 35,000 high school pupils in Pakistan are said to drop out of the education system each year because they have been punished or abused in school.[109]		Corporal punishment is prohibited in private and public schools.[110]		In 1783, Poland became the first country in the world to prohibit corporal punishment.[111] Peter Newell assumes that perhaps the most influential writer on the subject was the English philosopher John Locke, whose Some Thoughts Concerning Education explicitly criticised the central role of corporal punishment in education. Locke's work was highly influential, and may have helped influence Polish legislators to ban corporal punishment from Poland's schools in 1783. Today, the ban of corporal punishment in all forms is vested in Constitution of Poland[112][113]		Corporal punishment was banned in Russian schools in 1917.[16] Article 336 of the Labor Code of the Russian Federation states that any teacher who has used corporal punishment on a pupil shall be dismissed.		Corporal punishment is legal in Singapore schools (for male students only, it is illegal to inflict it on female students) and fully encouraged by the government in order to maintain strict discipline.[114] Only a light rattan cane may be used.[115] This must be administered in a formal ceremony by the school management after due deliberation, not by classroom teachers. Most secondary schools (whether independent, autonomous or government-controlled), and also some primary schools, use caning to deal with misconduct by boys.[116] At the secondary level, the rattan strokes are nearly always delivered to the student's clothed buttocks. The Ministry of Education has stipulated a maximum of six strokes per occasion. In some cases the punishment is carried out in front of the rest of the school instead of in private.[117]		The use of corporal punishment in schools was prohibited by the South African Schools Act, 1996. According to section 10 of the act:		(1) No person may administer corporal punishment at a school to a learner.		(2) Any person who contravenes subsection (1) is guilty of an offence and liable on conviction to a sentence which could be imposed for assault.[118]		In the case of Christian Education South Africa v Minister of Education the Constitutional Court rejected a claim that the constitutional right to religious freedom entitles private Christian schools to impose corporal punishment.		Since 2011, all forms of caning are completely banned in the liberal regions of Seoul Metropolitan City, Gyeonggi Province, Gangwon Province, Gwangju Metropolitan City, North Jeolla Province and South Jeolla Province. Other more conservative regions are governed by a national law enacted in 2011 which states that while caning is generally forbidden, it can be used indirectly to maintain school discipline.[119] but it is still known to be practised [120]		Corporal punishment in Spanish schools was banned in 1985.[121]		Corporal punishment at school has been prohibited in folkskolestadgan (the elementary school ordinance) since 1 January 1958. Its use by ordinary teachers in grammar schools had been outlawed in 1928.[122]		In 2006 Taiwan made corporal punishment in the school system illegal,[123] but it is still known to be practised (see Corporal punishment in Taiwan).		Corporal punishment in schools is illegal under the Ministry of Education Regulation on Student Punishment (2005) and the National Committee on Child Protection Regulation on Working Procedures of Child Protection Officers Involved in Promoting Behaviour of Students (2005), pursuant to article 65 of the Child Protection Act.[124]		In Ukraine, "physical or mental violence" against children is forbidden by the Constitution (Art.52.2) and the Law on Education (Art.51.1, since 1991) which states that students and other learners have the right “to the protection from any form of exploitation, physical and psychological violence, actions of pedagogical and other employees who violate the rights or humiliate their honour and dignity”.[125] Standard instructions for teachers provided by the Ministry of Science and Education state that a teacher who has used corporal punishment to a pupil (even once), shall be dismissed.		A federal law was implemented in 1998 which banned school corporal punishment. The law applied to all schools, both public and private.[126][127] Any teacher who engages in the practice would not only lose their job and teaching license, but will also face criminal prosecution for engaging in violence against minors and will also face child abuse charges.[128]		In state-run schools, and also in private schools where at least part of the funding came from government, corporal punishment was outlawed by the British Parliament in 1986.[129] In other private schools, it was banned in 1998 (England and Wales), 2000 (Scotland) and 2003 (Northern Ireland).[130] Schools had to keep a record of punishments inflicted,[131] and there are occasional press reports of examples of these "punishment books" having survived.[132][133]		The implement used in many state and private schools in England and Wales was often a rattan cane, struck either across the student's hands or (especially in the case of teenage boys) the clothed buttocks. "Slippering"—striking the buttocks with a rubber-soled gym shoe, or plimsoll shoe—was widely used in many schools, for example King's School, Macclesfield, a boys grammar school in Cheshire.[134] In a few English cities, a strap was used instead of the cane.[135] In Scotland a leather strap, the tawse, administered to the palms of the hands, was universal in state schools,[136] but some private schools used the cane.[137]		Prior to the ban in private schools in England, the "slippering" of a student at an independent boarding school was challenged in 1993 before the European Court of Human Rights.[138] The Court ruled 5–4 in that case that the punishment was not severe enough to infringe the student's "freedom from degrading punishment" under article 3 of the European Convention on Human Rights. The dissenting judges argued that the ritualised nature of the punishment, given after several days and without parental consent, should qualify it as "degrading punishment".[139]		R (Williamson) v Secretary of State for Education and Employment (2005) was an unsuccessful challenge to the prohibition of corporal punishment contained in the Education Act 1996, by several headmasters of private Christian schools who argued that it was a breach of their religious freedom.		In response to a 2008 poll of 6,162 UK teachers by the Times Educational Supplement, 22% of secondary school teachers and 16% of primary school teachers supported "the right to use corporal punishment in extreme cases". The National Union of Teachers said that it "could not support the views expressed by those in favour of hitting children".[140][141]		There is no federal law addressing corporal punishment in public or private schools. In 1977, the Supreme Court ruling in Ingraham v. Wright held that the Eighth Amendment clause prohibiting "cruel and unusual punishments" did not apply to school students, and that teachers could punish children without parental permission.		As of 2015, 31 states and the District of Columbia have banned corporal punishment in public schools, though in some of these there is no explicit prohibition. Corporal punishment is also unlawful in private schools in Iowa and New Jersey. In 19 U.S. states, corporal punishment is lawful in both public and private schools.[142] It is still common in some schools in the South, and more than 167,000 students were paddled in the 2011-2012 school year in American public schools.[143] Students can be physically punished from kindergarten to the end of high school, meaning that even legal adults who have reached the age of majority are sometimes spanked by school officials.[144]		Corporal punishment in all settings, including schools, was prohibited in Venezuela in 2007. According to the Law for the Protection of Children and Adolescents, "All children and young people have a right to be treated well. This right includes a non-violent education and upbringing... Consequently, all forms of physical and humiliating punishment are prohibited".[145]		
Nishapur or Nishabur ( pronunciation (help·info); Persian: نیشابور‎‎, also Romanized as Nīshāpūr, Nīshābūr, and Neyshābūr from Middle Persian: New-Shabuhr, meaning "New City of Shapur", "Fair Shapur",[4] or "Perfect built of Shapur")[5] is a city in the Khorasan Province, capital of the Nishapur County and former capital of Province Khorasan, in northeastern Iran, situated in a fertile plain at the foot of the Binalud Mountains. It had an estimated population of 239,185 as of 2011 and its county 433,105. Nearby are the turquoise mines that have supplied the world with turquoise for at least two millennia. The city was founded in the 3rd century by Shapur I as a Sasanian satrapy capital. Nishapur later became the capital of Tahirid dynasty and was reformed by Abdullah Tahir in 830, and was later selected as the capital of Seljuq dynasty by Tughril in 1037. From the Abbasid era to the Mongol invasion of Khwarezmia and Eastern Iran, the city evolved into a significant cultural, commercial, and intellectual center within the Islamic world. Nishapur, along with Merv, Herat and Balkh were one of the four great cities of Greater Khorasan and one of the greatest cities in the middle ages, a seat of governmental power in eastern of caliphate, a dwelling place for diverse ethnic and religious groups, a trading stop on commercial routes from Transoxiana and China, Iraq and Egypt.		Nishapur reached the height of its prosperity under the Samanids in the 10th century, but was destroyed by Mongols in 1221, and further ruined by other invasions and earthquakes in the 13th century. After that time, a much smaller settlement was established just north of the ancient town, and the once bustling metropolis lay underground—until a team of excavators from the Metropolitan Museum of Art arrived in the mid-20th century. They worked at Nishapur between 1935 and 1940, returning for a final season in the winter of 1947–48.[6] What remains of old Nishapur region is a 3500-hectare "Kohandejh" area, south of the current city of Nishapur.		Little archaeology has been done on this vast and complicated site. George Curzon remarked that Nishapur had been destroyed and rebuilt more times than any other city in history,[7] an evocative statement whether or not it is statistically true. The Metropolitan Museum of Art undertook excavations from 1935 that were interrupted in 1940. Searching largely for museum-worthy trophies that they shared with the government of the Shah, the Metropolitan's publications were limited to its own Nishapur ceramics. The site of Nishapur has been ransacked for half a century since World War II, to feed the international market demand for early Islamic works of art.		Shadiyakh ("Palace of Happiness") was one of the main palaces of old Nishapur up to the 9th century AD, which became more important and populated after that. Some notable people like Attar lived there. Attar's tomb is nowadays in that area. This palace was perhaps completely ruined in the 13th century.		Nishapur occupies an important strategic position astride the old Silk Road that linked Anatolia and the Mediterranean Sea with China. On the Silk Road, Nishapur has often defined the flexible frontier between the Iranian plateau and Central Asia. The town derived its name from its reputed founder, the Sassanian king Shapur I, who is said to have established it in the 3rd century CE. Nearby are the turquoise mines that supplied the world with turquoise for at least two millennia.		It became an important town in the Khorasan region but subsequently declined in significance until a revival in its fortunes in the 9th century under the Tahirid dynasty, when the glazed ceramics of Nishapur formed an important item of trade to the west. For a time Nishapur rivaled Baghdad or Cairo: Toghrül, the first ruler of the Seljuk dynasty, made Nishapur his residence in 1037 and proclaimed himself sultan there, but it declined thereafter, as Seljuk fortunes were concentrated in the west. In the year 1000 CE, it was among the ten largest cities on earth.[8]		After the husband of Genghis Khan's daughter was killed at Nishapur in 1221, she or Genghis ordered the death of the entire population of the city, which was reputedly 1.7 million.[citation needed] Their skulls were reputedly piled in pyramids by the Mongols.[citation needed] This massacre, combined with subsequent earthquakes destroyed the pottery industry.[citation needed]		The special Anthem of Nishapur was unveiled for the first time on April 14, 2011;[9] it has introduction and three parts, noted on three invasive and destructive in the history of Nishapur, delineated by frightening sounds of bells, along with sounds of percussion and wailing women represent the miseries caused by these attacks.[10][11]		About the arts in Nishapur or Old Nishapur:		Nishapur during the Islamic Golden Age, especially the 9th and 10th centuries, was one of the great centers of pottery and related arts.[12] Most of the Ceramic artifacts discovered in Nishapur are preserved in the Metropolitan Museum of Art and Museums in Tehran and Mashhad. Ceramics produced at Nishapur showed links with Sassanid art and Central Asian.[13] Nowadays there are 4 Pottery workshop in Nishapur.[14]		Weaving carpets and rugs common in the more than 470 villages in Nishapur County, the most important carpet Workshop located in the villages of: Shafi' Abad, Garineh Darrud Baghshan Kharv Bozghan Sayyed Abad Sar Chah Suleymani Sultan Abad and Eshgh Abad. Nishapur Carpet workshops weaved the biggest Carpets in the world, like carpets of : Sheikh Zayed Mosque,[15] Sultan Qaboos Grand Mosque,[16] Armenian Presidential Palace, Embassy of Finland in Tehran, Mohammed Al-Ameen Mosque in Oman.[17]		Modern art of carpet in Nishapur began in 1946 after inauguration of a carpet-weaving workshop in a caravansary.		For at least 2,000 years, Iran, known before as Persia, has remained an important source of turquoise, which was named by Iranians initially "pirouzeh" meaning "victory" and later after Arab invasion "firouzeh".[citation needed] In Iranian architecture, the blue turquoise was used to cover the domes of the Iranian palaces because its intense blue colour was also a symbol of heaven on earth.[citation needed]		This deposit, which is blue naturally, and turns green when heated due to dehydration, is restricted to a mine-riddled region in Nishapur, the 2,012-metre (6,601 ft) mountain peak of Ali-mersai, which is tens of kilometers from Mashhad, the capital of Khorasan province, Iran. A weathered and broken trachyte is host to the turquoise, which is found both in situ between layers of limonite and sandstone, and amongst the scree at the mountain's base. These workings, together with those of the Sinai Peninsula, are the oldest known.		In many important historical or modern monuments and buildings the art of Tiles are widely used in Nishapur,		Most people speak Persian in Nishapur.		Khorasani Turkic, Kurdish and Arabic are also spoken.		Islam is first religion and Twelever Mahdist Shia is first Madhab in Nishapur.		Sorted by date		Islamic Azad University of Nishapur is a main branch of Islamic Azad University. It was established in 1985 and has two faculties in IAUM, Agriculture, and Engineering faculty offers Bachelor, and master's degrees.		Al-Ghadir library:Moalem street		Nishapur has one professional football team, Jahan Electric Nishapur, who compete in Razavi Khorasan's Provincial Leagues.		Enghelab Sports Complex is an indoor arena in Nishapur. The arena houses Nishapur's basketball, volleyball, and futsal teams.		On 18 February 2004, runaway train wagons crashed into the village of Khayyam near Nishapur, causing an explosion and killing over 300 people. The entire village of Khayyam was destroyed.[citation needed]		Road 44 is a highway that goes from Tehran to Mashhad and also passes Nishapur on the way.		Foolad steel complex which is producing steel.		Nishapur is located at an elevation of 1213 metres on a wide fertile plain almost at the southwestern foot of Mount Binalud in northcentral Razavi Khorasan Province. The city is connected by both railroad and highways to Mashhad and Tehran and also to South Khorasan Province. Among its agricultural products are cereals and cotton. Making pottery and weaving carpets are among important crafts.		Nishapur has a generally Mediterranean climate with the rainy seasons mostly in the spring and winter. Towards the west of the city, however, the weather gradually changes to a cold semi-desert climate.		The city of Nishapur lies on a Holocene alluvial plain on top of the Pleistocene sediments in the southwestern part of the Binalud Mountains. The Binalud Range, running northwest-southeast, is made predominantly of Triassic and Jurassic rocks. On the southern side of the northwestern part of the range there is a section of Eocene rocks that are volcanic in origin. The well-known Nishabur turquoise comes from the weathered and broken trachytes and andesites of the Eocene volcanic rocks of this part of the mountain range. The main turquoise mines are situated about 50 kilometres northwest of the city of Nishapur in the foothills of the Binalud Range.[19]		Nishapur is located in a region with a rather high risk of earthquakes. Many earthquakes have seriously harmed the city; among the important ones are the historical earthquakes that ruined the city in the 12th and 13th centuries.		General publications in Nishapur includes the weekly and local newspapers. The first local newspaper of Khorasan province is Morning of Nishapur, published since 1989. Others include Shadiakh, published since 2000, Khayyam Nameh, since 2004, Nasim, since 2006, and Far reh Simorgh, since 2010.[20]		IRIB center of Mashhad covers news of Nishapur.		Two book publishers working in the city:Klidar & Abar Shahr.[21][22]		US band Santana released an instrumental track entitled "Incident at Neshabur" on their 1970 LP release, Abraxas.		The most important Nishapur souvenirs include turquoise and rhubarb.		Neyshabur Turquoise has been used for more than 2000 years and for this turquoise it is sometimes called "the turquoise land". Neyshabur turquoise and jewellery made from it are sold as souvenirs in Neyshabur and Mashhad resorts.		Rhubarb (Persian rivaas or rivand), a sour vegetable, grows at the foot of the eponymous Rivand Mountains (more recently, Turkified as Mount Binalud). Soft drinks made from the stems of the plant, such as "Sharbate rivaas" (شربت ریواس) and "Khoshaabe rivaas" (خوشاب ریواس), are sold at some Nishapur resorts as souvenirs.		
International students are those who travel to a country different from their own for the purpose of tertiary study.[1]						The definition of "international student" varies in each country in accordance to their own national education system.[2]		In Australia, international students are defined as those studying onshore only with visa subclasses 570 to 575, excluding students on Australian-funded scholarships or sponsorship or students undertaking study while in possession of other temporary visas.[3] New Zealand citizens do not require a visa to study in Australia, so are not classed as international students.		In Japan, international students are defined as foreign nationals who study at any Japanese university, graduate school, junior college, college of technology, professional training college or university preparatory course on a ‘college student’ visa, as defined by the Immigration Control and Refugee Recognition Act.[3]		Annually around 750,000 Chinese and 400,000 Indian students apply to overseas higher education institutions,[4][5] This mobility is largely driven by rapidly increasing wealth which funds foreign travel and study. Much of the increase in international students in the U.S. during 2013–2014 was fueled by undergraduate students from China, the report's authors found. The number of Chinese students increased to 31 percent of all international students in the U.S. – the highest concentration the top country of origin has had since IIE began producing the report in 1948. [1] This is changing quickly with demographic projections showing a large impending decrease in volumes of international students from China and Russia and steady increases in international students from India and Africa. The number of international students in tertiary (university or college) education is also rapidly increasing as higher education becomes an increasingly global venture.[6] During 2014-15, 974,926 international students came to study in the U.S, which is almost double the population from 2005. Chinese students have always been the largest demographic amongst international students. The top 10 sending places of origin and percentage of total international student enrollment are: China, India, South Korea, Saudi Arabia, Canada, Brazil, Taiwan, Japan, Vietnam, and Mexico. The total number of international students from all places of origin by field of study are: Business/Management, Engineering, Mathematics and Computer Sciences, Social Sciences, Physical and Life Sciences, Humanities, Fine and Applied Arts, Health Professions, Education, and Agriculture.[7]		Top 10 sending places of origin and percentage of total international student enrollment 2015-2016		Total number of international students from all places of origin by field of study 2015-2016		According to the United Nations Educational, Scientific and Cultural Organization (UNESCO) in their 2009 World Conference on Higher Education report, Over 2.5 million students were studying outside their home country. UNESCO also predicted that the number of international students might rise to approximately 7 million by the year 2020. The main destinations preferred by international students are the United States, United Kingdom, Germany, France, Canada and Australia. Overall, the number of international students more than doubled to over 2 million between 2000 and 2007.[8] However the sharpest percentage increases of international students have occurred in New Zealand, Korea, the Netherlands, Greece, Spain, Italy and Ireland.[8]		In recent years, some Asian and Middle East countries have started to attract more international students. These regions have entered the market with declared ambitions to become regional education centers by attracting as many as several hundred thousand international students to their countries.[3]		Top 10 countries for international student enrollment		The US is the undisputed leader with approximately 723,277 foreign enrollments in 2010-11,[12] Traditionally the U.S and U.K have been the most prestigious choices, due to dominating university top 10 rankings with the likes of Harvard, Oxford, MIT and Cambridge. More recently however they have had to compete with the rapidly growing Asian higher education market. While US is the leading destination for international students, there is increasing competition from several destinations in East Asia like China, Korea, Japan and Taiwan which are keen on attracting international students for reputation and demographic reasons.[13]		International student mobility in the first decade of the 21st century has been transformed by two major external events, 9/11 and the recession of 2008.[14] 9/11 forced US to tighten visa requirements for students and Australia and the UK cashed in on this opportunity and were successful in absorbing most of the growth in international students. The growth story for Australia and the UK would have continued, but the recession of 2008 exposed two aspects of international student enrollment in these countries—unmanageable high proportion of international students compared to home students and issues of quality raised by the use of aggressive recruitment practices.[15] In 2009, international students represented 21.5% and 15.3% of higher education enrollment in Australia and the UK, compared to less than 4% in the US, according to the OECD.[16]		According to OECD, almost one out of five international students is regionally mobile. This segment of regionally mobile students who seek global education at local cost is defined as "glocal" students. Many "glocal" students consider pursuing transnational or cross-border education which allows them to earn a foreign credential while staying in their home country.[17]		The number of US visas issued to Chinese students to study at US universities has increased by 30 per cent, from more than 98,000 in 2009 to nearly 128,000 in October 2010, placing China as the top country of origin for international students, according to the "2010 Open Doors Report" published on the US Embassy in China website. The number of Chinese students increased. Overall, the total number of international students with a US Visa to study at colleges and universities increased by 3 per cent to a record high of nearly 691,000 in the 2009/2010 academic year. The 30 per cent increase in Chinese student enrolment was the main contributor to this year's growth, and now Chinese students account for more than 18 percent of the total international students.[18]		In 2006, with approximately 20% of the world's foreign students, or 515,000 out of the 2.7 million students studying outside their countries, Germany and France are best understood as secondary higher education destinations.[3]		Japan, Canada and New Zealand are perceived as evolving destinations for international students. In 2006, Japan, Canada and New Zealand together shared roughly 13% of the international student market, with approximately 327,000 of the 2.7 million students who traveled abroad for the purposes of higher education.[3] Japan has around 180 000 overseas students studying at its institutions and the government has set targets to increase this to 300, 000 over the next few years.[19] Canada has seen a large increase in the number of Indian students, where the number of Indian students rose 280% in 2010 compared to 2008.[20] Organizations such as Learnhub are taking advantage of this growing trend of Indian international students by providing recruitment services that bring Indian students abroad.[21] In 2012, in Canada 10 percent of university students were international students.[22] Canada accepted more than 100,000 international students for the first time,[23] bringing the total number of international students in Canada to 260,000,[24] which is nearly identical to that of Australia's 280,000. Recent changes to Canada's immigration regulations that came into effect on January 1, 2015 have placed international graduates from Canadian universities at a disadvantage. Under the new rules, foreign students who hold a degree or diploma from Canadian educational institutions will be treated on par with other groups of skilled workers.[25]		Malaysia, Singapore and India are the emerging destinations for international students. These three countries have combined share of approximately 12% of the global student market with somewhere between 250,000 and 300,000 students having decided to pursue higher education studies in these countries in 2005–2006.[3]		The flow of international students above indicates the South-North phenomenon. In this sense, students from Asia prefer to pursue their study particularly in the United States.		The recent statistics on mobility of international students can be found in;		Prospective international students are usually required to sit for language tests, such as Cambridge English: First,[29] Cambridge English: Advanced,[30] Cambridge English: Proficiency,[31] IELTS,[32] TOEFL,[33] iTEP,[34] PTE Academic,[35] DELF[36] or DELE,[37] before they are admitted. Tests notwithstanding, while some international students already possess an excellent command of the local language upon arrival, some find their language ability, considered excellent domestically, inadequate for the purpose of understanding lectures, and/or of conveying oneself fluently in rapid conversations. A research report commissioned by NAFSA: Association of International Educators investigated the scope of third-party providers offerings intensive English preparation programs with academic credit for international students in the United States.[38] These pathway programs are designed to recruit and support international students needing additional help with English and academic preparation before matriculating to a degree program.		Generally, international students as citizens of other foreign countries are required to obtain a student visa, which ascertains their legal status for staying in the second country.[39] In the United States, before international students come to the country, the students must select a school to attend to qualify for a student visa. The course of study and the type of school an international student plans to attend determine whether an F-1 visa or an M-1 visa is needed. Each student visa applicant must prove they have the financial ability to pay for their tuition, books and living expenses while they study in the states.[40]		Research from the National Association of Foreign Student Advisers (NAFSA) shows the economic benefits of the increasing international higher-education enrollment in the United States. According to their 2013-2014 academic year analysis, international students have contributed $26.8 billion to the U.S economy and 340,000 jobs. This is almost a 12% increase in dollars added to the economy and an 8.5% increase associated with job support and creation compared to the previous year. International students contribute more than job and monetary gains to the economy. NAFSA Executive Director and CEO Marlene M. Johnson has stated, "[international students] bring global perspectives into U.S. classrooms and research labs, and support U.S. innovation through science and engineering coursework."[41] According to NAFSA's research, their diverse views contribute to technological innovation has increased America's ability to compete in the global economy.		Marketing of higher education is a well-entrenched macro process today, especially in the major English-speaking nations i.e. Australia, Canada, New Zealand, the UK, and the USA. One of the major factors behind the worldwide evolution of educational marketing could be a result of globalization, which has dramatically shriveled the world. Due to intensifying competition for overseas students amongst MESDCs, i.e. major English-speaking destination countries, higher educational institutions recognize the significance of marketing themselves, in the international arena.[42] To build sustainable international student recruitment strategies Higher Education Institutions (HEIs) need to diversify the markets from which they recruit, both to take advantage of future growth potential from emerging markets, and to reduce dependency on – and exposure to risk from – major markets such as China, India and Nigeria, where demand has proven to be volatile.[43] For recruitment strategies, there are some approaches that higher education institutions adopt to ensure stable enrollments of international students, such as developing university preparation programs, like the Global Assessment Certificate (GAC) Program, and launching international branch campuses in foreign countries.		The Global Assessment Certification (GAC) Program is a university preparation program, developed and provided by ACT Education Solution, Ltd., for the purpose of helping students to prepare for admission and enrolment overseas.[7] Moreover, the program helps students from non-English speaking backgrounds to prepare for university-level study, so they are able to successfully finish a bachelor's degree at university. This program is primarily getting great attention from non-English-speaking countries like China and South Korea. Students who complete the GAC program have the opportunity to be admitted to 120 universities, so called Pathway Universities, located in popular destinations including the United States, the United Kingdom, and Canada.[44] Mainly, the program consists of curriculums, such as Academic English, Mathematics, Computing, Study Skills, Business, Science and Social Science.Moreover, program also provides the opportunity to get prepared for the ACT exam and English Proficiency tests like TOEFL and IELTS.[45]		International branch campus is a new strategy of recruiting international students in overseas countries in order to build strong global outreach by overcoming the limitations of physical distance. Indeed, opening international branch campuses play a significant role of widening the landscape of the international higher education. In the past, along with high demand for higher education, many universities in the United States established their branch campuses in foreign countries.[46] According to a report by the Observatory on Borderless Higher Education (OBHE), there was a 43% increase in the number of international branch campuses in the worldwide scale since 2006. American higher education institutions mostly take a dominant position in growth rate and the number of international branch campuses, accounting for almost 50 percent of current international branch campuses.[47] However, some research reports that recently said internationally branching campuses are facing several challenges and setbacks, for example interference of local government,[48] sustainability problems, and long-term prospects like damage on academic reputations and finance.		There is a trend for more and more students to go abroad to study in the U.S., Canada, U.K., and Australia to gain a broader education.[citation needed] English is the only common language spoken at universities in these countries. International students not only need to acquire good communication skills and fluent English both in writing and speaking, but also absorb the Western academic writing culture in style, structure, reference, and the local policy toward academic integrity in academic writing.[49] International students may have difficulty completing satisfactory assignments because of the difficulty with grammar and spelling, differences in culture, or a lack of confidence in English academic writing. Insightful opinions may lose the original meaning when transformed from the student's native language to English. Even if international students acquire good scores in English proficiency exams or are able to communicate with native British students frequently in class, they often find that the wording and formatting of academic papers in English-speaking universities are different from what they are used to.[50]		Most international students encounter difficulties in language use. Such issues make it difficult for the student to make domestic friends and gain familiarity with the local culture. Sometimes, these language barriers can subject international students to ignorance or disrespect from native speakers.[51] Most international students are also lacking a support groups in the country they are studying. Although all the colleges in North America, that are in a student exchange programs, do have International Student Office, it sometimes does not have resources and capability to consider their students' individual needs when it comes to adapting the new environment. The more a particular college has students coming from the same country the better the support is for getting involved to the new culture.		International students have several challenges in their academic studies at North American universities. Studies have shown that these challenges include several different factors: inadequate English proficiency; unfamiliarity with North American culture; lack of appropriate study skills or strategies; academic learning anxiety; low social self-efficacy; financial difficulties; and separation from family and friends.[52] Despite the general perception that American culture is characterized more by diversity than by homogeneity, the American ideology of cultural homogeneity implies an American mindset that because Eurocentric cultures are superior to others, people with different cultures should conform to the dominant monocultural canon and norms.[53]		U.S. colleges and universities have long welcomed students from China, where their higher-education system cannot meet the demand. Three years ago, a record 10 million students throughout China took the national college entrance test, competing for 5.7 million university slots. Because foreign undergraduates typically fail to qualify for U.S. federal aid, colleges here can provide limited financial help. Now, thanks to China's booming economy in recent years, more Chinese families can afford to pay. U.S. colleges also face challenges abroad. Worries about fraud on test scores and transcripts make occasional headlines. And even Chinese students who test high on an English-language proficiency test may not be able to speak or write well enough to stay up to speed in a U.S. classroom, where essay writing and discussions are common.[54] Chinese international students face other challenges besides language proficiency. The Chinese educational structure focuses on exam-oriented education, with educational thinking and activities aimed towards meeting the entrance examination. Students stress more on exam performance, and teachers are inclined to focus on lecturing to teach students what may be on the test. In addition, "parents are also convinced that the more students listened to the lectures, the better they would score on the finals."[55] With more than 304,040 Chinese students enrolled in the US in 2014/15, China is by far the leading source of international students at American universities and colleges, however, there are three waves of growth in Chinese students in the US. Each of the three waves differs in terms of needs and expectations and corresponding support services needed. Unfortunately, many higher education institutions have not adapted to the changing needs.[56] It is no surprise that many Chinese students are now questioning if it is worth investing in studying abroad.[57]		International students also face cross-cultural barriers that hinder their ability to succeed in a new environment.[58] For example, there are differences in terms of receiving and giving feedback which influences the academic engagement and even job and internship search approach of international students.[59]		Plagiarism is the most serious offense in academia.[60] Plagiarism has two subtle forms, one of which includes the omission of elements required for proper citations and references.[61] The second form is unacknowledged use or incorporation of another person's work or achievement. Violation of either form can result in a student's expulsion. For the international students the word plagiarism is a foreign word.[62] Most of them are unfamiliar with American academic standards and colleges aren’t good about giving a clear definition of the word's meaning. For example, many international students don’t know using even one sentence of someone else's work can be considered plagiarism. Most colleges give students an E on their plagiarized assignments and future offenses often result in failing class or being kicked out of university.		
A master's degree[fn 1] (from Latin magister) is a second-cycle academic degree awarded by universities or colleges upon completion of a course of study demonstrating mastery or a high-order overview of a specific field of study or area of professional practice.[1] A master's degree normally requires previous study at the bachelor's level, either as a separate degree or as part of an integrated course. Within the area studied, master's graduates are expected to possess advanced knowledge of a specialized body of theoretical and applied topics; high order skills in analysis, critical evaluation, or professional application; and the ability to solve complex problems and think rigorously and independently.						The master's degree dates back to the origin of European universities, with a Papal bull of 1233 decreeing that anyone admitted to the mastership in the University of Toulouse should be allowed to teach freely in any other university. The original meaning of the master's degree was thus that someone who had been admitted to the rank (degree) of master (i.e. teacher) in one university should be admitted to the same rank in other universities. This gradually became formalised as the licentia docendi (licence to teach). Originally, masters and doctors were not distinguished, but by the 15th century it had become customary in the English universities to refer to the teachers in the lower faculties (arts and grammar) as masters and those in the higher faculties as doctors.[2] Initially, the Bachelor of Arts (BA) was awarded for the study of the trivium and the Master of Arts (MA) for the study of the quadrivium.[3]		From the late Middle Ages until the nineteenth century, the pattern of degrees was therefore to have a bachelor's and master's degree in the lower (undergraduate) faculties and to have bachelor's and doctorates in the higher (postgraduate) faculties. In the United States, the first master's degrees (Magister Artium, or Master of Arts) were awarded at Harvard University soon after its foundation.[4] In Scotland the pre-Reformation universities (St Andrews, Glasgow and Aberdeen) developed so that the Scottish MA became their first degree, while in Oxford, Cambridge and Trinity College, Dublin, the MA was awarded to BA graduates of a certain standing without further examination from the late seventeenth century, its main purpose being to confer full membership of the university.[5] At Harvard the 1700 regulations required that candidates for the master's degree had to pass a public examination,[6] but by 1835 this was awarded Oxbridge-style 3 years after the BA.[7]		The nineteenth century saw a great expansion in the variety of master's degrees offered. At the start of the century, the only master's degree was the MA, and this was normally awarded without any further study or examination.The Master in Surgery degree was introduced by the University of Glasgow in 1815.[8] By 1861 this had been adopted throughout Scotland as well as by Cambridge and Durham in England and the University of Dublin in Ireland.[9] When the Philadelphia College of Surgeons was established in 1870, it too conferred the Master of Surgery, "the same as that in Europe".[10]		In Scotland, Edinburgh maintained separate BA and MA degrees until the mid nineteenth century,[11] although there were major doubts as to the quality of the Scottish degrees of this period. In 1832 Lord Brougham, the Lord Chancellor and an alumnus of the University of Edinburgh, told the House of Lords that "In England the Universities conferred degrees after a considerable period of residence, after much labour performed, and if they were not in all respects so rigorous as the statutes of the Universities required, nevertheless it could not be said, that Masters of Arts were created at Oxford and Cambridge as they were in Scotland, without any residence, or without some kind of examination. In Scotland all the statutes of the Universities which enforced conditions on the grant of degrees were a dead letter."[12]		It was not until 1837 that separate examinations were reintroduced for the MA in England, at the newly established Durham University (even though, as in the ancient English universities, this was to confer full membership), to be followed in 1840 by the similarly new University of London, which was only empowered by its charter to grant degrees by examination.[13][14][15] However, by the middle of the century the MA as an examined second degree was again under threat, with Durham moving to awarding it automatically to those who gained honours in the BA in 1857, along the lines of the Oxbridge MA, and Edinburgh following the other Scottish universities in awarding the MA as its first degree, in place of the BA, from 1858.[16] At the same time, new universities were being established around the then British Empire along the lines of London, including examinations for the MA: the University of Sydney in Australia and the Queen's University of Ireland in 1850, and the Universities of Bombay (now the University of Mumbai), Madras and Calcutta in India in 1857.		In the US, the revival of master's degrees as an examined qualification began in 1856 at the University of North Carolina, followed by the University of Michigan in 1859,[17] although the idea of a master's degree as an earned second degree was not well established until the 1870s, alongside the PhD as the terminal degree.[18] Sometimes it was possible to earn an MA either by examination or by seniority in the same institution, e.g. in Michigan the "in course" MA was introduced in 1848 and was last awarded in 1882, while the "on examination" MA was introduced in 1859.[19]		Probably the most important master's degree introduced in the 19th century was the Master of Science (MS in the US, MSc in the UK). At the University of Michigan this was introduced in two forms in 1858: "in course", first awarded in 1859, and "on examination", first awarded in 1862. The "in course" MS was last awarded in 1876.[19] In Britain, however, the degree took a while longer to arrive. When London introduced its Faculty of Sciences in 1858, the University was granted a new charter giving it the power "to confer the several Degrees of Bachelor, Master, and Doctor, in Arts, Laws, Science, Medicine, Music",[20] but the degrees it awarded in science were the Bachelor of Science and the Doctor of Science.[21] The same two degrees, again omitting the master's, were awarded at Edinburgh, despite the MA being the standard undergraduate degree for Arts in Scotland.[22] In 1862, a Royal Commission suggested that Durham should award master's degrees in theology and science (interestingly with the suggested abbreviations MT and MS, contrary to later British practice of using MTh or MTheol and MSc for these degrees),[23] but its recommendations were not enacted. In 1877, Oxford introduced the Master of Natural Science, along with the Bachelor of Natural Science, to stand alongside the MA and BA degrees and be awarded to students who took their degrees in the honours school of natural sciences.[24] In 1879 a statute to actually establish the faculty of Natural Sciences at Oxford was promulgated,[25] but in 1880 a proposal to rename the degree as a Master of Science was rejected along with a proposal to grant Masters of Natural Sciences a Master of Arts degree, in order to make them full members of the University.[26] This scheme would appear to have then been quietly dropped, with Oxford going on to award BAs and MAs in science.		The Master of Science (MSc) degree was finally introduced in Britain in 1878 at Durham,[27] followed by the new Victoria University in 1881.[28] At the Victoria University both the MA and MSc followed the lead of Durham's MA in requiring a further examination for those with an ordinary bachelor's degree but not for those with an honours degree.[29]		At the start of the twentieth century there were therefore four different sorts of master's degree in the UK: the Scottish MA, granted as a first degree; the Master of Arts (Oxbridge and Dublin), granted to all BA graduates a certain period after their first degree without further study; master's degrees that could be gained either by further study or by gaining an honours degree (which, at the time in the UK involved further study beyond the ordinary degree, as it still does in Scotland and some Commonwealth countries); and master's degrees that could only be obtained by further study (including all London master's degrees). In 1903, the London Daily News criticised the practice of Oxford and Cambridge, calling their MAs "the most stupendous of academic frauds" and "bogus degrees".[30] Ensuing correspondence pointed out that "A Scotch M.A., at the most, is only the equivalent of an English B.A." and called for common standards for degrees, while defenders of the ancient universities said that "the Cambridge M.A. does not pretend to be a reward of learning" and that "it is rather absurd to describe one of their degrees as a bogus one because other modern Universities grant the same degree for different reasons".[31][32]		In 1900, Dartmouth College introduced the Master of Commercial Science (MCS), first awarded in 1902. This was the first master's degree in business, the forerunner of the modern MBA.[33] The idea quickly crossed the Atlantic, with Manchester establishing a Faculty of Commerce, awarding Bachelor and Master of Commerce degrees, in 1903.[34] Over the first half of the century the automatic master's degrees for honours graduates vanished as honours degrees became the standard undergraduate qualification in the UK. In the 1960s, new Scottish universities (with the exception of Dundee, which inherited the undergraduate MA from St Andrews) reintroduced the BA as their undergraduate degree in Arts, restoring the MA to its position as a postgraduate qualification. Oxford and Cambridge retained their MAs, but renamed many of their postgraduate bachelor's degrees in the higher faculties as master's degrees, e.g. the Cambridge LLB became the LLM in 1982,[35] and the Oxford BLitt, BPhil (except in philosophy) and BSc became the MLitt, MPhil and MSc.[36]		In 1983, the Engineering Council issued a "'Statement on enhanced and extended undergraduate engineering degree courses", proposing the establishment of a 4-year first degree (Master of Engineering).[37][38] These were up and running by the mid 1980s and were followed in the early 1990s by the MPhys for physicists and since then integrated master's degrees in other sciences such as MChem, MMath, MGeol, etc., and in some institutions general or specific MSci (Master in Science) and MArts (Master in Arts) degrees. This development was noted by the Dearing Report into UK Higher Education in 1997, which called for the establishment of a national framework of qualifications and identified five different routes to master's degrees:[39]		This led to the establishment of the Quality Assurance Agency, which was charged with drawing up the framework.		In 2000 renewed pressure was put on Oxbridge MAs in the UK Parliament, with Labour MP Jackie Lawrence introducing an early day motion calling for them to be scrapped and telling the Times Higher Education it was a "discriminatory practice" and that it "devalues and undermines the efforts of students at other universities".[40][41] The following month the Quality Assurance Agency announced the results of a survey of 150 major employers showing nearly two thirds mistakenly thought the Cambridge MA was a postgraduate qualification and just over half made the same error regarding the Edinburgh MA, with QAA chief executive John Randall calling the Oxbridge MA "misleading and anachronistic".[42]		The QAA released the first "framework for higher education qualifications in England, Wales and Northern Ireland" in January 2001. This specified learning outcomes for M-level (master's) degrees and advised that the title "Master" should only be used for qualifications that met those learning outcomes in full. It addressed many of the Dearing Report's concerns, specifying that shorter courses at H-level (honours), e.g. conversion courses, should be styled Graduate Diploma or Graduate Certificate rather than as master's degrees, but confirmed that the extended undergraduate degrees were master's degrees, saying that "Some Masters degrees in science and engineering are awarded after extended undergraduate programmes that last, typically, a year longer than Honours degree programmes". It also addressed the Oxbridge MA issue, noting that "the MAs granted by the Universities of Oxford and Cambridge are not academic qualifications".[43] The first "framework for qualifications of Higher Education Institutes in Scotland", also published in January 2001, used the same qualifications descriptors, adding in credit values that specified that a stand-alone master should be 180 credits and a "Masters (following an integrated programme from undergraduate to Masters level study)" should be 600 credits with a minimum of 120 at M-level. It was specified that the title "Master" should only be used for qualifications that met the learning outcomes and credit definitions, although it was noted that "A small number of universities in Scotland have a long tradition of labelling certain first degrees as 'MA'. Reports of Agency reviews of such provision will relate to undergraduate benchmarks and will make it clear that the title reflects Scottish custom and practice, and that any positive judgement on standards should not be taken as implying that the outcomes of the programme were at postgraduate level."[44]		The Bologna declaration in 1999 started the Bologna Process, leading to the creation of the European Higher Education Area (EHEA). This established a three-cycle bachelor's—master's—doctorate classification of degrees, leading to the adoption of master's degrees across the continent, often replacing older long-cycle qualifications such as the magister degree in Germany.[45] As the process continued, descriptors were introduced for all three levels in 2004, and ECTS credit guidelines were developed. This led to questions as to the status of the integrated master's degrees and one-year master's degrees in the UK.[46] However, the Framework for Higher Education Qualifications in England, Wales and Northern Ireland and the Framework for Qualifications of Higher Education Institutes in Scotland have both been aligned with the overarching framework for the EHEA with these being accepted as master's-level qualifications.		Master's degrees are commonly titled using the form 'Master of ...', where either a faculty (typically Arts or Science) or a field (Engineering, Physics, Chemistry, Business Administration, etc.) is specified. The two most common titles of master's degrees are the Master of Arts (MA/M.A./A.M) and Master of Science (MSc/M.S./S.M.) degrees; which normally consist of a mixture of research and taught material.[47][48] Integrated master's degrees and postgraduate master's degrees oriented towards professional practice are often more specifically named for their field of study ("tagged degrees"), including, for example, the Master of Business Administration, Master of Divinity, Master of Engineering and Master of Physics. A few titles are more general, for example Master of Philosophy (MPhil), used (in the same manner as Doctor of Philosophy) to indicate degrees with a large research component,[49] Master of Studies (MSt)/Master of Advanced Study (MASt)/Master of Advanced Studies (M.A.S.), and Professional Master's (MProf).		The form "Master in ..." is also sometimes used, particularly where a faculty title is used for an integrated master's in addition to its use in a traditional postgraduate master's, e.g. Master in Science (MSci) and Master in Arts (MArts). This form is also sometimes used with other integrated master's degrees,[50] and occasionally for postgraduate master's degrees (e.g. Master's in Accounting).[51] Some universities use Latin degree names; because of the flexibility of syntax in Latin, the Master of Arts and Master of Science degrees may be known in these institutions as Magister artium and Magister scientiæ or reversed from the English order to Artium magister and Scientiæ magister. Examples of the reversed usage include Harvard University, the University of Chicago and MIT, leading to the abbreviations A.M. and S.M. for these degrees. The forms "Master of Science" and "Master in Science" are indistinguishable in Latin, thus MSci is "Master of Natural Sciences" at the University of Cambridge.		In the UK, stops (periods) are not used in degree abbreviations.[52][53] In the US, The Gregg Reference Manual recommends placing periods in degrees (e.g. B.S., Ph.D.), however The Chicago Manual of Style recommends writing degrees without periods (e.g. BS, PhD).[54]		Master of Science is generally abbreviated M.S. or MS in countries following United States usage and MSc in countries following British usage, where MS would refer to the degree of Master of Surgery. In Australia, some extended master's degrees use the title "doctor": Juris doctor and Doctors of Medical Practice, Physiotherapy, Dentistry, Optometry and Veterinary Practice. Despite their titles these are still master's degree and may not be referred to as doctoral degrees, nor may graduates use the title "doctor".[55]		The UK Quality Assurance Agency defines three categories of Master's degrees:[59]		The United States Department of Education classifies master's degree as research or professional. Research master's degrees in the US, e.g. M.A./A.M. or M.S., require the completion of taught courses and examinations in a major and one or more minor subjects, and (normally) a research thesis. Professional master's degrees may be structured like research master's (e.g. M.E./M.Eng.) or may concentrate on a specific discipline (e.g. M.B.A.), and often substitute a project for the thesis.[48]		The Australian Qualifications Framework classifies master's degrees as research, coursework or extended. Research master's degrees typically take one to two years, and two thirds of their content consists of research, research training and independent study. Coursework master's degrees typically also last one to two years, and consist mainly of structured learning with some independent research and project work or practice-related learning. Extended master's degrees typically take three to four years and contain significant practice-related learning that must be developed in collaboration with relevant professional, statutory or regulatory bodies.[61]		In Ireland, master's degrees may be either Taught or Research. Taught master's degrees are normally one to two year courses, rated at 60 - 120 ECTS credits, while research master's degrees are normally two year courses, either rated at 120 ECTS credits or not credit rated.[62]		There are a range of pathways to the degree, with entry based on evidence of a capacity to undertake higher degree studies in the proposed field. A dissertation may or may not be required, depending on the program. In general, the structure and duration of a program of study leading to a master's degree will differ by country and by university.		Stand-alone master's programs in the US are normally two years in length. In some fields/programs, work on a doctorate begins immediately after the bachelor's degree, but a master's may be granted along the way as an intermediate qualification if the student petitions for it.[48] Some universities offer evening options so that students can work during the day and earn a master's degree in the evenings.[63]		In the UK, postgraduate master's degrees may be either "research" or "taught", with taught degrees being further subdivided into "specialist or advanced study" or "professional or practice" (see above). Taught degrees (of both forms) typically take a full calendar year (i.e. three semesters, 12 months), although some may be completed within an academic year (i.e. two semesters, 8 months), while research degrees often take either a full calendar year or two academic years.[47] The UK integrated master's degree is combined with a bachelor's degree for a four (England, Wales and Northern Ireland) or five (Scotland) academic year total period - one academic year longer than a normal bachelor's degree.[56]		In Australia, master's degrees vary from 1 year for a "research" or "coursework" master's following on from an Australian honours degree in a related field, with an extra six months if following on straight from an ordinary bachelor's degree and another extra six months if following on from a degree in a different field, to four years for an "extended" master's degree.[61]		In the Overarching Framework of Qualifications for the European Higher Education Area defined as part of the Bologna process, a "second cycle" (i.e. master's degree) programme is typically 90–120 ECTS credits, with a minimum requirement of at least 60 ECTS credits at second-cycle level.[64] The definition of ECTS credits is that "60 ECTS credits are allocated to the learning outcomes and associated workload of a full-time academic year or its equivalent",[65] thus European master's degrees should last for between one calendar year and two academic years, with at least one academic year of study at master's level. The Framework for Higher Education Qualification (FHEQ) in England Wales and Northern Ireland level 7 qualifications and the Framework for Qualification of Higher Education Institutes in Scotland (FQHEIS) level 11 qualifications (postgraduate and integrated master's degrees, with the exception of MAs from the ancient universities of Scotland and Oxbridge MAs) have been certified as meeting this requirement.[66][67]		Irish master's degrees are 1 – 2 years (60 - 120 ECTS credits) for taught degrees and 2 years (not credit rated) for research degrees. These have also been certified as compatible with the FQ-EHEA.[68]		Admission to a master's degrees normally requires successful completion of study at bachelor's degree level either (for postgraduate degrees) as a stand-alone degree or (for integrated degrees) as part of an integrated scheme of study. In countries where the bachelor's degree with honours is the standard undergraduate degree, this is often the normal entry qualification.[59][69] In addition, students will normally have to write a personal statement and, in the arts and humanities, will often have to submit a portfolio of work.[70]		In the UK, students will normally need to have a 2:1 for a taught master's course, and possibly higher for a research master's.[71] Graduate schools in the US may require students to take one or more standardised tests, such as the GRE, GMAT or LSAT.[72]		In some European countries, a magister is a first degree and may be considered equivalent to a modern (standardized) master's degree (e.g., the German, Austrian and Polish university Diplom/Magister, or the similar five-year Diploma awarded in several subjects in Greek,[73] Spanish, Portuguese, and other universities and polytechnics).[clarification needed]		Under the Bologna Process, countries in the European Higher Education Area (EHEA) are moving to a three cycle (bachelor's - master's - doctorate) system of degrees. Two thirds of EHEA countries have standardised on 120 ECTS credits for their second-cycle (master's) degrees, but 90 ECTS credits is the main form in Cyprus, Ireland and Scotland and 60-75 credits in Montenegro, Serbia and Spain.[74] The combined length of the first and second cycle varies from "3 + 1" years (240 ECTS credits), through "3 + 2" or "4 + 1" years (300 ECTS credits), to "4 + 2" years (360 ECTS credits). As of 2015, 31 EHEA countries have integrated programmes that combine the first and second cycle and lead to a second-cycle qualification (e.g. the UK integrated master's degree), particularly in STEM subjects and subjects allied to medicine. These typically have a duration of 300 – 360 ECTS credits (five to six years), with the integrated master's degrees in England, Wales and Northern Ireland being the shortest at 240 ECTS credits (four years).[75]		In Brazil, after a regular graduation (after acquiring a bachelor's degree), students have the option to continue their academic career through a master's course (a.k.a. stricto sensu post-graduation) or specialization (a.k.a. lato sensu post-graduation) degrees. At the master's degree ("mestrado", in Portuguese, also referred as "pós-graduação stricto sensu") there are 2–3 years of full-time graduate-level studies. Usually focused on academic research, the master's degree (on any specific knowledge area) requires the development of a thesis, presented (and defended) to a board of Ph.D. after the period of research. Differently, the "specialization" degree (also referred as "pós-graduação lato-sensu"), also comprehends a 1–2 years studies, but do not require a new thesis to be purposed and defended, being usually attended by professionals looking for a complimentary formation on a different knowledge area than their original graduation.		In addition, a great part of Brazilian universities offers a M.B.A. (Master of Business Administration) degree. Those, nevertheless, are not the equivalent of US M.B.A. degree though, as it does not formally certifies the student/professional with a master's degree (stricto-sensu) but a post-graduation degree instead. A regular post-graduation course has to comply with a minimum of 360 class-hours, while a M.B.A. degree has to comply with a minimum of 400 class-hours. Master's degree (stricto sensu) does not requires minimum class-hours, but it's practically impossible to finish it before 1.5 year due the workload and research required; an average time for the degree is 2.5 years[citation needed]. Specialization (lato sensu) and M.B.A. degrees can be also offered as distance education courses, while the master's degree (stricto-sensu) requires physical attendance. In Brazil, the degree often serves as additional qualification for those seeking to differentiate themselves in the job market, or for those who want to pursue a Ph.D. It corresponds to the European (Bologna Process) 2nd Cycle or the North American master's.		M.Arch., M.L.A., M.U.D., M.A., M.Sc., M.Soc.Sc., M.S.W., M.Eng., LL.M. Hong Kong requires one or two years of full-time coursework to achieve a master's degree.		For part-time study, two or three years of study are normally required to achieve a postgraduate degree.		M.Phil. As in the United Kingdom, M.Phil. or Master of Philosophy is a research degree awarded for the completion of a thesis, and is a shorter version of the Ph.D.		In Pakistani education system, there are two different master's degree programmes[citation needed]:		Both M.A. and M.S. are offered in all major subjects.		In the Indian system, a master's degree is a postgraduate degree following a Bachelor's degree and preceding a Doctorate, usually requiring two years to complete. The available degrees include:		In Nepal, after bachelor's degree about to at least three or four years with full-time study in college and university with an entrance test for those people who want to study further can study in master and further Ph.D. and Doctorate degree. All Doctoral and Ph.D. or third cycle degree are based on research and experience oriented and result based. Master of Engineering (M.Eng.), Master of Education (M.Ed.), Master of Arts (M.A.) and all law and medicine related courses are studied after completion of successful bachelor towards doctoral degree. M.B.B.S. is only a medical degree with six and half years of study resulting medical doctor and need to finish its study o 4 years of period joining after master degree with minimum education with 15 or 16 years of university bachelor's degree education. The most professional and internationalised program in Nepal are:		In Taiwan, bachelor's degrees are about four years (with honors) and there is an entrance examination required for people who want to study in master and Ph.D. degrees. The courses offered for master and PhD normally are research-based. The most foreign student-friendly programs in Taipei, Taiwan are at:		Programs are entirely in English and tuition is less than would be paid in North America, with as little as US$5000 for an M.B.A.[citation needed] As an incentive to increase the number of foreign students, the government of Taiwan and universities have made extra efforts to provide a range of quality scholarships available.[citation needed] These are university-specific scholarships ranging from tuition waivers, up to NT$20,000 per month. The government offers the Taiwan Scholarship ranging from NT$20,000–30,000 per month for two years. (US$18,000–24,000 for a two-year program)		
Freshman Year may refer to:		
The School of Hard Knocks — or University of Hard Knocks — is an idiomatic phrase meaning the (sometimes painful) education one gets from life's usually negative experiences, often contrasted with formal education. The term is frequently misattributed to George Ade,[1] but was actually coined by Elbert Hubbard in a piece he wrote on himself for Cosmopolitan in 1902.[2][3]		It is a phrase which is most-typically used by a person to claim a level of wisdom imparted by life experience, which should be considered at least equal in merit to academic knowledge. It is a response that may be given when one is asked about his or her education, particularly if they do not have an extensive formal education but rather life experiences that should be valued instead. It may also be used facetiously, to suggest that formal education is not of practical value compared with "street" experience. In the UK and New Zealand, the phrases "University of Life" and "School of Hard Knocks" may be used interchangeably.[4]		In 1947, newspaperman James Franklin Comstock ("Jim" Comstock) founded the “University of Hard Knocks”, an honorary society with a mission to recognize people who have made a success of their life without the benefit of higher education. Alderson Broaddus College in Philippi, West Virginia, USA, sponsored the organization, which moved its offices to the A-B campus in 1976. The society was dissolved in 2014.		
A university (Latin: universitas, "a whole") is an institution of higher (or tertiary) education and research which awards academic degrees in various academic disciplines. Universities typically provide undergraduate education and postgraduate education.		The word "university" is derived from the Latin universitas magistrorum et scholarium, which roughly means "community of teachers and scholars."[1] Universities were created in Italy and evolved from Cathedral schools for the clergy during the High Middle Ages.[2]						The original Latin word "universitas" refers in general to "a number of persons associated into one body, a society, company, community, guild, corporation, etc."[3] At the time of the emergence of urban town life and medieval guilds, specialized "associations of students and teachers with collective legal rights usually guaranteed by charters issued by princes, prelates, or the towns in which they were located" came to be denominated by this general term. Like other guilds, they were self-regulating and determined the qualifications of their members.[4]		In modern usage the word has come to mean "An institution of higher education offering tuition in mainly non-vocational subjects and typically having the power to confer degrees,"[5] with the earlier emphasis on its corporate organization considered as applying historically to Medieval universities.[6]		The original Latin word referred to degree-granting institutions of learning in Western and Central Europe, where this form of legal organisation was prevalent, and from where the institution spread around the world.		An important idea in the definition of a university is the notion of academic freedom. The first documentary evidence of this comes from early in the life of the first university. The University of Bologna adopted an academic charter, the Constitutio Habita,[7] in 1158 or 1155,[8] which guaranteed the right of a traveling scholar to unhindered passage in the interests of education. Today this is claimed as the origin of "academic freedom".[9] This is now widely recognised internationally - on 18 September 1988, 430 university rectors signed the Magna Charta Universitatum,[10] marking the 900th anniversary of Bologna's foundation. The number of universities signing the Magna Charta Universitatum continues to grow, drawing from all parts of the world.		The university is generally regarded as a formal institution that has its origin in the Medieval Christian setting.[11][12] European higher education took place for hundreds of years in Christian cathedral schools or monastic schools (scholae monasticae), in which monks and nuns taught classes; evidence of these immediate forerunners of the later university at many places dates back to the 6th century.[13] The earliest universities were developed under the aegis of the Latin Church by papal bull as studia generalia and perhaps from cathedral schools. It is possible, however, that the development of cathedral schools into universities was quite rare, with the University of Paris being an exception.[14] Later they were also founded by Kings (University of Naples Federico II, Charles University in Prague, Jagiellonian University in Kraków) or municipal administrations (University of Cologne, University of Erfurt). In the early medieval period, most new universities were founded from pre-existing schools, usually when these schools were deemed to have become primarily sites of higher education. Many historians state that universities and cathedral schools were a continuation of the interest in learning promoted by monasteries.[15] Pope Gregory VII was critical in promoting and regulating the concept of modern university as his 1079 Papal Decree ordered the regulated establishment of cathedral schools that transformed themselves into the first European universities.[16]		The first universities in Europe with a form of corporate/guild structure were the University of Bologna (1088), the University of Paris (c.1150, later associated with the Sorbonne), and the University of Oxford (1167).		The University of Bologna began as a law school teaching the ius gentium or Roman law of peoples which was in demand across Europe for those defending the right of incipient nations against empire and church. Bologna's special claim to Alma Mater Studiorum[clarification needed] is based on its autonomy, its awarding of degrees, and other structural arrangements, making it the oldest continuously operating institution[8] independent of kings, emperors or any kind of direct religious authority.[17][18]		The conventional date of 1088, or 1087 according to some,[19] records when Irnerius commences teaching Emperor Justinian's 6th century codification of Roman law, the Corpus Iuris Civilis, recently discovered at Pisa. Lay students arrived in the city from many lands entering into a contract to gain this knowledge, organising themselves into 'Nationes', divided between that of the Cismontanes and that of the Ultramontanes. The students "had all the power … and dominated the masters".[20][21]		In Europe, young men proceeded to university when they had completed their study of the trivium–the preparatory arts of grammar, rhetoric and dialectic or logic–and the quadrivium: arithmetic, geometry, music, and astronomy.		All over Europe rulers and city governments began to create universities to satisfy a European thirst for knowledge, and the belief that society would benefit from the scholarly expertise generated from these institutions. Princes and leaders of city governments perceived the potential benefits of having a scholarly expertise develop with the ability to address difficult problems and achieve desired ends. The emergence of humanism was essential to this understanding of the possible utility of universities as well as the revival of interest in knowledge gained from ancient Greek texts.[22]		The rediscovery of Aristotle's works–more than 3000 pages of it would eventually be translated–fuelled a spirit of inquiry into natural processes that had already begun to emerge in the 12th century. Some scholars believe that these works represented one of the most important document discoveries in Western intellectual history.[23] Richard Dales, for instance, calls the discovery of Aristotle's works "a turning point in the history of Western thought."[24] After Aristotle re-emerged, a community of scholars, primarily communicating in Latin, accelerated the process and practice of attempting to reconcile the thoughts of Greek antiquity, and especially ideas related to understanding the natural world, with those of the church. The efforts of this "scholasticism" were focused on applying Aristotelian logic and thoughts about natural processes to biblical passages and attempting to prove the viability of those passages through reason. This became the primary mission of lecturers, and the expectation of students.		The university culture developed differently in northern Europe than it did in the south, although the northern (primarily Germany, France and Great Britain) and southern universities (primarily Italy) did have many elements in common. Latin was the language of the university, used for all texts, lectures, disputations and examinations. Professors lectured on the books of Aristotle for logic, natural philosophy, and metaphysics; while Hippocrates, Galen, and Avicenna were used for medicine. Outside of these commonalities, great differences separated north and south, primarily in subject matter. Italian universities focused on law and medicine, while the northern universities focused on the arts and theology. There were distinct differences in the quality of instruction in these areas which were congruent with their focus, so scholars would travel north or south based on their interests and means. There was also a difference in the types of degrees awarded at these universities. English, French and German universities usually awarded bachelor's degrees, with the exception of degrees in theology, for which the doctorate was more common. Italian universities awarded primarily doctorates. The distinction can be attributed to the intent of the degree holder after graduation – in the north the focus tended to be on acquiring teaching positions, while in the south students often went on to professional positions.[25] The structure of northern universities tended to be modeled after the system of faculty governance developed at the University of Paris. Southern universities tended to be patterned after the student-controlled model begun at the University of Bologna.[26] Among the southern universities, a further distinction has been noted between those of northern Italy, which followed the pattern of Bologna as a "self-regulating, independent corporation of scholars" and those of southern Italy and Iberia, which were "founded by royal and imperial charter to serve the needs of government."[27]		Their endowment by a prince or monarch and their role in training government officials made these Mediterranean universities similar to Islamic madrasas, although madrasas were generally smaller and individual teachers, rather than the madrasa itself, granted the license or degree.[28] Scholars like Arnold H. Green and Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madrasahs became universities.[29][30] George Makdisi and others,[31] however, argue that the European university has no parallel in the medieval Islamic world.[32] Other scholars regard the university as uniquely European in origin and characteristics.[11][33]		Many scholars (including Makdisi) have argued that early medieval universities were influenced by the religious madrasahs in Al-Andalus, the Emirate of Sicily, and the Middle East (during the Crusades).[35][36][37] Other scholars see this argument as overstated.[38] Lowe and Yasuhara have recently drawn on the well-documented influences of scholarship from the Islamic world on the universities of Western Europe to call for a reconsideration of the development of higher education, turning away from a concern with local institutional structures to a broader consideration within a global context.[39]		During the Early Modern period (approximately late 15th century to 1800), the universities of Europe would see a tremendous amount of growth, productivity and innovative research. At the end of the Middle Ages, about 400 years after the first university was founded, there were twenty-nine universities spread throughout Europe. In the 15th century, twenty-eight new ones were created, with another eighteen added between 1500 and 1625.[40] This pace continued until by the end of the 18th century there were approximately 143 universities in Europe, with the highest concentrations in the German Empire (34), Italian countries (26), France (25), and Spain (23) – this was close to a 500% increase over the number of universities toward the end of the Middle Ages. This number does not include the numerous universities that disappeared, or institutions that merged with other universities during this time.[41] It should be noted that the identification of a university was not necessarily obvious during the Early Modern period, as the term is applied to a burgeoning number of institutions. In fact, the term "university" was not always used to designate a higher education institution. In Mediterranean countries, the term studium generale was still often used, while "Academy" was common in Northern European countries.[42]		The propagation of universities was not necessarily a steady progression, as the 17th century was rife with events that adversely affected university expansion. Many wars, and especially the Thirty Years' War, disrupted the university landscape throughout Europe at different times. War, plague, famine, regicide, and changes in religious power and structure often adversely affected the societies that provided support for universities. Internal strife within the universities themselves, such as student brawling and absentee professors, acted to destabilize these institutions as well. Universities were also reluctant to give up older curricula, and the continued reliance on the works of Aristotle defied contemporary advancements in science and the arts.[43] This era was also affected by the rise of the nation-state. As universities increasingly came under state control, or formed under the auspices of the state, the faculty governance model (begun by the University of Paris) became more and more prominent. Although the older student-controlled universities still existed, they slowly started to move toward this structural organization. Control of universities still tended to be independent, although university leadership was increasingly appointed by the state.[44]		Although the structural model provided by the University of Paris, where student members are controlled by faculty "masters," provided a standard for universities, the application of this model took at least three different forms. There were universities that had a system of faculties whose teaching addressed a very specific curriculum; this model tended to train specialists. There was a collegiate or tutorial model based on the system at University of Oxford where teaching and organization was decentralized and knowledge was more of a generalist nature. There were also universities that combined these models, using the collegiate model but having a centralized organization.[45]		Early Modern universities initially continued the curriculum and research of the Middle Ages: natural philosophy, logic, medicine, theology, mathematics, astronomy (and astrology), law, grammar and rhetoric. Aristotle was prevalent throughout the curriculum, while medicine also depended on Galen and Arabic scholarship. The importance of humanism for changing this state-of-affairs cannot be underestimated.[46] Once humanist professors joined the university faculty, they began to transform the study of grammar and rhetoric through the studia humanitatis. Humanist professors focused on the ability of students to write and speak with distinction, to translate and interpret classical texts, and to live honorable lives.[47] Other scholars within the university were affected by the humanist approaches to learning and their linguistic expertise in relation to ancient texts, as well as the ideology that advocated the ultimate importance of those texts.[48] Professors of medicine such as Niccolò Leoniceno, Thomas Linacre and William Cop were often trained in and taught from a humanist perspective as well as translated important ancient medical texts. The critical mindset imparted by humanism was imperative for changes in universities and scholarship. For instance, Andreas Vesalius was educated in a humanist fashion before producing a translation of Galen, whose ideas he verified through his own dissections. In law, Andreas Alciatus infused the Corpus Juris with a humanist perspective, while Jacques Cujas humanist writings were paramount to his reputation as a jurist. Philipp Melanchthon cited the works of Erasmus as a highly influential guide for connecting theology back to original texts, which was important for the reform at Protestant universities.[49] Galileo Galilei, who taught at the Universities of Pisa and Padua, and Martin Luther, who taught at the University of Wittenberg (as did Melanchthon), also had humanist training. The task of the humanists was to slowly permeate the university; to increase the humanist presence in professorships and chairs, syllabi and textbooks so that published works would demonstrate the humanistic ideal of science and scholarship.[50]		Although the initial focus of the humanist scholars in the university was the discovery, exposition and insertion of ancient texts and languages into the university, and the ideas of those texts into society generally, their influence was ultimately quite progressive. The emergence of classical texts brought new ideas and led to a more creative university climate (as the notable list of scholars above attests to). A focus on knowledge coming from self, from the human, has a direct implication for new forms of scholarship and instruction, and was the foundation for what is commonly known as the humanities. This disposition toward knowledge manifested in not simply the translation and propagation of ancient texts, but also their adaptation and expansion. For instance, Vesalius was imperative for advocating the use of Galen, but he also invigorated this text with experimentation, disagreements and further research.[51] The propagation of these texts, especially within the universities, was greatly aided by the emergence of the printing press and the beginning of the use of the vernacular, which allowed for the printing of relatively large texts at reasonable prices.[52]		Examining the influence of humanism on scholars in medicine, mathematics, astronomy and physics may suggest that humanism and universities were a strong impetus for the scientific revolution. Although the connection between humanism and the scientific discovery may very well have begun within the confines of the university, the connection has been commonly perceived as having been severed by the changing nature of science during the scientific revolution. Historians such as Richard S. Westfall have argued that the overt traditionalism of universities inhibited attempts to re-conceptualize nature and knowledge and caused an indelible tension between universities and scientists.[53] This resistance to changes in science may have been a significant factor in driving many scientists away from the university and toward private benefactors, usually in princely courts, and associations with newly forming scientific societies.[54]		Other historians find incongruity in the proposition that the very place where the vast number of the scholars that influenced the scientific revolution received their education should also be the place that inhibits their research and the advancement of science. In fact, more than 80% of the European scientists between 1450–1650 included in the Dictionary of Scientific Biography were university trained, of which approximately 45% held university posts.[55] It was the case that the academic foundations remaining from the Middle Ages were stable, and they did provide for an environment that fostered considerable growth and development. There was considerable reluctance on the part of universities to relinquish the symmetry and comprehensiveness provided by the Aristotelian system, which was effective as a coherent system for understanding and interpreting the world. However, university professors still utilized some autonomy, at least in the sciences, to choose epistemological foundations and methods. For instance, Melanchthon and his disciples at University of Wittenberg were instrumental for integrating Copernican mathematical constructs into astronomical debate and instruction.[56] Another example was the short-lived but fairly rapid adoption of Cartesian epistemology and methodology in European universities, and the debates surrounding that adoption, which led to more mechanistic approaches to scientific problems as well as demonstrated an openness to change. There are many examples which belie the commonly perceived intransigence of universities.[57] Although universities may have been slow to accept new sciences and methodologies as they emerged, when they did accept new ideas it helped to convey legitimacy and respectability, and supported the scientific changes through providing a stable environment for instruction and material resources.[58]		Regardless of the way the tension between universities, individual scientists, and the scientific revolution itself is perceived, there was a discernible impact on the way that university education was constructed. Aristotelian epistemology provided a coherent framework not simply for knowledge and knowledge construction, but also for the training of scholars within the higher education setting. The creation of new scientific constructs during the scientific revolution, and the epistemological challenges that were inherent within this creation, initiated the idea of both the autonomy of science and the hierarchy of the disciplines. Instead of entering higher education to become a "general scholar" immersed in becoming proficient in the entire curriculum, there emerged a type of scholar that put science first and viewed it as a vocation in itself. The divergence between those focused on science and those still entrenched in the idea of a general scholar exacerbated the epistemological tensions that were already beginning to emerge.[59]		The epistemological tensions between scientists and universities were also heightened by the economic realities of research during this time, as individual scientists, associations and universities were vying for limited resources. There was also competition from the formation of new colleges funded by private benefactors and designed to provide free education to the public, or established by local governments to provide a knowledge hungry populace with an alternative to traditional universities.[60] Even when universities supported new scientific endeavors, and the university provided foundational training and authority for the research and conclusions, they could not compete with the resources available through private benefactors.[61]		By the end of the early modern period, the structure and orientation of higher education had changed in ways that are eminently recognizable for the modern context. Aristotle was no longer a force providing the epistemological and methodological focus for universities and a more mechanistic orientation was emerging. The hierarchical place of theological knowledge had for the most part been displaced and the humanities had become a fixture, and a new openness was beginning to take hold in the construction and dissemination of knowledge that were to become imperative for the formation of the modern state.		By the 18th century, universities published their own research journals and by the 19th century, the German and the French university models had arisen. The German, or Humboldtian model, was conceived by Wilhelm von Humboldt and based on Friedrich Schleiermacher's liberal ideas pertaining to the importance of freedom, seminars, and laboratories in universities.[citation needed] The French university model involved strict discipline and control over every aspect of the university.		Until the 19th century, religion played a significant role in university curriculum; however, the role of religion in research universities decreased in the 19th century, and by the end of the 19th century, the German university model had spread around the world. Universities concentrated on science in the 19th and 20th centuries and became increasingly accessible to the masses. In the United States, the Johns Hopkins University was the first to adopt the (German) research university model; this pioneered the adoption by most other American universities. In Britain, the move from Industrial Revolution to modernity saw the arrival of new civic universities with an emphasis on science and engineering, a movement initiated in 1960 by Sir Keith Murray (chairman of the University Grants Committee) and Sir Samuel Curran, with the formation of the University of Strathclyde.[64] The British also established universities worldwide, and higher education became available to the masses not only in Europe.		In 1963, the Robbins Report on universities in the United Kingdom concluded that such institutions should have four main "objectives essential to any properly balanced system: instruction in skills; the promotion of the general powers of the mind so as to produce not mere specialists but rather cultivated men and women; to maintain research in balance with teaching, since teaching should not be separated from the advancement of learning and the search for truth; and to transmit a common culture and common standards of citizenship."[65]		In the early 21st century, concerns were raised over the increasing managerialisation and standardisation of universities worldwide. Neo-liberal management models have in this sense been critiqued for creating "corporate universities (where) power is transferred from faculty to managers, economic justifications dominate, and the familiar 'bottom line' ecclipses pedagogical or intellectual concerns".[66] Academics' understanding of time, pedagogical pleasure, vocation, and collegiality have been cited as possible ways of alleviating such problems.[67]		A national university is generally a university created or run by a national state but at the same time represents a state autonomic institution which functions as a completely independent body inside of the same state. Some national universities are closely associated with national cultural, religious or political aspirations, for instance the National University of Ireland, which formed partly from the Catholic University of Ireland which was created almost immediately and specifically in answer to the non-denominational universities which had been set up in Ireland in 1850. In the years leading up to the Easter Rising, and in no small part a result of the Gaelic Romantic revivalists, the NUI collected a large amount of information on the Irish language and Irish culture.[citation needed] Reforms in Argentina were the result of the University Revolution of 1918 and its posterior reforms by incorporating values that sought for a more equal and laic[further explanation needed] higher education system.		Universities created by bilateral or multilateral treaties between states are intergovernmental. An example is the Academy of European Law, which offers training in European law to lawyers, judges, barristers, solicitors, in-house counsel and academics. EUCLID (Pôle Universitaire Euclide, Euclid University) is chartered as a university and umbrella organisation dedicated to sustainable development in signatory countries, and the United Nations University engages in efforts to resolve the pressing global problems that are of concern to the United Nations, its peoples and member states. The European University Institute, a post-graduate university specialised in the social sciences, is officially an intergovernmental organisation, set up by the member states of the European Union.		Although each institution is organized differently, nearly all universities have a board of trustees; a president, chancellor, or rector; at least one vice president, vice-chancellor, or vice-rector; and deans of various divisions. Universities are generally divided into a number of academic departments, schools or faculties. Public university systems are ruled over by government-run higher education boards. They review financial requests and budget proposals and then allocate funds for each university in the system. They also approve new programs of instruction and cancel or make changes in existing programs. In addition, they plan for the further coordinated growth and development of the various institutions of higher education in the state or country. However, many public universities in the world have a considerable degree of financial, research and pedagogical autonomy. Private universities are privately funded and generally have broader independence from state policies. However, they may have less independence from business corporations depending on the source of their finances.		The funding and organization of universities varies widely between different countries around the world. In some countries universities are predominantly funded by the state, while in others funding may come from donors or from fees which students attending the university must pay. In some countries the vast majority of students attend university in their local town, while in other countries universities attract students from all over the world, and may provide university accommodation for their students.[68]		The definition of a university varies widely, even within some countries. Where there is clarification, it is usually set by a government agency. For example:		In Australia, the Tertiary Education Quality and Standards Agency (TEQSA) is Australia's independent national regulator of the higher education sector. Students rights within university are also protected by the Education Services for Overseas Students Act (ESOS).		In the United States there is no nationally standardized definition for the term university, although the term has traditionally been used to designate research institutions and was once reserved for doctorate-granting research institutions. Some states, such as Massachusetts, will only grant a school "university status" if it grants at least two doctoral degrees.[69]		In the United Kingdom, the Privy Council is responsible for approving the use of the word university in the name of an institution, under the terms of the Further and Higher Education Act 1992.[70]		In India, a new designation deemed universities has been created for institutions of higher education that are not universities, but work at a very high standard in a specific area of study ("An Institution of Higher Education, other than universities, working at a very high standard in specific area of study, can be declared by the Central Government on the advice of the UGC as an Institution 'Deemed-to-be-university'"). Institutions that are 'deemed-to-be-university' enjoy the academic status and the privileges of a university.[71] Through this provision many schools that are commercial in nature and have been established just to exploit the demand for higher education have sprung up.[72]		In Canada, college generally refers to a two-year, non-degree-granting institution, while university connotes a four-year, degree-granting institution. Universities may be sub-classified (as in the Macleans rankings) into large research universities with many PhD-granting programs and medical schools (for example, McGill University); "comprehensive" universities that have some PhDs but are not geared toward research (such as Waterloo); and smaller, primarily undergraduate universities (such as St. Francis Xavier).		Colloquially, the term university may be used to describe a phase in one's life: "When I was at university..." (in the United States and Ireland, college is often used instead: "When I was in college..."). In Australia, Canada, New Zealand, the United Kingdom, Nigeria, the Netherlands, Italy, Spain and the German-speaking countries, university is often contracted to uni. In Ghana, New Zealand, Bangladesh and in South Africa it is sometimes called "varsity" (although this has become uncommon in New Zealand in recent years). "Varsity" was also common usage in the UK in the 19th century.[citation needed] "Varsity" is still in common usage in Scotland.		In many countries, students are required to pay tuition fees. Many students look to get 'student grants' to cover the cost of university. In 2012, the average outstanding student loan balance per borrower in the United States was US$23,300.[73] In many U.S. states, costs are anticipated to rise for students as a result of decreased state funding given to public universities.[74]		There are several major exceptions on tuition fees. In many European countries, it is possible to study without tuition fees. Public universities in Nordic countries were entirely without tuition fees until around 2005. Denmark, Sweden and Finland then moved to put in place tuition fees for foreign students. Citizens of EU and EEA member states and citizens from Switzerland remain exempted from tuition fees, and the amounts of public grants granted to promising foreign students were increased to offset some of the impact.[75]		Thus the university, as a form of social organization, was peculiar to medieval Europe. Later, it was exported to all parts of the world, including the Muslim East; and it has remained with us down to the present day. But back in the middle ages, outside of Europe, there was nothing anything quite like it anywhere.		
The term Goliardia refers to a traditional university student association in Italy. Around 80 associations of this type exist in all parts of the country (2002). These associations have a structure similar to the medieval knights’ orders. However in these Academic Orders the sword was represented by the art of dialectic. The Goliardi (the members of Goliardia) refer their origin to Peter Abelard (Latin: Petrus Abaelardus or Abailard) a medieval French scholastic philosopher, theologian and preeminent logician.		Goliardic associations can be compared to the American fraternities and sororities.				
Students for a Democratic Society (SDS) was a student activist movement in the United States that was one of the main representations of the New Left. The organization developed and expanded rapidly in the mid-1960s before dissolving at its last convention in 1969.		SDS has been an important influence on student organizing in the decades since its collapse.[citation needed] Participatory democracy, direct action, radicalism, student power, shoestring budgets, and its organizational structure are all present in varying degrees in current American student activist groups. Though various organizations have been formed in subsequent years as proposed national networks for left-wing student organizing, none has approached the scale of SDS, and most have lasted a few years at best.		A new incarnation of SDS was founded in 2006.						SDS developed from the Student League for Industrial Democracy (SLID), the youth branch of a socialist educational organization known as the League for Industrial Democracy (LID). LID descended from the Intercollegiate Socialist Society, started in 1905. Early in 1960, the SLID changed its name into SDS at the behest of its then acting Director, Aryeh Neier.[1] The phrase “industrial democracy” sounded too narrow and too labor oriented, making it more difficult to recruit students. Moreover, because the LID's leadership did not correspond to the expectations and the mood on the campuses, the SLID felt the need to dissociate itself from its parent organization. SDS held its first meeting in 1960 on the University of Michigan campus at Ann Arbor, Michigan, where Alan Haber was elected president. Its political manifesto, known as the Port Huron Statement, was adopted at the organization's first convention in 1962,[2] based on an earlier draft by staff member Tom Hayden.		The Port Huron Statement criticized the political system of the United States for failing to achieve international peace and critiqued Cold War foreign policy, the threat of nuclear war, and the arms race. In domestic matters, it criticized racial discrimination, economic inequality, big businesses, trade unions and political parties. In addition to its critique and analysis of the American system, the manifesto also suggested a series of reforms: it proclaimed a need to reshape into two genuine political parties to attain greater democracy, for stronger power for individuals through citizen's lobbies, for more substantial involvement by workers in business management, and for an enlarged public sector with increased government welfare, including a "program against poverty." The manifesto provided ideas of what and how to work for and to improve, and also advocated nonviolent civil disobedience as the means by which student youth could bring forth a "participatory democracy." Kirkpatrick Sale described the manifesto as "nothing less than an ideology, however raw and imperfect and however much would have resisted this word."[3]		The manifesto also presented SDS's break with the left-wing policies of the postwar years. Firstly, it was written with the same overall vision all along the document and reflected their view that all problems in every area were linked to each other and their willingness not to lead single-issue struggles but a broad struggle on all fronts at the same time. Then, it expressed SDS's willingness to work with groups whatever may be their political inclination and announced their rejection of anti-communism, a definitely new radical view contrasting with much of the American Left which had developed a policy of anti-communism. Without being Marxist or pro-communism, they denounced anti-communism as being a social problem and an obstruction to democracy. They also criticized the United States for its exaggerated paranoia and exclusive condemnation of the Soviet Union, and blamed this for being the reason for failing to achieve disarmament and to assure peace.		The Port Huron Convention opened with a symbol of this break with the policy of the past years: the delegate of the Communist Progressive Youth Organizing Committee asked to attend the conference as an observer. The people from the Young People's Socialist League objected while most of the SDSers insisted on letting him sit. He eventually sat. Later in the meeting, Michael Harrington, an LID member, became agitated over the manifesto because he found the stand they took toward the Soviet Union and authoritarian regimes in general was insufficiently critical, and because, according to him, they deliberately wrote sections to pique the liberals. Surprisingly, Roger Hagan, a liberal, defended the SDS and its policy. After lively debates between the two, the draft finally remained more or less unchanged. Some two weeks later, a meeting between the LID and SDS was held where the LID expressed its discontent about the manifesto. As a result, Haber and Hayden, at this time respectively the National secretary and the new President of the organization, were summoned to a hearing on the 6 July 1962. There, Hayden clashed with Michael Harrington (as he later would with Irving Howe[4]) over the perceived potential for totalitarianism among other things. Harrington denounced the seating of the PYOC member, SDS’s tolerance for communism and their lack of clarity in their condemnation of communist totalitarianism and authoritarianism, and he reproached SDS for providing only a mild critique of the Soviet Union and for blaming the cold war mostly on the United States. Hayden then asked him to read the manifesto more carefully, especially the section on values. Hayden later wrote:		The tension between SDS and the LID was greatly increased when SDS called for a national demonstration to take place during the spring of 1965. The LID was very concerned about "Communist" participation but SDS refused to restrict who could attend and what signs they could use. The rift opened even further when, at the 1965 SDS National Convention, the clause excluding communists from membership was deleted from the SDS constitution. During the summer of 1965 delegates from SDS and the LID met in Chicago and New York. The League for Industrial Democracy, SDS's sponsoring organization, objected to the removal of the exclusion clause in the SDS constitution,[6] as SDS benefited from LID's non-profit status, which excluded political activity. By mutual agreement the relationship was severed October 4, 1965.[7]		In the academic year 1962–1963, the President was Tom Hayden, the Vice President was Paul Booth and the National Secretary was Jim Monsonis. There were nine chapters with, at most, about 1000 members. The national office (NO) in New York City consisted of a few desks, some broken chairs, a couple of file cabinets and a few typewriters. As a student group with a strong belief in decentralization and a distrust for most organizations, the SDS did not have a strong central bureaucracy. The three stalwarts at the office, Don McKelvey, Steve Max, and the National Secretary, Jim Monsonis, worked long hours for little pay to service the local chapters, and to help establish new ones. Even during the Cuban Missile Crisis in October, little could be accomplished. Most activity was oriented toward civil rights issues and the Student Nonviolent Coordinating Committee (SNCC) played a key role in inspiring SDS.		By the end of the academic year, there were over 200 delegates at the annual convention at Pine Hill, New York, from 32 different colleges and universities. It was then decided to give more power to the chapters, who would then send delegates to the National Council (NC), which would meet quarterly to handle the ongoing activities. Also, in the spirit of participatory democracy, a consensus was reached to elect new officers each year. Lee Webb of Boston University was chosen as National Secretary, and Todd Gitlin of Harvard University was made president. Some continuity was preserved by retaining Paul Booth as Vice President. The search began for something to challenge the idealistic, budding activists.		It was at this time that the Black Power Movement was first gaining some momentum (although Stokely Carmichael would make the movement more mainstream in 1966). The movement made it impolitic for white activists, such as those in SDS, to presume to lead protests for black civil rights. Instead, SDS would try to organize white unemployed youths through a newly established program they called the Economic Research and Action Project (ERAP). This "into the ghetto" move was a practical failure, but the fact that it existed at all drew many young idealists to SDS.		At the summer convention in 1964 there was a split between those who were campus-oriented, and the ERAP supporters. Most of the old guard were ERAP supporters, but the campus activists were growing. Paul Potter was elected president, and by the end of summer there were ten ERAP programs in place, with about 125 student volunteers. C. Clark Kissinger of Shimer College in Illinois was elected as National Secretary, and he put the NO on a much more business-like basis. He and his assistant, Helen Garvey, mailed out the literature list, the newsletters and the news of chapter's activities to a growing membership list. Kissinger also worked to smooth the relationship with the LID.		A small faction of SDS that was interested in change through conventional electoral politics established a program called the Political Education Project (PEP). Its Director was Jim Williams of the University of Louisville, and Steve Max served as its Associate Director. This was never very large, and it was opposed by the mainstream SDSers, who were mostly opposed to such traditional, old-fashioned activity, and were looking for something new that "worked". The landslide victory of Lyndon B. Johnson in the November presidential election played its part, as well, and PEP soon withered away. A Peace Research and Education Project (PREP) headed by Paul Booth, Swarthmore, met a similar fate. Meanwhile, the local chapters got into all sorts of projects, from University reform, community-university relations, and now, in a small way, the issue of the draft and Vietnam War. With the passage of the Civil Rights Act of 1964 and the Voting Rights Act of 1965, the SDS broke with the pro-labor ideas in the Port Huron Statement and decided that it was best to shift the focus of civil rights away from the southern states and more towards urban cities in the north.[8]		Then, on October 1, the University of California, Berkeley exploded into the dramatic and prolonged agony that was the free speech movement. Led by a charismatic Friends of SNCC student activist named Mario Savio, upwards of three thousand students surrounded a police car in which a student was being taken away, arrested for setting up an informational card table for the Congress of Racial Equality (CORE) in defiance of the University's ban on politics. The sit-down prevented the police car from moving for 32 hours. The demonstrations, meetings and strikes that resulted all but shut the university down. Hundreds of students were arrested.[9]		In February 1965, United States President Johnson dramatically escalated the war in Vietnam by bombing North Vietnam in Operation Flaming Dart and introducing ground troops directly involved in fighting the Viet Cong in the South. Campus chapters of SDS all over the country started to lead small, localized demonstrations against the war and the NO became the focal group that organized the march against the war in Washington on April 17. Endorsements came from nearly all of the other peace groups and leading personalities, there was significant increase in income and by the end of March there were 52 chapters. The media began to cover the organization and the New Left. However, the call for the march and the openness of the organization in allowing other groups, even communist front groups, or communists themselves, to join in caused great strains with the LID and some other old left organizations.		The first teach-in against the war was held in the University of Michigan. Soon hundreds more, all over the country, were held. The demonstration in Washington, D.C. attracted about 25,000 anti-war protesters and SDS became the leading student group against the war on most U.S. campuses.		Representing its move into the heartland, the 1965 summer convention was held at Kewadin, a small camp in Northern Michigan. Moreover, its National Office, which had been located in Manhattan, was moved to Chicago at about the same time. The rapid growth of the membership rate during the preceding year brought with it a new breed with a new style:		The convention elected an Akron, Ohio student, Carl Oglesby, President and Jeff Shero, from the increasingly influential University of Texas chapter in Austin, as Vice President—in preference to "old guard" candidates.[11] The convention voted to remove the anti-communist exclusion clauses from the SDS constitution,[12] failed to provide for any national program,[13] and increased the reliance on local initiatives at the chapters. As a result, the National Office's leadership fell into ineffectual chaos. The League for Industrial Democracy, SDS's nominal sponsoring organization, was disappointed with removal of the exclusion clause from the SDS constitution, as SDS was covered under LID's non-profit status which excluded political activity. By mutual agreement the relationship was severed October 4, 1965.[14]		On November 27, 1965 there was a major anti-war demonstration in Washington, D.C. at which Carl Oglesby, the new SDS president, made a very successful speech, addressed to the liberal crowd,[15] and in circuitous terms alleged that the United States government was imperialist in nature. The speech received a standing ovation, substantial press coverage, and resulted in greatly increased national prominence for SDS.[3][16]		There was also resistance to women's equal participation, which drove some women away from the group. According to Jacqui Ceballos, "Women at a 1965 SDS conference [were] put down with she just needs a good screw; the following year SDS women [were] pelted with tomatoes when they demand[ed] a plank on women's liberation." [17]		The unexpected influx of substantial numbers of new members and chapters combined with the ousting of the previous leadership, the "old guard", resulted in a crisis which dogged SDS until its final breakup; despite repeated attempts to do so, consensus was never reached on what form the organization should take or what role it should play. A final attempt by the old guard at a "rethinking conference" to establish a coherent new direction for the organization failed. The conference, held on the University of Illinois campus at Champaign-Urbana over Christmas vacation, 1965, was attended by about 360 people from 66 chapters, many of whom were new to SDS. Despite a great deal of discussion, no substantial decisions were made.[18][19]		Nationally, the SDS continued to use the draft as an important issue for students, and over the rest of the academic year began to attack university complicity in it, as the universities had begun to supply students' class rankings, used to determine who was to be drafted. The University of Chicago's administration building was taken over in a three-day sit-in in May. Rank protests and sit-ins spread to many other universities.		The summer convention of 1966 was moved even farther west, this time to Clear Lake, Iowa. The "prairie people" continued to increase their influence. Nick Egleson was chosen as President, and Carl Davidson was elected Vice President. Greg Calvert, recently a History Instructor at Iowa State University, was chosen as National Secretary. It was at this convention that members of Progressive Labor Party (PL) first participated. PL was a Maoist group that had turned to SDS as fertile ground for recruiting new members sympathetic to its long-term strategy of organizing the industrial working class.[20] SDSers of that time were nearly all anti-communist, but they also refused to be drawn into actions that smacked of red-baiting, which they viewed as mostly irrelevant and old hat. PL soon began to organize a Worker Student Alliance. By 1968 and 1969 they would profoundly affect SDS, particularly at national gatherings of the membership, forming a well-groomed, disciplined faction which followed the Progressive Labor Party line.[citation needed]		The 1966 convention also marked an even greater turn towards organization around campus issues by local chapters, with the NO cast in a strictly supporting role. Campus issues ranged from bad food, powerless student "governments," various in loco parentis manifestations, on-campus recruiting for the military and, again, ranking for the draft. Campuses around the country were in a state of unprecedented ferment and activism. Despite the absence of a politically effective campus SDS chapter, Berkeley again became a center of particularly dramatic radical upheaval over the university's repressive anti-free-speech actions, and an effective student strike with very wide support occurred. Even Harvard endured an upheaval engendered by a visit there of United States Secretary of Defense Robert McNamara.		At this time many in SDS turned to a more anarchist-influenced politics and organized activities aimed at the country’s burgeoning countercultural community. These efforts were especially successful at the large and active University of Texas chapter in Austin where The Rag, an underground newspaper founded by SDS leaders Thorne Dreyer and Carol Neiman was, according to historian Abe Peck, the first underground paper in the country to incorporate the “participatory democracy, community organizing and synthesis of politics and culture that the New Left of the midsixties was trying to develop.” [21] And SDS' now legendary “Gentle Thursday” events on the UT campus helped to galvanize the Austin cultural community and turn it into a potent political force.[22] Austin's Gentle Thursday inspired similar activities at a number of other universities including Penn State and Iowa State. Austin, also a center of civil-rights and anti-war activities, was in 1967 the scene of an SDS-generated free speech movement (the University Freedom Movement) that mobilized thousands of students in massive demonstrations and other activities.[23]		The Winter and Spring of 1967 saw an escalation of the militancy of the protests at many campuses. SDSers and self-styled radicals were even elected into the student government at a few places. Demonstrations against Dow Chemical Company and other campus recruiters were widespread, and ranking and the draft issues grew in scale. The Federal Bureau of Investigation (FBI) (mainly through its secret COINTELPRO) and other law enforcement agencies were often exposed as having spies and informers in the chapters. Harassment by the authorities was also on the rise. The National Office became distinctly more effective in this period, and the three officers actually visited most of the chapters. New Left Notes, as well, became a potent vehicle for promoting some coherence and solidarity among the chapters. The Anti-War movement began to take hold among university students.		The 1967 convention took an egalitarian turn by eliminating the Presidential and Vice-Presidential offices and replacing them with a National Secretary (20-year-old Mike Spiegel), an Education Secretary (Texan Bob Pardun of the Austin chapter), and an Inter‑organizational Secretary (former VP Carl Davidson). A clear direction for a national program was not set but they did manage to pass strong resolutions on the draft, resistance within the Army itself, and they made a call for immediate withdrawal from Vietnam. A women's liberation resolution on the issue of male chauvinism was passed by conference attendees, for the first time.		This resolution on women's liberation, drafted in the Women’s Liberation Workshop, had two goals. They were to “free women to participate in other meaningful activities” and to “relieve our brothers of the burden of male chauvinism.” For the first goal, they had three specific subgoals. The first was the creation of communal childcare centers, so mothers at home could have free time to pursue their interests. The second was the acknowledgment of the right of women to choose when to have children. They said that free distribution of birth control information and competent medical abortion should be provided for all women. The third called for the even distribution of household chores between all adult members, male and female. For the second goal, to rid SDS of male chauvinism, they had four specific subgoals. The first was that the male SDS members should first work on their personal chauvinism first, and try and remove that from their work and social relationships. The second is for women to participate in all levels of SDS work, “from licking stamps to assuming leadership positions.” The third is for leaders to be aware of the power they hold in creating the dynamic of the leader/subordinate relationship, and to be responsible for not abusing that power. The fourth mentions that all programs created by the SDS must include a section on women’s right. The New Left Notes reprinted the statement, however, it was accompanied by a caricature of a woman dressed in a baby-doll dress, holding a sign with the slogan “We want our rights and we want them now![24]		That fall saw a great escalation of the anti-war actions of the New Left. The school year started with a large demonstration against university complicity in the war in allowing Dow recruiters on campus at the University of Wisconsin in Madison on October 17. Peaceful at first, the demonstrations turned to a sit-in that was violently dispersed by the Madison police and riot squad, resulting in many injuries and arrests. A mass rally and a student strike then closed the university for several days. A coordinated series of demonstrations against the draft led by members of the Resistance, the War Resisters League, and SDS added fuel to the fire of resistance. After conventional civil rights tactics of peaceful pickets seemed to have failed, the Oakland, California Stop the Draft Week ended in mass hit and run skirmishes with the police. The huge (100,000 people) October 21 March on the Pentagon saw hundreds arrested and injured. Night-time raids on draft offices began to spread.		In the spring of 1968, National SDS activists led an effort on the campuses called "Ten Days of Resistance" and local chapters cooperated with the Student Mobilization Committee in rallies, marches, sit-ins and teach-ins, which culminated in a one-day strike on April 26. About a million students stayed away from classes that day, the largest student strike in the history of the United States. It was largely ignored by the New York City-based national media, which focused on the student shutdown of Columbia University in New York, led by an inter-racial alliance of Columbia SDS chapter activists and Student Afro Society activists. As a result of the mass media publicity given to Columbia SDS activists such as Columbia SDS chairperson Mark Rudd during the Columbia Student Revolt, the organization was put on the map politically and "SDS" became a household name in the United States for a few years. Membership in SDS chapters around the United States increased dramatically during the 1968-69 academic year.		Also in 1968, an SDS organizer at the University of Washington told a meeting about white college men working with poor white men, and "[h]e noted that sometimes after analyzing societal ills, the men shared leisure time by 'balling a chick together.' He pointed out that such activities did much to enhance the political consciousness of poor white youth. A woman in the audience asked, 'And what did it do for the consciousness of the chick?'" (Hole, Judith, and Ellen Levine, Rebirth of Feminism, 1971, pg. 120).[25] After the meeting, a handful of women formed Seattle's first women's liberation group.[25]		Led by the Worker-Student Alliance and rival Joe Hill caucuses, SDS in San Francisco played a major role in the Third World Student Strike at San Francisco State College. This strike, the longest student strike in U.S. history, led to the creation of Black and other ethnic studies programs on campuses across the country.[26]		SDS members from Austin, Texas participated in a mass demonstration in San Antonio, Texas in April 1969 at the "Kings River Parade". San Antonio SNCC members called the demonstration to protest the killing of Bobby Joe Phillips by San Antonio Police Officers.		In the summer of 1969, the ninth SDS national convention was held at the Chicago Coliseum with some 2000 people attending. Many factions of the movement were present, and set up their literature tables all around the edges of the cavernous hall. The Young Socialist Alliance, Wobblies, Spartacists, Marxists and Maoists of various sorts, all together with various law-enforcement spies and informers contributed to the air of impending expectations.		Each delegate was given the convention issue of the newspaper New Left Notes, which contained a manifesto, "You don't need a Weatherman to know which way the wind blows", a line taken from Bob Dylan's "Subterranean Homesick Blues". This manifesto had been first presented at the Spring, 1969, SDS National Council Meeting in Austin, Texas. The document had been written by an 11-member committee that included Mark Rudd, Bernardine Dohrn and John Jacobs, and represented the position of the Revolutionary Youth Movement (RYM) wing of SDS, most of which later turned into the Weather Underground Organization. It has been noted that the Weather Underground was an off-branch of SDS for a number of reasons. The New Left Notes issue was full of the language of the Old Left of the 1930s; and was thus impenetrable and irrelevant to the majority of SDSers.		Once it became clear that the Worker Student Alliance (WSA) faction was the largest contingent with a majority of the delegates, the convention quickly fell into disarray, as the RYM and allied groups moved to expel Progressive Labor (PL) members and the WSA faction of SDS. The Black Panther representatives attacked PL and at the same time proved itself inclined towards sexism by advocating "pussy power." The entire convention fell into something approaching chaos, or worse, farce.[27]		The RYM and the National Office faction, led by Bernardine Dohrn, led a breakaway meeting from which PL and WSA members were barred. This group then voted by about 500 to 100 to expel PL from SDS, and then walked out of the conference hall with that 500. By the next day, there were two SDS organizations, which RYM termed "SDS-RYM" and "SDS-WSA."		In the fall of 1969, many of the SDS-RYM chapters also split up or disintegrated. The Weatherman faction evolved into a small underground organization that first took to street confrontations and then to a bombing campaign. The Weathermen held one final national convention in Flint, Michigan, from December 27–31, 1969. It was at this convention, more popularly known as the "Flint War Council," that the decision was made to disband what remained of SDS-RYM.[28] SDS-RYM was fully defunct by 1970, while SDS-WSA continued its activity.		Also in 1969, the New Left was present at a Counter-Inaugural to Richard Nixon’s first inauguration, at which the antiwar leader Dave Dellinger, serving as master of ceremonies, incorrectly announced, “The women have asked all the men to leave the stage.”[29] After that, SDS activist Marilyn Salzman Webb attempted to speak about women's oppression, and SDS men heckled her, shouting, "Take her off the stage and fuck her!" and so forth until she was drowned out.[29][30][31][32] Later Webb received a threatening phone call which she thought was from Cathy Wilkerson, but that was not confirmed, and it may have been from a government agent.[31] In any case, the call contributed to driving apart outspoken feminists in the national SDS and people who put anti-racist and anti-war work before feminism and went toward the Weathermen.[31]		SDS-Worker-Student Alliance (SDS-WSA) continued to function nationwide, with a focus on (a) fighting racism; and (b) supporting workers' struggles and strikes, including the 1969 General Electric strike and 1970 Postal Workers' strikes. The WSA organized a support demonstration for the post office strikers, which greatly worried Richard Nixon's administration. This is the entry from H.R. Haldeman’s diary:		P in early, to EOB, to work on briefing books. Had to spend quite a little time on postal problem. The settlement didn’t work, because rank and file won’t go back, have rejected leaders, and now SDS types involved, at least in New York." - [33]		Now calling itself simply SDS, SDS-WSA continued to publish the newspaper New Left Notes. It held a convention in Boston in 1971, at which a striking General Motors worker was a featured speaker.		In 1972, SDS-WSA demonstrated at the Democratic National Convention in Miami against Democratic presidential candidate George McGovern's retreating from his original stronger campaign positions against the Vietnam War. Several hundred SDS members staged a sit-in at the Doral Hotel as McGovern and his staff met upstairs with protesting members of Grassroots McGovern Volunteers and sympathizers angry over the same issues.		In Newark, New Jersey, SDS-WSA demonstrated against Anthony Imperiale and his North Ward Citizens' Council which was opposing the construction of Kawaida Towers, a building complex sponsored by a community organization led by Black nationalist and poet Amiri Baraka (formerly Leroy Jones) (New York Times January 3, 1973, p. 84)		SDS joined with PLP and others to protest the writings of Arthur Jensen, William Shockley, and Richard Herrnstein, all of whom promoted the notion that there might be a genetic component to the observed below-average performance of black people on IQ tests. In October 1973, SDS-WSA, PLP, and others organized a convention at the Loeb Student Center of New York University dedicated to opposing academic racism. SDS circulated a petition entitled "A Resolution Against Racism" that was published in the New York Times on October 28, 1973 (p. 211). Out of this convention the Committee Against Racism (CAR) was formed to continue the fight against racism. CAR later changed its name to International Committee Against Racism (InCAR), when some chapters were formed in Canada.		In 1974, National SDS(-WSA) voted to dissolve as a separate organization and reform as chapters of InCAR. However, individual chapters of SDS continued to exist for some time. A chapter at Purdue University was active as late as 1976.		All references to contemporary activities of SDS in sources such as the New York Times after early 1970 are to SDS-WSA. For example, SDS confronted Indiana Senator Vance Hartke at an antiwar rally in New York City in 1971 (New York Times July 3, 1971, p. 3 and July 4, 1971, p. 3). SDS denounced liberal Democrats as having been the authors of the Vietnam War in the first place. SDS demonstrated against the Republican National Convention in Miami Beach, Florida in August 1972 (New York Times August 21, 1972, p. 20; August 22, 1972, pp. 1,36; August 23, 1972, pp. 1, 28).		Unlike SDS-RYM and the Weathermen, SDS-WSA strongly opposed bombing and terrorism. In 1971, SDS-WSA published a pamphlet titled Who Are The Bombers?.[34] It warned readers against police agents sent into the anti-Vietnam War movement to foment violence to justify police attacks. It also sharply criticized the Weathermen, which had begun its campaign of bombings.		On June 26, 1972, the US Supreme Court gave a unanimous opinion, in the case Healy v. James, stating that members of the SDS had been unconstitutionally deprived of their First Amendment right to freedom of assembly when a group was denied permission to form on the campus of Central Connecticut State College in New Britain, Connecticut.[35]		A few early SDS leaders went on to careers as Democratic Party politicians, including Tom Hayden, a former member of the legislature of the state of California and well known as the former husband of actress Jane Fonda, a prolific author, and a former candidate for offices such as Governor of California, Mayor of Los Angeles, and US Senator.		A new incarnation of SDS was founded on January 16, 2006, Martin Luther King Jr. Day, and by 2010 had grown to over 150 chapters around the United States.[36] It has held five national conventions to date, including the fifth in 2010 in Milwaukee, Wisconsin.[37]		In the 1971 film The Andromeda Strain, when Mrs. Jeremy Stone (Susan Brown) informs her husband (Arthur Hill) that unexpected visitors have arrived, he responds, "The SDS, no doubt" before learning that the visitors are Air Force personnel. In the 1995 film Forrest Gump, Jenny (Robin Wright) introduces her boyfriend to Forrest (Tom Hanks) as Wesley (Geoffrey Blake), the president of the Berkeley chapter of SDS.[38]		
This is a list of some of the current and former democratic schools around the world. This list also includes sub-branches of democratic schools such as Sudbury schools inspired by the Sudbury Valley School and certain anarchistic free schools that align with the broad principles of democratic education.						New South Wales:		Queensland:		Victoria:		Tasmania:		Western Australia		South Australia		Australian Capital Territory		Secondary Schools		• Rebeca Wild and the Pesta		A more complete list of schools on EUDEC France map		For a detailed list of free alternative schools in Germany see this list of adresses.		Former schools:		Currently Open		Former democratic schools now closed		Alabama		Alaska		Arizona		California		Colorado		Connecticut		Delaware		Florida		Georgia		Hawaii		Illinois		Indiana		Kentucky		Maine		Maryland		Massachusetts		Michigan		Minnesota		New Hampshire		Montana		New Jersey		New Mexico		New York		North Carolina		Ohio		Oregon		Pennsylvania		Puerto Rico		Rhode Island		Tennessee		Texas		Utah		Vermont		Virginia		Washington		West Virginia		Wisconsin		Leue, Mary M., ed. (2000). Challenging the Giant: The best of [SKOLE], the Journal of Alternative Education. 4. Down-to-Earth Books. ISBN 1-878115-13-8. OCLC 250561130. 		
Student riots, college riots, or campus riots are riots precipitated by students, generally from a college, university, or other school.						Often student riots are political in nature, such as those that were common in the US and Western Europe during the Vietnam War era, although student riots can occur as a result of peaceful demonstration oppressed by the authorities and after sporting events (hooliganism). In the US, the student riots of recent years have been linked to alcohol consumption.[1]		There were huge student riots in China during 1989, when the students started protesting against the injustice of their politicians. The Chinese government opposed the protests and brutally beat and murdered those who participated in them.		In a number of countries, such as Mexico, Chile, Iran, Venezuela and Bangladesh, students form an active political force, and student riots can occur in the context of wider political or social grievances.[2][3][4]		Student riots can accompany a general strike, a student strike, or wider national protests.		The following are some examples of famous student riots:				
Coordinates: 25°S 133°E﻿ / ﻿25°S 133°E﻿ / -25; 133		Australia (/əˈstreɪliə/ ( listen), /ɒ-/, /-ljə/),[11][12] officially the Commonwealth of Australia,[13] is a country comprising the mainland of the Australian continent, the island of Tasmania and numerous smaller islands. It is the largest country in Oceania and the world's sixth-largest country by total area. The neighbouring countries are Papua New Guinea, Indonesia and East Timor to the north; the Solomon Islands and Vanuatu to the north-east; and New Zealand to the south-east. Australia's capital is Canberra, and its largest urban area is Sydney.		For about 50,000 years[14] before the first British settlement in the late 18th century,[15][16] Australia was inhabited by indigenous Australians,[17] who spoke languages classifiable into roughly 250 groups.[18][19] After the European discovery of the continent by Dutch explorers in 1606, Australia's eastern half was claimed by Great Britain in 1770 and initially settled through penal transportation to the colony of New South Wales from 26 January 1788. The population grew steadily in subsequent decades, and by the 1850s most of the continent had been explored and an additional five self-governing crown colonies established. On 1 January 1901, the six colonies federated, forming the Commonwealth of Australia. Australia has since maintained a stable liberal democratic political system that functions as a federal parliamentary constitutional monarchy comprising six states and several territories.		Australia has the world's 13th-largest economy and ninth-highest per capita income (IMF).[20] With the second-highest human development index globally, the country ranks highly in quality of life, health, education, economic freedom, and civil liberties and political rights.[21] Australia is a member of the United Nations, G20, Commonwealth of Nations, ANZUS, Organisation for Economic Co-operation and Development (OECD), World Trade Organization, Asia-Pacific Economic Cooperation, and the Pacific Islands Forum. The population of 25 million[6] is highly urbanised and heavily concentrated on the eastern seaboard.[22] As of 2015, Australia had the 9th largest number of people born overseas, higher than Spain (10th) and Italy (11th).[23]						The name Australia (pronounced [əˈstɹæɪljə, -liə] in Australian English[24]) is derived from the Latin Terra Australis ("southern land"), a name used for a hypothetical continent in the Southern Hemisphere since ancient times.[25] When Europeans first began visiting and mapping Australia in the 17th century, the name Terra Australis was naturally applied to the new territories.[N 4]		Until the early 19th century, Australia was best known as "New Holland", a name first applied by the Dutch explorer Abel Tasman in 1644 (as Nieuw-Holland) and subsequently anglicised. Terra Australis still saw occasional usage, such as in scientific texts.[N 5] The name Australia was popularised by the explorer Matthew Flinders, who said it was "more agreeable to the ear, and an assimilation to the names of the other great portions of the earth".[31] The first time that Australia appears to have been officially used was in April 1817, in which Governor Lachlan Macquarie acknowledged the receipt of Flinders' charts of Australia from Lord Bathurst.[32] In December 1817, Macquarie recommended to the Colonial Office that it be formally adopted.[33] In 1824, the Admiralty agreed that the continent should be known officially by that name.[34] The first official published use of the new name came with the 1830 publication of "The Australia Directory" by the Hydrographic Office.[35]		Colloquial names for Australia include "Oz" and "the Land Down Under" (usually shortened to just "Down Under"). Other epithets include "the Great Southern Land", "the Lucky Country", "the Sunburnt Country", and "the Wide Brown Land". The latter two both derive from Dorothea Mackellar's 1908 poem "My Country".[36]		Human habitation of the Australian continent is estimated to have begun around 65,000 years ago,[37] possibly with the migration of people by land bridges and short sea-crossings from what is now Southeast Asia. These first inhabitants may have been ancestors of modern Indigenous Australians.[38] At the time of European settlement in the late 18th century, most Indigenous Australians were hunter-gatherers, with a complex oral culture and spiritual values based on reverence for the land and a belief in the Dreamtime. The Torres Strait Islanders, ethnically Melanesian, were originally horticulturists and hunter-gatherers.[39] The northern coasts and waters of Australia were visited sporadically by fishermen from Maritime Southeast Asia.[40]		The first recorded European sighting of the Australian mainland, and the first recorded European landfall on the Australian continent (in 1606), are attributed to the Dutch. The first ship and crew to chart the Australian coast and meet with Aboriginal people was the Duyfken captained by Dutch navigator, Willem Janszoon.[41] He sighted the coast of Cape York Peninsula in early 1606, and made landfall on 26 February at the Pennefather River near the modern town of Weipa on Cape York.[42] The Dutch charted the whole of the western and northern coastlines and named the island continent "New Holland" during the 17th century, but made no attempt at settlement.[42] William Dampier, an English explorer and privateer, landed on the north-west coast of New Holland in 1688 and again in 1699 on a return trip.[43] In 1770, James Cook sailed along and mapped the east coast, which he named New South Wales and claimed for Great Britain.[44]		With the loss of its American colonies in 1783, the British Government sent a fleet of ships, the "First Fleet", under the command of Captain Arthur Phillip, to establish a new penal colony in New South Wales. A camp was set up and the flag raised at Sydney Cove, Port Jackson, on 26 January 1788,[16] a date which became Australia's national day, Australia Day, although the British Crown Colony of New South Wales was not formally promulgated until 7 February 1788. The first settlement led to the foundation of Sydney, and the exploration and settlement of other regions.		A British settlement was established in Van Diemen's Land, now known as Tasmania, in 1803, and it became a separate colony in 1825.[45] The United Kingdom formally claimed the western part of Western Australia (the Swan River Colony) in 1828.[46] Separate colonies were carved from parts of New South Wales: South Australia in 1836, Victoria in 1851, and Queensland in 1859.[47] The Northern Territory was founded in 1911 when it was excised from South Australia.[48] South Australia was founded as a "free province"—it was never a penal colony.[49] Victoria and Western Australia were also founded "free", but later accepted transported convicts.[50][51] A campaign by the settlers of New South Wales led to the end of convict transportation to that colony; the last convict ship arrived in 1848.[52]		The indigenous population, estimated to have been between 750,000 and 1,000,000 in 1788,[53] declined for 150 years following settlement, mainly due to infectious disease.[54] Thousands more died as a result of frontier conflict with settlers.[55] A government policy of "assimilation" beginning with the Aboriginal Protection Act 1869 resulted in the removal of many Aboriginal children from their families and communities—often referred to as the Stolen Generations—a practice which may also have contributed to the decline in the indigenous population.[56] As a result of the 1967 referendum, the Federal government's power to enact special laws with respect to a particular race was extended to enable the making of laws with respect to Aborigines.[57] Traditional ownership of land ("native title") was not recognised in law until 1992, when the High Court of Australia held in Mabo v Queensland (No 2) that the legal doctrine that Australia had been terra nullius ("land belonging to no one") did not apply to Australia at the time of British settlement.[58]		A gold rush began in Australia in the early 1850s[59] and the Eureka Rebellion against mining licence fees in 1854 was an early expression of civil disobedience.[60] Between 1855 and 1890, the six colonies individually gained responsible government, managing most of their own affairs while remaining part of the British Empire.[61] The Colonial Office in London retained control of some matters, notably foreign affairs,[62] defence,[63] and international shipping.		On 1 January 1901, federation of the colonies was achieved after a decade of planning, consultation and voting.[64] This established the Commonwealth of Australia as a dominion of the British Empire.[65][66] The Federal Capital Territory (later renamed the Australian Capital Territory) was formed in 1911 as the location for the future federal capital of Canberra. Melbourne was the temporary seat of government from 1901 to 1927 while Canberra was being constructed.[67] The Northern Territory was transferred from the control of the South Australian government to the federal parliament in 1911.[68] In 1914, Australia joined Britain in fighting World War I, with support from both the outgoing Commonwealth Liberal Party and the incoming Australian Labor Party.[69][70] Australians took part in many of the major battles fought on the Western Front.[71] Of about 416,000 who served, about 60,000 were killed and another 152,000 were wounded.[72] Many Australians regard the defeat of the Australian and New Zealand Army Corps (ANZACs) at Gallipoli as the birth of the nation—its first major military action.[73][74] The Kokoda Track campaign is regarded by many as an analogous nation-defining event during World War II.[75]		Britain's Statute of Westminster 1931 formally ended most of the constitutional links between Australia and the UK. Australia adopted it in 1942,[76] but it was backdated to 1939 to confirm the validity of legislation passed by the Australian Parliament during World War II.[77][78] The shock of the United Kingdom's defeat in Asia in 1942 and the threat of Japanese invasion caused Australia to turn to the United States as a new ally and protector.[79] Since 1951, Australia has been a formal military ally of the US, under the ANZUS treaty.[80] After World War II Australia encouraged immigration from mainland Europe. Since the 1970s and following the abolition of the White Australia policy, immigration from Asia and elsewhere was also promoted.[81] As a result, Australia's demography, culture, and self-image were transformed.[82] The final constitutional ties between Australia and the UK were severed with the passing of the Australia Act 1986, ending any British role in the government of the Australian States, and closing the option of judicial appeals to the Privy Council in London.[83] In a 1999 referendum, 55% of voters and a majority in every state rejected a proposal to become a republic with a president appointed by a two-thirds vote in both Houses of the Australian Parliament. Since the election of the Whitlam Government in 1972,[84] there has been an increasing focus in foreign policy on ties with other Pacific Rim nations, while maintaining close ties with Australia's traditional allies and trading partners.[85]		Australia's landmass of 7,617,930 square kilometres (2,941,300 sq mi)[86] is on the Indo-Australian Plate. Surrounded by the Indian and Pacific oceans,[N 6] it is separated from Asia by the Arafura and Timor seas, with the Coral Sea lying off the Queensland coast, and the Tasman Sea lying between Australia and New Zealand. The world's smallest continent[88] and sixth largest country by total area,[89] Australia—owing to its size and isolation—is often dubbed the "island continent",[90] and is sometimes considered the world's largest island.[91] Australia has 34,218 kilometres (21,262 mi) of coastline (excluding all offshore islands),[92] and claims an extensive Exclusive Economic Zone of 8,148,250 square kilometres (3,146,060 sq mi). This exclusive economic zone does not include the Australian Antarctic Territory.[93] Apart from Macquarie Island, Australia lies between latitudes 9° and 44°S, and longitudes 112° and 154°E.		The Great Barrier Reef, the world's largest coral reef,[94] lies a short distance off the north-east coast and extends for over 2,000 kilometres (1,240 mi). Mount Augustus, claimed to be the world's largest monolith,[95] is located in Western Australia. At 2,228 metres (7,310 ft), Mount Kosciuszko on the Great Dividing Range is the highest mountain on the Australian mainland. Even taller are Mawson Peak (at 2,745 metres or 9,006 feet), on the remote Australian territory of Heard Island, and, in the Australian Antarctic Territory, Mount McClintock and Mount Menzies, at 3,492 metres (11,457 ft) and 3,355 metres (11,007 ft) respectively.[96]		Australia's size gives it a wide variety of landscapes, with tropical rainforests in the north-east, mountain ranges in the south-east, south-west and east, and dry desert in the centre.[97] It is the flattest continent,[98] with the oldest and least fertile soils;[99][100] desert or semi-arid land commonly known as the outback makes up by far the largest portion of land.[101] The driest inhabited continent, its annual rainfall averaged over continental area is less than 500 mm.[102] The population density, 2.8 inhabitants per square kilometre, is among the lowest in the world,[103] although a large proportion of the population lives along the temperate south-eastern coastline.[104]		Eastern Australia is marked by the Great Dividing Range, which runs parallel to the coast of Queensland, New South Wales and much of Victoria. The name is not strictly accurate, because parts of the range consist of low hills, and the highlands are typically no more than 1,600 metres (5,249 ft) in height.[105] The coastal uplands and a belt of Brigalow grasslands lie between the coast and the mountains, while inland of the dividing range are large areas of grassland.[105][106] These include the western plains of New South Wales, and the Einasleigh Uplands, Barkly Tableland, and Mulga Lands of inland Queensland. The northernmost point of the east coast is the tropical-rainforested Cape York Peninsula.[107][108][109][110]		The landscapes of the Top End and the Gulf Country—with their tropical climate—include forest, woodland, wetland, grassland, rainforest and desert.[111][112][113] At the north-west corner of the continent are the sandstone cliffs and gorges of The Kimberley, and below that the Pilbara. To the south of these and inland, lie more areas of grassland: the Ord Victoria Plain and the Western Australian Mulga shrublands.[114][115][116] At the heart of the country are the uplands of central Australia. Prominent features of the centre and south include Uluru (also known as Ayers Rock), the famous sandstone monolith, and the inland Simpson, Tirari and Sturt Stony, Gibson, Great Sandy, Tanami, and Great Victoria deserts, with the famous Nullarbor Plain on the southern coast.[117][118][119][120]		The climate of Australia is significantly influenced by ocean currents, including the Indian Ocean Dipole and the El Niño–Southern Oscillation, which is correlated with periodic drought, and the seasonal tropical low-pressure system that produces cyclones in northern Australia.[121][122] These factors cause rainfall to vary markedly from year to year. Much of the northern part of the country has a tropical, predominantly summer-rainfall (monsoon)[102] The south-west corner of the country has a Mediterranean climate.[123] The south-east ranges from oceanic (Tasmania and coastal Victoria) to humid subtropical (upper half of New South Wales). The interior is arid to semi-arid.[102]		According to the Bureau of Meteorology's 2011 Australian Climate Statement, Australia had lower than average temperatures in 2011 as a consequence of a La Niña weather pattern; however, "the country's 10-year average continues to demonstrate the rising trend in temperatures, with 2002–2011 likely to rank in the top two warmest 10-year periods on record for Australia, at 0.52 °C (0.94 °F) above the long-term average".[124] Furthermore, 2014 was Australia's third warmest year since national temperature observations commenced in 1910.[125][126] Water restrictions are frequently in place in many regions and cities of Australia in response to chronic shortages due to urban population increases and localised drought.[127][128] Throughout much of the continent, major flooding regularly follows extended periods of drought, flushing out inland river systems, overflowing dams and inundating large inland flood plains, as occurred throughout Eastern Australia in 2010, 2011 and 2012 after the 2000s Australian drought.		Australia's carbon dioxide emissions per capita are among the highest in the world, lower than those of only a few other industrialised nations.[129] A carbon tax was introduced in 2012 and helped to reduce Australia's emissions but was scrapped in 2014 under the Liberal Government.[130] Since the carbon tax was repealed, emissions have again continued to rise.[131]		Although most of Australia is semi-arid or desert, it includes a diverse range of habitats from alpine heaths to tropical rainforests, and is recognised as a megadiverse country. Fungi typify that diversity; an estimated 250,000 species—of which only 5% have been described—occur in Australia.[132] Because of the continent's great age, extremely variable weather patterns, and long-term geographic isolation, much of Australia's biota is unique. About 85% of flowering plants, 84% of mammals, more than 45% of birds, and 89% of in-shore, temperate-zone fish are endemic.[133] Australia has the greatest number of reptiles of any country, with 755 species.[134] Besides Antarctica, Australia is the only continent that developed without feline species. Feral cats may have been introduced in the 17th century by Dutch shipwrecks, and later in the 18th century by European settlers. They are now considered a major factor in the decline and extinction of many vulnerable and endangered native species.[135]		Australian forests are mostly made up of evergreen species, particularly eucalyptus trees in the less arid regions; wattles replace them as the dominant species in drier regions and deserts.[136] Among well-known Australian animals are the monotremes (the platypus and echidna); a host of marsupials, including the kangaroo, koala, and wombat, and birds such as the emu and the kookaburra.[136] Australia is home to many dangerous animals including some of the most venomous snakes in the world.[137] The dingo was introduced by Austronesian people who traded with Indigenous Australians around 3000 BCE.[138] Many animal and plant species became extinct soon after first human settlement,[139] including the Australian megafauna; others have disappeared since European settlement, among them the thylacine.[140][141]		Many of Australia's ecoregions, and the species within those regions, are threatened by human activities and introduced animal, chromistan, fungal and plant species.[142] All these factors have led to Australia having the highest mammal extinction rate of any country in the world.[143] The federal Environment Protection and Biodiversity Conservation Act 1999 is the legal framework for the protection of threatened species.[144] Numerous protected areas have been created under the National Strategy for the Conservation of Australia's Biological Diversity to protect and preserve unique ecosystems;[145][146] 65 wetlands are listed under the Ramsar Convention,[147] and 16 natural World Heritage Sites have been established.[148] Australia was ranked 3rd out of 178 countries in the world on the 2014 Environmental Performance Index.[149]		Australia is a federal parliamentary constitutional monarchy[150] with Elizabeth II at its apex as the Queen of Australia, a role that is distinct from her position as monarch of the other Commonwealth realms. The Queen is represented in Australia by the Governor-General at the federal level and by the Governors at the state level, who by convention act on the advice of her ministers.[151][152] Thus, in practice the Governor-General has no actual decision-making or de facto governmental role, and merely acts as a legal figurehead for the actions of the Prime Minister and the Federal Executive Council. The Governor-General does have extraordinary reserve powers which may be exercised outside the Prime Minister's request in rare and limited circumstances, the most notable exercise of which was the dismissal of the Whitlam Government in the constitutional crisis of 1975.[153]		The federal government is separated into three branches:		In the Senate (the upper house), there are 76 senators: twelve each from the states and two each from the mainland territories (the Australian Capital Territory and the Northern Territory).[155] The House of Representatives (the lower house) has 150 members elected from single-member electoral divisions, commonly known as "electorates" or "seats", allocated to states on the basis of population,[156] with each original state guaranteed a minimum of five seats.[157] Elections for both chambers are normally held every three years simultaneously; senators have overlapping six-year terms except for those from the territories, whose terms are not fixed but are tied to the electoral cycle for the lower house; thus only 40 of the 76 places in the Senate are put to each election unless the cycle is interrupted by a double dissolution.[155]		Australia's electoral system uses preferential voting for all lower house elections with the exception of Tasmania and the ACT which, along with the Senate and most state upper houses, combine it with proportional representation in a system known as the single transferable vote. Voting is compulsory for all enrolled citizens 18 years and over in every jurisdiction,[158] as is enrolment (with the exception of South Australia).[159] The party with majority support in the House of Representatives forms the government and its leader becomes Prime Minister. In cases where no party has majority support, the Governor-General has the constitutional power to appoint the Prime Minister and, if necessary, dismiss one that has lost the confidence of Parliament.[160]		There are two major political groups that usually form government, federally and in the states: the Australian Labor Party and the Coalition which is a formal grouping of the Liberal Party and its minor partner, the National Party.[161][162] Within Australian political culture, the Coalition is considered centre-right and the Labor Party is considered centre-left.[163] Independent members and several minor parties have achieved representation in Australian parliaments, mostly in upper houses.		In September 2015, Malcolm Turnbull successfully challenged Abbott for leadership of the Coalition, and was sworn in as the 29th Prime Minister of Australia.[164] The most recent federal election was held on 2 July 2016 and resulted in the Coalition forming a majority government.[165]		Australia has six states—New South Wales (NSW), Queensland (QLD), South Australia (SA), Tasmania (TAS), Victoria (VIC) and Western Australia (WA)—and two major mainland territories—the Australian Capital Territory (ACT) and the Northern Territory (NT). In most respects these two territories function as states, except that the Commonwealth Parliament has the power to modify or repeal any legislation passed by the territory parliaments.[166]		Under the constitution, the States essentially have plenary legislative power to legislate on any subject, whereas the Commonwealth (federal) Parliament may only legislate within the subject areas enumerated under section 51. For example, State parliaments have the power to legislate with respect to education, criminal law and state police, health, transport, and local government, but the Commonwealth Parliament does not have any specific power to legislate in these areas.[167] However, Commonwealth laws prevail over State laws to the extent of the inconsistency.[168] In addition, the Commonwealth has the power to levy income tax which, coupled with the power to make grants to States, has given it the financial means to incentivize States to pursue specific legislative agendas within areas over which the Commonwealth does not have legislative power.		Each state and major mainland territory has its own parliament—unicameral in the Northern Territory, the ACT and Queensland, and bicameral in the other states. The states are sovereign entities, although subject to certain powers of the Commonwealth as defined by the Constitution. The lower houses are known as the Legislative Assembly (the House of Assembly in South Australia and Tasmania); the upper houses are known as the Legislative Council. The head of the government in each state is the Premier and in each territory the Chief Minister. The Queen is represented in each state by a Governor; and in the Northern Territory, the Administrator.[169] In the Commonwealth, the Queen's representative is the Governor-General.[170]		The Commonwealth Parliament also directly administers the following external territories: Ashmore and Cartier Islands; Australian Antarctic Territory; Christmas Island; Cocos (Keeling) Islands; Coral Sea Islands; Heard Island and McDonald Islands; and Jervis Bay Territory, a naval base and sea port for the national capital in land that was formerly part of New South Wales.[154] The external territory of Norfolk Island previously exercised considerable autonomy under the Norfolk Island Act 1979 through its own legislative assembly and an Administrator to represent the Queen.[171] In 2015, the Commonwealth Parliament abolished self-government, integrating Norfolk Island into the Australian tax and welfare systems and replacing its legislative assembly with a council.[172] Macquarie Island is administered by Tasmania, and Lord Howe Island by New South Wales.		Over recent decades, Australia's foreign relations have been driven by a close association with the United States through the ANZUS pact, and by a desire to develop relationships with Asia and the Pacific, particularly through ASEAN and the Pacific Islands Forum. In 2005 Australia secured an inaugural seat at the East Asia Summit following its accession to the Treaty of Amity and Cooperation in Southeast Asia, and in 2011 attended the Sixth East Asia Summit in Indonesia. Australia is a member of the Commonwealth of Nations, in which the Commonwealth Heads of Government meetings provide the main forum for co-operation.[173]		Australia has pursued the cause of international trade liberalisation.[174] It led the formation of the Cairns Group and Asia-Pacific Economic Cooperation.[175][176] Australia is a member of the Organisation for Economic Co-operation and Development and the World Trade Organization,[177][178] and has pursued several major bilateral free trade agreements, most recently the Australia–United States Free Trade Agreement[179] and Closer Economic Relations with New Zealand,[180] with another free trade agreement being negotiated with China—the Australia–China Free Trade Agreement—and Japan,[181] South Korea in 2011,[182][183] Australia–Chile Free Trade Agreement, and as of November 2015 has put the Trans-Pacific Partnership before parliament for ratification.[184]		Along with New Zealand, the United Kingdom, Malaysia and Singapore, Australia is party to the Five Power Defence Arrangements, a regional defence agreement. A founding member country of the United Nations, Australia is strongly committed to multilateralism[185] and maintains an international aid program under which some 60 countries receive assistance. The 2005–06 budget provides A$2.5 billion for development assistance.[186] Australia ranks fifteenth overall in the Center for Global Development's 2012 Commitment to Development Index.[187]		Australia's armed forces—the Australian Defence Force (ADF)—comprise the Royal Australian Navy (RAN), the Australian Army and the Royal Australian Air Force (RAAF), in total numbering 81,214 personnel (including 57,982 regulars and 23,232 reservists) as of November 2015. The titular role of Commander-in-Chief is vested in the Governor-General, who appoints a Chief of the Defence Force from one of the armed services on the advice of the government.[188] Day-to-day force operations are under the command of the Chief, while broader administration and the formulation of defence policy is undertaken by the Minister and Department of Defence.		In the 2015–16 budget, defence spending was A$31.9 billion or 1.92% of GDP,[189] representing the 13th largest defence budget.[190] Australia has been involved in UN and regional peacekeeping, disaster relief and armed conflict, including the 2003 invasion of Iraq; it currently has deployed about 2,241 personnel in varying capacities to 12 international operations in areas including Iraq and Afghanistan.[191]		Australia is a wealthy country; it generates its income from various sources including mining-related exports, telecommunications, banking and manufacturing.[193][194][195] It has a market economy, a relatively high GDP per capita, and a relatively low rate of poverty. In terms of average wealth, Australia ranked second in the world after Switzerland in 2013, although the nation's poverty rate increased from 10.2% to 11.8%, from 2000/01 to 2013.[196][197] It was identified by the Credit Suisse Research Institute as the nation with the highest median wealth in the world and the second-highest average wealth per adult in 2013.[196]		The Australian dollar is the currency for the nation, including Christmas Island, Cocos (Keeling) Islands, and Norfolk Island, as well as the independent Pacific Island states of Kiribati, Nauru, and Tuvalu. With the 2006 merger of the Australian Stock Exchange and the Sydney Futures Exchange, the Australian Securities Exchange became the ninth largest in the world.[198]		Ranked fifth in the Index of Economic Freedom (2017),[199] Australia is the world's twelfth largest economy and has the sixth highest per capita GDP (nominal) at US$56,291.[200] The country was ranked second in the United Nations 2016 Human Development Index.[201] All of Australia's major cities fare well in global comparative livability surveys;[202] Melbourne reached top spot for the fourth year in a row on The Economist's 2014 list of the world's most liveable cities, followed by Adelaide, Sydney, and Perth in the fifth, seventh, and ninth places respectively.[203] Total government debt in Australia is about $190 billion[204] – 20% of GDP in 2010.[205] Australia has among the highest house prices and some of the highest household debt levels in the world.[206]		An emphasis on exporting commodities rather than manufactured goods has underpinned a significant increase in Australia's terms of trade since the start of the 21st century, due to rising commodity prices. Australia has a balance of payments that is more than 7% of GDP negative, and has had persistently large current account deficits for more than 50 years.[208] Australia has grown at an average annual rate of 3.6% for over 15 years, in comparison to the OECD annual average of 2.5%.[208]		Australia was the only advanced economy not to experience a recession due to the global financial downturn in 2008–2009.[209] However, the economies of six of Australia's major trading partners have been in recession, which in turn has affected Australia, significantly hampering its economic growth in recent years.[210][211] From 2012 to early 2013, Australia's national economy grew, but some non-mining states and Australia's non-mining economy experienced a recession.[212][213][214]		The Hawke Government floated the Australian dollar in 1983 and partially deregulated the financial system.[215] The Howard Government followed with a partial deregulation of the labour market and the further privatisation of state-owned businesses, most notably in the telecommunications industry.[216] The indirect tax system was substantially changed in July 2000 with the introduction of a 10% Goods and Services Tax (GST).[217] In Australia's tax system, personal and company income tax are the main sources of government revenue.[218]		In May 2012, there were 11,537,900 people employed (either full- or part-time), with an unemployment rate of 5.1%.[219] Youth unemployment (15–24) stood at 11.2%.[219] Data released in mid-November 2013 showed that the number of welfare recipients had grown by 55%. In 2007 228,621 Newstart unemployment allowance recipients were registered, a total that increased to 646,414 in March 2013.[220] According to the Graduate Careers Survey, full-time employment for newly qualified professionals from various occupations has declined since 2011 but it increases for graduates three years after graduation.[221][222]		Since 2008, inflation has typically been 2–3% and the base interest rate 5–6%. The service sector of the economy, including tourism, education, and financial services, accounts for about 70% of GDP.[223] Rich in natural resources, Australia is a major exporter of agricultural products, particularly wheat and wool, minerals such as iron-ore and gold, and energy in the forms of liquified natural gas and coal. Although agriculture and natural resources account for only 3% and 5% of GDP respectively, they contribute substantially to export performance. Australia's largest export markets are Japan, China, the US, South Korea, and New Zealand.[224] Australia is the world's fourth largest exporter of wine, and the wine industry contributes $5.5 billion per year to the nation's economy.[225]		Until the Second World War, the vast majority of settlers and immigrants came from the British Isles, and a majority of Australians have some British or Irish ancestry. These Australians form an ethnic group known as Anglo-Celtic Australians. In the 2016 Australian census, the most commonly nominated ancestries were English (36.1%), Australian (33.5%),[226] Irish (11.0%), Scottish (9.3%), Chinese (5.6%), Italian (4.6%), German (4.5%), Indian (2.8%), Greek (1.8%), and Dutch (1.6%).[227]		Australia's population has quadrupled since the end of World War I,[228] much of this increase from immigration. Following World War II and through to 2000, almost 5.9 million of the total population settled in the country as new immigrants.[229] Most immigrants are skilled,[230] but the immigration quota includes categories for family members and refugees.[230] By 2050, Australia's population is currently projected to reach around 42 million.[231] Nevertheless, its population density, 2.8 inhabitants per square kilometre, remains among the lowest in the world.[103]		In 2016, more than a quarter (26%) of Australia's population were born overseas; the five largest immigrant groups were those born in England (3.9%), New Zealand (2.2%), Mainland China (2.2%), India (1.9%), and the Philippines (1%).[232] Following the abolition of the White Australia policy in 1973, numerous government initiatives have been established to encourage and promote racial harmony based on a policy of multiculturalism.[233] In 2015–16, there were 189,770 permanent immigrants admitted to Australia, mainly from Asia.[234]		The Indigenous population—Aborigines and Torres Strait Islanders—was counted at 649,171 (2.8% of the total population) in 2016.[235] The increase is partly due to many people with Indigenous heritage previously having been overlooked by the census due to undercount and cases where their Indigenous status had not been recorded on the form. Indigenous Australians experience higher than average rates of imprisonment and unemployment, lower levels of education, and life expectancies for males and females that are, respectively, 11 and 17 years lower than those of non-indigenous Australians.[224][236][237] Some remote Indigenous communities have been described as having "failed state"-like conditions.[238]		In common with many other developed countries, Australia is experiencing a demographic shift towards an older population, with more retirees and fewer people of working age. In 2004, the average age of the civilian population was 38.8 years.[239] A large number of Australians (759,849 for the period 2002–03;[240] 1 million or 5% of the total population in 2005[241]) live outside their home country.		Although Australia has no official language, English has always been entrenched as the de facto national language.[2] Australian English is a major variety of the language with a distinctive accent and lexicon,[244] and differs slightly from other varieties of English in grammar and spelling.[245] General Australian serves as the standard dialect.		According to the 2016 census, English is the only language spoken in the home for close to 72.7% of the population. The next most common languages spoken at home are Mandarin (2.5%), Arabic (1.4%), Cantonese (1.2%), Vietnamese (1.2%) and Italian (1.2%).[246] A considerable proportion of first- and second-generation migrants are bilingual.		Over 250 Indigenous Australian languages are thought to have existed at the time of first European contact, of which less than 20 are still in daily use by all age groups.[247][248] About 110 others are spoken exclusively by older people.[248] At the time of the 2006 census, 52,000 Indigenous Australians, representing 12% of the Indigenous population, reported that they spoke an Indigenous language at home.[249] Australia has a sign language known as Auslan, which is the main language of about 5,500 deaf people.[250]		Australia has no state religion; Section 116 of the Australian Constitution prohibits the federal government from making any law to establish any religion, impose any religious observance, or prohibit the free exercise of any religion.[251] In the 2016 census, 52.1% of Australians were counted as Christian, including 22.6% as Roman Catholic and 13.3% as Anglican; 30.1% of the population reported having "no religion"; 7.3% identify with non-Christian religions, the largest of these being Islam (2.6%), followed by Buddhism (2.5%), Hinduism (1.9%) and Judaism (0.4%). The remaining 9.6% of the population did not provide an adequate answer. Those who reported having no religion increased conspicuously from 19% in 2006 to 30% in 2016. The largest change was between 2011 (22%) and 2016 (30.1%), when a further 2.2 million people reported having no religion.[252]		Before European settlement, the animist beliefs of Australia's indigenous people had been practised for many thousands of years. Mainland Aboriginal Australians' spirituality is known as the Dreamtime and it places a heavy emphasis on belonging to the land. The collection of stories that it contains shaped Aboriginal law and customs. Aboriginal art, story and dance continue to draw on these spiritual traditions. The spirituality and customs of Torres Strait Islanders, who inhabit the islands between Australia and New Guinea, reflected their Melanesian origins and dependence on the sea. The 1996 Australian census counted more than 7000 respondents as followers of a traditional Aboriginal religion.[253]		Since the arrival of the First Fleet of British ships in 1788, Christianity has grown to be the major religion practised in Australia. Christian churches have played an integral role in the development of education, health and welfare services in Australia. For much of Australian history the Church of England (now known as the Anglican Church of Australia) was the largest religious denomination. However, multicultural immigration has contributed to a decline in its relative position, and the Roman Catholic Church has benefitted from recent immigration to become the largest group. Similarly, Islam, Buddhism, Hinduism and Judaism have all grown in Australia over the past half-century.[254]		Australia has one of the lowest levels of religious adherence in the world.[255] In 2001, only 8.8% of Australians attended church on a weekly basis.[256]		Australia has the third and seventh highest life expectancy of males and females respectively in the world.[257] Life expectancy in Australia in 2010 was 79.5 years for males and 84.0 years for females.[258] Australia has the highest rates of skin cancer in the world,[259] while cigarette smoking is the largest preventable cause of death and disease, responsible for 7.8% of the total mortality and disease. Ranked second in preventable causes is hypertension at 7.6%, with obesity third at 7.5%.[260][261] Australia ranks 35th in the world[262] and near the top of developed nations for its proportion of obese adults [263] and nearly two thirds (63%) of its adult population is either overweight or obese.[264]		Total expenditure on health (including private sector spending) is around 9.8% of GDP.[265] Australia introduced universal health care in 1975.[266] Known as Medicare, it is now nominally funded by an income tax surcharge known as the Medicare levy, currently set at 1.5%.[267] The states manage hospitals and attached outpatient services, while the Commonwealth funds the Pharmaceutical Benefits Scheme (subsidising the costs of medicines) and general practice.[266]		School attendance, or registration for home schooling,[269] is compulsory throughout Australia. Education is the responsibility of the individual states and territories[270] so the rules vary between states, but in general children are required to attend school from the age of about 5 until about 16.[271][272] In some states (e.g., Western Australia,[273] the Northern Territory[274] and New South Wales[275][276]), children aged 16–17 are required to either attend school or participate in vocational training, such as an apprenticeship.		Australia has an adult literacy rate that was estimated to be 99% in 2003.[277] However, a 2011–12 report for the Australian Bureau of Statistics reported that Tasmania has a literacy and numeracy rate of only 50%.[278] In the Programme for International Student Assessment, Australia regularly scores among the top five of thirty major developed countries (member countries of the Organisation for Economic Co-operation and Development). Catholic education accounts for the largest non-government sector.		Australia has 37 government-funded universities and two private universities, as well as a number of other specialist institutions that provide approved courses at the higher education level.[279] The OECD places Australia among the most expensive nations to attend university.[280] There is a state-based system of vocational training, known as TAFE, and many trades conduct apprenticeships for training new tradespeople.[281] About 58% of Australians aged from 25 to 64 have vocational or tertiary qualifications,[224] and the tertiary graduation rate of 49% is the highest among OECD countries. The ratio of international to local students in tertiary education in Australia is the highest in the OECD countries.[282] In addition, 38 percent of Australia's population has a university or college degree, which is among the highest percentages in the world.[283][284]		Since 1788, the primary influence behind Australian culture has been Anglo-Celtic Western culture, with some Indigenous influences.[286][287] The divergence and evolution that has occurred in the ensuing centuries has resulted in a distinctive Australian culture.[288][289] Since the mid-20th century, American popular culture has strongly influenced Australia, particularly through television and cinema.[290] Other cultural influences come from neighbouring Asian countries, and through large-scale immigration from non-English-speaking nations.[290][291]		Indigenous Australian rock art is the oldest and richest in the world, dating as far back as 60,000 years and spread across hundreds of thousands of sites.[292] Traditional designs, patterns and stories infuse contemporary Indigenous Australian art, "the last great art movement of the 20th century";[293] its exponents include Emily Kame Kngwarreye.[294] Early colonial artists, trained in Europe, showed a fascination with the unfamiliar land.[295] The impressionistic works of Arthur Streeton, Tom Roberts and others associated with the 19th-century Heidelberg School—the first "distinctively Australian" movement in Western art—gave expression to a burgeoning Australian nationalism in the lead-up to Federation.[295] While the school remained influential into the new century, modernists such as Margaret Preston, and, later, Sidney Nolan and Arthur Boyd, explored new artistic trends.[295] The landscape remained a central subject matter for Fred Williams, Brett Whiteley and other post-World War II artists whose works, eclectic in style yet uniquely Australian, moved between the figurative and the abstract.[295][296] The national and state galleries maintain collections of local and international art.[297] Australia has one of the world's highest attendances of art galleries and museums per head of population.[298]		Australian literature grew slowly in the decades following European settlement though Indigenous oral traditions, many of which have since been recorded in writing, are much older.[300] 19th-century writers such as Henry Lawson and Banjo Paterson captured the experience of the bush using a distinctive Australian vocabulary. Their works are still popular; Paterson's bush poem "Waltzing Matilda" (1895) is regarded as Australia's unofficial national anthem.[301] Miles Franklin is the namesake of Australia's most prestigious literary prize, awarded annually to the best novel about Australian life.[302] Its first recipient, Patrick White, went on to win the Nobel Prize in Literature in 1973.[303] Australian winners of the Booker Prize include Peter Carey, Thomas Keneally and Richard Flanagan.[304] Author David Malouf, playwright David Williamson and poet Les Murray are also renowned literary figures.[305][306]		Many of Australia's performing arts companies receive funding through the federal government's Australia Council.[307] There is a symphony orchestra in each state,[308] and a national opera company, Opera Australia,[309] well known for its famous soprano Joan Sutherland.[310] At the beginning of the 20th century, Nellie Melba was one of the world's leading opera singers.[311] Ballet and dance are represented by The Australian Ballet and various state companies. Each state has a publicly funded theatre company.[312]		The Story of the Kelly Gang (1906), the world's first feature length film, spurred a boom in Australian cinema during the silent film era.[313] After World War I, Hollywood monopolised the industry,[314] and by the 1960s Australian film production had effectively ceased.[315] With the benefit of government support, the Australian New Wave of the 1970s brought provocative and successful films, many exploring themes of national identity, such as Wake in Fright and Gallipoli,[316] while "Crocodile" Dundee and the Ozploitation movement's Mad Max series became international blockbusters.[317] In a film market flooded with foreign content, Australian films delivered a 7.7% share of the local box office in 2015.[318] The AACTAs are Australia's premier film and television awards, and notable Academy Award winners from Australia include Geoffrey Rush, Nicole Kidman, Cate Blanchett and Heath Ledger.[319]		Australia has two public broadcasters (the Australian Broadcasting Corporation and the multicultural Special Broadcasting Service), three commercial television networks, several pay-TV services,[320] and numerous public, non-profit television and radio stations. Each major city has at least one daily newspaper,[320] and there are two national daily newspapers, The Australian and The Australian Financial Review.[320] In 2010, Reporters Without Borders placed Australia 18th on a list of 178 countries ranked by press freedom, behind New Zealand (8th) but ahead of the United Kingdom (19th) and United States (20th).[321] This relatively low ranking is primarily because of the limited diversity of commercial media ownership in Australia;[322] most print media are under the control of News Corporation and Fairfax Media.[323]		Most Indigenous Australian tribal groups subsisted on a simple hunter-gatherer diet of native fauna and flora, otherwise called bush tucker.[324][325] The first settlers introduced British food to the continent, much of which is now considered typical Australian food, such as the Sunday roast.[326][327] Multicultural immigration transformed Australian cuisine; post-World War II European migrants, particularly from the Mediterranean, helped to build a thriving Australian coffee culture, and the influence of Asian cultures has led to Australian variants of their staple foods, such as the Chinese-inspired dim sim and Chiko Roll.[328] Vegemite, pavlova, lamingtons and meat pies are regarded as iconic Australian foods.[329] Australian wine is produced mainly in the southern, cooler parts of the country.		Australia is also known for its cafe and coffee culture in urban centres, which has influenced coffee culture abroad, including New York City.[330] Australia and New Zealand were responsible for the flat white coffee.		About 24% of Australians over the age of 15 regularly participate in organised sporting activities.[224]		Australia is unique in that it has professional leagues for four football codes. Australian rules football, the world's oldest major football code and Australia's most popular sport in terms of revenue and spectatorship, originated in Melbourne in the late 1850s, and predominates in all states except New South Wales and Queensland, where rugby league holds sway, followed by rugby union. Soccer, while ranked fourth in popularity and resources, has the highest overall participation rates.[332]		Australia is a powerhouse in water-based sports, such as swimming and surfing.[333] The surf lifesaving movement originated in Australia, and the volunteer lifesaver is one of the country's icons.[334] Nationally, other popular sports include horse racing, basketball, and motor racing. The annual Melbourne Cup horse race and the Sydney to Hobart yacht race attract intense interest.[335] In 2016, the Australian Sports Commission revealed that swimming, cycling and soccer are the three most popular participation sports.[336][337]		Australia is one of five nations to have participated in every Summer Olympics of the modern era,[338] and has hosted the Games twice: 1956 in Melbourne and 2000 in Sydney.[339] Australia has also participated in every Commonwealth Games,[340] hosting the event in 1938, 1962, 1982, 2006 and will host the 2018 Commonwealth Games.[341] Australia made its inaugural appearance at the Pacific Games in 2015. As well as being a regular FIFA World Cup participant, Australia has won the OFC Nations Cup four times and the AFC Asian Cup once – the only country to have won championships in two different FIFA confederations.[342] The country regularly competes among the world elite basketball teams as it is among the global top three teams in terms of qualifications to the Basketball Tournament at the Summer Olympics. Other major international events held in Australia include the Australian Open tennis grand slam tournament, international cricket matches, and the Australian Formula One Grand Prix. The highest-rating television programs include sports telecasts such as the Summer Olympics, FIFA World Cup, The Ashes, Rugby League State of Origin, and the grand finals of the National Rugby League and Australian Football League.[343] Skiing in Australia began in the 1860s and snow sports take place in the Australian Alps and parts of Tasmania.		Click on a coloured area to see an article about English in that country or region		
Professional sports, as opposed to amateur sports, are sports in which athletes receive payment for their performance. Professional athleticism has come to the fore through a combination of developments. Mass media and increased leisure have brought larger audiences, so that sports organizations or teams can command large incomes.[1] As a result, more sportspeople can afford to make athleticism their primary career, devoting the training time necessary to increase skills, physical condition, and experience to modern levels of achievement.[1] This proficiency has also helped boost the popularity of sports.[1]		Most sports played professionally also have amateur players far outnumbering the professionals.						Baseball originated before the American Civil War (1861-1865). A humble game played on sandlots in particular, scoring and record-keeping gave baseball gravity. "Today," notes John Thorn in The Baseball Encyclopedia, "baseball without records is inconceivable."		In 1871 the first professional baseball league was created.[2] By the beginning of the 20th century, most large cities in the eastern United States had a professional baseball team. The teams were divided into two leagues, the National and American leagues. During the regular season, a team played only against other teams within its league. The most victorious team in each league was said to have won the "pennant;" the two pennant winners met after the end of the regular season in the World Series. The winner of at least four games (out of a possible seven) was the champion for that year. This arrangement still holds today, although the leagues are now subdivided and pennants are decided in post-season playoff series between the winners of each division.[2]		Baseball became popular in the 1920s, when Babe Ruth led the New York Yankees to several World Series titles and became a national hero on the strength of his home runs (balls that cannot be played because they have been hit out of the field). One of the most noteworthy players was the Brooklyn Dodgers' Jackie Robinson, who became the first African-American player in the major leagues in 1947. Prior to Robinson, black players had been restricted to the Negro League.[2]		Starting in the 1950s, major league baseball expanded its geographical range. Western cities got teams, either by luring them to move from eastern cities or by forming expansion teams with players made available by established teams. Until the 1970s, because of strict contracts, the owners of baseball teams also virtually owned the players; since then, the rules have changed so that players can become free agents, within certain limits, to sell their services to any team. The results have been bidding wars and stars who are paid millions of dollars a year. Disputes between the players' union and the owners have at times halted baseball for months at a time.[2]		Japan has also seen a prominent professional baseball circuit develop known as Nippon Professional Baseball, which was founded in 1934 and emerged as an international force after World War II. NPB is considered to be the highest caliber of baseball outside of the U.S. major leagues, and the best Japanese talent can emigrate to the U.S. by way of the posting system. Other prominent countries to play the game include South Korea (where their league has its own posting system with Major League Baseball), Mexico, Latin America, and the Caribbean states.		The game of American football was professionalized in the 1890s as a slow, and initially covert, process; William Heffelfinger and Ben "Sport" Donnelly were the first to secretly accept payment for playing the game in 1892.[citation needed] Regional leagues in Chicago, Pennsylvania, Ohio and New York had coalesced in the 1900s and 1910s, most of which gave way to the first truly national football league, the American Professional Football Association, in 1920.[citation needed] By 1920, pro football remained overshadowed by the college game. The first game involving an APFA team took place on September 26, 1920, at Douglas Park in Rock Island, Illinois, as the hometown Independents flattened the St. Paul Ideals 48-0. The first head-to-head battles in the league occurred one week later as Dayton topped Columbus 14-0 and Rock Island pasted Muncie 45-0.		Forward passes were rare, coaching from the sidelines was prohibited and players competed on both offense and defense. Money was so tight that Hals carried equipment, wrote press releases, sold tickets, taped ankles, played and coached for the Decatur club. As opposed to today’s standard 16-game schedule, clubs in 1920 scheduled their own opponents and could play non league and even college squads that counted toward their records. With no established guidelines, the number of games played—and the quality of opponents scheduled—by APFA teams varied, and the league did not maintain official standings.[3]		The Buffalo All-Americans, Chicago Tigers, Columbus Panhandles and Detroit Heralds joined the league before the end of the season, raising the total number of teams to 14, but the inaugural season was a struggle. Games received little attention from the fans—and even less from the press. According to Robert W. Peterson’s book "Pigskin: The Early Years of Pro Football," APFA games averaged crowds of 4,241. The association bylaws called for teams to pay a $100 entry fee, but no one ever did. Muncie played only one game before dropping out before the end of the season, which concluded on December 19.		At the conclusion of the season there were no playoffs (that innovation, although New York's regional league had used it, would not arrive until 1933) and it took more than four months before the league even bothered to crown a champion. Much as college football did for decades, the APFA determined its victor by ballot. On April 30, 1921, team representatives voted the Akron Pros, who completed the season undefeated with eight wins and three ties while yielding only a total of seven points, the champion in spite of protests by the one-loss teams in Decatur and Buffalo, who each had tied Akron and had more wins. The victors received a silver loving cup donated by sporting goods company Brunswick-Balke-Collender. While players were not given diamond-encrusted rings, they did receive golden fobs in the shape of a football inscribed with the words "World Champions."		Forgotten in the collective sports memory that the league’s official record books listed the 1920 championship as undecided until the 2013. The whereabouts of the Brunswick-Balke-Collender Cup, only given out that one time, are unknown. The legacy of two APFA franchises continues on, however. The Racine Cardinals now play in Arizona, and the Decatur Staleys moved to Chicago in 1921 and changed their name to the Bears the following year. Ten APFA players along with Carr are enshrined in the Pro Football Hall of Fame, which opened its doors in 1963 not far from the Canton automobile dealership that gave birth to the NFL in 1920.[4]		The APFA, by 1922 known as the National Football League, has remained the predominant professional football league in the United States, and, effectively, the entire world. The evolution from a haphazard collection of teams in big and small cities to the much more rigid structure it is in the present was gradual; the smaller markets were squeezed out in the late 1920s and early 1930s, a championship game was established in 1933, a draft was established in 1936, and schedules were standardized in the 1930s. A competing league has historically arisen to attempt to challenge the NFL's dominance every 10 to 15 years, but none managed to maintain long-term operations independent of the NFL and only two—the All-America Football Conference of the late 1940s and the American Football League of the 1960s—were strong enough to successfully compete against the league before the NFL subsumed their operations. Minor league football, although their leagues' memberships were unstable, began to arise in the late 1930s and remained viable as a business model up into the 1970s.		A major factor in the NFL's rise to dominance was its embrace of television early in the sport's history. As college football heavily restricted the rights of its teams to broadcast games (a policy eventually ruled to be illegal in 1984), the NFL instead allowed games to be televised nationwide, except in a team's home city; the restriction was softened in the early 1970s, by which point the NFL had secured broadcast deals with all of the major television networks, another major factor in the inability of any competing league to gain traction since then.		The related sport of Canadian football was eventually professionalized by the 1950s, which saw the evolution of the Canadian Football League. The CFL, despite losing all games in a series of contests against the NFL, was considered to be at least comparable in talent to the American leagues of the 1960s (its lone game against an AFL squad was a victory). Because Canada has a tenth of the population of the United States, the ability to make money from television was much lower, and although some of the cities of Canada were comparable to the major markets of the U.S., teams in places such as Saskatchewan and Hamilton were in markets quite small compared to even the small markets of the NFL, thus the CFL now pays noticeably less than other major professional leagues, but still more than enough to be considered fully professional.		The rise of indoor American football beginning in the late 1980s has allowed for smaller-scale professional football to be viable.		Ice hockey was first professionalized in Pittsburgh, Pennsylvania in the early 1900s (decade). As Canadians made up the vast majority of hockey players, early American professional leagues imported almost all of their talent before Canadian leagues began to form in the wake of a mining boom, depriving the U.S. leagues and teams of talent. Two distinct circuits formed: the Pacific Coast Hockey Association in western Canada and the northwestern U.S., and the National Hockey Association of central Canada, both of which competed for the then-independent Stanley Cup. The NHA's teams reorganized as the National Hockey League in 1917, and the West Coast circuit died out by the mid-1920s.		The NHL expanded to the United States in the 1920s, focusing on the northeastern and midwestern states. By the mid-1930s, the league had been set into six markets: Boston, New York City, Chicago and Detroit in the U.S., and Toronto and Montreal in Canada. These Original Six cities would be the only cities with NHL franchises from 1935 to 1967. During this time, the NHL was both stagnant and restrictive in its policies, giving teams territorial advantages, having teams with multiple owners in the same family (thus allowing the best players to be stacked onto certain teams), and restricting its players' salaries through reserve clauses. This stagnation allowed other leagues to arise: the Western Hockey League soon became the de facto major league of the western states and provinces, and the second-tier American Hockey League emerged in a number of midwestern markets the NHL had neglected, in addition to a handful of small towns.		Amid pressure from television networks that were threatening to offer the WHL a contract, the NHL doubled in size in 1967, beginning a period of expansion that lasted through much of the 1970s. The last major challenger to the NHL's dominance was the World Hockey Association, which successfully broke the NHL's reserve clause in court, drove up professional hockey salaries, and continued to pressure the older league into expansion. The WHA merged four of its remaining teams into the NHL in 1979, but had to give up most of its players, as they were still under NHL contract and had to return to their original teams. The NHL made its last pronounced realignment in the 1990s, moving most of the WHA teams out of their markets and establishing a number of new teams in the southern United States.		In Europe, the introduction of professionalism varied widely, and the highest-caliber league on the continent, the Soviet Championship League (proven to be at least equal to or better than the NHL in the 1970s), was officially composed of semi-professional works teams paid for their association with industries or government agencies (the Red Army squad employed members of the armed forces, and the Soviet Union often drafted the best hockey players in the country to serve on the squad). The modern-day descendant of the Soviet league, the Kontinental Hockey League, is fully professional and has a number of teams outside of Russia, to the point where it has the resources to sign NHL veterans. Other European countries such as Germany, Sweden, Switzerland, Norway, Finland, and Austria also have prominent professional leagues.		Basketball was invented in 1891 and the first professional leagues emerged in the 1920s. The Basketball Association of America was established in 1946 and three years later became the modern National Basketball Association. The NBA was slower to establish dominance of the sport than other sports in the U.S., as it would not do so until 1976, when it absorbed four teams from the American Basketball Association.		Professional basketball has the advantages of much smaller rosters than other professional sports, allowing the sport to be viable in smaller cities than other sports. Professional basketball leagues of varying caliber can be found around the world, especially in Europe and South America.		Professional athleticism is seen as a contradiction of the central ethos of sport, competition performed for its own sake and pure enjoyment, rather than as a means of earning a living.[1] Consequently, many organisations and commentators have resisted the growth of professional athleticism, saying that it was so incredible that it has impeded the development of sport. For example, rugby union was for many years a part-time sport engaged in by amateurs, and English cricket has allegedly suffered in quality because of a "non-professional" approach.[1] An important reason why professional sports has been resisted in history was that organisations for professional sports usually did not submit to the international sports federations, and could have their own rules.[citation needed] For example, the National Basketball Association was formerly not a member of the FIBA.		Men involved in professional sports can earn a great deal of money at the highest levels. For instance, the highest-paid team in professional baseball is New York Yankees.[5] Tiger Woods is the highest paid athlete totaling $127,902,706 including his endorsement income,[6] which massively exceeds what he earns from tournament golf. Woods recently became the world's first athlete to earn a billion dollars from prize money and endorsements.[7] It would have taken the salary of 2,000 1980s professional golfers each making $58,500 to match up with Tiger Woods’ current salary. Samuel Eto'o is the world's second highest earning athlete and the highest paid footballer in the world, raking in £35.7 million (over $54 million) a year excluding off-field earnings.[8] The top ten tennis players make about $3 million a year on average. Much of the growth in income for sports and athletes has come from broadcasting rights; for example, the most recent television contract for the NFL is valued at nearly US$5 billion per year.[9] Women in the U.S., on the other hand, make much less, for example as of 2014, the WNBA enforced a maximum salary of US$107,000 for star players (coaches could earn double that).[10] This is largely driven by the fact that the American viewing audience has far less interest in women's professional sport compared to men's; average in-person attendance and television viewership are both far higher for the NBA compared to the WNBA. According to investopedia.com, a male star like Kobe Bryant or LeBron James can individually earn a salary larger than every player in the WNBA combined.[11]		Outside of the highest leagues, however, the money professional athletes can earn drops dramatically, as fan bases are generally smaller and television revenues are nonexistent. For instance, while the National Football League's teams can afford to pay their players millions of dollars each year and still maintain a significant profit, the second-highest American football league in the United States, the United Football League, has consistently struggled to pay its bills and has continually lost money despite allotting its players only US$20,000 a year.[12][13][14] In the United States and Canada, most lower-end professional leagues run themselves as affiliated farm teams, effectively agreeing to develop younger players for eventual play in the major leagues in exchange for subsidizing those players' salaries; this is known as the minor league system and is most prevalent in professional baseball and professional ice hockey. Otherwise, the league may be required to classify itself as semi-professional, in other words, able to pay their players a small sum, but not enough to cover the player's basic costs of living.		Many professional athletes experience financial difficulties soon after retiring, due to a combination of bad investments, careless spending, and a lack of non-athletic skills. In the United States, some of these problems are mitigated by the fact that the college sports system ensures most professional athletes receive a college education with no student debt, a legacy that provides them with a career path after their sports career ends; the wear and tear of a career in pro sport, in contrast, can cause physical and mental side effects (such as chronic traumatic encephalopathy, a condition that has seen a massive rise in public awareness in the 2010s) that can harm a former professional athlete's employability.		In the NFL average annual salaries by position in 2009 were:[15]		The average salary of a player in the Chinese Super League was about ¥10.7 million (£1 million) for the 2011 season, up from ¥600,000 in the 2010 season. The highest paid player for the 2011 Chinese Super League season was Dario Conca of Guangzhou Evergrande who received an annual salary of ¥66.4 million ($10.5 million) after income tax, putting him among the highest paid players in the world.[17]		The highest paid player for the 2011-2012 Russian Premier League season was Samuel Eto'o of Anzhi Makhachkala, who at the end of the 2011-12 season was expected to receive a total salary of RUB 900.2 million (£35.7 million) after income tax, making Eto'o the second highest earning athlete in the world and the highest paid footballer in the world followed by Lionel Messi and Zlatan Ibrahimović.[8][18][19]		The average salary of a player in the German Bundesliga was about €3.3 million (£2.5 million) for the 2010-2011 season, up from €2.5 million in the 2009-2010 Bundesliga season.[20] The highest paid player for the 2010-2011 Bundesliga season was Franck Ribéry of Bayern Munich who received a salary of €6.3 million after income tax.[21]		In the Italian top league, Serie A, the average salary was about €5 million for the 2010-2011 Serie A season, up from €1 million in the 2005-2006 Serie A season.[22] The highest paid player for the 2010-2011 Serie A season was Zlatan Ibrahimović of A.C. Milan who received a salary of €25.9 million after income tax and which also includes Ibrahimović's bonuses and endorsements.[23]		Lionel Messi of FC Barcelona is the world's second highest paid player receiving a salary of £29.6 million (over US$45 million) a year after income taxation and which also includes the incomes of Messi's bonuses and endorsements.[24] In the Spanish La Liga, the average salary for the players of Lionel Messi's club FC Barcelona was €6.5 million for the 2010-2011 La Liga season, up from €5.5 million for the 2009-2010 La Liga season.[24]		The average salary of a player in the English Premier League was about £1.2 million in the 2007-2008 Premier League season, up from £676,000 in 2006-2007 Premier League season. Top players such as John Terry and Steven Gerrard can make up to £7 million per year with the players of Premier League club Manchester City F.C. receiving an average salary of £7 million for the 2010-2011 Premier League season, up from £5.5 million in the 2009-2010 Premier League season.[25][26] Players in lower divisions make significantly less money. In 2006-2007 season the average salary of a player in the Championship (the second tier of the English football pyramid) made £195,750 while the average salary for Football League and League Two (tier 4) was £49,600.[25]		The highest salary in Major League Soccer in 2015 was the $7.2 million paid to Kaká, who plays for the newly formed Orlando City SC.[27] Kaká was signed as a beneficiary of MLS' Designated Player Rule, which was instituted in 2007 for the express purpose of attracting international stars. Now-retired English star David Beckham was the first player signed under its provisions.[28] When the rule was instituted, each team had one "Designated Player" slot with a salary cap charge of $400,000, but no limit on actual salary paid.[28] Since then, the number of Designated Players per team has increased to three, with each counting for $387,500 of cap room in 2014.[29] The league's average salary is about $283,000 per year, but the median salary is closer to $110,000.[30] MLS' minimum player salary will increase in 2016 from $60,000 to $62,500 for most players, and roster players #25-28 will see their minimum salary increase from $50,000 to $51,500.[31][32]		In 1970, the average salary in Major League Baseball in the U.S. and Canada was $20,000 ($123,342 inflation-adjusted). By 2005, the average salary had increased to $2,632,655 ($3,228,355 inflation-adjusted) and the minimum salary was $316,000 (adjusted: $387,502).[33] In 2012 the average MLB salary was $3,440,000, the median salary was $1,075,000, and the minimum salary had grown to four times the inflation-adjusted average salary in 1970 ($480,000).[34]		
A hidden curriculum is a side effect of an education, "[lessons] which are learned but not openly intended"[1] such as the transmission of norms, values, and beliefs conveyed in the classroom and the social environment.[2]		Any learning experience may teach unintended lessons.[1] Hidden curriculum often refers to knowledge gained in primary and secondary school settings, usually with a negative connotation where the school strives for equal intellectual development (as a positive aim).[3] In this sense, a hidden curriculum reinforces existing social inequalities by educating students according to their class and social status. The unequal distribution of cultural capital in a society mirrors a corresponding distribution of knowledge among its students.[4]						Early workers in the field of education were influenced by the notion that the preservation of the social privileges, interests, and knowledge of one group within the population was worth the exploitation of less powerful groups.[4] Over time this theory has become less blatant, yet its underlying tones remain a contributing factor to the issue of the hidden curriculum.		Several educational theories have been developed to help give meaning and structure to the hidden curriculum and to illustrate the role that schools play in socialization. Three of these theories, as cited by Henry Giroux and Anthony Penna, are a structural-functional view of schooling, a phenomenological view related to the "new" sociology of education, and a radical critical view corresponding to the neo-Marxist analysis of the theory and practice of education. The structural-functional view focuses on how norms and values are conveyed within schools and how their necessities for the functioning of society become indisputably accepted. The phenomenological view suggests that meaning is created through situational encounters and interactions, and it implies that knowledge is somewhat objective. The radical critical view recognizes the relationship between economic and cultural reproduction and stresses the relationships among the theory, ideology, and social practice of learning. Although the first two theories have contributed to the analysis of the hidden curriculum, the radical critical view of schooling provides the most insight.[2] Most importantly it acknowledges the perpetuated economic and social aspects of education that are clearly illustrated by the hidden curriculum.		Various aspects of learning contribute to the success of the hidden curriculum, including practices, procedures, rules, relationships, and structures.[1] Many school-specific sources, some of which may be included in these aspects of learning, give rise to important elements of the hidden curriculum. These sources may include, but are not limited to, the social structures of the classroom, the teacher's exercise of authority, rules governing the relationship between teachers and students, standard learning activities, the teacher's use of language, textbooks, audio-visual aids, furnishings, architecture, disciplinary measures, timetables, tracking systems, and curricular priorities.[1] Variations among these sources promote the disparities found when comparing the hidden curricula corresponding to various class and social statuses. "Every school is both an expression of a political situation and a teacher of politics."[5]		While the actual material that students absorb through the hidden curriculum is of utmost importance, the personnel who convey it elicit special investigation. This particularly applies to the social and moral lessons conveyed by the hidden curriculum, for the moral characteristics and ideologies of teachers and other authority figures are translated into their lessons, albeit not necessarily with intention.[6] Yet these unintended learning experiences can result from interactions with not only instructors, but also with peers. Like interactions with authority figures, interactions amongst peers can promote moral and social ideals but also foster the exchange of information and are thus important sources of knowledge contributing to the success of the hidden curriculum.		Although the hidden curriculum conveys a great deal of knowledge to its students, the inequality promoted through its disparities among classes and social statuses often invokes a negative connotation. For example, Pierre Bourdieu asserts that education-related capital must be accessible to promote academic achievement. The effectiveness of schools becomes limited when these forms of capital are unequally distributed.[7] Since the hidden curriculum is considered to be a form of education-related capital, it promotes this ineffectiveness of schools as a result of its unequal distribution. As a means of social control, the hidden curriculum promotes the acceptance of a social destiny without promoting rational and reflective consideration.[8] According to Elizabeth Vallance, the functions of hidden curriculum include "the inculcation of values, political socialization, training in obedience and docility, the perpetuation of traditional class structure-functions that may be characterized generally as social control."[9] Hidden curriculum can also be associated with the reinforcement of social inequality, as evidenced by the development of different relationships to capital based on the types of work and work-related activities assigned to students varying by social class.[10]		Although the hidden curriculum has negative connotations, it is not inherently negative, and the tacit factors that are involved can potentially exert a positive developmental force on students. Some educational approaches, such as democratic education, actively seek to minimize, make explicit, and/ or reorient the hidden curriculum in such a way that it has a positive developmental impact on students. Similarly, in the fields of environmental education and education for sustainable development, there has been some advocacy for making school environments more natural and sustainable, such that the tacit developmental forces that these physical factors exert on students can become positive factors in their development as environmental citizens.[11][12]		While studies on the hidden curriculum mostly focus on fundamental primary and secondary education, higher education also feels the effects of this latent knowledge. For example, gender biases become present in specific fields of study; the quality of and experiences associated with prior education become more significant; and class, gender, and race become more evident at higher levels of education.[13]		One additional aspect of hidden curriculum that plays a major part in the development of students and their fates is tracking. This method of imposing educational and career paths upon students at young ages relies on various factors such as class and status to reinforce socioeconomic differences. Children tend to be placed on tracks guiding them towards socioeconomic occupations similar to that of their parents, without real considerations for their strengths and weaknesses. As students advance through the educational system, they follow along their tracks by completing the predetermined courses.[14] For example, this is one of the main factors limiting social mobility in America today.		John Dewey explored the hidden curriculum of education in his early 20th century works, particularly his classic, Democracy and Education. Dewey saw patterns evolving and trends developing in public schools which lent themselves to his pro-democratic perspectives. His work was quickly rebutted by educational theorist George Counts, whose 1929 book, Dare the School Build a New Social Order challenged the presumptive nature of Dewey's works. Where Dewey (and other child development theorists including Jean Piaget, Erik Erikson and Maria Montessori) hypothesized a singular path through which all young people travelled in order to become adults, Counts recognized the reactive, adaptive, and multifaceted nature of learning. This nature caused many educators to slant their perspectives, practices, and assessments of student performance in particular directions which affected their students drastically.[citation needed] Counts' examinations were expanded on by Charles A. Beard, and later, Myles Horton as he created what became the Highlander Folk School in Tennessee.		The phrase "hidden curriculum" was reportedly coined by Philip W. Jackson (Life In Classrooms, 1968). He argued that we need to understand "education" as a socialization process. Shortly after Jackson's coinage, MIT's Benson Snyder published The Hidden Curriculum, which addresses the question of why students—even or especially the most gifted—turn away from education. Snyder advocates the thesis that much of campus conflict and students' personal anxiety is caused by a mass of unstated academic and social norms, which thwart the students' ability to develop independently or think creatively.		The hidden curriculum has been further explored by a number of educators. Starting with Pedagogy of the Oppressed, published in 1972, through the late 1990s, Brazilian educator Paulo Freire explored various effects of presumptive teaching on students, schools, and society as a whole. Freire's explorations were in sync with those of John Holt and Ivan Illich, each of whom were quickly identified as radical educators. Other theorists who have identified the insidious nature of hidden curricula and hidden agendas include Neil Postman, Paul Goodman, Joel Spring, John Taylor Gatto, and others.		More recent definitions were given by Roland Meighan ("A Sociology of Education", 1981):		The hidden curriculum is taught by the school, not by any teacher...something is coming across to the pupils which may never be spoken in the English lesson or prayed about in assembly. They are picking-up an approach to living and an attitude to learning.		and Michael Haralambos ("Sociology: Themes and Perspectives", 1991):		The hidden curriculum consists of those things pupils learn through the experience of attending school rather than the stated educational objectives of such institutions.		Educational critics Henry Giroux,[2] bell hooks, and Jonathan Kozol also have examined the effects of hidden curriculum.		Developmental psychologist Robert Kegan addressed the hidden curriculum of everyday life in his 1994 book In Over Our Heads, which focused on the relation between cognitive development and the "cognitive demands" of cultural expectations.		Professor of communication Joseph Turow, in his 2017 book The Aisles Have Eyes, used the concept to describe acculturation to massive personal data collection; he wrote:[15]		The very activities that dismay privacy and anti-discrimination advocates are already beginning to become everyday habits in American lives, and part of Americans' cultural routines. Retailing is at the leading edge of a new hidden curriculum for American society—teaching people what they have to give up in order to get along in the twenty-first century.		
Coordinates: 13°N 105°E﻿ / ﻿13°N 105°E﻿ / 13; 105		in ASEAN  (dark grey)  –  [Legend]		Cambodia (/kæmˈboʊdiə/ ( listen);[6] Khmer: កម្ពុជា, or Kampuchea IPA: [kɑmpuˈciə]), officially known as the Kingdom of Cambodia (Khmer: ព្រះរាជាណាចក្រកម្ពុជា, Preăh Réachéanachâk Kâmpŭchéa, IPA: [ˈprĕəh riəciənaːˈcɑk kɑmpuˈciə]), is a country located in the southern portion of the Indochina Peninsula in Southeast Asia. It is 181,035 square kilometres (69,898 square miles) in area, bordered by Thailand to the northwest, Laos to the northeast, Vietnam to the east, and the Gulf of Thailand to the southwest.		Cambodia has a population of over 15 million. The official religion is Theravada Buddhism, practiced by approximately 95 percent of the population. The country's minority groups include Vietnamese, Chinese, Chams, and 30 hill tribes.[7] The capital and largest city is Phnom Penh, the political, economic, and cultural centre of Cambodia. The kingdom is a constitutional monarchy with Norodom Sihamoni, a monarch chosen by the Royal Throne Council, as head of state. The head of government is Hun Sen, who is currently the longest serving non-royal leader in South East Asia and has ruled Cambodia for over 30 years.		In 802 AD, Jayavarman II declared himself king, uniting the warring Khmer princes of Chenla under the name "Kambuja".[8] This marked the beginning of the Khmer Empire which flourished for over 600 years, allowing successive kings to control and exert influence over much of Southeast Asia and accumulate immense power and wealth. The Indianized kingdom built monumental temples including Angkor Wat, now a World Heritage Site, and facilitated the spread of first Hinduism, then Buddhism to much of Southeast Asia. After the fall of Angkor to Ayutthaya in the 15th century, a reduced and weakened Cambodia was then ruled as a vassal state by its neighbours. In 1863 Cambodia became a protectorate of France which doubled the size of the country by reclaiming the north and west from Thailand.		Cambodia gained independence in 1953. The Vietnam War extended into the country with the US bombing of Cambodia from 1969 until 1973. Following the Cambodian coup of 1970, the deposed king gave his support to his former enemies, the Khmer Rouge. The Khmer Rouge emerged as a major power, taking Phnom Penh in 1975 and later carrying out the Cambodian Genocide from 1975 until 1979, when they were ousted by Vietnam and the Vietnamese-backed People's Republic of Kampuchea in the Cambodian–Vietnamese War (1979–91). Following the 1991 Paris Peace Accords, Cambodia was governed briefly by a United Nations mission (1992–93). The UN withdrew after holding elections in which around 90 percent of the registered voters cast ballots. The 1997 coup placed power solely in the hands of Prime Minister Hun Sen and the Cambodian People's Party, who remain in power as of 2017[update].		The country faces numerous challenges. Important sociopolitical issues includes widespread poverty,[9] pervasive corruption,[10] lack of political freedoms,[11] low human development,[12] and a high rate of hunger.[13][14][15] Cambodia has been described by Human Rights Watch's Southeast Asian Director, David Roberts, as a "vaguely communist free-market state with a relatively authoritarian coalition ruling over a superficial democracy."[16] While per capita income remains low compared to most neighbouring countries, Cambodia has one of the fastest growing economies in Asia with growth averaging 6 percent over the last decade. Agriculture remains the dominant economic sector, with strong growth in textiles, construction, garments, and tourism leading to increased foreign investment and international trade.[17] Cambodia scored dismally in an annual index (2015) ranking the rule of law in 102 countries, placing 99th overall and the worst in the region.[18]		Cambodia also faces environmental destruction as an imminent problem. The most severe activity in this regard is considered to be the countrywide deforestation, which also involves national parks and wildlife sanctuaries. Overall, environmental destruction in Cambodia comprise many different activities, including illegal logging, poaching of endangered and endemic species, and destruction of important wildlife habitats from large scale construction projects and agricultural businesses. The degrading activities involve the local population, Cambodian businesses and political authorities, as well as foreign criminal syndicates and many transnational corporations from all over the world.						The "Kingdom of Cambodia" is the official English name of the country. The English "Cambodia" is an anglicisation of the French "Cambodge", which in turn is the French transliteration of the Khmer Kampuchea. Kampuchea is the shortened alternative to the country's official name in Khmer, Preah Reacheanachak Kampuchea (Khmer: ព្រះរាជាណាចក្រកម្ពុជា). The Khmer endonym Kampuchea derives from the Sanskrit name Kambujadeśa (कम्बोजदेश), composed of देश, desa ("land of" or "country of") and कम्बोज, Kambujas, which alludes to the foundation myths of the first ancient Khmer kingdom.[19]		Colloquially, Cambodians refer to their country as either Srok Khmer (Khmer pronunciation: [srok kʰmae]), meaning "Khmer's Land", or the slightly more formal Prateh Kampuchea (ប្រទេសកម្ពុជា), literally "Country of Kampuchea". The name "Cambodia" is used most often in the Western world while "Kampuchea" is more widely used in the East.[20][21][22]		There exists sparse evidence for a Pleistocene human occupation of present-day Cambodia, which includes quartz and quartzite pebble tools found in terraces along the Mekong River, in Stung Treng and Kratié provinces, and in Kampot Province, although their dating is unreliable.[25] Some slight archaeological evidence shows communities of hunter-gatherers inhabited the region during Holocene: the most ancient archaeological discovery site in Cambodia is considered to be the cave of L'aang Spean, in Battambang Province, which belongs to the Hoabinhian period. Excavations in its lower layers produced a series of radiocarbon dates as of 6000 BC.[25][26] Upper layers in the same site gave evidence of transition to Neolithic, containing the earliest dated earthenware ceramics in Cambodia[27]		Archaeological records for the period between Holocene and Iron Age remain equally limited. A pivotal event in Cambodian prehistory was the slow penetration of the first rice farmers from the north, which began in the late 3rd millennium BC.[28] The most curious prehistoric evidence in Cambodia are the various "circular earthworks" discovered in the red soils near Memot and in the adjacent region of Vietnam in the latter 1950s. Their function and age are still debated, but some of them possibly date from 2nd millennium BC.[29][30]		Other prehistoric sites of somewhat uncertain date are Samrong Sen (not far from the ancient capital of Oudong), where the first investigations began in 1875,[31] and Phum Snay, in the northern province of Banteay Meanchey.[32] An excavation at Phum Snay revealed 21 graves with iron weapons and cranial trauma which could point to conflicts in the past, possible with larger cities in Angkor.[28][33] [34] Prehistoric artefacts are often found during mining activities in Ratanakiri.[25]		Iron was worked by about 500 BC, with supporting evidence coming from the Khorat Plateau, in modern-day Thailand. In Cambodia, some Iron Age settlements were found beneath Baksei Chamkrong and other Angkorian temples while circular earthworks were found beneath Lovea a few kilometres north-west of Angkor. Burials, much richer than other types of finds, testify to improvement of food availability and trade (even on long distances: in the 4th century BC trade relations with India were already opened) and the existence of a social structure and labour organisation. Also, among the artefacts from the Iron Age, glass beads are important evidence. Different kinds of glass beads recovered from several sites across Cambodia, such as the Phum Snay site in northwest and the Prohear site in southeast, show that there were two main trading networks at the time. The two networks were separated by time and space, which indicate that there was a shift from one network to the other at about 2nd–4th century AD, probably with changes in socio-political powers.[35]		During the 3rd, 4th, and 5th centuries, the Indianised states of Funan and its successor, Chenla, coalesced in present-day Cambodia and southwestern Vietnam. For more than 2,000 years, what was to become Cambodia absorbed influences from India, passing them on to other Southeast Asian civilisations that are now Thailand and Laos.[36] Little else is known for certain of these polities, however Chinese chronicles and tribute records do make mention of them. It is believed that the territory of Funan may have held the port known to Alexandrian geographer Claudius Ptolemy as "Kattigara". The Chinese chronicles suggest that after Jayavarman I of Chenla died around 690, turmoil ensued which resulted in division of the kingdom into Land Chenla and Water Chenla which was loosely ruled by weak princes under the dominion of Java.		The Khmer Empire grew out of these remnants of Chenla becoming firmly established in 802 when Jayavarman II (reigned c790-850) declared independence from Java and proclaimed himself a Devaraja. He and his followers instituted the cult of the God-king and began a series of conquests that formed an empire which flourished in the area from the 9th to the 15th centuries.[37] During the rule of Jayavarman VIII the Angkor empire was attacked by the Mongol army of Kublai Khan, however the king was able to buy peace.[38] Around the 13th century, monks from Sri Lanka introduced Theravada Buddhism to Southeast Asia.[39] The religion spread and eventually displaced Hinduism and Mahayana Buddhism as the popular religion of Angkor; however it was not the official state religion until 1295; when Indravarman III took power.[40]		The Khmer Empire was Southeast Asia's largest empire during the 12th century. The empire's centre of power was Angkor, where a series of capitals were constructed during the empire's zenith. In 2007 an international team of researchers using satellite photographs and other modern techniques concluded that Angkor had been the largest pre-industrial city in the world with an urban sprawl of 1,150 square miles (2,978 square kilometres).[41] The city, which could have supported a population of up to one million people[42] and Angkor Wat, the best known and best-preserved religious temple at the site, still serve as reminders of Cambodia's past as a major regional power. The empire, though in decline, remained a significant force in the region until its fall in the 15th century.		After a long series of wars with neighbouring kingdoms, Angkor was sacked by the Ayutthaya Kingdom and abandoned in 1432 because of ecological failure and infrastructure breakdown.[43][44] This led to a period of economic, social, and cultural stagnation when the kingdom's internal affairs came increasingly under the control of its neighbours. By this time, the Khmer penchant for monument building had ceased. Older faiths such as Mahayana Buddhism and the Hindu cult of the god-king had been supplanted by Theravada Buddhism.		The court moved the capital to Longvek where the kingdom sought to regain its glory through maritime trade. The first mention of Cambodia in European documents was in 1511 by the Portuguese. Portuguese travellers described the city as a place of flourishing wealth and foreign trade. The attempt was short-lived however, as continued wars with Ayutthaya and the Vietnamese resulted in the loss of more territory and Longvek being conquered and destroyed by King Naresuan the Great of Ayutthaya in 1594. A new Khmer capital was established at Oudong south of Longvek in 1618, but its monarchs could survive only by entering into what amounted to alternating vassal relationships with the Siamese and Vietnamese for the next three centuries with only a few short-lived periods of relative independence.		The hill tribe people in Cambodia were "hunted incessantly and carried off as slaves by the Siamese (Thai), the Annamites (Vietnamese), and the Cambodians."[45]		In the nineteenth century a renewed struggle between Siam and Vietnam for control of Cambodia resulted in a period when Vietnamese officials attempted to force the Khmers to adopt Vietnamese customs. This led to several rebellions against the Vietnamese and appeals to Thailand for assistance. The Siamese–Vietnamese War (1841–1845) ended with an agreement to place the country under joint suzerainty. This later led to the signing of a treaty for French Protection of Cambodia by King Norodom Prohmborirak.		In 1863, King Norodom, who had been installed by Thailand,[8] sought the protection of France from the Thai rule. In 1867, the Thai king signed a treaty with France, renouncing suzerainty over Cambodia in exchange for the control of Battambang and Siem Reap provinces which officially became part of Thailand. The provinces were ceded back to Cambodia by a border treaty between France and Thailand in 1907.		Cambodia continued as a protectorate of France from 1867 to 1953, administered as part of the colony of French Indochina, though occupied by the Japanese empire from 1941 to 1945.[46] Between 1874 and 1962, the total population increased from about 946,000 to 5.7 million.[47] After King Norodom's death in 1904, France manipulated the choice of king, and Sisowath, Norodom's brother, was placed on the throne. The throne became vacant in 1941 with the death of Monivong, Sisowath's son, and France passed over Monivong's son, Monireth, feeling he was too independently minded. Instead, Norodom Sihanouk, a maternal grandson of King Sisowath was enthroned. The French thought young Sihanouk would be easy to control.[46] They were wrong, however, and under the reign of King Norodom Sihanouk, Cambodia gained independence from France on 9 November 1953.[46]		Cambodia became a constitutional monarchy under King Norodom Sihanouk. When French Indochina was given independence, Cambodia lost hope of regaining control over the Mekong Delta as it was awarded to Vietnam. Formerly part of the Khmer Empire, the area had been controlled by the Vietnamese since 1698, with King Chey Chettha II granting Vietnamese permission to settle in the area decades before.[48] This remains a diplomatic sticking point with over one million ethnic Khmers (the Khmer Krom) still living in this region. The Khmer Rouge attempted invasions to recover the territory which, in part, led to Vietnam's invasion of Cambodia and deposition of the Khmer Rouge.		In 1955, Sihanouk abdicated in favour of his father to participate in politics and was elected prime minister. Upon his father's death in 1960, Sihanouk again became head of state, taking the title of prince. As the Vietnam War progressed, Sihanouk adopted an official policy of neutrality in the Cold War. Sihanouk allowed the Vietnamese communists to use Cambodia as a sanctuary and a supply route for their arms and other aid to their armed forces fighting in South Vietnam. This policy was perceived as humiliating by many Cambodians. In December 1967 Washington Post journalist Stanley Karnow was told by Sihanouk that if the US wanted to bomb the Vietnamese communist sanctuaries, he would not object, unless Cambodians were killed.[49]		The same message was conveyed to US President Johnson's emissary Chester Bowles in January 1968.[50] However, in public Sihanouk refuted the US' right to use air strikes in Cambodia and on 26 March Prince Sihanouk said "these criminal attacks must immediately and definitively stop..." and on 28 March a press conference was held and Sihanouk appealed to the international media "I appeal to you to publicise abroad this very clear stand of Cambodia—that is, I will in any case oppose all bombings on Cambodian territory under whatever pretext." Nevertheless, the public pleas of Sihanouk were ignored and the bombing continued.[51]		Members of the government and army became resentful of Sihanouk's ruling style as well as his tilt away from the United States.		While visiting Beijing in 1970 Sihanouk was ousted by a military coup led by Prime Minister General Lon Nol and Prince Sisowath Sirik Matak. US support for the coup remains unproven.[53] However, once the coup was completed, the new regime, which immediately demanded that the Vietnamese communists leave Cambodia, gained the political support of the United States. The North Vietnamese and Viet Cong forces, desperate to retain their sanctuaries and supply lines from North Vietnam, immediately launched armed attacks on the new government. The king urged his followers to help in overthrowing this government, hastening the onset of civil war.[54]		Soon Khmer Rouge rebels began using him to gain support. However, from 1970 until early 1972, the Cambodian conflict was largely one between the government and army of Cambodia, and the armed forces of North Vietnam. As they gained control of Cambodian territory, the Vietnamese communists imposed a new political infrastructure, which was eventually dominated by the Cambodian communists now referred to as the Khmer Rouge.[55] Between 1969 and 1973, Republic of Vietnam and US forces bombed Cambodia in an effort to disrupt the Viet Cong and Khmer Rouge.		Documents uncovered from the Soviet archives after 1991 reveal that the North Vietnamese attempt to overrun Cambodia in 1970 was launched at the explicit request of the Khmer Rouge and negotiated by Pol Pot's then second in command, Nuon Chea.[56] NVA units overran many Cambodian army positions while the Communist Party of Kampuchea (CPK) expanded their small-scale attacks on lines of communication. In response to the North Vietnamese invasion, US President Richard Nixon announced that US and South Vietnamese ground forces had entered Cambodia in a campaign aimed at destroying NVA base areas in Cambodia (see Cambodian Incursion).[57] Although a considerable quantity of equipment was seized or destroyed by US and South Vietnamese forces, containment of North Vietnamese forces proved elusive.		The Khmer Republic's leadership was plagued by disunity among its three principal figures: Lon Nol, Sihanouk's cousin Sirik Matak, and National Assembly leader In Tam. Lon Nol remained in power in part because neither of the others was prepared to take his place. In 1972, a constitution was adopted, a parliament elected, and Lon Nol became president. But disunity, the problems of transforming a 30,000-man army into a national combat force of more than 200,000 men, and spreading corruption weakened the civilian administration and army.		The Communist insurgency inside Cambodia continued to grow, aided by supplies and military support from North Vietnam. Pol Pot and Ieng Sary asserted their dominance over the Vietnamese-trained communists, many of whom were purged. At the same time, the CPK forces became stronger and more independent of their Vietnamese patrons. By 1973, the CPK were fighting battles against government forces with little or no North Vietnamese troop support, and they controlled nearly 60% of Cambodia's territory and 25% of its population. The government made three unsuccessful attempts to enter into negotiations with the insurgents, but by 1974, the CPK were operating openly as divisions, and some of the NVA combat forces had moved into South Vietnam. Lon Nol's control was reduced to small enclaves around the cities and main transportation routes. More than 2 million refugees from the war lived in Phnom Penh and other cities.		On New Year's Day 1975, Communist troops launched an offensive which, in 117 days of the hardest fighting of the war, collapsed the Khmer Republic. Simultaneous attacks around the perimeter of Phnom Penh pinned down Republican forces, while other CPK units overran fire bases controlling the vital lower Mekong resupply route. A US-funded airlift of ammunition and rice ended when Congress refused additional aid for Cambodia. The Lon Nol government in Phnom Penh surrendered on 17 April 1975, just five (5) days after the US mission evacuated Cambodia.[58]		The Khmer Rouge reached Phnom Penh and took power in 1975. Led by Pol Pot, they changed the official name of the country to Democratic Kampuchea. The new regime modelled itself on Maoist China during the Great Leap Forward, immediately evacuated the cities, and sent the entire population on forced marches to rural work projects. They attempted to rebuild the country's agriculture on the model of the 11th century, discarded Western medicine and destroyed temples, libraries, and anything considered Western.		Estimates as to how many people were killed by the Khmer Rouge regime range from approximately one to three million; the most commonly cited figure is two million (about a quarter of the population).[59][60][61] This era gave rise to the term Killing Fields, and the prison Tuol Sleng became notorious for its history of mass killing. Hundreds of thousands fled across the border into neighbouring Thailand. The regime disproportionately targeted ethnic minority groups. The Cham Muslims suffered serious purges with as much as half of their population exterminated.[62] Pol Pot was determined to keep his power and disenfranchise any enemies or potential threats, and thus increased his violent and aggressive actions against his people.[63]		Forced repatriation in 1970 and deaths during the Khmer Rouge era reduced the Vietnamese population in Cambodia from between 250,000 and 300,000 in 1969 to a reported 56,000 in 1984.[47] However, most of the victims of the Khmer Rouge regime were not ethnic minorities but ethnic Khmer. Professionals, such as doctors, lawyers and teachers, were also targeted. According to Robert D. Kaplan, "eyeglasses were as deadly as the yellow star" as they were seen as a sign of intellectualism.[64]		Religious institutions were not spared by the Khmer Rouge as well, religion was so viciously persecuted to such a terrifying extent that the vast majority of Cambodia's historic architecture, 95% of Cambodia's Buddhist temples, was completely destroyed.[65]		In November 1978, Vietnamese troops invaded Cambodia in response to border raids by the Khmer Rouge.[66] The People's Republic of Kampuchea (PRK), a pro-Soviet state led by the Kampuchean People's Revolutionary Party, a party created by the Vietnamese in 1951, and led by a group of Khmer Rouge who had fled Cambodia to avoid being purged by Pol Pot and Ta Mok, was established.[clarification needed][67] It was fully beholden to the occupying Vietnamese army and under direction of the Vietnamese ambassador to Phnom Penh. Its arms came from Vietnam and the Soviet Union.[68]		In opposition to the newly created state, a government-in-exile referred to as the Coalition Government of Democratic Kampuchea (CGDK) was formed in 1981 from three factions.[68] This consisted of the Khmer Rouge, a royalist faction led by Sihanouk, and the Khmer People's National Liberation Front. Its credentials were recognised by the United Nations. The Khmer Rouge representative to the UN, Thiounn Prasith, was retained, but he had to work in consultation with representatives of the noncommunist Cambodian parties.[69][70] The refusal of Vietnam to withdraw from Cambodia led to economic sanctions[71] by the US and its allies.[specify]		Peace efforts began in Paris in 1989 under the State of Cambodia, culminating two years later in October 1991 in a Paris Comprehensive Peace Settlement. The UN was given a mandate to enforce a ceasefire and deal with refugees and disarmament known as the United Nations Transitional Authority in Cambodia (UNTAC).[72]		In 1993, Norodom Sihanouk was restored as King of Cambodia, but all power was in the hands of the government established after the UNTAC sponsored elections. The stability established following the conflict was shaken in 1997 by a coup d'état led by the co-Prime Minister Hun Sen against the non-communist parties in the government.[73] In recent years, reconstruction efforts have progressed and led to some political stability through a multiparty democracy under a constitutional monarchy.[1]		In July 2010, Kang Kek Iew was the first Khmer Rouge member found guilty of war crimes and crimes against humanity in his role as the former commandant of the S21 extermination camp and he was sentenced to life in prison.[74][75] However, Hun Sen has opposed extensive trials of former Khmer Rouge mass murderers.[76]		In August 2014, a U.N.-backed war crimes tribunal, the Extraordinary Chambers in the Courts of Cambodia (also known as the Khmer Rouge Tribunal), sentenced Khieu Samphan, the regime's 83-year-old former head of state, and Nuon Chea, its 88-year-old chief ideologue to life in prison on war crimes charges for their role in the country's terror period in the 1970s. The trial began in November 2011. Former Foreign Minister Ieng Sary died in 2013, while his wife, Social Affairs Minister Ieng Thirith, was deemed unfit to stand trial due to dementia in 2012. The group's top leader, Pol Pot, died in 1998.		Cambodia has an area of 181,035 square kilometres (69,898 square miles) and lies entirely within the tropics, between latitudes 10° and 15°N, and longitudes 102° and 108°E. It borders Thailand to the north and west, Laos to the northeast, and Vietnam to the east and southeast. It has a 443-kilometre (275-mile) coastline along the Gulf of Thailand.		Cambodia's landscape is characterised by a low-lying central plain that is surrounded by uplands and low mountains and includes the Tonle Sap (Great Lake) and the upper reaches of the Mekong River delta. Extending outward from this central region are transitional plains, thinly forested and rising to elevations of about 650 feet (200 metres) above sea level.		To the north the Cambodian plain abuts a sandstone escarpment, which forms a southward-facing cliff stretching more than 200 miles (320 kilometres) from west to east and rising abruptly above the plain to heights of 600 to 1,800 feet (180–550 metres). This cliff marks the southern limit of the Dângrêk Mountains.		Flowing south through the country's eastern regions is the Mekong River. East of the Mekong the transitional plains gradually merge with the eastern highlands, a region of forested mountains and high plateaus that extend into Laos and Vietnam. In southwestern Cambodia two distinct upland blocks, the Krâvanh Mountains and the Dâmrei Mountains, form another highland region that covers much of the land area between the Tonle Sap and the Gulf of Thailand.		In this remote and largely uninhabited area, Phnom Aural, Cambodia's highest peak rises to an elevation of 5,949 feet (1,813 metres). The southern coastal region adjoining the Gulf of Thailand is a narrow lowland strip, heavily wooded and sparsely populated, which is isolated from the central plain by the southwestern highlands.		The most distinctive geographical feature is the inundations of the Tonle Sap (Great Lake), measuring about 2,590 square kilometres (1,000 square miles) during the dry season and expanding to about 24,605 square kilometres (9,500 square miles) during the rainy season. This densely populated plain, which is devoted to wet rice cultivation, is the heartland of Cambodia. Much of this area has been designated as a biosphere reserve.		Cambodia's climate, like that of the rest of Southeast Asia, is dominated by monsoons, which are known as tropical wet and dry because of the distinctly marked seasonal differences.		Cambodia has a temperature range from 21 to 35 °C (69.8 to 95.0 °F) and experiences tropical monsoons. Southwest monsoons blow inland bringing moisture-laden winds from the Gulf of Thailand and Indian Ocean from May to October. The northeast monsoon ushers in the dry season, which lasts from November to April. The country experiences the heaviest precipitation from September to October with the driest period occurring from January to February.		According to the International Development Research Center and The United Nations, Cambodia is considered Southeast Asia’s most vulnerable country to the effects of climate change, alongside the Phillipines.[77][78] Rural coastal populations are particularly at risk. Shortages of clean water, extreme flooding, mudslides, higher sea levels and potentially destructive storms are of particular concern, according to the Cambodia Climate Change Alliance.		Cambodia has two distinct seasons. The rainy season, which runs from May to October, can see temperatures drop to 22 °C (71.6 °F) and is generally accompanied with high humidity. The dry season lasts from November to April when temperatures can rise up to 40 °C (104 °F) around April. Disastrous flooding occurred in 2001 and again in 2002, with some degree of flooding almost every year.[79]		Cambodia's biodiversity is largely founded on its seasonal tropical forests, containing some 180 recorded tree species, and riparian ecosystems. There are 212 mammal species, 536 bird species, 240 reptile species, 850 freshwater fish species (Tonle Sap Lake area), and 435 marine fish species recorded by science. Much of this biodiversity is contained around the Tonle Sap Lake and the surrounding biosphere.[80]		The Tonle Sap Biosphere Reserve is a reserve surrounding the Tonle Sap lake. It encompasses the lake and nine provinces: Kampong Thom, Siem Reap, Battambang, Pursat, Kampong Chhnang, Banteay Meanchey, Pailin, Oddar Meanchey and Preah Vihear. In 1997, it was successfully nominated as a UNESCO Biosphere Reserve.[81] Other key habitats include the dry forest of Mondolkiri and Ratanakiri provinces and the Cardamom Mountains ecosystem, including Bokor National Park, Botum-Sakor National Park, and the Phnom Aural and Phnom Samkos wildlife sanctuaries.		The Worldwide Fund for Nature recognises six distinct terrestrial ecoregions in Cambodia – the Cardamom Mountains rain forests, Central Indochina dry forest, Southeast Indochina dry evergreen forest, Southern Annamite Range rain forest, Tonle Sap freshwater swamp forest, and Tonle Sap-Mekong peat swamp forest.[82]		Cambodia has a bad but improving performance in the global Environmental Performance Index (EPI) with an overall ranking of 146 out of 180 countries in 2016. This is among the worst in the Southeast Asian region, only ahead of Laos and Myanmar. The EPI was established in 2001 by the World Economic Forum as a global gauge to measure how well individual countries perform in implementing the United Nations' Sustainable Development Goals. The environmental areas where Cambodia performs worst (i.e. highest ranking) are air quality (148), water resource management (140) and health impacts of environmental issues (137), with the areas of sanitation, environmental impacts of fisheries and forest management following closely. Cambodia performs best when it comes to handling the nitrogen balance in the agricultural industry specifically, an area where Cambodia excels and are among the best in the world. In addition, Cambodia has an unusually large area of wildlife protections, both on land and at sea, with the land-based protections covering about 20% of the country. This secures Cambodia a better than average ranking of 61 in relation to biodiversity and habitat, even though illegal logging, construction and poaching are heavily deteriorating these protections and habitats in reality.[83][84][85]		The rate of deforestation in Cambodia is one of the highest in the world and it is often perceived as the most destructive, singular environmental issue in the country.[85] Cambodia's primary forest cover fell from over 70% in 1969 to just 3.1% in 2007. In total, Cambodia lost 25,000 km2 (9,700 sq mi) of forest between 1990 and 2005 – 3,340 km2 (1,290 sq mi) of which was primary forest. Since 2007, less than 3,220 km2 (1,243 sq mi) of primary forest remain with the result that the future sustainability of the forest reserves of Cambodia is under severe threat.[86][87] In 2010–2015, the annual rate of deforestation was 1.3%. The environmental degradation also includes national parks and wildlife sanctuaries on a large scale and many endangered and endemic species are now threatened with extinction due to loss of habitats. There are many reasons for the deforestation in Cambodia, which range from opportunistic illegal loggings to large scale clearings from big construction projects and agricultural activities. The global issue of land grabbing is particularly rampant in Cambodia. The deforestation involves the local population, Cambodian businesses and authorities as well as transnational corporations from all over the world.[88][89]		Plans for hydroelectric development in the region, by Laos in particular, pose a "real danger to the food supply of Vietnam and Cambodia. Upstream dams will imperil the fish stocks that provide the vast majority of Cambodia's protein and could also denude the Mekong River of the silt Vietnam needs for its rice basket." The rich fisheries of Tonle Sap, the largest freshwater lake in Southeast Asia, largely supply the impoverished country's protein. The lake is unusual: It all but disappears in the dry season and then expands massively as water flow from the Mekong backs up when the rains come. "Those fish are so important for their livelihoods, both economically and nutritionally," said Gordon Holtgrieve, a professor at the University of Washington who researches Cambodia's freshwater fish and he points out that none of the dams that are either built or being built on the Mekong river "are pointing at good outcomes for the fisheries."[90]		In the 2010s, the Cambodian government and educational system has increased its involvement and co-operation with both national and international environmental groups.[91][92][93] A new National Environmental Strategy and Action Plan (NESAP) for Cambodia is to be implemented from late 2016 to 2023 and contains new ideas for how to incite a green and environmentally sustainable growth for the country.[94]		Officially a multiparty democracy, in reality "the country remains a one-party state dominated by the Cambodian People's Party and Prime Minister Hun Sen, a recast Khmer Rouge official in power since 1985. The open doors to new investment during his reign have yielded the most access to a coterie of cronies of his and his wife, Bun Rany."[95] Cambodia's government has been described by the Human Rights Watch’s Southeast Asian director, David Roberts, as a "vaguely communist free-market state with a relatively authoritarian coalition ruling over a superficial democracy."[16]		Prime Minister Hun Sen has vowed to rule until he is 74.[96][97] He is a former Khmer Rouge member who defected. His government is regularly accused of ignoring human rights and suppressing political dissent. The 2013 election results were disputed by Hun Sen's opposition, leading to demonstrations in the capital. Demonstrators were injured and killed in Phnom Penh where a reported 20,000 protesters gathered, with some clashing with riot police.[98] From a humble farming background, Hun Sen was just 33 when he took power in 1985, and is by some considered a long ruling dictator.[99]		National politics in Cambodia take place within the framework of the nation's constitution of 1993. The government is a constitutional monarchy operated as a parliamentary representative democracy. The Prime Minister of Cambodia, an office held by Hun Sen since 1985, is the head of government, while the King of Cambodia (currently Norodom Sihamoni) is the head of state. The prime minister is appointed by the king, on the advice and with the approval of the National Assembly. The prime minister and the ministerial appointees exercise executive power.		Legislative powers are shared by the executive and the bicameral Parliament of Cambodia, which consists of a lower house, the National Assembly (រដ្ឋសភាកម្ពុជា, rotsaphea) and an upper house, the Senate (ព្រឹទ្ធសភានៃព្រះរាជាណាចក្រកម្ពុជា, preutsaphea). Members of the 123-seat Assembly are elected through a system of proportional representation and serve for a maximum term of five years. The Senate has 61 seats, two of which are appointed by the king and two others by the National Assembly, and the rest elected by the commune councillors from 24 provinces of Cambodia. Senators serve six-year terms.[100]		On 14 October 2004, King Norodom Sihamoni was selected by a special nine-member Throne Council, part of a selection process that was quickly put in place after the abdication of King Norodom Sihanouk a week prior. Sihamoni's selection was endorsed by Prime Minister Hun Sen and National Assembly Speaker Prince Norodom Ranariddh (the king's half-brother and current chief advisor), both members of the throne council. He was enthroned in Phnom Penh on 29 October 2004.		The Cambodian People's Party (CPP) is the major ruling party in Cambodia. The CPP controls the lower and upper chambers of parliament, with 68 seats in the National Assembly and 46 seats in the Senate. The opposition Cambodia National Rescue Party (CNRP) is the second largest party in Cambodia with 55 seats in the National Assembly but has yet to compete in any Senate elections. The Sam Rainsy Party (SRP) has 11 seats in the Senate.		Hun Sen and his government have seen much controversy. Hun Sen was a former Khmer Rouge commander who was originally installed by the Vietnamese and, after the Vietnamese left the country, maintains his strong man position by violence and oppression when deemed necessary.[101] In 1997, fearing the growing power of his co–prime minister, Prince Norodom Ranariddh, Hun launched a coup, using the army to purge Ranariddh and his supporters. Ranariddh was ousted and fled to Paris while other opponents of Hun Sen were arrested, tortured, and some summarily executed.[101][102]		In addition to political oppression, the Cambodian government has been accused of corruption in the sale of vast areas of land to foreign investors resulting in the eviction of thousands of villagers[103] as well as taking bribes in exchange for grants to exploit Cambodia's oil wealth and mineral resources.[104] Cambodia is consistently listed as one of the most corrupt governments in the world.[105][106][107] Amnesty International currently recognises one prisoner of conscience in the country: 33-year-old land rights activist Yorm Bopha.[108]		Journalists covering a protest over disputed election results in Phnom Penh on 22 September 2013 say they were deliberately attacked by police and men in plain clothes, with slingshots and stun guns. The attack against the president of the Overseas Press Club of Cambodia, Rick Valenzuela, was captured on video. The violence came amid political tensions as the opposition boycotted the opening of Parliament due to concerns about electoral fraud. Seven reporters sustained minor injuries while at least two Cambodian protesters were hit by slingshot projectiles and hospitalized.[109]		The level of corruption in Cambodia exceeds most countries in the world. Despite adopting an 'Anti-Corruption Law' in 2010, corruption prevails throughout the country. Corruption affects the judiciary, the police and other state institutions. Favouritism by government officials and impunity is commonplace. Lack of a clear distinction between the courts and the executive branch of government also makes for a deep politicisation of the judicial system.[110]		Examples of areas where Cambodians encounter corrupt practices in their everyday lives include obtaining medical services, dealing with alleged traffic violations, and pursuing fair court verdicts. Companies deal with extensive red tape when obtaining licenses and permits, especially construction related permits, and the demand for and supply of bribes are commonplace in this process. The 2010 Anti-Corruption Law provided no protection to whistle-blowers, and whistle-blowers can be jailed for up to 6 months if they report corruption that cannot be proven.[110]		The foreign relations of Cambodia are handled by the Ministry of Foreign Affairs under Prak Sokhon. Cambodia is a member of the United Nations, the World Bank, and the International Monetary Fund. It is a member of the Asian Development Bank (ADB), ASEAN, and joined the WTO in 2004. In 2005 Cambodia attended the inaugural East Asia Summit in Malaysia.		Cambodia has established diplomatic relations with numerous countries; the government reports twenty embassies in the country[111] including many of its Asian neighbours and those of important players during the Paris peace negotiations, including the US, Australia, Canada, China, the European Union (EU), Japan, and Russia.[112] As a result of its international relations, various charitable organisations have assisted with social, economic, and civil infrastructure needs.		While the violent ruptures of the 1970s and 1980s have passed, several border disputes between Cambodia and its neighbours persist. There are disagreements over some offshore islands and sections of the boundary with Vietnam and undefined maritime boundaries. Cambodian and Thailand also has border disputes, with troops clashing over land immediately adjacent to the Preah Vihear temple in particular, leading to a deterioration in relations. Most of the territory belongs to Cambodia, but a combination of Thailand disrespecting international law, Thai troop upbuild in the area and lack of resources for the Cambodian military have left the situation unsettled since 1962.[113][114]		Cambodia and China have cultivated ties in the 2010s. A Chinese company with the support of the People's Liberation Army built a deep-water seaport along 90 km stretch of Cambodian coastline of the Gulf of Thailand in Koh Kong province; the port is sufficiently deep to be used by cruise ships, bulk carriers or warships. Cambodia's diplomatic support has been invaluable to Beijing's effort to claim disputed areas in the South China Sea. Because Cambodia is a member of ASEAN, and because under ASEAN rules "the objections of one member can thwart any group initiative," Cambodia is diplomatically useful to China as a counterweight to southeast Asian nations that have closer ties to the United States.[115]		The Royal Cambodian Army, Royal Cambodian Navy, Royal Cambodian Air Force and Royal Gendarmerie collectively form the Royal Cambodian Armed Forces, under the command of the Ministry of National Defence, presided over by the Prime Minister of Cambodia. His Majesty King Norodom Sihamoni is the Supreme Commander of the Royal Cambodian Armed Forces (RCAF), and the country's Prime Minister Hun Sen effectively holds the position of commander-in-chief.		The introduction of a revised command structure early in 2000 was a key prelude to the reorganisation of the Cambodian military. This saw the defence ministry form three subordinate general departments responsible for logistics and finance, materials and technical services, and defence services under the High Command Headquarters (HCHQ).		The minister of National Defense is General Tea Banh. Banh has served as defence minister since 1979. The Secretaries of State for Defense are Chay Saing Yun and Por Bun Sreu.		In 2010, the Royal Cambodian Armed Forces comprised about 102,000 active personnel (200,000 reserve). Total Cambodian military spending stands at 3% of national GDP. The Royal Gendarmerie of Cambodia total more than 7,000 personnel. Its civil duties include providing security and public peace, to investigate and prevent organised crime, terrorism and other violent groups; to protect state and private property; to help and assist civilians and other emergency forces in a case of emergency, natural disaster, civil unrest and armed conflicts.		Hun Sen has accumulated highly centralised power in Cambodia, including a praetorian guard that 'appears to rival the capabilities of the country's regular military units', and is allegedly used by Hun Sen to quell political opposition.'[116]		A US State Department report says "forces under Hun Sen and the Cambodian People's Party have committed frequent and large-scale abuses, including extrajudicial killings and torture, with impunity".[117] Events of 2013. Amnesty International and the Cambodian Center for Human Rights, located in Cambodia, also raised 'impunity' as a concern. "Impunity for perpetrators of human rights abuses and lack of an independent judiciary remained serious problems," Amnesty's 2012 Annual Report said. Since June, NGOs reported that authorities "abused at least 30 prisoners – 29 while in police custody and one in prison. Kicking, punching and pistol whipping were the most common methods of reported physical abuse, but electric shock, suffocation, caning and whipping with wires were also used." The US State Department report says "politicized and ineffective judiciary is one of the country's key human rights abuses." That report says "the government generally does not respect judicial independence, and that there has been widespread corruption among judges, prosecutors and court officials."[118]		Forced land evictions by senior officials, security forces, and government-connected business leaders are commonplace in Cambodia. Land has been confiscated from hundreds of thousands of Cambodians over more than a decade for the purpose of self-enrichment and maintaining power of various groups of special interests. Credible non-governmental organisations estimate that "770,000 people have been adversely affected by land grabbing covering at least four million hectares (nearly 10 million acres) of land that have been confiscated," says Paris-based International Federation for Human Rights (FIDH).[119]		Across Cambodia, authorities routinely detain alleged drug users, homeless people, "street" children, sex workers, and people perceived to have disabilities in a "haphazard system of detention centers around the country". Some of those detention centres are ostensibly for drug treatment, while others are ostensibly for "social rehabilitation". In addition to Prey Speu, the Ministry of Social Affairs also has authority for the Phnom Bak centre in Sisophon town, Banteay Meanchey province, and manages a drug detention centre with the military on a military base in Koh Kong town, Koh Kong province. There are "a further six drug detention centers" in Cambodia "that each year hold at least 2,000 people without due process".[120]		"Human Rights Watch documented how guards and staff at informal detention centers "whip detainees with rubber water hoses, beat them with bamboo sticks or palm fronds, shock them with electric batons, sexually abuse them, and punish them with physical exercises intended to cause intense physical pain." Informal detainees held in extra judicial centres have been forced to work on construction sites, including in at least one instance to help build a hotel.[120]		There are documented cases in Cambodia of people committing murder and then paying state officials so as not to be prosecuted. "Impunity enjoyed by the rich and powerful helps explain a lack of public trust in Cambodia's judicial and law enforcement institutions."[121]		The capital (reach thani) and provinces (khaet) of Cambodia are first-level administrative divisions. Cambodia is divided into 25 provinces including the capital.		Municipalities and districts are the second-level administrative divisions of Cambodia. The provinces are subdivided into 159 districts and 26 municipalities. The districts and municipalities in turn are further divided into communes (khum) and quarters (sangkat).		In 2016 Cambodia's per capita income is $3,735 in PPP and $1,227 in nominal per capita. Cambodia graduated from the status of a Least Developed Country to a Lower Middle Income country in the same year 2016. Most rural households depend on agriculture and its related sub-sectors. Rice, fish, timber, garments and rubber are Cambodia's major exports. The International Rice Research Institute (IRRI) reintroduced more than 750 traditional rice varieties to Cambodia from its rice seed bank in the Philippines.[122] These varieties had been collected in the 1960s.		Based on the Economist, IMF: Annual average GDP growth for the period 2001–2010 was 7.7% making it one of the world's top ten countries with the highest annual average GDP growth. Tourism was Cambodia's fastest growing industry, with arrivals increasing from 219,000 in 1997 to over 2 million in 2007. In 2004, inflation was at 1.7% and exports at $1.6 billion US$.		In the Cambodia country assessment "Where Have All The Poor Gone? Cambodia Poverty Assessment 2013", the World Bank concludes: "Over the seven years from 2004 through 2011, Cambodian economic growth was tremendous, ranking amid the best in the world. Moreover, household consumption increased by nearly 40 percent. And this growth was pro-poor—not only reducing inequality, but also proportionally boosting poor people's consumption further and faster than that of the non-poor. As a result, the poverty rate dropped from 52.2 to 20.5 percent, surpassing all expectations and far exceeding the country's Millennium Development Goals (MDGs) poverty target. However, the majority of these people escaped poverty only slightly: they remain highly vulnerable—even to small shocks—which could quickly bring them back into poverty.".[123]		"Two decades of economic growth have helped make Cambodia a global leader in reducing poverty. The success story means the Southeast Asian nation that overcame a vicious civil war now is classified as a lower-middle income economy by the World Bank Group (WBG). Among 69 countries that have comparable data, Cambodia ranked fourth in terms of the fastest poverty reduction in the world from 2004–2008. (See more details of Cambodia's achievements on poverty reduction. The poverty rate fell to 10 percent in 2013, and further reduction of poverty is expected for both urban and rural households throughout 2015–2016. However, human development, particularly in the areas of health and education, remains an important challenge and development priority for Cambodia" [124]		Oil and natural gas deposits found beneath Cambodia's territorial waters in 2005 yield great potential but remain mostly untapped, due in part to territorial disputes with Thailand.[125][126]		The National Bank of Cambodia is the central bank of the kingdom and provides regulatory oversight to the country's banking sector and is responsible in part for increasing the foreign direct investment in the country. Between 2010 and 2012 the number of regulated banks and micro-finance institutions increased from 31 covered entities to over 70 individual institutions underlining the growth within the Cambodian banking and finance sector.		In 2012, Credit Bureau Cambodia was established with direct regulatory oversight by the National Bank of Cambodia.[127] The Credit Bureau further increases the transparency and stability within the Cambodian Banking Sector as all banks and microfinance companies are now required by law to report accurate facts and figures relating to loan performance in the country.		One of the largest challenges facing Cambodia is still the fact that the older population often lacks education, particularly in the countryside, which suffers from a lack of basic infrastructure. Fear of renewed political instability and corruption within the government discourage foreign investment and delay foreign aid, although there has been significant aid from bilateral and multilateral donors. Donors pledged $504 million to the country in 2004,[1] while the Asian Development Bank alone has provided $850 million in loans, grants, and technical assistance.[128] Bribes are often demanded from companies operating in Cambodia when obtaining licences and permits, such as construction-related permits.[129]		Cambodia ranked among the worst places in the world for organised labour in the 2015 International Trade Union Confederation (ITUC) Global Rights Index, landing in the category of countries with "no guarantee of rights".'[130]		In April 2016 Cambodia's National Assembly has adopted a Law on Trade Unions. "The law was proposed at a time when workers have been staging sustained protests in factories and in the streets demanding wage increases and improvements in their working conditions".[131] The concerns about Cambodia's new law are shared not only by labour and rights groups, but international organisations more generally. The International Labor Organization Country Office for Thailand, Cambodia and Lao PDR, has noted that the law has "several key concerns and gaps".[132] Independent unions and employers remain as divided as ever. "How can a factory with 25 unions survive?" asked Van Sou Ieng, chairman of the Garment Manufacturers Association in Cambodia (GMAC), adding that it was incomprehensible to expect an employer to negotiate a dispute with 25 different union leaders. A law was necessary to rein in the country's unions, Van Sou Ieng said. According to GMAC, last year there were 3,166 unions for the more than 500,000 workers employed in the country's 557 garment and textile exporting factories, and 58 footwear factories. Though garment production is already Cambodia's largest industry, which accounts for 26.2 percent of the country's Gross Domestic Product, Van Sou Ieng said without the trade union law, foreign investors will not come to do business".[133]		"Only with the trade union law will we, employers, be able to survive…. not only Cambodia, every country has trade union law. Those who criticize [the law] should do businesses, and [then] they will understand."		The garment industry represents the largest portion of Cambodia's manufacturing sector, accounting for 80% of the country's exports. In 2012, the exports grew to $4.61 billion up 8% over 2011. In the first half of 2013, the garment industry reported exports worth $1.56 billion.[134] The sector employs 335,400 workers, of which 91% are female.		The tourism industry is the country's second-greatest source of hard currency after the textile industry.[72] Between January and December 2007, visitor arrivals were 2.0 million, an increase of 18.5% over the same period in 2006. Most visitors (51%) arrived through Siem Reap with the remainder (49%) through Phnom Penh and other destinations.[135]		Other tourist destinations include Sihanoukville in the south west which has several popular beaches and the sleepy riverside town of Battambang in the north west, both of which are a popular stop for backpackers who make up a large of portion of visitors to Cambodia.[136] The area around Kampot and Kep including the Bokor Hill Station are also of interest to visitors. Tourism has increased steadily each year in the relatively stable period since the 1993 UNTAC elections; in 1993 there were 118,183 international tourists, and in 2009 there were 2,161,577 international tourists.[137]		Most of the tourists were Japanese, Chinese, Filipinos, Americans, South Koreans and French, said the report, adding that the industry earned some 1.4 billion US dollars in 2007, accounting for almost ten percent of the kingdom's gross national product. Chinese-language newspaper Jianhua Daily quoted industry officials as saying that Cambodia will have three million foreign tourist arrivals in 2010 and five million in 2015. Tourism has been one of Cambodia's triple pillar industries. The Angkor Wat historical park in Siem Reap province, the beaches in Sihanoukville and the capital city Phnom Penh are the main attractions for foreign tourists.[138]		Cambodia's reputation as a safe destination for tourism however has been hindered by civil and political unrest [139][140][141] and multiple high profile examples of serious crime perpetrated against tourists visiting the Kingdom.[142][143][144]		Cambodia's tourist souvenir industry employs a lot of people around the main places of interest. Obviously, the quantity of souvenirs that are produced is not sufficient to face the increasing number of tourists and a majority of products sold to the tourists on the markets are imported from China, Thailand and Vietnam.[145] Some of the locally produced souvenirs include:		Agriculture is the traditional mainstay of the Cambodian economy. Agriculture accounted for 90 percent of GDP in 1985 and employed approximately 80 percent of the work force. Rice is the principal commodity.		Major secondary crops include maize, cassava, sweet potatoes, groundnuts, soybeans, sesame seeds, dry beans, and rubber. The principal commercial crop is rubber. In the 1980s it was an important primary commodity, second only to rice, and one of the country's few sources of foreign exchange.		The civil war and neglect severely damaged Cambodia's transport system. With assistance from other countries Cambodia has been upgrading the main highways to international standards and most are vastly improved from 2006. Most main roads are now paved.		Cambodia has two rail lines, totalling about 612 kilometres (380 miles) of single, one-metre (3-foot-3-inch) gauge track.[148] The lines run from the capital to Sihanoukville on the southern coast. Trains are again running to and from the Cambodian capital and popular destinations in the south. After 14 years, regular rail services between the two cities restarted in May – offering a safer option than road for travelers aiming for some beach time.[149] Trains also run from Phnom Penh to Sisophon (although trains often run only as far as Battambang). As of 1987, only one passenger train per week operated between Phnom Penh and Battambang but a $141 million project, funded mostly by the Asian Development Bank, has been started to revitalise the languishing rail system that will "(interlink) Cambodia with major industrial and logistics centers in Bangkok and Ho Chi Minh City".[148]		Besides the main interprovincial traffic artery connecting Phnom Penh with Sihanoukville, resurfacing a former dirt road with concrete / asphalt and implementation of 5 major river crossings by means of bridges have now permanently connected Phnom Penh with Koh Kong, and hence there is now uninterrupted road access to neighbouring Thailand and their vast road system.		Cambodia's road traffic accident rate is high by world standards. In 2004, the number of road fatalities per 10,000 vehicles was ten times higher in Cambodia than in the developed world, and the number of road deaths had doubled in the preceding three years.[150]		Cambodia's extensive inland waterways were important historically in international trade. The Mekong and the Tonle Sap River, their numerous tributaries, and the Tonle Sap provided avenues of considerable length, including 3,700 kilometres (2,300 miles) navigable all year by craft drawing 0.6 metres (2.0 feet) and another 282 kilometres (175 miles) navigable to craft drawing 1.8 metres (5.9 feet).[151]		Cambodia has two major ports, Phnom Penh and Sihanoukville, and five minor ones. Phnom Penh, located at the junction of the Bassac, the Mekong, and the Tonle Sap rivers, is the only river port capable of receiving 8,000-ton ships during the wet season and 5,000-ton ships during the dry season. With increasing economic activity has come an increase in automobile and motorcycle use, though bicycles still predominate.[152] "Cyclo" (as hand-me-down French) or Cycle rickshaws are an additional option often used by visitors. These kind of rickshaws are unique to Cambodia in that the cyclist is situated behind the passenger(s) seat,[153] as opposed to Cycle rickshaws in neighbouring countries where the cyclist is at the front and "pulls" the carriage.		Cambodia has three commercial airports. Phnom Penh International Airport (Pochentong) in Phnom Penh is the second largest in Cambodia. Siem Reap-Angkor International Airport is the largest and serves the most international flights in and out of Cambodia. The other airport is in Sihanoukville.		The level of access to water supply in rural areas is low (66% in 2012) compared to relatively high access to an Improved water source in urban areas (94%).[154] Within the government, urban water supply policy is the responsibility of the Ministry of Industry, Mines and Energy. Service provision in urban areas is the responsibility of two water utilities in the largest cities, the Phnom Penh Water Supply Authority (PPWSA) and the Siem Reap Water Supply Authority (SRWSA), 11 Provincial Water Supply Authorities (known as PWWKs) as well as 147 smaller utilities. The Department of Rural Water Supply (DRWS) and Department of Rural Health Care (DRHC) of the Ministry of Rural Development are responsible for rural water supply for the smaller towns and villages with less than 1,000 households.[155]		As of 2013[update], Cambodia has an estimated population of 15,205,539 people. Cambodia's birth rate is 25.4 per 1,000. Its population growth rate is 1.7%.[156]		Fifty percent of the Cambodian population is younger than 22 years old. At a 1.04 female to male ratio, Cambodia has the most female-biased sex ratio in the Greater Mekong Subregion.[157] Among the Cambodian population aged over 65, the female to male ratio is 1.6:1.[1]		The total fertility rate in Cambodia was 3.0 children per woman in 2010.[158] The fertility rate was 4.0 children in 2000.[158] Women in urban areas have 2.2 children on average, compared with 3.3 children per woman in rural areas.[158] Fertility is highest in Mondol Kiri and Rattanak Kiri Provinces, where women have an average of 4.5 children, and lowest in Phnom Penh where women have an average of 2.0 children.[158]		Ninety percent of Cambodia's population is of Khmer origin and speak the Khmer language, the country's official language. Cambodia's population is relatively homogeneous. Its minority groups include Chams (1.2%), Vietnamese (0.1%) and Chinese (0.1%).[1]		The largest ethnic group in Cambodia are the Khmers, who comprise around 90% of the total population in Cambodia, and are indigenous to the lowland Mekong subregion in which they inhabit. The Khmers historically have lived near the lower Mekong River in a contiguous diagonal arc, from where modern-day Thailand, Laos and Cambodia meet in the northwest, all the way to the mouth of the Mekong River in southeastern Vietnam.		The Vietnamese are the second largest ethnic minority in Cambodia, with an estimated 16,000 living in provinces concentrated in the southeast of the country adjacent to the Mekong Delta. Although the Vietnamese language has been determined to be a Mon–Khmer language, there are very few cultural connections between the two peoples because the early Khmers were influenced by the Indian cultural sphere while the Vietnamese are part of the Chinese cultural sphere.[159] Ethnic tensions between the Khmer and the Vietnamese can be traced to the Dark Ages of Cambodia (from the 16th to 19th centuries), during which time a nascent Vietnam and Thailand each attempted to vassalise a weakened post-Angkor Cambodia, and effectively dominate all of Indochina.[159]		Chinese Cambodians are approximately 0.1% of the population.[156][160] Most Chinese are descended from 19th–20th century settlers who came in search of trade and commerce opportunities during the time of the French protectorate. Most are urban dwellers, engaged primarily in commerce.		The indigenous ethnic groups of the mountains are known collectively as Montagnards or Khmer Loeu, a term meaning "Highland Khmer". They are descended from neolithic migrations of Mon–Khmer speakers via southern China and Austronesian speakers from insular Southeast Asia. Being isolated in the highlands, the various Khmer Loeu groups were not Indianized like their Khmer cousins and consequently are culturally distant from modern Khmers and often from each other, observing many pre-Indian-contact customs and beliefs.		The Cham are descended from the Austronesian people of Champa, a former kingdom on the coast of central and southern present-day Vietnam and former rival to the Khmer Empire. The Cham in Cambodia number under a million and often maintain separate villages in the southeast of the country. Almost all Cham in Cambodia are Muslims.				The Khmer language is a member of the Mon–Khmer subfamily of the Austroasiatic language group. French, once the language of government in Indochina, is still spoken by many older Cambodians, and is also the language of instruction in some schools and universities that are funded by the government of France. There is also a French-language newspaper and some TV channels are available in French. Cambodia is a member of La Francophonie. Cambodian French, a remnant of the country's colonial past, is a dialect found in Cambodia and is sometimes used in government, particularly in court. However, since 1993, there has been a growing use of English, that has been replacing French as the main foreign language. English is widely taught in several universities and there is also a significant press in that language, while street signs are now bilingual in Khmer and English.[162] Due to this shift, English is now mostly used in Cambodia's international relationships and has replaced French both in Cambodia's stamps, since 2002, and currency.[163]		Khmer script is derived from the South Indian Pallava script.		Religion in Cambodia (2010)[164]		Theravada Buddhism is the official religion of Cambodia, practised by more than 95 percent of the population with an estimated 4,392 monastery temples throughout the country.[165] Cambodian Buddhism is deeply pervaded by Hinduism, Tantrism, and native animism. Key concepts in Cambodian Buddhism include reincarnation, and religious activities are focused on acquiring bonn (Pali punna, merit), and erasing kamm (Pali kamma, karma), which, for Khmers, means the negative results accrued from past actions.		Key concepts deriving from animism include the close interrelationship between spirits and the community, the efficacy of apotropaic and luck-attracting actions and charms, and the possibility of manipulating one's life through contact with spiritual entities such as the "baromey" spirits. Hinduism has left little trace beyond the magical practices of Tantricism and a host of Hindu gods now assimilated into the spirit world (for example, the important neak ta spirit called Yeay Mao is the modern avatar of the Hindu goddess Kali).		Mahayana Buddhism is the religion of the majority of Chinese and Vietnamese in Cambodia. Elements of other religious practices, such as the veneration of folk heroes and ancestors, Confucianism, and Taoism mix with Chinese Buddhism are also practised.		Islam is followed by about 2% of the population and comes in three varieties, two practised by the Cham people and a third by the descendants of Malays resident in the country for generations. Cambodia's Muslim population is reported to be 80% ethnic Cham.[166]		Cambodian life expectancy was 72 years in 2014,[167] a major improvement since 1999 when the average life expectancy was 49.8 and 46.8[clarification needed][citation needed]. Health care is offered by both public and private practitioners and research has found that trust in health providers is a key factor in improving the uptake of health care services in rural Cambodia.[168] The government plans to increase the quality of healthcare in the country by raising awareness of HIV/AIDS, malaria, and other diseases.		Cambodia's infant mortality rate has decreased from 115 per 1,000 live births in 1993 to 54 in 2009. In the same period, the under-five mortality rate decreased from 181 to 115 per 1,000 live births.[169] In the province with worst health indicators, Ratanakiri, 22.9% of children die before age five.[170]		Cambodia was once one of the most landmined countries in the world. According to some estimates, unexploded land mines have been responsible for over 60,000 civilian deaths and thousands more maimed or injured since 1970.[171] The number of reported landmine casualties has sharply decreased, from 800 in 2005 to 111 in 2013 (22 dead and 89 injured).[172] Adults that survive landmines often require amputation of one or more limbs and have to resort to begging for survival.[171] Cambodia is expected to be free of land mines by 2020[173] but the social and economic legacy, including orphans and one in 290 people being an amputee,[174] is expected to affect Cambodia for years to come.		"In Cambodia, landmines and exploded ordnance alone have caused 44,630 injuries between 1979 and 2013, according to the Cambodia Mine/UXO Victim Information System"[175]		The Ministry of Education, Youth and Sports is responsible for establishing national policies and guidelines for education in Cambodia. The Cambodian education system is heavily decentralised, with three levels of government, central, provincial and district – responsible for its management. The constitution of Cambodia promulgates free compulsory education for nine years, guaranteeing the universal right to basic quality education.		The 2008 Cambodian census estimated that 77.6% of the population was literate (85.1% of men and 70.9% of women).[1] Male youth age (15–24 years) have a literacy rate of 89% compared to 86% for females.[176]		The education system in Cambodia continues to face many challenges, but during the past years there have been significant improvements, especially in terms of primary net enrolment gains, the introduction of program based-budgeting, and the development of a policy framework which helps disadvantaged children to gain access to education. The country has also significantly invested in vocational education, especially in rural areas, to tackle poverty and unemployment. [177][178] Two of Cambodia's most acclaimed universities are based in Phnom Penh.		Traditionally, education in Cambodia was offered by the wats (Buddhist temples), thus providing education exclusively for the male population.[179] During the Khmer Rouge regime, education suffered significant setbacks.		With respects to academic performance among Cambodian primary school children, research showed that parental attitudes and beliefs played a significant role.[180] Specifically, the study found that poorer academic achievement among children were associated with parents holding stronger fatalistic beliefs (i.e., human strength cannot change destiny). The study further found that "length of residence" of parents in the community in which they stay predicted better academic achievement among their children. Overall, the study pointed out to the role of social capital in educational performance and access in the Cambodian society in which family attitudes and beliefs are central to the findings.		In 2012, Cambodia had a murder rate of 6.5 per 100,000 population.[181] There were a total of 964 murders in Cambodia in 2012.[181]		Prostitution is against the law in Cambodia, yet is still prevalent. In a series of 1993 interviews of women about prostitution, three quarters of the interviewees found being a prostitute to be a norm and a profession they felt was not shameful having.[182] That same year, it was estimated that there were one hundred thousand sex workers in Cambodia.[182]		Various factors contribute to the Cambodian culture including Theravada Buddhism, Hinduism, French colonialism, Angkorian culture, and modern globalisation. The Cambodian Ministry of Culture and Fine Arts is responsible for promoting and developing Cambodian culture. Cambodian culture not only includes the culture of the lowland ethnic majority, but also some 20 culturally distinct hill tribes colloquially known as the Khmer Loeu, a term coined by Norodom Sihanouk to encourage unity between the highlanders and lowlanders.		Rural Cambodians wear a krama scarf which is a unique aspect of Cambodian clothing. The sampeah is a traditional Cambodian greeting or a way of showing respect to others. Khmer culture, as developed and spread by the Khmer empire, has distinctive styles of dance, architecture and sculpture, which have been exchanged with neighbouring Laos and Thailand throughout history. Angkor Wat (Angkor means "city" and Wat "temple") is the best preserved example of Khmer architecture from the Angkorian era along with hundreds of other temples that have been discovered in and around the region.		Traditionally, the Khmer people have a recorded information on Tra leaves. Tra leaf books record legends of the Khmer people, the Ramayana, the origin of Buddhism and other prayer books. They are taken care of by wrapping in cloth to protect from moisture and the climate.[183]		Bon Om Tuuk (Festival of Boat Racing), the annual boat rowing contest, is the most attended Cambodian national festival. Held at the end of the rainy season when the Mekong river begins to sink back to its normal levels allowing the Tonle Sap River to reverse flow, approximately 10% of Cambodia's population attends this event each year to play games, give thanks to the moon, watch fireworks, dine, and attend the boat race in a carnival-type atmosphere.[184]		Popular games include cockfighting, soccer, kicking a sey, which is similar to a footbag, and chess. Based on the classical Indian solar calendar and Theravada Buddhism, the Cambodian New Year is a major holiday that takes place in April. Recent artistic figures include singers Sinn Sisamouth and Ros Serey Sothea (and later Meng Keo Pichenda), who introduced new musical styles to the country.		Rice is the staple grain, as in other Southeast Asian countries. Fish from the Mekong and Tonle Sap rivers is also an important part of the diet. The supply of fish and fish products for food and trade as of 2000[update] was 20 kilograms (44 pounds) per person or 2 ounces per day per person.[185] Some of the fish can be made into prahok for longer storage.		The cuisine of Cambodia contains tropical fruits, soups and noodles. Key ingredients are kaffir lime, lemon grass, garlic, fish sauce, soy sauce, curry, tamarind, ginger, oyster sauce, coconut milk and black pepper. Some delicacies are នំបញ្ចុក (Num Banh chok), អាម៉ុក (Amok), អាពីង (Ah Ping). The country also boasts various distinct local street foods, such as fried spiders.		French influence on Cambodian cuisine includes the Cambodian red curry with toasted baguette bread. The toasted baguette pieces are dipped in the curry and eaten. Cambodian red curry is also eaten with rice and rice vermicelli noodles. Probably the most popular dine out dish, kuy teav, is a pork broth rice noodle soup with fried garlic, scallions, green onions that may also contain various toppings such as beef balls, shrimp, pork liver or lettuce. Kampot pepper is reputed to be the best in the world and accompanies crab at the Kep crab shacks and squid in the restaurants on the Ou Trojak Jet river.[186] The cuisine is relatively unknown to the world compared to that of its neighbours Thailand and Vietnam.		Cambodians drink plenty of tea, grown in Mondulkiri Province and around Kirirom.[187] tai krolap is a strong tea, made by putting water and a mass of tea leaves into a small glass, placing a saucer on top, and turning the whole thing upside down to brew. When it’s dark enough, the tea is decanted into another cup and plenty of sugar added, but no milk. Lemon tea tai kdao kroich chhmaa, made with Chinese red-dust tea and lemon juice, is refreshing both hot and iced, and is generally served with a hefty dose of sugar.[188]		Regarding coffee, the beans are generally imported from Laos and Vietnam – although domestically produced coffee from Ratanakiri Province and Mondulkiri Province can be found in some places. Beans are traditionally roasted with butter and sugar, plus various other ingredients that might include anything from rum to pork fat, giving the beverage a strange, sometimes faintly chocolatey aroma.[189]		Cambodia has several industrial breweries, located mainly in Sihanoukville Province and Phnom Penh. There are also a growing number of microbreweries in Phnom Penh and Siem Reap.[190][191]		Rice wine is a popular alcoholic drink. Its quality varies widely and it is often infused with fruits or medicinal herbs.[192] When prepared with macerated fruits or spices, like the Sombai liqueur, it is called sraa tram (or soaked wine) and has gained more and more popularity with the development of tourism as it is smoother to drink than plain rice wine.[193][194][195]		Khmer women are traditionally supposed to be modest, soft-spoken, "light" walkers, well-mannered,[196] industrious,[197] belong to the household, act as the family's caregivers and caretakers[196] and financial controllers,[197] perform as the "preserver of the home", maintain their virginity until marriage, become faithful wives,[196] and act as advisors and servants to their husbands.[197] The "light" walking and refinement of Cambodian women is further described as being "quiet in […] movements that one cannot hear the sound of their silk skirt rustling".[197] As financial controllers, the women of Cambodia can be identified as having real household authority at the familial level.[198]		Football (soccer) is one of the most popular sports, although professional organised sports are not as prevalent in Cambodia as in western countries because of the economic conditions. Soccer was brought to Cambodia by the French and became popular with the locals.[199] The Cambodia national football team managed fourth in the 1972 Asian Cup, but development has slowed since the civil war.		Western sports such as basketball, volleyball, bodybuilding, field hockey, rugby union, golf, and baseball are gaining popularity. Volleyball is by far the most popular sport in the country. Native sports include traditional boat racing, buffalo racing, Pradal Serey, Khmer traditional wrestling and Bokator. Cambodia first participated in the Olympics during the 1956 Summer Olympic Games sending equestrian riders. Cambodia also hosted the GANEFO Games, the alternative to the Olympics, in the 1960s.		Cambodian dance can be divided into three main categories: Khmer classical dance, folk dance, and social dances. The exact origins of Khmer classical dance are disputed. Most native Khmer scholars trace modern dance forms back to the time of Angkor, seeing similarities in the temple engravings of the period, while others hold that modern Khmer dance styles were learned (or re-learned) from Siamese court dancers in the 1800s.		Khmer classical dance is the form of stylised performance art established in the royal courts of Cambodia exhibited for both entertainment and ceremonial purposes.[200] The dances are performed by intricately costumed, highly trained men and women on public occasions for tribute, invocation or to enact traditional stories and epic poems such as Reamker, the Khmer version of the Ramayana.[201] Known formally as Robam Preah Reach Trop (របាំព្រះរាជទ្រព្យ "theater of royal wealth") it is set to the music of a pinpeat ensemble accompanied by a vocal chorus.		Cambodian folk dance, often performed to mahori music, celebrates the various cultural and ethnic groups of Cambodia. Folk dances originated in the villages and are performed, for the most part, by the villagers for the villagers.[202] The movements are less stylised and the clothing worn is that of the people the dancers are portraying, such as hill tribes, Chams or farmers. Typically faster-paced than classical dance, folk dances display themes of the "common person" such as love, comedy or warding off evil spirits.[202]		Social dances are those performed by guests at banquets, parties or other informal social gatherings. Khmer traditional social dances are analogous to those of other Southeast Asian nations. Examples include the circle dances Romvong and Romkbach as well as Saravan and Lam Leav. Modern western popular dances including Cha-cha, Bolero, and the Madison, have also influenced Cambodian social dance.		Traditional Cambodian music dates back as far as the Khmer Empire.[203] Royal dances like the Apsara Dance are icons of the Cambodian culture as are the Mahori ensembles that accompany them. More rural forms of music include Chapei and A Yai. The former is popular among the older generation and is most often a solo performance of a man plucking a Cambodian guitar (chapei) in between a cappella verses. The lyrics usually have moral or religious theme.		A Yai can be performed solo or by a man and woman and is often comedic in nature. It is a form of lyrical poetry, often full of double entendres, that can be either scripted or completely impromptu and ad-libbed. When sung by a duo, the man and women take turns, "answering" the other's verse or posing riddles for the other to solve, with short instrumental breaks in between verses. Pleng kaah (lit. "wedding music") is a set of traditional music and songs played both for entertainment and as accompaniment for the various ceremonial parts of a traditional, days-long Khmer wedding.		Cambodian popular music is performed with western style instruments or a mixture of traditional and western instruments. Dance music is composed in particular styles for social dances. The music of crooner Sinn Sisamouth and Ros Sereysothea from the 1960s to the 1970s is considered to be the classic pop music of Cambodia. During the Khmer Rouge Revolution, many classic and popular singers of the 1960s and 1970s were murdered, starved to death, or overwork to death by the Khmer Rouge.[204] and many original master tapes from the period were lost or destroyed.		In the 1980s, Keo Surath, (a refugee resettled in the United States) and others carried on the legacy of the classic singers, often remaking their popular songs. The 1980s and 1990s also saw the rise in popularity of kantrum, a music style of the Khmer Surin set to modern instrumentation.[205]		The Australian hip hop group Astronomy Class has recorded with Kak Channthy, a native born Cambodian female singer.[206][207]		The Dengue Fever rock and roll band features a Cambodian female singer and back-up band from California. It is classified as "world music" and combines Cambodian music with Western style rock.		Civil Society		
A secondary school is both an organization that provides secondary education and the building where this takes place. Some secondary schools provide both lower secondary education and (upper) secondary education (levels 2 and 3 of the ISCED scale), but these are often provided in separate schools. When lower secondary education is provided in the same school as primary education or in a separate school, usually called a middle school, it is usually not called secondary education (except by some education experts) and is considered to be the second and final phase of basic education. Secondary schools typically follow on from primary schools and lead into vocational and tertiary education. Attendance is compulsory in most countries for students between the ages of 13 and 16. The organisations, buildings, and terminology are more or less unique in each country.[1][2]						Within the English speaking world, there are three widely used systems to describe the age of the child. The first is the 'equivalent ages', then countries that base their education systems on the 'British model' use one of two methods to identify the year group, while countries that base their systems on the 'American K-12 model' have refer to their year groups as 'grades'. This terminology extends into research literature. Below is a convenient comparison [3]		School building design does not happen in isolation. The building (or school plant US) needs to accommodate:		Each country will have a different education system and priorities. [4] Schools need to accommodate students, staff, storage, mechanical and electrical systems, storage, support staff, ancillary staff and administration. The number of rooms required can be determined from the predicted roll of the school and the area needed. A general classroom for 30 students needs to be 55 m², or more generously 62 m². A general art room for 30 students needs to be 83 m², but 104 m² for 3D textile work. A drama studio or a specialist science laboratory for 30 needs to be 90 m². Examples are given on how this can be configured for a 1,200 place secondary (practical specialism).,[5] and 1,850 place secondary school.[6]		The building providing the education has to fulfil the needs of: The students, the teachers, the non-teaching support staff, the administrators and the community. It has to meet general government building guidelines, health requirements, minimal functional requirements for classrooms, toilets and showers, electricity and services, preparation and storage of textbooks and basic teaching aids. [7] An optimum secondary school will meet the minimum conditions and will have :		Government accountants having read the advice then publish minimum guidelines on schools. These enable environmental modelling and establishing building costs. Future design plans are audited to ensure that these standards are met but not exceeded. Government ministries continue to press for the 'minimum' space and cost standards to be reduced.		The UK government published this downwardly revised space formula in 2014. It said the floor area should be 1050m² (+ 350m² if there is a sixth form) + 6.3m²/pupil place for 11- to 16-year-olds + 7m²/pupil place for post-16s. The external finishes were to be downgraded to meet a build cost of £1113/m². [8]		A secondary school, locally may be called high school or senior high school. In some countries there are two phases to secondary education (ISCED 2) and (ISCED 3), here the junior high school, intermediate school, lower secondary school or middle school occurs between the primary school(ISCED 1) and high school.		
Student-centered learning, also known as learner-centered education, broadly encompasses methods of teaching that shift the focus of instruction from the teacher to the student. In original usage, student-centered learning aims to develop learner autonomy and independence [1] by putting responsibility for the learning path in the hands of students.[2][3][4] Student-centered instruction focuses on skills and practices that enable lifelong learning and independent problem-solving.[5] Student-centered learning theory and practice are based on the constructivist learning theory that emphasizes the learner's critical role in constructing meaning from new information and prior experience.		Student-centered learning puts students' interests first, acknowledging student voice as central to the learning experience. In a student-centered learning space, students choose what they will learn, how they will learn, and how they will assess their own learning.[4] This is in contrast to traditional education, also dubbed "teacher-centered learning", which situates the teacher as the primarily "active" role while students take a more "passive", receptive role. In a teacher-centered classroom, teachers choose what the students will learn, how the students will learn, and how the students will be assessed on their learning. In contrast, student-centered learning requires students to be active, responsible participants in their own learning and with their own pace of learning.[6]		Usage of the term "student-centered learning" may also simply refer to educational mindsets or instructional methods that recognize individual differences in learners.[7] In this sense, student-centered learning emphasizes each student's interests, abilities, and learning styles, placing the teacher as a facilitator of learning for individuals rather than for the class as a whole.						Theorists like John Dewey, Jean Piaget and Lev Vygotsky, whose collective work focused on how students learn, have informed the move to student-centered learning. Carl Rogers' ideas about the formation of the individual also contributed to student-centered learning. Rogers wrote that "the only learning which significantly influences behavior [and education] is self discovered".[8] Maria Montessori was also a forerunner of student-centered learning, where preschool children learn through independent self-directed interaction with previously presented activities.		Self-determination theory focuses on the degree to which an individual’s behavior is self-motivated and 'self-determined'. When students are given the opportunity to gauge their learning, learning becomes an incentive.		Student-centered learning means inverting the traditional teacher-centered understanding of the learning process and putting students at the centre of the learning process. In the teacher-centered classroom, teachers are the primary source for knowledge. On the other hand, in student-centered classrooms, active learning is strongly encouraged. Armstrong (2012) claimed that "traditional education ignores or suppresses learner responsibility".[9]		A further distinction from a teacher-centered classroom to that of a student-centered classroom is when the teacher acts as a facilitator, as opposed to instructor. In essence, the teacher’s goal in the learning process is to guide students into making new interpretations of the learning material, thereby 'experiencing' content, reaffirming Rogers' notion that "significant learning is acquired through doing".[8]		Through peer-to-peer interaction, collaborative thinking can lead to an abundance of knowledge. In placing a teacher closer to a peer level, knowledge and learning is enhanced, benefitting the student and classroom overall. According to Lev Vygotsky's theory of the zone of proximal development (ZPD), students typically learn vicariously through one another. Scaffolding is important when fostering independent thinking skills. Vygotsky proclaims, "Learning which is oriented toward developmental levels that have already been reached is ineffective from the viewpoint of the child's overall development. It does not aim for a new stage of the developmental process but rather lags behind this process."[10]		One of the most critical differences between student-centered learning and teacher-centered learning is in assessment.[11] Student-centered learning typically involves more formative assessment and less summative assessment than teacher-centered learning.[12] In student-centered learning, students participate in the evaluation of their learning.[13] This means that students are involved in deciding how to demonstrate their learning. Developing assessment that supports learning and motivation is essential to the success of student-centered approaches.		Student-centered learning environments have been shown to be effective in higher education.[14] They have been defined specifically within higher education as both a mindset and a culture within a given educational institution and as a learning approach broadly related to, and supported by, constructivist theories of learning. They are characterised by innovative methods of teaching which aim to promote learning in communication with teachers and other learners and which take students seriously as active participants in their own learning and foster transferable skills such as problem-solving, critical thinking, and reflective thinking.[15] The revised European Standards and Guidelines for Quality Assurance, due to be approved by the ministers of European higher education in May 2015, include the following passage on student-centred learning: "Institutions should ensure that programmes are delivered in a way that encourages students to take an active role in creating the learning process and [should ensure] that the assessment of students reflects this approach."		A research university in Hong Kong sought to promote student-centered learning across the entire university by employing the following methods:[16]		The success of this initiative was evaluated by surveying the students. After two years, the mean ratings indicating the students' perception of the quality of the teaching and learning environment at the university all rose significantly.[17] The study is one of many examining the process of implementing student-centered pedagogies in large institutions of higher education.[18]		
A politician (from "politics" + "-ian", from the Greek title of Aristotle's book Πολιτικά "Politika", meaning "Civic Affairs") is a person active in party politics, or a person holding or seeking office in government. In democratic countries, politicians seek elective positions within a government through elections or, at times, temporary appointment to replace politicians who have died, resigned or have been otherwise removed from office. In non-democratic countries, they employ other means of reaching power through appointment, bribery, revolutions and intrigues. Some politicians are experienced in the art or science of government.[1] Politicians propose, support and create laws or policies that govern the land and, by extension, its people. Broadly speaking, a "politician" can be anyone who seeks to achieve political power in any bureaucratic institution.						Politicians are people who are politically active, especially in party politics. Positions range from local offices to executive, legislative, and judicial offices of regional and national governments.[2][3] Some elected law enforcement officers, such as sheriffs, are considered politicians.[4][5]		Politicians are known for their rhetoric, as in speeches or campaign advertisements. They are especially known for using common themes that allow them to develop their political positions in terms familiar to the voters.[6] Politicians of necessity become expert users of the media.[7] Politicians in the 19th century made heavy use of newspapers, magazines, and pamphlets, as well as posters.[8] In the 20th century, they branched into radio and television, making television commercials the single most expensive part of an election campaign.[9] In the 21st century, they have become increasingly involved with the social media based on the Internet and smart phones.[10]		Rumor has always played a major role in politics, with negative rumors about an opponent typically more effective than positive rumors about one's own side.[11]		Once elected, the politician becomes a government official and has to deal with a permanent bureaucracy of non-politicians. Historically, there has been a subtle conflict between the long-term goals of each side.[12] In patronage-based systems, such as the United States and Canada in the 19th century, winning politicians replace the bureaucracy with local politicians who formed their base of support, the "spoils system". Civil service reform was initiated to eliminate the corruption of government services that were involved.[13] However, in many less developed countries, the spoils system is in full-scale operation today.[14]		Mattozzi and Merlo argue that there are two main career paths which are typically followed by politicians in modern democracies. First come the career politicians. They are politicians who work in the political sector until retirement. Second are the "political careerists". These are politicians who gain reputation for expertise in controlling certain bureaucracies, then leave politics for a well-paid career in the private sector making use of their political contacts.[15]		Numerous scholars have studied the characteristics of politicians, comparing those at the local and national levels, and comparing the more liberal or the more conservative ones, and comparing the more successful and less successful in terms of elections.[16] In recent years, special attention has focused on the distinctive career path of women politicians.[17] For example, there are studies of the "Supermadre" model in Latin American politics.[18]		Many politicians have the knack to remember thousands of names and faces and recall personal anecdotes about their constituents—it is an advantage in the job, rather like being seven-foot tall for a basketball player. United States Presidents George W. Bush and Bill Clinton were renowned for their memories.[19][20]		Many critics attack politicians for being out of touch with the public. Areas of friction include the manner in which politicians speak, which has been described as being overly formal and filled with many euphemistic and metaphorical expressions and commonly perceived as an attempt to "obscure, mislead, and confuse".[21]		In the popular image, politicians are thought of as clueless, incompetent and corrupt, taking money in exchange for goods or services, rather than working for the general public good.[22]		Many ex-politicians who could not bear the leadership in politics that causes reprisals for critical thought criticize those who remain politicians for lacking critical thought.[23]		
Student activism is work by students to cause political, environmental, economic, or social change. Although often focused on schools, curriculum, and educational funding, student groups have influenced greater political events.[1]		Modern student activist movements vary widely in subject, size, and success, with all kinds of students in all kinds of educational settings participating, including public and private school students; elementary, middle, senior, undergraduate, and graduate students; and all races, socio-economic backgrounds, and political perspectives.[2] Some student protests focus on the internal affairs of a specific institution; others focus on broader issues such as a war or dictatorship. Likewise, some student protests focus on an institution's impact on the world, such as a disinvestment campaign, while others may focus on a regional or national policy's impact on the institution, such as a campaign against government education policy. Although student activism is commonly associated with left-wing politics, right-wing student movements are not uncommon; for example, large student movements fought on both sides of the apartheid struggle in South Africa.[3]		Student activism at the university level is nearly as old as the university itself. Students in Paris and Bologna staged collective actions as early as the 13th century, chiefly over town and gown issues.[4] Student protests over broader political issues also have a long pedigree. In Joseon Dynasty Korea, 150 Sungkyunkwan students staged an unprecedented remonstration against the king in 1519 over the Kimyo purge.[5]						In Argentina, as elsewhere in Latin America, the tradition of student activism dates back to at least the 19th century, but it was not until after 1900 that it became a major political force.[6] in 1918 student activism triggered a general modernization of the universities especially tending towards democratization, called the University Revolution (Spanish: revolución universitaria).[7] The events started in Córdoba and were accompanied by similar uprisings across Latin America.[6]		Australian Students have a long history of being active in political debates. This is particularly true in the newer universities that have been established in suburban areas.[8]		For much of the 20th century, the major campus organizing group across Australia was the Australian Union of Students, which was founded in 1937 as the Union of Australian University Students.[9] The AUS folded in 1984.[10] It was replaced by the National Union of Students in 1987.[11]		Student politics of Bangladesh is reactive, confrontational and violent. Student organizations act as the armament of the political parties they are part of. So every now and then there are affrays and commotions. Over the years, political clashes and factional feuds in the educational institutes killed many, seriously hampering academic atmosphere. To check those hitches, universities have no options but go to lengthy and unexpected closures. So classes are not completed on time and there are session jams.		The student wings of ruling parties dominate the campuses and residential halls through crime and violence to enjoy various unauthorized facilities. They control the residential halls to manage seats in favor of their party members and loyal pupils. They eat and buy for free from the restaurants and shops nearby. They extort and grab tenders to earn illicit money. They take money from the freshmen candidates and put pressures on teachers to get an acceptance for them. They take money from the job seekers and put pressures on university administrations to appoint them.[12]		In Canada, New Left student organizations from the late 1950s and 1960s became mainly two: SUPA (Student Union for Peace Action) and CYC (Company of Young Canadians). SUPA grew out of the CUCND (Combined Universities Campaign for Nuclear Disarmament) in December 1964, at a University of Saskatchewan conference.[13] While CUCND had focused on protest marches, SUPA sought to change Canadian society as a whole.[14] The scope expanded to grass-roots politics in disadvantaged communities and 'consciousness raising' to radicalize and raise awareness of the 'generation gap' experienced by Canadian youth. SUPA was a decentralized organization, rooted in local university campuses. SUPA however disintegrated in late 1967 over debates concerning the role of working class and 'Old Left'.[15] Members moved to the CYC or became active leaders in CUS (Canadian Union of Students), leading the CUS to assume the mantle of New Left student agitation.		In 1968, SDU (Students for a Democratic University) was formed at McGill and Simon Fraser Universities. SFU SDU, originally former SUPA members and New Democratic Youth, absorbed members from the campus Liberal Club and Young Socialists. SDU was prominent in an Administration occupation in 1968, and a student strike in 1969.[16] After the failure of the student strike, SDU broke up. Some members joined the IWW and Yippies (Youth International Party). Other members helped form the Vancouver Liberation Front in 1970. The FLQ (Quebec Liberation Front) was considered a terrorist organization, causing the use of the War Measures Act after 95 bombings in the October Crisis. This was the only peacetime use of the War Measures Act.[17]		Anti-Bullying Day (a.k.a. Pink Shirt Day) was created by high school students David Shepherd, and Travis Price of Berwick, Nova Scotia,[18] and is now celebrated annually across Canada.		In 2012, the Quebec Student Movement arose due to an increase of tuition of 75%; that took students out of class and into the streets because that increase did not allow students to comfortably extend their education, because of fear of debt or not having money at all. Following elections that year, premier Jean Charest promised to repeal anti-assembly laws and cancel the tuition hike.[19]		Since the 1970s, PIRGs (Public Interest Research Groups) have been created as a result of Student Union referendums across Canada in individual provinces. Like their American counterparts, Canadian PIRGs are student directed, run, and funded.[20] Most operate on a consensus decision making model. Despite efforts at collaboration, Canadian PIRGs are independent of each other.		From 2011 to 2013, Chile was rocked by a series of student-led nationwide protests across Chile, demanding a new framework for education in the country, including more direct state participation in secondary education and an end to the existence of profit in higher education. Currently in Chile, only 45% of high school students study in traditional public schools and most universities are also private. No new public universities have been built since the end of the Chilean transition to democracy in 1990, even though the number of university students has swelled. Beyond the specific demands regarding education, the protests reflected a "deep discontent" among some parts of society with Chile's high level of inequality.[21] Protests have included massive non-violent marches, but also a considerable amount of violence on the part of a side of protestors as well as riot police.		The first clear government response to the protests was a proposal for a new education fund[22] and a cabinet shuffle which replaced Minister of Education Joaquín Lavín[23] and was seen as not fundamentally addressing student movement concerns. Other government proposals were also rejected.		Since the defeat of the Qing Dynasty during the First (1839–1842) and Second Opium Wars (1856–1860), student activism has played a significant role in the modern Chinese history.[24] Fueled mostly by Chinese nationalism, Chinese student activism strongly believes that young people are responsible for China's future.[24] This strong nationalistic belief has been able to manifest in several forms such as Democracy, anti-Americanism and Communism.[24]		One of the most important acts of student activism in Chinese history is the 1919 May Fourth Movement that saw over 3,000 students of Peking University and other schools gathered together in front of Tiananmen and holding a demonstration. It is regarded as an essential step of the democratic revolution in China, and it had also give birth to Chinese Communism. Anti-Americanism movements led by the students during the Chinese Civil War were also instrumental in discrediting the KMT government and bring the Communist victory in China.[24] In 1989, the democracy movement led by the students at the Tiananmen Square protests ended in a brutal government crackdown which would later be called a massacre.		During communist rule, students in Eastern Europe were the force behind several of the best-known instances of protest. The chain of events leading to the 1956 Hungarian Revolution was started by peaceful student demonstrations in the streets of Budapest, later attracting workers and other Hungarians. In Czechoslovakia, one of the most known faces of the protests following the Soviet-led invasion that ended the Prague Spring was Jan Palach, a student who committed suicide by setting fire to himself on January 16, 1969. The act triggered a major protest against the occupation.[25]		Student-dominated youth movements have also played a central role in the "color revolutions" seen in post-communist societies in recent years. The first example of this was the Serbian Otpor! ("Resistance!" in Serbian), formed in October 1998 as a response to repressive university and media laws that were introduced that year. In the presidential campaign in September 2000, the organisation engineered the "Gotov je" ("He's finished") campaign that galvanized Serbian discontent with Slobodan Milošević, ultimately resulting in his defeat.[26]		Otpor has inspired other youth movements in Eastern Europe, such as Kmara in Georgia, which played an important role in the Rose Revolution, and Pora in Ukraine, which was key in organising the demonstrations that led to the Orange Revolution.[27] Like Otpor, these organisations have consequently practiced non-violent resistance and used ridiculing humor in opposing authoritarian leaders. Similar movements include KelKel in Kyrgyzstan, Zubr in Belarus and MJAFT! in Albania.		Opponents of the "color revolutions" have accused the Soros Foundations and/or the United States government of supporting and even planning the revolutions in order to serve western interests.[28] Supporters of the revolutions have argued that these allegations are greatly exaggerated, and that the revolutions were positive events, morally justified, whether or not Western support had an influence on the events.		In France, student activists have been influential in shaping public debate. In May 1968 the University of Paris at Nanterre was closed due to problems between the students and the administration.[29] In protest of the closure and the expulsion of Nanterre students, students of the Sorbonne in Paris began their own demonstration.[30] The situation escalated into a nationwide insurrection.		The events in Paris were followed by student protests throughout the world. The German student movement participated in major demonstrations against proposed emergency legislation. In many countries, the student protests caused authorities to respond with violence. In Spain, student demonstrations against Franco's dictatorship led to clashes with police. A student demonstration in Mexico City ended in a storm of bullets on the night of October 2, 1968, an event known as the Tlatelolco massacre. Even in Pakistan, students took to the streets to protest changes in education policy, and on November 7 two college students died after police opened fire on a demonstration.[31] The global reverberations from the French uprising of 1968 continued into 1969 and even into the 1970s.[32]		In 1815 in Jena (Germany) the "Urburschenschaft" was founded. That was a Studentenverbindung that was concentrated on national and democratic ideas. In 1817, inspired by liberal and patriotic ideas of a united Germany, student organisations gathered for the Wartburg festival at Wartburg Castle, at Eisenach in Thuringia, on the occasion of which reactionary books were burnt.		In 1819 the student Karl Ludwig Sand murdered the writer August von Kotzebue, who had scoffed at liberal student organisations.		In May 1832 the Hambacher Fest was celebrated at Hambach Castle near Neustadt an der Weinstraße with about 30 000 participants, amongst them many students. Together with the Frankfurter Wachensturm in 1833 planned to free students held in prison at Frankfurt and Georg Büchner's revolutionary pamphlet Der Hessische Landbote that were events that led to the revolutions in the German states in 1848.		In the 1960s, the worldwide upswing in student and youth radicalism manifested itself through the German student movement and organisations such as the German Socialist Student Union. The movement in Germany shared many concerns of similar groups elsewhere, such as the democratisation of society and opposing the Vietnam War, but also stressed more nationally specific issues such as coming to terms with the legacy of the Nazi regime and opposing the German Emergency Acts.		Hong Kong Student activist group Scholarism began an occupation of the Hong Kong government headquarters on 30 August 2012. The goal of the protest was, expressly, to force the government to retract its plans to introduce Moral and National Education as a compulsory subject.[33] On 1 September, an open concert was held as part of the protest, with an attendance of 40,000.[34] At last, the government de facto striked down the Moral and National Education.		Student organizations made important roles during the Umbrella Movement. Standing Committee of the National People's Congress (NPCSC) made decisions on the Hong Kong political reform on 31 August 2014, which the Nominating Committee would tightly control the nomination of the Chief Executive candidate, candidates outside the Pro-Beijing camp would not have opportunities to be nominated. The Hong Kong Federation of Students and Scholarism led a strike against the NPCSC's decision beginning on 22 September 2014, and started protesting outside the government headquarters on 26 September 2014.[35] On 28 September, the Occupy Central with Love and Peace movement announced that the beginning of their civil disobedience campaign.[36] Students and other members of the public demonstrated outside government headquarters, and some began to occupy several major city intersections.[37]		In 16 January 2017 a large group of students (nearly more than 20 lakhs) protested in state of Tamil Nadu and Puducherry for the ban on Jallikattu. The first protest took place in Alanganallur madurai district where few were involved and have been arrested in pay for that a mass huge protest started at Marina beach Chennai, after that Puducherry, Trichy, Salem, Coimbatore, Erode, Tirunelveli, Dharmapuri, Krishnagiri, Vellore, and almost all district of Tamil Nadu participated in the protest. The ban was made by Supreme court of India in 2014 when PETA filed a petition against Jallikattu as a cruelty to animals.		On 20 Jan temporary ordinance has been passed on lifting the ban on Jallikattu.		It was the only Students protest in India done very peacefully. Food, water, mobile restrooms were all arranged by the students and were provided free to more than 20 lakhs student. Most of the Students stayed in the protest place. Many families were also joined the protest. No harassment of women has been filed during the protest as the women also stayed in the protest.Everyone was wondering how such a remarkable protest take place in Tamil Nadu by tamilians. No political parties were allowed in the protest. And from that many tamilians across the various countries showed their support. Not only tamilians everyone everybody started supporting this in spite of religion their language.		Indonesia has hosted "some of the most important acts of student resistance in the world's history".[38] university student groups have repeatedly been the first groups to stage street demonstrations calling for governmental change at key points in the nation's history, and other organizations from across the political spectrum have sought to align themselves with student groups. In 1928, the Youth Pledge (Sumpah Pemuda) helped to give voice to anti-colonial sentiments.		During the political turmoil of the 1960s, right-wing student groups staged demonstrations calling for then-President Sukarno to eliminate alleged Communists from his government, and later demanding that he resign.[39] Sukarno did step down in 1967, and was replaced by Army general Suharto.[40]		Student groups also played a key role in Suharto's 1998 fall by initiating large demonstrations that gave voice to widespread popular discontent with the president in the aftermath of the May 1998 riots.[41] High school and university students in Jakarta, Yogyakarta, Medan, and elsewhere were some of the first groups willing to speak out publicly against the military government. Student groups were a key part of the political scene during this period. Upon taking office after Suharto stepped down, B. J. Habibie made numerous mostly unsuccessful overtures to placate the student groups that had brought down his predecessor. When that failed, he sent a combined force of police and gangsters to evict protesters occupying a government building by force.[42] The ensuing carnage left two students dead and 181 injured.[42]		In Iran, students have been at the forefront of protests both against the pre-1979 secular monarchy and, in recent years, against the theocratic islamic republic. Both religious and more moderate students played a major part in Ruhollah Khomeini's opposition network against the Shah Mohammad Reza Pahlavi.[43] In January 1978 the army dispersed demonstrating students and religious leaders, killing several students and sparking a series of widespread protests that ultimately led to the Iranian Revolution the following year. On November 4, 1979, militant Iranian students calling themselves the Muslim Students Following the Line of the Imam seized the U.S. embassy in Tehran holding 52 embassy employees hostage for a 444 days (see Iran hostage crisis).		Recent years have seen several incidents when liberal students have clashed with the Iranian government, most notably the Iranian student riots of July 1999. Several people were killed in a week of violent confrontations that started with a police raid on a university dormitory, a response to demonstrations by a group of students of Tehran University against the closure of a reformist newspaper. Akbar Mohammadi was given a death sentence, later reduced to 15 years in prison, for his role in the protests. In 2006, he died at Evin prison after a hunger strike protesting the refusal to allow him to seek medical treatment for injuries suffered as a result of torture.[44]		At the end of 2002, students held mass demonstrations protesting the death sentence of reformist lecturer Hashem Aghajari for alleged blasphemy. In June 2003, several thousand students took to the streets of Tehran in anti-government protests sparked by government plans to privatise some universities.[45]		In the May 2005 Iranian presidential election, Iran's largest student organization, The Office to Consolidate Unity, advocated a voting boycott.[46] After the election of President Mahmoud Ahmadinejad, student protests against the government has continued. In May 2006, up to 40 police officers were injured in clashes with demonstrating students in Tehran.[47] At the same time, the Iranian government has called for student action in line with its own political agenda. In 2006, President Ahmadinejad urged students to organize campaigns to demand that liberal and secular university teachers be removed.[48]		In 2009, after the disputed presidential election, a series of student protests broke out, which became known as the Iranian Green Movement. The violent measures used by the Iranian government to suppress these protests have been the subject of widespread international condemnation.[49]		In Israel the students were amongst the leading figures in the 2011 Israeli social justice protests that grew out of the Cottage cheese boycott.[50]		Japanese student movement began during the Taishō Democracy, and grew in activity after World War II. They were mostly carried out by activist students. One such event was the Anpo opposition movement, which occurred in 1960, in opposition to the Anpo treaty.[51] In the subsequent student uprising in 1968, leftist activists barricaded themselves in universities, resulting in armed conflict with the Japanese police force.[52] Some wider causes were supported including opposition to the Vietnam War and apartheid, and for the acceptance of the hippie lifestyle.		Since the amendment of Section 15 of the Universities and University Colleges Act 1971 (UUCA) in 1975, students were barred from being members of, and expressing support or opposition to, any political parties or "any organization, body or group of persons which the Minister, after consultation with the Board, has specified in writing to the Vice-Chancellor to be unsuitable to the interests and well-being of the students or the University." However, in October 2011, the Court of Appeal ruled that the relevant provision in Section 15 UUCA was unconstitutional due to Article 10 of the Federal Constitution pertaining to freedom of expression.[53]		Since the act prohibiting students from expressing "support, sympathy or opposition" to any political party was enacted in 1971, Malaysian students have repeatedly demanded that the ban on political involvement be rescinded. The majority of students are not interested in politics because they are afraid that the universities will take action against them. The U.U.C.A. (also known by its Malaysian acronym AUKU) not however been entirely successful in eliminating student activism and political engagement.[54]		In Kuala Lumpur on 14 April 2012, student activists camped out at Independence Square and marched against a government loan program that they said charged students high interest rates and left them with debt.[55]		The largest student movement in Malaysia is the Solidariti Mahasiswa Malaysia (SMM)(Student Solidarity of Malaysia). SMM is a coalition group that represents numerous student organizations.[56] Currently, SMM is actively campaigning against the UUCA and a free education at primary, secondary and tertiary level.		During the protests of 1968, Mexican government killed an estimated 30 to 300 students and civilian protesters. This killing is known as in the Tlatelolco massacre. killing of an estimated 30 to 300 students and civilians by military and police on October 2, 1968, in the Plaza de las Tres Culturas in the Tlatelolco section of Mexico City. The events are considered part of the Mexican Dirty War, when the government used its forces to suppress political opposition. The massacre occurred 10 days before the opening of the 1968 Summer Olympics in Mexico City.[57]		More recent student movements include Yo Soy 132 in 2012. Yo Soy 132 was a social movement composed for the most part of Mexican university students from private and public universities, residents of Mexico, claiming supporters from about 50 cities around the world.[58] It began as opposition to the Institutional Revolutionary Party (PRI) candidate Enrique Peña Nieto and the Mexican media's allegedly biased coverage of the 2012 general election.[59] The name Yo Soy 132, Spanish for "I Am 132", originated in an expression of solidarity with the original 131 protest's initiators. The phrase drew inspiration from the Occupy movement and the Spanish 15-M movement.[60][61][62] The protest movement was self-proclaimed as the "Mexican spring" (an allusion to the Arab Spring) by its first spokespersons,[63] and called the "Mexican occupy movement" in the international press.[64]		Following the 2014 Iguala mass kidnapping, students responded nationally in protest from marches to destruction of property. Through social media, hashtags such as #TodosSomosAyotzinapa spread and prompted global student response.[65]		Student political activism has existed in U.K since the 1880s with the formation of the student representative councils, precursors of union organisations designed to present students interests. These later evolved into unions, many of which became part of the National Union of Students formed in 1921. However, the NUS was designed to be specifically outside of "political and religious interests", reducing its importance as a centre for student activism. During the 1930s students began to become more politically involved with the formation of many socialist societies at universities, ranging from social democratic to Marxist–Leninist and Trotskyite, even leading to Brian Simon, a communist, becoming head of the NUS.[66]		However, it was not until the 1960s that student activism became important in British universities. Here, like many other countries, the Vietnam war and issues of racism became a focus for many other local frustrations, such as fees and student representation. In 1962, the first student protest against the Vietnam War was held, with CND. However, student activism did not begin on a large scale until the mid-1960s. In 1965, a student protest of 250 students was held outside Edinburgh's American embassy and the beginning of protests against the Vietnam war in Grovesnor square. It also saw the first major teach-in in Britain in 1965, where students debated the Vietnam War and alternative non-violent means of protest at the London School of Economics, sponsored by the Oxford Union.[67]		In 1966 the Radical Student Alliance and Vietnam Solidarity Campaign were formed, both of which became centres for the protest movement. However, the first student sit-in was held at the London School of Economics in 1967 by their Student's Union over the suspension of two students. Its success and a national student rally of 100,000 held in the same year is usually considered to mark the start of the movement. Up until the mid-1970s student activities were held including a protest of up to 80,000 strong in Grosvenor Square, anti-racist protests and occupations in Newcastle, the breaking down of riot control gates and forced closure of the London School of Economics, and Jack Straw becoming the head of the NUS for the RSA. However, many protests were over more local issues, such as student representation in college governance,[68] better accommodation, lower fees or even canteen prices.		Student protests erupted again in 2010 during the Premiership of David Cameron over the issue of tuition fees, higher education funding cuts and withdrawal of the Education Maintenance Allowance.[69]		In the United States, student activism is often understood as a form of youth activism that is specifically oriented toward change in the American educational system. Student activism in the United States dates to the beginning of public education, if not before. Some of the first well documented, directed activism occurred on the campuses of black institutions like Fisk and Howard in the 1920s. At Fisk, student's concerns surrounding disciplinary rules designed to undermine black identity coalesced into demands for the resignation of President Fayette Avery McKenzie. Spurred by alum W.E.B. Du Bois' 1924 commencement speech, the students ignored the 10p.m. curfew to protest, and staged subsequent walkouts. After a committee formed to investigate the protests ruled unfavorably on Mckenzie's abilities and handling of the unrest, he resigned on April 16, 1925. Events at Fisk had wide repercussions, as black students elsewhere began to question the repressive status quo of the postwar black university.[70]		The next wave of activism was spurred by Depression-era realities of the 1930s. The American Youth Congress was a student-led organization in Washington, DC, which lobbied the US Congress against war and racial discrimination and for youth programs. It was heavily supported by First Lady Eleanor Roosevelt.[71]		The counterculture era of the 1960s and early 1970s saw several waves of student activists gaining increasing political prominence in American society. Students formed social movements that moved them from resistance to liberation.[72] An early important national student group was the Student's Peace Union, established in 1959.[73] Another highlight of this period was Students for a Democratic Society (SDS) launched in Ann Arbor, Michigan, was a student-led organization that focused on schools as a social agent that simultaneously oppresses and potentially uplifts society. SDS eventually spun off the Weather Underground. Another successful group was Ann Arbor Youth Liberation, which featured students calling for an end to state-led education. Also notable were the Student Nonviolent Coordinating Committee and the Atlanta Student Movement, predominantly African American groups that fought against racism and for integration of public schools across the US.		The longest student strike in American history started on November 6, 1968 and lasted until March 21, 1969 at San Francisco State College to raise awareness of third world student access to higher education.[74][75][better source needed]		The largest student strike in American history took place in May and June 1970, in response to the Kent State shootings and the American invasion of Cambodia. Over four million students participated in this action.[76]		American society saw an increase in student activism again in the 1990s. The popular education reform movement has led to a resurgence of populist student activism against standardized testing and teaching,[77] as well as more complex issues including military/industrial/prison complex and the influence of the military and corporations in education[78] There is also increased emphasis on ensuring that changes that are made are sustainable, by pushing for better education funding and policy or leadership changes that engage students as decision-makers in schools. Notably, universities participated in the Disinvestment from South Africa movement, University of California, Berkeley being the first institution to disinvest completely from companies implicated in and profiting from the Apartheid movement following student organizing and activism.		Major contemporary campaigns include work for funding of public schools, against increased tuitions at colleges or the use of sweatshop labor in manufacturing school apparel (e.g. United students against sweatshops), for increased student voice throughout education planning, delivery, and policy-making (e.g. The Roosevelt Institution), and to raise national and local awareness of the humanitarian consequences of the Darfur Conflict.[79] There is also increasing activism around the issue of global warming. Antiwar activism has also increased leading to the creation of the Campus Antiwar Network and the refounding of SDS in 2006.		Following the national growth of the Black Lives Matter Movement, and more intensely since the 2016 election of U.S. President Donald Trump, student activism has been on the rise. Alt-Right Breitbart senior editor Milo Yiannopoulos' tour Dangerous Faggot sparked protest at University of California, Davis, where he was scheduled to speak alongside "Pharma Bro" Martin Shkreli and University of California, Berkeley, all shutting his talks down before they started through large-scale protest. [80]		
Veterinary education is the tertiary education of veterinarians. To become a veterinarian, one must first complete a veterinary degree (DVM, VMD, BVS, BVSc, BVMS, BVM, cand.med.vet).		Many veterinary schools outside North America use the title "Faculty of Veterinary Science" instead of "College of Veterinary Medicine" or "School of Veterinary Medicine", and some veterinary schools in China, Japan and South Korea (such as the DVM degree-awarding Department of Veterinary Science and Animal Husbandry at Guangxi University in China and the Department of Veterinary Medicine at Tokyo University of Agriculture and Technology use the term "Department".[1]) Veterinary schools are distinct from departments of animal science offering a pre-veterinary curriculum, teaching the biomedical sciences (and awarding a Bachelor of Science degree or the equivalent), and providing graduate veterinary education in disciplines such as microbiology, virology, and molecular biology.						Aspiring veterinarians can earn several types of degrees, differing by country and involving undergraduate or graduate education.[1] In the United States, schools award the Doctor of Veterinary Medicine degree (DVM).[2] This degree is also awarded in Bangladesh, Canada, Ethiopia, Hungary, Iran, Malaysia, Nigeria, Pakistan, Philippines, South Korea, Thailand, Tobago and Trinidad.[1] Other countries offer a degree equivalent to the North American DVM. In the United Kingdom and countries which have adopted the undergraduate system of higher education, a bachelor's degree is equivalent to a DVM (after five or six years of study). In the US, a four-year DVM degree such as Bachelor of Veterinary Science, Bachelor of Veterinary Medicine or Bachelor of Veterinary Medicine and Surgery follows a four-year undergraduate degree (eight years of study after high school).[3] In Ireland, the Veterinary Medicine Programme at the University College Dublin awards the Bachelor of Veterinary Medicine (MVB).[4] At the University of Edinburgh and the University of Glasgow, the degree awarded is the Bachelor of Veterinary Medicine & Surgery (BVMS).[5] Some veterinary schools offer a degree enabling the recipient to practice veterinary medicine in their home country but does not permit the individual to take a licensing examination abroad; for example, veterinary schools in Afghanistan offer only the Bachelor of Science (BS) degree.[3] Although Ethiopia awards a Doctor of Veterinary Medicine degree, it is not recognized in the US or Western Europe.[6]		About 50 percent of veterinarians own their own business when they graduate from school. Nearly every country requires an individual with a veterinary degree to be licensed before practicing. Most countries require a non-national with a veterinary degree to pass a separate licensure exam for foreign graduates before practicing. In the US, the Educational Commission for Foreign Veterinary Graduates (ECFVG) administers a four-step examination recognized by all American state and territorial veterinary licensing boards, the US federal government, and the District of Columbia.[7] The European Parliament issued a September 30, 2005 directive providing EU-wide standards for veterinary medical education and the recognition of veterinary degrees from member states.[8]		Licensure requirements are diverse. In South Africa, the Veterinary and Para-Veterinary Professions Act, Act 19 of 1982 provides for automatic licensure if an individual has graduated from one of several universities in South Africa, New Zealand, or the United Kingdom (including the University of Pretoria, Medical University of South Africa, Massey University, University of Bristol, University of Cambridge, University of Edinburgh, University of Glasgow, University of Liverpool, and the University of London as of 2008) or has passed the licensure examination administered by the Royal College of Veterinary Surgeons. All others must pass an examination and register with the South African Veterinary Council.[9] India has a similar system, in which degrees awarded by certain schools are "deemed" to qualify an individual to practice veterinary medicine, but has forgone an exam in favor of state tribunals which investigate credentials and control a registry of licensed practitioners.[10]		All developed countries and most newly industrialized and developing countries accredit veterinary schools.[11] Those in the US are accredited by the American Veterinary Medical Association (AVMA) Council on Education (COE)[12][13] The EU is developing an accreditation standard, with accreditation usually provided by the European Association of Establishments for Veterinary Education (EAEVE) as of 2008.[14][15][16]		Accreditation systems vary widely in developing nations. In Mexico El Consejo Nacional de Educación de la Medicina Veterinaria y Zootecnia (CONEVET) accredits veterinary medical colleges, although few schools are accredited.[17] The accreditation system is poor (or nonexistent) in other developing nations; Ethiopia has focused on building veterinary medical colleges rather than accrediting existing schools. With almost no accreditation system, the country's veterinary education is poor.[6]		Admissions practices, requirements and difficulty vary widely among veterinary schools and by country. Admission is generally competitive, due to the small number of places available.[18] Most AVMA-accredited institutions in Australia, Canada, Ireland, New Zealand, the United Kingdom, and the United States share an online application system, known as the Veterinary Medical College Application Service (VMCAS).[18] Many VMCAS colleges also have additional, individualized application requirements, and admissions standards are high.[18][19]		Admissions standards in Europe, South America, Asia, and Africa also vary widely, with many veterinary schools limiting admission to students from their area, state or country. Twenty-five of the 28 veterinary schools in the US are public universities and, by law, may reserve few places for out-of-state residents.[18] Other countries have similar schemes. In India, federal law requires each veterinary college to reserve 15 percent of its places for students from other parts of India. The Veterinary Council of India (a body of the federal government) conducts the All India Common Entrance Examination, and the top scorers are placed throughout the country.[20]		The cost of attending veterinary school also varies widely. The value of the national currency, the cost of veterinary school relative to the cost of living (or median national income), and government education subsidies and other financial aid influence its cost. In countries where a veterinary degree is a professional degree taken as a second degree, governments may not subsidize veterinary-school students as much as undergraduate students.[citation needed]		Veterinary school curricula are not standardized, with programs lasting from three to six years. In the United States and Canada the program is generally four years long, usually after a four-year pre-vet undergraduate degree). For the first three years, students learn anatomy, physiology, histology, neuroanatomy, pharmacology, immunology, bacteriology, virology, pathology, parasitology, toxicology, herd health (also called population health), nutrition, radiography, and epidemiology. During the third year, students learn anesthesiology, diagnostics, surgery, ophthalmology, orthopedics, and dentistry. For the fourth year, often 12 months long instead of nine, students care for a wide range of animals.[21] Clinical education is a focus of most veterinary school curricula worldwide. In 2005, for the first time in its 104-year-history, the Veterinary Medicine Programme at University College Dublin instituted a lecture-free final year focusing on clinical training.[22] The Institute of Veterinary Pathology at the University of Zurich has implemented a curriculum for teaching pathology with an extensive clinical component.[23] Veterinary schools in Israel,[24] Spain,[25] the Czech Republic,[26] and Slovakia[27] also emphasize clinical training.		However, clinical training is limited in some schools and countries; in Japan, students do not receive clinical education until they have studied for six years.[28] In Sri Lanka, until recently there were few companion animals; veterinary education focused on herd health, with little attention to clinical skills.[29] In Ethiopia few schools have clinical training facilities, and the government has prioritized opening more schools over improving existing colleges.[6] There is concern in the United States that clinical training may suffer because many veterinary teaching hospitals are in financial trouble.[30]		Most veterinary schools do not allow students to engage in "species specialization", and students must be able to treat a broad range of species.[31] However, most veterinary programs allow students to take elective courses which will enable them to specialize at graduation. Veterinary schools in Australia, Canada, the United Kingdom and the United States engage in "tracking", and students are asked which branch of veterinary medicine they intend to practice (such as companion-animal, bovine, equine, food-supply, avian, wildlife, and public-health).[32] Although tracking is controversial, about 60 percent of US and Canadian veterinary schools engage in full or partial tracking of students and there are calls for full tracking by some North American veterinary-medical-education organizations.[33][34] It is argued that enhanced tracking should be linked to "limited licensure" of veterinarians to practice only in the species (or specialty) in which they were trained.[33][35] Although very few veterinary schools require students to enroll in an internship or residency upon graduation, internships and residencies are often required for veterinarians seeking board certification in Canada, Europe and the US.[36]		Lectures and rote learning are two of the most common teaching methods in veterinary education.[37] To a lesser degree, outcome-based education[38] discovery learning, and inquiry-based learning are also used.[39] Problem-based learning has been adopted in most veterinary schools in developed countries, particularly Australia, Canada, New Zealand, the United States, and Western Europe.[40]		
Abitur (German: [abiˈtuːɐ̯]; German: Abiturium (old-fashioned) from Latin abiturus (future active participle of abire): "someone who is going to leave", "something (masculine) that is going to leave", or ) is a university-preparatory school leaving qualification in Germany, Lithuania, Finland, and Estonia. It is conferred to students who pass their final exams at the end of their secondary education, usually after twelve or thirteen years of schooling (see also for Germany Abitur after twelve years). As a matriculation examination, Abitur can be compared to A-level, Matura or the International Baccalaureate Diploma, which are all ranked as level 4 in the European Qualifications Framework.						The Zeugnis der Allgemeinen Hochschulreife ("certificate of general qualification for university entrance"), often referred to as Abiturzeugnis ("Abitur certificate"), issued after candidates have passed their final exams and have had appropriate grades in both the last and second last school year, is the document which contains their grades and formally enables them to attend university. Thus, it encompasses the functions of both a school graduation certificate and a college entrance exam.[1]		The official term in Germany for this certificate of education is Allgemeine Hochschulreife; the contraction Abi is common in colloquial usage. In 2005, a total of 231,465 students passed the Abitur exam in Germany. The numbers have risen steadily and in 2012, a total of 305,172 students obtained the Allgemeine Hochschulreife.[2] This number, reflecting those who pass the traditional Abitur at their high school, is, however, lower than the total count. Adding (for 2012) the 51,912 students who obtained the Hochschulreife at vocational training schools, that total number increases to 357,084. If those who obtain the Fachhochschulreife (144,399 in 2012) are also added, then the total of those who obtained the right to study at a university or a Fachhochschule is 501,483 (2012).[3]		Until the 18th century, every German university had its own entrance examination. In 1788 Prussia introduced the Abiturreglement, a law, for the first time within Germany, establishing the Abitur as an official qualification. It was later also established in the other German states. In 1834, it became the only university entrance exam in Prussia, and it remained so in all states of Germany until 2004. Since then, the German state of Hesse allows students with Fachhochschulreife (see below) to study at the universities within that state.		The academic level of the Abitur is comparable to the International Baccalaureate, the GCE Advanced Level and the Advanced Placement tests. Indeed, the study requirements for the International Baccalaureate differ little from the German exam requirements. It is the only school-leaving certificate in all states of Germany that allows the graduate (or Abiturient) to move directly to university. The other school leaving certificates, the Hauptschulabschluss and the Realschulabschluss, do not allow their holders to matriculate at a university. Those granted certificates of Hauptschulabschluss or Realschulabschluss can gain a specialized Fachabitur or an Abitur if they graduate from a Berufsschule and then attend Berufsoberschule or graduate from a Fachoberschule.		However, the Abitur is not the only path to university studies, as some universities set up their own entrance examinations. Students who successfully passed a "Begabtenprüfung" ("test of aptitude") are also eligible. Students from other countries who hold a high school leaving certificate that is not counted as being equivalent to the Abitur (such as the American high school diploma) and who do well enough on the ACT or SAT test, may also enter German universities. A person who does not hold the Abitur and did not take an aptitude test may still be admitted to university by completing at least the 10th grade and doing well on an IQ-Test (see: Hochbegabtenstudium).		In German, the European Baccalaureate is called europäisches Abitur, and the International Baccalaureate is called internationales Abitur, both not to be confused with the German Abitur.		The term Fachabitur was used in all of Western Germany for a variation of the Abitur until the 1990s; the official term for the German qualification is fachgebundene Hochschulreife. This qualification includes only one foreign language (usually, English). The Abitur, in contrast, usually requires two foreign languages. The Fachabitur also allows the graduate to start studying at a university but is limited to a specified range of majors, depending on the specific subjects covered in his Fachabitur examinations. But the graduate is allowed to study for all majors at a Fachhochschule (University of Applied Sciences, in some ways comparable to polytechnics). Today, the school leaving certificate is called fachgebundenes Abitur ('restricted subject Abitur').		Now the term Fachabitur is used in most parts of Germany for the Fachhochschulreife (FHR). It was introduced in West Germany in the 1970s together with the Fachhochschulen. It enables the graduate to start studying at a Fachhochschule and, in Hesse, also at a university within that state. In the Gymnasiums of some states it is awarded in the year before the Abitur is reached. However, the normal way to obtain Fachhochschulreife is graduation from a German Fachoberschule, a vocational high school, also introduced in the 1970s.		The term Notabitur ('emergency Abitur') describes a qualification used only during World War I and World War II. It was granted to male German Gymnasium (prep school) students who voluntarily enlisted for military service before graduation as well as young women who were evacuated from the major cities before they could complete their Gymnasium education as planned (approximately three to five million children and teenagers had to be evacuated during the war). The Notabitur during World War I included an examination, roughly equivalent to the Abitur exam. The World War II Notabitur, in contrast, was granted without an examination. After the war this was a major disadvantage for the students concerned since, unlike its World War I counterpart, the certificate was generally not recognised in West Germany and never recognised in East Germany. Universities requested the Abitur to consist of written exams including at least two foreign languages (almost always Latin and French, the latter sometimes replaced by English). Students, who received the Notabitur during World War II were offered to re-enter school to prepare for and take the exam after the war had ended. Those special Abitur preparation classes were made up of young adults of different age and sex, which was very unusual at the time.		The equivalent graduation certificate in Austria, Poland and other countries of continental Europe is the Matura; while in England, Wales, Northern Ireland, Hong Kong, Singapore, and the West Indies, it is A-levels; in Scotland it is Higher Grade; in the Republic of Ireland it is the Leaving Certificate; in Greece and Cyprus it is the "apolytirion" (a kind of high school diploma); in Malta it is the Matriculation Certificate (MATSEC), in Hungary it is called "érettségi bizonyítvány" roughly equivalent with the German phrase Zeugnis der Allgemeinen Hochschulreife as it is originating from the Austrian-Hungarian Monarchy.		In Australia, the graduation certificate awarded to high school students is the Senior Secondary Certificate of Education (SSCE). However, the name of the SSCE varies from state to state. In Victoria, it is called the Victorian Certificate of Education (VCE).		In India various states call it differently. Each Indian state has its own examination board, some individual states having their own entrance test system. Passing the specified examination qualifies the student to enter into undergraduate program in a university. For example, in the states of Andhra Pradesh and Telangana this is known as Board of Intermediate Examination (BIE).		For professional, specialist programs and well reputed institutions there are entrance tests. For engineering there is a Joint Engineering Entrance Joint Entrance Examination conducted at all India level. For medical undergraduate MBBS programs there is a national eligibility and entrance test known as NEET-UG National Eligibility and Entrance Test conducted at all of India. There is also an all India level examination conducted by Central Board of Secondary education CBSE the certification is known as 12th class.		During the final examinations (Abiturprüfungen), students are tested in four or five subjects (at least one of which is oral). Procedures vary by state.		Although some tested subjects are chosen by the student, three areas must be covered:		Occasionally, schools (especially berufsorientierte Gymnasien) offer vocational subjects such as pedagogy, business informatics, biotechnology and mechanical engineering.		Final exams are usually taken from March to May or June. Each written basic-level examination takes about three hours; advanced-level examinations take four-and-a-half hours, and written exams are in essay format. Oral examinations last about 20 min. Papers are graded by at least two teachers at the school. In some parts of Germany students may prepare a presentation, research paper or participate in a competition, and may take additional oral exams to pass the Abitur if the written exam is poor.		Before reunification, Abitur exams were given locally in West Germany, but Bavaria conducted centralized exams (Zentralabitur) since 1854. After reunification, most states of the former East Germany continued centralized exams, and at the beginning of the 21st century, many states adopted centralized exams. In 2013, all other states except Rheinland-Pfalz also introduced centralized written exams at least in the core subjects (German, mathematics and the first foreign language, usually English). The exams are structured as follows:		The Kultusministerkonferenz (KMK) of several states expanded the exams to scientific subjects and the social sciences. The physics and chemistry exams include an experiment that must be performed and analyzed.		Each semester of a subject studied in the final two years yields up to 15 points for a student, where advanced courses count double. The final examinations each count quadruple.		The exact scoring system depends on the Bundesland, in which one takes Abitur. Passing the Abitur usually requires a composite score of at least 50%. Students with a score below that minimum fail and do not receive an Abitur. There are some other conditions that the student also has to meet in order to receive the Abitur: taking mandatory courses in selected subject areas, and limits to the number of failing grades in core subjects. Finally, students often have the option of omitting some courses from their composite score if they have taken more courses than the minimum required.		The best possible grade of 1.0 can be achieved if the score ranges between 823 and 900 points; the fraction of students achieving this score is normally only around 0.2-3%[4] even among the already selective population of Abitur candidates. Around 12%-30% of Abitur candidates achieve grades between 1.0 and 1.9.[5]		Historically, very few people received their Abitur in Germany because many attractive jobs did not require one. The number of persons holding the Abitur has steadily increased since the 1970s, and younger jobholders are more likely to hold the Abitur than older ones. The percentage of students qualified for tertiary education is still lower than the OECD average.		Percentage of students graduating with Abitur or FHR (Studienberechtigtenquote):		Percentage of jobholders holding Hauptschulabschluss, Realschulabschluss or Abitur in Germany:[8]		
The protests of 1968 comprised a worldwide escalation of social conflicts, predominantly characterized by popular rebellions against military and bureaucratic elites, who responded with an escalation of political repression.		In capitalist countries, these protests marked a turning point for the civil rights movement in the United States, which produced revolutionary movements like the Black Panther Party. In reaction to the Tet Offensive, protests also sparked a broad movement in opposition to the Vietnam War all over the United States and even into London, Paris, Berlin and Rome. Mass socialist movements grew not only in the United States but also in most European countries. The most spectacular manifestation of this were the May 1968 protests in France, in which students linked up with wildcat strikes of up to ten million workers, and for a few days the movement seemed capable of overthrowing the government. In many other capitalist countries, struggles against dictatorships, state repression, and colonization were also marked by protests in 1968, such as the beginning of the Troubles in Northern Ireland, the Tlatelolco massacre in Mexico City, and the escalation of guerrilla warfare against the military dictatorship in Brazil.		In the socialist countries there were also protests against lack of freedom of speech and violation of other civil rights by the Communist bureaucratic and military elites. In Central and Eastern Europe there were widespread protests that escalated, particularly in the Prague Spring in Czechoslovakia, in Warsaw in Poland and in Yugoslavia.						Background speculations of overall causality vary about the political protests centering on the year 1968. Some[who?] argue that protests could be attributed to the social changes during the twenty years following the end of World War II. Many protests were a direct response to perceived injustices, such as those voiced in opposition to United States involvement in the Vietnam War.[1]		After World War II, much of the world experienced an unusual surge in births, creating a large age demographic. These babies were born during a time of peace and prosperity for most countries. This was the first generation to grow up with television in their homes.[2] Television had a profound effect on this generation in two ways. First, it gave them a common perspective from which to view the world.[3] The children growing up in this era shared not only the news and programs that they watched on television, they also got glimpses of each other's worlds. Secondly, television allowed them to experience major public events. Public education was becoming more widely attended and more standardized, creating another shared experience. Chain stores and franchised restaurants were bringing shared shopping and dining experiences to people in different parts of the world.[4] These factors all combined to create a generation that was more self-aware and more united as a group than the generations before it.[citation needed]		The Cuban Missile Crisis and the Cold War was another shared experience of this generation. The knowledge that a nuclear attack could end their life at any moment was reinforced with classroom bomb drills[5] creating an atmosphere of fear. As they became older teens, the anti-war movement and the feminist movement were becoming a force in much of the world.		The Eastern Bloc had already seen several mass protests in the decades following World War II, including the Hungarian Revolution, the uprising in East Germany and several labour strikes in Poland, especially important ones in Poznań in 1956.		The feminist movement made a generation question their belief that the family was more important than the individual. The peace movement made them question and distrust authority even more than they had already.[6] By the time they started college, many were part of the anti-establishment culture and became the impetus for a wave of rebellion that started on college campuses and swept the world.		Waves of social movements throughout the 1960s began to shape the values of the generation that were college students during 1968. In America, the Civil Rights Movement was at its most violent. So, too, in Northern Ireland, where it paved the way for an organised revolt against British governance. Italy and France were in the midst of a socialist movement. The New Left political movement was causing political upheavals in many European and South American countries. The Israeli–Palestinian conflict had already started. Great Britain's anti-war movement was very strong and African independence was a continuing struggle. In Poland in March 1968, student demonstrations at Warsaw University broke out when the government banned the performance of a play by Adam Mickiewicz (Dziady, written in 1824) at the Polish Theatre in Warsaw, on the grounds that it contained "anti-Soviet references". It became known as the March 1968 events.		The college students of 1968 embraced the New Left politics. Their socialist leanings and distrust of authority led to many of the 1968 conflicts. The dramatic events of the year showed both the popularity and limitations of New Left ideology, a radical leftist movement that was also deeply ambivalent about its relationship to communism during the middle and later years of the Cold War.		The 2–3 June 1968 student demonstrations in Belgrade, the capital of Yugoslavia, were the first mass protest in the country after the Second World War. The authorities suppressed the protest, while President Josip Broz Tito had the protests gradually cease by giving in to some of the students’ demands. Protests also broke out in other capitals of Yugoslav republics - Sarajevo, Zagreb and Ljubljana—but they were smaller and shorter than in Belgrade.[7][8]		In 1968, Czechoslovakia underwent a process known as the Prague Spring. In the August 1968 Soviet invasion of Czechoslovakia, Czechoslovakian citizens responded to the attack on their sovereignty with passive resistance. Soviet troops were frustrated as street signs were painted over, their water supplies mysteriously shut off, and buildings decorated with flowers, flags, and slogans like, "An elephant cannot swallow a hedgehog." Passers-by painted swastikas on the sides of Soviet tanks. Road signs in the country-side were over-painted to read, in Russian script, "Москва" (Moscow), as hints for the Soviet troops to leave the country.		On 25 August 1968 eight Russian citizens staged a demonstration on Moscow's Red Square to protest the Soviet invasion of Czechoslovakia. After about five minutes, the demonstrators were beaten up and transferred to a police station. Seven of them received harsh sentences up to several years in prison.		The protests that raged throughout 1968 included a large number of workers, students, and poor people facing increasingly violent state repression all around the world. Liberation from state repression itself was the most common current in all protests listed below. These refracted into a variety of social causes that reverberated with each other: in the United States alone, for example, protests for civil liberties, against racism and in opposition to the Vietnam War, as well as feminism and the beginnings of the ecological movement, including protests against biological and nuclear weapons, all boiled up together during this year.[9] Television, so influential in forming the political identity of this generation, became the tool of choice for the revolutionaries. They fought their battles not just on streets and college campuses, but also on the television screen by courting media coverage.[10]		As the waves of protests coming along the 1960s intensified to a new high in 1968, repressive governments through widespread police crack downs, shootings, executions and even massacres marked social conflicts in Mexico, Brazil, Spain, Poland, Czechoslovakia, and China. In West Berlin, Rome, London, Paris, Italy, many American cities, and Argentina, labor unions and students played major roles and also suffered political repression.		The environmental movement can trace its beginnings back to the protests of 1968. The environmental movement evolved from the anti-nuclear movement. France was particularly involved in environmental concerns. In 1968, the French Federation of Nature Protection Societies and the French branch of Friends of the Earth were formed and the French scientific community organized Survivre et Vivre (Survive and Live). The Club of Rome was formed in 1968. The Nordic countries were at the forefront of environmentalism. In Sweden, students protested against hydroelectric plans. In Denmark and the Netherlands, environmental action groups protested about pollution and other environmental issues.[9] The Northern Ireland civil rights movement began to start, but resulted in the conflict now known as The Troubles.		In January, police used clubs on 400 anti-war protestors outside of a dinner for U.S. Secretary of State Rusk.[11] In February, students from Harvard, Radcliffe, and Boston University held a four-day hunger strike to protest the war.[12] 10,000 West Berlin students held a sit-in against American involvement in Vietnam.[12] People in Canada protested the war by mailing 5,000 copies of the paperback, Manual for Draft Age Immigrants to Canada to the United States.[13] On March 6, 500 New York University (NYU) students demonstrated against Dow Chemical because the company was the principal manufacturer of napalm, used by the U.S. military in Vietnam.[14] On March 17, an anti-war demonstration in Grosvenor Square, London, ended with 86 people injured and 200 demonstrators arrested.[15] Japanese students protested the presence of the American military in Japan because of the Vietnam War.[16] In March, British students turned violent in their anti-war protests (opposing the Vietnam War), physically attacking the British defense secretary, the secretary of state for education and the Home Secretary.[16] In August, the 1968 Democratic National Convention in Chicago was disrupted by five days of street demonstrations by thousands of anti-war protesters. Chicago's mayor escalated the riots with excessive police presence and by ordering up the National Guard and the army to suppress the protests.[17] In September, the women's liberation movement gained international recognition when it demonstrated at the annual Miss America beauty pageant. The week-long protest and its disruption of the pageant gained the movement much needed attention in the press.[18]		In the United States, the Civil Rights Movement had turned away from the south and toward the cities in the north with the issues of open housing and the Black Consciousness Movement. The Black movement unified and gained international recognition with the emergence of the Black Power and Black Panthers organizations and their support of violence as a means of protest.[19] The Orangeburg massacre on February 8, a civil rights protest in Orangeburg, South Carolina, turned deadly with the death of three college students.[20] In March, students in North Carolina organized a sit-in at a local lunch counter that spread to 15 cities.[21] In March, students from all five public high schools in East L.A. walked out of their classes protesting against unequal conditions in Los Angeles Unified School District high schools. Over the next several days, they inspired similar walkouts at fifteen other schools.[22] On April 4, Martin Luther King, Jr., was killed, sparking violent protests in more than 115 American cities, notably Louisville, Baltimore and Washington, D.C.[23] On April 23, students at Columbia University protested the school's allegedly racist policies, three school officials were taken hostage for 24 hours.[14] This was just one of a number of Columbia University protests of 1968.		On January 30, 300 student protesters from the University of Warsaw and the National Theater School were beaten with clubs by state arranged anti-protestors.[24] On March 8, the 1968 Polish political crisis began with students from the University of Warsaw who marched for student rights and were beaten with clubs. The next day over two thousand students marched in protest of the police involvement on campus and were clubbed and arrested again. By March 11, the general public had joined the protest in violent confrontations with students and police in the streets. The government fought a propaganda campaign against the protestors, labeling them Zionists. The twenty days of protest ended when the state closed all of the universities and arrested more than a thousand students. Most Polish Jews left the country to avoid persecution by the government.[25]		The German student movements were largely a reaction against the perceived authoritarianism and hypocrisy of the German government and other Western governments, particularly in relation to the poor living conditions of students. Students in 108 German universities protested for recognition of East Germany, the removal of government officials with Nazi pasts and for the rights of students.[26] In February, protests by professors at the German University of Bonn demanded the resignation of the university's president because of his involvement in the building of concentration camps during the war.[27]		On May 3rd activists protested the participation of two apartheid nations, Rhodesia and South Africa's, in the international tennis competition held in Båstad, Sweden. The protest was among the most violent between Swedish police and demonstrators during the 1960s, resulting in a dialogue between the Swedish Government and organizers to curb the escalation of violence. The match was later played in secrecy, with Sweden winning 4-1.[28]		At Stockholm University leftist students occupied their Student Union Building at Holländargatan from May 24–27 to send a political message to the government. Inspired by the protests in France earlier that month, the Stockholm protests were calmer than those in Paris.[30] In reaction to the protests, right-wing students organized Borgerliga Studenter, or "Bourgeois Students", whose leaders included future prime ministers Carl Bildt and Fredrik Reinfeldt. The Student Union building would later be absorbed by the Stockholm School of Economics.		The admittance of the South African team brought the issue of Apartheid to the 1968 Summer Olympics in Mexico City. After more than 40 teams threatened to boycott, the committee reconsidered and again banned the South African team. The Olympics were targeted as a venue to bring the Black Movement into public view. The entire summer was a series of escalating conflicts between Mexican students and the police.[31] On October 2, after a summer of protests against the Mexican government and the occupation of the central campus of the National Autonomous University (UNAM) by the army, a student demonstration in Tlatelolco Plaza in Mexico City ended with police, paratroopers and paramilitary units firing on students, killing over a hundred persons.[32]		In what became known as Prague Spring, Czechoslovakia's first secretary Alexander Dubček began a period of reform, which gave way to outright civil protest, only ending when the USSR invaded the country in August.[33] In August the 25, anti-war protesters gathered in red square only to be dispersed. It was titled the 1968 Red Square demonstration.		Workers were joined by students at the University of Madrid to protest the involvement of police in demonstrations against dictator Francisco Franco's regime, demanding democracy, trade unions and worker rights, and education reform.[34] In April, Spanish students protested against the actions of the Franco regime in sanctioning a mass for Adolf Hitler. At the beginning of spring the University of Madrid was closed for thirty-eight days due to student demonstrations.[26] Students protesting against the military dictatorship were killed in Brazil.[35]		On March 1, a clash known as battle of Valle Giulia took place between students and police in the faculty of architecture in the Sapienza University of Rome. In March, Italian students closed the University of Rome for 12 days during an anti-war protest.[26]		The French May protests started with student protests over university reform and escalated into a month-long protest. The trade unions joined the protest resulting in a general strike.[36]		On March 28, the Military Police of Brazil killed high school student Edson Luís de Lima Souto at a protest for cheaper meals at a restaurant for low-income students. The aftermath of his death generated one of the first major protests against the military dictatorship. On April 20, Enoch Powell made an anti-immigration speech that sparked demonstrations throughout Britain. His Rivers of Blood speech helped define immigration as a political issue and helped legitimize anti-immigration sentiment.[37] On May 24–27, students in Stockholm institute the occupation of the Student Union Building. In October, the Rodney Riots in Kingston, Jamaica, were inspired when the Jamaican government of Hugh Shearer banned Guyanese university lecturer Dr. Walter Rodney from returning to his teaching position at the University of the West Indies. Rodney, a historian of Africa, had been active in the Black power movement, and had been sharply critical of the middle class in many Caribbean countries. Rodney was an avowed socialist who worked with the poor of Jamaica in an attempt to raise their political and cultural consciousness.		
New Students for a Democratic Society (SDS) is a United States student organization representing left wing ideals. It takes its name and inspiration from the original SDS of 1960–1969, then the largest radical student organization in US history. The contemporary SDS is a distinct youth and student-led organization with over 120 chapters worldwide.[1][2][3][4]						Beginning January 2006, a movement to revive the Students for a Democratic Society took shape. Two high school students, Jessica Rapchik and Pat Korte, decided to reach out to former members of the "Sixties" SDS, to re-establish a student movement in the United States.[1] Korte did this by contacting Alan Haber.[4] They called for a new generation of SDS, to build a radical multi-issue organization grounded in the principle of participatory democracy. Several chapters at various colleges and high schools were subsequently formed. On Martin Luther King, Jr. Day of 2006, these chapters banded together to issue a press release that stated their intentions to recreate the national SDS organization.[5] In the press release, the SDS called for the organization's first national convention since 1969 to be held in the summer of 2006 and to have it preceded by a series of regional conferences occurring during the Memorial Day weekend. These regional conferences would also be the first of their kind since 1969. On April 23, 2006, SDS held a northeast regional conference at Brown University.		Since its foundation in 2006, the organization's activities have centered on two broad areas. The first is building opposition to the US government-led wars in Iraq, Afghanistan, and neighboring countries in South Asia and the Middle East. The second area is advocating for "students rights", broadly defined. For example, SDSers have played a major role in the national movement against budget cuts and tuition hikes in education in the recent period.[6] Practically speaking, each SDS chapter is additionally also involved in a variety of local issues having relevance to their particular area.		SDS is a chapter based organization.[7] Individuals belong to particular chapter, and a national working committee coordinates national campaigns and communications between the chapters. Working groups form on an as-needed basis and give reports back to the national working committee. Certain caucuses based on specific historical oppressions fulfill an ombudsman-like role, in addition to acting in support and networking capacities for the organization generally.[8]		The new SDS has organized and participated in numerous actions against the Iraq War and made clear its opposition to any possible military action against Iran by the US. The Pace University chapter of SDS protested against a speech by Bill Clinton held at the University's New York City campus, prompting the university to hand over two students, Lauren Giaccone and Brian Kelly, to the United States Secret Service. After the threatened expulsion of the two protesters, Pace SDS began a campaign that helped pressure the President of Pace to resign.[9]		Beginning in March and continuing into April and May 2006, SDS chapters across the country participated in a series of actions supporting Immigrant Rights. SDS chapters, such as at Brandeis, Connecticut College, and Harvard coordinated with large coalitions of students to strike and walk out of their classes on May Day.		The newly formed SDS held its first national convention from August 4 to August 7, 2006 at the University of Chicago.[10]		In early March 2007, SDS members and allies in Tacoma, Washington led a blockade of the Port of Tacoma, where the US military was loading Stryker vehicles onto ships to be transported to Iraq. After confrontations every night for a week, the police broke the human blockade through the use of rubber bullets and pepper spray.		On March 12, 2007, one week before the anniversary of the invasion of Iraq, the New School chapter of SDS held a Campus Moratorium against the Iraq War. Students left classes and proceeded down 5th Avenue to the Chambers Street military recruitment center where they met with the Pace University chapter of SDS. The students entered the Recruitment Center, barricaded the door and held a nonviolent sit-in, effectively closing the recruitment center for about two hours. Twenty members of SDS were arrested and charged with criminal trespassing, a misdemeanor.		On March 17, 2007, SDS groups from across the country met and participated in the March on the Pentagon, in which parts of the SDS contingent along with allies occupied a bridge near The Pentagon. Five demonstrators were arrested.		On March 20, 2007, 83 SDS chapters from around the country held coordinated actions against the Iraq war.[11] One such action in the Bay Area shut down the entrance to Chevron's World Headquarters.[11]		The Summer of 2007 was a critical turning point for SDS as a national organization. First, SDS fielded a large contingent at the first US Social Forum in Atlanta on June 27 – July 1. SDS found itself part of a national movement to change the US; at the forum, SDS members gave workshops, demonstrated, and formed bonds with members from across the country.		The second SDS National Convention took place July 27–30, 2007 at Wayne State University in Detroit, Michigan. Approximately 200 members of SDS attended what was a constitutional convention. The primary focus of the convention was to democratically create a national structure and vision for the organization. These goals were achieved, though all decisions made at the convention will be sent back to the SDS chapters for a process of ratification which is currently under way.		The first national SDS Action Camps[12] took place from August 13–16 in Lancaster, Pennsylvania. The camp was hosted by the Lancaster chapter of SDS. It included anti-oppression/collective liberation trainings, and workshops about a variety of things – including media skills, meeting facilitation, and direct action. The camp was held in order to provide students with skills needed to become better organizers, and deepen the sophistication of their vision and strategy.		On September 15, 2007, SDS chapters from several colleges across the country (including Ohio, Indiana, Washington D.C., Harrisburg, PA and New York) gathered and marched in the ANSWER coalition march from the White House steps, to the Capitol building. The protest was estimated to include up 80,000 people. At least 150 were arrested, and there was at least one incident where police pepper-sprayed protesters.[13]		In early November 2007, SDS members were again present at a similar blockade at the Port of Olympia, Washington. The blockade was broken only after 67 arrests, as well as use of pepper spray, rubber bullets, and other crowd-control weapons. A similar confrontation had occurred in May 2006 at the Port of Olympia.		Members and Chapters around the US and Canada participated in a large series of semi-coordinated events and demonstrations between March 17 and March 21 to bring awareness to the 5th anniversary of the invasion of Iraq.[14]		The 2008 National Convention was held in College Park, Maryland. Members at the meeting decided on a national structure: the National Work Committee and a national campaign: Student Power for Accessible Education.		In September, SDS chapters from around the country converged on St. Paul, Minnesota to participate in the four days of protests against the Republican National Convention.[15][16][17]		Members of Providence SDS took over a board meeting of the Rhode Island Public Transit Authority RIPTA to protest proposed route cuts. The group also argues that the RIPTA board is detached from its riders and doesn’t represent them.[18]		The University of North Texas and several other chapters opened. In 2008, the University of Houston opened a chapter and added to the efforts of immigrant rights actions that Texas Grassroots Leadership had begun in 2006,[19] holding many protests centered on detention centers in Texas, particularly the family detention center T. Don Hutto that incarcerated immigrant mothers with children in Taylor, the center in Raymondville and Houston's Processing Center who's in contract with ICE.[20] These efforts across Texas saw a big win when the T. Don Hutto detention center changed its policies and stopped incarcerating children in late 2009. SDS at the University of Houston in Houston, Texas has continued the protests of these detention centers and plans for more in 2010. New efforts in Texas SDS chapters are being made to support the DREAM Act, as well as 2010's May Day.		SDS at the University of Houston also participated in the March 4 National Day of Action to Defend Education [21] along with SDS chapters nationwide [22][23] as well as national anti-war,[24] anti-occupation and Israeli apartheid Week campaigns.		In March 2010, members of the University of Wisconsin Milwaukee's chapter of SDS staged a protest outside the Chancellor's building. The event, designed to protest rising tuition costs, was met with a police presence. Police began using pepper spray, and arrested sixteen members of the protest, including both SDS members and allied organizations on campus through the Education Rights Campaign.[25]		On August 19, 2012, Occupy Colleges officially merged with SDS.[26]		
A student or pupil is a learner or someone who attends an educational institution. In Britain, those attending university are termed "students". In the United States, and more recently also in Britain, the term "student" is applied to both categories: school and university students. In its widest use, student is used for anyone who is learning, including mid-career adults who are taking vocational education or returning to university. When speaking about learning outside an institution, "student" is also used to refer to someone who is learning a topic or who is "a student of" a certain topic or person. In the widest sense of the word, a student is anyone seeking to learn or to grow by experience, such as a student of the School of Hard Knocks.						In Nigeria, education is classified into four system known as 6-3-3-4 system of education. It implies six years in primary school, three years in junior secondary, three years in senior secondary and four years in the university. However, the number of years to be spent in university is mostly determined by the course of study. Some courses have longer study length than others. Those in primary school are often referred to as pupils. Those in university, as well as those in secondary school, are being referred to as students.[citation needed]		Six years of primary school education in Singapore is compulsory.[1]		There are also schools which have the integrated program, such as River Valley High School (Singapore), which means they stay in the same school from Secondary 1 to Junior College 2, without having to take the "O" level examinations which most students take at the end of Secondary school.		International Schools are subject to overseas curriculums, such as the British, American, Canadian or Australian Boards.		Primary education is compulsory in Bangladesh. It's a near crime to not to send children to primary school when they are of age. But it is not a punishable crime (sending children to work instead of school is a crime). Because of the socio-economic state of Bangladesh, child labour is sometimes legal. But the guardian must ensure the primary education. Everyone who is learning in any institute or even online may be called student in Bangladesh. Sometimes students taking undergraduate education is called undergraduates and students taking post-graduate education may be called post-graduates.		Education System Of Bangladesh:		Education is free in Brunei. Darussalam not limited to government educational institutions but also private educational institutions. There are mainly two types of educational institutions: government or public, and private institutions. Several stages have to be undergone by the prospective students leading to higher qualifications, such as Bachelor's Degree.		It takes six and five years to complete the primary and secondary levels respectively. Upon completing these two crucial stages, students/pupils have freedom to progress to sixth-form centers, colleges or probably straight to employment. Students are permitted to progress towards university level programs in both government and private university colleges.[citation needed]		Education in Cambodia is free for all the students who study in Primary School, Secondary School or High School.		After basic education, students can opt to take a bachelor's (undergraduate) degree at a higher education institution (i.e. a college or university), which normally lasts for four years though the length of some courses may be longer or shorter depending on the institution.		In Nepal 12-year school is categorized in three stages: Primary school, Secondary school and Higher Secondary school. For college it averages 4 years for bachelor's degree (except MBBS which is 5 and half years programme) and 2 years master's degree.		In Pakistan, 12-year school is categorized in three stages: Primary school, Secondary school and Higher Secondary school. It takes 5 years for a student to graduate from Primary school, 5 years for Secondary school and 2 years for Higher Secondary school (also called College). Most bachelor's degrees span over four years, followed by 2 years master's degree.[citation needed]		The Philippines is currently in the midst of a transition to a K-12 (also called K+12) basic education system.[3][4][5] Education ideally begins with one year of kinder. Once the transition is complete, elementary or grade school comprises grades 1 to 6. Although the term student may refer to learners of any age or level, the term 'pupil' is used by the Department of Education to refer to learners in the elementary level, particularly in public schools. Secondary level or high school comprises two major divisions: grades 7 to 10 will be collectively referred to as 'junior high school', whereas grades 11 to 12 will be collectively referred to as 'senior high school'. The Department of Education refers to learners in grade 7 and above as students.		After basic education, students can opt to take a bachelor's (undergraduate) degree at a higher education institution (i.e. a college or university), which normally lasts for four years though the length of some courses may be longer or shorter depending on the institution.[citation needed]		In Iran 12-year school is categorized in 2 stages: Elementary school and High school. It takes 6 years for a student to graduate from elementary school and 5 years for high school and 1 year for pre-university college( independed on your field) . After graduating from high school(or pre-university if needed), students acquire a diploma. Having a diploma, a student can participate in the Iranian University Entrance Exam or Konkoor. The university entrance exam is conducted every year by National Organization of Education Assessment,[6] an organization under the supervision of the Ministry of Science, Research and Technology.[7] Members of the Bahá'í religion, a much-persecuted minority are officially forbidden to attend university, in order to prevent members of the faith becoming doctors, lawyers or other professionals.		In Australia, Pre-school is optional for three and four year olds. At age five, children begin compulsory education at Primary School, known as Kindergarten in New South Wales, Preparatory School (prep) in Victoria, and Reception in South Australia, students then continue to year one through six (ages 6 to 12). Before 2014, primary school continued on to year seven in Western Australia, South Australia and Queensland. However, the state governments agreed that by 2014, all primary schooling will complete at year six. Students attend High School in year seven through twelve (ages 13 - 18). After year twelve, students may attend tertiary education at University or vocational training at TAFE (Technical and Further Education).		In New Zealand, after kindergarten or pre-school, which is attended from ages three to five, children begin primary school, 'Year One', at five years of age. Years One to Six are Primary School, where children commonly attend local schools in the area for that specific year group. Then Year Seven and Year Eight are Intermediate, and from Year Nine until Year Thirteen, a student would attend a secondary school or a college. Primary School children are known as Pupils and Intermediate/Secondary School children are known as Students.[citation needed]		Europe uses the traditional, first form, second form, third form, fourth form, fifth form and six form grade system which is up to age eleven.[citation needed]		In Finland a student is called "opiskelija" (plural being 'opiskelijat'), though children in compulsory education are called "oppilas" (plural being 'oppilaat'). First level of education is "esikoulu" (literally 'preschool'), which used to be optional, but has been compulsory since the beginning of year 2015. Children attend esikoulu the year they turn six, and next year they start attending "peruskoulu" (literally "basic school", corresponds to American elementary school, middle school and junior high), which is compulsory. Peruskoulu is divided to "alakoulu" (years 1 through 6) and "yläkoulu" (years 7 through 9). After compulsory education most children attend second level education (toisen asteen koulutus), either lukio (corresponds to high school) or ammattikoulu (Vocational School), at which point they are called students (opiskelija). Some attend "kymppiluokka", which is a retake on some yläkoulu's education.[citation needed]		To attend ammattikorkeakoulu (University of applied sciences) or a university a student must have a second level education. The recommended graduation time is five years. First year students are called "fuksi" and students that have studied more than five years are called "N:nnen vuoden opiskelija" (Nth year student).		In some French schools, a bleu or "bizuth" is a first-year student. Second-year students are sometimes called "carrés" (squares). Some other terms may apply in specific schools, some depending on the classe préparatoire aux grandes écoles attended. The generic term "Étudiant" (lit. Student) applies specifically to someone attending a University or a school of a similar level, that is to sat pupils in a cursus reserved to people already owning a Baccalauréat.[citation needed]		In Germany, the German cognate term Student (male) or "Studentin" (female) is reserved for those attending a university. University students in their first year are colloquially called Ersties ("firsties"). Different terms for school students exist, depending on which kind of school is attended by the student. The general term for a person going to school is Schüler or Schülerin. Students attending a university preparatory school are called Gymnasiasten, while those attending other schools are called Hauptschüler or Realschüler. Students who graduate with the Abitur are called Abiturienten. The abbreviation stud. + the abbreviation of the faculty p. e. phil. for philosophiae is a post-nominal for all students of a baccalaureus course. The abbreviation cand. for candidatus + the abbreviation of the faculty is given as a post-nominal to those close to the final exams. First name surname, stud. phil. or First name surname, cand. jur.[citation needed]		In Ireland, pupils officially start with primary school which consists of eight years: junior infants, senior infants, first class to sixth class (ages 5–11). After primary school, pupils proceed to the secondary school level. Here they first enter the junior cycle, which consists of first year to third year (ages 12–15). At the end of third year, all students must sit a compulsory state examination called the Junior Certificate. After third year, pupils have the option of taking a "transition year" or fourth year (usually at age 15-16). In transition year pupils take a break from regular studies to pursue other activities that help to promote their personal, social, vocational and educational development, and to prepares them for their role as autonomous, participative and responsible members of society. It also provides a bridge to enable pupils to make the transition from the more dependent type of learning associated with the Junior Cert. to the more independent learning environment associated with the senior cycle.[citation needed]		After the junior cycle pupils advance to the senior cycle, which consists of fifth year and sixth year (usually ages between 16 and 18). At the end of the sixth year a final state examination is required to be sat by all pupils, known as the Leaving Certificate. The Leaving Cert. is the basis for all Irish pupils who wish to do so to advance to higher education via a points system. A maximum of 600 points can be achieved. All higher education courses have a minimum of points needed for admission.[citation needed]		At Trinity College, Dublin under-graduate students are formally called "junior freshmen", "senior freshmen", "junior sophister" or "senior sophister", according to the year they have reached in the typical four year degree course. Sophister is another term for a sophomore, though the term is rarely used in other institutions and is largely limited to Trinity College Dublin.		At university, the term "fresher" is used to describe new students who are just beginning their first year. The term, "first year" is the more commonly used and connotation-free term for students in their first year. The week at the start of a new year is called "Freshers' Week" or "Welcome Week", with a programme of special events to welcome new students. An undergraduate in the last year of study before graduation is generally known as a "finalist."		In Italian, a matricola is a first-year student. Some other terms may apply in specific schools, some depending on the liceo classico or liceo scientifico attended.		According to the goliardic initiation traditions the grades granted (following approximately the year of enrollment at university) are: matricola (freshman), fagiolo (sophomore), colonna (junior), and anziano (senior), but most of the distinctions are rarely used outside Goliardia.		In Sweden, only those studying at university level are called students (student, plural studenter). To graduate from upper secondary school (gymnasium) is called ta studenten (literally "to take the student"), but after the graduation festivities, the graduate is no longer a student unless he or she enrolls at university-level education. At lower levels, the word elev (plural elever) is used. As a general term for all stages of education, the word studerande (plural also studerande) is used, meaning 'studying [person]'.		Traditionally, the term "student" is reserved for people studying at university level in the United Kingdom.		At universities in the UK, the term "fresher" is used informally to describe new students who are just beginning their first year. Although it is not unusual to call someone a fresher after their first few weeks at university, they are typically referred to as "first years" or "first year students".		The ancient Scottish University of St Andrews uses the terms "bejant" for a first year (from the French "bec-jaune" – "yellow beak", "fledgling"). Second years are called "semi-bejants", third years are known as "tertians", and fourth years, or others in their final year of study, are called "magistrands".		In England and Wales, primary school begins with an optional "nursery" year followed by reception and then move on to "year one, year two" and so on until "year six". In state schools, children join secondary school when they are 11–12 years old in what used to be called "first form" and is now known as "year 7". They go up to year 11 (formerly "fifth form") and then join the sixth form, either at the same school or at a separate sixth form college. A pupil entering a private, fee-paying school (usually at age 13) would join the "third form" — equivalent to year 9. Many schools have an alternate name for first years, some with a derogatory basis, but in others acting merely as a description — for example "shells" (non-derogatory) or "grubs" (derogatory).		In Northern Ireland and Scotland, it is very similar but with some differences. Pupils start off in nursery or reception aged 3 to 4, and then start primary school in "P1" (P standing for primary) or year 1. They then continue primary school until "P7" or year 7. After that they start secondary school at 11 years old, this is called "1st year" or year 8 in Northern Ireland, or "S1" in Scotland. They continue secondary school until the age of 16 at "5th year", year 12 or "S5", and then it is the choice of the individual pupil to decide to continue in school and (in Northern Ireland) do AS levels (known as "lower sixth") and then the next year to do A levels (known as "upper sixth"). In Scotland, students aged 16–18 take Highers, followed by Advanced Highers. Alternatively, pupils can leave and go into full-time employment or to start in a technical college.		Large increases in the size of student populations in the UK and the effect this has had on some university towns or on areas of cities located near universities have become a concern in the UK since 2000. A report by Universities UK, "Studentification: A Guide to Opportunities, Challenges and Practice" (2006) has explored the subject and made various recommendations.[8] A particular problem in many locations is seen as the impact of students on the availability, quality and price of rented and owner-occupied property.		Education in Canada is within the constitutional jurisdiction of the provinces, and the overall curriculum is overseen by the provincial governments. As there is no overall national coordinating authority, the way the educational stages are grouped and named differs from region to region. Education is generally divided into primary education, followed by secondary education, and post-secondary education. Primary and secondary education are generally divided into numbered grades from 1 to 12, although the first grade may be preceded by kindergarten (optional in many provinces). Ontario and Quebec offer a pre-kindergarten, called a "junior kindergarten" in Ontario, and a "garderie" in Quebec.		Education in Ontario once involved an Ontario Academic Credit (OAC) as university preparation, but that was phased out in 2007, and now all provinces except Quebec have 12 grades. The OAC was informally known as "grade 13" and the name was also used to refer to the students who took it.		Education in Quebec differs from the other provinces in that it has an école primaire (literally "primary school") consisting of grades 1-6, and an école secondaire (literally "secondary school") consisting of secondaries I-V. Secondaries I-V are equivalent to grades 7-11. A student graduating from high school (grade 11) can then either complete a three-year college program or attend a two-year pre-university program required before attending university. In some English High Schools, as well as in most French schools, high school students will refer to secondary 1-5 as year one through five. So if someone in Secondary three is asked "what grade/year are you in?" they will reply "three" or "sec 3". It is presumed that the person asking the question knows that they are not referring to "Grade 3" but rather "Secondary 3". This can be confusing for those outside of Quebec.		In some provinces, grades 1 through 6 are called "elementary school", grades 6 to 8 are called "middle school" or "junior high school", and grades 9 to 12 are considered high school. Other provinces, such as British Columbia, mainly divide schooling into elementary school (Kindergarten to grade 7) and secondary school (grades 8 through 12). In Alberta and Nova Scotia, elementary consists of kindergarten through grade 6. Junior high consists of Grades 7-9. High school consists of Grades 10-12. In English provinces, the high school (known as academy or secondary school) years can be referred to simply as first, second, third and fourth year. Some areas call it by grade such as grade 10, grade 11 and grade 12.		The difference between college and university is significantly different from in the United States or even the United Kingdom. A Canadian college is more similar to an American community college but also the British, French and other European and British Commonwealth such as Australian and New Zealand etc., on the other hand. In contrast, a Canadian university is also quite comparable to an American university as well as many other universities among the English-speaking world and Francosphere. In Canada, colleges are generally geared for individuals seeking applied careers, while universities are geared for individuals seeking more academic careers.		University students are generally classified as first, second, third or fourth-year students, and the American system of classifying them as "freshmen", "sophomores", "juniors" and "seniors" is seldom used or even understood in Canada. In some occasions, they can be called "senior ones", "twos", "threes" and "fours".		In the United States, the first official year of schooling is called kindergarten, which is why the students are called kindergarteners. Kindergarten is optional in most states, but few students skip this level. Pre-kindergarten, also known as "preschool" (and sometimes shortened to "Pre-K") is becoming a standard of education as academic expectations for the youngest students continue to rise. Many public schools offer pre-kindergarten programs.		In the United States there are 12 years of mandatory schooling. The first eight are solely referred to by numbers (e.g. 1st grade, 5th grade) so students may be referred to as 1st graders, 5th graders, then once in middle school before high school you are ratio referred to as 6th, 7th, 8th graders. Upon entering high school, grades 9 through 12 (high school) also have alternate names for students, namely freshman, sophomore, junior and senior. The actual divisions of which grade levels belong to which division (whether elementary, middle, junior high or high school) is a matter decided by state or local jurisdictions.		Accordingly, college students are often called Freshmen, Sophomores, Juniors and Seniors (respectively), unless their undergraduate program calls for more than the traditional 4 years.		The first year of college or high school is referred to as Freshman year. A freshman (slang alternatives that are usually derogatory in nature include "fish", "new-g", "fresher", "frosh", "newbie", "freshie", "snotter", "fresh-meat", "skippie", etc.) is a first-year student in college, university or high school.		In the U.S., a sophomore, also called a "soph," is a second-year student. Outside the United States, the term Sophomore is rarely used, with second-year students simply called "second years". Folk etymology indicates that the word means "wise fool"; consequently "sophomoric" means "pretentious, bombastic, inflated in style or manner; immature, crude, superficial" (according to the Oxford English Dictionary). It is widely assumed to be formed from Greek "sophos", meaning "wise", and "moros" meaning "foolish", although the etymology suggests an origin from the now-defunct "sophumer", an obsolete variant of "sophism".[9]		In the U.S., a Junior is a student in the penultimate (usually third) year and a Senior is a student in the last (usually fourth) year of college, university, or high school. A student who takes more than the normal number of years to graduate is sometimes referred to as a "super senior".[10] This term is often used in college, but can be used in high school as well. The term Underclassman is used to refer collectively to Freshmen and Sophomores, and Upperclassman to refer collectively to Juniors and Seniors, sometimes even Sophomores. The term Middler is used to describe a third-year student of a school (generally college) that offers five years of study. In this situation, the fourth and fifth years would be referred to as Junior and Senior years, respectively, and the first two years would be the Freshman and Sophomore years.		A graduate student is a student who continues his/her education after graduation. Some examples of graduate programs are: business school, law school, medical school, and veterinary school. Degrees earned in graduate programs include the Master’s degree, a research doctoral degree, or a first professional degree.		Students attending vocational school focus on their jobs and learning how to work in specific fields of work. A vocational program typically takes much less time to complete than a four-year degree program, lasting 12–24 months.[11] Liberal Arts that are required in four-year Universities are less important to these students because the skills necessary for their careers take precedence in order for a timely completion of the program.		Students have their own current of politics and activism on and off campus. The student rights movement has centered itself on the empowerment of students similar to the labor movement.		A mature, non-traditional, or adult student in tertiary education (at a university or a college) is normally classified as an (undergraduate) student who is at least 21–23 years old at the start of their course and usually having been out of the education system for at least two years. Mature students can also include students who have been out of the education system for decades, or students with no secondary education. Mature students also make up graduate and postgraduate populations by demographic of age.		University students have been associated with pranks and japes since the creation of universities in the Middle Ages.[12][13][14][15][16] These can often involve petty crime, such as the theft of traffic cones and other public property,[17] or hoaxes. It is also not uncommon for students from one school to steal or deface the mascot of a rival school.[18] In fact, pranks play such a significant part in student culture that numerous books have been published that focus on the issue.[19][20]		"Freshman" and "sophomore" are sometimes used figuratively, almost exclusively in the United States, to refer to a first or second effort ("the singer's sophomore album"), or to a politician's first or second term in office ("freshman senator") or an athlete's first or second year on a professional sports team. "Junior" and "senior" are not used in this figurative way to refer to third and fourth years or efforts, because of those words' broader meanings of "younger" and "older." A junior senator is therefore not one who is in a third term of office, but merely one who has not been in the Senate as long as the other senator from their state. Confusingly, this means that it is possible to be both a "freshman Senator" and a "senior Senator" simultaneously: for example, if a Senator wins election in 2008, and then the other Senator from the same state steps down and a new Senator elected in 2010, the former Senator is both senior Senator (as in the Senate for two years more) and a freshman Senator (since still in the first term).		International Students' Day (17th November) remembers the anniversary of the 1939 Nazi storming of the University of Prague after student demonstrations against the German occupation of Czechoslovakia. Germans closed all Czech universities and colleges, sent over 1200 students to Nazi concentration camps, and had nine student leaders executed (on November 17).[21]		Techtech		
The classes préparatoires aux grandes écoles (CPGE) (English: Higher School Preparatory Classes), commonly called classes prépas or prépas, are part of the French post-secondary education system. They consist of two very intensive years (extendable to three or exceptionally four years) which act as a preparatory course (or cram school) with the main goal of training undergraduate students for enrollment in one of the grandes écoles. The workload is one of the highest in the world[1] (between 35 and 45 contact hours a week, plus usually between 4 and 6 hours of written exams, plus between 2 and 4 hours of oral exams a week and homework filling all the remaining free time[2]).		The students from CPGE have to take national competitive exams to be allowed to enroll in one of the Grandes Écoles. These Grandes Écoles are higher education establishments (graduate schools) delivering master's degrees and/or doctorates. They include science and engineering schools, business schools, the four veterinary colleges and the four écoles normales supérieures but do not include medical institutes or architecture institutes. Their competitive entrance exams make having attended one of the grandes écoles being often regarded as a status symbol as they have traditionally produced most of France's scientists, executives and intellectuals (École Polytechnique, écoles normales supérieures, ParisTech Schools, Centrale schools...).		Hence, there are three kinds of different prépas: scientific, economic and literary CPGE. Each of them prepare to pass the competitive exams of those grandes écoles.						The CPGE are located within high schools for historical reasons (Napoleon created them at first as fourth to sixth year of high school) but pertain to tertiary education, which means that each student must have successfully passed their baccalauréat (or equivalent) to be admitted to CPGE. Moreover, the admission to the CPGE is usually based on performance during the last two years of high school, called première and terminale. Thus, each CPGE receives hundreds of applications from around the world every April and May, and selects its new students under its own criteria (mostly excellence). A few CPGE programmes, mainly the private CPGEs (which account for 10% of CPGEs), also have an interview process or look at a student's involvement in the community.		In June 2007, 534,300 students passed the "Baccalauréat", and 40,000 (7.5%)[3] of them were admitted to CPGE. On a given class at one of the prep schools listed above, around 1500 application files will be examined for only 40 places.[4] Students are selected according to their grades in High school and the first part of "Baccalauréat" (equivalent to A-levels in the United Kingdom or Advanced Placement in the United States).		Preparatory classes are officially not authorized to deliver any degrees, but they give ECTS (university equivalence) since the 2009-2010 academic year, and students who decide to can carry on their studies at university.[5]		However, many prépas also establish conventions with universities to validate a full 2nd or 3rd year degree for CPGE students who did their job well, especially in literary prépas ("khâgne"). Most of the students in these classes continue their cursus at the university, so the teachers' council can deliver them the corresponding grade in one or two disciplines at the end of the year (only up to a bachelor's degree for 3 years of CPGE).		CPGE exist in three different fields of study: business, science & engineering, and humanities. All CPGE programs have a nominal duration of two years, but the second year is sometimes repeated once.		Those CPGEs which are focused on economics (which prepare the admission to business schools such as HEC Paris, ESSEC, ESCP Europe, EDHEC Business School, EM Lyon or Skema Business School) are known as Prépa HEC and are split into three parts:		Hours/week		Classe préparatoire ECS are for those who graduated with the general Baccalauréat S (Scientific), Classe préparatoire ECE are for those who were in the economics section at their Lycée (and received the general Baccalauréat ES (Economics and Social)), whiles the Classe préparatoire ECT are for those who obtained a Baccalauréat Technologique .		However, both the first and the second year programmes include ten hours of mathematics teaching per week and also six hours of business history and geography, six hours of French and philosophy, and three hours of each language (2 languages) in the "ECS" section.		There is also the D1 and D2 CPGE, also known as ENS Cachan CPGE:		D1 and D2 are very rare but offer a complete and multidisciplinary training.		The oldest CPGEs are the scientific ones, which can be accessed only by scientific Bacheliers. The different tracks are the following:		The classes that especially train students for admission to the elite schools, such as Écoles Normales Supérieures or ParisTech schools, have an asterisk added to their name. For example, MP*, are usually called MP étoile ("MP star") (except for the BCPST2 and TB2 classes, which all prepare to the elite schools).		Both the first and second year programmes include as much as ten to twelve hours of mathematics teaching per week, ten hours of physics, two hours of literature and philosophy, two to four hours of (one or two) foreign language(s) teaching and two to eight hours of minor options: either SI, engineering industrial science, chemistry or theoretical computer science (including some programming using the "Pascal", "CaML" or Python programming languages, as practical work), biology-geology, biotechnologies.[9] Added to this are several hours of homework, which can amount to as much as the official hours of class.		The BCPST classes prepare for exams of engineering schools of life sciences (agronomy, forestry, environmental and food sciences) but also to veterinary schools, engineering schools of earth sciences, and the three Ecoles Normales Supérieures. Compare to the other classes, it teaches biology and geology.		In scientific CPGE, the first year of CPGE is usually called the maths sup, or hypotaupe (sup for "classe de mathématiques supérieures", superior in French, meaning post-high school), and second year maths spé, or taupe, (spés for "classe de mathématiques spéciales", special in French). The students of these classes are called taupins.		The word taupe means "mole" in French. Its signification comes from the lifestyle of students in classes preparatoires. The intensive workload makes students sacrifice their social life for the classes preparatoires years. They spend most of their time studying inside and barely go outside.		A very specific kind of CPGE is targeting technicians. They are called ATS classes, Adaptation Techniciens Supérieurs ("Adaptation for Skilled Workers ") and last only a year. They are mainly based on the curriculum of PTSI and PCSI, but the courses are summed up.		The literary and humanities CPGEs are focused on a strong pluri-disciplinary course, including all humanities: philosophy, literature, history, geography, foreign languages, and ancient languages (Latin and Ancient Greek). These prépas also have their own nicknames: "hypokhâgne" for the first year and "khâgne" for the second year. The students are called the "hypokhâgneux" and the "khâgneux". These classes prepare for the entrance exam of the elite schools called Écoles Normales Supérieures, which are considered among the most difficult exams of the French system. Nevertheless, the students can now also apply for many other entrance exams.		There are three types of Khâgne:		Now, the grouping of many examinations make the difference between khâgnes "Lyon" and "Ulm" is slight, and lots of prépas have mixed classes with many students preparing for both ENS (or even the three for students specialising in English).		Khâgneux can apply to many grandes écoles, other high schools and all universities, among which are the following:		The amount of work required of the students is exceptionally high.[10]		In addition to class time and homework, students spend several hours each week completing exams and colles (very often written "khôlles" to look like a Greek word, this way of writing being initially a "khâgneux" joke). The so-called "colles" are unique to French academic education in CPGEs. They consist of oral examinations twice a week, in maths, physics, chemistry, biology and earth sciences (in BCPST classes), French and a foreign language, usually English, German or Spanish. Students, usually in groups of three, spend an hour facing a professor alone in a room, answering questions and solving problems. In "Prépa ECE/ECS", students are taken every two weeks in maths, history, philosophy, and in their two chosen languages (usually English and Spanish/German).		In "hypokhâgne/khâgne", the system of "colles" is a bit different. They are taken every quarter in every subject. Students usually have one hour to prepare a short presentation that takes the form of a French-style dissertation (a methodologically codified essay, typically structured in three parts: thesis, counter-thesis, and synthesis) in history, philosophy, etc. on a given topic, or that of a commentaire composé (a methodologically codified commentary) in literature and foreign languages; as for the Ancient Greek or Latin, they involve a translation and a commentary. The student then has 20 min to present her or his prepared work (so just one part of their work) to the teacher, who ends the session by asking some questions on the presentation and on the corresponding topic.		"Khôlles" are important as they prepare the students, from the very first year, for the oral part of the competitive examination.		A student (in a scientific CPGE) who repeats the second year obtains the status of cinq demis ("five halves"). They were only trois demis ("three halves") during their first second-year and un demi ("one half") in the first year. The explanation behind these names is that the most coveted engineering school is the École polytechnique, nicknamed the "X" (as the mathematical unknown). A student who enrolls in (the word for which is "integrates" in French) this school after the second year of preparatory class is traditionally called a "3/2" because this is the value of the integral of x from 1 to 2.		∫ 1 2 x d x   = 2 2 2 − 1 2 2 = 3 2 {\displaystyle \int _{1}^{2}\!x\,dx\ ={\frac {2^{2}}{2}}-{\frac {1^{2}}{2}}={\frac {3}{2}}}		The same idea is valid for cinq demis: the integral of x from 2 to 3 is "5/2".		∫ 2 3 x d x   = 3 2 2 − 2 2 2 = 5 2 {\displaystyle \int _{2}^{3}\!x\,dx\ ={\frac {3^{2}}{2}}-{\frac {2^{2}}{2}}={\frac {5}{2}}}		Students in their first year of literary and business CPGEs are called bizuths and, in their second year, carrés ("squares"). Students enrolled in their "second" second year are also called cubes (or "Khûbes"), and a few turn to bicarrés for a third and final second year. Some ambitious professors encourage their top students to avoid or postpone admittance to other prestigious schools in order to try to get a better school.				
Democratic education is an educational ideal in which democracy is both a goal and a method of instruction. It brings democratic values to education and can include self-determination within a community of equals, as well as such values as justice, respect and trust. Democratic education is often specifically emancipatory, with the students' voices being equal to the teacher's.[1]						The history of democratic education spans from at least the 1600s. While it is associated with a number of individuals, there has been no central figure, establishment, or nation that advocated democratic education.[2]		In 1693, John Locke published Some Thoughts Concerning Education. In describing the teaching of children, he declares,		None of the things they are to learn, should ever be made a burthen to them, or impos'd on them as a task. Whatever is so propos'd, presently becomes irksome; the mind takes an aversion to it, though before it were a thing of delight or indifferency. Let a child but be order'd to whip his top at a certain time every day, whether he has or has not a mind to it; let this be but requir'd of him as a duty, wherein he must spend so many hours morning and afternoon, and see whether he will not soon be weary of any play at this rate.[3]		Jean-Jacques Rousseau’s book of advice on education, Émile, was first published in 1762. Émile, the imaginary pupil he uses for illustration, was only to learn what he could appreciate as useful.[4] He was to enjoy his lessons, and learn to rely on his own judgement and experience. “The tutor must not lay down precepts, he must let them be discovered,”[5] wrote Rousseau, and urged him not make Émile learn science, but let him discover it.[6] He also said that we should not substitute books for personal experience because this does not teach us to reason; it teaches us to use other people’s reasoning; it teaches us to believe a great deal but never to know anything.[7]		While Locke and Rousseau were concerned only with the education of the children of the wealthy, in the 19th century Leo Tolstoy set up a school for peasant children. This was on his own estate at Yasnaya Polyana, Russia, in the late 19th century. He tells us that the school evolved freely from principles introduced by teachers and pupils; that in spite of the preponderating influence of the teacher, the pupil had always had the right not to come to school, or, having come, not to listen to the teacher, and that the teacher had the right not to admit a pupil, and was able to use all the influence he could muster to win over the community, where the children were always in the majority.[8][9]		In 1912, Janusz Korczak founded Dom Sierot, the Jewish orphanage in Warsaw, which was run on democratic lines until 1940, when he accompanied all his charges to the gas-chambers of the Treblinka extermination camp.[10][11][12]		The oldest democratic school that still exists is Summerhill, in Suffolk, England, founded in 1921. It features voluntary class attendance and a School Meeting with broad powers. In the 1960s, hundreds of "free schools" opened, many based on Summerhill,.[13] However A.S. Neill, the founder of Summerhill, distanced himself from American Summerhill schools for not successfully implementing the philosophy of "Freedom, not license."[14]		Sudbury Valley School, founded in Framingham, Massachusetts in 1968, has full democratic governance: The School Meeting manages all aspects of the school, including staff hiring and facilities.[15] A "Sudbury school" is now a general class of school modeled after this original.		The Democratic School of Hadera, founded in Israel in 1987, is publicly funded. It offers voluntary classes.[16] There are now more than twenty democratic schools in Israel.[17]		Progressive education (including many schools based on Summerhill[18]) became a broad movement in the 1960s and 1970s, but was largely renounced by the 1980s.[19] See: Free school movement		Networks supporting democratic education include:		IDEC 2005 named 2 core beliefs: self-determination and democratic governance.[22] EUDEC has both of these beliefs, and mutual respect is also in their belief statement.[23] IDEN supports schools that self-identify as democratic.[24]		Democratic education, same as a Democracy or a democratic government, comes in many different forms. These are some of the areas in which democratic schools differ.		Democratic schools are characterized by involving students in the decision-making process that affects what and how they learn. Democratic schools have no mandatory curriculum, considering forced learning to be undemocratic. Some democratic schools officially offer voluntary courses, and many help interested students to prepare for national examinations so they gain qualifications for further study or future employment.[25] Some democratic schools have no official offering of courses, although courses can be offered or requested by school members.[26]		Democratic schools often have meetings open to all students and staff, where everyone present has a voice and sometimes an equal vote. Some include parents.[27] These school meetings can cover anything from small matters to the appointment or dismissal of staff and the creation or annulment of rules, or to general expenditure and the structure of the school day. At some schools all students are expected to attend these meetings, at others they are voluntary.[28] The main school meeting may also set up sub-committees to deal with particular issues, such as conflict resolution.[29]		Within the purview of democratic values, there is wide scope for how conflicts are resolved. There may be a formal system, with due process and the rule of law.[29] There may be rules but no punishments.[30] Other possibilities include, but are not limited to, a consensus process, mediation, and informal dialogue.[citation needed]33		Finance: Some democratic learning environments are parent-funded, some charity-funded.[31][32] Schools may have a sliding scale based on family income.[33] Publicly funded democratic schools exist in Canada[34][35] and Israel[36]		Size: Democratic schools vary in size from a few students to a few hundred.[citation needed] Even an individual unschooler can be described as learning democratically, if they are treated with democratic values.		Age range: Age mixing is a deliberate policy in some democratic schools. It may include very young children, even babies.[37] Some democratic schools only enroll older students.[38][39]		Location: Democratic education is not limited to any particular setting. Settings for democratic learning communities include in an office building,[40] on city streets,[41] and in a rural area.[42]		While types of democratic education are as numerous as types of democracy, a general definition of democratic education is "an education that democratizes learning itself."[43] The goals of democratic education vary according to the participants, the location, and access to resources.[44]		There is no unified body of literature, spanning multiple disciplines, on democratic education. However, there are theories of democratic education from the following perspectives:		During the practice theory movement, there was renewed interest in child development. Jean Piaget's theory of universal steps in comprehension and general patterns in the acquisition of knowledge was challenged by experiences at democratic schools. "No two kids ever take the same path. Few are remotely similar. Each child is so unique, so exceptional."[45]		Jean Lave was one of the first and most prominent social anthropologists to discuss cognition within the context of cultural settings presenting a firm argument against the functionalist psychology that many educationalists refer to implicitly. For Lave, learning is a process ungone by an actor within a specific context. The skills or knowledge learned in one process are not generalizable nor reliably transferred to other areas of human action. Her primary focus was on mathematics in context and mathematics education.		The broader implications reached by Lave and others who specialize in situated learning are that beyond the argument that certain knowledge is necessary to be a member of society (a Durkheimian argument), knowledge learned in the context of a school is not reliably transferable to other contexts of practice.		John Locke argues that children are capable of reasoning at a young age: “It will perhaps be wonder’d, that I mention reasoning with children; and yet I cannot but think that the true way of dealing with them. They understand it as early as they do language; and, if I misobserve not, they love to be treated as rational creatures, sooner than is imagin’d,”[46] Rousseau disagreed: “Use force with children and reasoning with men."[47]		Humans are innately curious, and democratic education supports the belief that the drive to learn is sufficiently strong to motivate children to become effective adults.[48]		The human brain is not fully developed until adulthood.[49] A disadvantage of teenagers being responsible for their own education is that "young brains have both fast-growing synapses and sections that remain unconnected. This leaves teens easily influenced by their environment and more prone to impulsive behavior".[50]		Democracy can be valued on ethical grounds.[51]		Democratic education is consistent with the cultural theory that "learning in school must be continuous with life outside of school" and that children should become active participants in the control and organization of their community.[52]		Research on hunter-gatherer societies indicates that free play and exploration were effective transmitters of the societies' culture to children.[53]		According to George Dennison, democratic environments are social regulators: Our desire to cultivate friendships, engender respect, and maintain what George Dennison terms ‘natural authority’ encourages us to act in socially acceptable ways (i.e. culturally informed practices of fairness, honesty, congeniality, etc.).[54]		Children are influenced by many curricula beyond the school curriculum: TV curricula, advertisers' curricula, curricula of religious communities, Girl Scouts and Boy Scouts, encyclopedias etc. and therefore "one of the most significant tasks any school can undertake is to try to develop in youngsters an awareness of these other curricula and an ability to criticize them…it is utter nonsense to think that by turning children loose in an unplanned and unstructured environment they can be freed in any significant way. Rather, they are thereby abandoned to the blind forces of the hucksters, whose primary concern is neither the children, nor the truth, nor the decent future of ... society."[55]		Émile Durkheim argues that the transition from primitive to modern societies occurred in part as elders made a conscious decision to transmit what were deemed the most essential elements of their culture to the following generations. He concludes that modern societies are so complex—much more complex than primitive hunter-gatherer societies—and the roles that individuals must fill in society are so varied, that formal mass-education is necessary to instill social solidarity and what he terms ‘secular morality’.[56]		There are a variety of political components to democratic education. One author identifies those elements as inclusivity and rights, equal participation in decision-making, and equal encouragement for success.[57] The Institute for Democratic Education's principles of democratic education identifies several political principles,		The type of political socialization that takes place in democratic schools is strongly related to deliberative democracy theory. Claus Offe and Ulrich Preuss, two theorists of the political culture of deliberative democracies argue that in its cultural production deliberative democracy requires “an open-ended and continuous learning process in which the roles of both ‘teacher’ and ‘curriculum’ are missing. In other words, what is to be learned is a matter that we must settle in the process of learning itself."[59]		The political culture of a deliberative democracy and its institutions, they argue, would facilitate more “dialogical forms of making one’s voice heard” which would “be achieved within a framework of liberty, within which paternalism is replaced by autonomously adopted self-paternalism, and technocratic elitism by the competent and self-conscious judgment of citizens."[60]		As a curricular, administrative and social operation within schools, democratic education is essentially concerned with equipping people to make "real choices about fundamental aspects of their lives"[61] and happens within and for democracy.[62] It can be "a process where teachers and students work collaboratively to reconstruct curriculum to include everyone."[57] In at least one conception, democratic education teaches students "to participate in consciously reproducing their society, and conscious social reproduction."[63] This role necessitates democratic education happening in a variety of settings and being taught by a variety of people, including "parents, teachers, public officials, and ordinary citizens." Because of this "democratic education begins not only with children who are to be taught but also with citizens who are to be their teachers."[64]		The "strongest, political rationale" for democratic education is that it teaches "the virtues of democratic deliberation for the sake of future citizenship."[65] This type of education is often alluded to in the deliberative democracy literature as fulfilling the necessary and fundamental social and institutional changes necessary to develop a democracy that involves intensive participation in group decision making, negotiation, and social life of consequence.[66]		The concept of the hidden curriculum includes the belief that anything taught in an authoritarian setting is implicitly teaching authoritarianism. Thus civic education, if taught in a compulsory setting, undermines its own lessons in democracy.[67] A common belief in democratic schools is that democracy must be experienced to be learned.[68][69][70] This argument conforms to the cognition-in-context research by Lave.		Another common belief, which supports the practice of compulsory classes in civic education, is that passing on democratic values requires an imposed structure.[71]		Arguments about how to transmit democracy, and how much and how early to treat children democratically, are made in various literatures concerning student voice, youth participation and other elements of youth empowerment.[72][73]		Standard progressive visions of education as collaboration tend to downplay the workings of power in society. If learners are to "develop a democracy," some scholars have argued, they must be provided the tools for transforming the non-democratic aspects of a society. Democracy in this sense involves not just "participation in decision making," a vision ascribed especially to Dewey, but the ability to confront power with solidarity. [74] [75]		Core features of democratic education align with the emerging consensus on 21st century business and management priorities. Such features include increased collaboration, decentralized organization, and radical creativity.[76]		While democratic schools don't have an official curriculum, what each student actually does might be considered their own curriculum.[77] Dewey [78] was an early advocate of inquiry education, in which student questions and interests shaped curriculum, a sharp contrast to the "factory model" that began to predominate education during the 20th century as standardization became a guiding principle of many educational practices. Although there was a resurgence of inquiry education in the 1980s and 1990s [79] the standards movement of the 21st century and the attendant school reform movement have squashed most attempts at authentic inquiry-oriented democratic education practices. The standards movement has reified standardized tests in literacy and writing, neglecting science inquiry, the arts, and critical literacy.		Democratic schools may not consider only reading, writing and arithmetic to be the real basics for being a successful adult.[80] A.S. Neill said "To hell with arithmetic."[81] Nonetheless, there is a common belief that people will eventually learn "the basics" when they develop internal motivation.[82][83] Furthermore, an educator implementing inquiry projects will look at the "next steps" in a student's learning and incorporate basic subject matter as needed. This is easier to accomplish in elementary school settings than in secondary school settings, as elementary teachers typically teach all subjects and have large blocks of time that allow for in-depth projects that integrate curriculum from different knowledge domains.		Allen Koshewa [84] conducted research that highlighted the tensions between democratic education and the role of teacher control, showing that children in a fifth grade classroom tried to usurp democratic practices by using undue influence to sway others, much as representative democracies often fail to focus on the common good or protect minority interests. He found that class meetings, service education, saturation in the arts, and an emphasis on interpersonal caring helped overcome some of these challenges. Despite the challenges of inquiry education, classrooms that allow students to make choices about curriculum propel students to not only learn about democracy but also to experience it.		A striking feature of democratic schools is the ubiquity of play. Students of all ages — but especially the younger ones — often spend most of their time either in free play, or playing games (electronic or otherwise). All attempts to limit, control or direct play must be democratically approved before being implemented.[85] Play is seen as activity every bit as worthy as academic pursuits, often even more valuable. Play is considered essential for learning, particularly in fostering creativity.[86]		It was Invented by Liam Doherty. Interest in learning to read happens at a wide variety of ages.[83] Progressive educators emphasise students' choice in reading selections, as well as topics for writing. In addition, Stephen Krashen [87] and other proponents of democratic education emphasise the role of libraries in promoting democratic education. Others, such as children's author Judy Blume, have spoken out against censorship as antagonistic to democratic education,[88] while the school reform movement, which gained traction under the federal initiative 'No Child Left Behind' and later under 'Race to the Top' and the Common Core Standards movement, emphasise strict control over curriculum.		Democracy must be experienced to be learned.[89]		As English aristocracy was giving way to democracy, Matthew Arnold investigated popular education in France and other countries to determine what form of education suited a democratic age.[90] Arnold wrote that "the spirit of democracy" is part of "human nature itself", which engages in "the effort to affirm one's own essence...to develop one's own existence fully and freely."[91]		During the industrial age, John Dewey argued that children should not all be given the same pre-determined curriculum. In Democracy and Education he develops a philosophy of education based on democracy. He argues that while children should be active participants in the creation of their education, and while children must experience democracy to learn democracy, they need adult guidance to develop into responsible adults.[92]		Amy Gutmann argues in Democratic Education that in a democratic society, there is a role for everyone in the education of children. These roles are best agreed upon through deliberative democracy.[93]		The journal Democracy and Education investigates "the conceptual foundations, social policies, institutional structures, and teaching/learning practices associated with democratic education." By "democratic education" they mean "educating youth...for active participation in a democratic society."[94]		Israel's Institute for Democratic Education and Kibbutzim College in Tel Aviv collaborate to offer a Bachelor of Education (B. Ed.) degree with a Specialization Certificate in Democratic Education. Student teaching placements are in both regular schools and democratic schools.[95]		United Nations agreements both support and place restrictions on education options, including democratic education:		Article 26(3) of the United Nations Universal Declaration of Human Rights states that "Parents have a prior right to choose the kind of education that shall be given to their children."[96] While this in itself may allow parents the right to choose democratic education, Articles 28 and 29 of the United Nations Convention on the Rights of the Child place requirements on educational programs: Primary education is compulsory, all aspects of each student must be developed to their full potential, and education must include the development of respect for things such as national values and the natural environment, in a spirit of friendship among all peoples.[97]		Furthermore, while Article 12(1) of the Convention mandates that children be able to have input on all matters that effect them, their input will have limited weight, "due weight in accordance with the age and maturity of the child."[97]		In 1999, Summerhill received a 'notice of complaint' over its policy of non-compulsory lessons, a procedure which would usually have led to closure; Summerhill contested the notice[98] and went before a special educational tribunal. Summerhill was represented by a noted human rights lawyer, Geoffrey Robertson QC. The government's case soon collapsed, and a settlement was offered. This offer was discussed and agreed at a formal school meeting which had been hastily convened in the courtroom from a quorum of pupils and teachers who were present in court. The settlement guaranteed that future inspections of Summerhill would be consistent with Summerhill's educational philosophy.[99]		
School teachers are commonly the subject of bullying but they are also sometimes the originators of bullying within a school environment. When an adult bullies a child, it is referred to as psychological, emotional, or verbal abuse. According to the American Psychological Association, it is as harmful as sexual or physical abuse. "Children who are emotionally abused and neglected face similar and sometimes worse mental health problems as children who are physically or sexually abused, yet psychological abuse is rarely addressed in prevention programs or in treating victims, according to a new study published by the American Psychological Association."[1]						While teacher bullying is recognized as serious and harmful, there are no statistics.[2] However, according to one article, a high-percentage of teachers admit to bullying students.[3]		Comprehensive research carried out in the UK found that teaching was one of the occupations at highest risk from bullying:[4]		In another survey, the Economic and Social Research Institute found bullying to be more prevalent in schools (13.8pc) than other workplaces (7.9pc).[5]		There are complex issues with reporting bullying by teachers, not only for children, but also parents. By means of their position of power over the child, power that enables them to impact the child's present and future,[6] children and parents are reluctant to report.[7] There are specific signs that parents should watch for as their child is unlikely to disclose that the teacher is in fact the bully.[8]		Furthermore, a teacher who bullies may present as a Jekyll and Hyde figure: they are often celebrated and popular so their abuse can go on for long periods of time undetected.[9] Lacking research on teachers in classrooms, once again it is hard to be sure, but certainly in teaching a sport, we see adults often rewarded for bullying conduct that would never be tolerated or condoned if done by a child.[10]		In a school setting, this is true for teachers in the classroom as well as in their role as coaches of school sports.		Parsons identifies teacher bullying as often being part of a wider bullying culture within a school, with a complex web of dynamics such as:[11]		A common manifestation of teacher bullying is staffroom bullying where teachers are bullied by other teachers or school managers.[5][15][16][17][18][19]		In investigating teacher bullying, it is important to differentiate a teacher or coach who is demanding versus one who is demeaning. So "yelling" for instance can be highly productive and motivating, but if it involves belittling and is laced with putdowns and swearing, it becomes abusive.[20] Bullying by teachers can take many forms in order to harass and intimidate including:[21]		Bullying of teachers can take many forms in order to harass and intimidate including:[23]		Bullies often exploit positions of seniority over the colleagues they are intimidating (see rankism) by:[23]		In some cases, teachers are ignored and isolated by colleagues in the staffroom or turned down for promotion or training courses (see silent treatment).[23] Other times, teachers are ostracized as whistleblowers when they report to administrators on students' reports of bullying being done by their colleagues.[24]		Notably, there is little to no research on teachers bullying students. The power imbalance of teacher to student is greater than peer to peer and may well intensify the impact. Studies of child to child bullying, or parent to child bullying, for the present, must be extrapolated to consider the possible impacts of bullying by teachers which include:		The possible impacts of bullying of teachers include:		In April 2012, Stuart Chaifetz, a father of an autistic boy, released a video on YouTube[36] providing evidence that his son was allegedly the subject of emotional abuse at the hands of his teacher and aide at Horace Mann Elementary School, in the Cherry Hill Public Schools district.[37] The evidence was secured when Chaifetz wired his son before sending him to school. When he listened to the audio recording, according to one news report, "Chaifetz says he caught his son's teachers gossiping, talking about alcohol and violently yelling at students. He took the audio to the Cherry Hill School District, where officials fired one of the teachers involved after hearing the tape. Chaifetz's son was relocated to a new school, where Chaifetz says he is doing well."[38][39] However, it appears that students with learning disabilities may be especially at risk for teacher bullying.[40]		In 2011, select members of the Board, the Chaplain and Headmaster at St. Michaels University School were informed that teachers were abusing students in the basketball program. They received an eleven-page document written by a lawyer, who was also a parent of a student at the school, outlining the incidences of "child abuse" occurring on basketball teams at the Senior School. Parents were not informed; teachers remained in position. Although not knowing about this document, throughout the year, at least five families made significant formal complaints to Board members, the Chaplain and Headmaster about the abusive coaching conduct. In 2012, at least thirteen students, at the request of the Headmaster, approved of detailed, written testimonies about the verbal, emotional and some physical abuse they were suffering at the hands of their teachers who were coaching them as a co-curricular.		How they were treated by the Headmaster, the school's Board of Governors, lawyers hired by the school, and educational authorities was the subject of a front-page story by award-winning investigative journalist, Robert Cribb,[41] as well as a CTV W5 episode.[42] The story was the catalyst for a book, Teaching Bullies: Zero Tolerance on the Court or in the Classroom by Jennifer Fraser, PhD.[43] Fraser's book puts the story in the context of extensive research into the work of psychologists, psychiatrists, and neuroscientists in order to explore the oftentimes taboo subject of Teacher and Coach Bullying.		Informed by research into the serious, extensive, and often irreparable damage to adolescent brains in particular, Fraser has launched an awareness campaign on Facebook[44] and Twitter (@teachingbullies) in an attempt to get lawmakers to put emotional abuse into the criminal code along with sexual and physical abuse. Neuroscientists believe it does similar if not identical harm to developing brains.		In June 2014, Britain proposed the "Cinderella Law" which would put emotional abuse in the Criminal Code.[45]		Teachers being portrayed as bullies have made into popular culture, along with works with teachers being bullied by other teachers, students, and even the principal.		Books		Academic papers		
Coordinates: 53°20′40″N 6°15′28″W﻿ / ﻿53.3444°N 6.2577°W﻿ / 53.3444; -6.2577		The College of the Holy and Undivided Trinity of Queen Elizabeth near Dublin[1]		Trinity College (Irish: Coláiste na Tríonóide) is the sole constituent college of the University of Dublin, a research university in Ireland. The college was founded in 1592 as the "mother" of a new university,[Note 1] modelled after the collegiate universities of Oxford and of Cambridge, but, unlike these, only one college was ever established; as such, the designations "Trinity College" and "University of Dublin" are usually synonymous for practical purposes. It is one of the seven ancient universities of Britain and Ireland,[7] as well as Ireland's oldest university.		Originally it was established outside the city walls of Dublin in the buildings of the dissolved Augustinian Priory of All Hallows. Trinity College was set up in part to consolidate the rule of the Tudor monarchy in Ireland, and it was seen as the university of the Protestant Ascendancy for much of its history. Although Catholics and Dissenters had been permitted to enter as early as 1793,[8] certain restrictions on their membership of the college remained until 1873 (professorships, fellowships and scholarships were reserved for Protestants).[9] From 1871 to 1970, the Catholic Church in Ireland forbade its adherents from attending Trinity College without permission. Women were first admitted to the college as full members in January 1904.[10]		Trinity College is now surrounded by Dublin and is located on College Green, opposite the former Irish Houses of Parliament. The college proper occupies 190,000 m2 (47 acres), with many of its buildings ranged around large quadrangles (known as 'squares') and two playing fields. Academically, it is divided into three faculties comprising 25 schools, offering degree and diploma courses at both undergraduate and postgraduate levels. As of 2016, it was ranked by the Times Higher Education (THE) World University Rankings as the 96th best university in the world, by the QS World University Rankings as the 88th best, by the Academic Ranking of World Universities as within the 151–200 range, and by all three as the best university in Ireland.[11][12][13] The Library of Trinity College is a legal deposit library for Ireland and the United Kingdom, containing over 6.2 million printed volumes and significant quantities of manuscripts (including the Book of Kells), maps and music.						The first University of Dublin (known as the Medieval University of Dublin and unrelated to the current university) was created by the Pope in 1311,[14] and had a Chancellor, lecturers and students (granted protection by the Crown) over many years, before coming to an end at the Reformation.		Following this, and some debate about a new university at St. Patrick's Cathedral, in 1592 a small group of Dublin citizens obtained a charter by way of letters patent from Queen Elizabeth[Note 1] incorporating Trinity College at the former site of All Hallows monastery, to the south east of the city walls, provided by the Corporation of Dublin.[15] The first Provost of the College was the Archbishop of Dublin, Adam Loftus (after whose former college at Cambridge the institution was named),[4] and he was provided with two initial Fellows, James Hamilton and James Fullerton. Two years after foundation, a few Fellows and students began to work in the new College, which then lay around one small square. The ecclesiastical origins of the College are reflected in its motto, which is directly derived from Christian Scripture (1 Thessalonians 5:21), "ΠΑΝΤΑ ΔΟΚΙΜΑΖΕΤΕ ΤΟ ΚΑΛΟΝ ΚΑΤΕΧΕΤΕ" (Greek, transliterating as PANTA DOKIMAZETE TO KALON KATECHETE), or "Prove all things; hold fast that which is good."		During the following fifty years the community increased and endowments, including considerable landed estates, were secured, new fellowships were founded, the books which formed the foundation of the great library were acquired, a curriculum was devised and statutes were framed. The founding Letters Patent were amended by succeeding monarchs on a number of occasions, such as by James I in 1613 and most notably in 1637 by Charles I (who increased the number of fellows from seven to sixteen, established the Board – then the Provost and the seven senior Fellows – and reduced the panel of Visitors in size) and supplemented as late as the reign of Queen Victoria (and later still amended by the Oireachtas in 2000).		During the eighteenth century Trinity College was seen as the university of the Protestant Ascendancy. Parliament, meeting on the other side of College Green, made generous grants for building. The first building of this period was the Old Library building, begun in 1712, followed by the Printing House and the Dining Hall. During the second half of the century Parliament Square slowly emerged. The great building drive was completed in the early nineteenth century by Botany Bay, the square which derives its name in part from the herb garden it once contained (and which was succeeded by Trinity College's own Botanic Gardens). Following early steps in Catholic Emancipation, Catholics were first allowed to apply for admission in 1793,[16] prior to the equivalent change at the University of Cambridge and the University of Oxford. Certain disabilities remained. In December 1845 Denis Caulfield Heron was the subject of a hearing at Trinity College. Heron had previously been examined and, on merit, declared a scholar of the college but had not been allowed to take up his place due to his Catholic religion. Heron appealed to the Courts which issued a writ of mandamus requiring the case to be adjudicated by the Archbishop of Dublin and the Primate of Ireland.[17] The decision of Richard Whately and John George de la Poer Beresford was that Heron would remain excluded from Scholarship.[18] This decision confirmed that the legal position remained that persons who were not Anglicans (Presbyterians were also affected) could not be elected to Scholarship, Fellowship or be made a Professor. However within less than three decades of this all disabilities imposed on Catholics were repealed as in 1873, all religious tests were abolished, except for entry to the divinity school. However, the Irish Catholic bishops responding to the increased ease, due to these changes, with which Catholics could attend an Institution which the Bishops saw as thoroughly Protestant in ethos, and in light of the establishment of the Catholic University, in 1871 implemented a general ban on Catholics entering Trinity College, with few exceptions. "The ban" despite its longevity, is associated in the popular mind with Archbishop of Dublin John Charles McQuaid as he was made responsible for enforcing the ban from 1956 until it was rescinded by the Catholic Bishops of Ireland in 1970, shortly before McQuaid's retirement. Prior to 1956 it was the responsibility of the local Bishop.		The nineteenth century was also marked by important developments in the professional schools. The Law School was reorganised after the middle of the century. Medical teaching had been given in the College since 1711, but it was only after the establishment of the school on a firm basis by legislation in 1800, and under the inspiration of one Macartney, that it was in a position to play its full part, with such teachers as Graves and Stokes, in the great age of Dublin medicine. The Engineering School was established in 1842 and was one of the first of its kind in Ireland and Britain.		Women were admitted to Trinity College as full members for the first time in 1904. For the period from 1904-1907, women from Oxford and Cambridge came to Trinity College to receive their ad eundem degree and were known as Steamboat ladies.		In 1907, the Chief Secretary for Ireland proposed the reconstitution of the University of Dublin. A "Dublin University Defence Committee" was created and was successful in campaigning against any change to the status quo, while the Catholic bishops' rejection of the idea ensured its failure among the Catholic population. Chief among the concerns of the bishops was the remains of the Catholic University of Ireland, which would become subsumed into a new university, which on account of Trinity College would be part Anglican. Ultimately this episode led to the creation of the National University of Ireland. Trinity College was one of the targets of the Volunteer and Citizen Army forces during the 1916 Easter Rising but was successfully defended by a small number of unionist students[19] most of whom were members of the University Officers' Training Corps. From July 1917 until March 1918 the Irish Convention met in the College in an attempt to address the political aftermath of the Easter rising. (Subsequently, following the failure of the Convention to reach "substantial agreement", the Irish Free State was set up in 1922.) In the post-independence period Trinity College suffered from a cool relationship with the new state. On 3 May 1955 the Provost, Dr A.J.McConnell, pointed out in a piece in the Irish Times that certain state-funded County Council scholarships excluded Trinity College from the list of approved institutions. This, he suggested, amounted to religious discrimination, which was forbidden by the constitution.		The School of Commerce was established in 1925, and the School of Social Studies in 1934. Also in 1934, the first female professor was appointed.		1958 saw the first Catholic to reach the Board of Trinity as a Senior Fellow.		In 1962 the School of Commerce and the School of Social Studies amalgamated to form the School of Business and Social Studies. In 1969 the several schools and departments were grouped into Faculties as follows: Arts (Humanities and Letters); Business, Economic and Social Studies; Engineering and Systems Sciences; Health Sciences (since October 1977 all undergraduate teaching in dental science in the Dublin area has been located in Trinity College); Science.		In 1970 the Catholic Church lifted its ban on Catholics attending the college without special dispensation. At the same time, the Trinity College authorities invited the appointment of a Catholic chaplain to be based in the college.[20] There are now two such Catholic chaplains.[21]		In the late 1960s, there was a proposal for University College, Dublin, of the National University of Ireland, to become a constituent college of a newly reconstituted University of Dublin. This plan, suggested by Brian Lenihan and Donogh O'Malley, was dropped after opposition by Trinity College students.		From 1975, the Colleges of Technology that now form the Dublin Institute of Technology had their degrees conferred by the University of Dublin. This arrangement was discontinued in 1998 when the DIT obtained degree-granting powers of its own.		The School of Pharmacy was established in 1977 and around the same time, the Faculty of Veterinary Medicine was transferred to University College, Dublin. Student numbers increased sharply during the 1980s and 1990s, with total enrolment more than doubling, leading to pressure on resources and subsequent investment programme.		1991 saw Thomas Noel Mitchell become the first Roman Catholic elected Provost of Trinity College.[22]		Trinity College is today in the centre of Dublin. At the beginning of the new century, it embarked on a radical overhaul of academic structures to reallocate funds and reduce administration costs, resulting in, for instance, the mentioned reduction from six to just three faculties. The ten-year strategic plan prioritises four research themes with which the College seeks to compete for funding at the global level.[23] The Hamilton Mathematics Institute in Trinity College, named in honour of William Rowan Hamilton, was launched in 2005 and aims to improve the international profile of Irish mathematics, to raise public awareness of mathematics and to support local mathematical research through workshops, conferences and a visitor programme.		Trinity College retains a tranquil collegiate atmosphere despite its location in the centre of a capital city (and despite its being one of the most significant tourist attractions in Dublin). This is, in large part, due to the compact design of the college, whose main buildings look inwards and are arranged in large quadrangles (called squares), and the existence of only a few public entrances.		The main college grounds are approximately 190,000 m2 (47 acres), including the Trinity College Enterprise Centre nearby, and buildings account for around 200,000 m², ranging from works of older architecture to more modern buildings. The main entrance to the college is on the College Green, and its grounds are bounded by Nassau and Pearse Streets. The college is bisected by College Park, which has a cricket and rugby pitch.		The western side of the college is older, featuring the iconic Campanile, as well as many fine buildings, including the Chapel and Examination Hall (designed by Sir William Chambers), Graduates Memorial Building, Museum Building, and the Rubrics, all spread across College's five squares. The Provost's House sits a little way up from the College Front Gate such that the House is actually on Grafton Street, one of the two principal shopping streets in the city, while its garden faces into the College. The Douglas Hyde Gallery, a contemporary art gallery, is located in the College as is the Samuel Beckett Theatre. It hosts national and international performances, and is used by the Dublin International Theatre Festival, the Dublin Dance Festival, and The Fringe Festival, among others. During the academic term it is predominantly used as a teaching and performance space for Drama students and staff.		The eastern side of the college is occupied by Science buildings, most of which are modern developments, arranged in three rows instead of quadrangles. In 2010, Forbes ranked the it as one of the 15 most beautiful college grounds in the world.[24]		The College also incorporates a number of buildings and facilities spread throughout the city, from the Politics and Sociology Departments, located on Dame Street, to the Faculty of Health Sciences buildings, located at St James's Hospital and the Adelaide and Meath complex incorporating the National Children's Hospital, Tallaght. The Trinity Centre at St James's Hospital incorporates additional teaching rooms, as well as the Institute of Molecular Medicine and John Durkan Leukaemia Institute. The College also owns a large set of residences four kilometres to the south of the college on the Dartry Road, in Rathmines, called Trinity Hall.[Note 2]		The current chapel was completed in 1798, and was designed by George III’s architect, Sir William Chambers, who also designed the public theatre opposing the chapel on Parliament Square.[25] Reflecting the college's Anglican heritage, there are daily services of Morning prayer, weekly services of Evensong, and Holy Communion is celebrated on Wednesdays and Sundays, however it is no longer compulsory for students to attend these.		The chapel has been ecumenical since 1970, and is now also used daily in the celebration of Mass for the Roman Catholic members of the college. In addition to the Anglican chaplain, who is known as the Dean of Residence, there are two Roman Catholic chaplains and one Methodist chaplain. Ecumenical events are often held in the chapel, such as the annual carol service and the service of thanksgiving on Trinity Monday.[26]		The Library of Trinity College is the largest research library in Ireland. As a result of its historic standing, Trinity College Library Dublin is a legal deposit library (as per Legal Deposit Libraries Act 2003) for the United Kingdom of Great Britain and Northern Ireland, and has a similar standing in Irish law. The College is therefore legally entitled to a copy of every book published in Great Britain and Ireland and consequently receives over 100,000 new items every year. The Library contains about five million books, including 30,000 current serials and significant collections of manuscripts, maps, and printed music. Three million books are held in the book depository, "Stacks", in Santry, from which requests are retrieved twice daily.		The Library proper is composed of several library buildings in college. The original (Old) Library is Thomas Burgh's masterpiece. A huge building, it originally towered over the university and city after its completion. Even today, surrounded by similarly scaled buildings, it is imposing and dominates the view of the university from Nassau Street. It was founded with the College and first endowed by James Ussher (1625–56), Archbishop of Armagh, who endowed his own valuable library, comprising several thousand printed books and manuscripts, to the College. The Book of Kells is by far the Library's most famous book and is located in the Old Library, along with the Book of Durrow, the Book of Howth and other ancient texts. Also incorporating the Long Room, the Old Library is one of Ireland's biggest tourist attractions, and holds thousands of rare, and in many cases very early, volumes. In the 18th century, the college received the Brian Boru harp, one of the three surviving medieval Gaelic harps, and a national symbol of Ireland, which is now housed in the library.		The buildings referred to as the College's BLU (Berkeley Lecky Ussher) Arts library complex consist of the Berkeley Library in Fellow's Square, built in 1956, the Lecky Library, attached to the Arts building, and the James Ussher Library which, opening officially in 2003, overlooks College Park and houses the Glucksman Map Library. The Glucksman Library contains half a million printed maps, the largest collection of cartographic materials in Ireland. This includes the first Ordnance Surveys of Ireland, conducted in the early 19th century.		The Library also includes the William Hamilton Science and Engineering Library and the John Stearne Medical Library, housed at St James's Hospital.		The College, officially incorporated as The Provost, Fellows and Scholars of the College of the Holy and Undivided Trinity of Queen Elizabeth near Dublin, is headed by the Provost. Patrick Prendergast has been the Provost since 2011.		The body corporate of the College consists of the Provost, Fellows and Scholars. In general, amendments to the College Statutes (which must be proposed by the Board) require the consent of the Fellows. Where a change requires parliamentary legislation, the consent of the whole Body Corporate may be needed, with Scholars voting alongside Fellows. This last happened when the governance of the College and University was revised and re stated by an Act of the Oireachtas in 2000.		The Provost serves a ten-year term and is elected primarily by fellow academic staff. A small number of students have votes. Election to Fellowship and Scholarship is for academic staff and undergraduates respectively. The decision to elect is made by the Board. Fellows were once elected for life on the basis of a competitive examination. The number of Fellows was fixed and a competition to fill a vacancy would occur on the death or resignation of a Fellow. Fellows are now elected from amongst existing College academics and serve until reaching retirement age, and there is no formal limit on their number. They are drawn from College academics. Election to Fellowship is recognition that they have excelled in their field and as such, amounts to a promotion for those receiving it. Any person appointed to a Professorship who is not already a Fellow, is elected a Fellow at the next opportunity.		Scholars continue to be selected by competitive examination from the Undergraduate body. The Scholarship examination is now set according to the several undergraduate courses. (So there is a scholarship examination in History, or in Mathematics or Engineering, and so forth). The Scholarship examination is taken in the second year of a four-year degree course (though, in special circumstances, such as illness, bereavement, or studying abroad during the second year, permission may be given to sit the examination in the third year). In theory, a student can sit the examination in any subject, not just the one they are studying. They hold their Scholarship until they are of "MA standing" that is, three years after obtaining the BA degree. (So most are Scholars for a term of five years).		Fellows are entitled to residence in the College free of charge; most Fellows do not exercise this right in practice, with the legal requirement to provide accommodation to them being fulfilled by providing an office. Scholars are also entitled to residence in the College free of charge, they also receive an allowance, and. if relevant, have the fees paid for courses they are taking within the college. However, due to pressure on College accommodation Scholars are no longer entitled (as they once were) to free rooms, or any rooms at all, if they are not actually students whether undergraduate or post graduate. Fellows and Scholars are also entitled to one free meal a day, usually in the evening ("Commons"). Scholars retain the right to free meals even after graduation, and ceasing to be students, should they choose to exercise it.		Aside from the Provost, Fellows and Scholars, Trinity College has a Board (dating from 1637), which carries out general governance. Originally the Board consisted of the Provost and Senior Fellows only. There were seven Senior Fellows, defined as those seven fellows that had served longest, fellowship at that time being for life, unless resigned. Over the years a representational element was added, for example by having elected representatives of the Junior Fellows and of those Professors who were not Fellows, with the last revision before Irish Independence being made by Royal Letters Patent in 1911.		The governance of Trinity College was next formally changed in 2000, by the Oireachtas, in legislation proposed by the Board of the college, and approved by the Body Corporate viz The Trinity College, Dublin (Charters and Letters Patent Amendment) Act, 2000. This was introduced separately from the Universities Act 1997. It states that the Board shall comprise:		The fellows, non-fellow academic staff as well as non-academic staff are elected to serve for a fixed term. The four student members are the President, Education Officer and Welfare Officer of the Students' Union and the president of the Graduate Students' Union (all ex officio) and are elected annually for one-year terms. The vice-provost/chief academic officer, senior lecturer, registrar and bursar are 'annual officers' appointed for one-year (renewable) terms by the Provost.		There is a Council (dating from 1874), which oversees academic matters. All decisions of the Council require the approval of the Board, but if the decision in question does not require a new expenditure, the approval is normally formal, without debate. The Council had a significant number of elected representatives from the start, and was also larger than the Board, which at that time, continued to consist of the Provost and seven Senior Fellows only. The Council is the formal body which makes academic staff appointments, always, in practice on the recommendation of appointments panels, but which have themselves been appointed by the Council. An illustration of the relationship between the Board and the Council, is where a decision is made to create a new professorial chair. As this involves paying a salary, the initial decision to create the chair is made by the Council, but the decision to make provision for the salary is made by the Board, consequently the Board might over rule, or defer a Council decision on grounds of cost.		The University of Dublin was modelled on University of Oxford and University of Cambridge in the form of a collegiate university, Trinity College being named by the Queen as the mater universitas ("mother of the university"). As no other college was ever established, the College is the sole constituent college of the university and so Trinity College and the University of Dublin are for most practical purposes synonymous. However, the actual statutes of the university and the college[27] grant the university separate corporate legal rights to own property and borrow money and employ staff. Moreover, while the Board of the College has the sole power to propose amendments to the statutes of the University and College, amendments to the university statutes require the consent of the Senate of the University. Consequently, in theory, the Senate can overrule the Board, but only in very limited and particular circumstances. However it is also the case that the University cannot act independently of the initiative of the Board of Trinity College. The most common example of when the two bodies must collaborate is when a decision is made to establish a new degree. All matters relating to syllabus, examination and teaching are for the College to determine, but actual clearance for the award of the degree is a matter for the University. In the same way when an individual is awarded an Honorary Degree, the proposal for the award is made by the Board of Trinity College, but this is subject to agreement by a vote of the Senate of Dublin University. All graduates of the University who have at least a master's degree are eligible to be members of the Senate, but in practice only a few hundred are, with a large proportion being current members of the staff of Trinity College.		The College also has an oversight structure, the Chancellor of the University who is elected by the Senate and the judicial Visitor who is appointed by the Irish Government from a list of two names submitted by the Senate of the University of Dublin. The current judicial Visitor is the Hon. Dr. Justice Maureen Harding Clark. In the event of a disagreement between the two visitors the opinion of the Chancellor prevails. The visitors act as a final "court of appeal" within the College, with their mode of appointment giving them the needed independence from the College administration.		Trinity College is a sister college to Oriel College of the University of Oxford and St John's College of the University of Cambridge.[28][29]		Two teaching hospitals are associated with the college:		A number of teaching institutions are involved in jointly taught courses:		The School of Business in association with the Irish Management Institute forms the Trinity-IMI Graduate School of Management, incorporating the faculties of both organisations. Trinity College has also been associated in the past with a number of other teaching institutions. These include St Catherine's College of Education for Home Economics (now closed), Magee College and Royal Irish Academy of Music, which is a music conservatoire, as well as The Lir National Academy of Dramatic Art, which is the national conservatoire for theatre training actors, technicians, playwrights and designers to a professional and industry standard - the Lir is also advised by the Royal Academy of Dramatic Art in the UK.		The University has been represented since 1613 when James I granted it the right to elect two members of parliament (MPs) to the Irish House of Commons. Since the new Constitution of Ireland in 1937, the University has elected three Senators to Seanad Éireann. The current representatives of the University are Ivana Bacik, David Norris and Lynn Ruane. Notable representatives have included Edward Gibson, W. E. H. Lecky, Edward Carson, Noel Browne, Conor Cruise O'Brien and Mary Robinson. The franchise was originally restricted to the Provost, Fellows and Scholars of Trinity College. This was expanded in 1832 to include those who had received an M.A. and in 1918 all those who had received a degree from the University.		Since considerable academic restructuring in 2008, the college has three academic faculties:		Each faculty is headed by a dean (there is also a Dean of Postgraduate Studies), and faculties are divided into schools, of which there were 24 as of 2012.[30]		Since 2014, Trinity College's Science Department has established and operated a scheme for second-level students to study science, technology, engineering, and mathematics. The system, similar to DCU's CTYI programme, encourages academically gifted secondary students with a high aptitude for the STEM subjects, was named the Walton Club[31] in honour of Ernest Walton, Ireland's first and only Nobel laureate for Physics. The educators in the programme are PhD students in the college, and they impart an advanced, undergraduate-level curriculum onto the students. The club was set up with a specific ethos around the mentoring of STEM subjects, and not as a grinds school.[32][33] The scheme, now in its third year, has been immensely successful and is perpetually growing in scope and scale year on year. It has also diversified beyond its traditional weekly club structure, running camps during school holidays to offer an opportunity to study STEM to those unable to join the club.[34] It has also represented the college in many activities, meeting Chris Hadfield and attending the Young Scientist and Technology Exhibition and the Web Summit. Students, or alphas as they are dubbed in honour of the eponymous physicist, develop projects in the Club, with innovations pioneered there including a health-focused electroencephalogram.[32] The club was founded by Professors Igor Shvets and Arlene O'Neill of the School of Physics in Trinity College.[33]		Most undergraduate courses require four years of study. First-year students at the undergraduate level are called Junior Freshmen (regardless of sex); second years, Senior Freshmen; third years, Junior Sophisters; and fourth years, Senior Sophisters. The Freshman Years usually have a set or minimally flexible basic curriculum with the Sophister years allowing for a much greater degree of course variation, as well as taking a year abroad. The passing of two sets of examinations is a prerequisite for a degree. Junior and Senior Freshmen sit preliminary annual exams in Trinity Term of each year which must be passed so that they "rise" to the year above. At the end of the Junior Sophister year, undergraduates sit Part I of the Moderatorship exams, subject to attaining an upper-second[citation needed], allows them to take an Honours degree and sit the Part II (Final) of the Moderatorship exams. Successful candidates receive first-, upper or lower second-, or third-class honours, or simply a "pass" without honours if they perform insufficiently in Part I of the Moderatorship.		Most non-professional courses take a Bachelor of Arts (BA) degree. As a matter of tradition, bachelor's degree graduates are eligible, after nine terms from matriculation and without additional study, to purchase for a fee an upgrade of their bachelor's degree to a Master of Arts The four-year degree structure makes undergraduate teaching at Dublin closer to the North American model than that of other universities in England and Ireland (Scottish universities, like Dublin, generally also require four years of study for a bachelor's degree).		Degree titles vary according to the subject of study. The Law School awards the LL.B., the LL.B. (ling. franc.) and the LL.B. (ling. germ.). Other degrees include the BAI (engineering) and BBS (business studies). The BSc degree is not in wide use although it is awarded by the School of Nursing and Midwifery; most science and computer science students are awarded a BA.		At postgraduate level, Trinity offers a range of taught and research degrees in all faculties. About 31% of students are post-graduate level, with 1,600 students reading for a research degree and an additional 2,200 on taught courses (see Research and Innovation).[6][35]		Trinity College's Strategic Plan sets "the objective of doubling the number of PhDs across all disciplines by 2013 in order to move towards a knowledge society. In order to achieve this, the College has received some of the largest allocations of Irish Government funding which have become competitively available to date."[36]		In addition to academic degrees, the college offers Postgraduate Diploma (non-degree) qualifications, either directly, or through associated institutions.		The academic year is divided into three terms. Michaelmas term lasts from October to December; Hilary term from January to March; and Trinity term from April to June, with each term separated by a vacation. Whilst teaching takes place across all three terms in postgraduate courses, for undergraduate programmes, teaching is condensed within the first two terms since 2009, with each term consisting of a twelve-week period of teaching known as the Teaching Term. These are followed by three revision weeks and a four-week exam period during the Trinity Term.[37]		Internally at least, the weeks in the term are often referred to by the time elapsed since the start of teaching Term: thus the first week is called "1st week" or "week 1" and the last is "Week 12"/"12th week".		The first week of Trinity Term (which marks conclusion of lecturing for that year) is known as Trinity Week; normally preceded by a string of balls, it consists of a week of sporting and academic events. This includes the Trinity Ball and the Trinity Regatta( a premier social event on the Irish rowing calendar held since 1898),[38] the election of Scholars and Fellows and a College banquet.		Admission to undergraduate study for European Union school-leavers is generally handled by the CAO (Central Applications Office), and not by Trinity College. Applicants have to compete for university places solely on the basis of the results of their school leaving exams. Through the CAO, candidates may list several courses at Trinity College and at other third-level institutions in Ireland in order of priority. Places are awarded in mid-August every year by the CAO after matching the number of places available to the academic attainments of the applicants. Qualifications are measured as "points", with specific scales for the Irish Leaving Certificate, and all other European Union school leaving results, such as the UK GCE A-level, the International Baccalaureate along with other national school leaving exams.[39]		For applicants who are not citizens or residents of the European Union, different application procedures apply.;[40] 16% of students are from outside Ireland, and 40% of these are from outside the European Union.[citation needed] Disadvantaged, disabled, or mature students can also be admitted through a program that is separate from the CAO, the Trinity Access Programme,[41] which aims to facilitate the entry of sectors of society which would otherwise be under-represented. The numbers admitted on this program are significant relative to other universities, up to 15% of the annual undergraduate intake.		Admission to graduate study is handled by Trinity College.		Students who enter with exceptional Leaving Certificate or other public examination results are awarded an Entrance Exhibition. This entails a prize in the form of book tokens to the value of €150.00.[42] Exhibitioners who are of limited means are made Sizars, entitled to Commons (evening meal) free of charge.		Undergraduate students of Senior Freshmen standing may elect to sit the Foundation Scholarship examination, which takes place in the Christmas Vacation, on the last week before Hilary term. On Trinity Monday (the first day of Trinity Term), the Board of the College sits and elects to the Scholarship all those who achieve First in the examination. Those from EU member countries are entitled to free rooms and Commons (the College's Formal Hall), an annual stipend and exemption from fees for the duration of their scholarship, which lasts for fifteen terms. Scholars from non-EU member countries have their fees reduced by the current value of EU member fees. Scholars may add the suffix "Sch." to their names, have the note "discip. schol." appended to their name at Commencements and are entitled to wear Bachelor's Robes and a velvet mortarboard.		Competition for Scholarship involves a searching examination and successful candidates must be of exceptional ability. The concept of Scholarship is a valued tradition of the College and many of the College's most distinguished members were elected Scholars (including Samuel Beckett and Ernest Walton). The Scholars' dinner, to which 'Scholars of the decade' (those elected in the current year, and every year multiple of a decade previous to it, e.g., 2013, 2003,..) are invited, forms one of the major events in Trinity's calendar. A Scholarship at Trinity College is a prestigious undergraduate award; a principal aim of the College is the pursuit of excellence and one of the most tangible demonstrations of this is the institution of Scholarship.		Under the Foundation Charter (of 1592), Scholars were part of the body corporate (three Scholars were named in the charter "in the name of many"). Until 1609 there were about 51 Scholars at any one time. A figure of seventy was permanently fixed in the revising Letters Patent of Charles I in 1637. Trinity Monday was appointed as the day when all future elections to Fellowship and Scholarship would be announced (at this time Trinity Monday was always celebrated on the Monday after the feast of the Holy Trinity). Up to this point all undergraduates were Scholars, but soon after 1637 the practice of admitting students other than Scholars commenced.		Until 1856, only the classical subjects were examined. The questions concerned all the classical authors prescribed for the entrance examination and for the undergraduate course up to the middle of the Junior Sophister year. So candidates had no new material to read, 'but they had to submit to a very searching examination on the fairly lengthy list of classical texts which they were supposed by this time to have mastered'. The close link with the undergraduate syllabus is underlined by the refusal until 1856 to admit Scholars to the Library (a request for admission was rejected by the Board in 1842 on the grounds that Scholars should stick to their prescribed books and not indulge in 'those desultory habits' that admission to an extensive library would encourage). During the second half of the nineteenth century the content of the examination gradually came to include other disciplines.		Around the turn of the 20th century, further examinations for "Non-Foundation" Scholarships were introduced. This initially was a formula to permit women to become Scholars, but without entitling them to the same voting rights as men. Non-Foundation Scholarships are now awarded to those who meet the qualifying standards and Foundation Scholarships given to those whose performance was considered particularly exceptional. While the number of Foundation Scholars remains fixed at seventy, there is in theory no limit on the number of Non-Foundation scholars. The only practical difference between the two is that the Foundation Scholars are members of the body corporate of the College and are entitled to certain voting rights.		Trinity College is the most productive internationally recognised research centre in Ireland.[43] The University operates an Innovation Centre which fosters academic innovation and consultancy, provides patenting advice and research information and facilitates the establishment and operation of industrial laboratories and campus companies.		In 1999, the University purchased an Enterprise Centre on Pearse Street, seven minutes' walk from the on-site "Innovation Centre". The site has over 19,000 m² (200,000 ft²) of built space and contains a protected building, the Tower, which houses a Craft Centre. The Trinity Enterprise Centre houses companies drawn from the University research sector in Dublin.		In response to a long-term decline in rankings (from 43rd according to the last combined THE/QS ranking in 2009[51] to 98th in QS[52] and 160th in THE for 2016) Trinity announced a plan in 2014 to reverse the trend, aiming to re-enter the top 50 bracket.[53]		Trinity College has a student life with 124 societies (in 2011). Student societies operate under the aegis of the Dublin University Central Societies Committee which is composed of the Treasurers of each of the Societies within the College. Society size varies enormously, and it is often hard to determine exact figures for most societies – several claiming to be the largest in the college with thousands of members, while smaller groups may have only 40–50 members.		Situated within the Graduates Memorial Building is the University Philosophical Society (the Phil), sometimes referred to as one of the oldest of Ireland's such societies. Claiming to have been founded in 1683 (though university records list its foundation as having occurred in 1853[55]) the society has strong history in debating and paper-reading. Consequently, over the past four centuries it has been addressed by the world's preeminent thinkers and orators. The society meets each Thursday evening to debate motions of interest in the chamber of the Graduates Memorial Building. It counts among its Honorary Patrons multiple Noble Prize laureates, Heads of State, notable actors, entertainers, well-known intellectuals, such as Al Pacino, Desmond Tutu, Sir Christopher Lee, Stephen Fry, and John Mearsheimer.		Another such society is the College Historical Society (the Hist) which shares the GMB, founded in 1770 (which it makes it the oldest Society on Campus according to the College Calender),[55] meets each Wednesday evening of Term to debate motions in the chamber of the Graduate Memorial Building, has been addressed by many notable orators including Winston Churchill and Ted Kennedy, and counts among its former members many of the most prominent men and women in Ireland's history.		Other societies include Vincent de Paul Society (VDP), which organises a large number of charitable activities in the local community; DU Players, one of the most prolific student-drama societies in Europe which hosts more than 50 shows and events a year in the Samuel Beckett Theatre; The DU Film Society (Formerly DU Filmmakers, formerly the DU "Videographic Society", founded in 1987) which organises filmmakers and film-lovers in College through workshops, screenings, production funding, etc.; The DU Radio Society, known as Trinity FM, broadcasts a variety of student made productions on a special events licence on FM frequency 107.8FM for six weeks a year; The Trinity LGBT society, which is the oldest LGBT society in Ireland and celebrated its 25th anniversary in the 2007/2008 year; The Card and Bridge Society also holds weekly poker and bridge tournaments and was the starting point to many notable alum including Andy Black, Padraig Parkinson and Donnacha O'Dea ;The Dublin University Comedy Society, known as DU Comedy, hosts comedy events for its members and has hosted gigs in college from comedians such as Andrew Maxwell, David O'Doherty, Neil Delamere and Colin Murphy; The Dance Society, known as dudance, provides classes in Latin and ballroom dancing, as well as running events around other dance styles such as swing dancing.[56] In 2011 the Laurentian Society was revived. This society played a key role as a society for the few Catholic students who studied at Trinity while "the Ban" was still in force.[57] The Trinity Fashion Society was established in 2009, since then it holds an annual charity fashion show and hosts an international trip to London Fashion Week.[58]		There is a sporting tradition at Trinity and the college has 50 sports clubs affiliated to the Dublin University Central Athletic Club (DUCAC).[59]		The Central Athletic Club is made up of five committees that oversee the development of sport in the college: the Executive Committee which is responsible overall for all activities, the Captains' Committee which represents the 49 club captains and awards University Colours (Pinks), the Pavilion Bar Committee which runs the private members' bar, the Pavilion Members' Committee and the Sports Facilities Committee.		The oldest clubs include the Dublin University Cricket Club (1835),[60] the Dublin University Boat Club (1836)[61] and the Dublin University Rifle Club (1840).[62] Dublin University Football Club, founded in 1854, plays rugby football and is the world's oldest documented "football club". The Dublin University Association Football Club (soccer) was founded in 1883,[63] the Dublin University Hockey Club in 1893,[64] and the Dublin University Harriers and Athletic Club in 1885.[65]		The largest sports club in the college is the Boarding Club with over 1000 registered members. The newest club in the University is the American football team, who were accepted into the Irish American Football League (IAFL) in 2008. Initially known as the Trinity Thunderbolts, the club now competes under the name "Trinity College". The most successful Trinity College sports club – based on Intervarsities victories – is Dublin University Fencing Club (DU Fencing Club). A total of thirty-six Intervarsity titles have been won by the club in fifty-nine years of competition. While the modern DU Fencing Club was founded in 1941, its origins can be dated to the 1700s when a 'Gentleman's Club of the Sword' existed, primarily for duelling practice.[66] There are several graduate sport clubs that exist separate to the Central Athletic Club including the Dublin University Museum Players (cricket), the Lady Elizabeth Boat Club (rowing) and the Mary Lyons Memorial Mallets (croquet).[citation needed]		Trinity College has a tradition of student publications, ranging from the serious to the satirical. Most student publications are administered by Trinity Publications, previously called the Dublin University Publications Committee (often known as 'Pubs'), which maintains and administers the Publications office (located in No 6) and all the associated equipment needed to publish newspapers and magazines.		The Students' Union funds a monthly newspaper called The University Times. This paper was launched in 2009 replacing the University Record. The Record, first published in 1997, had previously replaced an older publication called Aontas.		Trinity News is also published in Trinity, and is Ireland's oldest student newspaper, having been founded in 1953. As of 2010 it is published on a fortnightly basis, producing twelve issues in total during the academic year. The focus is on students with sections including College News, National News, International News, Features, Science, Sports Features and College Sports. The paper also includes the cultural magazine called TN2. For the last 10 years the paper has been edited by a full-time student editor, who takes a sabbatical year from their studies, supported by a voluntary part-time staff of 30 student section editors and writers. Ireland's only student-run financial newspaper The Bull is also published quarterly. Student magazines currently in publication as of 2012 include the satirical newspaper The Piranha[67] (formerly Piranha! magazine but rebranded in 2009), the generalist T.C.D. Miscellany (founded in 1895; one of Ireland's oldest magazines), the film journal Trinity Film Review (TFR) and the literary Icarus. Other publications include the Student Economic Review and the Trinity College Law Review, produced independently by students of economics and law respectively, the Trinity College Journal of Postgraduate Research, produced by the Graduate Students Union, the Social and Political Review (SPR),[68] now in its 22nd year, the Trinity Student Medical Journal,[69] The Attic, student writing produced by the Dublin University Literary Society and the Afro-Caribbean Journal produced by the Afro-Caribbean Society. Some older titles currently not in publication include In Transit, Central Review, Harlot, Evoke, and Alternate.		The Trinity Ball is an annual event that draws 7,000 attendants.[70] Until 2010, it was held annually on the last teaching day of Trinity term to celebrate the end of lectures and the beginning of Trinity Week. Due to a restructuring of the teaching terms of the College the 2010 Ball was held on the last day of Trinity Week. In 2011, the ball was held on the final day of teaching of Hilary Term, before the commencement of Trinity Week. The Ball is run by Trinity Ents, Trinity Students' Union and Trinity's Central Societies Committee in conjunction with event promoters MCD Productions, who hold the contract to run the Ball until 2012.[71] The Ball celebrated its 50th anniversary in 2009.[72]		The Students' Union's primary role is to provide a recognised representative channel between undergraduates and the University and College authorities. The Campaigns Executive, the Administrative Executive and Sabbatical Officers manage the business and affairs of the Union. The Sabbatical Officers are: The President, Communications Officer, Welfare Officer, Education Officer and Entertainments Officer and are elected on an annual basis; all capitated students are entitled to vote. The SU President, Welfare Officer and Education Officer are ex-officio members of the College Board.		The Graduate Students' Union's primary role is to provide a recognised representative channel between postgraduates and the University and College authorities.[73] The GSU president is an ex-officio member of the College Board.		The Graduate Students' Union publish the annual "Journal of Postgraduate Research".		The Latin Grace is said "before and after meat" at Commons, a three-course meal served in the College Dining Hall Monday to Friday. Commons is attended by Scholars and Fellows and Sizars of the College, as well other members of the College community and their guests.		Each year, Trinity Week is celebrated in mid-April on Trinity Monday and on the afternoon of Trinity Wednesday no lectures or demonstrations are held. College races are held each year on Trinity Wednesday.		There is a long-standing rivalry with nearby University College Dublin, which is largely friendly in nature. Every year, Colours events are contested between the sporting clubs of each University.		The more superstitious students of the college (during their undergraduate studies) never walk underneath the Campanile, as the tradition suggests that should the bell ring whilst they pass under it, they will fail their annual examinations.		In James Plunkett's Farewell Companions, one of the characters claims to have been "through Trinity", having entered at College Green and left at the Nassau Street Gate.		Parts of Michael Collins,[74] The First Great Train Robbery,[75] Circle of Friends,[76] Educating Rita,[77] Ek Tha Tiger[78] and Quackser Fortune Has a Cousin in the Bronx[79] were filmed in Trinity College. It served as the filming location for Luftwaffe headquarters in The Blue Max.		The Irish writer J.P. Donleavy was a student in Trinity. A number of his books feature characters who attend Trinity, including The Ginger Man and The Beastly Beatitudes of Balthazar B. H.A. Hinkson has written two books about Trinity, Student Life in T.C.D. and the fictional O'Grady of Trinity – A Story of Irish University Life.		Fictional Naval Surgeon Stephen Maturin of Patrick O'Brian's popular Aubrey–Maturin series is a graduate of Trinity College.		In the Channel 4 television series Hollyoaks, Craig Dean attends Trinity College. He left Hollyoaks to study in Ireland in 2007 and now lives there with his boyfriend, John Paul McQueen, after they got their sunset ending in September 2008.		All Names Have Been Changed a novel by Claire Kilroy is set in Trinity College in the 1990s. The story follows a group of creative writing students and their enigmatic professor. A photograph of Trinity is used in the cover art.[80]		In Karen Marie Moning's The Fever Series Trinity College is said to be where the main character, MacKayla Lane's, sister Alina was attending school on scholarship before she was murdered. The college is also where several of the minor characters who inform Ms. Lane about her sister are said to work.		In the novel Thanks for the Memories, written by Irish author Cecelia Ahern, Justin Hitchcock is a guest lecturer at Trinity College.[81]		Amongst the past students (and some staff) are included notable people such as:		Others include three previous holders of the office of President of Ireland; Erskine Hamilton Childers, Mary Robinson and Mary McAleese.		
Grade retention or grade repetition is the process of having a student repeat a grade, because the previous year, the student experienced developmental delays which made the student fail the grade. Students who repeat a grade are referred as "repeaters".[1] Repeaters can be referred to as having been "held back". Students do not necessarily repeat the grade in the same classroom, but it will be the same grade.		The alternative to grade retention (for those who have failed) is a policy of social promotion, under the ideological principle that staying with their same-age peers is important. Social promotion is the obligatory promotion of all students, regardless of achievement and absences.[2] Social promotion is somewhat more accepted in countries which use tracking to group students according to academic ability. Regardless of whether a failing student is retained or promoted, academic scholars believe that underperformance must be addressed with intensive remedial help, such as summer school programs.		In most countries, grade retention has been banned or strongly discouraged. In the United States, grade retention can be used in kindergarten through twelfth grade. However, with older students, retention is usually restricted to the specific classes that the student failed, so that a student can be, for example, promoted in a math class but retained in a language class.		Where it is permitted, grade retention is most common among students in early elementary school.[3] Students with intellectual disabilities are only retained when parents and school officials agree to do so. Children who are relatively young in their age cohort are four times more likely to be retained.[4]						Different schools have used different approaches throughout history. Grade retention or repetition was essentially meaningless in the one-room schoolhouses of more than a century ago, because access to outside standards were very limited, and the small scale of the school, with perhaps only a few students of each age, was conducive to individualized instruction. With the proliferation of larger, graded schools in the middle of the 19th century, retention became a common practice. In fact, a century ago, approximately half of all American students were retained at least once before the age of 13.[5]		Social promotion began to spread in the 1930s with concerns about the psychosocial effects of retention.[5] Social promotion is the promoting of underperforming students under the ideological principle that staying with their same-age peers is important to success. This trend reversed in the 1980s, as concern about slipping academic standards rose.The practice of grade retention in the U.S. has been climbing steadily since the 1980s.[6] The practice of making retention decisions on the basis of the results of a single test—called high-stakes testing—is widely condemned by professional educators.[7][8] Test authors generally advise that their tests are not adequate for high-stakes decisions, and that decisions should be made based on all the facts and circumstances.[7]		There is conclusive evidence that grade retention is significantly helpful, and much of the existing research has been methodologically invalid.[9] Three different kinds of studies exist or have been proposed, and each has its inherent pitfalls:		Non-academic outcomes: Retention is associated with poor "social adjustment, attitudes toward school, behavioral outcomes, and attendance."[10] Retention is a "stronger predictor of delinquency than socioeconomic status, race, or ethnicity," and is also a strong predictor of drug and alcohol use and teenage pregnancy.[6]:54–55 (Study style #1 favors social promotion.)		Australia uses grade retention, although in 2010, the New South Wales Department of Education and Training made a new policy of no student repeating at any school for various reasons. For example, as of 2010, students are not repeating Year 11 or Year 12 because of all the post-school services after they complete Year 12 such as TAFEs or universities.		In New Zealand, secondary schools commonly use a system of internal academic streaming in which children of the same age are subdivided on the basis of ability, and lower achieving students (those who would be retained under the North American system) are taught in different classes, and at a different rate, from higher achieving students, but are kept within their own age group. This system has largely rendered grade retention unnecessary in all but the most exceptional circumstances.[citation needed]		In most cases where streaming alone is insufficient, additional special needs provision is usually seen as being preferable to grade retention, particularly when behavioral difficulties are involved.		Japan and Korea do not allow grade retention. North Korea does not have school retention. Malaysia also does not practice grade retention.		Singapore practices grade retention in Secondary Schools if a student is unsuccessful in achieving a satisfactory accumulated percentage grade. The school authorities may also decide that it would be more appropriate for the student to advance to a higher level in a lower stream such as in the cases of Express and Normal (Academic) students. Grade retention is most common in Junior Colleges where a promotional criteria is set in place. Hong Kong practices grade retention in elementary and secondary school if the student obtains a failing grade even after taking a re-test, though grade retention is very rare.		Norway, Denmark and Sweden do not allow grade retention.		In the United Kingdom, a similar streaming system to New Zealand's is used (see above).		Germany, Italy, Austria, Netherlands, France, Finland and Switzerland use grade retention.		Greece allows grade retention if a student fails more than 5 final exams, or 5 or less both in May examinations and in September examination. A student who has missed more than 114 periods of class can also repeat a grade.		The United States and Canada both use grade retention.		In the U.S., six-year-old students are most likely to be retained, with another spike around the age of 12.[9] In particular, some large schools have a transitional classroom, sometimes called "Kindergarten 2", for six-year-olds who are not reading-ready.		School officials in some US states have the authority to allow students to be held back if they do not attend summer school.[11]		The following are common arguments regarding this practice.		Opponents of "no social promotion" policies do not defend social promotion so much as say that retention is even worse. They argue that retention is not a cost-effective response to poor performance when compared to cheaper or more effective interventions, such as additional tutoring and summer school. They point to a wide range of research findings that show no advantage to, or even harm from, retention, and the tendency for gains from retention to wash out.		Harm from retention cited by these critics include:		Critics of retention also note that retention is expensive for school systems: requiring a student to repeat a grade is essentially to add one student for a year to the school system, assuming that the student does not drop out.		The possibility of grade retention has been shown to be a significant source of stress for students. In one study of childhood fears performed in the 1980s, the top three[clarification needed] fears for US sixth graders were a parent's death, going blind, and being retained. After two decades of increasing retention practices, a repeat of the study in 2001 found that grade retention was the single greatest fear, higher than loss of a parent or going blind.[13] This change likely reflects the students' correct perception that they were statistically far more likely to repeat the sixth grade than to suffer the death of a parent or the loss of their vision.		Opponents of social promotion argue that passing a child who did not learn the necessary material cheats the child of an education. As a result, when the child gets older, the student will likely fail classes or be forced to attend summer school. Opponents of social promotion argue that some children would benefit from an additional year, especially in Kindergarten, to mature and develop social and emotional skills. This additional time will assist students with improved academic performance. Opponents of social promotion argue that it has the following negative impacts:		
– in Europe  (dark green and light green & dark grey) – in the United Kingdom  (dark green and light green)		England and Wales (Welsh: Cymru a Lloegr) is a legal jurisdiction covering England and Wales, two of the four countries of the United Kingdom. "England and Wales" forms the constitutional successor to the former Kingdom of England and follows a single legal system, known as English law.		The devolved National Assembly for Wales (Welsh: Cynulliad Cenedlaethol Cymru) was created in 1999 by the Parliament of the United Kingdom under the Government of Wales Act 1998 and provides a degree of self-government in Wales. The powers of the Assembly were expanded by the Government of Wales Act 2006, which allows it to pass its own laws, and the Act also formally separated the Welsh Government from the Assembly. There is no equivalent body for England, which is directly governed by the Parliament and the government of the United Kingdom.						During the Roman occupation of Britain, the area of present-day England and Wales was administered as a single unit, with the exception of the land to the north of Hadrian's Wall. At that time, most of the native inhabitants of Roman Britain spoke Brythonic languages, and were all regarded as Britons, divided into numerous tribes. After the conquest, the Romans administered this region as a single unit, the province of Britain.		After the departure of the Romans, the Britons of what became Wales developed their own system of law, first codified by Hywel Dda (Hywel the Good; reigned 942–950) when he was king of most of present-day Wales, while in England Anglo-Saxon law was initially codified by Alfred the Great in his Legal Code, c. 893. However, following the Norman invasion of Wales in the 11th century, English law came to be practised in the parts of Wales conquered by the Normans (the Welsh Marches). In 1283 the English, led by Edward I, with the biggest army brought together in England since the 11th century, conquered the remainder of Wales, then organised as the Principality of Wales, which was united with the English crown by the Statute of Rhuddlan of 1284. This aimed to replace Welsh criminal law with English law.		Welsh law continued to be used for civil cases until the annexation of Wales to England in the 16th century. The Laws in Wales Acts 1535–1542 then consolidated the administration of all the Welsh territories and incorporated them fully into the legal system of the Kingdom of England.[1]		Prior to 1746 it was not clear whether a reference to "England" in legislation included Wales, and so in 1746 Parliament passed the Wales and Berwick Act. This specified that in all prior and future laws, references to "England" would by default include Wales (and Berwick). The Wales and Berwick Act was repealed in 1967, although the statutory definition of "England" it created is preserved for acts passed prior to its repeal. Since the Act's repeal what was referred to as "England" is now "England and Wales", while references to "England" and "Wales" refer to those political divisions.[citation needed]		England and Wales are treated as a single unit, for most purposes, because the two form the constitutional successor to the former Kingdom of England. The continuance of Scots law was guaranteed under the 1706 Treaty of Union that led to the Acts of Union 1707, and as a consequence English law—and after 1801, Irish law— continued to be separate. Following the two Acts of Union, Parliament can restrict the effect of its laws to part of the realm, and generally the effect of laws, where restricted, was originally applied to one or more of the former kingdoms. Thus, most laws applicable to England also applied to Wales. However, Parliament now passes laws applicable to Wales and not to England (and vice versa), a practice which was rare before the middle of the 20th century. Examples are the Welsh Language Acts 1967 and 1993 and the Government of Wales Act 1998. Measures and Acts of the National Assembly for Wales passed since the Government of Wales Act 2006 also apply in Wales but not in England.		Following the Government of Wales Act, effective since May 2007, the National Assembly for Wales can legislate on matters devolved to it. Following a referendum on 3 March 2011, the Welsh Assembly gained direct law-making powers, without the need to consult Westminster. This was the first time in almost 500 years that Wales had its own powers to legislate. Each piece of Welsh legislation is known as an Act of the Assembly.		For a company to be incorporated in the United Kingdom, its application for registration with Companies House must state "whether the company's registered office is to be situated in England and Wales (or in Wales), in Scotland or in Northern Ireland",[2] which will determine the law applicable to that business entity. A registered office may be specified as "in Wales" if the company wishes to use a name ending cyfyngedig or cyf, rather than Limited or Ltd. and/or to avail itself of certain other privileges relating to the official use of the Welsh language.		Outside of the legal system the position is mixed. Some organisations combine as "England and Wales", others are separate.		The order of precedence in England and Wales is distinct from those of Northern Ireland and Scotland, and from Commonwealth realms.		The national parks of England and Wales have a distinctive legislative framework and history.		
The Port Huron Statement is a 1962 political manifesto of the North American student activist movement Students for a Democratic Society (SDS). It was written primarily by Tom Hayden, a University of Michigan student and then the Field Secretary of SDS, with help from 58 other SDS members, and completed on June 15, 1962, at a United Auto Workers retreat in Port Huron, Michigan (now Lakeport State Park), for the group’s first national convention.[1][2] A few years later, however, the SDS shifted away from labor unions and more towards the Student Non-Violent Coordinating Committee (SNCC).[3]		In December 1964, with the political climate drastically changing, a second printing of the manifesto was issued which included an introductory disclaimer which indicated that "while few of its original writers would agree today with all of its conclusions," the manifesto retained its importance as an "essential source of SDS direction" and "one of the earliest embodiments of the feelings of the new movement of young people which began in the sixties.[4]						The 25,700-word statement "articulated the fundamental problems of American society and laid out a radical vision for a better future".[2] It issued a nonideological call for participatory democracy, "both as a means and an end",[2] based on non-violent civil disobedience and the idea that individual citizens could help make "those social decisions determining the quality and direction" of their lives.[5] Also known as the “Agenda for a Generation”, it "brought the term 'participatory democracy' into the common parlance".[6]		It has been described as "a seminal moment in the development of the New Left"[2] and a "classic statement of [its] principles", but it also revealed the 1960s' tension between communitarianism and individualism.[7] In particular, the statement viewed race ("symbolized by the Southern struggle against racial bigotry") and Cold War–induced alienation ("symbolized by the presence of the Bomb") as the two main problems of modern society.[8]		“Universal controlled disarmament must replace deterrence and arms control as the national defense goal.”[8]		"An imperative task for these publicly disinherited groups, then, is to demand a Democratic Party responsible to their interests. They must support Southern voter registration and Negro political candidates and demand that Democratic Party liberals do the same (in the last Congress, Dixiecrats split with Northern Democrats on 119 of 300 roll-calls, mostly on civil rights, area redevelopment and foreign aid bills; and breach was much larger than in the previous several sessions). Labor should begin a major drive in the South. In the North, reform clubs (either independent or Democratic) should be formed to run against big city regimes on such issues as peace, civil rights, and urban needs. Demonstrations should be held at every Congressional or convention seating of Dixiecrats. A massive research and publicity campaign should be initiated, showing to every housewife, doctor, professor, and worker the damage done to their interests every day a racist occupies a place in the Democratic Party. Where possible, the peace movement should challenge the "peace credentials" of the otherwise-liberals by threatening or actually running candidates against them."[8]		The Port Huron Statement argued that because "the civil rights and peace and student movements are too poor and socially slighted, and the labor movement too quiescent", it should rally support and strengthen itself by looking to universities, which benefit from their "permanent position of social influence" and being "the only mainstream institution that is open to participation by individuals of nearly any viewpoint". However, it stated that this "will involve national efforts at university reform by an alliance of students and faculty" who "must wrest control of the educational process from the administrative bureaucracy", ally with groups outside the university, integrate "major public issues into the curriculum", "make debate and controversy". In short, "They must consciously build a base for their assault upon the loci of power."[8]		
The German occupation of Czechoslovakia (1938–1945) began with the German annexation of Czechoslovakia's northern and western border regions, formerly being part of German-Austria known collectively as the Sudetenland, under terms outlined by the Munich Agreement. German leader Adolf Hitler's pretext for this action was the alleged privations suffered by the ethnic German population living in those regions. New and extensive Czechoslovak border fortifications were also located in the same area.		Following the Anschluss of Austria to Nazi Germany, in March 1938, the conquest of Czechoslovakia became Hitler's next ambition. The incorporation of the Sudetenland into Germany that began on 1 October 1938 left the rest of Czechoslovakia weak, and it became powerless to resist subsequent occupation. On 15 March 1939, the German Wehrmacht moved into the remainder of Czechoslovakia and, from Prague Castle, Hitler proclaimed Bohemia and Moravia the Protectorate of Bohemia and Moravia. The occupation ended with the surrender of Germany following World War II.[1]						Sudeten German pro-Nazi leader Konrad Henlein offered the Sudeten German Party (SdP) as the agent for Hitler's campaign. Henlein met with Hitler in Berlin on 28 March 1938, where he was instructed to raise demands unacceptable to the Czechoslovak government led by president Edvard Beneš. On 24 April, the SdP issued the Karlsbader Programm, demanding autonomy for the Sudetenland and the freedom to profess National Socialist ideology. If Henlein's demands were granted, the Sudetenland would then be able to align itself with Nazi Germany.		I am asking neither that Germany be allowed to oppress three and a half million Frenchmen, nor am I asking that three and a half million Englishmen be placed at our mercy. Rather I am simply demanding that the oppression of three and a half million Germans in Czechoslovakia cease and that the inalienable right to self-determination take its place.		As the tepid reaction to the German Anschluss with Austria had shown, the governments of France, the United Kingdom and Czechoslovakia were set on avoiding war at any cost. The French government did not wish to face Germany alone and took its lead from the British government and its prime minister, Neville Chamberlain. Chamberlain contended that Sudeten German grievances were justified and believed that Hitler's intentions were limited. Britain and France, therefore, advised Czechoslovakia to concede to the German demands. Beneš resisted, and on 20 May 1938 a partial mobilization was under way in response to possible German invasion. Ten days later, Hitler signed a secret directive for war against Czechoslovakia to begin no later than 1 October.		In the meantime, the British government demanded that Beneš request a mediator. Not wishing to sever his government's ties with Western Europe, Beneš reluctantly accepted. The British appointed Lord Runciman and instructed him to persuade Beneš to agree to a plan acceptable to the Sudeten Germans. On 2 September, Beneš submitted the Fourth Plan, granting nearly all the demands of the Karlsbader Programm. Intent on obstructing conciliation, however, the SdP held demonstrations that provoked police action in Ostrava on 7 September. The Sudeten Germans broke off negotiations on 13 September, after which violence and disruption ensued. As Czechoslovak troops attempted to restore order, Henlein flew to Germany, and on 15 September issued a proclamation demanding the takeover of the Sudetenland by Germany.		On the same day, Hitler met with Chamberlain and demanded the swift takeover of the Sudetenland by the Third Reich under threat of war. The Czechs, Hitler claimed, were slaughtering the Sudeten Germans. Chamberlain referred the demand to the British and French governments; both accepted. The Czechoslovak government resisted, arguing that Hitler's proposal would ruin the nation's economy and lead ultimately to German control of all of Czechoslovakia. The United Kingdom and France issued an ultimatum, making a French commitment to Czechoslovakia contingent upon acceptance. On 21 September, Czechoslovakia capitulated. The next day, however, Hitler added new demands, insisting that the claims of Poland and Hungary also be satisfied.		The Czechoslovak capitulation precipitated an outburst of national indignation. In demonstrations and rallies, Czechs and Slovaks called for a strong military government to defend the integrity of the state. A new cabinet—under General Jan Syrový—was installed, and on 23 September 1938 a decree of general mobilization was issued. The Czechoslovak army—modern and possessing an excellent system of frontier fortifications—was prepared to fight. The Soviet Union announced its willingness to come to Czechoslovakia's assistance. Beneš, however, refused to go to war without the support of the Western powers.[citation needed]		On 28 September, Chamberlain appealed to Hitler for a conference. Hitler met the next day, at Munich, with the chiefs of governments of France, Italy and Britain. The Czechoslovak government was neither invited nor consulted. On 29 September, the Munich Agreement was signed by Germany, Italy, France, and Britain. The Czechoslovak government capitulated on 30 September and agreed to abide by the agreement. The Munich Agreement stipulated that Czechoslovakia must cede Sudeten territory to Germany. German occupation of the Sudetenland would be completed by 10 October. An international commission representing Germany, Britain, France, Italy, and Czechoslovakia would supervise a plebiscite to determine the final frontier. Britain and France promised to join in an international guarantee of the new frontiers against unprovoked aggression. Germany and Italy, however, would not join in the guarantee until the Polish and Hungarian minority problems were settled.		On 5 October 1938, Beneš resigned as President of Czechoslovakia, realising that the fall of Czechoslovakia was a fait accompli. Following the outbreak of World War II, he would form a Czechoslovak government-in-exile in London.		In early November 1938, under the First Vienna Award, which was a result of the Munich agreement, Czechoslovakia (and later Slovakia)—after it had failed to reach a compromise with Hungary and Poland—was forced by Germany and Italy to cede southern Slovakia (one third of Slovak territory) to Hungary, while Poland invaded Zaolzie territory shortly after.		As a result, Bohemia, Moravia and Silesia lost about 38% of their combined area to Germany, with some 3.2 million German and 750,000 Czech inhabitants. Hungary, in turn, received 11,882 km2 (4,588 sq mi) in southern Slovakia and southern Ruthenia; according to a 1941 census, about 86.5% of the population in this territory was Hungarian. Meanwhile, Poland annexed the town of Český Těšín with the surrounding area (some 906 km2 (350 sq mi)), some 250,000 inhabitants, Poles made about 36% of population,[2] and two minor border areas in northern Slovakia, more precisely in the regions Spiš and Orava. (226 km2 (87 sq mi), 4,280 inhabitants, only 0.3% Poles).		Soon after Munich, 115,000 Czechs and 30,000 Germans fled to the remaining rump of Czechoslovakia. According to the Institute for Refugee Assistance, the actual count of refugees on 1 March 1939 stood at almost 150,000.[3]		On 4 December 1938, there were elections in Reichsgau Sudetenland, in which 97.32% of the adult population voted for the National Socialist Party. About 500,000 Sudeten Germans joined the National Socialist Party, which was 17.34% of the German population in Sudetenland (the average National Socialist Party participation in Nazi Germany was 7.85%). This means the Sudetenland was the most pro-Nazi region in the Third Reich.[4] Because of their knowledge of the Czech language, many Sudeten Germans were employed in the administration of the Protectorate of Bohemia and Moravia and in Nazi organizations (Gestapo, etc.) The most notable was Karl Hermann Frank, the SS and police general and Secretary of State in the Protectorate.		The greatly weakened Czechoslovak Republic was forced to grant major concessions to the non-Czechs. The executive committee of the Slovak People's Party met at Žilina on 5 October 1938, and with the acquiescence of all Slovak parties except the Social Democrats formed an autonomous Slovak government under Jozef Tiso. Similarly, the two major factions in Subcarpathian Ruthenia, the Russophiles and Ukrainophiles, agreed on the establishment of an autonomous government, which was constituted on 8 October. Reflecting the spread of modern Ukrainian national consciousness, the pro-Ukrainian faction, led by Avhustyn Voloshyn, gained control of the local government and Subcarpathian Ruthenia was renamed Carpatho-Ukraine.		A last-ditch attempt to save Czechoslovakia from total ruin was made by the British and French governments, who on 27 January 1939, concluded an agreement of financial assistance with the Czechoslovak government. In this agreement, the British and French governments undertook to lend the Czechoslovak government ₤8 million and make a gift of ₤4 million. Part of the funds were allocated to help resettle Czechs and Slovaks who had fled from territories lost to Germany, Hungary, and Poland in the Munich Agreement or the Vienna Arbitration Award.[5]		In November 1938, Emil Hácha—succeeding Beneš—was elected president of the federated Second Republic, renamed Czecho-Slovakia and consisting of three parts: Bohemia and Moravia, Slovakia, and Carpatho-Ukraine. Lacking its natural frontier and having lost its costly system of border fortification, the new state was militarily indefensible. In January 1939, negotiations between Germany and Poland broke down. Hitler—intent on war against Poland—needed to eliminate Czechoslovakia first. He scheduled a German invasion of Bohemia and Moravia for the morning of 15 March. In the interim, he negotiated with the Slovak People's Party and with Hungary to prepare the dismemberment of the republic before the invasion. On 13 March, he invited Tiso to Berlin and on 14 March, the Slovak Diet convened and unanimously declared Slovak independence. Carpatho-Ukraine also declared independence but Hungarian troops occupied it on 15 March and eastern Slovakia on 23 March.		Hitler summoned President Hácha to Berlin and during the early hours of 15 March, informed Hácha of the imminent German invasion. Threatening a Luftwaffe attack on Prague, Hitler persuaded Hácha to order the capitulation of the Czechoslovak army. Hácha suffered a heart attack during the meeting, and had to be kept awake by medical staff, eventually giving in and accepting Hitler's surrender terms. Then on the morning of 15 March, German troops entered Bohemia and Moravia, meeting practically no resistance (the only instance of organized resistance took place in Místek where an infantry company commanded by Karel Pavlík fought invading German troops). The Hungarian invasion of Carpatho-Ukraine encountered resistance but the Hungarian army quickly crushed it. On 16 March, Hitler went to Czechoslovakia and from Prague Castle proclaimed the German protectorate of Bohemia and Moravia.		Thus, independent Czechoslovakia collapsed in the wake of foreign aggression and internal tensions. Subsequently, interwar Czechoslovakia has been idealized by its proponents as the only bastion of democracy surrounded by authoritarian and fascist regimes. It has also been condemned by its detractors as an artificial and unworkable creation of intellectuals supported by the great powers. Interwar Czechoslovakia comprised lands and peoples that were far from being integrated into a modern nation-state. Moreover, the dominant Czechs—who had suffered political discrimination under the Habsburgs—were not able to cope with the demands of other nationalities; however, some of the minority demands served as mere pretexts to justify intervention by Germany. Czechoslovakia was able to maintain a viable economy and a democratic political system under the adverse circumstances of the inter-war period.		Shortly before World War II, Czechoslovakia ceased to exist. Its territory was divided into the Protectorate of Bohemia and Moravia, the newly declared Slovak State and the short lived Carpathian Ukraine. While the considerable part of Czechoslovakia was directly joined to the Third Reich, the Carpathian Ukraine was swiftly overrun by the Hungarian Forces and aided by Poland. Some parts (e.g., Zaolzie, Southern Slovakia) were annexed by Poland and Hungary in the autumn of 1938. The Zaolzie region was directly joined to the Third Reich after the German invasion of Poland in September 1939.		The German economy—burdened by heavy militarisation—urgently needed foreign currency. Setting up an artificially high exchange rate between the Czechoslovak Koruna and the Reichsmark brought consumer goods to Germans (and soon created shortages in the Czech lands).		Czechoslovakia was a major manufacturer of machine guns, tanks, and artillery, most of which were assembled in the Škoda factory and had a modern army of 35 divisions. Many of these factories continued to produce Czech designs until factories were converted for German designs. Czechoslovakia also had other major manufacturing companies. Entire steel and chemical factories were moved from Czechoslovakia and reassembled in Linz, Austria which incidentally remains a heavily industrialized sector of the country. In a speech delivered in Reichstag, Hitler stressed also the military importance of occupation, noting that by occupying Czechoslovakia, Germany gained 2,175 field cannons, 469 tanks, 500 anti-aircraft artillery pieces, 43,000 machine guns, 1,090,000 military rifles, 114,000 pistols, about a billion rounds of ammunition and three millions of anti-aircraft grenades. This amount of weaponry would be sufficient to arm about half of the then Wehrmacht.[6] Czechoslovak weaponry later played major part in the German conquest of Poland and France, the countries that pressured the country's surrender to Germany in 1938.		Beneš—the leader of the Czechoslovak government-in-exile—and František Moravec—head of Czechoslovak military intelligence—organized and coordinated a resistance network. Hácha, Prime Minister Alois Eliáš, and the Czechoslovak resistance acknowledged Beneš's leadership. Active collaboration between London and the Czechoslovak home front was maintained throughout the war years. The most important event of the resistance was Operation Anthropoid, the assassination of Reinhard Heydrich, SS leader Heinrich Himmler's deputy and the then Protector of Bohemia and Moravia. Infuriated, Hitler ordered the arrest and execution of 10,000 randomly selected Czechs. Over 10,000 were arrested, and at least 1,300 executed. According to one estimate, 5,000 were killed in reprisals. The assassination resulted in one of the most well-known reprisals of the war. The Nazis completely destroyed the villages of Lidice and Ležáky; all men over 16 years from the village were murdered, and the rest of the population was sent to Nazi concentration camps where many women and nearly all the children were killed.		The Czechoslovak resistance comprised four main groups:		The democratic groups—ON, PÚ, and PVVZ—united in early 1940 and formed the Central Committee of the Home Resistance (Ústřední výbor odboje domácího, ÚVOD). Involved primarily in intelligence gathering, the ÚVOD cooperated with a Soviet intelligence organization in Prague. Following the German invasion of the Soviet Union in June 1941, the democratic groups attempted to create a united front that would include the KSČ. Heydrich's appointment in the fall thwarted these efforts. By mid-1942, the Germans had succeeded in exterminating the most experienced elements of the Czechoslovak resistance forces.		Czechoslovak forces regrouped in 1942–1943. The Council of the Three (R3)—in which the communist underground was also represented—emerged as the focal point of the resistance. The R3 prepared to assist the liberating armies of the U.S. and the Soviet Union. In cooperation with Red Army partisan units, the R3 developed a guerrilla structure.		Guerrilla activity intensified with a rising number of parachuted units in 1944, leading to establishment of partisan groups such as 1st Czechoslovak Partisan Brigade of Jan Žižka, Jan Kozina Brigade or Master Jan Hus Brigade, and especially after the formation of a provisional Czechoslovak government in Košice on 4 April 1945. "National committees" took over the administration of towns as the Germans were expelled. More than 4,850 such committees were formed between 1944 and the end of the war under the supervision of the Red Army. On 5 May, a national uprising began spontaneously in Prague, and the newly formed Czech National Council (Česká národní rada) almost immediately assumed leadership of the revolt. Over 1,600 barricades were erected throughout the city, and some 30,000 Czech men and women battled for three days against 37,000–40,000 German troops backed by tanks and artillery. On 8 May, the German Wehrmacht capitulated; Soviet troops arrived on 9 May.		The Slovak National Uprising ("1944 Uprising") was an armed struggle between German Wehrmacht forces and rebel Slovak troops August–October 1944. It was centered at Banská Bystrica.		The rebel Slovak Army, formed to fight the Germans, had an estimated 18,000 soldiers in August, a total which first increased to 47,000 after mobilisation on 9 September 1944, and later to 60,000, plus 20,000 partisans. However, in late August, German troops were able to disarm the Eastern Slovak Army, which was the best equipped, and thus significantly decreased the power of the Slovak Army. Many members of this force were sent to concentration camps in the Third Reich; others escaped and joined partisan units.		The Slovaks were aided in the Uprising by soldiers and partisans from the Soviet Union, United Kingdom, USA, France, the Czech Republic, and Poland. In total, 32 nations were involved in the Uprising.		Edvard Beneš had resigned as president of the first Czechoslovak Republic on 5 October 1938 after the Nazi coup. In London, he and other Czechoslovak exiles organized a Czechoslovak government-in-exile and negotiated to obtain international recognition for the government and a renunciation of the Munich Agreement and its consequences. After World War II broke out, a Czechoslovak national committee was constituted in France, and under Beneš's presidency sought international recognition as the exiled government of Czechoslovakia. This attempt led to some minor successes, such as the French-Czechoslovak treaty of 2 October 1939, which allowed for the reconstitution of the Czechoslovak army on French territory, yet full recognition was not reached. (The Czechoslovak army in France was established on 24 January 1940, and units of its 1st Infantry Division took part in the last stages of the Battle of France, as did some Czechoslovak fighter pilots in various French fighter squadrons.)		Beneš hoped for a restoration of the Czechoslovak state in its pre-Munich form after the anticipated Allied victory, a false hope. The government in exile—with Beneš as president of republic—was set up in June 1940 in exile in London, with the President living at Aston Abbotts. On 18 July 1940, it was recognised by the British government. Belatedly, the Soviet Union (in the summer of 1941) and the U.S. (in the winter) recognised the exiled government. In 1942, Allied repudiation of the Munich Agreement established the political and legal continuity of the First Republic and de jure recognition of Beneš's de facto presidency. The success of Operation Anthropoid—which resulted in the British-backed assassination of one of Hitler's top henchmen, Reichsprotektor of Bohemia and Moravia Reinhard Heydrich, by Jozef Gabčík and Jan Kubiš on 27 May—influenced the Allies in this repudiation.		The Munich Agreement had been precipitated by the subversive activities of the Sudeten Germans. During the latter years of the war, Beneš worked toward resolving the German minority problem and received consent from the Allies for a solution based on a postwar transfer of the Sudeten German population. The First Republic had been committed to a Western policy in foreign affairs. The Munich Agreement was the outcome. Beneš determined to strengthen Czechoslovak security against future German aggression through alliances with Poland and the Soviet Union. The Soviet Union, however, objected to a tripartite Czechoslovak-Polish-Soviet commitment. In December 1943, Beneš's government concluded a treaty just with the Soviets.		Beneš's interest in maintaining friendly relations with the Soviet Union was motivated also by his desire to avoid Soviet encouragement of a post-war communist coup in Czechoslovakia. Beneš worked to bring Czechoslovak communist exiles in Britain into cooperation with his government, offering far-reaching concessions, including nationalization of heavy industry and the creation of local people's committees at the war's end. In March 1945, he gave key cabinet positions to Czechoslovak communist exiles in Moscow.		Especially after the German reprisals for the assassination of Reinhard Heydrich, most of the Czech resistance groups demanded, with eerie irony and based on Nazi terror during the occupation, ethnic cleansing or the "final solution of the German question" (Czech: konečné řešení německé otázky) which would have to be "solved" by deportation of the ethnic Germans from their homeland.[8] These reprisals included massacres in villages Lidice and Ležáky, although these villages were not connected with Czech resistance.[9]		These demands were adopted by the government-in-exile, which sought the support of the Allies for this proposal, beginning in 1943.[10][11][11] During the occupation of Czechoslovakia, the Government-in-Exile promulgated a series of laws that are now referred to as the "Beneš decrees". One part of these decrees dealt with the status of ethnic Germans and Hungarians in postwar Czechoslovakia, and laid the ground for the deportation of some 3,000,000 Germans and Hungarians from the land that had been their home for centuries (see expulsion of Germans from Czechoslovakia, and Hungarians in Slovakia). The Beneš decrees declared that German property was to be confiscated without compensation. However, the final agreement authorizing the forced population transfer of the Germans was not reached until 2 August 1945 at the end of the Potsdam Conference.		On 8 May 1944, Beneš signed an agreement with Soviet leaders stipulating that Czechoslovak territory liberated by Soviet armies would be placed under Czechoslovak civilian control.		On 21 September, Czechoslovak troops formed in the Soviet Union liberated the village Kalinov, the first liberated settlement of Czechoslovakia near the Dukla Pass in northeastern Slovakia. Czechoslovakia was liberated mostly by Soviet troops (the Red Army), supported by Czech and Slovak resistance, from the east to the west; only southwestern Bohemia was liberated by other Allied troops from the west. Except for the brutalities of the German occupation in Bohemia and Moravia (after the August 1944 Slovak National Uprising also in Slovakia), Czechoslovakia suffered relatively little from the war. Even at the end of the war, German troops massacred Czech civilians, as was for example in Massacre in Trhová Kamenice or Massacre in Javoříčko.		A provisional Czechoslovak government was established by the Soviets in the eastern Slovak city of Košice on 4 April 1945. "National committees" (supervised by the Red Army) took over the administration of towns as the Germans were expelled. Bratislava was taken by the Soviets on 4 April. Prague was taken on 9 May by Soviet troops during the Prague Offensive. When the Soviets arrived, Prague was already in a general state of confusion due to the Prague Uprising. Soviet and other Allied troops were withdrawn from Czechoslovakia in the same year.		On 5 May 1945, in the last moments of the war in Europe, the Prague uprising (Czech: Pražské povstání) began. It was an attempt by the Czech resistance to liberate the city of Prague from German occupation during World War II. The uprising went on until 8 May 1945, ending in a ceasefire the day before the arrival of the Red Army and one day after Victory in Europe Day.		It is estimated that about 345,000 World War II casualties were from Czechoslovakia, 277,000 of them Jews. As many as 144,000 Soviet troops died during the liberation of Czechoslovakia.[12]		In October 1944, Subcarpathian Ruthenia was taken by the Soviets. A Czechoslovak delegation under František Němec was dispatched to the area. The delegation was to mobilize the liberated local population to form a Czechoslovak army and to prepare for elections in cooperation with recently established national committees. Loyalty to a Czechoslovak state was tenuous in Carpathian Ruthenia. Beneš's proclamation of April 1944 excluded former collaborationist Hungarians, Germans and the Rusynophile Ruthenian followers of Andrej Brody and the Fencik Party (who had collaborated with the Hungarians) from political participation. This amounted to approximately ⅓ of the population. Another ⅓ was communist, leaving ⅓ of the population presumably sympathetic to the Czechoslovak Republic.		Upon arrival in Subcarpathian Ruthenia, the Czechoslovak delegation set up headquarters in Khust, and on 30 October issued a mobilization proclamation. Soviet military forces prevented both the printing and the posting of the Czechoslovak proclamation and proceeded instead to organize the local population. Protests from Beneš's government were ignored.[citation needed] Soviet activities led much of the local population to believe that Soviet annexation was imminent. The Czechoslovak delegation was also prevented from establishing a cooperative relationship with the local national committees promoted by the Soviets. On 19 November, the communists—meeting in Mukachevo—issued a resolution requesting separation of Subcarpathian Ruthenia from Czechoslovakia and incorporation into the Ukrainian Soviet Socialist Republic. On 26 November, the Congress of National Committees unanimously accepted the resolution of the communists. The congress elected the National Council and instructed that a delegation be sent to Moscow to discuss union. The Czechoslovak delegation was asked to leave Subcarpathian Ruthenia. Negotiations between the Czechoslovak government and Moscow ensued. Both Czech and Slovak communists encouraged Beneš to cede Subcarpathian Ruthenia. The Soviet Union agreed to postpone annexation until the postwar period to avoid compromising Beneš's policy based on the pre-Munich frontiers.		The treaty ceding Carpathian Ruthenia to the Soviet Union was signed in June 1945. Czechs and Slovaks living in Subcarpathian Ruthenia and Ruthenians (Rusyns) living in Czechoslovakia were given the choice of Czechoslovak or Soviet citizenship.		In May 1945, Czechoslovak troops took possession of the borderland. A Czechoslovak administrative commission composed exclusively of Czechs was established. Sudeten Germans were subjected to restrictive measures and conscripted for compulsory labor.[14] On 15 June, however, Beneš called Czechoslovak authorities to order. In July, Czechoslovak representatives addressed the Potsdam Conference (the U.S., Britain and the Soviet Union) and presented plans for a "humane and orderly transfer" of the Sudeten German population. There were substantial exceptions from expulsions that applied to about 244,000 ethnic Germans who were allowed to remain in Czechoslovakia.		The following groups of ethnic Germans were not deported:		It is estimated that between 700,000 and 800,000 Germans were affected by "wild" expulsions between May and August 1945.[15] The expulsions were encouraged by Czechoslovak politicians and were generally carried out by the order of local authorities, mostly by groups of armed volunteers.[16] However, in some cases it was initiated or pursued by assistance of the regular army.[16]		The expulsion according the Potsdam Conference proceeded from 25 January 1946 till October of that year. An estimated 1.6 million ethnic Germans were deported to the American zone of what would become West Germany. An estimated 800,000 were deported to the Soviet zone (in what would become East Germany).[17] Several thousand died violently during the expulsion and many more died from hunger and illness as a consequence. These casualties include violent deaths and suicides, deaths in "internment camps"[18] and natural causes.[19] The joint Czech-German commission of historians stated in 1996 the following numbers: The deaths caused by violence and abnormal living conditions amount to approximately 10,000 persons killed. Another 5,000–6,000 people died of unspecified reasons related to expulsion making the total amount of victims of the expulsion 15,000–16,000 (this excludes suicides, which make another approximately 3,400 cases).[20][21]		Approximately 225,000 Germans remained in Czechoslovakia, of whom 50,000 emigrated or were expelled soon after.[citation needed]		a ČSR; boundaries and government established by the 1920 constitution. b Annexed by Nazi Germany. c ČSR; included the autonomous regions of Slovakia and Carpathian Ruthenia. d Annexed by Hungary (1939–1945).		e ČSR; declared a "people's democracy" (without formal name change) under the Ninth-of-May Constitution following the 1948 coup. f ČSSR; from 1969, after the Prague Spring, consisted of the Czech Socialist Republic (ČSR) and Slovak Socialist Republic (SSR). g Oblast of the Ukrainian SSR. h Oblast of Ukraine.		
A U.S. state is a constituent political entity of the United States of America. There are currently 50 states, which are bound together in a union with each other. Each state holds governmental jurisdiction over a defined geographic territory, and shares its sovereignty with the United States federal government. Due to the shared sovereignty between each state and the federal government, Americans are citizens of both the federal republic and of the state in which they reside.[3] State citizenship and residency are flexible, and no government approval is required to move between states, except for persons covered by certain types of court orders (e.g., paroled convicts and children of divorced spouses who are sharing custody).		States range in population from just under 600,000 (Wyoming) to over 39 million (California), and in area from 1,214 square miles (3,140 km2) (Rhode Island) to 663,268 square miles (1,717,860 km2) (Alaska). Four states use the term commonwealth rather than state in their full official names.		States are divided into counties or county-equivalents, which may be assigned some local governmental authority but are not sovereign. County or county-equivalent structure varies widely by state. State governments are allocated power by the people (of each respective state) through their individual constitutions. All are grounded in republican principles, and each provides for a government, consisting of three branches: executive, legislative, and judicial.[4]		States possess a number of powers and rights under the United States Constitution. States and their residents are represented in the United States Congress, a bicameral legislature consisting of the Senate and the House of Representatives. Each state is also entitled to select a number of electors (equal to the total number of representatives and senators from that state) to vote in the Electoral College, the body that directly elects the President of the United States. Additionally, each state has the opportunity to ratify constitutional amendments, and, with the consent of Congress, two or more states may enter into interstate compacts with one another.		Historically, the tasks of local law enforcement, public education, public health, regulating intrastate commerce, and local transportation and infrastructure have generally been considered primarily state responsibilities, although all of these now have significant federal funding and regulation as well. Over time, the Constitution has been amended, and the interpretation and application of its provisions have changed. The general tendency has been toward centralization and incorporation, with the federal government playing a much larger role than it once did. There is a continuing debate over states' rights, which concerns the extent and nature of the states' powers and sovereignty in relation to the federal government and the rights of individuals.		The Constitution grants to Congress the authority to admit new states into the Union. Since the establishment of the United States in 1776, the number of states has expanded from the original 13 to 50. Alaska and Hawaii are the most recent states admitted, both in 1959. The Constitution is silent on the question of whether states have the power to secede (withdraw) from the Union. Shortly after the Civil War, the U.S. Supreme Court, in Texas v. White, held that a state cannot unilaterally do so.[5][6]						The 50 U.S. states, in alphabetical order, along with each state's flag:				As sovereign entities, each of the 50 states reserves the right to organize its individual government in any way (within the broad parameters set by the U.S. Constitution) deemed appropriate by its people. As a result, while the governments of the various states share many similar features, they often vary greatly with regard to form and substance. No two state governments are identical.		The government of each state is structured in accordance with its individual constitution. Many of these documents are more detailed and more elaborate than their federal counterpart. The Constitution of Alabama, for example, contains 310,296 words – more than 40 times as many as the U.S. Constitution.[7] In practice, each state has adopted a three-branch system of government, modeled after the federal government, and consisting of three branches (although the three-branch structure is not required): executive, legislative, and judicial.[7][8]		In each state, the chief executive is called the governor, who serves as both head of state and head of government. The governor may approve or veto bills passed by the state legislature, as well as push for the passage of bills supported by the party of the Governor. In 43 states, governors have line item veto power.[9]		Most states have a "plural executive" in which two or more members of the executive branch are elected directly by the people. Such additional elected officials serve as members of the executive branch, but are not beholden to the governor and the governor cannot dismiss them. For example, the attorney general is elected, rather than appointed, in 43 of the 50 U.S. states.		The legislatures of 49 of the 50 states are made up of two chambers: a lower house (termed the House of Representatives, State Assembly, General Assembly or House of Delegates) and a smaller upper house, always termed the Senate. The exception is the unicameral Nebraska Legislature, which is composed of only a single chamber.		Most states have part-time legislatures, while six of the most populated states have full-time legislatures. However, several states with high population have short legislative sessions, including Texas and Florida.[10]		In Baker v. Carr (1962) and Reynolds v. Sims (1964), the U.S. Supreme Court held that all states are required to elect their legislatures in such a way as to afford each citizen the same degree of representation (the one person, one vote standard). In practice, most states choose to elect legislators from single-member districts, each of which has approximately the same population. Some states, such as Maryland and Vermont, divide the state into single- and multi-member districts, in which case multi-member districts must have proportionately larger populations, e.g., a district electing two representatives must have approximately twice the population of a district electing just one. If the governor vetoes legislation, all legislatures may override it, usually, but not always, requiring a two-thirds majority.		In 2013, there were a total of 7,383 legislators in the 50 state legislative bodies. They earned from $0 annually (New Mexico) to $90,526 (California). There were various per diem and mileage compensation.[11]		States can also organize their judicial systems differently from the federal judiciary, as long as they protect the federal constitutional right of their citizens to procedural due process. Most have a trial level court, generally called a District Court, Superior Court or Circuit Court, a first-level appellate court, generally called a Court of Appeal (or Appeals), and a Supreme Court. However, Oklahoma and Texas have separate highest courts for criminal appeals. In New York State the trial court is called the Supreme Court; appeals are then taken to the Supreme Court's Appellate Division, and from there to the Court of Appeals.		Most states base their legal system on English common law (with substantial indigenous changes and incorporation of certain civil law innovations), with the notable exception of Louisiana, a former French colony, which draws large parts of its legal system from French civil law.		Only a few states choose to have the judges on the state's courts serve for life terms. In most of the states the judges, including the justices of the highest court in the state, are either elected or appointed for terms of a limited number of years, and are usually eligible for re-election or reappointment.		All states have unitary governments, local governments are created under state law, and ultimately, local governments within each state are subject to the central authority of that particular state. State governments commonly delegate some authority to local units and channel policy decisions down to them for implementation.[12] In a few states, local units of government are permitted a degree of home rule over various matters. The prevailing legal theory of state preeminence over local governments, referred to as Dillon's Rule, holds that,		A municipal corporation possesses and can exercise the following powers and no others: First, those granted in express words; second, those necessarily implied or necessarily incident to the powers expressly granted; third, those absolutely essential to the declared objects and purposes of the corporation-not simply convenient but indispensible; fourth, any fair doubt as to the existence of a power is resolved by the courts against the corporation-against the existence of the powers.[13]		Each state defines for itself what powers it will allow local governments. Generally, four categories of power may be given to local jurisdictions:		Each state admitted to the Union by Congress since 1789 has entered it on an equal footing with the original States in all respects.[15] With the growth of states' rights advocacy during the antebellum period, the Supreme Court asserted, in Lessee of Pollard v. Hagan (1845), that the Constitution mandated admission of new states on the basis of equality.[16] With the consent of Congress, states may enter into interstate compacts, agreements between two or more states. Compacts are frequently used to manage a shared resource, such as transportation infrastructure or water rights.[17]		Under Article IV of the Constitution, which outlines the relationship between the states, each state is required to give full faith and credit to the acts of each other's legislatures and courts, which is generally held to include the recognition of most contracts and criminal judgments, and before 1865, slavery status. Under the Extradition Clause, a state must extradite people located there who have fled charges of "treason, felony, or other crimes" in another state if the other state so demands. The principle of hot pursuit of a presumed felon and arrest by the law officers of one state in another state are often permitted by a state.[18]		The full faith and credit expectation does have exceptions, some legal arrangements, such as professional licensure and marriages, may be state-specific, and until recently states have not been found by the courts to be required to honor such arrangements from other states.[19] Such legal acts are nevertheless often recognized state-to-state according to the common practice of comity. States are prohibited from discriminating against citizens of other states with respect to their basic rights, under the Privileges and Immunities Clause.		Under Article IV, each state is guaranteed a form of government that is grounded in republican principles, such as the consent of the governed.[20] This guarantee has long been at the fore-front of the debate about the rights of citizens vis-à-vis the government. States are also guaranteed protection from invasion, and, upon the application of the state legislature (or executive, if the legislature cannot be convened), from domestic violence. This provision was discussed during the 1967 Detroit riot, but was not invoked.		Under Article I of the Constitution, which establishes the U.S. Congress, each of the 50 states is represented in the Senate (irrespective of population size) by two senators, and each is guaranteed at least one representative (based on population size) in the House. Senators are elected by popular vote to staggered terms of six years. There are presently 100 U.S. Senators. Representatives are elected from single-member districts by popular vote to terms of two years (not staggered). The size of the House—presently 435 voting members—is set by federal statute. Seats in the House are distributed among the states in proportion to the most recent constitutionally mandated decennial census.[21]		Citizens in each state plus those in the District of Columbia indirectly elect the president and vice president. When casting ballots in presidential elections they are voting for presidential electors, who then, using procedures provided in the 12th amendment, elect the president and vice president. There were 538 electors for the most recent presidential election in 2016.[22] Each state is entitled to a number of electors equal to the total number of representatives and senators from that state; the District of Columbia is entitled to three electors.[23]		While the Constitution does set parameters for the election of federal officials, state law, not federal, regulates most aspects of elections in the U.S., including primaries, the eligibility of voters (beyond the basic constitutional definition), the running of each state's electoral college, as well as the running of state and local elections. All elections—federal, state and local—are administered by the individual states, and some voting rules and procedures may differ among states.[24]		Article V of the Constitution accords states a key role in the process of amending the U.S. Constitution. Amendments may be proposed either by Congress with a two-thirds vote in both the House and the Senate, or by a convention of states called for by two-thirds of the state legislatures.[25] To become part of the Constitution, an amendment must be ratified by either—as determined by Congress—the legislatures of three-quarters of the states or state ratifying conventions in three-quarters of the states.[26] The vote in each state (to either ratify or reject a proposed amendment) carries equal weight, regardless of a state's population or length of time in the Union.		Since the early 20th century, the Supreme Court has interpreted the Commerce Clause of the Constitution of the United States to allow greatly expanded scope of federal power over time, at the expense of powers formerly considered purely states' matters. The Cambridge Economic History of the United States says, "On the whole, especially after the mid-1880s, the Court construed the Commerce Clause in favor of increased federal power."[27] In Wickard v. Filburn 317 U.S. 111 (1942), the court expanded federal power to regulate the economy by holding that federal authority under the commerce clause extends to activities which may appear to be local in nature but in reality effect the entire national economy and are therefore of national concern.[28]		For example, Congress can regulate railway traffic across state lines, but it may also regulate rail traffic solely within a state, based on the reality that intrastate traffic still affects interstate commerce. In recent years, the Court has tried to place limits on the Commerce Clause in such cases as United States v. Lopez and United States v. Morrison.[clarification needed]		Another example of congressional power is its spending power – the ability of Congress to impose taxes and distribute the resulting revenue back to the states (subject to conditions set by Congress).[29] An example of this is the system of federal aid for highways, which include the Interstate Highway System. The system is mandated and largely funded by the federal government, and also serves the interests of the states. By threatening to withhold federal highway funds, Congress has been able to pressure state legislatures to pass a variety of laws.[citation needed] An example is the nationwide legal drinking age of 21, enacted by each state, brought about by the National Minimum Drinking Age Act. Although some objected that this infringes on states' rights, the Supreme Court upheld the practice as a permissible use of the Constitution's Spending Clause in South Dakota v. Dole 483 U.S. 203 (1987).		Article IV also grants to Congress the authority to admit new states into the Union. Since the establishment of the United States in 1776, the number of states has expanded from the original 13 to 50. Each new state has been admitted on an equal footing with the existing states.[16] It also forbids the creation of new states from parts of existing states without the consent of both the affected states and Congress. This caveat was designed to give Eastern states that still had Western land claims (including Georgia, North Carolina, and Virginia), to have a veto over whether their western counties could become states,[15] and has served this same function since, whenever a proposal to partition an existing state or states in order that a region within might either join another state or to create a new state has come before Congress.		Most of the states admitted to the Union after the original 13 were formed from an organized territory established and governed by Congress in accord with its plenary power under Article IV, Section 3, Clause 2.[30] The outline for this process was established by the Northwest Ordinance (1787), which predates the ratification of the Constitution. In some cases, an entire territory has become a state; in others some part of a territory has.		When the people of a territory make their desire for statehood known to the federal government, Congress may pass an enabling act authorizing the people of that territory to organize a constitutional convention to write a state constitution as a step towards admission to the Union. Each act details the mechanism by which the territory will be admitted as a state following ratification of their constitution and election of state officers. Although the use of an enabling act is a traditional historic practice, a number of territories have drafted constitutions for submission to Congress absent an enabling act and were subsequently admitted. Upon acceptance of that constitution, and upon meeting any additional Congressional stipulations, Congress has always admitted that territory as a state.		In addition to the original 13, six subsequent states were never an organized territory of the federal government, or part of one, before being admitted to the Union. Three were set off from an already existing state, two entered the Union after having been sovereign states, and one was established from unorganized territory:		Congress is under no obligation to admit states, even in those areas whose population expresses a desire for statehood. Such has been the case numerous times during the nation's history. In one instance, Mormon pioneers in Salt Lake City sought to establish the state of Deseret in 1849. It existed for slightly over two years and was never approved by the United States Congress. In another, leaders of the Five Civilized Tribes (Cherokee, Chickasaw, Choctaw, Creek, and Seminole) in Indian Territory proposed to establish the state of Sequoyah in 1905, as a means to retain control of their lands.[38] The proposed constitution ultimately failed in the U.S. Congress. Instead, the Indian Territory, along with Oklahoma Territory were both incorporated into the new state of Oklahoma in 1907. The first instance occurred while the nation still operated under the Articles of Confederation. The State of Franklin existed for several years, not long after the end of the American Revolution, but was never recognized by the Confederation Congress, which ultimately recognized North Carolina's claim of sovereignty over the area. The territory comprising Franklin later became part of the Southwest Territory, and ultimately the state of Tennessee.		Additionally, the entry of several states into the Union was delayed due to distinctive complicating factors. Among them, Michigan Territory, which petitioned Congress for statehood in 1835, was not admitted to the Union until 1837, due to a boundary dispute with the adjoining state of Ohio. The Republic of Texas requested annexation to the United States in 1837, but fears about potential conflict with Mexico delayed the admission of Texas for nine years.[39] Also, statehood for Kansas Territory was held up for several years (1854–61) due to a series of internal violent conflicts involving anti-slavery and pro-slavery factions.		Puerto Rico referred to itself as the "Commonwealth of Puerto Rico" in the English version of its constitution, and as "Estado Libre Asociado" (literally, Associated Free State) in the Spanish version.		As with any non-state territory of the United States, its residents do not have voting representation in the federal government. Puerto Rico has limited representation in Congress in the form of a Resident Commissioner, a delegate with limited voting rights in the Committee of the Whole House on the State of the Union, and no voting rights otherwise.[40]		A non-binding referendum on statehood, independence, or a new option for an associated territory (different from the current status) was held on November 6, 2012. Sixty one percent (61%) of voters chose the statehood option, while one third of the ballots were submitted blank.[41][42]		On December 11, 2012, the Legislative Assembly of Puerto Rico enacted a concurrent resolution requesting the President and the Congress of the United States to respond to the referendum of the people of Puerto Rico, held on November 6, 2012, to end its current form of territorial status and to begin the process to admit Puerto Rico as a State.[43]		Another status referendum was held on June 11, 2017. Ninety-seven percent of voters chose statehood. Turnout was low, as only 23% of voters went to the polls.[44]		The intention of the Founding Fathers was that the United States capital should be at a neutral site, not giving favor to any existing state; as a result, the District of Columbia was created in 1800 to serve as the seat of government. As it is not a state, the district does not have representation in the Senate and has a non-voting delegate in the House; neither does it have a sovereign elected government. Additionally, prior to ratification of the 23rd amendment in 1961, district citizens did not the right to vote in Presidential elections.		Some residents of the District support statehood of some form for that jurisdiction – either statehood for the whole district or for the inhabited part, with the remainder remaining under federal jurisdiction. In November 2016, Washington, D.C. residents voted in a statehood referendum in which 86% of voters supported statehood for Washington, D.C.[45] For statehood to be achieved, it must be approved by Congress and signed by the President.[46]		Other possible new states are Guam and the US Virgin Islands, both of which are unincorporated organized territories of the United States. Also, either the Northern Mariana Islands or American Samoa, an unorganized, unincorporated territory, could seek statehood.		The Constitution is silent on the issue of the secession of a state from the union. However, its predecessor document, the Articles of Confederation, stated that the United States "shall be perpetual." The question of whether or not individual states held the right to unilateral secession remained a difficult and divisive one until the American Civil War. In 1860 and 1861, eleven southern states seceded, but following their defeat in the American Civil War were brought back into the Union during the Reconstruction Era. The federal government never recognized the secession of any of the rebellious states.[5][47]		Following the Civil War, the United States Supreme Court, in Texas v. White, held that states did not have the right to secede and that any act of secession was legally void. Drawing on the Preamble to the Constitution, which states that the Constitution was intended to "form a more perfect union" and speaks of the people of the United States in effect as a single body politic, as well as the language of the Articles of Confederation, the Supreme Court maintained that states did not have a right to secede. However, the court's reference in the same decision to the possibility of such changes occurring "through revolution, or through consent of the States," essentially means that this decision holds that no state has a right to unilaterally decide to leave the Union.[5][47]		Four states – Kentucky, Massachusetts, Pennsylvania, and Virginia – adopted constitutions early in their post-colonial existence identifying themselves as commonwealths, rather than states. These commonwealths are states, but legally, each is a commonwealth because the term is contained in its constitution.[48] As a result, "commonwealth" is used in all public and other state writings, actions or activities within their bounds.		The term, which refers to a state in which the supreme power is vested in the people, was first used in Virginia during the Interregnum, the 1649–60 period between the reigns of Charles I and Charles II during which parliament's Oliver Cromwell as Lord Protector established a republican government known as the Commonwealth of England. Virginia became a royal colony again in 1660, and the word was dropped from the full title. When Virginia adopted its first constitution on June 29, 1776, it was reintroduced.[49] Pennsylvania followed suit when it drew up a constitution later that year, as did Massachusetts, in 1780, and Kentucky, in 1792.[48]		The U.S. territories of the Northern Marianas and Puerto Rico are also referred to as commonwealths. This designation does have a legal status different from that of the 50 states. Both of these commonwealths are unincorporated territories of the United States.		The 50 states have taken their names from a wide variety of languages. Twenty-four state names originate from Native American languages. Of these, eight are from Algonquian languages, seven are from Siouan languages, three are from Iroquoian languages, one is from Uto-Aztecan languages and five others are from other indigenous languages. Hawaii's name is derived from the Polynesian Hawaiian language.		Of the remaining names, 22 are from European languages: Seven from Latin (mainly Latinized forms of English names), the rest are from English, Spanish and French. Eleven states are named after individual people, including seven named for royalty and one named after an American president. The origins of six state names are unknown or disputed. Several of the states that derive their names from (corrupted) names used for Native peoples, have retained the plural ending of "s".		The borders of the 13 original states were largely determined by colonial charters. Their western boundaries were subsequently modified as the states ceded their western land claims to the Federal government during the 1780s and 1790s. Many state borders beyond those of the original 13 were set by Congress as it created territories, divided them, and over time, created states within them. Territorial and new state lines often followed various geographic features (such as rivers or mountain range peaks), and were influenced by settlement or transportation patterns. At various times, national borders with territories formerly controlled by other countries (British North America, New France, New Spain including Spanish Florida, and Russian America) became institutionalized as the borders of U.S. states. In the West, relatively arbitrary straight lines following latitude and longitude often prevail, due to the sparseness of settlement west of the Mississippi River.		Once established, most state borders have, with few exceptions, been generally stable. Only two states, Missouri (Platte Purchase) and Nevada, grew appreciably after statehood. Several of the original states ceded land, over a several year period, to the Federal government, which in turn became the Northwest Territory, Southwest Territory, and Mississippi Territory. In 1791 Maryland and Virginia ceded land to create the District of Columbia (Virginia's portion was returned in 1847). In 1850, Texas ceded a large swath of land to the federal government. Additionally, Massachusetts and Virginia (on two occasions), have lost land, in each instance to form a new state.		There have been numerous other minor adjustments to state boundaries over the years due to improved surveys, resolution of ambiguous or disputed boundary definitions, or minor mutually agreed boundary adjustments for administrative convenience or other purposes.[31] Occasionally the United States Congress or the United States Supreme Court have settled state border disputes. One notable example is the case New Jersey v. New York, in which New Jersey won roughly 90% of Ellis Island from New York in 1998.[50]		States may be grouped in regions; there are endless variations and possible groupings. Many are defined in law or regulations by the federal government. For example, the United States Census Bureau defines four statistical regions, with nine divisions.[51] The Census Bureau region definition is "widely used … for data collection and analysis,"[52] and is the most commonly used classification system.[53][54][55] Other multi-state regions are unofficial, and defined by geography or cultural affinity rather than by state lines.		
A super senior is a student in an American four-year educational institution such as a high school or university who has been attending the institution for 5 or more years or has more than the usual number of credits required to graduate without achieving a diploma or bachelor's degree.						Applied to education in the United States, super-seniors usually have the minimum number of credits to graduate but do not for various reasons. These students will advance through the grades (freshman, sophomore, junior- senior) on schedule and are classified as a "senior" for two or more years.		A student-athlete may have been made a redshirt their freshman year, which delays their four-year eligibility for competition in the NCAA until their sophomore year. As a result, they will still be eligible the year after their senior year and may stay in college to continue competition.		Students who intend to complete two or more degrees at the same time are often required to earn approximately 140-150 credits to receive their degrees, rather than the normal 120. It is often in the student's best interest to earn all desired/required undergraduate degrees at the same time as financial aid is more readily available to undergraduates than to graduates returning for additional undergraduate degrees. They could also use summer school sessions as well.		Students may choose to change majors after they are well advanced in their schooling. These students often have enough general education credits and overall credits to graduate but do not have their major-specific credits completed.		Students may take a leave of absence from their university for the difficult circumstances described below, or to pursue other endeavors, such as living, working, or studying abroad, tending to a fledgling business, or pursuing opportunities in their chosen career. Stanford University in particular is known for its sizable constituency of fifth-year (and sometimes even sixth-year) seniors who took time off to volunteer or work overseas. Many of Stanford's more famous alumni have taken a leave of absence and never returned to the school.		Another school well known in this regard is Brigham Young University, the flagship school of The Church of Jesus Christ of Latter-day Saints. Many male BYU students, both at the main campus in Utah and at its branch campuses in Hawaii and Idaho, take two years off during their studies to go on LDS missions. Before a 2012 change in church policy, female students were much less likely to take time off to serve missions (which, for women, last 18 months). Under the old policy, men were allowed to serve at age 19, while women had to wait until 21 (by which time many would be close to finishing degrees, married, or both). With the new policy, males can go on missions immediately after high school as long as they are at least 18, while females can serve once they turn 19.		Super-seniors often include students who chose their classes without considering graduation requirements. They are often missing important requirements for their major or core/general education. Additionally, in changing of majors this creates an important concept in core classes as well as other classes in the curriculum.		For music majors, participation in music ensembles is an obvious requirement over the course of their college career. At many colleges, large ensembles (full choir, concert/marching/jazz band, and/or symphony orchestra) rehearse four hours a week for only one credit (typically, the number of credits correspond to hours spent in class per week). This is to encourage non-music majors to be in the ensembles; however, it is not unusual for music majors to carry 8 classes in a semester for a normal credit load of 15-16 credits.		In many schools, the grade of "D" is enough to receive credit for a class, but not high enough to count towards the student's major or core/general education requirements. A student who receives too many D grades may have enough credits to graduate but not good enough grades.		Some super-seniors have enough credits and requirements to graduate, but do not apply for graduation. In most cases, universities do not automatically grant graduation without application, and as long as tuition is paid, students are not forced to leave the school. This is normally not applicable to high school.		Some super-seniors may be in their fifth or later year in the school but do not have enough credits to graduate. These students appear on paper as juniors or (simple) seniors and do not have enough credits to graduate with their peers.		Some super-seniors take a leave of absence or extra majors on purpose to remain in school for as long as possible.		Students in high-stress majors such as mathematics or those who take a difficult double-major (within the same degree) may choose to take a minimum number of classes (two, three, or four depending on the number of credits offered per class) at one time in order to achieve higher grades or superior retention.		Occasionally a student becomes ill or is injured in mid-semester and cannot complete/pass the classes he/she has begun. If this occurs after the drop-deadline, the school may make a judgment as to whether to count the grades against him/her or drop the student from the semester. In either case, the student would need an additional semester to graduate.		Students may have other difficulties that prevent them from completing the graduation requirements in the traditional four years, such as raising a child or being required to work full-time. Such students may only attend classes on a part-time basis, taking over a decade to complete a bachelor's degree.		Occasionally a student may keep his or her grade point average high enough to remain in school but still fail enough classes that they do not have enough credits to graduate at the end of his or her four years.		At the University of Rochester, students are given the unique opportunity to partake in the Take Five Scholars Program. In this program, the student receives a fifth year, or one additional semester, tuition free while the student studies a subject outside of their major. The student has to apply for this program before their eighth semester.[1]		Agnes Scott College also offers a Fifth Year program, tuition-free. It is typically used to take courses that interest them but they could not take during their first four years, to finish a second major, or to take classes that are prerequisites for a graduate program they wish to pursue.[2]		Many colleges and universities offer a Cooperative Education program. In this program, a student has the opportunity to integrate work experience into his/her education through full-time employment during one or more semesters. Generally, a student will not enroll in any courses during the semester(s) of his/her full-time employment. Therefore, a student participating in Cooperative Education will require more than four years to complete his/her studies.		Students are wait-listed when applying for college entrance or when enrolling in classes at their college or university. A spot on a wait-list delays students that are earning credits for core/general education or toward their major. Being wait-listed at the beginning of their college career or at any point during their time at a college or university prevents them from having enough credits to graduate in the traditional four years.		Some universities reduce or withdraw state funding after a student's fifth year. In addition, parents who are paying for their children's education and living expenses may lose patience and cut their student's funding. Fifth year students are becoming more common as students increasingly choose to progress through difficult courses of study slow paces or pursue multiple majors or degrees. Although States often reduce or withdraw funding, there is still funding available to the students in the form of scholarships, as well as federal money such as work study and student loans.		Public high schools may allow a fifth year super-senior who does not have enough credits to graduate to continue attending their regular school, though students older than age 19 are usually required to attend "continuation school" meant for adults who have not graduated with a high school degree. Students forced into continuation school often choose to drop-out or take the GED.[citation needed]		In the United States, federal law (IDEA) states that public (state) schools are required to keep special-needs students in regular public schools (or a special education school if the student's IEP warrants it) until they graduate from that school or reach the age of 21.		
