Classical antiquity (also the classical era, classical period or classical age) is the long period of cultural history centered on the Mediterranean Sea, comprising the interlocking civilizations of ancient Greece and ancient Rome, collectively known as the Greco-Roman world. It is the period in which Greek and Roman society flourished and wielded great influence throughout Europe, North Africa and Southwestern Asia.		Conventionally, it is taken to begin with the earliest-recorded Epic Greek poetry of Homer (8th–7th century BC), and continues through the emergence of Christianity and the decline of the Roman Empire (5th century AD). It ends with the dissolution of classical culture at the close of Late Antiquity (300–600), blending into the Early Middle Ages (600–1000). Such a wide sampling of history and territory covers many disparate cultures and periods. "Classical antiquity" may refer also to an idealised vision among later people of what was, in Edgar Allan Poe's words, "the glory that was Greece, and the grandeur that was Rome."[1]		The culture of the ancient Greeks, together with some influences from the ancient Near East, was the basis of art,[2] philosophy, society, and educational ideals, until the Roman imperial period. The Romans preserved, imitated and spread over Europe these ideals until they were able to competitively rival the Greek culture, as the Latin language became widespread and the classical world became bilingual, Greek and Latin.[3][4] This Greco-Roman cultural foundation has been immensely influential on the language, politics, law, educational systems, philosophy, science, warfare, poetry, historiography, ethics, rhetoric, art and architecture of the modern world. From the surviving fragments of classical antiquity, a revival movement was gradually formed from the 14th century onwards which came to be known later in Europe as the Renaissance, and again resurgent during various neo-classical revivals in the 18th and 19th centuries.						The earliest period of classical antiquity takes place before the background of gradual re-appearance of historical sources following the Bronze Age collapse. The 8th and 7th centuries BC are still largely proto-historical, with the earliest Greek alphabetic inscriptions appearing in the first half of the 8th century. Homer is usually assumed to have lived in the 8th or 7th century BC, and his lifetime is often taken as marking the beginning of classical antiquity. In the same period falls the traditional date for the establishment of the Ancient Olympic Games, in 776 BC.		The Phoenicians originally expanded from Canaan ports, by the 8th century dominating trade in the Mediterranean. Carthage was founded in 814 BC, and the Carthaginians by 700 BC had firmly established strongholds in Sicily, Italy and Sardinia, which created conflicts of interest with Etruria.		The Archaic period followed the Greek Dark Ages, and saw significant advancements in political theory, and the rise of democracy, philosophy, theatre, poetry, as well as the revitalisation of the written language (which had been lost during the Dark Ages).		In pottery, the Archaic period sees the development of the Orientalizing style, which signals a shift from the Geometric style of the later Dark Ages and the accumulation of influences derived from Egypt, Phoenicia and Syria.		Pottery styles associated with the later part of the Archaic age are the black-figure pottery, which originated in Corinth during the 7th century BC and its successor, the red-figure style, developed by the Andokides Painter in about 530 BC.		The Etruscans had established political control in the region by the late 7th century BC, forming the aristocratic and monarchial elite. The Etruscans apparently lost power in the area by the late 6th century BC, and at this point, the Italic tribes reinvented their government by creating a republic, with much greater restraints on the ability of rulers to exercise power.[5]		According to legend, Rome was founded on April 21, 753 BC by twin descendants of the Trojan prince Aeneas, Romulus and Remus.[6] As the city was bereft of women, legend says that the Latins invited the Sabines to a festival and stole their unmarried maidens, leading to the integration of the Latins and the Sabines.[7]		Archaeological evidence indeed shows first traces of settlement at the Roman Forum in the mid-8th century BC, though settlements on the Palatine Hill may date back to the 10th century BC.[8][9]		The seventh and final king of Rome was Tarquinius Superbus. As the son of Tarquinius Priscus and the son-in-law of Servius Tullius, Superbus was of Etruscan birth. It was during his reign that the Etruscans reached their apex of power.		Superbus removed and destroyed all the Sabine shrines and altars from the Tarpeian Rock, enraging the people of Rome. The people came to object to his rule when he failed to recognize the rape of Lucretia, a patrician Roman, at the hands of his own son. Lucretia's kinsman, Lucius Junius Brutus (ancestor to Marcus Brutus), summoned the Senate and had Superbus and the monarchy expelled from Rome in 510 BC. After Superbus' expulsion, the Senate voted to never again allow the rule of a king and reformed Rome into a republican government in 509 BC. In fact the Latin word "Rex" meaning King became a dirty and hated word throughout the Republic and later on the Empire.[citation needed]		The classical period of Ancient Greece corresponds to most of the 5th and 4th centuries BC, in particular, from the fall of the Athenian tyranny in 510 BC to the death of Alexander the Great in 323 BC. In 510, Spartan troops helped the Athenians overthrow the tyrant Hippias, son of Peisistratos. Cleomenes I, king of Sparta, put in place a pro-Spartan oligarchy conducted by Isagoras.		The Greco-Persian Wars (499–449 BC), concluded by the Peace of Callias gave not only way to the liberation of Greece, Macedon, Thrace, and Ionia from Persian rule, but also resulted in giving the dominant position of Athens in the Delian League, which led to conflict with Sparta and the Peloponnesian League, resulting in the Peloponnesian War (431–404 BC), which ended in a Spartan victory.		Greece entered the 4th century under Spartan hegemony. But by 395 BC the Spartan rulers removed Lysander from office, and Sparta lost her naval supremacy. Athens, Argos, Thebes and Corinth, the latter two of which were formerly Spartan allies, challenged Spartan dominance in the Corinthian War, which ended inconclusively in 387 BC. Later, in 371 BC, the Theban generals Epaminondas and Pelopidas won a victory at the Battle of Leuctra. The result of this battle was the end of Spartan supremacy and the establishment of Theban hegemony. Thebes sought to maintain its position until it was finally eclipsed by the rising power of Macedon in 346 BC.		Under Philip II, (359–336 BC), Macedon expanded into the territory of the Paeonians, the Thracians and the Illyrians. Philip's son, Alexander the Great, (356–323 BC) managed to briefly extend Macedonian power not only over the central Greek city-states, but also to the Persian Empire, including Egypt and lands as far east as the fringes of India. The classical period conventionally ends at the death of Alexander in 323 BC and the fragmentation of his empire, which was at this time divided among the Diadochi.		Classical Greece entered the Hellenistic period with the rise of Macedon and the conquests of Alexander the Great. Greek became the lingua franca far beyond Greece itself, and Hellenistic culture interacted with the cultures of Persia, Central Asia, India and Egypt. Significant advances were made in the sciences (geography, astronomy, mathematics etc.), notably with the followers of Aristotle (Aristotelianism).		The Hellenistic period ended with the rise of the Roman Republic to a super-regional power in the 2nd century BC and the Roman conquest of Greece in 146 BC.		The republican period of Ancient Rome began with the overthrow of the Monarchy c. 509 BC and lasted over 450 years until its subversion, through a series of civil wars, into the Principate form of government and the Imperial period. During the half millennium of the Republic, Rome rose from a regional power of the Latium to the dominant force in Italy and beyond. The unification of Italy under Roman hegemony was a gradual process, brought about in a series of conflicts of the 4th and 3rd centuries, the Samnite Wars, Latin War, and Pyrrhic War. Roman victory in the Punic Wars and Macedonian Wars established Rome as a super-regional power by the 2nd century BC, followed up by the acquisition of Greece and Asia Minor. This tremendous increase of power was accompanied by economic instability and social unrest, leading to the Catiline conspiracy, the Social War and the First Triumvirate, and finally the transformation to the Roman Empire in the latter half of the 1st century BC.		Determining the precise end of the Republic is a task of dispute by modern historians;[10] Roman citizens of the time did not recognize that the Republic had ceased to exist. The early Julio-Claudian "Emperors" maintained that the res publica still existed, albeit under the protection of their extraordinary powers, and would eventually return to its full Republican form. The Roman state continued to call itself a res publica as long as it continued to use Latin as its official language.		Rome acquired imperial character de facto from the 130s BC with the acquisition of Cisalpine Gaul, Illyria, Greece and Hispania, and definitely with the addition of Iudaea, Asia Minor and Gaul in the 1st century BC. At the time of the empire's maximal extension under Trajan (AD 117), Rome controlled the entire Mediterranean as well as Gaul, parts of Germania and Britannia, the Balkans, Dacia, Asia Minor, the Caucasus and Mesopotamia.		Culturally, the Roman Empire was significantly hellenized, but also saw the rise of syncratic "eastern" traditions, such as Mithraism, Gnosticism, and most notably Christianity. The empire began to decline in the crisis of the third century		Late Antiquity saw the rise of Christianity under Constantine I, finally ousting the Roman imperial cult with the Theodosian decrees of 393. Successive invasions of Germanic tribes finalized the decline of the Western Roman Empire in the 5th century, while the Eastern Roman Empire persisted throughout the Middle Ages, in a state called the Roman Empire by its citizens, and labelled the Byzantine Empire by later historians. Hellenistic philosophy was succeeded by continued developments in Platonism and Epicureanism, with Neoplatonism in due course influencing the theology of the Church Fathers.		Many individuals have attempted to put a specific date on the symbolic "end" of antiquity with the most prominent dates being the deposing of the last Western Roman Emperor in 476,[11][12] the closing of the last Platonic Academy in Athens by the Eastern Roman Emperor Justinian I in 529,[13] and the conquest of much of the Mediterranean by the new Muslim faith from 634-718.[14] These Muslim conquests, of Syria (637), Egypt (639), Cyprus (654), North Africa (665), Hispania (718), Southern Gaul (720), Crete (820), and Sicily (827), Malta (870) (and the sieges of the Eastern Roman capital, First Arab Siege of Constantinople (674–78) and Second Arab Siege of Constantinople (717–18)) severed the economic, cultural, and political links that had traditionally united the classical cultures around the Mediterranean, ending antiquity, see (Pirenne Thesis).[15]		The original Roman Senate continued to express decrees into the late 6th century, and the last Eastern Roman emperor to use Latin as the language of his court in Constantinople was emperor Maurice, who reigned until 602. The overthrow of Maurice by his mutinying Danube army under Phocas resulted in the Slavic invasion of the Balkans and the decline of Balkan and Greek urban culture (leading to the flight of Balkan Latin speakers to the mountains, see Origin of the Romanians), and also provoked the Byzantine–Sasanian War of 602–628 in which all the great eastern cities except Constantinople were lost. The resulting turmoil did not end until the Muslim conquests of the 7th century finalized the irreversible loss of all the largest Eastern Roman imperial cities besides the capital itself. The emperor Heraclius in Constantinople, who emerged during this period, conducted his court in Greek, not Latin, though Greek had always been an administrative language of the eastern Roman regions. Eastern-Western links weakened with the ending of the Byzantine Papacy.		The Eastern Roman empire's capital city of Constantinople was left as the only unconquered large urban center of the original Roman empire, as well as being the largest city in Europe. Over the next millennium the Roman culture of that city would slowly change, leading modern historians to refer to it by a new name, Byzantine, though many classical books, sculptures, and technologies survived there along with classical Roman cuisine and scholarly traditions, well into the Middle Ages, when much of it was "rediscovered" by visiting Western crusaders. Indeed, the inhabitants of Constantinople continued to refer to themselves as Romans, as did their eventual conquerors in 1453, the Ottomans. (See Rûm and Romaioi.) The classical scholarship and culture that was still preserved in Constantinople was brought by refugees fleeing its conquest in 1453 and helped to spark the Renaissance, see Greek scholars in the Renaissance.		Ultimately, it was a slow, complex, and graduated change in the socioeconomic structure in European history that led to the changeover between Classical Antiquity and Medieval society and no specific date can truly exemplify that.		Respect for the ancients of Greece and Rome affected politics, philosophy, sculpture, literature, theater, education, architecture, and even sexuality.		In politics, the late Roman conception of the Empire as a universal state, headed by one supreme divinely-appointed ruler, united with Christianity as a universal religion likewise headed by a supreme patriarch, proved very influential, even after the disappearance of imperial authority in the west.		That model continued to exist in Constantinople for the entirety of the Middle Ages; the Byzantine Emperor was considered the sovereign of the entire Christian world. The Patriarch of Constantinople was the Empire's highest-ranked cleric, but even he was subordinate to the Emperor, who was "God's Vicegerent on Earth". The Greek-speaking Byzantines and their descendants continued to call themselves "Romans" until the creation of a new Greek state in 1832.		After the fall of Constantinople in 1453, the Russian Czars (a title derived from Caesar) claimed the Byzantine mantle as the champion of Orthodoxy; Moscow was described as the "Third Rome" and the Czars ruled as divinely-appointed Emperors into the 20th century.		Despite the fact that the Western Roman secular authority disappeared entirely in Europe, it still left traces. The Papacy and the Catholic Church in particular maintained Latin language, culture and literacy for centuries; to this day the popes are called Pontifex Maximus which in the classical period was a title belonging to the Emperor, and the ideal of Christendom carried on the legacy of a united European civilisation even after its political unity had disappeared.		The political idea of an Emperor in the West to match the Emperor in the East continued after the Western Roman Empire's collapse; it was revived by the coronation of Charlemagne in 800; the self-described Holy Roman Empire ruled over central Europe until 1806.		The Renaissance idea that the classical Roman virtues had been lost under medievalism was especially powerful in European politics of the 18th and 19th centuries. Reverence for Roman republicanism was strong among the Founding Fathers of the United States and the Latin American revolutionaries; the Americans described their new government as a republic (from res publica) and gave it a Senate and a President (another Latin term), rather than make use of available English terms like commonwealth or parliament.		Similarly in Revolutionary and Napoleonic France, republicanism and Roman martial virtues were upheld by the state, as can be seen in the architecture of the Panthéon, the Arc de Triomphe, and the paintings of Jacques-Louis David. During the revolution France itself followed the transition from kingdom to republic to dictatorship to Empire (complete with Imperial Eagles) that Rome had undergone centuries earlier.		Epic poetry in Latin continued to be written and circulated well into the 19th century. John Milton and even Arthur Rimbaud got their first poetic education in Latin. Genres like epic poetry, pastoral verse, and the endless use of characters and themes from Greek mythology left a deep mark on literature of the Western World.		In architecture, there have been several Greek Revivals, which seem more inspired in retrospect by Roman architecture than Greek. Washington, DC is filled with large marble buildings with facades made out to look like Roman temples, with columns constructed in the classical orders of architecture.		In philosophy, the efforts of St Thomas Aquinas were derived largely from the thought of Aristotle, despite the intervening change in religion from Hellenic Polytheism to Christianity. Greek and Roman authorities such as Hippocrates and Galen formed the foundation of the practice of medicine even longer than Greek thought prevailed in philosophy. In the French theater, tragedians such as Molière and Racine wrote plays on mythological or classical historical subjects and subjected them to the strict rules of the classical unities derived from Aristotle's Poetics. The desire to dance like a latter-day vision of how the ancient Greeks did it moved Isadora Duncan to create her brand of ballet.		
Reuters /ˈrɔɪtərz/ is an international news agency headquartered in London, England. It is a division of Thomson Reuters.		Until 2008, the Reuters news agency formed part of an independent company, Reuters Group plc, which was also a provider of financial market data. Since the acquisition of Reuters Group by the Thomson Corporation in 2008, the Reuters news agency has been a part of Thomson Reuters, making up the media division. Reuters transmits news in English, French, Arabic, Spanish, German, Italian, Portuguese, Russian, Japanese, Korean, Urdu, and Chinese. It was established in 1851.						The Reuter agency was established in 1851 by Paul Julius Reuter in Britain at the London Royal Exchange. Paul Reuter worked at a book-publishing firm in Berlin and was involved in distributing radical pamphlets at the beginning of the Revolutions in 1848. These publications brought much attention to Reuter, who in 1850 developed a prototype news service in Aachen using homing pigeons and electric telegraphy from 1851 on in order to transmit messages between Brussels and Aachen.[2]		Upon moving to England, he founded Reuter's Telegram Company in 1851. Headquartered in London, the company initially covered commercial news, serving banks, brokerage houses, and business firms.[2] The first newspaper client to subscribe was the London Morning Advertiser in 1858.[2][3] Afterwards more newspapers signed up, with Britannica Encyclopedia writing that "the value of Reuters to newspapers lay not only in the financial news it provided but in its ability to be the first to report on stories of international importance."[2] Reuter's agency built a reputation in Europe and the rest of the world as the first to report news scoops from abroad.[4] Reuters was the first to report Abraham Lincoln's assassination in Europe, for instance, in 1865.[2][4] In 1872, Reuters expanded into the far east, followed by South America in 1874. Both expansions were made possible by advances in overland telegraphs and undersea cables.[4] In 1883, Reuters began transmitting messages electrically to London newspapers.[4]		In 1923, Reuters began using radio to transmit news internationally, a pioneering act.[4] In 1925, The Press Association (PA) of Great Britain acquired a majority interest in Reuters and became full owner some years later.[2] During the world wars, The Guardian reported that Reuters "came under pressure from the British government to serve national interests. In 1941 Reuters deflected the pressure by restructuring itself as a private company." The new owners formed the Reuters Trust.[4] In 1941, the PA sold half of Reuters to the Newspaper Proprieters' Association, and co-ownership was expanded in 1947 to associations that represented daily newspapers in New Zealand and Australia.[2] The Reuters Trust Principles were put in place to maintain the company's independence.[1] At "that point, Reuters had become "one of the world's major news agencies, supplying both text and images to newspapers, other news agencies, and radio and television broadcasters."[2] Also at that point, it directly or through national news agencies provided service "to most countries, reaching virtually all the world's leading newspapers and many thousands of smaller ones," according to Brittanica.[2]		In 1961, Reuters scooped news of the erection of the Berlin Wall.[5] Becoming one of the first news agencies to transmit financial data over oceans via computers in the 1960s,[2] in 1973 Reuters "began making computer-terminal displays of foreign-exchange rates available to clients."[2] In 1981, Reuters began making electronic transactions on its computer network, and afterwards developed a number of electronic brokerage and trading services.[2] Reuters was floated as a public company in 1984,[5] when Reuters Trust was listed on the stock exchanges[4] such as the London Stock Exchange (LSE) and NASDAQ.[2] Reuters published the first story of the Berlin Wall being breached in 1989.[5]		Share price grew during the dotcom boom, then fell after the banking troubles in 2001.[4] In 2002, Brittanica wrote that most news throughout the world came from three major agencies: the Associated Press, Reuters, and Agence France-Presse.[6] Reuters merged with Thomson Corporation in Canada in 2008, forming Thomson Reuters.[2] In 2009, Thomson Reuters withdrew from the LSE and the NASDAQ, instead listing its shares on the Toronto Stock Exchange and the New York Stock Exchange.[2] The last surviving member of the Reuters family founders, Marguerite, Baroness de Reuter, died at age 96 on 25 January 2009.[7] As of 2010, Reuters was headquartered in New York City, and provided financial information to clients while also maintaining its traditional news-agency business.[2]		In 2012, Thomson Reuters appointed Jim Smith as CEO.[1] Almost every major news outlet in the world subscribed to Reuters as of 2014. Reuters operated in more than 200 cities in 94 countries in about 20 languages as of 2014.[citation needed] In July 2016, Thomson Reuters agreed to sell its intellectual property and science operation for $3.55 billion to private equity firms.[8] In October 2016, Thomson Reuters announced expansions and relocations to Toronto.[8] As part of cuts and restructuring, in November 2016, Thomson Reuters Corp. eliminated 2,000 worldwide jobs out of its around 50,000 employees.[8]		The Reuters News Agency employs some 2,500 journalists and 600 photojournalists in about 200 locations worldwide. Reuters journalists use the Reuters Handbook of Journalism[9] as a guide for fair presentation and disclosure of relevant interests, to maintain the values of integrity and freedom upon which their reputation for reliability, accuracy, speed and exclusivity relies.[9]		In May 2000, Kurt Schork, an American reporter, was killed in an ambush while on assignment in Sierra Leone. In April and August 2003, news cameramen Taras Protsyuk and Mazen Dana were killed in separate incidents by U.S. troops in Iraq. In July 2007, Namir Noor-Eldeen and Saeed Chmagh were killed when they were struck by fire from a U.S. military Apache helicopter in Baghdad.[10][11] During 2004, cameramen Adlan Khasanov in Chechnya and Dhia Najim in Iraq were also killed. In April 2008, cameraman Fadel Shana was killed in the Gaza Strip after being hit by an Israeli tank.[12]		The first Reuters journalist to be taken hostage[dubious – discuss] in action was Anthony Grey. Detained by the Chinese government while covering China's Cultural Revolution in Peking in the late 1960s, it was said to be in response to the jailing of several Chinese journalists by the colonial British government of Hong Kong.[13] He was considered to be the first political hostage of the modern age and was released after being imprisoned for 27 months from 1967 to 1969. Awarded an OBE by the British Government after his release, he went on to become a best-selling historical novelist.		In May 2016 the Ukrainian website Myrotvorets published the names and personal data of 4,508 journalists, including Reuters reporters, and other media staff from all over the world, who were accredited by the self-proclaimed authorities in the separatist-controlled regions of eastern Ukraine.[14]		Reuters has a policy of taking a "value-neutral approach," which extends to not using the word "terrorist" in its stories, a practice which has attracted criticism following the September 11 attacks.[16] Reuters' editorial policy states: "We are committed to reporting the facts and in all situations avoid the use of emotive terms. The only exception is when we are quoting someone directly or in indirect speech."[17] (The Associated Press, by contrast, does use the term "terrorist" in reference to non-governmental organizations who carry out attacks on civilian populations.[16])		Following the September 11 attacks, Reuters global head of news Stephen Jukes reiterated the policy in an internal memo and later explained to media columnist Howard Kurtz (who criticized the policy): "We all know that one man's terrorist is another man's freedom fighter, and that Reuters upholds the principle that we do not use the word terrorist...We're trying to treat everyone on a level playing field, however tragic it's been and however awful and cataclysmic for the American people and people around the world. We're there to tell the story. We're not there to evaluate the moral case."[16]		In early October 2001, CEO Tom Glocer and editor-in-chief Geert Linnebank and Jukes later released a statement acknowledging that Jukes' memo "had caused deep offence among members of our staff, our readers, and the public at large" and wrote: "Our policy is to avoid the use of emotional terms and not make value judgments concerning the facts we attempt to report accurately and fairly. We apologize for the insensitive manner in which we characterized this policy and extend our sympathy to all those who have been affected by these tragic events."[18]		In September 2004, The New York Times reported that Reuters global managing editor, David A. Schlesinger objected to Canadian newspapers' editing of Reuters articles to insert the word terrorist. Schlesinger said: "my goal is to protect our reporters and protect our editorial integrity."[19]		In July 2013, David Fogarty, former Reuters climate change correspondent in Asia, resigned after a career of almost 20 years with the company and wrote about a "climate of fear" which resulted in "progressively, getting any climate change-themed story published got harder" following comments from then deputy editor-in-chief Paul Ingrassia that he was a "climate change sceptic". In his comments, Fogarty stated that "Some desk editors happily subbed and pushed the button. Others agonised and asked a million questions. Debate on some story ideas generated endless bureaucracy by editors frightened to take a decision, reflecting a different type of climate within Reuters—the climate of fear," and that "by mid-October, I was informed that climate change just wasn't a big story for the present. …Very soon after that conversation I was told my climate change role was abolished."[20][21] Ingrassia, currently Reuters' managing editor, formerly worked for The Wall Street Journal and Dow Jones for 31 years.[22] Reuters responded to Fogarty's piece by stating that "Reuters has a number of staff dedicated to covering this story, including a team of specialist reporters at Point Carbon and a columnist. There has been no change in our editorial policy."[23]		Subsequently, climate blogger Joe Romm cited a Reuters article on climate as employing "false balance", and quoted Dr. Stefan Rahmstorf, Co-Chair of Earth System Analysis at the Potsdam Institute that "[s]imply, a lot of unrelated climate skeptics nonsense has been added to this Reuters piece. In the words of the late Steve Schneider, this is like adding some nonsense from the Flat Earth Society to a report about the latest generation of telecommunication satellites. It is absurd." Romm opined that "We can't know for certain who insisted on cramming this absurd and non-germane 'climate sceptics nonsense' into the piece, but we have a strong clue. If it had been part of the reporter's original reporting, you would have expected direct quotes from actual skeptics, because that is journalism 101. The fact that the blather was all inserted without attribution suggests it was added at the insistence of an editor."[24]		According to Ynetnews, Reuters was accused of bias against Israel in its coverage of the 2006 Israel–Lebanon conflict after the wire service used two doctored photos by a Lebanese freelance photographer, Adnan Hajj.[25] In August 2006, Reuters announced it had severed all ties with Hajj and said his photographs would be removed from its database.[26]		In 2010, Reuters was criticised again by Haaretz for "anti-Israeli" bias when it cropped the edges of photos, removing commandos' knives held by activists and a naval commando's blood from photographs taken aboard the Mavi Marmara during the Gaza flotilla raid, a raid that left nine Turkish activists dead. It has been alleged that in two separate photographs, knives held by the activists were cropped out of the versions of the pictures published by Reuters.[27] Reuters said it is standard operating procedure to crop photos at the margins, and replaced the cropped images with the original ones after it was brought to the agency's attention.[27]		In March 2015, the Brazilian affiliate of Reuters released a text containing an interview with Brazilian ex-president Fernando Henrique Cardoso about the ongoing Petrobrás scandal. One of the paragraphs mentioned a comment by a former Petrobrás manager, in which he suggests corruption in that company may date back to Cardoso's presidency. Attached to it, there was a comment between parenthesis: "Podemos tirar se achar melhor" ("we can take it out if [you] think it's better"),[28] which is now absent from the current version of the text.[29] The agency later issued a text in which they confirm the mistake, explaining it was a question by one of the Brazilian editors to the journalist who wrote the original text in English, and that it was not supposed to be published.[30]		Bibliography		1 2 3 4 5 6 7		NBC Wall Street Journal Agence France-Presse MSNBC Bloomberg BNA Washington Examiner Talk Media News/Univision		Fox News CBS Radio AP Radio Foreign Pool Time Yahoo! News Dallas Morning News		CBS News Bloomberg McClatchy Washington Times SiriusXM Salem Radio Globe/Roll Call		AP NPR AURN The Hill Regionals Newsmax CBN		ABC News Washington Post Politico Fox News Radio CSM/NY Post Daily Mail BBC/OAN		Reuters NY Times Chicago Tribune VOA RealClearPolitics HuffPost/NY Daily News BuzzFeed/Daily Beast		CNN USA Today ABC Radio National Journal Al Jazeera/PBS Westwood One Financial Times/Guardian		
The Microsoft acquisition hoax is a bogus 1994 press release suggesting that the information technology company Microsoft had acquired the Roman Catholic Church. It is considered to be the first Internet hoax to reach a mass audience.[1][2]		The hoax comprises part of a joke cycle in which Microsoft Corporation is portrayed as a wealthy but evil monopoly built on bloated or unreliable desktop software, planned obsolescence of products, corporate takeovers of once-innovative rivals and litigiousness. While multiple books have been devoted to the subject,[3][4] the jokes most commonly circulate online as Internet memes.		The hoax consisted of a press release, purportedly from the Associated Press, that circulated around the Internet in 1994. The press release claimed that Microsoft "will acquire the Roman Catholic Church in exchange for an unspecified number of shares of Microsoft common stock," and that the company expects "a lot of growth in the religious market in the next five to ten years... the combined resources of Microsoft and the Catholic Church will allow us to make religion easier and more fun for a broader range of people."[5]		Many of the press release's claims were unrealistic, from suggesting that Catholics would soon be able to take Holy Communion through their computer to claiming that conversion to Catholicism was an "upgrade". Despite these warning signs, several readers of the false press release contacted Microsoft to confirm the claims of the hoax, and on December 16, 1994, Microsoft formally debunked the claims.[1][6]		Follow-up press releases made similarly outrageous claims—for example, one false press release claimed that IBM had acquired the Episcopal Church, and another suggested that the Italian television network RAI had invested in what the release claimed to be "Microsoft Corp.'s planned on-line computer service, the Microsoft Divine Network."[1][7]		An Internet meme "Microsoft Acquires" spawned a series of similarly-formatted mock press releases with an assortment of varying acquisition targets, including the government of the United States of America. According to the release, "United States citizens will be able to expect lower taxes, increases in government services, discounts on all Microsoft products and the immediate arrest of all executive officials of Sun Microsystems Inc. and Netscape Corp."[8] One meta-joke claimed that Microsoft ultimately put an end to the jokes by acquiring "Microsoft Acquires".		Despite the proliferation of chain emails circulating the Internet both in 1994 and in the present, the Microsoft hoax was considered the first such hoax to reach a mass audience through the Internet.[1]		
A joke is a humorous question, short story or quip.		Joke(s) or The Joke may also refer to:		In music:		
Used colloquially as a noun or adjective, "highbrow" is synonymous with intellectual; as an adjective, it also means elite, and generally carries a connotation of high culture. The word draws its metonymy from the pseudoscience of phrenology, and was originally simply a physical descriptor.[1]						"Highbrow" can be applied to music, implying most of the classical music tradition; to literature—i.e., literary fiction and poetry; to films in the arthouse line; and to comedy that requires significant understanding of analogies or references to appreciate. The term highbrow is considered by some (with corresponding labels as 'middlebrow' 'lowbrow') as discriminatory or overly selective;[2] and highbrow is currently distanced from the writer by quotation marks: "We thus focus on the consumption of two generally recognised 'highbrow' genres—opera and classical".[3] The first usage in print of highbrow was recorded in 1884.[4] The term was popularized in 1902 by Will Irvin, a reporter for The Sun who adhered to the phrenological notion of more intelligent people having high foreheads.[5]		The opposite of highbrow is lowbrow, and between them is middlebrow, describing culture that is neither high nor low; as a usage, middlebrow is derogatory, as in Virginia Woolf's unsent letter to the New Statesman, written in the 1930s and published in The Death of the Moth and Other Essays (1942). According to the Oxford English Dictionary, the word middlebrow first appeared in print in 1925, in Punch: "The BBC claims to have discovered a new type—'the middlebrow'. It consists of people who are hoping that some day they will get used to the stuff that they ought to like".[6] The term had previously appeared in hyphenated form in 1912:		The Nation, 25th of January, 1912:		It was popularized by the American writer and poet Margaret Widdemer, whose essay "Message and Middlebrow" appeared in the Review of Literature in 1933. The three genres of fiction, as American readers approached them in the 1950s and as obscenity law differentially judged them, are the subject of Ruth Pirsig Wood, Lolita in Peyton Place: Highbrow, Middlebrow, and Lowbrow Novels, 1995.		Prince Hamlet was considered by Virginia Woolf as a highbrow lacking orientation in the world once he had lost the lowbrow Ophelia with her grip on earthly realities: this, she thought, explained why in general highbrows "honour so wholeheartedly and depend so completely upon those who are called lowbrows".[7]		
Robert Hetzron, born Herzog (31 December 1938, Budapest – 12 August 1997, Santa Barbara, California), was a Hungarian-born linguist known for his work on the comparative study of Afro-Asiatic languages, as well as for his study of Cushitic and Ethiopian Semitic languages.[1]						Born in Hungary, Hetzron studied at the University of Budapest. After the 1956 Uprising in Hungary, he moved to Vienna and then to Paris, where he studied with André Martinet and Joseph Tubiana. In 1960/61 he studied Finnish at Jyväskylä, Somali in London, and Italian at Perugia. He received his M.A. degree at the Hebrew University of Jerusalem, 1961-1964 under the supervision of Hans Jakob Polotsky, and his Ph.D. at the University of California, Los Angeles, 1964-1966 under the supervision of Wolf Leslau. From 1966 and until his death he was professor at the University of California, Santa Barbara.[1] Hetzron offered original ideas; first of all, about lingusitic subgrouping in diachrony. According to his explicit and theoretically grounded classification of Semitic, Arabic was grouped in Central rather than South Semitic. He demonstrated that in Ethiopian Semitic, the Gurage group is not genetically valid. His attempt to integrate the description of stress and intonation into syntax is unique (see his Hungarian publications).		The 35th annual meeting of the North American Conference on Afroasiatic Linguistics (NACAL 35, San Antonio, 2007), which was initiated by Robert Hetzron at Santa Barbara in 1972, is dedicated to his memory.		
Stith Thompson (March 7, 1885 – January 10, 1976)[1] was an American scholar of folklore. He is the "Thompson" of the Aarne–Thompson classification system, which indexes certain folktales by their structure and assigns them AT numbers. He also developed an alpha-decimal motif-index system (A~Z followed by numeral) for cataloging individual motifs.						Stith Thompson, born in Bloomfield, Nelson County, Kentucky, on March 7, 1885 as the son of John Warden and Eliza (McClaskey) Thompson moved with his family to Indianapolis at the age of twelve. He attended Butler University and obtained his BA degree from University of Wisconsin.		For the next two years he taught at Lincoln High School in Portland, Oregon, during which time he learned Norwegian from lumberjacks. He earned his master's degree in English literature from the University of California, Berkeley in 1912.		He studied at Harvard University from 1912 to 1914 under George Lyman Kittredge, writing the dissertation "European Borrowings and Parallels in North American Indian Tales," and earning his Ph.D. (The revised thesis was later published in 1919).[2][3] This grew out of Kittredge's assignment, whose theme was investigating a certain tale called "The Blue Band",[a] collected from the Chipewyan tribe in Saskatchewan may derive from contact with an analogous Scandinavian tale.[4][5]		Thompson was English instructor at the University of Texas, Austin from 1914 to 1918, teaching composition. In 1921, he was appointed associate professor at the English Department of the Indiana University (Bloomington), which also had the responsibility of overseeing its composition program.[2]		He collected and archived traditional ballads, tales, proverbs, aphorisms, riddles, etc. The parallels and worldwide distributions of these could be studied using his motif cataloguing apparatus. The first volume of his Motif-Index was printed in 1955.[4]		He organized an informal quadrennial summertime "Institute of Folklore" beginning in 1942 which lasted beyond his retirement from tenure in 1956.[6] In 1962, a permanent Institute of Folklore was established at Bloomington, with Richard Dorson serving as its administrator and chief editor of its journal publication.		In 1976, Thompson died at home of heart failure in Columbus, Indiana.[7]		While Thompson wrote, co-wrote, or translated numerous books and articles on folklore, he became arguably best known for his work on the classification of motifs in folk tales. His six-volume Motif-Index of Folk-Literature (1955–1958) is considered the international key to traditional material.		Thompson's 1954 article for The Filson Club History Quarterly entitled "The Beauchamp Family" continues in use by genealogists as of 2011[update].[8] In this article Thompson states that he is descended from a Costin Beauchamp (b.1738) from Somerset Co., Maryland which extends back to John Beauchamp one of the members of the Plymouth Company.[9]		
The title of Shakespeare's Jest Book has been given to two quite different early Tudor period collections of humorous anecdotes, published within a few years of each other. The first was The Hundred Merry Tales, the only surviving complete edition of which was published in 1526.[1] The other, published about 1530, was titled Merry Tales and Quick Answers and originally contained 113 stories.[2] An augmented edition of 1564 contained 140.		The explanation of the title comes from a reference to one or other collection in William Shakespeare's play Much Ado About Nothing in which the character Beatrice has been accused 'that I had my good wit out of the 100 Merry Tales' (II.sc.1). By that time it seems that the two works were being confounded with each other.[3]		The stories in the 1526 Hundred Merry Tales are largely set in England, mostly in London or the surrounding area, and contain the stock figures of stupid clergymen, unfaithful wives, and Welshmen, the butt of many jokes at the time. Most are followed by a comment on what can be learned from the story. The book's Victorian editors identified a few Italian and French sources from earlier centuries but it was mainly a depository for popular lore that was to figure in more focussed collections published later. In particular, one story there (Of the thre wyse men of Gotam, 24) features the proverbial villagers of Gotham. Another (Of mayster Skelton that broughte the bysshop of Norwiche ii fesauntys, 40) concerns the raffish priest and poet John Skelton, of whom many more stories were to be told in the Merie Tales of Skelton (1566).		Merry Tales and Quick Answers has a wider and more literary range of reference. Among its contents are to be found two of Aesop's Fables dealing with human subjects, Of Thales the astronomer that fell in a ditch (25) and Of the olde woman that had sore eies (89), and two popular tales that were credited to Aesop in later collections: Of hym that sought his wyfe, that was drowned agaynst the streme (55) and Of the olde man and his sonne that brought his asse to the towne to sylle (59). Three of these and yet one more, Of the yonge woman, that sorowed so greatly the deathe of her husbande (10), were to figure later among La Fontaine's Fables. The story of the young widow is a close translation of a fable that had appeared in the Latin collection of Laurentius Abstemius only three decades earlier.		The anecdotes recorded in the work range from Classical history to near contemporary times across the cities of Europe. One scholar comments that the work is ‘mostly drawn from Erasmus and Poggio Bracciolini, but acknowledges little of its inheritance beyond ascribing a handful of its jests to Plutarch’.[4] It certainly owes to Poggio a good deal of its scabrous and scatological content. The following is a list of the principal stories that are common to the English jest book and Poggio's Facetiae.		Shakespeare Jest Books, (London 1864) at Google Books		
Christopher Peterson (February 18, 1950 – October 9, 2012)[1] was the Arthur F. Thurnau professor of psychology at the University of Michigan in Ann Arbor, Michigan and the former chair of the clinical psychology area. He was science director of the VIA Institute on Character, and co-author of Character Strengths and Virtues for the classification of character strengths.[2] He is noted for his work in the study of optimism, health, character, well-being and one of the founders of positive psychology.[3][4] In 2010, Dr. Peterson won the 2010 Golden Apple Award – the most prestigious teaching award at the University of Michigan.		
The chronology of the first dynasty of Babylonia is debated as there is a Babylonian King List A[1] and a Babylonian King List B.[2] In this chronology, the regnal years of List A are used due to their wide usage. The reigns in List B are longer, in general.		The short chronology:		The actual origins of the dynasty are rather hard to pinpoint with great certainty simply because Babylon itself, due to a high water table, yields very few archaeological materials intact. Thus any evidence must come from surrounding regions and written records. Not much is known about the kings from Sumuabum through Sin-muballit other than the fact they were Amorites rather than indigenous Akkadians. What is known, however, is that they accumulated little land. When Hammurabi (also an Amorite) ascended the throne of Babylon, the empire only consisted of a few towns in the surrounding area: Dilbat, Sippar, Kish, and Borsippa. Once Hammurabi was king, his military victories gained land for the empire. However, Babylon remained but one of several important areas in Mesopotamia, along with Assyria, then ruled by Shamshi-Adad I, and Larsa, then ruled by Rim-Sin I.		In Hammurabi's thirtieth year as king, he really began to establish Babylon as the center of what would be a great empire. In that year, he conquered Larsa from Rim-Sin I, thus gaining control over the lucrative urban centers of Nippur, Ur, Uruk, and Isin. In essence, Hammurabi gained control over all of south Mesopotamia. The other formidable political power in the region in the 2nd millennium was Eshnunna, which Hammurabi succeeded in capturing in c. 1761. Babylon exploited Eshnunna's well-established commercial trade routes and the economic stability that came with them. It was not long before Hammurabi's army took Assyria (another economic powerhouse) and parts of the Zagros Mountains. In 1760, Hammurabi finally captured Mari, the final piece of the puzzle that gave him control over virtually all of the territory that made up Mesopotamia under the Third Dynasty of Ur in the 3rd millennium.		Hammurabi's other name was Hammurapi-ilu,[citation needed] meaning "Hammurapi the god" or perhaps "Hammurapi is god." He could have been Amraphel king of Shinar or Sinear in the Jewish records and the Bible, a contemporary of Abraham. Abraham lived from 1871 to 1784, according to modern interpretations of the Old Testament's figures that have been sometimes reckoned in modern half years before the Exodus, from equinox to equinox.[citation needed]		A recent translation of the Chogha Gavaneh tablets which date back to 1800 BC indicates there were close contacts between this town located in the intermontane valley of Islamabad in Central Zagros and Dyala region.		The Venus tablets of Ammisaduqa (i.e., several ancient versions on clay tablets) are famous, and several books had been published about them. Several dates have been offered but the old dates of many sourcebooks seems to be outdated and incorrect. There are further difficulties: the 21-year span of the detailed observations of the planet Venus may or may not coincide with the reign of this king, because his name is not mentioned, only the Year of the Golden Throne. A few sources, some printed almost a century ago, claim that the original text mentions an occultation of the Venus by the moon. However, this may be a misinterpretation.[3] Calculations support 1659 for the fall of Babylon, based on the statistical probability of dating based on the planet's observations. The presently accepted middle chronology is too low from the astronomical point of view.[4]		A text about the fall of Babylon by the Hittites of Mursilis I at the end of Samsuditana's reign which tells about a twin eclipse is crucial for a correct Babylonian chronology. The pair of lunar and solar eclipses occurred in the month Shimanu (Sivan). The lunar eclipse took place on February 9, 1659 BC. It started at 4:43 and ended at 6:47. The latter was invisible which satisfies the record which tells that the setting moon was still eclipsed. The solar eclipse occurred on February 23, 1659. It started at 10:26, has its maximum at 11:45, and ended at 13:04.[5]		
The combined oral contraceptive pill (COCP), often referred to as the birth control pill or colloquially as "the pill", is a birth control method that includes a combination of an estrogen (estradiol) and a progestogen (progestin). When taken by mouth every day, these pills reversibly inhibit female fertility.		They were first approved for contraceptive use in the United States in 1960, and are a very popular form of birth control. They are currently used by more than 100 million women worldwide and by almost 12 million women in the United States.[8] As of 2012, 16% of U.S. women aged 15–44 reported being on the birth control pill, making it the most widely used contraceptive method among women of that age range.[9] Use varies widely by country,[10] age, education, and marital status. One third of women aged 16–49 in the United Kingdom currently use either the combined pill or progestogen-only pill,[11][12] compared with only 1% of women in Japan.[13][needs update]		Two forms are on the World Health Organization's List of Essential Medicines, the most important medications needed in a basic health system.[14] The pill was a catalyst for the sexual revolution.[15]		Combined oral contraceptive pills should be taken at the same time each day. If one or more tablets are forgotten for more than 12 hours, contraceptive protection will be reduced.[16] Most brands of combined pills are packaged in one of two different packet sizes, with days marked off for a 28-day cycle. For the 21-pill packet, a pill is consumed daily for three weeks, followed by a week of no pills. For the 28-pill packet, 21 pills are taken, followed by a week of placebo or sugar pills. A woman on the pill will have a withdrawal bleed sometime during the placebo week, and is still protected from pregnancy during this week. There are also two newer combination birth control pills (Yaz 28 and Loestrin 24 Fe) that have 24 days of active hormone pills, followed by 4 days of placebo.[17]		The placebo pills allow the user to take a pill every day; remaining in the daily habit even during the week without hormones. Placebo pills may contain an iron supplement,[18][19] as iron requirements increase during menstruation.		Failure to take pills during the placebo week does not impact the effectiveness of the pill, provided that daily ingestion of active pills is resumed at the end of the week.		The withdrawal bleeding that occurs during the break from active pills was thought to be reassuring, as a physical confirmation of not being pregnant.[20] The 28-day pill package also simulates the average menstrual cycle, though the hormonal events during a pill cycle are significantly different from those of a normal ovulatory menstrual cycle. The pill suppresses the normal cycle, and the withdrawal bleeding occurs while the placebo pills are taken. The withdrawal bleeding is also predictable. Unexpected breakthrough bleeding can be a possible side effect of longer term active regimens.[21]		If the pill formulation is monophasic, it is possible to skip withdrawal bleeding and still remain protected against conception by skipping the placebo pills and starting directly with the next packet. Attempting this with bi- or tri-phasic pill formulations carries an increased risk of breakthrough bleeding and may be undesirable. It will not, however, increase the risk of getting pregnant.		Starting in 2003, women have also been able to use a three-month version of the Pill.[22] Similar to the effect of using a constant-dosage formulation and skipping the placebo weeks for three months, Seasonale gives the benefit of less frequent periods, at the potential drawback of breakthrough bleeding. Seasonique is another version in which the placebo week every three months is replaced with a week of low-dose estrogen.		A version of the combined pill has also been packaged to completely eliminate placebo pills and withdrawal bleeds. Marketed as Anya or Lybrel, studies have shown that after seven months, 71% of users no longer had any breakthrough bleeding, the most common side effect of going longer periods of time without breaks from active pills.		The estimated probability of pregnancy during the first year of perfect use of the pill is 0.3%, and the estimated probability of pregnancy during the first year of typical use of the pill is 9%.[1] The perfect use failure rate is based on a review of pregnancy rates in clinical trials, the typical use failure rate is based on a weighted average of estimates from the 1995 and 2002 U.S. National Surveys of Family Growth (NSFG), corrected for underreporting of abortions.[1]		Several factors account for typical use effectiveness being lower than perfect use effectiveness:		For instance, someone using oral forms of hormonal birth control might be given incorrect information by a health care provider as to the frequency of intake, forget to take the pill one day, or simply not go to the pharmacy on time to renew the prescription.		COCPs provide effective contraception from the very first pill if started within five days of the beginning of the menstrual cycle (within five days of the first day of menstruation). If started at any other time in the menstrual cycle, COCPs provide effective contraception only after 7 consecutive days use of active pills, so a backup method of contraception must be used until active pills have been taken for 7 consecutive days. COCPs should be taken at approximately the same time every day.[23][24]		Contraceptive efficacy may be impaired by: 1) missing more than one active pill in a packet, 2) delay in starting the next packet of active pills (i.e., extending the pill-free, inactive or placebo pill period beyond 7 days), 3) intestinal malabsorption of active pills due to vomiting or diarrhea, 4) drug interactions with active pills that decrease contraceptive estrogen or progestogen levels.[23]		The effectiveness of the combined oral contraceptive pill appears to be similar whether the active pills are taken continuously for prolonged periods of time or if they are taken for 21 active days and 7 days as placebo.[25]		Instruction for missed pills:		The hormones in "the Pill" have also been used to treat other medical conditions, such as polycystic ovary syndrome (PCOS), endometriosis, amenorrhea, menstrual cramps, adenomyosis, menorrhagia (excessive menstral bleeding), menstruation-related anemia and dysmenorrhea (painful menstruation).[27] Though extensively used for these conditions, no oral contraceptives have been approved by the U.S. FDA for those uses because of lack of convincing scientific evidence that the benefits outweigh the risks.[citation needed] In addition, oral contraceptives are sometimes prescribed as medication for mild or moderate acne, although none are approved by the U.S. FDA for that sole purpose.[28] Three different oral contraceptives have been FDA approved to treat moderate acne if the person is at least 14 or 15 years old, have already begun menstruating, and need contraception. They include Ortho Tri-Cyclen, Estrostep, and YAZ.[29] Although the pill is sometimes prescribed to induce menstruation on a regular schedule for women bothered by irregular menstrual cycles, it actually suppresses the normal menstrual cycle and then mimics a regular 28-day monthly cycle.		Women who are experiencing menstrual dysfunction due to female athlete triad are sometimes prescribed oral contraceptives as pills that can create menstrual bleeding cycles.[30] However, the condition's underlying cause is energy deficiency and should be treated by correcting the imbalance between calories eaten and calories burned by exercise. Oral contraceptives should not be used as an initial treatment for female athlete triad.[30]		Some drugs reduce the effect of the Pill and can cause breakthrough bleeding, or increased chance of pregnancy. These include drugs such as rifampicin, barbiturates, phenytoin and carbamazepine. In addition cautions are given about broad spectrum antibiotics, such as ampicillin and doxycycline, which may cause problems "by impairing the bacterial flora responsible for recycling ethinylestradiol from the large bowel" (BNF 2003).[31][32][33][34]		The traditional medicinal herb St John's Wort has also been implicated due to its upregulation of the P450 system in the liver.		It is generally accepted that the health risks of oral contraceptives are lower than those from pregnancy and birth,[35] and "the health benefits of any method of contraception are far greater than any risks from the method".[36] Some organizations have argued that comparing a contraceptive method to no method (pregnancy) is not relevant—instead, the comparison of safety should be among available methods of contraception.[37]		Different sources note different incidences of side effects. The most common side effect is breakthrough bleeding. A 1992 French review article said that as many as 50% of new first-time users discontinue the birth control pill before the end of the first year because of the annoyance of side effects such as breakthrough bleeding and amenorrhea.[38] One study found that women using birth control pills blinked 32% more often than those not using the contraception.[39]		On the other hand, the pills can sometimes improve conditions such as pelvic inflammatory disease, dysmenorrhea, premenstrual syndrome, and acne,[40] reduce symptoms of endometriosis and polycystic ovary syndrome, and decrease the risk of anemia.[41] Use of oral contraceptives also reduces lifetime risk of ovarian cancer.[42][43]		Nausea, vomiting, headache, bloating, breast tenderness, swelling of the ankles/feet (fluid retention), or weight change may occur. Vaginal bleeding between periods (spotting) or missed/irregular periods may occur, especially during the first few months of use.[44]		Combined oral contraceptives increase the risk of venous thromboembolism (including deep vein thrombosis [DVT] and pulmonary embolism [PE]).[45]		COC pills with more than 50 µg of estrogen increase the risk of ischemic stroke and myocardial infarction but lower doses appear safe.[46] These risks are greatest in women with additional risk factors, such as smoking (which increases risk substantially) and long-continued use of the pill, especially in women over 35 years of age.[47]		The overall absolute risk of venous thrombosis per 100,000 woman-years in current use of combined oral contraceptives is approximately 60, compared with 30 in non-users.[48] The risk of thromboembolism varies with different types of birth control pills; compared with combined oral contraceptives containing levonorgestrel (LNG), and with the same dose of estrogen and duration of use, the rate ratio of deep venous thrombosis for combined oral contraceptives with norethisterone is 0.98, with norgestimate 1.19, with desogestrel (DSG) 1.82, with gestodene 1.86, with drospirenone (DRSP) 1.64, and with cyproterone acetate 1.88.[48] In comparison, venous thromboembolism occurs in 100–200 per 100.000 pregnant women every year.[48]		One study showed more than a 600% increased risk of blood clots for women taking COCPs with drospirenone compared with non-users, compared with 360% higher for women taking birth control pills containing levonorgestrel.[49] The U.S. Food and Drug Administration (FDA) initiated studies evaluating the health of more than 800,000 women taking COCPs and found that the risk of VTE was 93% higher for women who had been taking drospirenone COCPs for 3 months or less and 290% higher for women taking drospirenone COCPs for 7–12 months, compared with women taking other types of oral contraceptives.[50]		Based on these studies, in 2012 the FDA updated the label for drospirenone COCPs to include a warning that contraceptives with drospirenone may have a higher risk of dangerous blood clots.[51]		A systematic review in 2010 did not support an increased overall cancer risk in users of combined oral contraceptive pills, but did find a slight increase in breast cancer risk among current users, which disappears 5–10 years after use has stopped.[52]		COC decreased the risk of ovarian cancer, endometrial cancer,[23] and colorectal cancer.[3][40][53] Two large cohort studies published in 2010 both found a significant reduction in adjusted relative risk of ovarian and endometrial cancer mortality in ever-users of OCs compared with never-users.[2][54]		The use of oral contraceptives (birth control pills) for five years or more decreases the risk of ovarian cancer in later life by 50%.[53] Combined oral contraceptive use reduces the risk of ovarian cancer by 40% and the risk of endometrial cancer by 50% compared with never users. The risk reduction increases with duration of use, with an 80% reduction in risk for both ovarian and endometrial cancer with use for more than 10 years. The risk reduction for both ovarian and endometrial cancer persists for at least 20 years.[23]		A report by a 2005 International Agency for Research on Cancer (IARC) working group said COCs increase the risk of cancers of the breast (among current and recent users),[3] cervix and liver (among populations at low risk of hepatitis B virus infection).[3] A 2013 meta-analysis concluded that every use of birth control pills is associated with a modest increase in the risk of breast cancer (relative risk 1.08) and a reduced risk of colorectal cancer (relative risk 0.86) and endometrial cancer (relative risk 0.57). Cervical cancer risk in those infected with human papilloma virus is increased.[55] A similar small increase in breast cancer risk was seen in other meta analyses.[56][57]		A 2011 Cochrane systematic review found that studies of combination hormonal contraceptives showed no large difference in weight when compared with placebo or no intervention groups.[58] The evidence was not strong enough to be certain that contraceptive methods do not cause some weight change, but no major effect was found.[58] This review also found "that women did not stop using the pill or patch because of weight change."[58]		COCPs may increase natural vaginal lubrication.[59] Other women experience reductions in libido while on the pill, or decreased lubrication.[59][60] Some researchers question a causal link between COCP use and decreased libido;[61] a 2007 study of 1700 women found COCP users experienced no change in sexual satisfaction.[62] A 2005 laboratory study of genital arousal tested fourteen women before and after they began taking COCPs. The study found that women experienced a significantly wider range of arousal responses after beginning pill use; decreases and increases in measures of arousal were equally common.[63][medical citation needed]		A 2006 study of 124 pre-menopausal women measured sex hormone binding globulin (SHBG), including before and after discontinuation of the oral contraceptive pill. Women continuing use of oral contraceptives had SHBG levels four times higher than those who never used it, and levels remained elevated even in the group that had discontinued its use.[64][medical citation needed] Theoretically, an increase in SHBG may be a physiologic response to increased hormone levels, but may decrease the free levels of other hormones, such as androgens, because of the unspecificity of its sex hormone binding.		A 2007 study found the pill can have a negative effect on sexual attractiveness: lapdancers found that women who were in estrus were received much more in tips than those who weren't, while those on the oral contraceptive pill had no such earnings peak.[65]		Low levels of serotonin, a neurotransmitter in the brain, have been linked to depression. High levels of estrogen, as in first-generation COCPs, and progestin, as in some progestin-only contraceptives, have been shown to lower the brain serotonin levels by increasing the concentration of a brain enzyme that reduces serotonin. This observation, along with some small research studies[66] have inspired speculation that the pill causes depression. In 2016, a large Danish study of one million women showed that use of COCPs, especially among adolescents, was associated with a statistically significantly increased risk of subsequent depression, although the sizes of the effects are small (for example, 2.1% of the women who took any form of oral birth control were prescribed anti-depressants for the first time, compared to 1.7% of women in the control group).[67]		Progestin-only contraceptives are known to worsen the condition of women who are already depressed.[68][medical citation needed] However, current medical reference textbooks on contraception[23] and major organizations such as the American ACOG,[69] the WHO,[70] and the United Kingdom's RCOG[71] agree that current evidence indicates low-dose combined oral contraceptives are unlikely to increase the risk of depression, and unlikely to worsen the condition in women that are currently depressed.		Bradykinin lowers blood pressure by causing blood vessel dilation. Certain enzymes are capable of breaking down bradykinin (Angiotensin Converting Enzyme, Aminopeptidase P). Progesterone can increase the levels of Aminopeptidase P (AP-P), thereby increasing the breakdown of bradykinin, which increases the risk of developing hypertension.[72]		Other side effects associated with low-dose COCPs are leukorrhea (increased vaginal secretions), reductions in menstrual flow, mastalgia (breast tenderness), and decrease in acne. Side effects associated with older high-dose COCPs include nausea, vomiting, increases in blood pressure, and melasma (facial skin discoloration); these effects are not strongly associated with low-dose formulations.		Excess estrogen, such as from birth control pills, appears to increase cholesterol levels in bile and decrease gallbladder movement, which can lead to gallstones.[73] Progestins found in certain formulations of oral contraceptive pills can limit the effectiveness of weight training to increase muscle mass.[74][medical citation needed] This effect is caused by the ability of some progestins to inhibit androgen receptors. One study claims that the pill may affect what male body odors a woman prefers, which may in turn influence her selection of partner.[75][medical citation needed] Use of combined oral contraceptives is associated with a reduced risk of endometriosis, giving a relative risk of endometriosis of 0.63 during active use, yet with limited quality of evidence according to a systematic review.[76]		Combined oral contraception decreases total testosterone levels by approximately 0.5 nmol/l, free testosterone by approximately 60%, and increases the amount of sex hormone binding globulin (SHBG) by approximately 100 nmol/l. Contraceptives containing second generation progestins and/or estrogen doses of around 20 –25 mg EE were found to have less impact on SHBG concentrations.[77] Combined oral contraception may also reduce bone density.[78]		Combined oral contraceptives are generally accepted to be contraindicated in women with pre-existing cardiovascular disease, in women who have a familial tendency to form blood clots (such as familial factor V Leiden), women with severe obesity and/or hypercholesterolemia (high cholesterol level), and in smokers over age 35.[79]		COC are also contraindicated for women with liver tumors, hepatic adenoma or severe cirrhosis of the liver, those who have migraine with aura and for those with known or suspected breast cancer.[79]		Combined oral contraceptive pills were developed to prevent ovulation by suppressing the release of gonadotropins. Combined hormonal contraceptives, including COCPs, inhibit follicular development and prevent ovulation as a primary mechanism of action.[80][81][82][83]		Progestogen negative feedback decreases the pulse frequency of gonadotropin-releasing hormone (GnRH) release by the hypothalamus, which decreases the secretion of follicle-stimulating hormone (FSH) and greatly decreases the secretion of luteinizing hormone (LH) by the anterior pituitary. Decreased levels of FSH inhibit follicular development, preventing an increase in estradiol levels. Progestogen negative feedback and the lack of estrogen positive feedback on LH secretion prevent a mid-cycle LH surge. Inhibition of follicular development and the absence of a LH surge prevent ovulation.[80][81][82]		Estrogen was originally included in oral contraceptives for better cycle control (to stabilize the endometrium and thereby reduce the incidence of breakthrough bleeding), but was also found to inhibit follicular development and help prevent ovulation. Estrogen negative feedback on the anterior pituitary greatly decreases the secretion of FSH, which inhibits follicular development and helps prevent ovulation.[80][81][82]		Another primary mechanism of action of all progestogen-containing contraceptives is inhibition of sperm penetration through the cervix into the upper genital tract (uterus and fallopian tubes) by decreasing the water content and increasing the viscosity of the cervical mucus.[80]		The estrogen and progestogen in COCPs have other effects on the reproductive system, but these have not been shown to contribute to their contraceptive efficacy:[80]		Insufficient evidence exists on whether changes in the endometrium could actually prevent implantation. The primary mechanisms of action are so effective that the possibility of fertilization during COCP use is very small. Since pregnancy occurs despite endometrial changes when the primary mechanisms of action fail, endometrial changes are unlikely to play a significant role, if any, in the observed effectiveness of COCPs.[80]		Oral contraceptives come in a variety of formulations, some containing both estrogen and progestins, and some only containing progestin. Doses of component hormones also vary among products, and some pills are monophasic (delivering the same dose of hormones each day) while others are multiphasic (doses vary each day).		COCPs have been somewhat inconsistently grouped into "generations" in the medical literature based on when they were introduced.[84][85]		By the 1930s, Andriy Stynhach had isolated and determined the structure of the steroid hormones and found that high doses of androgens, estrogens or progesterone inhibited ovulation,[86][87][88][89] but obtaining them from European pharmaceutical companies produced from animal extracts was extraordinarily expensive.[90]		In 1939, Russell Marker, a professor of organic chemistry at Pennsylvania State University, developed a method of synthesizing progesterone from plant steroid sapogenins, initially using sarsapogenin from sarsaparilla, which proved too expensive. After three years of extensive botanical research, he discovered a much better starting material, the saponin from inedible Mexican yams (Dioscorea mexicana and Dioscorea composita) found in the rain forests of Veracruz near Orizaba. The saponin could be converted in the lab to its aglycone moiety diosgenin. Unable to interest his research sponsor Parke-Davis in the commercial potential of synthesizing progesterone from Mexican yams, Marker left Penn State and in 1944 co-founded Syntex with two partners in Mexico City. When he left Syntex a year later the trade of the barbasco yam had started and the period of the heyday of the Mexican steroid industry had been started. Syntex broke the monopoly of European pharmaceutical companies on steroid hormones, reducing the price of progesterone almost 200-fold over the next eight years.[23][90][91][92][93][94][95][96][97][98][99][100][101]		Midway through the 20th century, the stage was set for the development of a hormonal contraceptive, but pharmaceutical companies, universities and governments showed no interest in pursuing research.[94]		In early 1951, reproductive physiologist Gregory Pincus, a leader in hormone research and co-founder of the Worcester Foundation for Experimental Biology (WFEB) in Shrewsbury, Massachusetts, first met American birth control movement founder Margaret Sanger at a Manhattan dinner hosted by Abraham Stone, medical director and vice president of Planned Parenthood (PPFA), who helped Pincus obtain a small grant from PPFA to begin hormonal contraceptive research.[102][103][104] Research started on April 25, 1951 with reproductive physiologist Min Chueh Chang repeating and extending the 1937 experiments of Makepeace et al. that showed injections of progesterone suppressed ovulation in rabbits. In October 1951, G. D. Searle & Company refused Pincus' request to fund his hormonal contraceptive research, but retained him as a consultant and continued to provide chemical compounds to evaluate.[90][95][105]		In March 1952, Sanger wrote a brief note mentioning Pincus' research to her longtime friend and supporter, suffragist and philanthropist Katharine Dexter McCormick, who visited the WFEB and its co-founder and old friend Hudson Hoagland in June 1952 to learn about contraceptive research there. Frustrated when research stalled from PPFA's lack of interest and meager funding, McCormick arranged a meeting at the WFEB on June 6, 1953 with Sanger and Hoagland, where she first met Pincus who committed to dramatically expand and accelerate research with McCormick providing fifty times PPFA's previous funding.[95][106]		Pincus and McCormick enlisted Harvard clinical professor of gynecology John Rock, chief of gynecology at the Free Hospital for Women and an expert in the treatment of infertility, to lead clinical research with women. At a scientific conference in 1952, Pincus and Rock, who had known each other for many years, discovered they were using similar approaches to achieve opposite goals. In 1952, Rock induced a three-month anovulatory "pseudo-pregnancy" state in eighty of his infertility patients with continuous gradually increasing oral doses of estrogen (diethylstilbestrol 5–30 mg/day) and progesterone (50–300 mg/day) and within the following four months 15% became pregnant.[95][96][107]		In 1953, at Pincus' suggestion, Rock induced a three-month anovulatory "pseudo-pregnancy" state in twenty-seven of his infertility patients with an oral 300 mg/day progesterone-only regimen for 20 days from cycle days 5–24 followed by pill-free days to produce withdrawal bleeding. This produced the same 15% pregnancy rate during the following four months without the amenorrhea of the previous continuous estrogen and progesterone regimen. But 20% of the women experienced breakthrough bleeding and in the first cycle ovulation was suppressed in only 85% of the women, indicating that even higher and more expensive oral doses of progesterone would be needed to initially consistently suppress ovulation.[108]		Pincus asked his contacts at pharmaceutical companies to send him chemical compounds with progestogenic activity. Chang screened nearly 200 chemical compounds in animals and found the three most promising were Syntex's norethisterone and Searle's noretynodrel and norethandrolone.[109]		Chemists Carl Djerassi, Luis Miramontes, and George Rosenkranz at Syntex in Mexico City had synthesized the first orally highly active progestin norethisterone in 1951. Frank B. Colton at Searle in Skokie, Illinois had synthesized the orally highly active progestins noretynodrel (an isomer of norethisterone) in 1952 and norethandrolone in 1953.[90]		In December 1954, Rock began the first studies of the ovulation-suppressing potential of 5–50 mg doses of the three oral progestins for three months (for 21 days per cycle—days 5–25 followed by pill-free days to produce withdrawal bleeding) in fifty of his infertility patients in Brookline, Massachusetts. Norethisterone or noretynodrel 5 mg doses and all doses of norethandrolone suppressed ovulation but caused breakthrough bleeding, but 10 mg and higher doses of norethisterone or noretynodrel suppressed ovulation without breakthrough bleeding and led to a 14% pregnancy rate in the following five months. Pincus and Rock selected Searle's noretynodrel for the first contraceptive trials in women, citing its total lack of androgenicity versus Syntex's norethisterone very slight androgenicity in animal tests.[110][111]		Noretynodrel (and norethisterone) were subsequently discovered to be contaminated with a small percentage of the estrogen mestranol (an intermediate in their synthesis), with the noretynodrel in Rock's 1954–5 study containing 4–7% mestranol. When further purifying noretynodrel to contain less than 1% mestranol led to breakthrough bleeding, it was decided to intentionally incorporate 2.2% mestranol, a percentage that was not associated with breakthrough bleeding, in the first contraceptive trials in women in 1956. The noretynodrel and mestranol combination was given the proprietary name Enovid.[111][112]		The first contraceptive trial of Enovid led by Celso-Ramón García and Edris Rice-Wray began in April 1956 in Río Piedras, Puerto Rico.[113][114][115][116][117][118][119] A second contraceptive trial of Enovid (and norethisterone) led by Edward T. Tyler began in June 1956 in Los Angeles.[93][120] On January 23, 1957, Searle held a symposium reviewing gynecologic and contraceptive research on Enovid through 1956 and concluded Enovid's estrogen content could be reduced by 33% to lower the incidence of estrogenic gastrointestinal side effects without significantly increasing the incidence of breakthrough bleeding.[121]		On June 10, 1957, the Food and Drug Administration (FDA) approved Enovid 10 mg (9.85 mg noretynodrel and 150 µg mestranol) for menstrual disorders, based on data from its use by more than 600 women. Numerous additional contraceptive trials showed Enovid at 10, 5, and 2.5 mg doses to be highly effective. On July 23, 1959, Searle filed a supplemental application to add contraception as an approved indication for 10, 5, and 2.5 mg doses of Enovid. The FDA refused to consider the application until Searle agreed to withdraw the lower dosage forms from the application. On May 9, 1960, the FDA announced it would approve Enovid 10 mg for contraceptive use, and did so on June 23, 1960. At that point, Enovid 10 mg had been in general use for three years and, by conservative estimate, at least half a million women had used it.[97][116][122]		Although FDA-approved for contraceptive use, Searle never marketed Enovid 10 mg as a contraceptive. Eight months later, on February 15, 1961, the FDA approved Enovid 5 mg for contraceptive use. In July 1961, Searle finally began marketing Enovid 5 mg (5 mg noretynodrel and 75 µg mestranol) to physicians as a contraceptive.[97][98]		Although the FDA approved the first oral contraceptive in 1960, contraceptives were not available to married women in all states until Griswold v. Connecticut in 1965 and were not available to unmarried women in all states until Eisenstadt v. Baird in 1972.[94][98]		The first published case report of a blood clot and pulmonary embolism in a woman using Enavid (Enovid 10 mg in the U.S.) at a dose of 20 mg/day did not appear until November 1961, four years after its approval, by which time it had been used by over one million women.[116][123][124] It would take almost a decade of epidemiological studies to conclusively establish an increased risk of venous thrombosis in oral contraceptive users and an increased risk of stroke and myocardial infarction in oral contraceptive users who smoke or have high blood pressure or other cardiovascular or cerebrovascular risk factors.[97] These risks of oral contraceptives were dramatized in the 1969 book The Doctors' Case Against the Pill by feminist journalist Barbara Seaman who helped arrange the 1970 Nelson Pill Hearings called by Senator Gaylord Nelson.[125] The hearings were conducted by senators who were all men and the witnesses in the first round of hearings were all men, leading Alice Wolfson and other feminists to protest the hearings and generate media attention.[98] Their work led to mandating the inclusion of patient package inserts with oral contraceptives to explain their possible side effects and risks to help facilitate informed consent.[126][127][128] Today's standard dose oral contraceptives contain an estrogen dose that is one third lower than the first marketed oral contraceptive and contain lower doses of different, more potent progestins in a variety of formulations.[23][97][98]		The first oral contraceptive introduced outside the United States was Schering's Anovlar (norethisterone acetate 4 mg + ethinylestradiol 50 µg) on January 1, 1961 in Australia.[129]		The first oral contraceptive introduced in Europe was Schering's Anovlar on June 1, 1961 in West Germany.[129] The lower hormonal dose, still in use, was studied by the Belgian Gynaecologist Ferdinand Peeters.[130][131]		Before the mid-1960s, the United Kingdom did not require pre-marketing approval of drugs. The British Family Planning Association (FPA) through its clinics was then the primary provider of family planning services in Britain and provided only contraceptives that were on its Approved List of Contraceptives (established in 1934). In 1957, Searle began marketing Enavid (Enovid 10 mg in the U.S.) for menstrual disorders. Also in 1957, the FPA established a Council for the Investigation of Fertility Control (CIFC) to test and monitor oral contraceptives which began animal testing of oral contraceptives and in 1960 and 1961 began three large clinical trials in Birmingham, Slough, and London.[116][132]		In March 1960, the Birmingham FPA began trials of noretynodrel 2.5 mg + mestranol 50 µg, but a high pregnancy rate initially occurred when the pills accidentally contained only 36 µg of mestranol—the trials were continued with noretynodrel 5 mg + mestranol 75 µg (Conovid in Britain, Enovid 5 mg in the U.S.).[133] In August 1960, the Slough FPA began trials of noretynodrel 2.5 mg + mestranol 100 µg (Conovid-E in Britain, Enovid-E in the U.S.).[134] In May 1961, the London FPA began trials of Schering's Anovlar.[135]		In October 1961, at the recommendation of the Medical Advisory Council of its CIFC, the FPA added Searle's Conovid to its Approved List of Contraceptives.[136] On December 4, 1961, Enoch Powell, then Minister of Health, announced that the oral contraceptive pill Conovid could be prescribed through the NHS at a subsidized price of 2s per month.[137][138] In 1962, Schering's Anovlar and Searle's Conovid-E were added to the FPA's Approved List of Contraceptives.[116][134][135]		On December 28, 1967, the Neuwirth Law (fr) legalized contraception in France, including the pill.[139] The pill is the most popular form of contraception in France, especially among young women. It accounts for 60% of the birth control used in France. The abortion rate has remained stable since the introduction of the pill.[140]		In Japan, lobbying from the Japan Medical Association prevented the Pill from being approved for general use for nearly 40 years. The higher dose "second generation" pill was approved for use in cases of gynecological problems, but not for birth control. Two main objections raised by the association were safety concerns over long-term use of the Pill, and concerns that the Pill use would lead to diminished use of condoms and thereby potentially increase sexually transmitted infection (STI) rates.[141]		However, when the Ministry of Health and Welfare approved Viagra's use in Japan after only six months of the application's submission, while still claiming that the Pill required more data before approval, women's groups cried foul.[142] The Pill was subsequently approved for use in June 1999. However, the Pill has not become popular in Japan.[143] According to estimates, only 1.3 percent of 28 million Japanese females of childbearing age use the Pill, compared with 15.6 percent in the United States. The Pill prescription guidelines the government has endorsed require Pill users to visit a doctor every three months for pelvic examinations and undergo tests for sexually transmitted diseases and uterine cancer. In the United States and Europe, in contrast, an annual or bi-annual clinic visit is standard for Pill users. However, beginning as far back as 2007, many Japanese OBGYNs have required only a yearly visit for pill users, with multiple checks a year recommended only for those who are older or at increased risk of side effects.[13] As of 2004, condoms accounted for 80% of birth control use in Japan, and this may explain Japan's comparatively low rates of AIDS.[13]		The Pill was approved by the FDA in the early 1960s; its use spread rapidly in the late part of that decade, generating an enormous social impact. Time magazine placed the pill on its cover in April, 1967.[144] In the first place, it was more effective than most previous reversible methods of birth control, giving women unprecedented control over their fertility.[citation needed] Its use was separate from intercourse, requiring no special preparations at the time of sexual activity that might interfere with spontaneity or sensation, and the choice to take the Pill was a private one. This combination of factors served to make the Pill immensely popular within a few years of its introduction.[91][98] Claudia Goldin, among others, argue that this new contraceptive technology was a key player in forming women's modern economic role, in that it prolonged the age at which women first married allowing them to invest in education and other forms of human capital as well as generally become more career-oriented. Soon after the birth control pill was legalized, there was a sharp increase in college attendance and graduation rates for women.[145] From an economic point of view, the birth control pill reduced the cost of staying in school. The ability to control fertility without sacrificing sexual relationships allowed women to make long term educational and career plans.[146]		Because the Pill was so effective, and soon so widespread, it also heightened the debate about the moral and health consequences of pre-marital sex and promiscuity. Never before had sexual activity been so divorced from reproduction. For a couple using the Pill, intercourse became purely an expression of love, or a means of physical pleasure, or both; but it was no longer a means of reproduction. While this was true of previous contraceptives, their relatively high failure rates and their less widespread use failed to emphasize this distinction as clearly as did the Pill. The spread of oral contraceptive use thus led many religious figures and institutions to debate the proper role of sexuality and its relationship to procreation. The Roman Catholic Church in particular, after studying the phenomenon of oral contraceptives, re-emphasized the stated teaching on birth control in the 1968 papal encyclical Humanae vitae. The encyclical reiterated the established Catholic teaching that artificial contraception distorts the nature and purpose of sex.[147] On the other side Anglican and other Protestant churches, such as the Evangelical Church in Germany (EKD) accepted the combined oral contraceptive pill.[148]		The United States Senate began hearings on the Pill in 1970 and there were different viewpoints heard from medical professionals. Dr. Michael Newton, President of the College of Obstetricians and Gynecologists said:		"The evidence is not yet clear that these still do in fact cause cancer or related to it. The FDA Advisory Committee made comments about this, that if there wasn't enough evidence to indicate whether or not these pills were related to the development of cancer, and I think that's still thin; you have to be cautious about them, but I don't think there is clear evidence, either one way or the other, that they do or don't cause cancer."[149]		Another physician, Dr. Roy Hertz of the Population Council, said that anyone who takes this should know of "our knowledge and ignorance in these matters" and that all women should be made aware of this so she can decide to take the Pill or not.[149]		The Secretary of Health, Education, and Welfare at the time, Robert Finch, announced the federal government had accepted a compromise warning statement which would accompany all sales of birth control pills.[149]		At the same time, society was beginning to take note of the impact of the Pill on traditional gender roles. The Pill Versus the Springhill Mine Disaster was the title poem of a 1968 collection by Richard Brautigan, comparing "the people lost inside of" the poet's lover with miners buried underground. Women now did not have to choose between a relationship and a career; singer Loretta Lynn commented on this in her 1974 album with a song entitled "The Pill", which told the story of a married woman's use of the drug to liberate herself from her traditional role as wife and mother. According to writer Margaret Wente, "The pill decoupled sex and marriage, and it also decoupled marriage and procreation. The purpose of marriage was mutual satisfaction, not children, and once that happened, gay marriage probably became inevitable."[150]		A woman using COCPs excretes from her urine and feces natural estrogens, estrone (E1) and estradiol (E2), and synthetic estrogen ethinylestradiol (EE2).[151] These hormones can pass through water treatment plants and into rivers.[152] Other forms of contraception, such as the contraceptive patch, use the same synthetic estrogen (EE2) that is found in COCPs, and can add to the hormonal concentration in the water when flushed down the toilet.[153] This excretion is shown to play a role in causing endocrine disruption, which affects the sexual development and the reproduction, in wild fish populations in segments of streams contaminated by treated sewage effluents.[151][154] A study done in British rivers supported the hypothesis that the incidence and the severity of intersex wild fish populations were significantly correlated with the concentrations of the E1, E2, and EE2 in the rivers.[151]		A review of activated sludge plant performance found estrogen removal rates varied considerably but averaged 78% for estrone, 91% for estradiol, and 76% for ethinylestradiol (estriol effluent concentrations are between those of estrone and estradiol, but estriol is a much less potent endocrine disruptor to fish).[155]		Numerous studies have demonstrated that increasing access to contraception, including birth control pills, can be an effective strategy for climate change mitigation as well as adaptation.[156][157] According to Thomas Wire, contraception is the 'greenest technology' because of its cost-effectiveness in combating global warming — each $7 spent on family planning would reduce global carbon emissions by 1 tonne over four decades, while achieving the same result with low-carbon technologies would require $32.[158] If all the current unmet need for contraception were met, that would reduce global carbon dioxide emissions by 34 gigatonnes between 2010 and 2050.[159]		Mechanism of action COCs prevent fertilization and, therefore, qualify as contraceptives. There is no significant evidence that they work after fertilization. The progestins in all COCs provide most of the contraceptive effect by suppressing ovulation and thickening cervical mucus, although the estrogens also make a small contribution to ovulation suppression. Cycle control is enhanced by the estrogen. Because COCs so effectively suppress ovulation and block ascent of sperm into the upper genital tract, the potential impact on endometrial receptivity to implantation is almost academic. When the two primary mechanisms fail, the fact that pregnancy occurs despite the endometrial changes demonstrates that those endometrial changes do not significantly contribute to the pill's mechanism of action.		Ten different progestins have been used in the COCs that have been sold in the United States. Several different classification systems for the progestins exist, but the one most commonly used system recapitulates the history of the pill in the United States by categorizing the progestins into the so-called "generations of progestins." The first three generations of progestins are derived from 19-nortestosterone. The fourth generation is drospirenone. Newer progestins are hybrids. First-generation progestins. First-generation progestins include noretynodrel, norethisterone, norethisterone acetate, and etynodiol diacetate… These compounds have the lowest potency and relatively short half-lives. The short half-life did not matter in the early, high-dose pills but as doses of progestin were decreased in the more modern pills, problems with unscheduled spotting and bleeding became more common. Second-generation progestins. To solve the problem of unscheduled bleeding and spotting, the second generation progestins (norgestrol and levonorgestrel) were designed to be significantly more potent and to have longer half-lives than norethisterone-related progestins... The second-generation progestins have been associated with more androgen-related side-effects such as adverse effect on lipids, oily skin, acne, and facial hair growth. Third-generation progestins. Third-generation progestins (desogestrel, norgestimate and elsewhere, gestodene) were introduced to maintain the potent progestational activity of second-generation progestins, but to reduce androgeneic side effects. Reduction in androgen impacts allows a fuller expression of the pill's estrogen impacts. This has some clinical benefits… On the other hand, concern arose that the increased expression of estrogen might increase the risk of venous thromboembolism (VTE). This concern introduced a pill scare in Europe until international studies were completed and correctly interpreted. Fourth-generation progestins. Drospirenone is an anologue of spironolactone, a potassium-sparing diuretic used to treat hypertension. Drospirenone possesses anti-mineralocorticoid and anti-androgenic properties. These properties have led to new contraceptive applications, such as treatment of premenstrual dysphoric disorder and acne… In the wake of concerns around possible increased VTE risk with less androgenic third-generation formulations, those issues were anticipated with drospirenone. They were clearly answered by large international studies. Next-generation progestins. More recently, newer progestins have been developed with properties that are shared with different generations of progestins. They have more profound, diverse, and discrete effects on the endometrium than prior progestins. This class would include dienogest (United States) and nomegestrol (Europe).		Media related to Contraceptive pills at Wikimedia Commons		
In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the ISO.[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.		A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.		The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.[4][5][6]		The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.						A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]		For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).		DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.		The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.		The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".[15] Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL provides the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18] This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.		Major applications of the DOI system currently include:		In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]		A multilingual European DOI registration agency activity, mEDRA, and a Chinese registration agency, Wanfang Data, are active in non-English language markets.		The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.		The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[20] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[21] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[22]		The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.		The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.		A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[23]		A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[24]		The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).		A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.		DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.		To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.		Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[25] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.		Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[26][27] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[28] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[26][28]		An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[29] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.		The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[30] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.		The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.		Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[31]		Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.		The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[32] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[33] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[34] The final standard was published on 23 April 2012.[1]		DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[35]		The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[36]		The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:		URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.		
Psychology is the science of behavior and mind, embracing all aspects of conscious and unconscious experience as well as thought. It is an academic discipline and a social science which seeks to understand individuals and groups by establishing general principles and researching specific cases.[1][2]		In this field, a professional practitioner or researcher is called a psychologist and can be classified as a social, behavioral, or cognitive scientist. Psychologists attempt to understand the role of mental functions in individual and social behavior, while also exploring the physiological and biological processes that underlie cognitive functions and behaviors.		Psychologists explore behavior and mental processes, including perception, cognition, attention, emotion (affect), intelligence, phenomenology, motivation (conation), brain functioning, and personality. This extends to interaction between people, such as interpersonal relationships, including psychological resilience, family resilience, and other areas. Psychologists of diverse orientations also consider the unconscious mind.[3] Psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. In addition, or in opposition, to employing empirical and deductive methods, some—especially clinical and counseling psychologists—at times rely upon symbolic interpretation and other inductive techniques. Psychology has been described as a "hub science",[4] with psychological findings linking to research and perspectives from the social sciences, natural sciences, medicine, humanities, and philosophy.		While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts psychology ultimately aims to benefit society.[5][6] The majority of psychologists are involved in some kind of therapeutic role, practicing in clinical, counseling, or school settings. Many do scientific research on a wide range of topics related to mental processes and behavior, and typically work in university psychology departments or teach in other academic settings (e.g., medical schools, hospitals). Some are employed in industrial and organizational settings, or in other areas[7] such as human development and aging, sports, health, and the media, as well as in forensic investigation and other aspects of law.						The word psychology derives from Greek roots meaning study of the psyche, or soul (ψυχή psukhē, "breath, spirit, soul" and -λογία -logia, "study of" or "research").[8] The Latin word psychologia was first used by the Croatian humanist and Latinist Marko Marulić in his book, Psichiologia de ratione animae humanae in the late 15th century or early 16th century.[9] The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary which refers to "Anatomy, which treats the Body, and Psychology, which treats of the Soul."[10]		In 1890, William James defined psychology as "the science of mental life, both of its phenomena and their conditions". This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by radical behaviorists such as John B. Watson, who in his 1913 manifesto defined the discipline of psychology as the acquisition of information useful to the control of behavior. Also since James defined it, the term more strongly connotes techniques of scientific experimentation.[11][12] Folk psychology refers to the understanding of ordinary people, as contrasted with that of psychology professionals.[13]		The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise),[14] addressed the workings of the mind.[15] As early as the 4th century BC, Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes.[16]		In China, psychological understanding grew from the philosophical works of Laozi and Confucius, and later from the doctrines of Buddhism. This body of knowledge involves insights drawn from introspection and observation, as well as techniques for focused thinking and acting. It frames the universe as a division of, and interaction between, physical reality and mental reality, with an emphasis on purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship focused on the brain advanced in the Qing Dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function.[17]		Distinctions in types of awareness appear in the ancient thought of India, influenced by Hinduism. A central idea of the Upanishads is the distinction between a person's transient mundane self and their eternal unchanging soul. Divergent Hindu doctrines, and Buddhism, have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga is a range of techniques used in pursuit of this goal. Much of the Sanskrit corpus was suppressed under the British East India Company followed by the British Raj in the 1800s. However, Indian doctrines influenced Western thinking via the Theosophical Society, a New Age group which became popular among Euro-American intellectuals.[18]		Psychology was a popular topic in Enlightenment Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum—most notably, that among an infinity of human perceptions and desires, the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia empirica in 1732 and Psychologia rationalis in 1734. This notion advanced further under Immanuel Kant, who established the idea of anthropology, with psychology as an important subdivision. However, Kant explicitly and notoriously rejected the idea of experimental psychology, writing that "the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object." Having consulted philosophers Hegel and Herbart, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation.[19] In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's well-populated mental asylums.[20]		Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s, articulating the principle that human perception of a stimulus varies logarithmically according to its intensity.[21] Fechner's 1860 Elements of Psychophysics challenged Kant's stricture against quantitative study of the mind.[19] In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, establishing the psychological laboratory which brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of material.[22] Paul Flechsig and Emil Kraepelin soon created another influential psychology laboratory at Leipzig, this one focused on more on experimental psychiatry.[19]		Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories.[23] G. Stanley Hall who studied with Wundt, formed a psychology lab at Johns Hopkins University in Maryland, which became internationally influential. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo.[24] Wundt assistant Hugo Münsterberg taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta.[18] Wundt students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests for mental ability. Catell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Wittmer focused on mental testing of children; Scott, on selection of employees.[25]		Another student of Wundt, Edward Titchener, created the psychology program at Cornell University and advanced a doctrine of "structuralist" psychology. Structuralism sought to analyze and classify different aspects of the mind, primarily through the method of introspection.[26] William James, John Dewey and Harvey Carr advanced a more expansive doctrine called functionalism, attuned more to human–environment actions. In 1890 James wrote an influential book, The Principles of Psychology, which expanded on the realm of structuralism, memorably described the human "stream of consciousness", and interested many American students in the emerging discipline.[26][27][28] Dewey integrated psychology with social issues, most notably by promoting the cause progressive education to assimilate immigrants and inculcate moral values in children.[29]		A different strain of experimentalism, with more connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires.[30] Russia, too, placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, "Who Is to Develop Psychology and How?" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic viewpoint on human behavior.[31]		Wolfgang Kohler, Max Wertheimer and Kurt Koffka co-founded the school of Gestalt psychology (not to be confused with the Gestalt therapy of Fritz Perls). This approach is based upon the idea that individuals experience things as unified wholes. Rather than breaking down thoughts and behavior into smaller elements, as in structuralism, the Gestaltists maintained that whole of experience is important, and differs from the sum of its parts. Other 19th-century contributors to the field include the German psychologist Hermann Ebbinghaus, a pioneer in the experimental study of memory, who developed quantitative models of learning and forgetting at the University of Berlin,[32] and the Russian-Soviet physiologist Ivan Pavlov, who discovered in dogs a learning process that was later termed "classical conditioning" and applied to human beings.[33]		One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted 1885–1893. The first meeting of the International Congress of Psychology took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the four hundred attendees. The American Psychological Association was founded soon after, in 1892. The International Congress continued to be held, at different locations in Europe, with wider international participation. The Sixth Congress, Geneva 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the American Psychological Association[23] Tokyo Imperial University led the way in bringing the new psychology to the East, and from Japan these ideas diffused into China.[17][24]		American psychology gained status during World War I, during which a standing committee headed by Robert Yerkes administered mental tests ("Army Alpha" and "Army Beta") to almost 1.8 million GIs.[34] Subsequent funding for behavioral research came in large part from the Rockefeller family, via the Social Science Research Council.[35][36] Rockefeller charities funded the National Committee on Mental Hygiene, which promoted the concept of mental illness and lobbied for psychological supervision of child development.[34][37] Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations established sex research as a viable discipline in the U.S.[38] Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also had a significant impact on American psychology; in the 1910s and 1920s, eugenics became a standard topic in psychology classes.[39]		During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology—through the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941, and "the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government." Cartwright also wrote that psychologists had significant roles in managing the domestic economy.[40] The Army rolled out its new General Classification Test and engaged in massive studies of troop morale. In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency to fund research on psychological warfare.[41] In 1965, public controversy called attention to the Army's Project Camelot—the "Manhattan Project" of social science—an effort which enlisted psychologists and anthropologists to analyze foreign countries for strategic purposes.[42][43]		In Germany after World War I, psychology held institutional power through the military, and subsequently expanded along with the rest of the military under the Third Reich.[19] Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler.[44] The Göring Institute was well-financed throughout the war with a mandate to create a "New German Psychotherapy". This psychotherapy aimed to align suitable Germans with the overall goals of the Reich; as described by one physician: "Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the Volk and the Gemeinschaft." Psychologists were to provide Seelenführung, leadership of the mind, to integrate people into the new vision of a German community.[45] Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed.[46] Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process.[47] After the war, some new institutions were created and some psychologists were discredited due to Nazi affiliation. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche and with funding from the Rockefeller Foundation established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students.[48]		After the Russian Revolution, psychology was heavily promoted by the Bolsheviks as a way to engineer the "New Man" of socialism. Thus, university psychology departments trained large numbers of students, for whom positions were made available at schools, workplaces, cultural institutions, and in the military. An especial focus was pedology, the study of child development, regarding which Lev Vygotsky became a prominent writer.[31] The Bolsheviks also promoted free love and embranced the doctrine of psychoanalysis as an antidote to sexual repression.[49] Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet state.[31] Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society.[50] Following World War II, Jewish psychologists past and present (including Vygotsky, A. R. Luria, and Aron Zalkind) were denounced; Ivan Pavlov (posthumously) and Stalin himself were aggrandized as heroes of Soviet psychology.[51] Soviet academics was speedily liberalized during the Khrushchev Thaw, and cybernetics, linguistics, genetics, and other topics became acceptable again. There emerged a new field called "engineering psychology" which studied mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior.[52]		Twentieth-century Chinese psychology originally modeled the United States, with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning, with the aspiration that education would enable modernization and nationalization. John Dewey, who lectured to Chinese audiences in 1918–1920, had a significant influence on this doctrine. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism.[53] After the Chinese Communist Party gained control of the country, the Stalinist USSR became the leading influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved concept of behavior change. Chinese psychologists elaborated on Lenin's model of a "reflective" consciousness, envisioning an "active consciousness" (tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of "recognition" (jen-shih) which referred the interface between individual perceptions and the socially accepted worldview. (Failure to correspond with party doctrine was "incorrect recognition".)[54] Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951 the Academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Most leading psychologists were educated in the United States, and the first concern of the Academy was re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for nationally cohesive education remained a central goal of the discipline.[55]		In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology.[23] The IAAP is considered the oldest international psychology association.[56] Today, at least 65 international groups deal with specialized aspects of psychology.[56] In response to male predominance in the field, female psychologists in the U.S. formed National Council of Women Psychologists in 1941. This organization became the International Council of Women Psychologists after World War II, and the International Council of Psychologists in 1959. Several associations including the Association of Black Psychologists and the Asian American Psychological Association have arisen to promote non-European racial groups in the profession.[56]		The world federation of national psychological societies is the International Union of Psychological Science (IUPsyS), founded in 1951 under the auspices of UNESCO, the United Nations cultural and scientific authority.[23][57] Psychology departments have since proliferated around the world, based primarily on the Euro-American model.[18][57] Since 1966, the Union has published the International Journal of Psychology.[23] IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis.[56]		The International Union recognizes 66 national psychology associations and at least 15 others exist.[56] The American Psychological Association is the oldest and largest.[56] Its membership has increased from 5,000 in 1945 to 100,000 in the present day.[26] The APA includes 54 divisions, which since 1960 have steadily proliferated to include more specialties. Some of these divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups.[56]		The Interamerican Society of Psychology, founded in 1951, aspires to promote psychology and coordinate psychologists across the Western Hemisphere. It holds the Interamerican Congress of Psychology and had 1000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international groups organize psychologists in different regions.[56]		In some places, governments legally regulate who can provide psychological services or represent themselves as a "psychologist".[58] The American Psychological Association defines a psychologist as someone with a doctoral degree in psychology.[59]		Early practitioners of experimental psychology distinguished themselves from parapsychology, which in the late nineteenth century enjoyed great popularity (including the interest of scholars such as William James), and indeed constituted the bulk of what people called "psychology". Parapsychology, hypnotism, and psychism were major topics of the early International Congresses. But students of these fields were eventually ostractized, and more or less banished from the Congress in 1900–1905.[23] Parapsychology persisted for a time at Imperial University, with publications such as Clairvoyance and Thoughtography by Tomokichi Fukurai, but here too it was mostly shunned by 1913.[24]		As a discipline, psychology has long sought to fend off accusations that it is a "soft" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking the agreement on overarching theory found in mature sciences such as chemistry and physics.[60] Because some areas of psychology rely on research methods such as surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion, cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities.[61][62][63]		Divisions still exist within the field, with some psychologists more oriented towards the unique experiences of individual humans, which cannot be understood only as data points within a larger population. Critics inside and outside the field have argued that mainstream psychology has become increasingly dominated by a "cult of empiricism" which limits the scope of its study by using only methods derived from the physical sciences.[64] Feminist critiques along these lines have argued that claims to scientific objectivity obscure the values and agenda of (historically mostly male)[34] researchers. Jean Grimshaw, for example, argues that mainstream psychological research has advanced a patriarchal agenda through its efforts to control behavior.[65]		Psychologists generally consider the organism the basis of the mind, and therefore a vitally related area of study. Psychiatrists and neuropsychologists work at the interface of mind and body.[66] Biological psychology, also known as physiological psychology,[67] or neuropsychology is the study of the biological substrates of behavior and mental processes. Key research topics in this field include comparative psychology, which studies humans in relation to other animals, and perception which involves the physical mechanics of sensation as well as neural and mental processing.[68] For centuries, a leading question in biological psychology has been whether and how mental functions might be localized in the brain. From Phineas Gage to H. M. and Clive Wearing, individual people with mental issues traceable to physical damage have inspired new discoveries in this area.[67] Modern neuropsychology could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal gyrus, thereby also demonstrating hemispheric lateralization of brain function. Soon after, Carl Wernicke identified a related area necessary for the understanding of speech.[69]		The contemporary field of behavioral neuroscience focuses on physical causes underpinning behavior. For example, physiological psychologists use animal models, typically rats, to study the neural, genetic, and cellular mechanisms that underlie specific behaviors such as learning and memory and fear responses.[70] Cognitive neuroscientists investigate the neural correlates of psychological processes in humans using neural imaging tools, and neuropsychologists conduct psychological assessments to determine, for instance, specific aspects and extent of cognitive deficit caused by brain damage or disease. The biopsychosocial model is an integrated perspective toward understanding consciousness, behavior, and social interaction. It assumes that any given behavior or mental process affects and is affected by dynamically interrelated biological, psychological, and social factors.[71]		Evolutionary psychology examines cognition and personality traits from an evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychology offers complementary explanations for the mostly proximate or developmental explanations developed by other areas of psychology: that is, it focuses mostly on ultimate or "why?" questions, rather than proximate or "how?" questions. "How?" questions are more directly tackled by behavioral genetics research, which aims to understand how genes and environment impact behavior.[72]		The search for biological origins of psychological phenomena has long involved debates about the importance of race, and especially the relationship between race and intelligence. The idea of white supremacy and indeed the modern concept of race itself arose during the process of world conquest by Europeans.[73] Carl von Linnaeus's four-fold classification of humans classifies Europeans as intelligent and severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as drapetomania and dysaesthesia aethiopica—the behavior of uncooperative African slaves.[74] After the creation of experimental psychology, "ethnical psychology" emerged as a subdiscipline, based on the assumption that studying primitive races would provide an important link between animal behavior and the psychology of more evolved humans.[75]		Psychologists take human behavior as a main area of study. Much of the research in this area began with tests on mammals, based on the idea that humans exhibit similar fundamental tendencies. Behavioral research ever aspires to improve the effectiveness of techniques for behavior modification.		Early behavioral researchers studied stimulus–response pairings, now known as classical conditioning. They demonstrated that behaviors could be linked through repeated association with stimuli eliciting pain or pleasure. Ivan Pavlov—known best for inducing dogs to salivate in the presence of a stimulus previous linked with food—became a leading figure in the Soviet Union and inspired followers to use his methods on humans.[31] In the United States, Edward Lee Thorndike initiated "connectionism" studies by trapping animals in "puzzle boxes" and rewarding them for escaping. Thorndike wrote in 1911: "There can be no moral warrant for studying man's nature unless the study will enable us to control his acts."[76] From 1910–1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards "behavioralism", and in 1913 John B. Watson coined the term behaviorism for this school of thought.[77] Watson's famous Little Albert experiment in 1920 demonstrated that repeated use of upsetting loud noises could instill phobias (aversions to other stimuli) in an infant human.[12][78] Karl Lashley, a close collaborator with Watson, examined biological manifestations of learning in the brain.[67]		Embraced and extended by Clark L. Hull, Edwin Guthrie, and others, behaviorism became a widely used research paradigm.[26] A new method of "instrumental" or "operant" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically.[79] Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B. F. Skinner, who emerged as a leading intellectual of the behaviorist movement.[80][81]		Noam Chomsky delivered an influential critique of radical behaviorism on the grounds that it could not adequately explain the complex mental process of language acquisition.[82][83][84] Martin Seligman and colleagues discovered that the conditioning of dogs led to outcomes ("learned helplessness") that opposed the predictions of behaviorism.[85][86] Skinner's behaviorism did not die, perhaps in part because it generated successful practical applications.[82] Edward C. Tolman advanced a hybrid "cognitive behaviorial" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a modified maze.[87]		The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has been especially influential in Latin America, where it has a regional organization known as ALAMOC: La Asociación Latinoamericana de Análisis y Modificación del Comportamiento. Behaviorism also gained a strong foothold in Japan, where it gave rise to the Japanese Society of Animal Psychology (1933), the Japanese Association of Special Education (1963), the Japanese Society of Biofeedback Research (1973), the Japanese Association for Behavior Therapy (1976), the Japanese Association for Behavior Analysis (1979), and the Japanese Association for Behavioral Science Research (1994).[88] Today the field of behaviorism is also commonly referred to as behavior modification or behavior analysis.[88]		Green Red Blue Purple Blue Purple		Blue Purple Red Green Purple Green		The Stroop effect refers to the fact that naming the color of the first set of words is easier and quicker than the second.		Cognitive psychology studies cognition, the mental processes underlying mental activity. Perception, attention, reasoning, thinking, problem solving, memory, learning, language, and emotion are areas of research. Classical cognitive psychology is associated with a school of thought known as cognitivism, whose adherents argue for an information processing model of mental function, informed by functionalism and experimental psychology.		On a broader level, cognitive science is an interdisciplinary enterprise of cognitive psychologists, cognitive neuroscientists, researchers in artificial intelligence, linguists, human–computer interaction, computational neuroscience, logicians and social scientists. Computer simulations are sometimes used to model phenomena of interest.		Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist—concerned with information and its processing—and, eventually, constituted a part of the wider cognitive science.[89] Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis.[89]		Social learning theorists, such as Albert Bandura, argued that the child's environment could make contributions of its own to the behaviors of an observant subject.[90]		Technological advances also renewed interest in mental states and representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena with the structure and function of the brain. The rise of computer science, cybernetics and artificial intelligence suggested the value of comparatively studying information processing in humans and machines. Research in cognition had proven practical since World War II, when it aided in the understanding of weapons operation.[91]		A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalogue of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind.		Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck. Cognitive psychology was subsumed along with other disciplines, such as philosophy of mind, computer science, and neuroscience, under the cover discipline of cognitive science.		Social psychology is the study of how humans think about each other and how they relate to each other. Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion), and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology in order to understand how people process, remember, or distort social information. The study of group dynamics reveals information about the nature and potential optimization of leadership, communication, and other phenomena that emerge at least at the microsocial level. In recent years, many social psychologists have become increasingly interested in implicit measures, mediational models, and the interaction of both person and social variables in accounting for behavior. The study of human society is therefore a potentially valuable source of information about the causes of psychiatric disorder. Some sociological concepts applied to psychiatric disorders are the social role, sick role, social class, life event, culture, migration, social, and total institution.		Psychoanalysis comprises a method of investigating the mind and interpreting experience; a systematized set of theories about human behavior; and a form of psychotherapy to treat psychological or emotional distress, especially conflict originating in the unconscious mind.[92] This school of thought originated in the 1890s with Austrian medical doctors including Josef Breuer (physician), Alfred Adler (physician), Otto Rank (psychoanalyst), and most prominently Sigmund Freud (neurologist). Freud's psychoanalytic theory was largely based on interpretive methods, introspection and clinical observations. It became very well known, largely because it tackled subjects such as sexuality, repression, and the unconscious. These subjects were largely taboo at the time, and Freud provided a catalyst for their open discussion in polite society.[49] Clinically, Freud helped to pioneer the method of free association and a therapeutic interest in dream interpretation.[93][94]		Swiss psychiatrist Carl Jung, influenced by Freud, elaborated a theory of the collective unconscious—a primordial force present in all humans, featuring archetypes which exerted a profound influence on the mind. Jung's competing vision formed the basis for analytical psychology, which later led to the archetypal and process-oriented schools. Other well-known psychoanalytic scholars of the mid-20th century include Erik Erikson, Melanie Klein, D. W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, and Sigmund Freud's daughter, Anna Freud. Throughout the 20th century, psychoanalysis evolved into diverse schools of thought which could be called Neo-Freudian. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis.		Psychologists such as Hans Eysenck and philosophers including Karl Popper criticized psychoanalysis. Popper argued that psychoanalysis had been misrepresented as a scientific discipline,[95] whereas Eysenck said that psychoanalytic tenets had been contradicted by experimental data. By the end of 20th century, psychology departments in American universities mostly marginalized Freudian theory, dismissing it as a "desiccated and dead" historical artifact.[96] However, researchers in the emerging field of neuro-psychoanalysis today defend some of Freud's ideas on scientific grounds,[97] while scholars of the humanities maintain that Freud was not a "scientist at all, but ... an interpreter".[96]		Humanistic psychology developed in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis.[99] The humanistic approach sought to glimpse the whole person, not just fragmented parts of the personality or isolated cognitions.[100] Humanism focused on uniquely human issues, such as free will, personal growth, self-actualization, self-identity, death, aloneness, freedom, and meaning. It emphasized subjective meaning, rejection of determinism, and concern for positive growth rather than pathology.[citation needed] Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy. Later, positive psychology opened up humanistic themes to scientific modes of exploration.		The American Association for Humanistic Psychology, formed in 1963, declared:		Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a "third force" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts.[101]		In the 1950s and 1960s, influenced by philosophers Søren Kierkegaard and Martin Heidegger and, psychoanalytically trained American psychologist Rollo May pioneered an existential branch of psychology, which included existential psychotherapy: a method based on the belief that inner conflict within a person is due to that individual's confrontation with the givens of existence. Swiss psychoanalyst Ludwig Binswanger and American psychologist George Kelly may also be said to belong to the existential school.[102] Existential psychologists differed from more "humanistic" psychologists in their relatively neutral view of human nature and their relatively positive assessment of anxiety.[103] Existential psychologists emphasized the humanistic themes of death, free will, and meaning, suggesting that meaning can be shaped by myths, or narrative patterns,[104] and that it can be encouraged by an acceptance of the free will requisite to an authentic, albeit often anxious, regard for death and other future prospects.		Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections garnered from his own internment.[105] He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a will to meaning (in one's life), as opposed to Adler's Nietzschean doctrine of will to power or Freud's will to pleasure.[106]		Personality psychology is concerned with enduring patterns of behavior, thought, and emotion—commonly referred to as personality—in individuals. Theories of personality vary across different psychological schools and orientations. They carry different assumptions about such issues as the role of the unconscious and the importance of childhood experience. According to Freud, personality is based on the dynamic interactions of the id, ego, and super-ego.[107] In order to develop a taxonomy of personality constructs,trait theorists, in contrast, attempt to describe the personality sphere in terms of a discrete number of key traits using the statistical data-reduction method of factor analysis. Although the number of proposed traits has varied widely, an early biologically-based model proposed by Hans Eysenck, the 3rd mostly highly cited psychologist of the 20th Century (after Freud, and Piaget respectively), suggested that at least three major trait constructs are necessary to describe human personality structure: extraversion–introversion, neuroticism-stability, and psychoticism-normality. Raymond Cattell, the 7th most highly cited psychologist of the 20th Century (based on the scientific peer-reviewed journal literature)[108] empirically derived a theory of 16 personality factors at the primary-factor level, and up to 8 broader second-stratum factors (at the Eysenckian level of analysis), rather than the "Big Five" dimensions.[109][110][111][112] Dimensional models of personality are receiving increasing support, and a version of dimensional assessment has been included in the DSM-V. However, despite a plethora of research into the various versions of the "Big Five" personality dimensions, it appears necessary to move on from static conceptualizations of personality structure to a more dynamic orientation, whereby it is acknowledged that personality constructs are subject to learning and change across the lifespan.[113][114]		An early example of personality assessment was the Woodworth Personal Data Sheet, constructed during World War I. The popular, although psychometrically inadequate Myers–Briggs Type Indicator[115] sought to assess individuals' "personality types" according to the personality theories of Carl Jung. Behaviorist resistance to introspection led to the development of the Strong Vocational Interest Blank and Minnesota Multiphasic Personality Inventory, in an attempt to ask empirical questions that focused less on the psychodynamics of the respondent.[116] However, the MMPI has been subjected to critical scrutiny, given that it adhered to archaic psychiatric nosology, and since it required individuals to provide subjective, introspective responses to the hundreds of items pertaining to psychopathology.[117]		Study of the unconscious mind, a part of the psyche outside the awareness of the individual which nevertheless influenced thoughts and behavior was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C. S. Peirce and Joseph Jastrow found in 1884 that subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference.[118] Freud popularized this concept, with terms like Freudian slip entering popular culture, to mean an uncensored intrusion of unconscious thought into one's speech and action. His 1901 text The Psychopathology of Everyday Life catalogues hundreds of everyday events which Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the scrutiny of the subject.[119]		Behaviorism notwithstanding, the unconscious mind has maintained its importance in psychology. Cognitive psychologists have used a "filter" model of attention, according to which much information processing takes place below the threshold of consciousness, and only certain processes, limited by nature and by simultaneous quantity, make their way through the filter. Copious research has shown that subconscious priming of certain ideas can covertly influence thoughts and behavior.[119] A significant hurdle in this research is proving that a subject's conscious mind has not grasped a certain stimulus, due to the unreliability of self-reporting. For this reason, some psychologists prefer to distinguish between implicit and explicit memory. In another approach, one can also describe a subliminal stimulus as meeting an objective but not a subjective threshold.[120]		The automaticity model, which became widespread following exposition by John Bargh and others in the 1980s, describes sophisticated processes for executing goals which can be selected and performed over an extended duration without conscious awareness.[121][122] Some experimental data suggests that the brain begins to consider taking actions before the mind becomes aware of them.[120][123] This influence of unconscious forces on people's choices naturally bears on philosophical questions free will. John Bargh, Daniel Wegner, and Ellen Langer are some prominent contemporary psychologists who describe free will as an illusion.[121][122][124]		Psychologists such as William James initially used the term motivation to refer to intention, in a sense similar to the concept of will in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation.[125] According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as physical demands made by the organism on the nervous system. However, they believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in Beyond the Pleasure Principle, Freud introduced the concept of the death drive, a compulsion towards aggression, destruction, and psychic repetition of traumatic events.[126] Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking.[125][127] Clark Hull formalized the latter idea with his drive reduction model.[128]		Hunger, thirst, fear, sexual desire, and thermoregulation all seem to constitute fundamental motivations for animals.[127] Humans also seem to exhibit a more complex set of motivations—though theoretically these could be explained as resulting from primordial instincts—including desires for belonging, self-image, self-consistency, truth, love, and control.[129][130]		Motivation can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food palatability, and cost.[127] Abstract motivations are also malleable, as evidenced by such phenomena as goal contagion: the adoption of goals, sometimes unconsciously, based on inferences about the goals of others.[131] Vohs and Baumeister suggest that contrary to the need-desire-fulfilment cycle of animal instincts, human motivations sometimes obey a "getting begets wanting" rule: the more you get a reward such as self-esteem, love, drugs, or money, the more you want it. They suggest that this principle can even apply to food, drink, sex, and sleep.[132]		Mainly focusing on the development of the human mind through the life span, developmental psychology seeks to understand how people come to perceive, understand, and act within the world and how these processes change as they age. This may focus on cognitive, affective, moral, social, or neural development. Researchers who study children use a number of unique research methods to make observations in natural settings or to engage them in experimental tasks. Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful, and researchers have even devised clever methods to study the mental processes of infants. In addition to studying children, developmental psychologists also study aging and processes throughout the life span, especially at other times of rapid change (such as adolescence and old age). Developmental psychologists draw on the full range of psychological theories to inform their research.		All researched psychological traits are influenced by both genes and environment, to varying degrees.[133][134] These two sources of influence are often confounded in observational research of individuals or families. An example is the transmission of depression from a depressed mother to her offspring. Theory may hold that the offspring, by virtue of having a depressed mother in his or her (the offspring's) environment, is at risk for developing depression. However, risk for depression is also influenced to some extent by genes. The mother may both carry genes that contribute to her depression but will also have passed those genes on to her offspring thus increasing the offspring's risk for depression. Genes and environment in this simple transmission model are completely confounded. Experimental and quasi-experimental behavioral genetic research uses genetic methodologies to disentangle this confound and understand the nature and origins of individual differences in behavior.[72] Traditionally this research has been conducted using twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, the availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic,[135][136][137][138][139] where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to understand the genetic and environmental bases of behavior and their interaction.		Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior:		Psychological testing has ancient origins, such as examinations for the Chinese civil service dating back to 2200 BC. Written exams began during the Han dynasty (202 BC.–AD. 200). By 1370, the Chinese system required a stratified series of tests, involving essay writing and knowledge of diverse topics. The system was ended in 1906.[140] In Europe, mental assessment took a more physiological approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BC Greece. Physiognomy remained current through the Enlightenment, and added the doctrine of phrenology: a study of mind and intelligence based on simple assessment of neuroanatomy.[141]		When experimental psychology came to Britain, Francis Galton was a leading practitioner, and, with his procedures for measuring reaction time and sensation, is considered an inventor of modern mental testing (also known as psychometrics).[142] James McKeen Cattell, a student of Wundt and Galton, brought the concept to the United States, and in fact coined the term "mental test".[143] In 1901, Cattell's student Clark Wissler published discouraging results, suggesting that mental testing of Columbia and Barnard students failed to predict their academic performance.[143] In response to 1904 orders from the Minister of Public Instruction, French psychologists Alfred Binet and Théodore Simon elaborated a new test of intelligence in 1905–1911, using a range of questions diverse in their nature and difficulty. Binet and Simon introduced the concept of mental age and referred to the lowest scorers on their test as idiots. Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as imbecile and feebleminded. In 1916 (after Binet's death), Stanford professor Lewis M. Terman modified the Binet-Simon scale (renamed the Stanford–Binet scale) and introduced the intelligence quotient as a score report.[144] From this test, Terman concluded that mental retardation "represents the level of intelligence which is very, very common among Spanish-Indians and Mexican families of the Southwest and also among negroes. Their dullness seems to be racial."[145]		Following the Army Alpha and Army Beta tests for soldiers in World War I, mental testing became popular in the US, where it was soon applied to school children. The federally created National Intelligence Test was administered to 7 million children in the 1920s, and in 1926 the College Entrance Examination Board created the Scholastic Aptitude Test to standardize college admissions.[146] The results of intelligence tests were used to argue for segregated schools and economic functions—i.e. the preferential training of Black Americans for manual labor. These practices were criticized by black intellectuals such a Horace Mann Bond and Allison Davis.[145] Eugenicists used mental testing to justify and organize compulsory sterilization of individuals classified as mentally retarded.[39] In the United States, tens of thousands of men and women were sterilized. Setting a precedent which has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1907 case Buck v. Bell.[147]		Today mental testing is a routine phenomenon for people of all ages in Western societies.[148] Modern testing aspires to criteria including standardization of procedure, consistency of results, output of an interpretable score, statistical norms describing population outcomes, and, ideally, effective prediction of behavior and life outcomes outside of testing situations.[149]		The provision of psychological health services is generally called clinical psychology in the U.S. The definitions of this term are various and may include school psychology and counseling psychology. Practitioners typically includes people who have graduated from doctoral programs in clinical psychology but may also include others. In Canada, the above groups usually fall within the larger category of professional psychology. In Canada and the US, practitioners get bachelor's degrees and doctorates, then spend one year in an internship and one year in postdoctoral education. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctorate degrees; instead, they take a three-year professional course following high school.[59] Clinical psychology is at present the largest specialization within psychology.[150] It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychologically based distress, dysfunction or mental illness and to promote subjective well-being and personal development. Central to its practice are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration.[151]		Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince.[150] For the most part, in the first part of the twentieth century, most mental health care in the United States was performed by specialized medical doctors called psychiatrists. Psychology entered the field with its refinements of mental testing, which promised to improve diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill.[34] In this type of treatment, a specially trained therapist develops a close relationship with the patient, who discusses wishes, dreams, social relationships, and other aspects of mental life. The therapist seeks to uncover repressed material and to understand why the patient creates defenses against certain thoughts and feelings. An important aspect of the therapeutic relationship is transference, in which deep unconscious feelings in a patient reorient themselves and become manifest in relation to the therapist.[152]		Psychiatric psychotherapy blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities and behavioral therapy, a thoroughly non-psychodynamic model which used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy arose, using similar methods and now including the cognitive constructs which had gained popularity in theoretical psychology. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned.[153]		Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of "intense competition and role confusion".[34] Graduate programs issuing doctorates in psychology (PsyD) emerged in the 1950s and underwent rapid increase through the 1980s. This degree is intended to train practitioners who might conduct scientific research.[59]		Some clinical psychologists may focus on the clinical management of patients with brain injury—this area is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of disaster psychology (see crisis intervention) involves professionals who respond to large-scale traumatic events.[154]		The work performed by clinical psychologists tends to be influenced by various therapeutic approaches, all of which involve a formal relationship between professional and client (usually an individual, couple, family, or small group). Typically, these approaches encourage new ways of thinking, feeling, or behaving. Four major theoretical perspectives are psychodynamic, cognitive behavioral, existential–humanistic, and systems or family therapy. There has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of issues regarding culture, gender, spirituality, and sexual orientation. With the advent of more robust research findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance.[155][156] Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation.[157][158][159][160][161]		Diagnosis in clinical psychology usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM), a handbook first published by the American Psychiatric Association in 1952. New editions over time have increased in size and focused more on medical language.[162] The study of mental illnesses is called abnormal psychology.		Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. The work of child psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand.		School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research.[163]		Industrialists soon brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. This field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology.[164] An important early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924–1932. With funding from the Laura Spelman Rockefeller Fund and guidance from Australian psychologist Elton Mayo, Western Electric experimented on thousands of factory workers to assess their responses to illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people work harder when they think they're being watched.[165]		The name industrial and organizational psychology (I–O) arose in the 1960s and became enshrined as the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association, in 1973.[164] The goal is to optimize human potential in the workplace. Personnel psychology, a subfield of I–O psychology, applies the methods and principles of psychology in selecting and evaluating workers. I–O psychology's other subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity.[166] The majority of I–O psychologists work outside of academia, for private and public organizations and as consultants.[164] A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company.[167]		One role for psychologists in the military is to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia, to provide psychological training for military staff military.[34][168] Today, U.S Army psychology includes psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as other aspects of health and workplace psychology such as smoking cessation.[169]		Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychologically warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. In the case of so-called black propaganda the propaganda is designed to seem like it originates from a different source.[170] The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD.[171] The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO).[172] Psychologists are sometimes involved in assisting the interrogation and torture of suspects, though this has sometimes been denied by those involved and sometimes opposed by others.[173]		Medical facilities increasingly employ psychologists to perform various roles. A prominent aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance.[174]		Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people.[175]		Black American psychologists Kenneth and Mamie Clark studied the psychological impact of segregation and testified with their findings in the desegregation case Brown v. Board of Education (1954).[176]		Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently health. In 2010 Clinical Psychological Review published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. Positive psychological interventions have been limited in scope, but their effects are thought to be superior to that of placebos, especially with regard to helping people with body image problems.		Quantitative psychological research lends itself to the statistical testing of hypotheses. Although the field makes abundant use of randomized and controlled experiments in laboratory settings, such research can only assess a limited range of short-term phenomena. Thus, psychologists also rely on creative statistical methods to glean knowledge from clinical trials and population data.[177] These include the Pearson product–moment correlation coefficient, the analysis of variance, multiple linear regression, logistic regression, structural equation modeling, and hierarchical linear modeling. The measurement and operationalization of important constructs is an essential part of these research designs.		A true experiment with random allocation of subjects to conditions allows researchers to make strong inferences about causal relationships. In an experiment, the researcher alters parameters of influence, called independent variables, and measures resulting changes of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment.		Repeated-measures experiments are those which take place through intervention on multiple occasions. In research on the effectiveness of psychotherapy, experimenters often compare a given treatment with placebo treatments, or compare different treatments against each other. Treatment type is the independent variable. The dependent variables are outcomes, ideally assessed in several ways by different professionals.[180] Using crossover design, researchers can further increase the strength of their results by testing both of two treatments on two groups of subjects.		Quasi-experimental design refers especially to situations precluding random assignment to different conditions. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity.[181] For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes.		Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data supports the predictions. (These predictions may originate from a more abstract scientific hypothesis about how the phenomenon under study actually works.) Analysis of variance (ANOVA) statistical techniques are used to distinguish unique results of the experiment from the null hypothesis that variations result from random fluctuations in data. In psychology, the widely usd standard ascribes statistical significance to results which have less than 5% probability of being explained by random variation.[182]		Statistical surveys are used in psychology for measuring attitudes and traits, monitoring changes in mood, checking the validity of experimental manipulations, and for other psychological topics. Most commonly, psychologists use paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects.		Neuropsychological tests, such as the Wechsler scales and Wisconsin Card Sorting Test, are mostly questionnaires or simple tasks used which assess a specific type of mental function in the respondent. These can be used in experiments, as in the case of lesion experiments evaluating the results of damage to a specific part of the brain.[183]		Observational studies analyze uncontrolled data in search of correlations; multivariate statistics are typically used to interpret the more complex situation. Cross-sectional observational studies use data from a single point in time, whereas longitudinal studies are used to study trends across the life span. Longitudinal studies track the same people, and therefore detect more individual, rather than cultural, differences. However, they suffer from lack of controls and from confounding factors such as selective attrition (the bias introduced when a certain type of subject disproportionately leaves a study).		Exploratory data analysis refers to a variety of practices which researchers can use to visualize and analyze existing sets of data. In Peirce's three modes of inference, exploratory data anlysis corresponds to abduction, or hypothesis formation.[184] Meta-analysis is the technique of integrating the results from multiple studies and interpreting the statistical properties of the pooled dataset.[185]		A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature "brain waves": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep.[186]		Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic neural network model rather than a localized function model.[187][188][189]		Psychiatric interventions such as transcranial magnetic stimulation and of course drugs also provide information about brain–mind interactions. Psychopharmacology is the study of drug-induced mental effects.		Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior.[190] This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that couldn't be directly observed in a human. Connectionism uses neural networks to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.		Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, rats, and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. Of course, there are pitfalls in generalizing findings from animal studies to humans through animal models.[191]		Comparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology.[192] Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E. O. Wilson.[193] Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience.		Research designed to answer questions about the current state of affairs such as the thoughts, feelings, and behaviors of individuals is known as descriptive research. Descriptive research can be qualitative or quantitative in orientation. Qualitative research is descriptive research that is focused on observing and describing events as they occur, with the goal of capturing all of the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations have been made.		Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identifies five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers[194] sometimes aim to enrich interpretations or critiques of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's study of Nazi voting[citation needed] or Stanley Milgram's studies of obedience to authority.		Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out.		In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying a possible publication bias.[196][197][198] Similarly, Fanelli (2010)[199] found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space- or geosciences. Fanelli argues that this is because researchers in "softer" sciences have fewer constraints to their conscious and unconscious biases.		Some popular media outlets have in recent years spotlighted a replication crisis in psychology, arguing that many findings in the field cannot be reproduced. Repeats of some famous studies have not reached the same conclusions, and some researchers have been accused of outright fraud in their results. Focus on this issue has led to renewed efforts in the discipline to re-test important findings.[200][201][202][203]		Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts.[204] Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on the Fisherian p < .05 significance criterion (whereby an observed difference is deemed "statistically significant" if an effect of that size or larger would occur with 5% -or less- probability in independent replications, assuming the truth of the null-hypothesis of no difference between the treatments).[citation needed]		In 2010, a group of researchers reported a systemic bias in psychology studies towards WEIRD ("western, educated, industrialized, rich and democratic") subjects.[205] Although only 1/8 people worldwide fall into the WEIRD classification, the researchers claimed that 60–90% of psychology studies are performed on WEIRD subjects. The article gave examples of results that differ significantly between WEIRD subjects and tribal cultures, including the Müller-Lyer illusion.		Some observers perceive a gap between scientific theory and its application—in particular, the application of unsupported or unsound clinical practices.[206] Critics say there has been an increase in the number of mental health training programs that do not instill scientific competence.[207] One skeptic asserts that practices, such as "facilitated communication for infantile autism"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity.[208] In 1984, Allen Neuringer made a similar point[vague] regarding the experimental analysis of behavior.[209] Psychologists, sometimes divided along the lines of laboratory vs. clinic, continue to debate these issues.[210]		Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes (Ethics Code of the American Psychological Association, the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report).		The most important contemporary standards are informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential studies led to the establishment of this rule; such studies included the MIT and Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, and Stanley Milgram's studies of obedience to authority.		University psychology departments have ethics committees dedicated to the rights and well-being of research subjects. Researchers in psychology must gain approval of their research projects before conducting any experiment to protect the interests of human participants and laboratory animals.[211]		The ethics code of the American Psychological Association originated in 1951 as "Ethical Standards of Psychologists." This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption. In 1989 the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between "aspirational" ethical standards and "enforceable" ones. Members of the public have a 5-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a 3-year window.[212]		Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing.[212] Some of the most common complaints against clinical psychologists include sexual misconduct, and involvement in child custody evaluations.[212]		Current ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research.[213] Keeping this in mind, psychologists can use certain research techniques on animals that could not be used on humans.				
"You have two cows" refers to a form of political satire involving variations of a scenario, where what occurs to the eponymous cows is used to demonstrate how certain political systems function.						Jokes of this type attracted the attention of a scholar in the USA as early as 1944. An article in The Modern Language Journal lists the following classical ones:[1]		Bill Sherk mentions that such lists circulated throughout the United States since around 1936 under the title "Parable of the Isms".[2] A column in The Chicago Daily Tribune in 1938 attributes a version involving socialism, communism, fascism and New Dealism[nb 1] to an address by Silas Strawn to the Economic Club of Chicago on 29 November 1935.[3]		Jokes of this genre formed the base of a monologue by comedian Pat Paulsen on The Smothers Brothers Comedy Hour in the late 1960s. Satirising the satire, he appended this comment to capitalism: "...Then put both of them in your wife's name and declare bankruptcy." This material was later used as an element of his satirical US presidential campaign in 1968, and was included on his 1968 comedy album Pat Paulsen for President.[4]		Richard M Steers and Luciara Nardon in their book about global economy use the "two cows" metaphor to illustrate the concept of cultural differences. They write that jokes of the kind:[5]		– are considered funny because they are realistic caricatures of various cultures, and the pervasiveness of such jokes stems from the significant cultural differences. Steers and Nardon also state that others believe such jokes present cultural stereotypes and must be viewed with caution.		
Laughter is a physical reaction in humans and some other species of primate, consisting typically of rhythmical, often audible contractions of the diaphragm and other parts of the respiratory system. It is a response to certain external or internal stimuli. Laughter can arise from such activities as being tickled,[1] or from humorous stories or thoughts.[2] Most commonly, it is considered a visual expression of a number of positive emotional states, such as joy, mirth, happiness, relief, etc. On some occasions, however, it may be caused by contrary emotional states such as embarrassment, apology, or confusion such as nervous laughter or courtesy laugh. Age, gender, education, language, and culture are all factors[3] as to whether a person will experience laughter in a given situation.		Laughter is a part of human behavior regulated by the brain, helping humans clarify their intentions in social interaction and providing an emotional context to conversations. Laughter is used as a signal for being part of a group—it signals acceptance and positive interactions with others. Laughter is sometimes seen as contagious, and the laughter of one person can itself provoke laughter from others as a positive feedback.[4] This may account in part for the popularity of laugh tracks in situation comedy television shows.		The study of humor and laughter, and its psychological and physiological effects on the human body, is called gelotology.						Laughter might be thought of as an audible expression or appearance of excitement, an inward feeling of joy and happiness. It may ensue from jokes, tickling, and other stimuli completely unrelated to psychological state, such as nitrous oxide. One group of researchers speculated that noises from infants as early as 16 days old may be vocal laughing sounds or laughter,[5] however the weight of the evidence supports its appearance at 15 weeks to four months of age.		Laughter researcher Robert Provine (es) said: "Laughter is a mechanism everyone has; laughter is part of universal human vocabulary. There are thousands of languages, hundreds of thousands of dialects, but everyone speaks laughter in pretty much the same way." Babies have the ability to laugh before they ever speak. Children who are born blind and deaf still retain the ability to laugh.[6]		Provine argues that "Laughter is primitive, an unconscious vocalization." Provine argues that it probably is genetic. In a study of the "Giggle Twins", two happy twins who were separated at birth and only reunited 43 years later, Provine reports that "until they met each other, neither of these exceptionally happy ladies had known anyone who laughed as much as they did." They reported this even though they both had been brought together by their adoptive parents, who they indicated were "undemonstrative and dour." He indicates that the twins "inherited some aspects of their laugh sound and pattern, readiness to laugh, and maybe even taste in humor."[7]		Norman Cousins developed a recovery program incorporating megadoses of Vitamin C, along with a positive attitude, love, faith, hope, and laughter induced by Marx Brothers films. "I made the joyous discovery that ten minutes of genuine belly laughter had an anesthetic effect and would give me at least two hours of pain-free sleep," he reported. "When the pain-killing effect of the laughter wore off, we would switch on the motion picture projector again and not infrequently, it would lead to another pain-free interval."[8][9]		Scientists have noted the similarity in forms of laughter induced by tickling among various primates, which suggests that laughter derives from a common origin among primate species.[10][11]		A very rare neurological condition has been observed whereby the sufferer is unable to laugh out loud, a condition known as aphonogelia.[12]		Neurophysiology indicates that laughter is linked with the activation of the ventromedial prefrontal cortex, that produces endorphins.[13] Scientists have shown that parts of the limbic system are involved in laughter. This system is involved in emotions and helps us with functions necessary for humans' survival. The structures in the limbic system that are involved in laughter are the hippocampus and the amygdala.[14]		The December 7, 1984, Journal of the American Medical Association describes the neurological causes of laughter as follows:		Some drugs are well known for their laughter-facilitating properties (e. g. ethanol and cannabis), while the others, like salvinorin A (the active ingredient of Salvia divinorum), can even induce bursts of uncontrollable laughter.[15]		A link between laughter and healthy function of blood vessels was first reported in 2005 by researchers at the University of Maryland Medical Center with the fact that laughter causes the dilatation of the inner lining of blood vessels, the endothelium, and increases blood flow.[16] Drs. Michael Miller (University of Maryland) and William Fry (Stanford), theorize that beta-endorphin like compounds released by the hypothalamus activate receptors on the endothelial surface to release nitric oxide, thereby resulting in dilation of vessels. Other cardioprotective properties of nitric oxide include reduction of inflammation and decreased platelet aggregation.[17][18]		Laughter has proven beneficial effects on various other aspects of biochemistry. It has been shown to lead to reductions in stress hormones such as cortisol and epinephrine. When laughing the brain also releases endorphins that can relieve some physical pain.[19] Laughter also boosts the number of antibody-producing cells and enhances the effectiveness of T-cells, leading to a stronger immune system.[20] A 2000 study found that people with heart disease were 40 percent less likely to laugh and be able to recognize humor in a variety of situations, compared to people of the same age without heart disease.[21]		A number of studies using methods of conversation analysis and discourse analysis have documented the systematic workings of laughter in a variety of interactions, from casual conversations to interviews,meetings, and therapy sessions.[22] Working with recorded interactions, researchers have created detailed transcripts that indicate not only the presence of laughter but also features of its production and placement.		These studies challenge several widely held assumptions about the nature of laughter. Contrary to notions that it is spontaneous and involuntary, research documents that laughter is sequentially-organized and precisely placed relative to surrounding talk. Far more than merely a response to humor, laughter often works to manage delicate and serious moments. More than simply an external behavior “caused” by an inner state, laughter is highly communicative and helps accomplish actions and regulate relationships.		Common causes for laughter are sensations of joy and humor; however, other situations may cause laughter as well.		A general theory that explains laughter is called the relief theory. Sigmund Freud summarized it in his theory that laughter releases tension and "psychic energy". This theory is one of the justifications of the beliefs that laughter is beneficial for one's health.[23] This theory explains why laughter can be used as a coping mechanism when one is upset, angry or sad.		Philosopher John Morreall theorizes that human laughter may have its biological origins as a kind of shared expression of relief at the passing of danger. Friedrich Nietzsche, by contrast, suggested laughter to be a reaction to the sense of existential loneliness and mortality that only humans feel.		For example: a joke creates an inconsistency and the audience automatically try to understand what the inconsistency means; if they are successful in solving this 'cognitive riddle' and they realize that the surprise was not dangerous, they laugh with relief. Otherwise, if the inconsistency is not resolved, there is no laugh, as Mack Sennett pointed out: "when the audience is confused, it doesn't laugh." This is one of the basic laws of a comedian, referred to as "exactness". It is important to note that sometimes the inconsistency may be resolved and there may still be no laugh.[citation needed] Because laughter is a social mechanism, an audience may not feel as if they are in danger, and the laugh may not occur. In addition, the extent of the inconsistency (and aspects of it timing and rhythm) has to do with the amount of danger the audience feels, and how hard or long they laugh.		Laughter can also be brought on by tickling. Although most people find it unpleasant, being tickled often causes heavy laughter, thought to be an (often uncontrollable) reflex of the body.[24][25]		Laughter can be classified according to:		A normal laugh has the structure of "ha-ha-ha" or "ho-ho-ho." It is unnatural, and one is physically unable, to have a laugh structure of "ha-ho-ha-ho." The usual variations of a laugh most often occur in the first or final note in a sequence- therefore, "ho-ha-ha" or "ha-ha-ho" laughs are possible. Normal note durations with unusually long or short "inter-note intervals" do not happen due to the result of the limitations of our vocal cords. This basic structure allows one to recognize a laugh despite individual variants.[28]		It has also been determined that eyes moisten during laughter as a reflex from the tear glands.[20]		Laughter is not always a pleasant experience and is associated with several negative phenomena. Excessive laughter can lead to cataplexy, and unpleasant laughter spells, excessive elation, and fits of laughter can all be considered negative aspects of laughter. Unpleasant laughter spells, or "sham mirth," usually occur in people who have a neurological condition, including patients with pseudobulbar palsy, multiple sclerosis and Parkinson's disease. These patients appear to be laughing out of amusement but report that they are feeling undesirable sensations "at the time of the punch line."		Excessive elation is a common symptom associated with manic-depressive psychoses and mania/hypomania. Those who suffer from schizophrenic psychoses seem to suffer the opposite—they do not understand humor or get any joy out of it. A fit describes an abnormal time when one cannot control the laughter or one’s body, sometimes leading to seizures or a brief period of unconsciousness. Some believe that fits of laughter represent a form of epilepsy.[29]		Laughter has been used as a therapeutic tool for many years because it is a natural form of medicine. Laughter is available to everyone and it provides benefits to a person's physical, emotional, and social well being. Some of the benefits of using laughter therapy are that it can relieve stress and relax the whole body.[30] It can also boost the immune system and release endorphins to relieve pain.[31] Additionally, laughter can help prevent heart disease by increasing blood flow and improving the function of blood vessels. Some of the emotional benefits include diminishing anxiety or fear, improving overall mood, and adding joy to one's life. Laughter is also known to reduce allergic reactions in a preliminary study related to dust mite allergy sufferers.[32]		Laughter therapy also has some social benefits, such as strengthening relationships, improving teamwork and reducing conflicts, and making oneself more attractive to others. Therefore, whether a person is trying to cope with a terminal illness or just trying to manage their stress or anxiety levels, laughter therapy can be a significant enhancement to their life.[33][34]		Laughter in literature, although considered understudied by some,[35] is a subject that has received attention in the written word for millennia. The use of humor and laughter in literary works has been studied and analyzed by many thinkers and writers, from the Ancient Greek philosophers onward. Henri Bergson's Laughter: An Essay on the Meaning of the Comic (Le rire, 1901) is a notable 20th-century contribution.		For Herodotus, laughers can be distinguished into three types:[36]		According to Donald Lateiner, Herodotus reports about laughter for valid literary and historiological reasons. "Herodotus believes either that both nature (better, the gods' direction of it) and human nature coincide sufficiently, or that the latter is but an aspect or analogue of the former, so that to the recipient the outcome is suggested."[36] When reporting laughter, Herodotus does so in the conviction that it tells the reader something about the future and/or the character of the person laughing. It is also in this sense that it is not coincidental that in about 80% of the times when Herodotus speaks about laughter it is followed by a retribution. "Men whose laughter deserves report are marked, because laughter connotes scornful disdain, disdain feeling of superiority, and this feeling and the actions which stem from it attract the wrath of the gods."[36]		Thomas Hobbes understood the superiority of the laughter in a much wider sense than the aesthetic and quasi-moral sense of Aristotle, the seeds of the superiority theory are definitely Greek.[37] In Hobbes' own words: "The passion of laughter is nothing else but sudden glory arising from sudden conception of some eminency in ourselves, by comparison with the infirmity of others, or with our own formerly."		Philosopher Arthur Schopenhauer devotes the 13th chapter of the first part of his major work, The World as Will and Representation, to laughter.		Friedrich Nietzsche distinguishes two different purposes for the use of laughter. In a positive sense, "man uses the comical as a therapy against the restraining jacket of logic morality and reason. He needs from time to time a harmless demotion from reason and hardship and in this sense laughter has a positive character for Nietzsche."[38] Laughter can, however, also have a negative connotation when it is used for the expression of social conflict. This is expressed, for instance, in The Gay Science: "Laughter -- Laughter means to be schadenfroh, but with clear conscience."[39]		"Possibly Nietzsche's works would have had a totally different effect, if the playful, ironical and joking in his writings would have been factored in better"[40]		In Laughter: An Essay on the Meaning of the Comic, French philosopher Henri Bergson, renowned for his philosophical studies on materiality, memory, life and consciousness, tries to determine the laws of the comic and to understand the fundamental causes of comic situations.[41] His method consists in determining the causes of comic instead of analyzing its effects. He also deals with laughter in relation to human life, collective imagination and art, to have a better knowledge of society.[42] One of the theories of the essay is that laughter, as a collective activity, has a social and moral role, in forcing people to eliminate their vices. It is a factor of uniformity of behaviours, as it condemns ludicrous and eccentric behaviours.[43]		In this essay, Bergson also asserts that there is a central cause that all comic situations are derived from: that of mechanism applied to life. The fundamental source of comic is the presence of inflexibility and rigidness in life. For Bergson, the essence of life is movement, elasticity and flexibility, and every comic situation is due the presence of rigidity and inelasticity in life. Hence, for Bergson the source of the comic is not ugliness but rigidity.[44] All the examples taken by Bergson (such as a man falling in the street, one person's imitation of another, the automatic application of conventions and rules, absent-mindedness, repetitive gestures of a speaker, the resemblance between two faces) are comic situations because they give the impression that life is subject to rigidity, automatism and mechanism.		Bergson closes by noting that most comic situations are not laughable because they are part of collective habits.[45] He defines laughter as an intellectual activity that requires an immediate approach to a comic situation, detached from any form of emotion or sensibility.[46] A situation is laughable when the attention and the imagination are focused on the resistance and rigidity of the body. Thus somebody is laughable when he or she gives the impression of being a thing or a machine.		Anthony Ludovici developed the thoughts of Hobbes and Darwin even further in The Secret of Laughter. His conviction is that there's something sinister in laughter, and that the modern omnipresence of humour and the idolatry of it are signs of societal weakness, as instinctive resort to humour became a sort of escapism from responsibility and action. Ludovici considered laughter to be an evolutionary trait and he offered many examples of different triggers for laughter with their own distinct explanations. [47]		
The September 11 attacks (also referred to as 9/11)[nb 1] were a series of four coordinated terrorist attacks by the Islamic terrorist group al-Qaeda on the United States on the morning of Tuesday, September 11, 2001. The attacks killed 2,996 people, injured over 6,000 others, and caused at least $10 billion in infrastructure and property damage.[2][3]		Four passenger airliners operated by two major U.S. passenger air carriers (United Airlines and American Airlines)—all of which departed from airports in the northeastern United States bound for California—were hijacked by 19 al-Qaeda terrorists. Two of the planes, American Airlines Flight 11 and United Airlines Flight 175, were crashed into the North and South towers, respectively, of the World Trade Center complex in New York City. Within an hour and 42 minutes, both 110-story towers collapsed, with debris and the resulting fires causing partial or complete collapse of all other buildings in the World Trade Center complex, including the 47-story 7 World Trade Center tower, as well as significant damage to ten other large surrounding structures. A third plane, American Airlines Flight 77, was crashed into the Pentagon (the headquarters of the United States Department of Defense) in Arlington County, Virginia, leading to a partial collapse of the building's western side. The fourth plane, United Airlines Flight 93, initially was steered toward Washington, D.C., but crashed into a field in Stonycreek Township near Shanksville, Pennsylvania, after its passengers tried to overcome the hijackers. 9/11 was the single deadliest incident for firefighters and law enforcement officers[4] in the history of the United States, with 343 and 72 killed respectively.		Suspicion quickly fell on al-Qaeda. The United States responded by launching the War on Terror and invading Afghanistan to depose the Taliban, which had harbored al-Qaeda. Many countries strengthened their anti-terrorism legislation and expanded the powers of law enforcement and intelligence agencies to prevent terrorist attacks. Although al-Qaeda's leader, Osama bin Laden, initially denied any involvement, in 2004 he claimed responsibility for the attacks.[1] Al-Qaeda and bin Laden cited U.S. support of Israel, the presence of U.S. troops in Saudi Arabia, and sanctions against Iraq as motives. After evading capture for almost a decade, Osama bin Laden was located and killed by SEAL Team Six of the U.S. Navy in May 2011.		The destruction of the World Trade Center and nearby infrastructure caused serious damage to the economy of Lower Manhattan and had a significant effect on global markets, resulting in the closing of Wall Street until September 17 and the civilian airspace in the U.S. and Canada until September 13. Many closings, evacuations, and cancellations followed, out of respect or fear of further attacks. Cleanup of the World Trade Center site was completed in May 2002, and the Pentagon was repaired within a year. On November 18, 2006, construction of One World Trade Center began at the World Trade Center site. The building was officially opened on November 3, 2014.[5][6] Numerous memorials have been constructed, including the National September 11 Memorial & Museum in New York City, the Pentagon Memorial in Arlington County, Virginia, and the Flight 93 National Memorial in a field in Stonycreek Township near Shanksville, Pennsylvania.						The origins of al-Qaeda can be traced to 1979 when the Soviet Union invaded Afghanistan. Osama bin Laden traveled to Afghanistan and helped organize Arab mujahideen to resist the Soviets.[7] Under the guidance of Ayman al-Zawahiri, bin Laden became more radical.[8] In 1996, bin Laden issued his first fatwā, calling for American soldiers to leave Saudi Arabia.[9]		In a second fatwā in 1998, bin Laden outlined his objections to American foreign policy with respect to Israel, as well as the continued presence of American troops in Saudi Arabia after the Gulf War.[10] Bin Laden used Islamic texts to exhort Muslims to attack Americans until the stated grievances are reversed. Muslim legal scholars "have throughout Islamic history unanimously agreed that the jihad is an individual duty if the enemy destroys the Muslim countries", according to bin Laden.[10]		Bin Laden, who orchestrated the attacks, initially denied but later admitted involvement.[1][11][12] Al Jazeera broadcast a statement by bin Laden on September 16, 2001, stating, "I stress that I have not carried out this act, which appears to have been carried out by individuals with their own motivation."[13] In November 2001, U.S. forces recovered a videotape from a destroyed house in Jalalabad, Afghanistan. In the video, bin Laden is seen talking to Khaled al-Harbi and admits foreknowledge of the attacks.[14] On December 27, 2001, a second bin Laden video was released. In the video, he said:		It has become clear that the West in general and America in particular have an unspeakable hatred for Islam. ... It is the hatred of crusaders. Terrorism against America deserves to be praised because it was a response to injustice, aimed at forcing America to stop its support for Israel, which kills our people. ... We say that the end of the United States is imminent, whether Bin Laden or his followers are alive or dead, for the awakening of the Muslim umma (nation) has occurred		but he stopped short of admitting responsibility for the attacks.[15] The transcript refers several times to the United States specifically targeting Muslims.		Shortly before the U.S. presidential election in 2004, in a taped statement, bin Laden publicly acknowledged al-Qaeda's involvement in the attacks on the U.S. and admitted his direct link to the attacks. He said that the attacks were carried out because:		we are free ... and want to regain freedom for our nation. As you undermine our security, we undermine yours.[16]		Bin Laden said he had personally directed his followers to attack the World Trade Center and the Pentagon.[12][17] Another video obtained by Al Jazeera in September 2006 shows bin Laden with Ramzi bin al-Shibh, as well as two hijackers, Hamza al-Ghamdi and Wail al-Shehri, as they make preparations for the attacks.[18] The U.S. never formally indicted bin Laden for the 9/11 attacks but he was on the FBI's Most Wanted List for the bombings of the U.S. Embassies in Dar es Salaam, Tanzania, and Nairobi, Kenya.[19][20] After a 10-year manhunt, bin Laden was killed by American special forces in a compound in Abbottabad, Pakistan on May 2, 2011.[21][22]		The journalist Yosri Fouda of the Arabic television channel Al Jazeera reported that, in April 2002, Khalid Sheikh Mohammed admitted his involvement, along with Ramzi bin al-Shibh.[23][24][25] The 9/11 Commission Report determined that the animosity towards the United States felt by Mohammed, the principal architect of the 9/11 attacks, stemmed from his "violent disagreement with U.S. foreign policy favoring Israel".[26] Mohammed was also an adviser and financier of the 1993 World Trade Center bombing and the uncle of Ramzi Yousef, the lead bomber in that attack.[27][28]		Mohammed was arrested on March 1, 2003, in Rawalpindi, Pakistan, by Pakistani security officials working with the CIA, then transported to Guantanamo Bay and interrogated using methods including waterboarding.[29][30] During U.S. hearings at Guantanamo Bay in March 2007, Mohammed again confessed his responsibility for the attacks, stating he "was responsible for the 9/11 operation from A to Z" and that his statement was not made under duress.[25][31]		In "Substitution for Testimony of Khalid Sheikh Mohammed" from the trial of Zacarias Moussaoui, five people are identified as having been completely aware of the operation's details. They are bin Laden, Khalid Sheikh Mohammed, Ramzi bin al-Shibh, Abu Turab al-Urduni, and Mohammed Atef.[32] To date, only peripheral figures have been tried or convicted for the attacks.		On September 26, 2005, the Spanish high court sentenced Abu Dahdah to 27 years in prison for conspiracy on the 9/11 attacks and being a member of the terrorist organization al-Qaeda. At the same time, another 17 al-Qaeda members were sentenced to penalties of between six and eleven years.[33] On February 16, 2006, the Spanish Supreme Court reduced the Abu Dahdah penalty to 12 years because it considered that his participation in the conspiracy was not proven.[34]		Also, in 2006, Moussaoui, who some originally suspected might have been the assigned 20th hijacker, was convicted for the lesser role of conspiracy to commit acts of terrorism and air piracy. He is serving a life sentence without parole in the United States.[35][36] Mounir el-Motassadeq, an associate of the Hamburg-based hijackers, is serving 15 years in Germany for his role in helping the hijackers prepare for the attacks.[37]		The Hamburg cell in Germany included radical Islamists who eventually came to be key operatives in the 9/11 attacks.[38] Mohamed Atta, Marwan al-Shehhi, Ziad Jarrah, Ramzi bin al-Shibh, and Said Bahaji were all members of al-Qaeda's Hamburg cell.[39]		Osama bin Laden's declaration of a holy war against the United States, and a 1998 fatwā signed by bin Laden and others, calling for the killing of Americans,[10] are seen by investigators as evidence of his motivation.[40] In bin Laden's November 2002 "Letter to America", he explicitly stated that al-Qaeda's motives for their attacks include:		After the attacks, bin Laden and al-Zawahiri released additional video tapes and audio tapes, some of which repeated those reasons for the attacks. Two particularly important publications were bin Laden's 2002 "Letter to America",[46] and a 2004 video tape by bin Laden.[47]		Bin Laden interpreted Muhammad as having banned the "permanent presence of infidels in Arabia".[48] In 1996, bin Laden issued a fatwā calling for American troops to leave Saudi Arabia. In 1998, al-Qaeda wrote, "for over seven years the United States has been occupying the lands of Islam in the holiest of places, the Arabian Peninsula, plundering its riches, dictating to its rulers, humiliating its people, terrorizing its neighbors, and turning its bases in the Peninsula into a spearhead through which to fight the neighboring Muslim peoples."[49]		In a December 1999 interview, bin Laden said he felt that Americans were "too near to Mecca", and considered this a provocation to the entire Muslim world.[50] One analysis of suicide terrorism suggested that without U.S. troops in Saudi Arabia, al-Qaeda likely would not have been able to get people to commit to suicide missions.[51]		In the 1998 fatwā, al-Qaeda identified the Iraq sanctions as a reason to kill Americans, condemning the "protracted blockade"[49] among other actions that constitute a declaration of war against "Allah, his messenger, and Muslims."[49] The fatwā declared that "the ruling to kill the Americans and their allies – civilians and military – is an individual duty for every Muslim who can do it in any country in which it is possible to do it, in order to liberate the al-Aqsa Mosque and the holy mosque of Mecca from their grip, and in order for their [the Americans'] armies to move out of all the lands of Islam, defeated and unable to threaten any Muslim."[10][52]		Bin Laden claimed, in 2004, that the idea of destroying the towers had first occurred to him in 1982, when he witnessed Israel's bombardment of high-rise apartment buildings during the 1982 Lebanon War.[53][54] Some analysts, including Mearsheimer and Walt, also claim that one motivation for the attacks was U.S. support of Israel.[42][50] In 2004 and 2010, bin Laden again connected the September 11 attacks with U.S. support of Israel, although most of the letter expressed bin Laden's disdain for President Bush and bin Laden's hope to "destroy and bankrupt" the U.S.[55][56]		Other motives have been suggested in addition to those stated by bin Laden and al-Qaeda, including western support of Islamic and non-Islamic authoritarian regimes in Saudi Arabia, Iran, Egypt, Iraq, Pakistan and northern Africa, and the presence of western troops in some of these countries.[57] Some authors suggest the "humiliation" resulting from the Islamic world falling behind the Western world – this discrepancy rendered especially visible by the globalization trend[58][59] and a desire to provoke the U.S. into a broader war against the Islamic world in the hope of motivating more allies to support al-Qaeda. Similarly, others have argued that 9/11 was a strategic move with the objective of provoking America into a war that would incite a pan-Islamic revolution.[60][61]		The idea for the attacks came from Khalid Sheikh Mohammed, who first presented it to Osama bin Laden in 1996.[62] At that time, bin Laden and al-Qaeda were in a period of transition, having just relocated back to Afghanistan from Sudan.[63] The 1998 African Embassy bombings and bin Laden's 1998 fatwā marked a turning point, as bin Laden became intent on attacking the United States.[63]		In late 1998 or early 1999, bin Laden gave approval for Mohammed to go forward with organizing the plot. A series of meetings occurred in early 1999, involving Mohammed, bin Laden, and his deputy Mohammed Atef.[63] Atef provided operational support for the plot, including target selections and helping arrange travel for the hijackers.[63] Bin Laden overruled Mohammed, rejecting some potential targets such as the U.S. Bank Tower in Los Angeles because, "there was not enough time to prepare for such an operation".[64][65]		Bin Laden provided leadership and financial support for the plot, and was involved in selecting participants.[66] Bin Laden initially selected Nawaf al-Hazmi and Khalid al-Mihdhar, both experienced jihadists who had fought in Bosnia. Hazmi and Mihdhar arrived in the United States in mid-January 2000. In spring 2000, Hazmi and Mihdhar took flying lessons in San Diego, California, but both spoke little English, performed poorly with flying lessons, and eventually served as secondary – or "muscle" – hijackers.[67][68]		In late 1999, a group of men from Hamburg, Germany arrived in Afghanistan, including Mohamed Atta, Marwan al-Shehhi, Ziad Jarrah, and Ramzi bin al-Shibh.[69] Bin Laden selected these men because they were educated, could speak English, and had experience living in the West.[70] New recruits were routinely screened for special skills and al-Qaeda leaders consequently discovered that Hani Hanjour already had a commercial pilot's license.[71] Mohammed later said that he helped the hijackers blend in by teaching them how to order food in restaurants and dress in Western clothing.[72]		Hanjour arrived in San Diego on December 8, 2000, joining Hazmi.[73]:6–7 They soon left for Arizona, where Hanjour took refresher training.[73]:7 Marwan al-Shehhi arrived at the end of May 2000, while Atta arrived on June 3, 2000, and Jarrah arrived on June 27, 2000.[73]:6 Bin al-Shibh applied several times for a visa to the United States, but as a Yemeni, he was rejected out of concerns he would overstay his visa and remain as an illegal immigrant.[73]:4, 14 Bin al-Shibh stayed in Hamburg, providing coordination between Atta and Mohammed.[73]:16 The three Hamburg cell members all took pilot training in South Florida.[73]:6		In spring of 2001, the secondary hijackers began arriving in the United States.[74] In July 2001, Atta met with bin al-Shibh in Spain, where they coordinated details of the plot, including final target selection. Bin al-Shibh also passed along bin Laden's wish for the attacks to be carried out as soon as possible.[75] Some of the hijackers received passports from corrupt Saudi officials who were family members, or used fraudulent passports to gain entry.[76]		In late 1999, al-Qaeda associate Khallad contacted Mihdhar, telling him to meet him in Kuala Lumpur, Malaysia; Hazmi and Abu Bara al Yemeni would also be in attendance. The NSA intercepted a telephone call mentioning the meeting, Mihdhar, and the name "Nawaf" (Hazmi). While the agency feared that "Something nefarious might be afoot", it took no further action. The CIA had already been alerted by Saudi intelligence to the status of Mihdhar and Hazmi as al-Qaeda members, and a CIA team broke into Mihdhar's Dubai hotel room and discovered that Mihdhar had a U.S. visa. While Alec Station alerted intelligence agencies worldwide about this fact, it did not share this information with the FBI. The Malaysian Special Branch observed the January 5, 2000, meeting of the two al-Qaeda members, and informed the CIA that Mihdhar, Hazmi, and Khallad were flying to Bangkok, but the CIA never notified other agencies of this, nor did it ask the State Department to put Mihdhar on its watchlist. An FBI liaison to Alec Station asked permission to inform the FBI of the meeting, but was told that "'This is not a matter for the FBI.'"[77]		By late June, senior counter-terrorism official Richard Clarke and CIA director George Tenet were "convinced that a major series of attacks was about to come", although the CIA believed that the attacks would likely occur in Saudi Arabia or Israel.[78] In early July, Clarke put domestic agencies on "full alert", telling them that "Something really spectacular is going to happen here... soon." He asked the FBI and the State Department to alert the embassies and police departments, and the Defense Department to go to "Threat Condition Delta."[79][80] Clarke would later write that "Somewhere in CIA there was information that two known al Qaeda terrorists had come into the United States... in [the] FBI there was information that strange things had been going on at flight schools in the United States... They had specific information about individual terrorists... None of that information got to me or the White House."[81]		On July 13, Tom Wilshire, a CIA agent assigned to the FBI's international terrorism division, emailed his superiors at the CIA's Counterterrorism Center (CTC), requesting permission to inform the FBI that Hazmi was in the country and that Mihdhar had a U.S. visa. However, the CIA never responded.[82]		The same day in July, Margarette Gillespie, an FBI analyst working in the CTC, was told to review material about the Malaysia meeting. She was not told of the participants' presence in the U.S. However, the CIA did give Gillespie surveillance photos of Mihdhar and Hazmi from the meeting to show to FBI counterterrorism, but did not tell her their significance. The Intelink database informed her not to share intelligence material on the meeting to criminal investigators. When shown the photos, the FBI were refused more details on their significance, and also did not receive Mihdhar's date of birth or passport number.[83] In late August 2001, Gillespie told the INS, the State Department, the Customs Service, and the FBI to put Hazmi and Mihdhar on their watchlists, but the FBI was prohibited from using criminal agents in the search for the duo, which hindered their efforts.[84]		Also in July, a Phoenix-based FBI agent sent a message to FBI headquarters, Alec Station, and to FBI agents in New York, alerting them to "the possibility of a coordinated effort by Osama bin Laden to send students to the United States to attend civil aviation universities and colleges." The agent, Kenneth Williams, suggested the need to interview all flight school managers and identify all Arab students seeking flight training.[85] In July, Jordan alerted the U.S. that al-Qaeda was planning an attack on the U.S.; "months later", Jordan notified the U.S. that the attack's codename was "The Big Wedding", and that it involved airplanes.[86]		On August 6, the CIA's Presidential Daily Brief, designated "For the President Only", was entitled "Bin Ladin Determined to Strike in U.S." The memo noted that "The FBI information... indicates patterns of suspicious activity in this country consistent with preparations for hijackings or other types of attacks."[87]		In mid-August, one Minnesota flight school alerted the FBI to Zacarias Moussaoui, who had asked "suspicious questions." The FBI found that he was a radical who had traveled to Pakistan, and the INS arrested him for overstaying his French visa. However, their request to search his laptop was denied by FBI headquarters due to the lack of probable cause.[88]		The failures in intelligence-sharing were attributed to 1995 Justice Department policies limiting intelligence sharing, combined with CIA and NSA reluctance in revealing "sensitive sources and methods" such as tapped phones.[89] Testifying before the 9/11 Commission in April 2004, then-Attorney General John Ashcroft recalled that the "single greatest structural cause for the September 11th problem was the wall that segregated or separated criminal investigators and intelligence agents."[90] Clarke also wrote that "There were failures in the organizations... failures to get information to the right place at the right time..."[91]		Early on the morning of September 11, 2001, 19 hijackers took control of four commercial airliners (two Boeing 757 and two Boeing 767) en route to California (three headed to LAX in Los Angeles, and one to SFO in San Francisco) after takeoffs from Logan International Airport in Boston, Massachusetts; Newark Liberty International Airport in Newark, New Jersey; and Washington Dulles International Airport in Loudoun and Fairfax counties in Virginia.[92] Large planes with long flights were selected for hijacking because they would be heavily fueled.[93]		The four flights were:		Media coverage was extensive during the attacks and aftermath, beginning moments after the first crash into the World Trade Center.[94]		At 8:46 a.m., five hijackers crashed American Airlines Flight 11 into the northern façade of the World Trade Center's North Tower (1 WTC), and at 9:03 a.m., another five hijackers crashed United Airlines Flight 175 into the southern façade of the South Tower (2 WTC).[96][97] Five hijackers flew American Airlines Flight 77 into the Pentagon at 9:37 a.m.[98] A fourth flight, United Airlines Flight 93, under the control of four hijackers, crashed near Shanksville, Pennsylvania, southeast of Pittsburgh, at 10:03 a.m. after the passengers fought the hijackers. Flight 93's target is believed to have been either the Capitol or the White House.[93] Flight 93's cockpit voice recorder revealed crew and passengers tried to seize control of the plane from the hijackers after learning through phone calls that Flights 11, 77, and 175 had been crashed into buildings that morning.[99] Once it became evident to the hijackers that the passengers might regain control of the plane, the hijackers rolled the plane and intentionally crashed it.[100][101]		Some passengers and crew members who called from the aircraft using the cabin airphone service and mobile phones provided details: several hijackers were aboard each plane; they used mace, tear gas, or pepper spray to overcome attendants; and some people aboard had been stabbed.[102][103][104][105][106][107][108] Reports indicated hijackers stabbed and killed pilots, flight attendants, and one or more passengers.[92][109] According to the 9/11 Commission's final report, the hijackers had recently purchased multi-function hand tools and assorted Leatherman-type utility knives with locking blades, which were not forbidden to passengers at the time, but were not found among the possessions left behind by the hijackers.[110][111] A flight attendant on Flight 11, a passenger on Flight 175, and passengers on Flight 93 said the hijackers had bombs, but one of the passengers said he thought the bombs were fake. The FBI found no traces of explosives at the crash sites, and the 9/11 Commission concluded that the bombs were probably fake.[92]		Three buildings in the World Trade Center collapsed due to fire-induced structural failure.[112] The South Tower collapsed at 9:59 a.m. after burning for 56 minutes in a fire caused by the impact of United Airlines Flight 175 and the explosion of its fuel.[112] The North Tower collapsed at 10:28 a.m. after burning for 102 minutes.[112] When the North Tower collapsed, debris fell on the nearby 7 World Trade Center building (7 WTC), damaging it and starting fires. These fires burned for hours, compromising the building's structural integrity, and 7 WTC collapsed at 5:21 p.m.[113][114] The west side of the Pentagon sustained significant damage.		At 9:42 a.m., the Federal Aviation Administration (FAA) grounded all civilian aircraft within the continental U.S., and civilian aircraft already in flight were told to land immediately.[116] All international civilian aircraft were either turned back or redirected to airports in Canada or Mexico, and were banned from landing on United States territory for three days.[117] The attacks created widespread confusion among news organizations and air traffic controllers. Among the unconfirmed and often contradictory news reports aired throughout the day, one of the most prevalent said a car bomb had been detonated at the U.S. State Department's headquarters in Washington, D.C.[118] Another jet—Delta Air Lines Flight 1989—was suspected of having been hijacked, but the aircraft responded to controllers and landed safely in Cleveland, Ohio.[119]		In an April 2002 interview, Khalid Sheikh Mohammed and Ramzi bin al-Shibh, who are believed to have organized the attacks, said Flight 93's intended target was the United States Capitol, not the White House.[120] During the planning stage of the attacks, Mohamed Atta, the hijacker and pilot of Flight 11, thought the White House might be too tough a target and sought an assessment from Hani Hanjour (who hijacked and piloted Flight 77).[121] Mohammed said al-Qaeda initially planned to target nuclear installations rather than the World Trade Center and the Pentagon, but decided against it, fearing things could "get out of control".[122] Final decisions on targets, according to Mohammed, were left in the hands of the pilots.[121]		The attacks caused the deaths of 2,996 people and the injuries of more than 6,000 others.[123] The death toll included 265 on the four planes (from which there were no survivors), 2,606 in the World Trade Center and in the surrounding area, and 125 at the Pentagon.[124][125] Nearly all of those who perished were civilians with the exceptions of 343 firefighters, 72 law enforcement officers, 55 military personnel, and the 19 terrorists who died in the attacks.[126][127] After New York, New Jersey lost the most state citizens, with the city of Hoboken having the most citizens that died in the attacks.[128] More than 90 countries lost citizens in the September 11 attacks;[129] for example, the 67 Britons who died were more than in any other terrorist attack anywhere as of June 2017[update].[130] As of June 2017[update] the attacks remained the worst terrorist attack in world history, and the deadliest foreign attack on American soil since the attack on Pearl Harbor on December 7, 1941.[3]		In Arlington County, Virginia, 125 Pentagon workers lost their lives when Flight 77 crashed into the western side of the building. Of these, 70 were civilians and 55 were military personnel, many of them who worked for the United States Army or the United States Navy. The Army lost 47 civilian employees, six civilian contractors, and 22 soldiers, while the Navy lost six civilian employees, three civilian contractors, and 33 sailors. Seven Defense Intelligence Agency (DIA) civilian employees were also among the dead in the attack, as well as an Office of the Secretary of Defense (OSD) contractor.[131][132][133] Lieutenant General Timothy Maude, an Army Deputy Chief of Staff, was the highest-ranking military official killed at the Pentagon.[134]		In New York City, more than 90% of the workers and visitors who died in the towers had been at or above the points of impact.[135] In the North Tower, 1,355 people at or above the point of impact were trapped and died of smoke inhalation, fell or jumped from the tower to escape the smoke and flames, or were killed in the building's eventual collapse. The destruction of all three staircases in the tower when Flight 11 hit made it impossible for anyone above the impact zone to escape. 107 people below the point of impact died as well.[135]		In the South Tower, one stairwell, Stairwell A, was left intact after Flight 175 hit, allowing 14 people located on the floors of impact (including one man who saw the plane coming at him) and four more from the floors above to escape. New York City 911 operators who received calls from individuals inside the tower were not well informed of the situation as it rapidly unfolded and as a result, told callers not to descend the tower on their own.[136] In total 630 people died in that tower, fewer than half the number killed in the North Tower.[135] Casualties in the South Tower were significantly reduced by some occupants deciding to start evacuating as soon as the North Tower was struck.[137]		At least 200 people fell or jumped to their deaths from the burning towers (as exemplified in the photograph The Falling Man), landing on the streets and rooftops of adjacent buildings hundreds of feet below.[138] Some occupants of each tower above the point of impact made their way toward the roof in hope of helicopter rescue, but the roof access doors were locked.[139] No plan existed for helicopter rescues, and the combination of roof equipment and thick smoke and intense heat prevented helicopters from approaching.[140] A total of 411 emergency workers died as they tried to rescue people and fight fires. The New York City Fire Department (FDNY) lost 343 firefighters, including a chaplain and two paramedics.[141] The New York City Police Department (NYPD) lost 23 officers.[142] The Port Authority Police Department (PAPD) lost 37 officers.[143] Eight emergency medical technicians (EMTs) and paramedics from private emergency medical services units were killed.[144]		Cantor Fitzgerald L.P., an investment bank on the 101st–105th floors of the North Tower, lost 658 employees, considerably more than any other employer.[145] Marsh Inc., located immediately below Cantor Fitzgerald on floors 93–100, lost 358 employees,[146][147] and 175 employees of Aon Corporation were also killed.[148] The National Institute of Standards and Technology (NIST) estimated that about 17,400 civilians were in the World Trade Center complex at the time of the attacks. Turnstile counts from the Port Authority suggest 14,154 people were typically in the Twin Towers by 8:45 a.m.[149][150] Most people below the impact zone safely evacuated the buildings.[151]		Weeks after the attack, the death toll was estimated to be over 6,000, more than twice the number of deaths eventually confirmed.[158] The city was only able to identify remains for about 1,600 of the World Trade Center victims. The medical examiner's office collected "about 10,000 unidentified bone and tissue fragments that cannot be matched to the list of the dead".[159] Bone fragments were still being found in 2006 by workers who were preparing to demolish the damaged Deutsche Bank Building. In 2010, a team of anthropologists and archaeologists searched for human remains and personal items at the Fresh Kills Landfill, where seventy-two more human remains were recovered, bringing the total found to 1,845. DNA profiling continues in an attempt to identify additional victims.[160][161][162] The remains are being held in storage in Memorial Park, outside the New York City Medical Examiner's facilities. It was expected that the remains would be moved in 2013 to a repository behind a wall at the 9/11 museum. In July 2011, a team of scientists at the Office of Chief Medical Examiner was still trying to identify remains, in the hope that improved technology will allow them to identify other victims.[162] On August 7, 2017, the 1,641st victim was identified as a result of newly available DNA technology.[163] There are still 1,112 victims who have not been identified.[164]		Along with the 110-floor Twin Towers, numerous other buildings at the World Trade Center site were destroyed or badly damaged, including WTC buildings 3 through 7 and St. Nicholas Greek Orthodox Church.[165] The North Tower, South Tower, the Marriott Hotel (3 WTC), and 7 WTC were completely destroyed. The U.S. Customs House (6 World Trade Center), 4 World Trade Center, 5 World Trade Center, and both pedestrian bridges connecting buildings were severely damaged. The Deutsche Bank Building on 130 Liberty Street was partially damaged and demolished some years later, starting in 2007.[166][167] The two buildings of the World Financial Center also suffered damage.[166]		The Deutsche Bank Building across Liberty Street from the World Trade Center complex was later condemned as uninhabitable because of toxic conditions inside the office tower, and was deconstructed.[168][169] The Borough of Manhattan Community College's Fiterman Hall at 30 West Broadway was condemned due to extensive damage in the attacks, and is being rebuilt.[170] Other neighboring buildings (including 90 West Street and the Verizon Building) suffered major damage but have been restored.[171] World Financial Center buildings, One Liberty Plaza, the Millenium Hilton, and 90 Church Street had moderate damage and have since been restored.[172] Communications equipment on top of the North Tower was also destroyed, but media stations were quickly able to reroute the signals and resume their broadcasts.[165][173]		The Pentagon was severely damaged by the impact of American Airlines Flight 77 and ensuing fires, causing one section of the building to collapse.[174] As the airplane approached the Pentagon, its wings knocked down light poles and its right engine hit a power generator before crashing into the western side of the building.[175][176] The plane hit the Pentagon at the first-floor level. The front part of the fuselage disintegrated on impact, while the mid and tail sections kept moving for another fraction of a second.[177] Debris from the tail section penetrated furthest into the building, breaking through 310 feet (94 m) of the three outermost of the building's five rings.[177][178]		The New York City Fire Department deployed 200 units (half of the department) to the World Trade Center. Their efforts were supplemented by numerous off-duty firefighters and emergency medical technicians.[179][180][181] The New York City Police Department sent Emergency Service Units and other police personnel, and deployed its aviation unit. Once on the scene, the FDNY, the NYPD, and the PAPD did not coordinate efforts and performed redundant searches for civilians.[179][182] As conditions deteriorated, the NYPD aviation unit relayed information to police commanders, who issued orders for its personnel to evacuate the towers; most NYPD officers were able to safely evacuate before the buildings collapsed.[182][183] With separate command posts set up and incompatible radio communications between the agencies, warnings were not passed along to FDNY commanders.		After the first tower collapsed, FDNY commanders issued evacuation warnings; however, due to technical difficulties with malfunctioning radio repeater systems, many firefighters never heard the evacuation orders. 9-1-1 dispatchers also received information from callers that was not passed along to commanders on the scene.[180] Within hours of the attack, a substantial search and rescue operation was launched. After months of around-the-clock operations, the World Trade Center site was cleared by the end of May 2002.[184]		The aftermath of the 9/11 attack resulted in immediate responses to the event, including domestic reactions, hate crimes, Muslim responses to the event, international responses to the attack, and military responses to the events. An extensive compensation program was quickly established by Congress in the aftermath to compensate the victims and families of victims of the 9/11 attack as well.[185][186]		At 8:32 a.m., FAA officials were notified Flight 11 had been hijacked and they in turn notified the North American Aerospace Defense Command (NORAD). NORAD scrambled two F-15s from Otis Air National Guard Base in Massachusetts and they were airborne by 8:53 a.m.[187] Because of slow and confused communication from FAA officials, NORAD had 9 minutes' notice that Flight 11 had been hijacked, and no notice about any of the other flights before they crashed.[187] After both of the Twin Towers had already been hit, more fighters were scrambled from Langley Air Force Base in Virginia at 9:30 a.m.[187] At 10:20 a.m. Vice President Dick Cheney issued orders to shoot down any commercial aircraft that could be positively identified as being hijacked. However, these instructions were not relayed in time for the fighters to take action.[187][188][189][190] Some fighters took to the air without live ammunition, knowing that to prevent the hijackers from striking their intended targets, the pilots might have to intercept and crash their fighters into the hijacked planes, possibly ejecting at the last moment.[191]		For the first time in U.S. history, SCATANA was invoked,[192] thus stranding tens of thousands of passengers across the world.[193] The FAA closed American airspace to all international flights, causing about five hundred flights to be turned back or redirected to other countries. Canada received 226 of the diverted flights and launched Operation Yellow Ribbon to deal with the large numbers of grounded planes and stranded passengers.[194]		The 9/11 attacks had immediate effects on the American people.[195] Police and rescue workers from around the country took leaves of absence, traveling to New York City to help recover bodies from the twisted remnants of the Twin Towers.[196] Blood donations across the U.S. surged in the weeks after 9/11.[197][198]		The deaths of adults in the attacks resulted in over 3,000 children losing a parent.[199] Subsequent studies documented children's reactions to these actual losses and to feared losses of life, the protective environment in the aftermath of the attacks, and effects on surviving caregivers.[200][201][202]		Following the attacks, President Bush's approval rating soared to 90%.[203] On September 20, 2001, he addressed the nation and a joint session of the United States Congress regarding the events of September 11 and the subsequent nine days of rescue and recovery efforts, and described his intended response to the attacks. New York City mayor Rudy Giuliani's highly visible role won him high praise in New York and nationally.[204]		Many relief funds were immediately set up to assist victims of the attacks, with the task of providing financial assistance to the survivors of the attacks and to the families of victims. By the deadline for victim's compensation on September 11, 2003, 2,833 applications had been received from the families of those who were killed.[205]		Contingency plans for the continuity of government and the evacuation of leaders were implemented soon after the attacks.[193] However, Congress was not told that the United States had been under a continuity of government status until February 2002.[206]		In the largest restructuring of the U.S. government in contemporary history, the United States enacted the Homeland Security Act of 2002, creating the Department of Homeland Security. Congress also passed the USA PATRIOT Act, saying it would help detect and prosecute terrorism and other crimes.[207] Civil liberties groups have criticized the PATRIOT Act, saying it allows law enforcement to invade the privacy of citizens and that it eliminates judicial oversight of law enforcement and domestic intelligence.[208][209][210] In an effort to effectively combat future acts of terrorism, the National Security Agency (NSA) was given broad powers. NSA commenced warrantless surveillance of telecommunications, which was sometimes criticized since it permitted the agency "to eavesdrop on telephone and e-mail communications between the United States and people overseas without a warrant".[211] In response to requests by various intelligence agencies, the United States Foreign Intelligence Surveillance Court permitted an expansion of powers by the U.S. government in seeking, obtaining, and sharing information on U.S. citizens as well as non-U.S. people from around the world.[212]		Shortly after the attacks, President Bush made a public appearance at Washington's largest Islamic Center and acknowledged the "incredibly valuable contribution" that millions of American Muslims made to their country and called for them "to be treated with respect."[213] However, numerous incidents of harassment and hate crimes against Muslims and South Asians were reported in the days following the attacks.[214][215][216] Sikhs were also targeted because Sikh males usually wear turbans, which are stereotypically associated with Muslims. There were reports of attacks on mosques and other religious buildings (including the firebombing of a Hindu temple), and assaults on people, including one murder: Balbir Singh Sodhi, a Sikh mistaken for a Muslim, was fatally shot on September 15, 2001, in Mesa, Arizona.[216]		According to an academic study, people perceived to be Middle Eastern were as likely to be victims of hate crimes as followers of Islam during this time. The study also found a similar increase in hate crimes against people who may have been perceived as Muslims, Arabs, and others thought to be of Middle Eastern origin.[217] A report by the South Asian American advocacy group known as South Asian Americans Leading Together, documented media coverage of 645 bias incidents against Americans of South Asian or Middle Eastern descent between September 11 and 17. Various crimes such as vandalism, arson, assault, shootings, harassment, and threats in numerous places were documented.[218][219]		Muslim organizations in the United States were swift to condemn the attacks and called "upon Muslim Americans to come forward with their skills and resources to help alleviate the sufferings of the affected people and their families".[220] These organizations included the Islamic Society of North America, American Muslim Alliance, American Muslim Council, Council on American-Islamic Relations, Islamic Circle of North America, and the Shari'a Scholars Association of North America. Along with monetary donations, many Islamic organizations launched blood drives and provided medical assistance, food, and shelter for victims.[221][222][223]		The attacks were denounced by mass media and governments worldwide. Across the globe, nations offered pro-American support and solidarity.[224] Leaders in most Middle Eastern countries, and Afghanistan, condemned the attacks. Iraq was a notable exception, with an immediate official statement that, "the American cowboys are reaping the fruit of their crimes against humanity".[225] The government of Saudi Arabia officially condemned the attacks, but privately many Saudis favored bin Laden's cause.[226][227] Although Palestinian Authority (PA) president Yasser Arafat also condemned the attacks, there were reports of celebrations in the West Bank, Gaza Strip, and East Jerusalem—with a celebration involving 3,000 Palestinians dancing in the streets and handing out candy being filmed in Nablus despite alleged PA warnings that it could not guarantee the safety of journalists attempting to document the event. Similar demonstrations took place in Amman, Jordan, where there is a large population of Palestinian descent.[228] As in the United States, the aftermath of the attacks saw tensions increase in other countries between Muslims and non-Muslims.[229]		United Nations Security Council Resolution 1368 condemned the attacks, and expressed readiness to take all necessary steps to respond and combat all forms of terrorism in accordance with their Charter.[230] Numerous countries introduced anti-terrorism legislation and froze bank accounts they suspected of al-Qaeda ties.[231][232] Law enforcement and intelligence agencies in a number of countries arrested alleged terrorists.[233][234]		British Prime Minister Tony Blair said Britain stood "shoulder to shoulder" with the United States.[235] A few days later, Blair flew to Washington to affirm British solidarity with the United States. In a speech to Congress, nine days after the attacks, which Blair attended as a guest, President Bush declared "America has no truer friend than Great Britain."[236] Subsequently, Prime Minister Blair embarked on two months of diplomacy to rally international support for military action; he held 54 meetings with world leaders and travelled more than 40,000 miles (60,000 km).[237]		Tens of thousands of people attempted to flee Afghanistan following the attacks, fearing a response by the United States. Pakistan, already home to many Afghan refugees from previous conflicts, closed its border with Afghanistan on September 17, 2001. Approximately one month after the attacks, the United States led a broad coalition of international forces to overthrow the Taliban regime from Afghanistan for their harboring of al-Qaeda.[238] Though Pakistani authorities were initially reluctant to align themselves with the United States against the Taliban, they permitted the coalition access to their military bases, and arrested and handed over to the U.S. over 600 suspected al-Qaeda members.[239][240]		The U.S. set up the Guantanamo Bay detention camp to hold inmates they defined as "illegal enemy combatants". The legitimacy of these detentions has been questioned by the European Union and human rights organizations.[241][242][243]		On September 25, 2001, Iran's fifth president, Mohammad Khatami meeting British Foreign Secretary, Jack Straw, said: "Iran fully understands the feelings of the Americans about the terrorist attacks in New York and Washington on September 11." He said although the American administrations had been at best indifferent about terrorist operations in Iran (since 1979), the Iranians instead felt differently and had expressed their sympathetic feelings with bereaved Americans in the tragic incidents in the two cities. He also stated that "Nations should not be punished in place of terrorists." [244] According to Radio Farda's website, when the attacks' news was released, some Iranian citizens gathered in front of the Embassy of Switzerland in Tehran, which serves as the protecting power of the United States in Iran (US interests protecting office in Iran), to express their sympathy and some of them lit candles as a symbol of mourning. This piece of news at Radio Farda's website also states that in 2011, on the anniversary of the attacks, United States Department of State, published a post at its blog, in which the Department thanked Iranian people for their sympathy and stated that they would never forget Iranian people's kindness on those harsh days.[245] After the attacks, both the President[246][247] and the Supreme Leader of Iran, condemned the attacks. The BBC and Time magazine published reports on holding candlelit vigils for the victims by Iranian citizens at their websites.[248][249] According to Politico Magazine, following the attacks, Sayyed Ali Khamenei, the Supreme Leader of Iran, "suspended the usual 'Death to America' chants at Friday prayers" temporarily.[250]		At 2:40 p.m. in the afternoon of September 11, Secretary of Defense Donald Rumsfeld was issuing rapid orders to his aides to look for evidence of Iraqi involvement. According to notes taken by senior policy official Stephen Cambone, Rumsfeld asked for, "Best info fast. Judge whether good enough hit S.H." (Saddam Hussein) "at same time. Not only UBL" (Osama bin Laden).[251] Cambone's notes quoted Rumsfeld as saying, "Need to move swiftly – Near term target needs – go massive – sweep it all up. Things related and not."[252][253] In a meeting at Camp David on September 15 the Bush administration rejected the idea of attacking Iraq in response to 9/11.[254] Nonetheless, they later invaded the country with allies, citing "Saddam Hussein's support for terrorism".[255] At the time, as many as 7 in 10 Americans believed the Iraqi president played a role in the 9/11 attacks.[256] Three years later, Bush conceded that he had not. [257]		The NATO council declared the attacks on the United States were an attack on all NATO nations which satisfied Article 5 of the NATO charter. This marked the first invocation of Article 5, which had been written during the Cold War with an attack by the Soviet Union in mind.[258] Australian Prime Minister John Howard who was in Washington D.C. during the attacks invoked Article IV of the ANZUS treaty.[259] The Bush administration announced a War on Terror, with the stated goals of bringing bin Laden and al-Qaeda to justice and preventing the emergence of other terrorist networks.[260] These goals would be accomplished by imposing economic and military sanctions against states harboring terrorists, and increasing global surveillance and intelligence sharing.[261]		On September 14, 2001, the U.S. Congress passed the Authorization for Use of Military Force Against Terrorists. Still in effect, it grants the President the authority to use all "necessary and appropriate force" against those whom he determined "planned, authorized, committed or aided" the September 11 attacks, or who harbored said persons or groups.[262]		On October 7, 2001, the War in Afghanistan began when U.S. and British forces initiated aerial bombing campaigns targeting Taliban and al-Qaeda camps, then later invaded Afghanistan with ground troops of the Special Forces.[263] This eventually led to the overthrow of the Taliban rule of Afghanistan with the Fall of Kandahar on December 7, 2001, by U.S. led coalition forces.[264] Conflict in Afghanistan between the Taliban insurgency and the Afghan forces backed by NATO Resolute Support Mission is ongoing. The Philippines and Indonesia, among other nations with their own internal conflicts with Islamic terrorism, also increased their military readiness.[265][266]		The military forces of the United States of America and the Islamic Republic of Iran cooperated with each other to overthrow the Taliban regime which had had conflicts with the government of Iran.[250] Iran's Quds Force helped US forces and Afghan rebels in the 2001 uprising in Herat.[267][268]		Hundreds of thousands of tons of toxic debris containing more than 2,500 contaminants, including known carcinogens, were spread across Lower Manhattan due to the collapse of the Twin Towers.[269][270] Exposure to the toxins in the debris is alleged to have contributed to fatal or debilitating illnesses among people who were at ground zero.[271][272] The Bush administration ordered the Environmental Protection Agency (EPA) to issue reassuring statements regarding air quality in the aftermath of the attacks, citing national security, but the EPA did not determine that air quality had returned to pre-September 11 levels until June 2002.[273]		Health effects extended to residents, students, and office workers of Lower Manhattan and nearby Chinatown.[274] Several deaths have been linked to the toxic dust, and the victims' names were included in the World Trade Center memorial.[275] Approximately 18,000 people have been estimated to have developed illnesses as a result of the toxic dust.[276] There is also scientific speculation that exposure to various toxic products in the air may have negative effects on fetal development. A notable children's environmental health center is currently analyzing the children whose mothers were pregnant during the WTC collapse, and were living or working nearby.[277] A study of rescue workers released in April 2010 found that all those studied had impaired lung functions, and that 30–40% were reporting little or no improvement in persistent symptoms that started within the first year of the attack.[278]		Years after the attacks, legal disputes over the costs of illnesses related to the attacks were still in the court system. On October 17, 2006, a federal judge rejected New York City's refusal to pay for health costs for rescue workers, allowing for the possibility of numerous suits against the city.[279] Government officials have been faulted for urging the public to return to lower Manhattan in the weeks shortly after the attacks. Christine Todd Whitman, administrator of the EPA in the aftermath of the attacks, was heavily criticized by a U.S. District Judge for incorrectly saying that the area was environmentally safe.[280] Mayor Giuliani was criticized for urging financial industry personnel to return quickly to the greater Wall Street area.[281]		The United States Congress passed the James L. Zadroga 9/11 Health and Compensation Act on December 22, 2010, and President Barack Obama signed the act into law on January 2, 2011. It allocated $4.2 billion to create the World Trade Center Health Program, which provides testing and treatment for people suffering from long-term health problems related to the 9/11 attacks.[282][283] The WTC Health Program replaced preexisting 9/11-related health programs such as the Medical Monitoring and Treatment Program and the WTC Environmental Health Center program.[283]		The attacks had a significant economic impact on United States and world markets.[284] The stock exchanges did not open on September 11 and remained closed until September 17. Reopening, the Dow Jones Industrial Average (DJIA) fell 684 points, or 7.1%, to 8921, a record-setting one-day point decline.[285] By the end of the week, the DJIA had fallen 1,369.7 points (14.3%), at the time its largest one-week point drop in history.[286] In 2001 dollars, U.S. stocks lost $1.4 trillion in valuation for the week.[286]		In New York City, about 430,000 job-months and $2.8 billion dollars in wages were lost in the three months after the attacks. The economic effects were mainly on the economy's export sectors.[287] The city's GDP was estimated to have declined by $27.3 billion for the last three months of 2001 and all of 2002. The U.S. government provided $11.2 billion in immediate assistance to the Government of New York City in September 2001, and $10.5 billion in early 2002 for economic development and infrastructure needs.[288]		Also hurt were small businesses in Lower Manhattan near the World Trade Center, 18,000 of which were destroyed or displaced, resulting in lost jobs and their consequent wages. Assistance was provided by Small Business Administration loans, federal government Community Development Block Grants, and Economic Injury Disaster Loans.[288] Some 31,900,000 square feet (2,960,000 m2) of Lower Manhattan office space was damaged or destroyed.[289] Many wondered whether these jobs would return, and if the damaged tax base would recover.[290] Studies of the economic effects of 9/11 show the Manhattan office real-estate market and office employment were less affected than first feared, because of the financial services industry's need for face-to-face interaction.[291][292]		North American air space was closed for several days after the attacks and air travel decreased upon its reopening, leading to a nearly 20% cutback in air travel capacity, and exacerbating financial problems in the struggling U.S. airline industry.[293]		The September 11 attacks also led to the U.S. wars in Afghanistan and Iraq,[294] as well as additional homeland security spending, totaling at least $5 trillion.[295]		The impact of 9/11 extends beyond geopolitics into society and culture in general. Immediate responses to 9/11 included greater focus on home life and time spent with family, higher church attendance, and increased expressions of patriotism such as the flying of flags.[296] The radio industry responded by removing certain songs from playlists, and the attacks have subsequently been used as background, narrative or thematic elements in film, television, music and literature. Already-running television shows as well as programs developed after 9/11 have reflected post-9/11 cultural concerns.[297] 9/11 conspiracy theories have become social phenomena, despite lack of support from expert scientists, engineers, and historians.[298] 9/11 has also had a major impact on the religious faith of many individuals; for some it strengthened, to find consolation to cope with the loss of loved ones and overcome their grief; others started to question their faith or lost it entirely, because they could not reconcile it with their view of religion.[299][300]		The culture of America succeeding the attacks is noted for heightened security and an increased demand thereof, as well as paranoia and anxiety regarding future terrorist attacks that includes most of the nation. Psychologists have also confirmed that there has been an increased amount of national anxiety in commercial air travel.[301]		As a result of the attacks, many governments across the world passed legislation to combat terrorism.[302] In Germany, where several of the 9/11 terrorists had resided and taken advantage of that country's liberal asylum policies, two major anti-terrorism packages were enacted. The first removed legal loopholes that permitted terrorists to live and raise money in Germany. The second addressed the effectiveness and communication of intelligence and law enforcement.[303] Canada passed the Canadian Anti-Terrorism Act, that nation's first anti-terrorism law.[304] The United Kingdom passed the Anti-terrorism, Crime and Security Act 2001 and the Prevention of Terrorism Act 2005.[305][306] New Zealand enacted the Terrorism Suppression Act 2002.[307]		In the United States, the Department of Homeland Security was created by the Homeland Security Act to coordinate domestic anti-terrorism efforts. The USA Patriot Act gave the federal government greater powers, including the authority to detain foreign terror suspects for a week without charge, to monitor telephone communications, e-mail, and Internet use by terror suspects, and to prosecute suspected terrorists without time restrictions. The FAA ordered that airplane cockpits be reinforced to prevent terrorists gaining control of planes, and assigned sky marshals to flights. Further, the Aviation and Transportation Security Act made the federal government, rather than airports, responsible for airport security. The law created the Transportation Security Administration to inspect passengers and luggage, causing long delays and concern over passenger privacy.[308] After suspected abuses of the USA Patriot Act were brought to light in June 2013 with articles about collection of American call records by the NSA and the PRISM program (see 2013 mass surveillance disclosures), Representative Jim Sensenbrenner, Republican of Wisconsin, who introduced the Patriot Act in 2001, said that the National Security Agency overstepped its bounds.[309][310]		Immediately after the attacks, the Federal Bureau of Investigation started PENTTBOM, the largest criminal inquiry in the history of the United States. At its height, more than half of the FBI's agents worked on the investigation and followed a half-million leads.[311] The FBI concluded that there was "clear and irrefutable" evidence linking al-Qaeda and bin Laden to the attacks.[312]		The FBI was quickly able to identify the hijackers, including leader Mohamed Atta, when his luggage was discovered at Boston's Logan Airport. Atta had been forced to check two of his three bags due to space limitations on the 19-seat commuter flight he took to Boston.[313] Due to a new policy instituted to prevent flight delays, the luggage failed to make it aboard American Airlines Flight 11 as planned. The luggage contained the hijackers' names, assignments and al-Qaeda connections. "It had all these Arab-language (sic) papers that amounted to the Rosetta stone of the investigation", said one FBI agent.[314] Within hours of the attacks, the FBI released the names and in many cases the personal details of the suspected pilots and hijackers.[315][316] On September 27, 2001, they released photos of all 19 hijackers, along with information about possible nationalities and aliases.[317] Fifteen of the men were from Saudi Arabia, two from the United Arab Emirates, one from Egypt, and one from Lebanon.[318]		By midday, the U.S. National Security Agency and German intelligence agencies had intercepted communications pointing to Osama bin Laden.[319] Two of the hijackers were known to have travelled with a bin Laden associate to Malaysia in 2000[320] and hijacker Mohammed Atta had previously gone to Afghanistan.[321] He and others were part of a terrorist cell in Hamburg.[322] One of the members of the Hamburg cell was discovered to have been in communication with Khalid Sheik Mohammed who was identified as a member of al-Qaeda.[323]		Authorities in the United States and Britain also obtained electronic intercepts, including telephone conversations and electronic bank transfers, which indicate that Mohammed Atef, a bin Laden deputy, was a key figure in the planning of the 9/11 attacks. Intercepts were also obtained that revealed conversations that took place days before September 11 between bin Laden and an associate in Pakistan. In those conversations, the two referred to "an incident that would take place in America on, or around, September 11" and they discussed potential repercussions. In another conversation with an associate in Afghanistan, bin Laden discussed the "scale and effects of a forthcoming operation." These conversations did not specifically mention the World Trade Center or Pentagon, or other specifics.[324]		The Inspector General of the Central Intelligence Agency (CIA) conducted an internal review of the agency's pre-9/11 performance and was harshly critical of senior CIA officials for not doing everything possible to confront terrorism. He criticized their failure to stop two of the 9/11 hijackers, Nawaf al-Hazmi and Khalid al-Mihdhar, as they entered the United States and their failure to share information on the two men with the FBI.[325] In May 2007, senators from both major U.S. political parties drafted legislation to make the review public. One of the backers, Senator Ron Wyden said, "The American people have a right to know what the Central Intelligence Agency was doing in those critical months before 9/11."[326]		In February 2002 the Senate Select Committee on Intelligence and the House Permanent Select Committee on Intelligence formed a joint inquiry into the performance of the U.S. Intelligence Community.[327] Their 832 page report released in December 2002[328] detailed failings of the FBI and CIA to use available information, including about terrorists the CIA knew were in the United States, in order to disrupt the plots.[329] The joint inquiry developed its information about possible involvement of Saudi Arabian government officials from non-classified sources.[330] Nevertheless, the Bush administration demanded 28 related pages remain classified.[329] In December 2002 the inquiry's chair Bob Graham (D-FL) revealed in an interview that there was "evidence that there were foreign governments involved in facilitating the activities of at least some of the terrorists in the United States."[331] September 11 victim families were frustrated by the unanswered questions and redacted material from the Congressional inquiry and demanded an independent commission.[329] September 11 victim families,[332] members of congress[333][334] and the Saudi Arabian government are still seeking release of the documents.[335][336] In June 2016, CIA chief John Brennan says that he believes 28 redacted pages of a congressional inquiry into 9/11 will soon be made public, and that they will prove that the government of Saudi Arabia had no involvement in the September 11 attacks.[337]		In September 2016, the Congress passed the Justice Against Sponsors of Terrorism Act that would allow relatives of victims of the September 11 attacks to sue Saudi Arabia for its government's alleged role in the attacks.[338][339][340]		The National Commission on Terrorist Attacks Upon the United States (9/11 Commission), chaired by Thomas Kean and Lee H. Hamilton, was formed in late 2002 to prepare a thorough account of the circumstances surrounding the attacks, including preparedness for and the immediate response to the attacks.[341] On July 22, 2004, the Commission issued the 9/11 Commission Report. The report detailed the events of 9/11, found the attacks were carried out by members of al-Qaeda, and examined how security and intelligence agencies were inadequately coordinated to prevent the attacks. Formed from an independent bipartisan group of mostly former Senators, Representatives, and Governors, the commissioners explained, "We believe the 9/11 attacks revealed four kinds of failures: in imagination, policy, capabilities, and management".[342] The Commission made numerous recommendations on how to prevent future attacks, and in 2011 was dismayed that several of its recommendations had yet to be implemented.[343]		The U.S. National Institute of Standards and Technology (NIST) investigated the collapses of the Twin Towers and 7 WTC. The investigations examined why the buildings collapsed and what fire protection measures were in place, and evaluated how fire protection systems might be improved in future construction.[344] The investigation into the collapse of 1 WTC and 2 WTC was concluded in October 2005 and that of 7 WTC was completed in August 2008.[345]		NIST found that the fireproofing on the Twin Towers' steel infrastructures was blown off by the initial impact of the planes and that, had this not occurred, the towers likely would have remained standing.[346] A 2007 study of the north tower's collapse published by researchers of Purdue University determined that, since the plane's impact had stripped off much of the structure's thermal insulation, the heat from a typical office fire would have softened and weakened the exposed girders and columns enough to initiate the collapse regardless of the number of columns cut or damaged by the impact.[347][348]		The director of the original investigation stated that, "the towers really did amazingly well. The terrorist aircraft didn't bring the buildings down; it was the fire which followed. It was proven that you could take out two thirds of the columns in a tower and the building would still stand."[349] The fires weakened the trusses supporting the floors, making the floors sag. The sagging floors pulled on the exterior steel columns causing the exterior columns to bow inward. With the damage to the core columns, the buckling exterior columns could no longer support the buildings, causing them to collapse. Additionally, the report found the towers' stairwells were not adequately reinforced to provide adequate emergency escape for people above the impact zones.[350] NIST concluded that uncontrolled fires in 7 WTC caused floor beams and girders to heat and subsequently "caused a critical support column to fail, initiating a fire-induced progressive collapse that brought the building down".[345]		On the day of the attacks, New York City mayor Rudy Giuliani stated: "We will rebuild. We're going to come out of this stronger than before, politically stronger, economically stronger. The skyline will be made whole again."[351]		The damaged section of the Pentagon was rebuilt and occupied within a year of the attacks.[352] The temporary World Trade Center PATH station opened in late 2003 and construction of the new 7 World Trade Center was completed in 2006. Work on rebuilding the main World Trade Center site was delayed until late 2006 when leaseholder Larry Silverstein and the Port Authority of New York and New Jersey agreed on financing.[353] The construction of One World Trade Center began on April 27, 2006, and reached its full height on May 20, 2013. The spire was installed atop the building at that date, putting 1 WTC's height at 1,776 feet (541 m) and thus claiming the title of the tallest building in the Western Hemisphere.[354] One WTC finished construction and opened on November 3, 2014.[5][355]		On the World Trade Center site, three more office towers are expected to be built one block east of where the original towers stood. Construction has begun on all three of these towers.[356]		In the days immediately following the attacks, many memorials and vigils were held around the world, and photographs of the dead and missing were posted around Ground Zero. A witness described being unable to "get away from faces of innocent victims who were killed. Their pictures are everywhere, on phone booths, street lights, walls of subway stations. Everything reminded me of a huge funeral, people quiet and sad, but also very nice. Before, New York gave me a cold feeling; now people were reaching out to help each other."[357]		One of the first memorials was the Tribute in Light, an installation of 88 searchlights at the footprints of the World Trade Center towers.[358] In New York, the World Trade Center Site Memorial Competition was held to design an appropriate memorial on the site.[359] The winning design, Reflecting Absence, was selected in August 2006, and consists of a pair of reflecting pools in the footprints of the towers, surrounded by a list of the victims' names in an underground memorial space.[360]		The Pentagon Memorial was completed and opened to the public on the seventh anniversary of the attacks in 2008.[361][362] It consists of a landscaped park with 184 benches facing the Pentagon.[363] When the Pentagon was repaired in 2001–2002, a private chapel and indoor memorial were included, located at the spot where Flight 77 crashed into the building.[364]		In Shanksville, a permanent Flight 93 National Memorial is planned to include a sculpted grove of trees forming a circle around the crash site, bisected by the plane's path, while wind chimes will bear the names of the victims.[365] A temporary memorial is located 500 yards (457 m) from the crash site.[366] New York City firefighters donated a cross made of steel from the World Trade Center and mounted on top of a platform shaped like the Pentagon.[367] It was installed outside the firehouse on August 25, 2008.[368] Many other permanent memorials are elsewhere. Scholarships and charities have been established by the victims' families, and by many other organizations and private figures.[369]		On every anniversary, in New York City, the names of the victims who died there are read out against a background of somber music. The President of the United States attends a memorial service at the Pentagon,[370] and asks Americans to observe Patriot Day with a moment of silence. Smaller services are held in Shanksville, Pennsylvania, which are usually attended by the President's spouse.		Multimedia		
Salvatore Attardo is a full professor at Texas A&M University–Commerce[1] and the editor-in-chief of Humor, the journal for the International Society of Humor Research.[2] He studied at Purdue University under Victor Raskin and extended Raskin's script-based semantic theory of humor (SSTH) into the general theory of verbal humor (GTVH). He publishes in the field of humor in literature and is considered to be one of the top authorities in the area.		He was born March 14, 1962, in Anderlecht, Belgium, to an Italian State Railways employee and a Belgian mother, living thereafter in Como, Italy, until adulthood. He has been a permanent resident of the United States since 1991. He has one daughter, Gaia, born in 1994. Attardo is a native speaker of Italian and French. He has served on the thesis and dissertation committees for other humor scholars, including Christian F. Hempelmann and Katrina Triezenberg.						As a teenager, Attardo attended a High School specializing in Humanities (Liceo Ginnasio Statale Alessando Volta, Como) where along with fellow students he published a satyrical magazine on the school life, its teachers and principal, called "Giravolta." In these early days, he was known by the nickname of "Pidou."		
In anthropology, a joking relationship is a relationship between two people that involves a ritualised banter of teasing or mocking.						Analysed by British social anthropologist Alfred Radcliffe-Brown in 1940,[1] it describes a kind of ritualised banter that takes place, for example between a man and his maternal mother-in-law in some South African tribal societies. Two main variations are described: an asymmetrical relationship where one party is required to take no offence at constant teasing or mocking by the other, and a symmetrical relationship where each party makes fun at the other's expense.		The joking relationship is an interaction that mediates and stabilizes social relationships where there is tension, competition, or potential conflict, such as between in-laws and between clans and tribes.[2]		While first encountered by Radcliffe-Brown in the 1920s, this type of relationship is now understood to be very widespread across societies in general. In West Africa, particularly in Mali, it is regarded as a centuries-old cultural institution known as sanankuya.		This type of relationship contrasts strongly with societies where so-called avoidance speech or "mother-in-law" language is imposed to minimise interaction between the two parties, as in many Australian Aboriginal languages. Donald F. Thomson's article "The Joking Relationship and Organized Obscenity in North Queensland" gives an in depth discussion of a number of societies where these two speech styles co-exist.[3] The joking relationships which are most unconstrained and free are between classificatory Father's Father and Son's Son—which appears to be the same situation in the Plains cultures of North America.		
There are many theories of humor which attempt to explain what humor is, what social functions it serves, and what would be considered humorous. Among the prevailing types of theories that attempt to account for the existence of humor, there are psychological theories, the vast majority of which consider humor to be very healthy behavior; there are spiritual theories, which consider humor to be an inexplicable mystery, very much like a mystical experience.[1] Although various classical theories of humor and laughter may be found, in contemporary academic literature, three theories of humor appear repeatedly: relief theory, superiority theory, and incongruity theory.[2] Among current humor researchers, there is no consensus about which of these three theories of humor is most viable.[2] Proponents of each one originally claimed their theory to be capable of explaining all cases of humor;[2][3] However, they now acknowledge that although each theory generally covers its own area of focus, many instances of humor can be explained by more than one theory.[2][3][4][5] Incongruity and superiority theories, for instance, seem to describe complementary mechanisms which together create humor.[6]						Relief theory maintains that laughter is a homeostatic mechanism by which psychological tension is reduced.[2][3][7] Humor may thus for example serve to facilitate relief of the tension caused by one's fears.[8] Laughter and mirth, according to relief theory, result from this release of nervous energy.[2] Humor, according to relief theory, is used mainly to overcome sociocultural inhibitions and reveal suppressed desires. It is believed that this is the reason we laugh whilst being tickled, due to a buildup of tension as the tickler "strikes".[2][9] According to Herbert Spencer, laughter is an "economical phenomenon" whose function is to release "psychic energy" that had been wrongly mobilized by incorrect or false expectations. The latter point of view was supported also by Sigmund Freud.		The superiority theory of humor traces back to Plato and Aristotle, and Thomas Hobbes' Leviathan. The general idea is that a person laughs about misfortunes of others (so called schadenfreude), because these misfortunes assert the person's superiority on the background of shortcomings of others.[10] Socrates was reported by Plato as saying that the ridiculous was characterized by a display of self-ignorance.[11] For Aristotle, we laugh at inferior or ugly individuals, because we feel a joy at feeling superior to them.[12]		The incongruity theory states that humor is perceived at the moment of realization of incongruity between a concept involved in a certain situation and the real objects thought to be in some relation to the concept.[10]		Since the main point of the theory is not the incongruity per se, but its realization and resolution (i.e., putting the objects in question into the real relation), it is often called the incongruity-resolution theory.[10]		Francis Hutcheson expressed in Thoughts on Laughter (1725) what became a key concept in the evolving theory of the comic: laughter as a response to the perception of incongruity.[13] Arthur Schopenhauer wrote that the perceived incongruity is between a concept and the real object it represents. Hegel shared almost exactly the same view, but saw the concept as an "appearance" and believed that laughter then totally negates that appearance.		The first formulation of the incongruity theory is attributed to the Scottish poet Beattie.[14]		The most famous version of the incongruity theory, however, is that of Kant, who claimed that the comic is "the sudden transformation of a strained expectation into nothing."[15] Henri Bergson attempted to perfect incongruity by reducing it to the "living" and "mechanical".[16]		An incongruity like Bergson's, in things juxtaposed simultaneously, is still in vogue. This is often debated against theories of the shifts in perspectives in humor; hence, the debate in the series Humor Research between John Morreall and Robert Latta.[17] Morreall presented mostly simultaneous juxtapositions,[18] with Latta focusing on a "cognitive shift" created by the sudden solution to some kind of problem.		Humor frequently contains an unexpected, often sudden, shift in perspective, which gets assimilated by the Incongruity Theory. This view has been defended by Latta (1998) and by Brian Boyd (2004).[19] Boyd views the shift as from seriousness to play. Nearly anything can be the object of this perspective twist; it is, however, in the areas of human creativity (science and art being the varieties) that the shift results from "structure mapping" (termed "bisociation" by Koestler) to create novel meanings.[20] Arthur Koestler argues that humor results when two different frames of reference are set up and a collision is engineered between them.		The Script-based Semantic Theory of Humor (SSTH) was introduced by Victor Raskin in "Semantic Mechanisms of Humor", published 1985.[21] While being a variant on the more general concepts of the Incongruity theory of humor (see above), it is the first theory to identify its approach as exclusively linguistic. As such it concerns itself only with verbal humor: written and spoken words used in narrative or riddle jokes concluding with a punch line.		The linguistic scripts (a.k.a. frames) referenced in the title include, for any given word, a "large chunk of semantic information surrounding the word and evoked by it [...] a cognitive structure internalized by the native speaker".[22] These scripts extend much further than the lexical definition of a word; they contain the speaker's complete knowledge of the concept as it exists in his world. Thus native speakers will have similar but not identical scripts for words they have in common.		To produce the humor of a verbal joke, Raskin posits, the following 2 conditions must be met:		Humor is evoked when a trigger at the end of the joke, the punch line, causes the audience to abruptly shift its understanding from the primary (or more obvious) script to the secondary, opposing script.		As an example Raskin uses the following joke:		For this example, the two scripts contained in the joke are DOCTOR and LOVER; the switch from one to the other is triggered by our understanding of the "whispered" reply of the "young and pretty wife". This reply only makes sense in the script of LOVER, but makes no sense in the script of a bronchial patient going to see the DOCTOR at his (home) office. Raskin expands further on his analysis with more jokes, examining in each how the scripts both overlap and oppose each other in the text.[25]		In order to fulfill the second condition of a joke, Raskin introduces different categories of script opposition. A partial list includes: actual (non-actual), normal (abnormal), possible (impossible), good (bad), life (death), obscene (non-obscene), money (no money), high (low) stature.[26] A complete list of possible script oppositions for jokes is finite and culturally dependent. For example, Soviet political humor does not use the same scripts to be found in Jewish humor.[27] However, for all jokes, in order to generate the humor a connection between the two scripts contained in a given joke must be established. "...one cannot simply juxtapose two incongruous things and call it a joke, but rather one must find a clever way of making them make pseudo-sense together".[28]		The General Theory of Verbal Humor (GTVH) was proposed by Victor Raskin and Salvatore Attardo in the article "Script theory revis(it)ed: joke similarity and joke representation model".[29] It integrated Raskin's ideas of Script Opposition (SO), developed in his Script-based Semantic Theory of Humor [SSTH], into the GTVH as one of six levels of independent Knowledge Resources (KRs).[30][31] These KRs could be used to model individual verbal jokes as well as analyze the degree of similarity or difference between them. The Knowledge Resources proposed in this theory are:[32]		To illustrate their theory, the authors use 7 examples of the light bulb joke, each variant shifted by a single Knowledge Resource.[25] Each one of the KRs, ordered hierarchically above and starting with the Script Opposition, has the ability to "determine the parameters below themselves, and are determined [circumscribed] by those above themselves. 'Determination' is to be intended as limiting or reducing the options available for the instantiation of the parameter; for example, the choice of the SO [script opposition] DUMB/SMART will reduce the options available to the generation in the TA (in North America to Poles, etc.)" [35]		One of the advantages of this theory (GTVH) over Raskin's script-based semantic theory (SSTH) is that through the inclusion of the Narrative Strategy (NS) any and all humorous texts can be categorized. Whereas Raskin's SSTH only deals with jokes, the GTVH considers all humorous text from spontaneous one-liners to funny stories and literature. This theory can also, by identifying how many of the Knowledge Resources are identical for any two humorous pieces, begin to define the degree of similarity between the two.		As to the ordering of the Knowledge Resources, there has been much discussion. Willibald Ruch, a distinguished German psychologist and humor researcher,[36] wanted to test empirically the ordering of the Knowledge Resources, with only partial success.[37][38] Nevertheless, both the listed Knowledge Resources in the GTVH and their relationship to each other has proven to be fertile ground in the further investigation of what exactly makes humor funny.[39]		The Computer Model of a Sense of Humor theory was suggested by Suslov in 1992.[40] Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory. However, an essentially new ingredient, the role of timing, is added to the well-known role of ambiguity. In biological systems, a sense of humor inevitably develops in the course of evolution, because its biological function consists of quickening the transmission of the processed information into consciousness and in a more effective use of brain resources. A realization of this algorithm in neural networks[41] justifies naturally Spencer's hypothesis on the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions.		The theory treats on equal footing the humorous effect created by the linguistic means (verbal humor), as well as created visually (caricature, clown performance) or by tickling. The theory explains the natural differences in susceptibility of people to humor, absence of humorous effect from a trite joke, the role of intonation in telling jokes, nervous laughter, etc. According to this theory, humor has a pure biological origin, while its social functions arose later. This conclusion corresponds to the known fact that monkeys (as pointed out by Charles Darwin) and even rats (as found recently) possess a sense of humor.[42]		A practical realization of this algorithm needs extensive databases, whose creation in the automatic regime was suggested recently.[43]		The Ontic-Epistemic Theory of Humor (OETC) proposed by P. Marteinson (2006) asserts that laughter is a reaction to a cognitive impasse, a momentary epistemological difficulty, in which the subject perceives that Social Being itself suddenly appears no longer to be real in any factual or normative sense. When this occurs material reality, which is always factually true, is the only percept remaining in the mind at such a moment of comic perception. This theory posits, as in Bergson, that human beings accept as real both normative immaterial percepts, such as social identity, and neological factual percepts, but also that the individual subject normally blends the two together in perception in order to live by the assumption they are equally real. The comic results from the perception that they are not. This same result arises in a number of paradigmatic cases: factual reality can be seen to conflict with and disprove social reality, which Marteinson calls Deculturation; alternatively, social reality can appear to contradict other elements of social reality, which he calls "Relativisation". Laughter, according to Marteinson, serves to reset and re-boot the faculty of social perception, which has been rendered non-functional by the comic situation: it anesthetizes the mind with its euphoria, and permits the forgetting of the comic stimulus, as well as the well-known function of communicating the humorous reaction to other members of society.[44]		Evolutionary psychologist Geoffrey Miller contends that, from an evolutionary perspective, humour would have had no survival value to early humans living in the savannas of Africa. He proposes that human characteristics like humor evolved by sexual selection. He argues that humour emerged as an indicator of other traits that were of survival value, such as human intelligence.[45]		In 2011, three researchers, Hurley, Dennett and Adams, published a book that reviews previous theories of humor and many specific jokes. They propose the theory that humor evolved because it strengthens the ability of the brain to find mistakes in active belief structures, that is, to detect mistaken reasoning.[46] This is somewhat consistent with the sexual selection theory, because, as stated above, humor would be a reliable indicator of an important survival trait: the ability to detect mistaken reasoning. However, the three researchers argue that humor is fundamentally important because it is the very mechanism that allows the human brain to excel at practical problem solving. Thus, according to them, humor did have survival value even for early humans, because it enhanced the neural circuitry needed to survive.		Misattribution is one theory of humor that describes an audience's inability to identify exactly why they find a joke to be funny. The formal theory is attributed to Zillmann & Bryant (1980) in their article, "Misattribution Theory of Tendentious Humor", published in Journal of Experimental Social Psychology. They derived the critical concepts of the theory from Sigmund Freud's Wit and Its Relation to the Unconscious (note: from a Freudian perspective, wit is separate from humor), originally published in 1905.		The benign violation theory (BVT) is developed by researchers A. Peter McGraw and Caleb Warren.[47] The BVT integrates seemingly disparate theories of humor to predict that humor occurs when three conditions are satisfied: 1) something threatens one's sense of how the world "ought to be", 2) the threatening situation seems benign, and 3) a person sees both interpretations at the same time.		From an evolutionary perspective, humorous violations likely originated as apparent physical threats, like those present in play fighting and tickling. As humans evolved, the situations that elicit humor likely expanded from physical threats to other violations, including violations of personal dignity (e.g., slapstick, teasing), linguistic norms (e.g., puns, malapropisms), social norms (e.g., strange behaviors, risqué jokes), and even moral norms (e.g., disrespectful behaviors). The BVT suggests that anything that threatens one's sense of how the world "ought to be" will be humorous, so long as the threatening situation also seems benign.		There is also more than one way a violation can seem benign. McGraw and Warren tested three contexts in the domain of moral violations. A violation can seem benign if one norm suggests something is wrong but another salient norm suggests it is acceptable. A violation can also seem benign when one is psychologically distant from the violation or is only weakly committed to the violated norm.		For example, McGraw and Warren find that most consumers were disgusted when they read about a church raffling off a Hummer SUV to recruit new members. However, many consumers were simultaneously amused. Consistent with the BVT, people who attended church were less likely to be amused than people who did not. Churchgoers are more committed to the belief that churches are sacred and, consequently, were less likely to consider the church's behavior benign.		According to George Eman Vaillant's (1977) categorization, humor is level IV defense mechanism: overt expression of ideas and feelings (especially those that are unpleasant to focus on or too terrible to talk about) that gives pleasure to others. Humor, which explores the absurdity inherent in any event, enables someone to "call a spade a spade", while "wit" is a form of displacement (level 3). Wit refers to the serious or distressing in a humorous way, rather than disarming it; the thoughts remain distressing, but they are "skirted round" by witticism.		One must have a sense of humor and a sense of seriousness to distinguish what is supposed to be taken literally or not. An even more keen sense is needed when humor is used to make a serious point.[48][49] Psychologists have studied how humor is intended to be taken as having seriousness, as when court jesters used humor to convey serious information. Conversely, when humor is not intended to be taken seriously, bad taste in humor may cross a line after which it is taken seriously, though not intended.[50]		Tony Veale, who takes a more formalised computational approach than Koestler, has written on the role of metaphor and metonymy in humour,[51][52][53] using inspiration from Koestler as well as from Dedre Gentner's theory of structure-mapping, George Lakoff and Mark Johnson's theory of conceptual metaphor, and Mark Turner and Gilles Fauconnier's theory of conceptual blending.		The O'Shannon model of humor (OMOH) was introduced by Dan O'Shannon in "What Are You Laughing At? A Comprehensive Guide to the Comedic Event", published in 2012.[54] The model integrates all the general branches of comedy into a unified framework. This framework consists of four main sections: context, information, aspects of awareness, and enhancers/inhibitors. Elements of context are in play as reception factors prior to the encounter with comedic information. This information will require a level of cognitive process to interpret, and contain a degree of incongruity (based on predictive likelihood). That degree may be high, or go as low as to be negligible. The information will be seen simultaneously through several aspects of awareness (the comedy’s internal reality, its external role as humor, its effect on its context, effect on other receivers, etc.). Any element from any of these sections may trigger enhancers / inhibitors (feelings of superiority, relief, aggression, identification, shock, etc.) which will have an impact on the receiver’s ultimate response. The various interactions of the model allow for a wide range of comedy; for example, a joke needn’t rely on high levels of incongruity if it triggers feelings of superiority, aggression, relief, or identification. Also, high incongruity humor may trigger a visceral response, while well-constructed word-play with low incongruity might trigger a more appreciative response. Also included in the book: evolutionary theories that account for visceral and social laughter, and the phenomenon of comedic entropy.		This model defines laughter as an acoustic signal to make individuals aware of an unnoticed fall-back to former behaviour patterns. To some extent it unifies superiority and incongruity theory. Ticklishness is also considered to have a defined relation to humor via the development of human bipedalism.[55]		
The pun, also called paronomasia, is a form of word play that exploits multiple meanings of a term, or of similar-sounding words, for an intended humorous or rhetorical effect.[1][2] These ambiguities can arise from the intentional use of homophonic, homographic, metonymic, or figurative language. A pun differs from a malapropism in that a malapropism is an incorrect variation on a correct expression, while a pun involves expressions with multiple correct interpretations. Puns may be regarded as in-jokes or idiomatic constructions, as their usage and meaning are specific to a particular language and its culture.		Puns have a long history in human writing. Sumerian cuneiform and Egyptian hieroglyphs were originally based on punning systems, and the Roman playwright Plautus was famous for his puns and word games.[3][4] Punning has been credited as the fundamental concept behind alphabets, writing, and even human civilization.[3]						Puns can be classified in various ways, including:		The homophonic pun, a common type, uses word pairs which sound alike (homophones) but are not synonymous. Walter Redfern summarized this type with his statement, "To pun is to treat homonyms as synonyms."[5] For example, in George Carlin's phrase "Atheism is a non-prophet institution", the word prophet is put in place of its homophone profit, altering the common phrase "non-profit institution". Similarly, the joke "Question: Why do we still have troops in Germany? Answer: To keep the Russians in Czech" relies on the aural ambiguity of the homophones check and Czech. Often, puns are not strictly homophonic, but play on words of similar, not identical, sound as in the example from the Pinky and the Brain cartoon film series: "I think so, Brain, but if we give peas a chance, won't the lima beans feel left out?" which plays with the similar—but not identical—sound of peas and peace in the anti-war slogan "Give Peace a Chance".[6]		A homographic pun exploits words which are spelled the same (homographs) but possess different meanings and sounds. Because of their nature, they rely on sight more than hearing, contrary to homophonic puns. They are also known as heteronymic puns. Examples in which the punned words typically exist in two different parts of speech often rely on unusual sentence construction, as in the anecdote: "When asked to explain his large number of children, the pig answered simply: 'The wild oats of my sow gave us many piglets.'" An example that combines homophonic and homographic punning is Douglas Adams's line "You can tune a guitar, but you can't tuna fish. Unless of course, you play bass." The phrase uses the homophonic qualities of tune a and tuna, as well as the homographic pun on bass, in which ambiguity is reached through the identical spellings of /ˈbeɪs/ (a string instrument), and /ˈbæs/ (a kind of fish).		Homonymic puns, another common type, arise from the exploitation of words which are both homographs and homophones. The statement "Being in politics is just like playing golf: you are trapped in one bad lie after another" puns on the two meanings of the word lie as "a deliberate untruth" and as "the position in which something rests". An adaptation of a joke repeated by Isaac Asimov gives us "Did you hear about the little moron who strained himself while running into the screen door?" playing on strained as "to give much effort" and "to filter".[7] A homonymic pun may also be polysemic, in which the words must be homonymic and also possess related meanings, a condition that is often subjective. However, lexicographers define polysemes as listed under a single dictionary lemma (a unique numbered meaning) while homonyms are treated in separate lemmata.		A compound pun is a statement that contains two or more puns. In this case, the word play cannot go into effect by utilizing the separate words or phrases of the puns that make up the entire statement. For example, a complex statement by Richard Whately includes four puns: "Why can a man never starve in the Great Desert? Because he can eat the sand which is there. But what brought the sandwiches there? Why, Noah sent Ham, and his descendants mustered and bred."[8] This pun uses sand which is there/sandwiches there, Ham/ham, mustered/mustard, and bred/bread. Similarly, the phrase "piano is not my forte" links two meanings of the words forte and piano, one for the dynamic markings in music and the second for the literal meaning of the sentence, as well as alluding to "pianoforte", the older name of the instrument. Compound puns may also combine two phrases that share a word. For example, "Where do mathematicians go on weekends? To a Möbius strip club!" puns on Möbius strip and strip club.		A recursive pun is one in which the second aspect of a pun relies on the understanding of an element in the first. For example, the statement "π is only half a pie." (π radians is 180 degrees, or half a circle, and a pie is a complete circle). Another example is "Infinity is not in finity", which means infinity is not in finite range. Another example is "a Freudian slip is when you say one thing but mean your mother."[9] The recursive pun "Immanuel doesn't pun, he Kant," is attributed to Oscar Wilde.[10]		Visual puns are sometimes used in logos, emblems, insignia, and other graphic symbols, in which one or more of the pun aspects is replaced by a picture. In European heraldry, this technique is called canting arms. Visual and other puns and word games are also common in Dutch gable stones as well as in some cartoons, such as Lost Consonants and The Far Side. Another type of visual pun exists in languages which use non-phonetic writing. For example, in Chinese, a pun may be based on a similarity in shape of the written character, despite a complete lack of phonetic similarity in the words punned upon.[11] Mark Elvin describes how this "peculiarly Chinese form of visual punning involved comparing written characters to objects."[12]		Richard J. Alexander notes two additional forms which puns may take: graphological (sometimes called visual) puns, such as concrete poetry; and morphological puns, such as portmanteaux.[13]		Puns are a common source of humour in jokes and comedy shows. They are often used in the punch line of a joke, where they typically give a humorous meaning to a rather perplexing story. These are also known as feghoots. The following example comes from the movie Master and Commander: The Far Side of the World, though the punchline stems from far older Vaudeville roots.[14] The final line puns on the stock phrase "the lesser of two evils".		Puns often are used in the titles of comedic parodies. A parody of a popular song, movie, etc., may be given a title that hints at the title of the work being parodied, substituting some of the words with ones that sound or look similar. For example, collegiate a cappella groups are often named after musical puns to attract fans through attempts at humor. Such a title can immediately communicate both that what follows is a parody and also which work is about to be parodied, making any further "setup" (introductory explanation) unnecessary.		2014 saw the inaugural UK Pun Championships, at the Leicester Comedy Festival, hosted by Lee Nelson. The winner was Darren Walsh. The competition included the line "My computer's got a Miley Virus. It's stopped twerking."[15] Walsh went on to take part in the O. Henry Pun-Off World Championships in Austin, Texas.[16] In 2015 the UK Pun Champion was Leo Kearse.[17]		Sometimes called "books never written" or "world's greatest books", these are jokes which consist of fictitious book titles with authors' names that contain a pun relating to the title.[18] Perhaps the best known example is: "Tragedy on the Cliff by Eileen Dover", which according to one source was devised by humourist Peter DeVries.[19] It is common for these puns to reference taboo subject matter, such as "What Boys Love by E. Norma Stitts".[18]		Non-humorous puns were and are a standard poetic device in English literature. Puns and other forms of word play have been used by many famous writers, such as Alexander Pope,[20] James Joyce,[21] Vladimir Nabokov,[22] Robert Bloch,[23] Lewis Carroll,[24] John Donne,[25] and William Shakespeare, who is estimated to have used over 3,000 puns in his plays.[26]		Here is an example from Shakespeare's Richard III:		Shakespeare was also noted for his frequent play with less serious puns, the "quibbles" of the sort that made Samuel Johnson complain, "A quibble is to Shakespeare what luminous vapours are to the traveller! He follows it to all adventures; it is sure to lead him out of his way, sure to engulf him in the mire. It has some malignant power over his mind, and its fascinations are irresistible."[27] Elsewhere, Johnson disparagingly referred to punning as the lowest form of humour.[28]		In the poem A Hymn to God the Father, John Donne, married to Anne More, reportedly puns repeatedly: "Son/sun" in the second quoted line, and two compound puns on "Donne/done" and "More/more". All three are homophonic, with the puns on "more" being both homographic and capitonymic. The ambiguities serve to introduce several possible meanings into the verses.		Alfred Hitchcock stated "Puns are the highest form of literature."[29]		Puns can function as a rhetorical device, where the pun serves as a persuasive instrument for an author or speaker. Although puns are sometimes perceived as trite or silly, if used responsibly a pun "…can be an effective communication tool in a variety of situations and forms".[30] A major difficulty in using puns in this manner is that the meaning of a pun can be interpreted very differently according to the audience’s background and can significantly subtract from a message.[31]		Like other forms of wordplay, paronomasia is occasionally used for its attention-getting or mnemonic qualities, making it common in titles and the names of places, characters, and organizations, and in advertising and slogans.[32][33]		Many restaurant and shop names use puns: Cane & Able mobility healthcare, Sam & Ella's Chicken Palace, Tiecoon tie shop, Planet of the Grapes wine and spirits,[34] Curl Up and Dye hair salon, as do books such as Pies and Prejudice, comics (YU+ME: dream) and films (Good Will Hunting). The Japanese anime Speed Racer's original title, Mach GoGoGo! refers to the English word itself, the Japanese word for five (the Mach 5's car number), and the name of the show's main character, Go Mifune. This is also an example of a multilingual pun, full understanding of which requires knowledge of more than one language on the part of the listener.		Names of fictional characters also often carry puns, such as Ash Ketchum and Goku ("Kakarrot"), the protagonists of the video game series Pokémon and the manga series Dragon Ball, respectively, both franchises which are known for including second meanings in the names of many of their characters. A recurring motif in the Austin Powers films repeatedly puns on names which suggest male genitalia. In the science fiction television series Star Trek, "B-4" is used as the name of one of four androids models constructed "before" the android Data, a main character.		The parallel sequel The Lion King 1½ advertised with the phrase "You haven't seen the 1/2 of it!". Wyborowa Vodka employed the slogan "Enjoyed for centuries straight", while Northern Telecom used "Technology the world calls on."[32]		On 1 June 2015 the BBC Radio 4 You and Yours included a feature on "Puntastic Shop Titles". Entries included a Chinese Takeaway in Ayr town centre called "Ayr’s Wok", a kebab shop in Ireland called "Abra Kebabra" and a tree-surgeon in Dudley called ‘’Special Branch." The winning competition entry, selected by Lee Nelson, was a dry cleaner's in Fulham and Chelsea called "Starchy and Starchy".[35]		Paronomasia has found a strong foothold in the media. William Safire of the New York Times suggests that "the root of this pace-growing [use of paranomasia] is often a headliner-writer’s need for quick catchiness, and has resulted in a new tolerance for a long-despised form of humor."[36] It can be argued that paronomasia is common in the media, especially headlines, to draw the reader’s interest. The rhetoric is important because it connects people with the topic. A notable example is the New York Post headline "Headless Body in Topless Bar." [37]		Paronomasia is prevalent orally as well. Salvatore Attardo believes that puns are verbal humor. He talks about Pepicello and Weisberg's linguistic theory of humor and believes the only form of linguistic humor is limited to puns.[38] This is because a pun is play on the word itself. Attardo believes that only puns are able to maintain humor and this humor has significance. It is able to help soften a situation and make it less serious, it can help make something more memorable, and using a pun can make the speaker seem witty.		Paronomasia is strong in print media and oral conversation so it can be assumed that paronomasia is strong in broadcast media as well. Examples of paranomasia in media are sound bites. They could be memorable because of the humor and rhetoric associated with paronomasia, thus making the significance of the sound bite stronger.		There exist subtle differences between paronomasia and other literary techniques, such as the double entendre. While puns are often simple wordplay for comedic or rhetorical effect, a double entendre alludes to a second meaning which is not contained within the statement or phrase itself, often one which purposefully disguises the second meaning. As both exploit the use of intentional double meanings, puns can sometimes be double entendres, and vice versa. Puns also bear similarities with paraprosdokian, syllepsis, and eggcorns. In addition, homographic puns are sometimes compared to the stylistic device antanaclasis, and homophonic puns to polyptoton. Puns can be used as a type of mnemonic device to enhance comprehension in an educational setting. Used discreetly, puns can effectively reinforce content and aid in the retention of material. Some linguists have encouraged the creation of neologisms to decrease the instances of confusions caused by puns.[39]		Puns were found in ancient Egypt, where they were heavily used in development of myths and interpretation of dreams.[40]		In China, Shen Dao (ca. 300 BC) used "shi", meaning "power", and "shi", meaning "position" to say that a king has power because of his position as king.[41]		In ancient Mesopotamia, about 2500 BC, punning was used by scribes to represent words in cuneiform.[42]		The Maya are known for having used puns in their hieroglyphic writing, and for using them in their modern languages.[43]		In Japan, "graphomania" was one type of pun.[44]		In Tamil, "Sledai" is the word used to mean pun in which a word with two different meanings. This is also classified as a poetry style in ancient Tamil literature. Similarly, in Telugu language, "Slesha" is the equivalent word and is among one of the several poetry styles in Telugu literature.		
Dave is a British television channel owned by UKTV, which is available in the United Kingdom and Ireland. The channel is available on cable, IPTV, Freeview and some satellite platforms. The channel took the name Dave in October 2007, but it had been on air under various identities and formats since October 1998.						UK Gold Classics, UKTV's first digital-only channel, was launched on 2 October 1998 and was only broadcast from Friday to Sunday on Sky Digital from 6.00pm to 2.00am. Around this time UK Gold began to move towards newer programmes instead of older ones; the 'classics' line-up included a number of early shows, including some black-and-white programmes, which had been acquired in the early years of the UK Gold service. They also showed some recent shows from the main channel, but the main part of the channel was older shows from the early years of UK Gold. On weekdays, the channel was off air, showing a still caption of all the UKTV channels and start-up times.		The 'Classics' format lasted just six months; the channel ended on 28 March 1999, and from 2 April 1999 the channel was renamed to UK Gold 2, and screened morning programmes from UK Gold time-shifted to the evening of the same day instead of classic shows.		The UK Gold subsidiary channel was again relaunched with a completely new programme line-up and renamed UKG² on 12 November 2003. The channel was promoted as being an edgier alternative to UK Gold; like that channel, the output was mainly comedy from the BBC with some shows produced inhouse. A fair amount is similar to the comedy output of UK Play/Play UK before that channel's closure; however, unlike Play, the channel did not include music videos.		Along with the rest of the UKTV network, the "UK" prefix was changed to "UKTV" on 8 March 2004 and therefore the channel name changed to UKTV G2.		Initially, the channel broadcast in the evenings only, but during the 'G2' era the decision was made to expand hours into the daytime; to expand the programming line-up, comedy was joined by popular-factual and magazine shows which were already running on UKTV People (then Blighty, now Drama) such as Top Gear and Airport.		On 7 October 2005, it was announced that they would show sports programming. This new line-up was called UKTV Sport and included a new show by the same name. UKTV Sport also had its own logo and DOG. There was talk that this could lead to a channel but it never happened.[1]		In February 2006, they picked up the rights to show highlights of the RBS Six Nations rugby union championship, with a highlights show broadcast on the evening of the games previously shown live on the BBC. On 16 March 2006, they announced a deal to air extensive coverage of the 2006 FIFA World Cup as a sub licensing of the BBC's rights to the tournament.[2] UKTV G2 simulcast the BBC's live matches, including the opening match between Germany and Costa Rica, England's game with Paraguay and the final. The channel also showed highlights of every match in the tournament.		In April 2006, the channel acquired the rights to the quarter-finals of Euroleague Basketball [3] and in August, UKTV G2 also picked up rights to the 2006 FIBA World Championship. [4], forming the programme 'UKTV Slam'.		In September 2007, UKTV announced that they would relaunch and rename UKTV G2 as Dave on 15 October.[5] UKTV said the name of the channel was chosen because "everyone knows a bloke called Dave".[6] The rebrand included the channel being available free-to-air on digital terrestrial platform, Freeview, replacing UKTV Bright Ideas which only averaged 0.1% of the audience share.[7] The move to Freeview saw Dave launch in the bandwidth previously used by UKTV History which was moved to the time limited (7.00am to 6.00pm) bandwidth once occupied by UKTV Bright Ideas. Dave is available daily, from 7.00am to 3.00am, on all platforms. It uses the tagline "the home of witty comedy banter" and uses Ralph Ineson as an announcer, along with David Flynn, Phill Jupitus, Iain Lee and BBC Radio 1 DJ Greg James.		To ensure that all Freeview viewers receive the channel on number 19, UKTV briefly placed a re-tuning notice on the programme's information. This later changed to the current location on Channel 12.		From 31 January 2008, the channel began broadcasting in widescreen, along with the other UKTV channels.[8]		In April 2009, they aired 3 new instalments of Red Dwarf, entitled Back to Earth. This marked the channel's first foray into scripted original programming. During the airing of the Red Dwarf mini-series, the Dave DOG in the top left corner of the screen had the word 'Lister' added after it in the same font after the show's lead character; during the show it is even suggested that the station is named after him. Back to Earth brought record breaking viewing figures, not just in the context of the channel's past, but for digital television in general.[9]		In June 2009, the logo was updated to incorporate the 'circle' logo branding of all the new UKTV channels (for example Home, GOLD and Really). At the same time, the voice of Dave became Nigel Grover, aka Scott Saunders, who had previously worked at a number of local radio stations. On the 29 April 2014, the 'circle' logo was removed and the original 2007 logo was restored.		On 29 July 2011, UKTV announced that it had secured a deal with BSkyB to launch three more high-definition channels on Sky.[10] As part of Virgin Media's deal to sell its share of UKTV, all five of UKTV's HD channels would also be added to Virgin's cable television service by 2012.[11] Dave HD launched on 10 October 2011 on Sky and Virgin Media,[12] two days before Watch HD, while Alibi HD launched in July 2012. All three channels are HD simulcasts of the standard-definition channel. Dave HD along with Good Food HD and Eden HD launched on BT TV on 3 October 2016. Six weeks after Watch HD and Alibi HD launched on BT TV.		A one-hour timeshifted service of the channel – then known as UKTV G2 – began to operate on 1 November 2004, under the name UKTV G2 +1. As UKTV G2 at the time was an evenings-only service, the timeshift also operated in the evenings only, using the satellite and cable capacity which, during daytime, was used by the now-defunct UKTV Bright Ideas. The sharing arrangement meant that when UKTV G2's hours extended into daytime, the timeshift remained evenings-only.		At the time of the Dave relaunch, UKTV Bright Ideas closed, freeing up the space to allow UKTV G2 +1 to expand its hours to follow those of the parent channel fully; due to the main channel's relaunch as Dave, UKTV G2 +1 became Dave +1.		The timeshift was initially available on the Virgin Media and Sky platforms; from 22 January 2009, following UKTV's acquisition of a further Freeview broadcast slot, Dave +1 was made available on the digital terrestrial platform.		On 24 February 2009, Dave +1 was renamed Dave ja vu (a play on the phrase déjà vu) on all platforms; this was carried out to "strengthen the brand's positioning as the home of witty banter"[13] according to UKTV bosses.		On 14 June 2011, UKTV announced that Really would launch on Freeview on 2 August 2011,[14] to facilitate this Dave ja vu's broadcast hours on the platform were reduced from 8.00am-4.00am to 2.00am-4.00am. On 22 November 2012, UKTV confirmed that it had secured a deal for another 24-hour DTT slot and would use it for Dave ja vu until it firmed up permanent plans for the slot.[15] Dave ja vu began to broadcast its full schedule on the platform again from 3 January 2013. Drama permanently took the slot from 8 July 2013,[16] however Dave ja vu continued to broadcast between 2.00am-5.00am.[17]		On 20 November 2014, Dave ja vu returned to 24-hour broadcasting.[18]		Within just one month of its launch, Dave had become the tenth largest television channel in the UK. The broadcaster puts daily averages at around 3 million viewers, although, much of the growth may be attributed to its presence on Freeview; nonetheless, it is performing significantly better in pay TV homes than UKTV G2 ever did. Over the month since its launch, Dave averaged a 1.32% share in multichannel homes and a 3.2% share in the 16–34 male demographic.[19]		Dave's positive reception is proven by an attraction of 4 million viewers throughout 18 November 2007 for its coverage of "Car of the Year", pushing it to second place in multichannel behind ITV2.[19]		Some of Dave's more popular shows, such as Mock the Week, Top Gear and Have I Got News For You, often mockingly reference the channel with phrases such as "...happened last week. Unless you're watching the repeat on Dave in which case it happened 3 years ago."[citation needed]		The shows with the highest ratings are Mock the Week (over 420,000 viewers), QI (over 400,000), Top Gear (350,000) and Dragons' Den (about 300,000).		The first episode of Red Dwarf: Back to Earth attracted 2,060,000 viewers on the first viewing,[20] though over 4 million viewed the episode at some point over its debut weekend.[9] The highest rating original commission before this had been Red Bull X-Fighters (about 185,000).		On 6 January 2016, UKTV announced that Dave would show its first-ever live sporting event with a boxing match between David Haye and Mark de Mori at the O2 Arena on 16 January 2016 produced by Salter Brothers Entertainment.[21]		In late May 2016 Dave broadcast full live coverage of the 2016 BDO World Trophy darts tournament.		In July 2016 Dave covered cricket's Caribbean Premier League. It broadcast five matches live, including the final and showed the other games in full on a delayed basis.		In early 2016, Dave aired western films during the daytime, including One More Train to Rob, Chief Crazy Horse, Buffalo Bill and the Indians, or Sitting Bull's History Lesson, The Great Northfield Minnesota Raid and The Hired Hand. Dave also show films every Friday and Saturday night, with Friday being a repeat of the film that was shown on a Saturday.		In August 2011, Dave launched a regular comedy podcast called The Dave Weekly hosted on joindave.co.uk and accessible via iTunes.[22] Presented by Ben Shires, the podcast comprises interviews with comedians such as Russell Kane, Jo Brand, Adam Buxton, Paul Foot and Alex Horne along with occasional features.		
"Why did the chicken cross the road?" is a common riddle joke. The answer or punch line is: "To get to the other side." It is an example of anti-humor, in that the curious setup of the joke leads the listener to expect a traditional punchline, but they are instead given a simple statement of fact. "Why did the chicken cross the road?" has become iconic as an exemplary generic joke to which most people know the answer, and has been repeated and changed numerous times over the course of history.						The riddle appeared in an 1847 edition of The Knickerbocker, a New York City monthly magazine:[1]		There are ‘quips and quillets’ which seem actual conundrums, but yet are none. Of such is this: ‘Why does a chicken cross the street?[’] Are you ‘out of town?’ Do you ‘give it up?’ Well, then: ‘Because it wants to get on the other side!’		The joke had become widespread by the 1890s,[citation needed] when a pun variant version appeared in the magazine Potter's American Monthly:[2]		Why should not a chicken cross the road? It would be a fowl proceeding.		There are many riddles that assume a familiarity with this well-known riddle and its answer, for example by supplying a different answer, such as "it was too far to walk around".[3] One class of variations enlists a creature other than the chicken to cross the road, in order to refer back to the original riddle. For example, a turkey or duck crosses "because it was the chicken's day off," and a dinosaur "because chickens didn't exist yet." Some variants are both puns and references to the original, such as "Why did the duck cross the road?" "To prove he's no chicken".		Other variations replace side with another word often to form a pun. Some examples are "Why did the chicken cross the playground? To get to the other slide" or "Why did the whale cross the ocean? To get to the other tide." A mathematical version asks, "Why did the chicken cross the Möbius strip?" "To get to the same side." Alternatively, the punchline can be regarded as the chicken "getting to the other side" as a euphemism for death, and crossing the road being its method of suicide.		Another class of variations, designed for written rather than oral transmission, employs parody by pretending to have notable individuals or institutions give characteristic answers to the question posed by the riddle.[4] As with the lightbulb joke, variants on these themes are widespread.		Additionally, anyone who raises chickens knows that chickens don't usually cross the road. Chickens are homebodies - they are afraid: they are 'chicken'. Chickens stick to familiar territory as long as there is food... so the straight answer would be 'because it was desperately hungry'.		
Collective intelligence Collective action Self-organized criticality Herd mentality Phase transition Agent-based modelling Synchronization Ant colony optimization Particle swarm optimization		Social network analysis Small-world networks Community identification Centrality Motifs Graph Theory Scaling Robustness Systems biology Dynamic networks		Evolutionary computation Genetic algorithms Genetic programming Artificial life Machine learning Evolutionary developmental biology Artificial intelligence Evolutionary robotics		Reaction-diffusion systems Partial differential equations Dissipative structures Percolation Cellular automata Spatial ecology Self-replication Spatial evolutionary biology		Operationalization Feedback Self-reference Goal-oriented System dynamics Sensemaking Entropy Cybernetics Autopoiesis Information theory Computation theory		Ordinary differential equations Iterative maps Phase space Attractors Stability analysis Population dynamics Chaos Multistability Bifurcation		Rational choice theory Bounded rationality Irrational behaviour		Artificial intelligence (AI) is intelligence exhibited by machines. In computer science, the field of AI research defines itself as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of success at some goal.[1] Colloquially, the term "artificial intelligence" is applied when a machine mimics "cognitive" functions that humans associate with other human minds, such as "learning" and "problem solving".[2]		As machines become increasingly capable, mental facilities once thought to require intelligence are removed from the definition. For instance, optical character recognition is no longer perceived as an example of "artificial intelligence", having become a routine technology.[3] Capabilities currently classified as AI include successfully understanding human speech,[4] competing at a high level in strategic game systems (such as chess and Go[5]), autonomous cars, intelligent routing in content delivery networks, military simulations, and interpreting complex data. In recent years, chatbot with artificial intelligence is also considered to be used for education, e.g. construction safety.[6]		AI research is divided into subfields[7] that focus on specific problems, approaches, the use of a particular tool, or towards satisfying particular applications.		The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects.[8] General intelligence is among the field's long-term goals.[9] Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy, neuroscience, artificial psychology and many others.		The field was founded on the claim that human intelligence "can be so precisely described that a machine can be made to simulate it".[10] This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity.[11] Some people also consider AI a danger to humanity if it progresses unabatedly.[12] Attempts to create artificial intelligence have experienced many setbacks, including the ALPAC report of 1966, the abandonment of perceptrons in 1970, the Lighthill Report of 1973, the second AI winter 1987–1993 and the collapse of the Lisp machine market in 1987.		In the twenty-first century, AI techniques, both hard (using a symbolic approach) and soft (sub-symbolic), have experienced a resurgence following concurrent advances in computer power, sizes of training sets, and theoretical understanding, and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.[13] Recent advancements in AI, and specifically in machine learning, have contributed to the growth of Autonomous Things such as drones and self-driving cars, becoming the main driver of innovation in the automotive industry.		While thought-capable artificial beings appeared as storytelling devices in antiquity,[14] the idea of actually trying to build a machine to perform useful reasoning may have begun with Ramon Llull (c. 1300 CE). With his Calculus ratiocinator, Gottfried Leibniz extended the concept of the calculating machine (Wilhelm Schickard engineered the first one around 1623), intending to perform operations on concepts rather than numbers.[15] Since the 19th century, artificial beings are common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. (Rossum's Universal Robots).[16]		The study of mechanical or "formal" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis.[17][page needed] Along with concurrent discoveries in neurology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain.[18] The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete "artificial neurons".[15]		The field of AI research was born at a workshop at Dartmouth College in 1956.[19] Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research.[20] They and their students produced programs that the press described as "astonishing":[21] computers were winning at checkers, solving word problems in algebra, proving logical theorems and speaking English.[22] By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[23] and laboratories had been established around the world.[24] AI's founders were optimistic about the future: Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do". Marvin Minsky agreed, writing, "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved".[25]		They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[26] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an "AI winter",[27] a period when obtaining funding for AI projects was difficult.		In the early 1980s, AI research was revived by the commercial success of expert systems,[28] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research.[29] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.[30]		In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas.[13] The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields and a commitment by researchers to mathematical methods and scientific standards.[31] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.[32]		Advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception.[33] By the mid 2010s, machine learning applications were used throughout the world.[34] In a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.[35] The Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research[36] as do intelligent personal assistants in smartphones.[37] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[5][38] In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie,[39] who at the time continuously held the world No. 1 ranking for two years[40][41]		According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a "sporadic usage" in 2012 to more than 2,700 projects. Clark also presents factual data indicating that error rates in image processing tasks have fallen significantly since 2011.[42] He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people.[42]		The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[8]		Erik Sandwell emphasizes planning and learning that is relevant and applicable to the given situation.[43]		Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[44] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[45]		For difficult problems, algorithms can require enormous computational resources—most experience a "combinatorial explosion": the amount of memory or computer time required becomes astronomical for problems of a certain size. The search for more efficient problem-solving algorithms is a high priority.[46]		Human beings ordinarily use fast, intuitive judgments rather than step-by-step deduction that early AI research was able to model.[47] AI has progressed using "sub-symbolic" problem solving: embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning; neural net research attempts to simulate the structures inside the brain that give rise to this skill; statistical approaches to AI mimic the human ability to guess.		Knowledge representation and knowledge engineering are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of "what exists" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language.[48] The most general ontologies are called upper ontologies, which act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos.[49].		Among the most difficult problems in knowledge representation are:		Intelligent agents must be able to set goals and achieve them.[56] They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or "value") of available choices.[57]		In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[58] However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that cannot only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.[59]		Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[60]		Machine learning, a fundamental concept of AI research since the field's inception,[61] is the study of computer algorithms that improve automatically through experience.[62][63]		Unsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning[64] the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.[65]		Within developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).[66][67][68][69]		Natural language processing[70] gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering[71] and machine translation.[72]		A common method of processing and extracting meaning from natural language is through semantic indexing. Although these indexes require a large volume of user input, it is expected that increases in processor speeds and decreases in data storage costs will result in greater efficiency.		Machine perception[73] is the ability to use input from sensors (such as cameras, microphones, tactile sensors, sonar and others) to deduce aspects of the world. Computer vision[74] is the ability to analyze visual input. A few selected subproblems are speech recognition,[75] facial recognition and object recognition.[76]		The field of robotics[77] is closely related to AI. Intelligence is required for robots to handle tasks such as object manipulation[78] and navigation, with sub-problems such as localization, mapping, and motion planning. These systems require that an agent is able to: Be spatially cognizant of its surroundings, learn from and build a map of its environment, figure out how to get from one point in space to another, and execute that movement (which often involves compliant motion, a process where movement requires maintaining physical contact with an object).[79][80]		Affective computing is the study and development of systems that can recognize, interpret, process, and simulate human affects.[82][83] It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science.[84] While the origins of the field may be traced as far back as the early philosophical inquiries into emotion,[85] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[86] on "affective computing".[87][88] A motivation for the research is the ability to simulate empathy, where the machine would be able to interpret human emotions and adapts its behavior to give an appropriate response to those emotions.		Emotion and social skills[89] are important to an intelligent agent for two reasons. First, being able to predict the actions of others by understanding their motives and emotional states allow an agent to make better decisions. Concepts such as game theory, decision theory, necessitate that an agent be able to detect and model human emotions. Second, in an effort to facilitate human-computer interaction, an intelligent machine may want to display emotions (even if it does not experience those emotions itself) to appear more sensitive to the emotional dynamics of human interaction.		A sub-field of AI addresses creativity both theoretically (the philosophical psychological perspective) and practically (the specific implementation of systems that generate novel and useful outputs).		Many researchers think that their work will eventually be incorporated into a machine with artificial general intelligence, combining all the skills mentioned above and even exceeding human ability in most or all these areas.[9][90] A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.[91][92]		Many of the problems above also require that general intelligence be solved. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered "AI-complete", but all of these problems need to be solved simultaneously in order to reach human-level machine performance.		There is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues.[93] A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?[94] Can intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems?[95] Can intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require "sub-symbolic" processing?[96] John Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as synthetic intelligence,[97] a term which has since been adopted by some non-GOFAI researchers.[98][99]		Stuart Shapiro divides AI research into three approaches, which he calls computational psychology, computational philosophy, and computer science. Computational psychology is used to make computer programs that mimic human behavior.[100] Computational philosophy, is used to develop an adaptive, free-flowing computer mind.[100] Implementing computer science serves the goal of creating computers that can perform tasks that only people could previously accomplish.[100] Together, the humanesque behavior, mind, and actions make up artificial intelligence.		In the 1940s and 1950s, a number of researchers explored the connection between neurology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England.[18] By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.		When access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and each one developed its own style of research. John Haugeland named these approaches to AI "good old fashioned AI" or "GOFAI".[101] During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on cybernetics or neural networks were abandoned or pushed into the background.[102] Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.		Economist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.[103][104]		Unlike Newell and Simon, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem solving, regardless of whether people used the same algorithms.[94] His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.[105] Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.[106]		Researchers at MIT (such as Marvin Minsky and Seymour Papert)[107] found that solving difficult problems in vision and natural language processing required ad-hoc solutions – they argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their "anti-logic" approaches as "scruffy" (as opposed to the "neat" paradigms at CMU and Stanford).[95] Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of "scruffy" AI, since they must be built by hand, one complicated concept at a time.[108]		When computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications.[109] This "knowledge revolution" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software.[28] The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.		By the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into "sub-symbolic" approaches to specific AI problems.[96] Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.		This includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive.[110] Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.		Interest in neural networks and "connectionism" was revived by David Rumelhart and others in the middle of 1980s.[111] Neural networks are an example of soft computing --- they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.[112]		In the 1990s, AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable, and they have been responsible for many of AI's recent successes. The shared mathematical language has also permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Stuart Russell and Peter Norvig describe this movement as nothing less than a "revolution" and "the victory of the neats".[31] Critics argue that these techniques (with few exceptions[113]) are too focused on particular problems and have failed to address the long-term goal of general intelligence.[114] There is an ongoing debate about the relevance and validity of statistical approaches in AI, exemplified in part by exchanges between Peter Norvig and Noam Chomsky.[115][116]		In the course of 60+ years of research, AI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.		Many problems in AI can be solved in theory by intelligently searching through many possible solutions:[120] Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[121] Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[122] Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[78] Many learning algorithms use search algorithms based on optimization.		Simple exhaustive searches[123] are rarely sufficient for most real world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use "heuristics" or "rules of thumb" that eliminate choices that are unlikely to lead to the goal (called "pruning the search tree"). Heuristics supply the program with a "best guess" for the path on which the solution lies.[124] Heuristics limit the search for solutions into a smaller sample size.[79]		A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.[125]		Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Forms of evolutionary computation include swarm intelligence algorithms (such as ant colony or particle swarm optimization)[126] and evolutionary algorithms (such as genetic algorithms, gene expression programming, and genetic programming).[127]		Logic[128] is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[129] and inductive logic programming is a method for learning.[130]		Several different forms of logic are used in AI research. Propositional or sentential logic[131] is the logic of statements which can be true or false. First-order logic[132] also allows the use of quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy logic,[133] is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). Fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. Subjective logic[134] models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief + disbelief + uncertainty = 1 within a Beta distribution. By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence.		Default logics, non-monotonic logics and circumscription[51] are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics;[135] situation calculus, event calculus and fluent calculus (for representing events and time);[136] causal calculus;[137] belief calculus;[138] and modal logics.[139]		Many problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.[140]		Bayesian networks[141] are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm),[142] learning (using the expectation-maximization algorithm),[143] planning (using decision networks)[144] and perception (using dynamic Bayesian networks).[145] Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[145]		A key concept from the science of economics is "utility": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[146] and information value theory.[57] These tools include models such as Markov decision processes,[147] dynamic decision networks,[145] game theory and mechanism design.[148]		The simplest AI applications can be divided into two types: classifiers ("if shiny then diamond") and controllers ("if shiny then pick up"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[149]		A classifier can be trained in various ways; there are many statistical and machine learning approaches. The most widely used classifiers are the neural network,[150] kernel methods such as the support vector machine,[151] k-nearest neighbor algorithm,[152] Gaussian mixture model,[153] naive Bayes classifier,[154] and decision tree.[155] The performance of these classifiers have been compared over a wide range of tasks. Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems; this is also referred to as the "no free lunch" theorem. Determining a suitable classifier for a given problem is still more an art than science.[156]		The study of non-learning artificial neural networks[150] began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others.		The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[157] Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning, GMDH or competitive learning.[158]		Today, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa,[159][160] and was introduced to neural networks by Paul Werbos.[161][162][163]		Hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.[164]		Deep learning in artificial neural networks with many layers has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.[165][166][167]		According to a survey,[168] the expression "Deep Learning" was introduced to the Machine Learning community by Rina Dechter in 1986[169] and gained traction after Igor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000.[170] The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965.[171][page needed] These networks are trained one layer at a time. Ivakhnenko's 1971 paper[172] describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning.[173] Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[174]		Deep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980.[175] In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.[176] Since 2011, fast implementations of CNNs on GPUs have won many visual pattern recognition competitions.[167]		Deep feedforward neural networks were used in conjunction with reinforcement learning by AlphaGo, Google Deepmind's program that was the first to beat a professional human Go player.[177]		Early on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs)[178] which are general computers and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence.[167] RNNs can be trained by gradient descent[179][180][181] but suffer from the vanishing gradient problem.[165][182] In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.[183]		Numerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997.[184] LSTM is often trained by Connectionist Temporal Classification (CTC).[185] At Google, Microsoft and Baidu this approach has revolutionised speech recognition.[186][187][188] For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users.[189] Google also used LSTM to improve machine translation,[190] Language Modeling[191] and Multilingual Language Processing.[192] LSTM combined with CNNs also improved automatic image captioning[193] and a plethora of other applications.		Control theory, the grandchild of cybernetics, has many important applications, especially in robotics.[194]		AI researchers have developed several specialized languages for AI research, including Lisp[195] and Prolog.[196]		In 1950, Alan Turing proposed a general procedure to test the intelligence of an agent now known as the Turing test. This procedure allows almost all the major problems of artificial intelligence to be tested. However, it is a very difficult challenge and at present all agents fail.[197]		Artificial intelligence can also be evaluated on specific problems such as small problems in chemistry, hand-writing recognition and game-playing. Such tests have been termed subject matter expert Turing tests. Smaller problems provide more achievable goals and there are an ever-increasing number of positive results.[198]		For example, performance at draughts (i.e. checkers) is optimal,[199] performance at chess is high-human and nearing super-human (see computer chess: computers versus human) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human.		A quite different approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence. Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from Kolmogorov complexity and data compression.[200] Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.		A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.[201]		AI is relevant to any intellectual task.[202] Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[203]		High-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, prediction of judicial decisions[204] and targeting online advertisements.[202][205][206]		With social media sites overtaking TV as a source for news for young people and news organisations increasingly reliant on social media platforms for generating distribution,[207] major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.[208]		There are a number of competitions and prizes to promote research in artificial intelligence. The main areas promoted are: general machine intelligence, conversational behavior, data-mining, robotic cars, robot soccer and games.		Artificial intelligence is breaking into the healthcare industry by assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer.[209]  There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are way too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called "Hanover". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers.[210] Another study is using artificial intelligence to try and monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions.[211]		According to CNN, there was a recent study by surgeons at the Children's National Medical Center in Washington which successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed.[212]		Advancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016, there are over 30 companies utilizing AI into the creation of driverless cars. A few companies involved with AI include Tesla, Google, and Apple.[213]		Many components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers are integrated into one complex vehicle.[214]		One main factor that influences the ability for a driver-less car to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings.[215] Some self-driving cars are not equipped with steering wheels or brakes, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.[216]		Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation.		Use of AI in banking can be tracked back to 1987 when Security Pacific National Bank in USA set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Apps like Kasisito and Moneystream are using AI in financial services		Banks use artificial intelligence systems to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place.[217] In August 2001, robots beat humans in a simulated financial trading competition.[218]		AI has also reduced fraud and crime by monitoring behavioral patterns of users for any changes or anomalies.[219]		Artificial intelligence is used to generate intelligent behaviors primarily in non-player characters (NPCs), often simulating human-like intelligence.[220]		A platform (or "computing platform") is defined as "some sort of hardware architecture or software framework (including application frameworks), that allows software to run". As Rodney Brooks pointed out many years ago,[221] it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation.		A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep-learning frameworks to robot platforms such as the Roomba with open interface.[222] Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.		Collective AI is a platform architecture that combines individual AI into a collective entity, in order to achieve global results from individual behaviors.[223][224] With its collective structure, developers can crowdsource information and extend the functionality of existing AI domains on the platform for their own use, as well as continue to create and share new domains and capabilities for the wider community and greater good.[225] As developers continue to contribute, the overall platform grows more intelligent and is able to perform more requests, providing a scalable model for greater communal benefit.[224] Organizations like SoundHound Inc. and the Harvard John A. Paulson School of Engineering and Applied Sciences have used this collaborative AI model.[226][224]		Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit partnership to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence.[227] They stated: "This partnership on AI will conduct research, organize discussions, provide thought leadership, consult with relevant third parties, respond to questions from the public and media, and create educational material that advance the understanding of AI technologies including machine perception, learning, and automated reasoning."[227] Apple joined other tech companies as a founding member of the Partnership on AI in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.[228][224]		There are three philosophical questions related to AI:		Can a machine be intelligent? Can it "think"?		Widespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to be how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.[238]		Machines with intelligence have the potential to use their intelligence to make ethical decisions. Research in this area includes "machine ethics", "artificial moral agents", and the study of "malevolent vs. friendly AI".		The development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.		A common concern about the development of artificial intelligence is the potential threat it could pose to mankind. This concern has recently gained attention after mentions by celebrities including Stephen Hawking, Bill Gates,[240] and Elon Musk.[241] A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1billion to OpenAI a nonprofit company aimed at championing responsible AI development.[242] The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.[243]		In his book Superintelligence, Nick Bostrom provides an argument that artificial intelligence will pose a threat to mankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not reflect humanity's - one example is an AI told to compute as many digits of pi as possible - it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.		For this danger to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching.[244][245] Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.[246]		Concern over risk from artificial intelligence has led to some high-profile donations and investments. In January 2015, Elon Musk donated ten million dollars to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to "grow wisdom with which we manage" the growing power of technology. Musk also funds companies developing artificial intelligence such as Google DeepMind and Vicarious to "just keep an eye on what's going on with artificial intelligence.[247] I think there is potentially a dangerous outcome there."[248][249]		Development of militarized artificial intelligence is a related concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers.[250]		Joseph Weizenbaum wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy[251] was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.[252]		Martin Ford, author of The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future,[253] and others argue that specialized artificial intelligence applications, robotics and other forms of automation will ultimately result in significant unemployment as machines begin to match and exceed the capability of workers to perform most routine and repetitive jobs. Ford predicts that many knowledge-based occupations—and in particular entry level jobs—will be increasingly susceptible to automation via expert systems, machine learning[254] and other AI-enhanced applications. AI-based applications may also be used to amplify the capabilities of low-wage offshore workers, making it more feasible to outsource knowledge work.[255][page needed]		This raises the issue of how ethically the machine should behave towards both humans and other AI agents. This issue was addressed by Wendell Wallach in his book titled Moral Machines in which he introduced the concept of artificial moral agents (AMA).[256] For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as "Does Humanity Want Computers Making Moral Decisions"[257] and "Can (Ro)bots Really Be Moral".[258] For Wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of AMAs.[259]		The field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making.[260] The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: "Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems—it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics."[261] Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition "Machine Ethics"[260] that stems from the AAAI Fall 2005 Symposium on Machine Ethics.[261]		Political scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent.[262] He argues that "any sufficiently advanced benevolence may be indistinguishable from malevolence." Humans should not assume machines or robots would treat us favorably, because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of mankind, and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.		Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could "spell the end of the human race".[263]		One proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI', and will then be able to control subsequently developed AIs. Some question whether this kind of check could really remain in place.		Leading AI researcher Rodney Brooks writes, "I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence."[264]		If an AI system replicates all key aspects of human intelligence, will that system also be sentient – will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.		Computationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing.[265] Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.		The philosophical position that John Searle has named "strong AI" states: "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."[266] Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the "mind" might be.[267]		Mary Shelley's Frankenstein considers a key issue in the ethics of artificial intelligence: if a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? The idea also appears in modern science fiction, such as the film A.I.: Artificial Intelligence, in which humanoid machines have the ability to feel emotions. This issue, now known as "robot rights", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature.[268] Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights.[269] The subject is profoundly discussed in the 2010 documentary film Plug & Pray.[270]		Are there limits to how intelligent machines – or human-machine hybrids – can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. ‘’Superintelligence’’ may also refer to the form or degree of intelligence possessed by such an agent.[90]		If research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[271] The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario "singularity".[272] Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[272][90]		Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.[272]		You awake one morning to find your brain has another lobe functioning. Invisible, this auxiliary lobe answers your questions with information beyond the realm of your own memory, suggests plausible courses of action, and asks questions that help bring out relevant facts. You quickly come to rely on the new lobe so much that you stop wondering how it works. You just use it. This is the dream of artificial intelligence.		Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either.[274] This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger, has been illustrated in fiction as well, for example in the manga Ghost in the Shell and the science-fiction series Dune.		In the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later "the Gynoids" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.		Edward Fredkin argues that "artificial intelligence is the next stage in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" (1863), and expanded upon by George Dyson in his book of the same name in 1998.[275]		Thought-capable artificial beings have appeared as storytelling devices since antiquity.[14]		The implications of a constructed machine exhibiting artificial intelligence have been a persistent theme in science fiction since the twentieth century. Early stories typically revolved around intelligent robots. The word "robot" itself was coined by Karel Čapek in his 1921 play R.U.R., the title standing for "Rossum's Universal Robots". Later, the SF writer Isaac Asimov developed the Three Laws of Robotics which he subsequently explored in a long series of robot stories. Asimov's laws are often brought up during layman discussions of machine ethics;[276] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[277]		The novel Do Androids Dream of Electric Sheep?, by Philip K. Dick, tells a science fiction story about Androids and humans clashing in a futuristic world. Elements of artificial intelligence include the empathy box, mood organ, and the androids themselves. Throughout the novel, Dick portrays the idea that human subjectivity is altered by technology created with artificial intelligence.[278]		Nowadays AI is firmly rooted in popular culture; intelligent robots appear in innumerable works. HAL, the murderous computer in charge of the spaceship in 2001: A Space Odyssey (1968), is an example of the common "robotic rampage" archetype in science fiction movies. The Terminator (1984) and The Matrix (1999) provide additional widely familiar examples. In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[279]		Cite error: A list-defined reference named "Knowledge_representation" is not used in the content (see the help page). Cite error: A list-defined reference named "Knowledge_engineering" is not used in the content (see the help page). Cite error: A list-defined reference named "Ontology" is not used in the content (see the help page).		See also: Logic machines in fiction and List of fictional computers		
Toilet humour, or scatological humour, is a type of off-colour humour dealing with defecation, urination and flatulence, and to a lesser extent vomiting and other body functions. It sees substantial crossover with sexual humour, such as dick jokes.		Toilet humour is popular among a wide range of ages, but is especially popular with children and teenagers, for whom cultural taboos related to acknowledgement of waste excretion still have a degree of novelty. The humour comes from the rejection of such taboos, and is a part of modern culture.[1] Examples can also be found in earlier literature, including The Canterbury Tales by Geoffrey Chaucer.						Toilet humour is common in television shows such as Beavis and Butt-Head, The Grim Adventures of Billy & Mandy, The Ren & Stimpy Show, Rugrats, Rocko's Modern Life, Aaahh!!! Real Monsters, Ed, Edd n Eddy, South Park, Family Guy, The Simpsons, Futurama, Rick and Morty, Adventure Time, My Little Pony: Friendship Is Magic, Regular Show, American Dad!, The Fairly OddParents, SpongeBob SquarePants and Kenny vs. Spenny.[2]		Toilet humour is also found in song and rhyme, particularly schoolboy songs. Examples of this are found in Mozart and scatology, and variants of the German folk schoolboys' song known as the Scheiße-Lied (English: "Shit-Song")[3][4] which is indexed in the German Volksliederarchiv.[5]		
Motif is a word used by folklorists who analyze, interpret, and describe the traditional elements found in the lore of particular folk groups and compare the folklore of various regions and cultures of the world based on these motif patterns. Ultimately, folklorists identify motifs in folklore to interpret where, how, and why these motifs are used, so they can understand the values, customs, and ways of life of unique cultures.		In cultural anthropology and folkloristics, the meaning of motif encompasses the meanings of motif used in the areas of music, literary criticism, visual arts, and textile arts because folklorists study motifs (i.e., recurring elements) in each of these areas, motifs that create recognizable patterns in folklore and folk-art traditions.						Folklorists also use motif to refer to the recognizable and consistently repeated story elements (e.g., common characters, objects, actions, and events) that are used in the traditional plot structures, or tale types, of many stories and folktales. These motifs, which Dr. Margaret Read Macdonald calls “each small part of a tale,”[1] were indexed in 1932 by Stith Thompson and published as the Motif-Index of Folk-Literature.[1]		Thompson built upon the research of Antti Aarne (and the tale type index he created) when he compiled, classified, and numbered the traditional motifs of the mostly European folktale types in Aarne’s index and then cross referenced those motifs with Aarne’s tale types (Dundes).[2] Folklorist Alan Dundes explains that Stith Thompson’s “six-volume Motif-Index of Folk-Literature and the Aarne-Thompson tale type index constitute two of the most valuable tools in the professional folklorist's arsenal of aids for analysis”.[2]		Below is a sample of the index's headings.[3]		In the book The Folktale, folklorist Stith Thompson explains how motifs and tale types are interrelated as he describes the role of one type of character found in many folk narratives, the helper:		The same is true for any character or other motif. In fact, Thompson also explains that a single motif may be found in numerous folktales “from all parts of the earth” (383).[4]		Thompson’s explanations show that the sequential order of motifs is also an important factor for folklorists to consider as they interpret individual motifs found in various folktale types used internationally.		Many of the source texts in Thompson's Motif-Index are no longer currently in print. Dr. Margaret Read McDonald's Storytellers Sourcebook[1] refers readers to stories in current books that also use motifs of folk literature. For example, Disney’s Cinderella contains many of the same traditional motifs that Read MacDonald points out in her preface to The Storyteller’s Sourcebook (e.g., Glass slipper, Cruel Stepmother, and Three-fold flight from ball) (x).[1]		Many folklorists have produced extensive motif and tale-type indices for culture areas not covered by Thompson, or covered only to a limited extent.[5] The following are some important works:		According to the Oxford English Dictionary (OED), folkloristic use of motif is not summed up in the definition for literary criticism (“Motif,” def. 3a), but deserves its own separate sense of this definition (“Motif,” def. 3b).[6] Similarly, the compound word motif index is used in cultural anthropology to denote “an index of standard motifs, esp. those found in folk tales” (OED, “Motif Index,” def. C2).[7]		
The history of humor on the Internet begins together with the Internet itself. Initially internet and its precursors, LANs and WANs, were used merely as another media to disseminate jokes and other kinds of humor, in addition to the traditional ones ("word of mouth", printed media, sound recording, radio, film, and TV).[1] In lockstep with the progress of electronic communication technologies, jokers took advantage of the ARPANET,[2] e-mail,[3] Usenet newsgroups (e.g., rec.humor and alt.humor), bulletin board systems,..., and finally the Whole World Wide Web. Gradually, new forms of humor evolved, based on the new possibilities delivered by electronic means of communication.[1]						Internet made an impact on humor in several important ways.		Similarly to other technical innovations (from printing to TV), Internet significantly increased the speed and the extent of the propagation of humor over the world.[4] The joke is a commonly transmitted type of internet meme. It is well-known that orally-transmitted jokes and other kind of folklore undergo evolution and mutations. Internet speeds up and globalizes these processes.[5]		A FAQ of rec.humor gave the following tongue-in-cheek description how jokes propagated in the era of newsgroups:[6]		On the opposite side, unlike previous technical means, internet as a whole eliminates censorship ans self-censorship of humor. For example, before the Internet extremely sick jokes, such as dead baby jokes, were almost exclusively spread orally.[4]		Internet blurred the lines between written and spoken in terms of language use and the directness of speech, between what is permitted in private and in public. Also, the YouTube blurred the distinction between a spoken and recorded joke, in that the narrator is actually present.[4]		Limor and Lemish observe that internet humor is a part of the participatory culture, where the "consumers" of jokes may reciprocate by generating and transmitting humor, i.e., act as "producers" and "distributors".[7]		New possibilities provided by electronic means of communication gave rise to new types of humor. An early example of these is humorous ASCII art. While the precursor of the ASCII art, the "typewrite art", has been known since 19th century, [8][9] it was available to few. Whereas ASCII art, including silly one, has become ubiquitous in sig blocks in discussion boards and e-mails. One may find quite a few silly examples in the Jargon File, which also mentions subgenres of ASCII art humor: puns on the letter/character names (e.g., if read "B" as "bee" and the caret character (^) as "carrot", the one may create an ASCII art rebus for a "bee in a carrot patch") and pictures of "silly cows" .[10]		The ability to easily manipulate with images and videos combined with ease of the dissemination of them via the Internet introduced new forms of graphical humor, such as lolcats, demotivators, and funny animations.		
Marta Kutas (born September 2, 1949) is a Professor and Chair of cognitive science and an adjunct professor of neuroscience at the University of California, San Diego. She also directs the Center for Research in Language at UCSD. Kutas is known for discovering the N400, an event-related potential (ERP) component typically elicited by unexpected linguistic stimuli, with her colleague Steven Hillyard in one of the first studies in what is now the field of neurolinguistics. Kutas received a B.A. in 1971 from Oberlin College and a Ph.D. in 1977 from the University of Illinois, Urbana-Champaign, and she completed a postdoctoral fellowship at the University of California, San Diego in 1980. She then accepted a position as a research neuroscientist in the Department of Neurosciences at UCSD, and she has been a member of the Department of Cognitive Science at UCSD since its founding in 1988.		Every year, Kutas teaches a year-long course for the second year Cognitive Science Ph.D. students. The intention of the course is to develop the "soft skills" necessary for becoming a successful scientist. Kutas has been teaching this class for several years, but has said that the group of second year students taking the course this year have made the 2017 version "one of the best".		
A practical joke, or prank, is a mischievous trick played on someone, generally causing the victim to experience embarrassment, perplexity, confusion, or discomfort.[1][2] A person who performs a practical joke is called a "practical joker".[1] Other terms for practical jokes include gag, jape, or shenanigan.		Practical jokes differ from confidence tricks or hoaxes in that the victim finds out, or is let in on the joke, rather than being talked into handing over money or other valuables. Practical jokes are generally lighthearted and without lasting impact; their purpose is to make the victim feel humbled or foolish, but not victimized or humiliated. In this fashion, most practical jokes are affectionate gestures of humour and designed to encourage laughter. However, practical jokes performed with cruelty can constitute bullying, whose intent is to harass or exclude rather than reinforce social bonds through ritual humbling.[3]		In Western culture, April Fools' Day is a day traditionally dedicated to conducting practical jokes.[4]						A practical joke is "practical" because it consists of someone doing something physical, in contrast to a verbal or written joke. For example, the joker who is setting up and conducting the practical joke might hang a bucket of water above a doorway and rig the bucket using pulleys so when the door opens the bucket dumps the water. The joker would then wait for the victim to walk through the doorway and be drenched by the bucket of water. Objects can also be used in practical jokes, like fake vomit, chewing gum bugs, exploding cigars, stink bombs, costumes and whoopee cushions.		Practical jokes often occur inside offices, usually to surprise co-workers. Covering the computer accessories with Jell-O, wrapping the desk with Christmas paper or aluminium foil or filling it with balloons are just some examples of office pranks.[5] Practical jokes are also common occurrences during sleepovers, whereby teens will play pranks on their friends as they come into the home, enter a room or even as they sleep.[6]		American humorist H. Allen Smith wrote a 320-page book in 1953 called The Compleat Practical Joker (ISBN 0-688-03705-4) that contains numerous examples of practical jokes. The book became a best seller not only in the United States but also in Japan.[7] Moira Marsh has written an entire volume about practical jokes.[2] One of her findings is that in the USA they are more often done by males than females.		A practical joke recalled as his favorite by the playwright Charles MacArthur, concerns the American painter and bohemian character Waldo Peirce. While living in Paris in the 1920s, Peirce "made a gift of a very big turtle to the woman who was the concierge of his building". The woman doted on the turtle and lavished care on it. A few days later Peirce substituted a somewhat larger turtle for the original one. This continued for some time, with larger and larger turtles being surreptitiously introduced into the woman's apartment. The concierge was beside herself with happiness and displayed her miraculous turtle to the entire neighborhood. Peirce then began to sneak in and replace the turtle with smaller and smaller ones, to her bewildered distress.[8] This was the storyline behind Esio Trot, by Roald Dahl.		Modern and successful pranks often take advantage of the modernization of tools and techniques. In Canada, engineering students have a reputation for annual pranks; at the University of British Columbia these usually involve leaving a Volkswagen beetle in an unexpected location (such as suspended from the Golden Gate Bridge[9] and the Lions Gate Bridge[10]). A similar prank was undertaken by engineering students at Cambridge University, England, where an Austin 7 car was put on top of the Senate House building.[11] Pranks can also adapt to the political context of the era.[12] Students at the Massachusetts Institute of Technology (MIT) are particularly known for their "hacks".[13]		Not unlike the Stone Louse of Germany, in the American West the jackalope has become an institutionalized practical joke perennially perpetrated by ruralites (as a class) on tourists, most of whom have never heard of the decades-old myth.[14]		The 2003 TV movie Windy City Heat, consists of an elaborate practical joke on the film's star, Perry Caravallo, who is led to believe that he is starring in a faux action film, Windy City Heat, where the filming which is ostensibly for the film's DVD extras actually documents the long chain of pranks and jokes performed at Caravallo's expense.[15]		Films featuring practical jokes include:		Genres: reality television (RT), comedy television program (CT), situation comedy (SC), comedy festival (CF)		USA		Canada		Ireland		Finland		Ukraine		Some people have developed reputations as practical jokers in addition to other work; others have made pranking their primary work. Many practical jokers are comedians or entertainers, while others engage in pranks connected to social activism or to protest movements.		
An ethnic joke is a remark attempting humor relating to an ethnic, racial or cultural group, often referring to an ethnic stereotype of the group in question for its punchline.		Perceptions of ethnic jokes are ambivalent. Many find them racist and offensive. On the other hand, jokes poking fun an one's own ethnicity are sometimes considered acceptable but Christie Davis challenges this notion of there being an innocuous ethnic joke.[1][2] Davis maintains that ethnic jokes reinforce ethnic stereotypes and sometimes lead to calls for violence.[3] The perceived damage to the ethnic group can be of great concern as when the ethnic Polish jokes became so common in the 1970s the Polish Ministry of Foreign Affairs approached the U.S. State Department to complain.[4]						The predominant and most widely known theory of ethnic humor attempts to discover social regularities in the anecdote traditions of different countries by contextually describing jokes. Professor Christie Davies, author of this theory, has posed the main arguments in his article Ethnic Jokes, Moral Values and Social Boundaries, published in 1982. His approach is based on Victor Raskin's (1985) Semantic Script Theory of Humor, or to be more precise, on the arguments connected with ethnic humor on binary oppositions. While Raskin merely describes the main binary oppositions providing examples (mostly from the Jewish humor), Davies explores the situations where the scripts apply; for example, he has discovered that the most common opposition, stupid/clever, is applied under particular circumstances in the social reality of two ethnic groups concerned.		Davies in his monograph published in 1990 has surmised that "Jokes in every country (or reasonably homogeneous cultural and linguistic domain) have certain targets for stupidity jokes – people who dwell on the edge of that nation or domain and who are perceived as culturally ambiguous by the dominant people of the center. In addition, they will likely be rustic people or immigrants in search of unskilled and low-prestige manual work. They are to a great extent similar to the joke-tellers themselves, share the same cultural background or even speak a similar or identical language." According to Davies, ethnic jokes are centered on the three main themes of stupidity, canniness and sexual behavior.		Davies is featured in the 2010 documentary film, Polack, exploring the source of the Polish joke.[5]		L Perry Curtis in examining ethnic humour aimed at the Irish in Victorian England describes the descent that the ethnic joke and the accompanying stereotype can undergo as the target that they are aimed at descends into depictions of violent behaviour: "My curiosity of 'Paddy's' transformation in comic art from a rather primitive, rustic, or simple-minded peasant to a degenerate man...bent on murder or outrage."[6]		Nevertheless, as it is explained by Samuel Schmidt, the ethnic jokes can also be a form of social resistance, and so they are addressed against those who are historically seen as the aggressors, like the multiple jokes published in Mexico about the Americans (also called gringos there).[7]		
The lexical definition of a term, also known as the dictionary definition, is the meaning of the term in common usage. As its other name implies, this is the sort of definition one is likely to find in the dictionary. A lexical definition is usually the type expected from a request for definition, and it is generally expected that such a definition will be stated as simply as possible in order to convey information to the widest audience.		Note that a lexical definition is descriptive, reporting actual usage within speakers of a language, and changes with changing usage of the term, rather than prescriptive, which would be to stick with a version regarded as "correct", regardless of drift in accepted meaning. They tend to be inclusive, attempting to capture everything the term is used to refer to, and as such are often too vague for many purposes.		When the breadth or vagueness of a lexical definition is unacceptable, a precising definition or a stipulative definition is often used.		Words can be classified as lexical or nonlexical. Lexical words are those that have independent meaning (such as a Noun (N), verb (V), adjective (A), adverb (Adv), or preposition (P).		The definition which reports the meaning of a word or a phrase as it is actually used by people is called a lexical definition. Meanings of words given in a dictionary are lexical definitions. As a word may have more than one meaning, it may also have more than one lexical definition.		Lexical definitions are either true or false. If the definition is the same as the actual use of the word then it is true, otherwise it is false.		
This page lists publications in humor research, with brief annotations. The list includes books, scholarly journals that regularly cover articles in humor research, as well as some seminal, frequently cited journal articles about humor.		This list is not intended for humorous books and joke collections that do not have any scholarly analysis of humor.						
The Facetiae is an anthology of jokes by Poggio Bracciolini (1380–1459), first published in 1470. The collection, "the most famous jokebook of the Renaissance",[1] is notable for its inclusion of scatological jokes and tales, six of the tales involving farting and six involving defecation.						Early editions of the Facetiae are rare, and they are not yet described in an organized fashion as is common for incunabula. It was, evidently, very popular: an 1894 bibliography lists twenty editions from the fifteenth century, and states that the oldest is printed by Georgius Lauer in Rome and is known as Hain 13179 (a quarto with 110 leaves). The second oldest is called Reichling 1919 (100 leaves). The 100-leaf edition, despite having been described elsewhere as the first printing, is now generally held to be later than the 110-leaf edition, which is traditionally thought to be the editio princeps; both were printed in Rome in 1470/1471. An edition from 1473–1476, Hain 13182, was printed somewhere in Poland. Christophorus Valdarfer in Venice likewise printed an edition (with 76 leaves) in 1470/1471, and Andreas Belfortis in Ferrara printed one dated 1471. According to Lotte Hellinga, the Venice edition by Valdarfer is probably older than Lauer's edition printed in Rome, and most likely served as its exemplar.[2]		There are surviving manuscript copies from the fifteenth century.[3]		The last tale on farting involves a wife and her husband. The wife, observing a ram copulating with a sheep, asks how the ram chooses his mate, to which the husband answers that the ram chooses the sheep that farts. He confirms to her that humans work the same way, after which she farts, and they have sex; she farts again, with the same result. When she farts a third time, the husband says, "I'm not making love to you again, even if you shit out your soul."[4]		
Antti Amatus Aarne (December 5, 1867 Pori – February 2, 1925 Helsinki) was a Finnish folklorist.		Antti was a student of Kaarle Krohn, the son of the folklorist Julius Krohn. He further developed their historic-geographic method of comparative folkloristics, and developed the initial version of what became the Aarne–Thompson classification system of classifying folktales, first published in 1910 and extended by Stith Thompson first in 1927 and again in 1961.		Early in 1925, Aarne died in Helsinki (Finland) where he had been a lecturer at the University since 1911 and where he had held a position as Professor extraordinarius since 1922.				
Folklore is expressive body of culture shared by a particular group of people; it encompasses the traditions common to that culture, subculture or group. These include oral traditions such as tales, proverbs and jokes. They include material culture, ranging from traditional building styles to handmade toys common to the group. Folklore also includes customary lore, the forms and rituals of celebrations such as Christmas and weddings, folk dances and initiation rites. Each one of these, either singly or in combination, is considered a folklore artifact. Just as essential as the form, folklore also encompasses the transmission of these artifacts from one region to another or from one generation to the next. For folklore is not taught in a formal school curriculum or studied in the fine arts. Instead these traditions are passed along informally from one individual to another either through verbal instruction or demonstration. The academic study of folklore is called folkloristics.						To fully understand folklore, it is helpful to clarify its component parts: the terms folk and lore. It is well-documented that the term was coined in 1846 by the Englishman William Thoms. He fabricated it to replace the contemporary terminology of "popular antiquities" or "popular literature". The second half of the compound word, lore, proves easier to define as its meaning has stayed relatively stable over the last two centuries. Coming from Old English lār 'instruction,' and with German and Dutch cognates, it is the knowledge and traditions of a particular group, frequently passed along by word of mouth.[1]		The concept of folk proves somewhat more elusive. When Thoms first created this term, folk applied only to rural, frequently poor and illiterate peasants. A more modern definition of folk is a social group which includes two or more persons with common traits, who express their shared identity through distinctive traditions. "Folk is a flexible concept which can refer to a nation as in American folklore or to a single family."[2] This expanded social definition of folk supports a broader view of the material, i.e. the lore, considered to be folklore artifacts. These now include all "things people make with words (verbal lore), things they make with their hands (material lore), and things they make with their actions (customary lore)".[3] Folklore is no longer circumscribed as being chronologically old or obsolete. The folklorist studies the traditional artifacts of a social group and how they are transmitted.		Transmission is a vital part of the folklore process. Without communicating these beliefs and customs within the group over space and time, they would become cultural shards relegated to cultural archaeologists. For folklore is also a verb. These folk artifacts continue to be passed along informally, as a rule anonymously and always in multiple variants. The folk group is not individualistic, it is community-based and nurtures its lore in community. "As new groups emerge, new folklore is created… surfers, motorcyclists, computer programmers".[4] In direct contrast to high culture, where any single work of a named artist is protected by copyright law, folklore is a function of shared identity within the social group.[5]		Having identified folk artifacts, the professional folklorist strives to understand the significance of these beliefs, customs and objects for the group. For these cultural units[6] would not be passed along unless they had some continued relevance within the group. That meaning can however shift and morph. So Halloween of the 21st century is not the All Hallows' Eve of the Middle Ages, and even gives rise to its own set of urban legends independent of the historical celebration. The cleansing rituals of Orthodox Judaism were originally good public health in a land with little water; now these customs signify identification as an Orthodox Jew. Compare this to brushing your teeth, also transmitted within a group, which remains a practical hygiene and health issue and does not rise to the level of a group-defining tradition.[7] For tradition is initially remembered behavior. Once it loses its practical purpose, there is no reason for further transmission unless it has been imbued with meaning beyond the initial practicality of the action. This meaning is at the core of folkloristics, the study of folklore.		With an increasingly theoretical sophistication of the social sciences, it has become evident that folklore is a naturally occurring and necessary component of any social group, it is indeed all around us.[8] It does not have to be old or antiquated. It continues to be created, transmitted and in any group is used to differentiate between "us" and "them".		Folklore began to distinguish itself as an autonomous discipline during the period of romantic nationalism in Europe. A particular figure in this development was Johann Gottfried von Herder, whose writings in the 1770s presented oral traditions as organic processes grounded in locale. After the German states were invaded by Napoleonic France, Herder's approach was adopted by many of his fellow Germans who systematized the recorded folk traditions and used them in their process of nation building. This process was enthusiastically embraced by smaller nations like Finland, Estonia, and Hungary, which were seeking political independence from their dominant neighbours.[9]		Folklore as a field of study further developed among 19th century European scholars who were contrasting tradition with the newly developing modernity. Its focus was the oral folklore of the rural peasant populations, which were considered as residue and survivals of the past that continued to exist within the lower strata of society.[10] The "Kinder- und Hausmärchen" of the Brothers Grimm (first published 1812) is the best known but by no means only collection of verbal folklore of the European peasantry of that time. This interest in stories, sayings and songs continued throughout the 19th century and aligned the fledgling discipline of folkloristics with literature and mythology. By the turn into the 20th century the number and sophistication of folklore studies and folklorists had grown both in Europe and North America. Whereas European folklorists remained focused on the oral folklore of the homogenous peasant populations in their regions, the American folklorists, led by Franz Boas and Ruth Benedict, chose to consider Native American cultures in their research, and included the totality of their customs and beliefs as folklore. This distinction aligned American folkloristics with cultural anthropology and ethnology, using the same techniques of data collection in their field research. This divided alliance of folkloristics between the humanities in Europe and the social sciences in America offers a wealth of theoretical vantage points and research tools to the field of folkloristics as a whole, even as it continues to be a point of discussion within the field itself.[11]		The term Folkloristics, along with its synonym Folklore Studies,[note 1] gained currency in the 1950s to distinguish the academic study of traditional culture from the folklore artifacts themselves. With the passage in 1976 of the American Folklife Preservation Act, (P.L. 94-201),[12] passed by the U.S. Congress in conjunction with the Bicentennial Celebration in 1976, folkloristics in the United States came of age.		"…[Folklife] means the traditional expressive culture shared within the various groups in the United States: familial, ethnic, occupational, religious, regional; expressive culture includes a wide range of creative and symbolic forms such as custom, belief, technical skill, language, literature, art, architecture, music, play, dance, drama, ritual, pageantry, handicraft; these expressions are mainly learned orally, by imitation, or in performance, and are generally maintained without benefit of formal instruction or institutional direction."		Added to the panoply of other legislation designed to protect the natural and cultural heritage of the United States, this law also marks a shift in national awareness. It gives voice to a growing understanding that cultural diversity is a national strength and a resource worthy of protection. Paradoxically, it is a unifying feature, not something that separates the citizens of a country. "We no longer view cultural difference as a problem to be solved, but as a tremendous opportunity. In the diversity of American folklife we find a marketplace teeming with the exchange of traditional forms and cultural ideas, a rich resource for Americans".[13] This diversity is celebrated annually at the Smithsonian Folklife Festival and many other folklife fests around the country.		The folk of the 19th century, the social group identified in the original term "folklore", was characterized by being rural, non-literate and poor. They were the peasants living in the countryside, in contrast to the urban populace of the cities. Only toward the end of the century did the urban proletariat (on the coattails of Marxist theory) become included with the rural poor as folk. The common feature in this expanded definition of folk was their identification as the underclass of society.[14]		Moving forward into the 20th century, in tandem with new thinking in the social sciences, folklorists also revised and expanded their concept of the folk group. By the 1960s it was understood that social groups, i.e. folk groups, were all around us; each individual is enmeshed in a multitude of differing identities and their concomitant social groups. The first group that each of us is born into is the family, and each family has its own unique folklore. As a child grows into an individual, its identities also increase to include age, language, ethnicity, occupation, etc. Each of these cohorts has its own folklore, and as one folklorist points out, this is "not idle speculation… Decades of fieldwork have demonstrated conclusively that these groups do have their own folklore."[4] In this modern understanding, folklore is a function of shared identity within any social group.[5]		This folklore can include jokes, sayings and expected behavior in multiple variants, always transmitted in an informal manner. For the most part it will be learned by observation, imitation, repetition or correction by other group members. This informal knowledge is used to confirm and re-inforce the identity of the group. It can be used both internally within the group to express their common identity, for example in an initiation ceremony for new members. Or it can be used externally to differentiate the group from outsiders, like a folkdance demonstration at a community festival. Significant to folklorists here is that there are two opposing but equally valid ways to use this in the study of a group: you can start with an identified group in order to explore its folklore, or you can identify folklore items and use them to identify the social group.[15]		Beginning in the 1960s, a further expansion of the concept of folk began to unfold in folkloristics. Individual researchers identified folk groups which had previously been overlooked and ignored. One major example of this is found in an issue of "The Journal of American Folklore", published 1975. This edition is dedicated exclusively to articles on women's folklore, with approaches that were not coming from a man's perspective.[note 2] Other groups that were highlighted as part of this broadened understanding of the folk group were non-traditional families, occupational groups, and families that pursued production of folk items through multiple generations.		Individual folklore artifacts are commonly classified as one of three types: material, verbal or customary lore. For the most part self-explanatory, these categories include physical objects (material folklore), common sayings, expressions, stories and songs (verbal folklore), and beliefs and ways of doing things (customary folklore). There is also a fourth major subgenre defined for children's folklore and games (childlore), as the collection and interpretation of this fertile topic is peculiar to school yards and neighborhood streets.[16] Each of these genres and their subtypes is intended to organize and categorize the folklore artifacts; they provide common vocabulary and consistent labeling for folklorists to communicate with each other.		That said, each artifact is unique; in fact one of the characteristics of all folklore artifacts is their variation within genres and types.[17] This is in direct contrast to manufactured goods, where the goal in production is to create products which are identical, and variations are considered mistakes. It is however just this required variation that makes identification and classification of the defining features a challenge. And while this classification is essential for the subject area of folkloristics, it remains just labeling, and adds little to an understanding of the traditional development and meaning of the artifacts themselves.[18]		Necessary as they are, genre classifications are misleading in their oversimplification of the subject area. Folklore artifacts are never self-contained, they do not stand in isolation but are particulars in the self-representation of a community. Different genres are frequently combined with each other to mark an event.[19] So a birthday celebration might include a song or formulaic way of greeting the birthday child (verbal), presentation of a cake and wrapped presents (material), as well as customs to honor the individual, such as sitting at the head of the table, and blowing out the candles with a wish. There might also be special games played at birthday parties which are not generally played at other times. Adding to the complexity of the interpretation, the birthday party for a seven-year-old will not be identical to the birthday party for that same child as a six-year-old, even though they follow the same model. For each artifact embodies a single variant of a performance in a given time and space. The task of the folklorist becomes to identify within this surfeit of variables the constants and the expressed meaning that shimmer through all variations: honoring of the individual within the circle of family and friends, gifting to express their value and worth to the group, and of course, the festival food and drink as signifiers of the event.		The formal definition of verbal lore is words, both written and oral, which are "spoken, sung, voiced forms of traditional utterance that show repetitive patterns."[20] Crucial here are the repetitive patterns. Verbal lore is not just any conversation, but words and phrases conforming to a traditional configuration recognized by both the speaker and the audience. For narrative types by definition have consistent structure, and follow an existing model in their narrative form.[note 3] As just one simple example, in English the phrase "An elephant walks into a bar…" instantaneously flags the following text as a joke. It might be one you've already heard, but it might be one that the speaker has just thought up within the current context. This is folklore in action. Another example is the child's song Old MacDonald Had a Farm, where each performance is distinctive in the animals named, their order and their sounds. Songs such as this are used to express cultural values (farms are important, farmers are old and weather-beaten) and teach children about different domesticated animals.[21] This is folklore in action.		Verbal folklore was the original folklore, the artifacts defined by William Thoms as older, oral cultural traditions of the rural populace. In his 1846 published call for help in documenting antiquities, Thoms was echoing scholars from across the European continent to collect artifacts of verbal lore. By the beginning of the 20th century these collections had grown to include artifacts from around the world and across several centuries. A system to organize and categorize them became necessary.[22] Antti Aarne published a first classification system for folktales in 1910. This was later expanded into the Aarne–Thompson classification system by Stith Thompson and remains the standard classification system for European folktales and other types of oral literature. As the number of classified oral artifacts grew, similarities were noted in items which had been collected from very different geographic regions, ethnic groups and epochs, giving rise to the Historic-Geographic Method, a methodology which dominated folkloristics in the first half of the 20th century.		When William Thoms first published his appeal to document the verbal lore of the rural populations, it was believed these folk artifacts would die out as the population became literate. Over the past two centuries this belief has proven to be wrong; folklorists continue to collect verbal lore in both written and spoken form from all social groups. Some variants might have been captured in published collections, but much of it is still transmitted orally and indeed continues to be generated in new forms and variants at an alarming rate.		Below is listed a small sampling of types and examples of verbal lore.		The genre of material culture includes all artifacts that you can touch, hold, live in or eat. They are tangible objects, with a physical presence intended for use either permanently or just at next meal. Most of these folklore artifacts are single objects which have been created by hand for a specific purpose. However folk artifacts can also be mass-produced, such as dreidels or Christmas decorations. These items continue to be considered folklore due to their long (pre-industrial) history and their customary use. All of these material objects "existed prior to and continue alongside mechanized industry. … [They are] transmitted across the generations and subject to the same forces of conservative tradition and individual variation"[20] that are found in all folk artifacts. Of interest to folklorists are their physical form, their method of manufacture or construction, their pattern of use as well as the procurement of the raw materials.[23] The meaning to those who both make and use these objects is important. Of primary significance in these studies is the complex balance of continuity over change in both their design and their decoration.		In Europe before the Industrial Revolution everything was made by hand. While some folklorists of the 19th century wanted to secure the oral traditions of the rural folk before the populace became literate, other folklorists sought to identify hand-crafted objects before their production processes were lost to industrial manufacturing. Just as verbal lore continues to be actively created and transmitted in today's culture, so these handicrafts, possibly with a shift in purpose and meaning, can still be found all around us. For there are many reasons to continue to hand make objects for use. It could mean these skills are needed to repair manufactured items. Or perhaps a unique design is wanted which is not (or cannot be) found in the stores. Many crafts are considered to be simple home maintenance, such as cooking, sewing and carpentry. Handicrafts have also become for many an enjoyable and satisfying hobby. Last but not least, handmade objects have taken on the sheen of prestige, where extra time and thought is spent in their creation and their uniqueness is valued.[24] For the folklorist, these hand-crafted objects embody multifaceted relationships in the lives of the craftsmen and the users, which is completely lacking in mass-produced items without connection to an individual craftsman.[25] Regardless of the motivation for the handicraft, this is folklore in action.		Many traditional crafts have been elevated to the fine or applied arts and taught in art schools, such as ironworking and glass-making.[26] Or they are repurposed as folk art, characterized as objects in which the decorative form supersedes its utilitarian needs. Folk art is found in hex signs on Pennsylvania Dutch barns, tin man sculptures made by metalworkers, front yard Christmas displays, decorated school lockers, carved gun stocks, and tattoos. "Words such as naive, self-taught, and individualistic are used to describe these objects, and the exceptional rather than the representative creation is featured."[27] This is in contrast to our understanding of folklore artifacts which are nurtured and passed along in community.[note 4]		Many objects of material folklore, big and small, are challenging to classify, difficult to archive and unwieldy to store. How do we preserve these bulky artifacts of material culture, and how do we use them? That is the assigned task of museums. Toward this goal the concept of the Living history or open-air museum has been developed, beginning in Scandinavia at the end of the 19th century. These museums are here to teach, not just display. Actors show how items were used, reenacting everyday living by people from all segments of society. In order to achieve this, these museums rely heavily on the material artifacts of a pre-industrial society. Many locations even duplicate the processing of the objects, thus creating new objects of an earlier historic time period. These Living history museums are now found throughout the United States and the world as part of a thriving Heritage industry. This is folklore in action.		This list represents just a small sampling of objects and skills which are included in studies of material culture.		Customary culture is remembered enactment, i.e. re-enactment. It is the patterns of expected behavior within a group, the "traditional and expected way of doing things"[28][29] A custom can be a single gesture, such as thumbs down or a handshake. It can also be a complex interaction of multiple folk customs and artifacts as seen in a child's birthday party, including verbal lore (Happy Birthday song), material lore (presents and a birthday cake), special games (Musical chairs) and individual customs (making a wish as you blow out the candles). Each of these is a folklore artifact in its own right, potentially worthy of investigation and cultural analysis. Together they combine to build the custom of a birthday party celebration, a scripted combination of multiple artifacts which have meaning within their social group.		Folklorists divide customs into several different categories.[28] A custom can be a seasonal celebration, such as Thanksgiving or New Year's. It can be a life cycle celebration for an individual, such as baptism, birthday or wedding. A custom can also mark a community festival or event; examples of this are Carnival in Cologne or Mardi Gras in New Orleans. This category also includes the Smithsonian Folklife Festival celebrated each summer on the Mall in Washington, DC. A fourth category includes customs related to folk beliefs. Walking under a ladder is just one of many symbols considered unlucky. Occupational groups tend to have a rich history of customs related to their life and work, so the traditions of sailors or lumberjacks.[note 5] The area of ecclesiastical folklore, which includes modes of worship not sanctioned by the established church[30] tends to be so large and complex that it is usually treated as a specialized area of folk customs; it requires considerable expertise in standard church ritual in order to adequately interpret folk customs and beliefs that originated in official church practice.		Customary folklore is by definition folklore in action; it is always a performance, be it a single gesture or a complex of scripted customs. Participating in the custom, either as performer or audience, signifies acknowledgment of that social group. Some customary behavior is intended to be performed and understood only within the group itself, so the handkerchief code sometimes used in the gay community or the initiation rituals of the Freemasons. Other customs are designed specifically to represent a social group to outsiders, those who do not belong to this group. The St. Patrick's Day Parade in New York and in other communities across the continent is a single example of an ethnic group parading their separateness (differential behavior[31]), and encouraging Americans of all stripes to show alliance to this colorful ethnic group. Another multicolored social group, the Gay Pride Movement, also parades in communities across the country to show the strength of their culture and demonstrate for recognition of their group within the contemporary legal and social systems.		These festivals and parades, with a target audience of people who do not belong to the social group, intersect with the interests and mission of public folklorists, who are engaged in the documentation, preservation, and presentation of traditional forms of folklife. With a swell in popular interest in folk traditions, these community celebrations are becoming more numerous throughout the western world. While ostensibly parading the diversity of their community, economic groups have discovered that these folk parades and festivals are good for business. All shades of people are out on the streets, eating, drinking and spending. This attracts support not only from the business community, but also from federal and state organizations for these local street parties.[32] Paradoxically, in parading diversity within the community, these events have come to authenticate true community, where business interests ally with the varied (folk) social groups to promote the interests of the community as a whole.		This is just a small sampling of types and examples of customary lore.		Childlore is a distinct branch of folklore that deals with activities passed on by children to other children, away from the influence or supervision of an adult.[33] Children's folklore contains artifacts from all the standard folklore genres of verbal, material and customary lore; it is however the child-to-child conduit that distinguishes these artifacts. For childhood is a social group where children teach, learn and share their own traditions, flourishing in a street culture outside the purview of adults. This is also ideally where it needs to be collected; as Iona and Peter Opie demonstrated in their pioneering book Children's Games in Street and Playground.[16] Here the social group of children is studied on its own terms, not as an derivative of adult social groups. It is shown that the culture of children is quite distinctive; it is generally unnoticed by the sophisticated world of adults, and quite as little affected by it.[34]		Of particular interest to folklorists here is the mode of transmission of these artifacts; this lore circulates exclusively within an informal pre-literate children's network or folkgroup. It does not include artifacts taught to children by adults. However children can take the taught and teach it further to other children, turning it into childlore. Or they can take the artifacts and turn them into something else; so Old McDonald's farm is transformed from animal noises to the scatological version of animal poop. This childlore is characterized by "its lack of dependence on literary and fixed form. Children…operate among themselves in a world of informal and oral communication, unimpeded by the necessity of maintaining and transmitting information by written means.[35] This is as close as folklorists can come to observing the transmission and social function of this folk knowledge before the spread of literacy during the 19th century.		As we have seen with the other genres, the original collections of children's lore and games in the 19th century was driven by a fear that the culture of childhood would die out.[36] Early folklorists, among them Alice Gomme in Britain and William Wells Newell in the United States, felt a need to capture the unstructured and unsupervised street life and activities of children before it was lost. This fear proved to be unfounded. In a comparison of any modern school playground during recess and the painting of "Children's Games" by Pieter Breugel the Elder we can see that the activity level is similar, and many of the games from the 1560 painting are recognizable and comparable to modern variations still played today.		These same artifacts of childlore, in innumerable variations, also continue to serve the same function of learning and practicing skills needed for growth. So bouncing and swinging rhythms and rhymes encourage development of balance and coordination in infants and children. Verbal rhymes like Peter Piper picked… serve to increase both the oral and aural acuity of children. Songs and chants, accessing a different part of the brain, are used to memorize series (Alphabet song). They also provide the necessary beat to complex physical rhythms and movements, be it hand-clapping, jump roping, or ball bouncing. Furthermore, many physical games are used to develop strength, coordination and endurance of the players. For some team games, negotiations about the rules can run on longer than the game itself as social skills are rehearsed.[37] Even as we are just now uncovering the neuroscience that undergirds the developmental function of this childlore, the artifacts themselves have been in play for centuries.		Below is listed just a small sampling of types and examples of childlore and games.		Lacking context, the folklore artifacts in the Smithsonian Folklife Archive contain as much life as the stuffed elephant down the street in the Natural History Museum. It is only in performance that they come alive as an active and meaningful component of a social group; this is where the intergroup communication lives, where transmission of these cultural elements takes place. "Folklore is folklore only when performed".[38] Without transmission, these items are not folklore, they are just individual quirky tales and objects.		This understanding in folkloristics only occurred in the second half of the 20th century, when the two terms "folklore performance" and "text and context" dominated discussions among folklorists. These terms are not contradictory or even mutually exclusive. As borrowings from other fields of study, one or the other linguistic formulation is more appropriate to any given discussion. Performance is frequently tied to verbal and customary lore, whereas context is used in discussions of material lore. Both formulations offer different perspectives on the same folkloric understanding, specifically that folklore artifacts need to remain embedded in their cultural environment if we are to gain insight into their meaning for the community.		The concept of cultural (folklore) performance is shared with ethnography and anthropology among other social sciences. The cultural anthropologist Victor Turner identified four universal characteristics of cultural performance. These are playfulness, framing, using symbolic language and employing the subjunctive mood.[39] In performance the audience leaves the daily reality to move into a mode of make-believe, "what if". That this fits well with all types of verbal lore, where reality finds no footing among the symbols, fantasies, and nonsense of traditional tales, proverbs, and jokes is self-evident. Customs and the lore of children and games also fit easily into the language of a folklore performance.		Material culture requires some kneading to turn it into a performance. Should we consider the performance of the production, as in a quilting party, or the performance of the recipients who use the quilt to cover their marriage bed? Here the language of context works better to describe the quilting of patterns copied from the grandmother, quilting as a social event during the winter months, or the gifting of a quilt to signify the importance of the event. Each of these, the traditional pattern chosen, the social event and the gifting occur within the broader context of the community. That said, even in a discussion of context the structure and characteristics of performance can be recognized, including an audience, a framing event, and the use of decorative figures and symbols which go beyond the utility of the object.		Before the Second World War, folk artifacts had been understood and collected as cultural shards of an earlier time. They were considered individual vestigial artifacts, with little or no function in the contemporary culture. Given this understanding, the goal of the folklorist was to capture and document them before they disappeared. They were collected with no supporting data, bound in books, archived and classified more or less successfully. The Historic-Geographic Method worked to isolate and track these collected artifacts, mostly verbal lore, across space and time.		Following the Second World War, folklorists began to articulate a more holistic approach toward their subject matter. In tandem with the growing sophistication in the social sciences, attention was no longer limited to the isolated artifact, but extended to include the artifact embedded in an active cultural environment. One early proponent was Alan Dundes with his essay "Texture, Text and Context", first published 1964.[40] A public presentation in 1967 by Dan Ben-Amos at the American Folklore Society brought the behavioral approach into open debate among folklorists. In 1972 Richard Dorson called out the "young Turks" for their movement toward a behavioral approach to folklore. This approach "shifted the conceptualization of folklore as an extractable item or 'text' to an emphasis on folklore as a kind of human behavior and communication. Conceptualizing folklore as behavior redefined the job of folklorists..."[41][note 6]		Folklore became a verb, an action, something that people do, not just something that they have.[42] It is in the performance and the active context that folklore artifacts get transmitted in informal, direct communication, either verbally or in demonstration. Performance became the umbrella term for all the different modes and manners in which this transmission occurs.		Transmission is a communicative process requiring a binary: one individual or group who actively transmits information in some form to another individual or group. Each of these is a defined role in the folklore process. The tradition-bearer[43] is the individual who actively passes along the knowledge of an artifact; this can be either a mother singing a lullaby to her baby, or an Irish dance troupe performing at a local festival. They are named individuals, usually well known in the community as knowledgeable in their traditional lore. They are not the anonymous "folk", the nameless mass without of history or individuality.		The audience of this performance is the other half in the transmission process; they listen, watch, and remember. Few of them will become active tradition-bearers; many more will be passive tradition-bearers who maintain a memory of this specific traditional artifact, in both its presentation and its content.		There is active communication between the audience and the performer. The performer is presenting to the audience; the audience in turn, through its actions and reactions, is actively communicating with the performer.[44] The purpose of this performance is not to create something new but to re-create something that already exists; the performance is words and actions which are known, recognized and valued by both the performer and the audience. For folklore is first and foremost remembered behavior. As members of the same cultural reference group, they identify and value this performance as a piece of shared cultural knowledge.		To initiate the performance, there must be a frame of some sort to indicate that what is to follow is indeed performance. The frame brackets it as outside of normal discourse. In customary lore such as life cycle celebrations (ex. birthday) or dance performances, the framing occurs as part of the event, frequently marked by location. The audience goes to the event location to participate. Games are defined primarily by rules,[45] it is with the initiation of the rules that the game is framed. The folklorist Barre Toelken describes an evening spent in a Navaho family playing string figure games, with each of the members shifting from performer to audience as they create and display different figures to each other.[46]		In verbal lore, the performer will start and end with recognized linguistic formulas. An easy example is seen in the common introduction to a joke: "Have you heard the one…", "Joke of the day…", or "An elephant walks into a bar". Each of these signals to the listeners that the following is a joke, not to be taken literally. The joke is completed with the punch line of the joke. Another traditional narrative marker in English is the framing of a fairy tale between the phrases "Once upon a time" and "They all lived happily ever after." Many languages have similar phrases which are used to frame a traditional tale. Each of these linguistic formulas removes the bracketed text from ordinary discourse, and marks it as a recognized form of stylized, formulaic communication for both the performer and the audience.		Framing as a narrative device serves to signal to both the story teller and the audience that the narrative which follows is indeed a fiction (verbal lore), and not to be understood as historical fact or reality. It moves the framed narration into the subjunctive mood, and marks a space in which "fiction, history, story, tradition, art, teaching, all exist within the narrated or performed expressive 'event' outside the normal realms and constraints of reality or time."[47] This shift from the realis to the irrealis mood is understood by all participants within the reference group. It enables these fictional events to contain meaning for the group, and can lead to very real consequences.[48]		The theory of self-correction in folklore transmission was first articulated by the folklorist Walter Anderson in the 1920s; this posits a feedback mechanism which would keep folklore variants closer to the original form.[49][note 8] This theory addresses the question about how, with multiple performers and multiple audiences, the artifact maintains its identity across time and geography. Anderson credited the audience with censoring narrators who deviated too far from the known (traditional) text.[50]		Any performance is a two-way communication process. The performer addresses the audience with words and actions; the audience in turn actively responds to the performer. If this performance deviates too far from audience expectations of the familiar folk artifact, they will respond with negative feedback. Wanting to avoid more negative reaction, the performer will adjust his performance to conform to audience expectations. "Social reward by an audience [is] a major factor in motivating narrators…"[51] It is this dynamic feedback loop between performer and audience which gives stability to the text of the performance.[52]		In reality, this model is not so simplistic; there is multiple redundancy in the active folklore process. The performer has heard the tale multiple times, he has heard it from different story tellers in multiple versions. In turn, he tells the tale multiple times to the same or a different audience, and they expect to hear the version they know. This expanded model of redundancy in a non-linear narrative process makes it difficult to innovate during any single performance; corrective feedback from the audience will be immediate.[53] "At the heart of both autopoetic self-maintenance and the 'virality' of meme transmission… it is enough to assume that some sort of recursive action maintains a degree of integrity [of the artifact] in certain features … sufficient to allow us to recognize it as an instance of its type."[54]		For material folk artifacts, it becomes more fruitful to return to the terminology of Alan Dundes: text and context. Here the text designates the physical artifact itself, the single item made by an individual for a specific purpose. The context is then unmasked by observation and questions concerning both its production and its usage. Why was it made, how was it made, who will use it, how will they use it, where did the raw materials come from, who designed it, etc. These questions are limited only by the skill of the interviewer.		In his study of southeastern Kentucky chair makers, Michael Owen Jones describes production of a chair within the context of the life of the craftsman.[55] For Henry Glassie in his study of Folk Housing in Middle Virginia[56] the investigation concerns the historical pattern he finds repeated in the dwellings of this region: the house is planted in the landscape just as the landscape completes itself with the house.[57] The artisan in his roadside stand or shop in the nearby town wants to make and display products which appeal to customers. There is "a craftsperson's eagerness to produce 'satisfactory items' due to a close personal contact with the customer and expectations to serve the customer again." Here the role of consumer "... is the basic force responsible for the continuity and discontinuity of behavior."[51]		In material culture the context becomes the cultural environment in which the object is made (chair), used (house), and sold (wares). None of these artisans is "anonymous" folk; they are individuals making a living with the tools and skills learned within and valued in the context of their community.		No two performances are identical. The performer attempts to keep the performance within expectations, but this happens despite a multitude of changing variables. He has given this performance one time more or less, the audience is different, the social and political environment has changed. In the context of material culture, no two hand-crafted items are identical. Sometimes these deviations in the performance and the production are unintentional, just part of the process. But sometimes these deviations are intentional; the performer or artisan want to play with the boundaries of expectation and add their own creative touch. They perform within the tension of conserving the recognized form and adding innovation.		The folklorist Barre Toelken identifies this tension as "… a combination of both changing ("dynamic") and static ("conservative") elements that evolve and change through sharing, communication and performance."[58] Over time, the cultural context shifts and morphs: new leaders, new technologies, new values, new awareness. As the context changes, so must the artifact, for without modifications to map existing artifacts into the evolving cultural landscape, they lose their meaning. Joking as an active form of verbal lore makes this tension visible as joke cycles come and go to reflect new issues of concern. Once an artifact is no longer applicable to the context, transmission becomes a nonstarter; it loses relevancy for a contemporary audience. If it is not transmitted, then it is no longer folklore and becomes instead an historic relic.[51]		It is too soon to identify how the advent of electronic communications will modify and change the performance and transmission of folklore artifacts. Just by looking at the development of one type of verbal lore, electronic joking, it is clear that the internet is modifying folkloric process, not killing it. Jokes and joking are as plentiful as ever both in traditional face-to-face interactions and through electronic transmission. New communication modes are also transforming traditional stories into many different configurations. The fairy tale Snow White is now offered in multiple media forms for both children and adults, including a television show, a video game, and a programming language.		A more generalized analysis of folklore in the electronic age will have to wait for further studies to be published in the field.		
Frame analysis (also called framing analysis) is a multi-disciplinary social science research method used to analyze how people understand situations and activities. The concept is generally attributed to the work of Erving Goffman and his 1974 book Frame analysis: An essay on the organization of experience and has been developed in social movement theory, policy studies and elsewhere.		Framing theory and frame analysis is a broad theoretical approach that has been used in communication studies, news (Johnson-Cartee, 1995), politics, and social movements among other applications. "Framing is the process by which a communication source, such as a news organization, defines and constructs a political issue or public controversy" (Nelson, Oxley, & Clawson, 1997, p. 221).						Frame analysis had been proposed as a type of rhetorical analysis for political actors in the 1980s. Political communication researcher Jim A. Kuypers first published his work advancing framing analysis as a rhetorical perspective in 1997. His approach begins inductively by looking for themes that persist across time in a text (for Kuypers, primarily news narratives on an issue or event), and then determining how those themes are framed. Kuypers' work begins with the assumption that frames are powerful rhetorical entities that "induce us to filter our perceptions of the world in particular ways, essentially making some aspects of our multi-dimensional reality more noticeable than other aspects. They operate by making some information more salient than other information. ..."[1] In "Framing Analysis From a Rhetorical Perspective" Kuypers details the differences between framing analysis as rhetorical criticism and as a social scientific endeavor, in particular arguing that framing criticism offers insights unavailable to social scientists.[2]		In his 2009 work, Rhetorical Criticism: Perspectives in Action[3] Kuypers offers a detailed template for doing framing analysis from a rhetorical perspective. According to Kuypers, "Framing is a process whereby communicators, consciously or unconsciously, act to construct a point of view that encourages the facts of a given situation to be interpreted by others in a particular manner. Frames operate in four key ways: they define problems, diagnose causes, make moral judgments, and suggest remedies. Frames are often found within a narrative account of an issue or event, and are generally the central organizing idea."[4] Kuypers' work is based on the premise that framing is a rhetorical process and as such it is best examined from a rhetorical point of view.		Framing has been utilized to explain the process of social movements (Snow & Benford, 1988).[5] Movements are carriers of beliefs and ideologies. In addition, they are part of the process of constructing meaning for participants and opposers (Snow & Benford, 1988). Mass movements are said to be successful when the frames projected align with the frames of participants to produce resonance between the two parties. This is a process known as frame alignment.		Snow and Benford (1988) say that frame alignment is an important element in social mobilization or movement. They argue that when individual frames become linked in congruency and complementariness, that "frame alignment" occurs (p. 198; Snow et al. 1986, p. 464[6]), producing "frame resonance", which is key to the process of a group transitioning from one frame to another (although not all framing efforts are successful). The conditions that affect or constrain framing efforts are:		Snow and Benford (1988) propose that once proper frames are constructed as described above, large-scale changes in society such as those necessary for social movement can be achieved through frame alignment.		Frame analysis for political thought has been dominated by two popular cognitive scientists: George Lakoff, nurturant parent governance; and Frank Luntz, strict father governance.[7]		There are four types, which include frame bridging, frame amplification, frame extension and frame transformation.		Frame bridging is the "linkage of two or more ideologically congruent but structurally unconnected frames regarding a particular issue or problem" (Snow et al., 1986, p. 467). It involves the linkage of a movement to "unmobilized sentiment pools or public opinion preference clusters" (p. 467) of people who share similar views or grievances but who lack an organizational base.		Frame amplification refers to "the clarification and invigoration of an interpretive frame that bears on a particular issue, problem, or set of events" (Snow et al., 1986, p. 469). This interpretive frame usually involves the invigorating of values or beliefs.		Frame extensions are a movement's effort to incorporate participants by extending the boundaries of the proposed frame to include or encompass the views, interests, or sentiments of targeted groups. (Snow et al., 1986, p. 469)		Frame transformation is a process required when the proposed frames "may not resonate with, and on occasion may even appear antithetical to, conventional lifestyles or rituals and extant interpretive frames" (Snow et al., 1986, p. 473).		When this happens, new values, new meanings and understandings are required in order to secure participants and support. Goffman (1974, p. 43–44) calls this "keying" where "activities, events, and biographies that are already meaningful from the standpoint of some primary framework transpose in terms of another framework" (Snow et al., 1986, p. 474) such that they are seen differently. There are two types of frame transformation:		
Discourse analysis (DA), or discourse studies, is a general term for a number of approaches to analyze written, vocal, or sign language use, or any significant semiotic event.		The objects of discourse analysis (discourse, writing, conversation, communicative event) are variously defined in terms of coherent sequences of sentences, propositions, speech, or turns-at-talk. Contrary to much of traditional linguistics, discourse analysts not only study language use 'beyond the sentence boundary' but also prefer to analyze 'naturally occurring' language use, not invented examples.[1] Text linguistics is a closely related field. The essential difference between discourse analysis and text linguistics is that discourse analysis aims at revealing socio-psychological characteristics of a person/persons rather than text structure.[2]		Discourse analysis has been taken up in a variety of disciplines in the humanities and social sciences, including linguistics, education, sociology, anthropology, social work, cognitive psychology, social psychology, area studies, cultural studies, international relations, human geography, communication studies, biblical studies, and translation studies, each of which is subject to its own assumptions, dimensions of analysis, and methodologies.						Topics of discourse analysis include:[3]		Political discourse analysis is a field of discourse analysis which focuses on discourse in political forums (such as debates, speeches, and hearings) as the phenomenon of interest. Policy analysis requires discourse analysis to be effective from the post-positivist perspective.[citation needed]		Political discourse is the informal exchange of reasoned views as to which of several alternative courses of action should be taken to solve a societal problem.[4]		An example of an analysis of political discourse is Roffee's 2016 examination into speech acts surrounding the justification of the legislative processes concerning the Australian federal government's intervening in the Northern Territory Aboriginal communities. The intervention was a hasty reaction to a social problem. Through this analysis, Roffee established that there was in fact an unwillingness to respond on behalf of the government, and the intervention was, in fact, no more than another attempt to control the Indigenous population. However, due to the political rhetoric used, this was largely unidentified.[5]		Although the ancient Greeks (among others) had much to say on discourse, some scholars[which?] consider Austria-born Leo Spitzer's Stilstudien (Style Studies) of 1928 the earliest example of discourse analysis (DA). It was translated into French by Michel Foucault.		However, the term first came into general use following the publication of a series of papers by Zellig Harris from 1952 reporting on work from which he developed transformational grammar in the late 1930s. Formal equivalence relations among the sentences of a coherent discourse are made explicit by using sentence transformations to put the text in a canonical form. Words and sentences with equivalent information then appear in the same column of an array. This work progressed over the next four decades (see references) into a science of sublanguage analysis (Kittredge & Lehrberger 1982), culminating in a demonstration of the informational structures in texts of a sublanguage of science, that of immunology, (Harris et al. 1989) and a fully articulated theory of linguistic informational content (Harris 1991). During this time, however, most linguists ignored such developments in favor of a succession of elaborate theories of sentence-level syntax and semantics.[6]		In January 1953, a linguist working for the American Bible Society, James A. Lauriault/Loriot, needed to find answers to some fundamental errors in translating Quechua, in the Cuzco area of Peru. Following Harris's 1952 publications, he worked over the meaning and placement of each word in a collection of Quechua legends with a native speaker of Quechua and was able to formulate discourse rules that transcended the simple sentence structure. He then applied the process to Shipibo, another language of Eastern Peru. He taught the theory at the Summer Institute of Linguistics in Norman, Oklahoma, in the summers of 1956 and 1957 and entered the University of Pennsylvania to study with Harris in the interim year. He tried to publish a paper Shipibo Paragraph Structure, but it was delayed until 1970 (Loriot & Hollenbach 1970).[citation needed] In the meantime, Kenneth Lee Pike, a professor at University of Michigan, Ann Arbor, taught the theory, and one of his students, Robert E. Longacre developed it in his writings.		Harris's methodology disclosing the correlation of form with meaning was developed into a system for the computer-aided analysis of natural language by a team led by Naomi Sager at NYU, which has been applied to a number of sublanguage domains, most notably to medical informatics. The software for the Medical Language Processor is publicly available on SourceForge.		In the late 1960s and 1970s, and without reference to this prior work, a variety of other approaches to a new cross-discipline of DA began to develop in most of the humanities and social sciences concurrently with, and related to, other disciplines, such as semiotics, psycholinguistics, sociolinguistics, and pragmatics. Many of these approaches, especially those influenced by the social sciences, favor a more dynamic study of oral talk-in-interaction. An example is "conversational analysis", which was influenced by the Sociologist Harold Garfinkel, the founder of Ethnomethodology.		In Europe, Michel Foucault became one of the key theorists of the subject, especially of discourse, and wrote The Archaeology of Knowledge. In this context, the term 'discourse' no longer refers to formal linguistic aspects, but to institutionalized patterns of knowledge that become manifest in disciplinary structures and operate by the connection of knowledge and power. Since the 1970s, Foucault´s works have had an increasing impact especially on discourse analysis in the social sciences. Thus, in modern European social sciences, one can find a wide range of different approaches working with Foucault´s definition of discourse and his theoretical concepts. Apart from the original context in France, there is, at least since 2005, a broad discussion on socio-scientific discourse analysis in Germany. Here, for example, the sociologist Reiner Keller developed his widely recognized 'Sociology of Knowledge Approach to Discourse (SKAD)'.[7] Following the sociology of knowledge by Peter L. Berger and Thomas Luckmann, Keller argues, that our sense of reality in everyday life and thus the meaning of every objects, actions and events are the product of a permanent, routinized interaction. In this context, SKAD has been developed as a scientific perspective that is able to understand the processes of 'The Social Construction of Reality' on all levels of social life by combining Michel Foucault's theories of discourse and power with the theory of knowledge by Berger/Luckmann. Whereas the latter primarily focus on the constitution and stabilisation of knowledge on the level of interaction, Foucault's perspective concentrates on institutional contexts of the production and integration of knowledge, where the subject mainly appears to be determined by knowledge and power. Therefore, the 'Sociology of Knowledge Approach to Discourse' can also be seen as an approach to deal with the vividly discussed micro–macro problem in sociology.		The following are some of the specific theoretical perspectives and analytical approaches used in linguistic discourse analysis:		Although these approaches emphasize different aspects of language use, they all view language as social interaction, and are concerned with the social contexts in which discourse is embedded.		Often a distinction is made between 'local' structures of discourse (such as relations among sentences, propositions, and turns) and 'global' structures, such as overall topics and the schematic organization of discourses and conversations. For instance, many types of discourse begin with some kind of global 'summary', in titles, headlines, leads, abstracts, and so on.		A problem for the discourse analyst is to decide when a particular feature is relevant to the specification is required. A question many linguist ask is: "Are there general principles which will determine the relevance or nature of the specification?"		
Comic timing is the use of rhythm, tempo, and pausing to enhance comedy and humour. The pacing of the delivery of a joke can have a strong impact on its comedic effect, even altering its meaning; the same can also be true of more physical comedy such as slapstick.[1]		A beat is a pause taken for the purposes of comic timing, often to allow the audience time to recognize the joke and react, or to heighten the suspense before delivery of the expected punch line. Pauses, sometimes called "dramatic pauses" in this context, can be used to discern subtext or even unconscious content—that is, what the speaker is really thinking about. A pause can also be used to heighten a switch in direction. As a speaker talks, the audience naturally "fills in the blanks", finishing the expected end of the thought. The pause allows this to happen before the comedian delivers a different outcome, surprising the listener and (hopefully) evoking laughter.		Jack Benny and Victor Borge are two comedians known for using the extended beat, allowing the pause itself to become a source of humour beyond the original joke. George Carlin and Rowan Atkinson are two other stand-up comedians considered[citation needed] to have superior timing.						Carlin's most famous routine was his "Seven Words You Can't Say On Television", in which much of the humour is derived from a sudden, rapid-fire delivery of the seven words. The remainder of the routine was a mock-scholarly analysis of why these words are not as bad as the world would have us believe. Here, comic timing is used again as Carlin moved from the rapid list to a more reasoned dissection of the words.		Atkinson is another example of timing in this regard. His "No One Called Jones" routine involves his reading a class roll of students at what we can assume is an exclusive English boarding school. In one version of this routine, each name is a double entendre. In this sort of routine, it is very important to use beats, as simply racing through the list would spoil the effect of many of the jokes.		Commonly recognized as a master of comic timing,[citation needed] Danish-American comedian Victor Borge provides even more examples of this art. Much of his routine involved references to particular pieces of classical music, opera and composers. Having learned English as a second language, Borge was known for frequently playing around with its conventions. A prime example is his question to his audience, "Is there anyone who would like to hear the famous Polonaise in A Flat by Chopin?" After hearing the inevitable calls of "Yes, yes", Borge would respond, "Very well, is there anyone here who can play it?" Another famous line is his explanation for the third pedal on a grand piano—"The pedal in the middle is there to separate the other two pedals...(beat)...which could be a problem for those of you who have three feet."		Borge, therefore, builds his audience up to the joke, but only delivers the actual punchline when he is fully aware that they are silent and prepared to hear it. His famous "Inflationary Language" routine demonstrates the other side of this statement. In this routine, Borge adds one to every "number in the language", (making "wonderful" into "two-derful" and so on) and his "Phonetic Punctuation" routine, wherein he assigns a sound to every punctuation mark. These routines then consist of Borge reading a story under one of these systems. The comic timing is seen by the way that he reads alternately slowly and rapidly, in keeping with the action of the story.		In Cohen's mockumentary Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan, the character Borat is coached on the importance of comic timing. He is to state "That suit is black (pause) NOT!" However, he does it with both no pause, with too long a pause, and even the word "pause."		The farce is another example of comic timing. Here, the humour is derived both from rapid speech and rapid movement—people running into and out of rooms at breakneck speed and managing to cause havoc in the process as done to perfection in the series Fawlty Towers.		A pregnant pause (as in the classical definition, "many possibilities") is a technique of comic timing used to accentuate a comedy element, which uses comic pauses at the end of a phrase to build up suspense. It's often used at the end of a comically awkward statement or in the silence after a seemingly non-comic phrase to build up a comeback. Refined[clarification needed] by Jack Benny[citation needed], the pregnant pause has become a staple of stand-up comedy.		
Till Eulenspiegel (German pronunciation: [tɪl ˈʔɔʏlənˌʃpiːɡəl], Low Saxon: Dyl Ulenspegel [dɪl ˈʔuːlnˌspeɪɡl̩], Dutch: Tijl Uilenspiegel) is a trickster figure originating in Middle Low German folklore. He appeared in chapbooks telling episodes that outlined his picaresque career in Germany, the Low Countries, Denmark, Bohemia, Poland, and Italy. He made his main entrance in English-speaking culture late in the nineteenth century as "Owlglass". However, he was first mentioned in English literature by Ben Jonson in his comedic play The Alchemist, or even earlier – Owleglasse – by Henry Porter in The Two Angry Women of Abington (1599).						According to the tradition, Eulenspiegel was born in Kneitlingen near Brunswick around 1300. He travelled through the Holy Roman Empire, especially Northern Germany, but also the Low Countries, Bohemia, and Italy. His mobility as a Landfahrer ("vagrant") allows him to be envisaged anywhere and everywhere in the late Middle Ages. Eulenspiegel was said to have died in Mölln, near Lübeck and Hamburg, of the Black Death in 1350, according to a gravestone attributed to him there, which was noted by Fynes Moryson in his Itinerary, 1591.[1] "Don't move this stone, let that be clear – Eulenspiegel's buried here"[2] is written on the stone in Low German.		Since the early 19th century, many German scholars have made attempts to find historical evidence of Till Eulenspiegel's existence. In his 1980 book Till Eulenspiegel, historian Bernd Ulrich Hucker mentions that according to a contemporary legal register of the city of Brunswick one Till van Cletlinge ("Till from/of Kneitlingen") was incarcerated there in the year 1339, along with four of his accomplices, for highway robbery.[3]		In the stories, he is presented as a trickster who plays practical jokes on his contemporaries, exposing vices at every turn, greed and folly, hypocrisy and foolishness. As Peter Carels notes, "The fulcrum of his wit in a large number of the tales is his literal interpretation of figurative language."[4] In these stories, anything that can go wrong in communication does go wrong due to the disparity in consciousness. And it is not the exception that communication gives rise to complications; rather, it is the rule. As a model of communication, Till Eulenspiegel is the inherent, unpredictable factor of complication that can throw any communication, whether with oneself or others, into disarray. These irritations, amounting to conflicts, have the potential of effecting mental paradigm changes and increases in the level of consciousness. Although craftsmen are featured as the principal victims of his pranks, neither the nobility nor the pope is exempt from being affected by him.		Many of Till's other pranks are scatological in nature, and involve tricking people into touching, smelling, or even eating Till's excrement.[5][6]		The two earliest printed editions,[7] in Early New High German, "Ein kurtzweilig Lesen von Dyl Ulenspiegel, geboren uß dem Land zu Brunßwick, wie er sein leben volbracht hat …", are Johannes Grüninger's in Strassburg, 1510–11 and 1515. The 1510-11 edition is considered the definitive text as far as it has been preserved; only one relatively complete copy (missing about 30 pages, which were replaced by pages from a then-contemporary edition when the book was rebound by an unknown owner around the year 1700) and a few pages that appear to have been printer's trials, made before the actual printing run began, are known to survive. In fact the page that would have contained the year number being among those lost, the time frame has been inferred from details of the type used by the printer: other books from Grüninger's shop dated 1510-11 were set from the same lead type (lead type had to be recast fairly frequently since it would be worn down rather quickly in a busy print shop). The 1515 edition is decidedly inferior, missing many of the illustrations of the older edition, and showing signs of careless copying of the text; a third Strassburg edition, of 1519, is better again and is usually used in modern editions to provide the sections that are missing in the surviving 1510-11 copy.[8] In spite of often-repeated suggestions to the effect "that the name 'Eulenspiegel' was used in tales of rogues and liars in Lower Saxony as early as 1400",[9] previous references to a Till Eulenspiegel actually turn out to be surprisingly elusive, Paul Oppenheimer concludes.[8] The author is supposedly Hermann Bote. Recent research has established that it was written in Early High German.		The literal translation of the High German name "Eulenspiegel" is "owl mirror", two symbols that identify Till Eulenspiegel in popular woodcuts (illustration). Another meaning hypothetically attributed to his name is "wipe the arse". In the eighteenth century, German satirists adopted episodes for social satire, and in the nineteenth and early twentieth-century versions of the tales are bowdlerized, to render them fit for children, who had come to be considered their chief natural audience, by expurgating their many scatological references.[4] In the current Oppenheimer edition (see above) scatological stories abound, beginning with Till's early childhood (in which he rides behind his father and exposes his rear-end to the townspeople) and persisting until his death bed (where he tricks a priest into soiling his hands with feces).[8]		The Legend of Thyl Ulenspiegel and Lamme Goedzak, a 1867 novel by Belgian author Charles De Coster, has been translated, often in mutilated versions, into many languages. In De Coster's story Uilenspiegel is said to have been born in Damme, West Flanders and to have become a Protestant hero of the 16th Century Dutch Revolt against Spanish rule - a feature obviously missing from the original folk tales. The author gives him a father, Claes, and mother, Soetkin, as well as a girlfriend, Nele, and a best friend, Lamme Goedzak. In the course of the story Claes is taken prisoner by the Spanish oppressors and burned at the stake, while Soetkin goes insane as a result. This tempts Thyl to start resistance against the Spanish oppressors. Thanks to the novel Uilenspiegel has become a symbol of Flemish nationalism. He also has a statue in Damme.		There are three museums in Germany featuring Till Eulenspiegel. One is located in the town of Schöppenstedt in Lower Saxony, which is nearby his assumed birthplace Kneitlingen. The second is located in the supposed place of his death, the city of Mölln in Schleswig-Holstein, and the third in Bernburg (Saale), Sachsen-Anhalt. In the town of Damme, Belgium, there is also a museum honoring him. Damme was Till's (Tijl in Dutch) birthplace in the novel by De Coster. And there is a fountain and statue featuring Till Eulenspiegel in the Marktplatz of Magdeburg, capital city of Sachsen-Anhalt.		In 1867 Belgian author Charles De Coster wrote The Legend of Thyl Ulenspiegel and Lamme Goedzak, a novel that reinvented the trickster character as a resistance leader against the Spanish occupation in the 16th century, making him a symbol of Flanders.		In 1927 Gerhart Hauptmann wrote the verse Till Eulenspiegel.		Between 1945 and 1950 a German satirical magazine was called: Ulenspiegel.		Since 1954 there has been a German satirical magazine called Eulenspiegel		Ulenspiegel was mentioned in Mikhail Bulgakov's "The Master and Margarita" as a possible prototype for the black cat character Behemoth.		Michael Rosen adapted the story into a 1989 children's novel, illustrated by Fritz Wegner: The Wicked Tricks of Till Owlyglass, ISBN 978-0744513462.		Clive Barker adapted it into the novel Crazyface.		Richard Strauss wrote the tone poem Till Eulenspiegels lustige Streiche, Op. 28, in 1894-1895.		In 1902 Emil von Reznicek adapted the story into an opera: Till Eulenspiegel.		In 1913 Walter Braunfels adapted the story into an opera Ulenspiegel.		The Ballets Russes adapted the story into a ballet in 1916, later re-adapted by George Balanchine for Jerome Robbins at New York City Ballet.		Wladimir Vogel was a Russian composer who wrote a drama-oratorio Thyl Claes in the late 30s or early 40s, derived from De Coster's book.		The Soviet composer Nikolai Karetnikov and his librettist filmmaker Pavel Lungin adapted De Coster's novel as the samizdat opera "Till Eulenspiegel" (1983), which had to be recorded piece-by-piece in secret and received its premiere (1993) only after the Soviet Union collapsed.		Dutch comics artist George van Raemdonck adapted the novel into a comic strip in the 1920s.[10] In the 1940s Ray Goossens made a gag-a-day comic about Uilenspiegel and Lamme Goedzak.[11] Willy Vandersteen drew two comic book albums about Uilenspiegel, "De Opstand der Geuzen" ("The Rebellion of the Geuzen") and "Fort Oranje" ("Fort Orange"), both drawn in a realistic, serious style and pre-published in the Belgian comics magazine Tintin between 1952 and 1954. They were published in comic book album format in 1954 and 1955. The stories were drawn in a realistic style and in some instances followed the original novel very closely, but sometimes followed his own imagination more.[12]		Kibbutz theatre director and producer Shulamit Bat-Dori created an open-air production of Till Eulenspiegel at Mishmar HaEmek, Israel, in 1955 that drew 10,000 viewers.[13][14]		In 1956 the film Les Aventures de Till L'Espiègle was made by Gérard Philipe and Joris Ivens, which adapted De Coster's novel. (English title: "Bold Adventure"). The film was a French-East German co-production.		In 1973 Walter van der Kamp directed Uilenspiegel, a Dutch film.		Rainer Simon directed Tijl Eulenspiegel in 1975, which was an East-German production.		Ulenspiegel (Legenda o Tile), was a 1976 Soviet film, based on De Coster's novel, and directed by Aleksandr Alov and Vladimir Naumov, "The Legend of Till Ullenspiegel" (1976).[15]		In 2003 Eberhard Junkersdorf adapted the story into a feature-length animated film.		In 2014 Christian Theede directed the film Till Eulenspiegel.		In 1961 the BRT (nowadays the VRT) made a children's TV series, Tijl Uilenspiegel.		Notes		Further reading		
Sardarji jokes or Sardar jokes, are a class of ethnic jokes based on stereotypes of Sikhs (who use the title of "Sardar", with -ji being an honorific). Although jokes about other ethnic and linguistic communities are found in various regions of India, Sardarji jokes are the most widely circulated ethnic jokes and found across the country.[1] Sardarji jokes are generally considered tasteless and inappropriate by members of the Sikh community, and have elicited protests as well as leading to arrests for hurting religious sentiments.[1][2]						Some of the dominant traits of the Sardar jokes include the Sardar being shown as naïve, inept, unintelligent, or not well-versed with the English language.[1][3][4] Many of the Sardar jokes are variations of other ethnic jokes or stereotype jokes. Some of them also depict Sardarjis as witty or using other people's stereotyped perceptions against them.[5]		Santa and Banta are two popular names for the stock characters in the Sardar jokes.[6] The researcher Jawaharlal Handoo associates some traits of the Sardar jokes with the stereotype of Sikhs being associated with jobs where physical fitness is more important than knowledge of the English language or intellect.[1] He also states that "In my opinion, the 'success-story' of the Sikh-community as a whole has taken the form of a deep-rooted anxiety in the collective minds of the non-Sikh majorities especially the Hindus of India....Sikhs are a very prosperous and successful people ....this may have threatened the Hindu ego and created the anxiety which in turn seems to have taken the form of various stereotypes and the resultant joke cycle."[1] Soumen Sen states that these jokes perhaps reflect the anxiety of the non-Sikh Indian elite, who may have suffered from a sense of insecurity due to the growing competition from the enterprising Sikhs.[7]		A popular category of Sardar jokes is the "12 o'clock jokes", which imply that Sikhs are in their senses only at night. Preetinder Singh explains the origin of the "12 o'clock joke" as follows:[8] The real reason for the "12 O'clock Association" with Sikhs comes from Nadir Shah's invasion of India. His troops passed through Punjab after plundering Delhi and killing hundreds of thousands of Hindus and Muslims, and taking hundreds of women as captive. The Sikhs decided to attack Nadir Shah's camp and free the captive women. Being outnumbered by Nadir Shah's huge army, they could not afford to make a frontal attack. Instead, they used to make midnight guerrilla raids on Nadir Shah's camp, free as many captive women as possible, and return them to their homes in order to "restore the diginity of the Hindu community".[8]		In jest, the Hindus would say that the Sikhs are in their senses only at night. This later became the trait of a widespread category of derisive jokes. Singh opines: "Hindus started referring to the relatively neutral 12 o'clock, rather than midnight" to avoid annoying the armed Sikhs, and the "final result was the safe, bald statement, 'It is 12 o'clock' shorn of all reference to its very interesting history.....When Hindus crack this joke, they are oblivious to the fact that had the Sikhs not intervened, their womenfolk would have been dishonoured and taken into exile".[8]		Some of the Sardar jokes, self-deprecatory in nature, were made up by the Sikhs themselves.[9][10][11][12][13][14] In The Other Face of India, M. V. Kamath wrote about "the Punjabi's enormous capacity to poke fun at himself, a trait that seems peculiar to the Punjabi, especially the Sikh."[15] In his book President Giani Zail Singh, the Sikh author Joginder Singh states "...who can enjoy a good joke against himself or against his tribe except a Punjabi and more particularly, a Sikh?"[16][17]		In recent years, there have been several cases of Sikh groups protesting against the Sardarji jokes. In Folk Narrative and Ethnic Identity: The 'Sardarji' Joke Cycle, Jawaharlal Handoo notes that the Sikh members in a group generally do not seem to enjoy a Sardarji joke, although they may pretend to enjoy the humour of the joke by smiling or joining the group laughter.[1]		In 2005, some Sikhs protested against a scene in the Pritish Nandy Communications (PNC) film Shabd. In the scene, Zayed Khan tries to cheer Aishwarya Rai by telling a Sardarji joke. As he begins the joke with the words "There was a Sardarji", Aishwarya starts giggling. A group of angry Sikhs stormed the PNC office, and demanded that the scene be deleted from the film.[18]		An organisation called The Sikh Brotherhood International wrote letters to the PNC, the Central Board of Film Certification, and the National Commission for Minorities (NCM), saying that the film had hurt the sentiments of the Sikh community. The Pritish Nandy Communications Limited tendered a written apology, stating that they respect the Sikh community and hold it in high esteem, and they had no intention of ridiculing anybody.[19] The Censor Board issued directions to delete the objectionable scenes in the film.[20]		On February 25, 2005, journalist Vir Sanghvi wrote a column in Hindustan Times, saying that the NCM was curbing free speech on behalf of the "forces of intolerance", while claiming to fight for minority rights. He wrote that the Sardarji joke is part of the "good-natured Indian tradition", and not an example of anti-minority feeling.[18] He pointed out that the best Sardarji jokes are told by the Sikhs themselves, presenting Khushwant Singh as an example. He further went on to say that the protesters should develop a sense of humour and that "All truth has the power to offend. Take away the offence and you end up suppressing the truth". Research has indicated however that suggesting truth in such stereotypes in ethnic jokes is not supported by facts which are contrary to this suggestion.[21]).		On March 2, 2005, The NCM filed a complaint against the Hindustan Times with the Press Council of India, stating that "the tone, tenor and the content of the article in question has a tendency to hurt the sentiments of Sikh community."[22] The Hindustan Times responded by stating that the article was not aimed to ridicule the Sikh community in any manner, and was a criticism of the NCM, justifiable under the right of free speech under the Constitution of India. The NCM decided not to proceed with the matter, and the case was closed as withdrawn.[22]		In March 2007, around 25 Sikh youths from Sikh Media and Culture Watch (SMCW) demanded arrest of Ranjit Parande, a Matunga-based book seller, for stocking the Santa and Banta Joke Book, a collection of Sardarji jokes. Based on a complaint filed by a Sikh businessman, the Mumbai Police arrested Parande under section 295 of the Indian Penal Code, for "hurting religious sentiments."[23] The SMCW members alleged that several of the Sardarji jokes border on the obscene, and have begun to have a demoralising effect on the Sikh youths.[23]		They later requested the cyber cell department of the Mumbai police crime branch to "ban jokes on the internet" which portray Sikhs as objects of ridicule. Swaranjit Singh Bajaj, the vice-president of SMCW, blamed the Sikh humorists such as Navjot Singh Sidhu and Khushwant Singh for perpetuating the stereotypical image of Sikhs.[23] Khushwant Singh, a Sikh author who has included several Sardarji jokes in his joke books, received a notice from the secretary of SGPC in 2004, asking him to desist from hurting the sentiments of the community. Singh also received similar notices from some Marwari organisations, the Shiv Sena and the RSS. However, he continued to include Sardarji jokes in his subsequent joke books. In the preface to his 7th joke book, he claimed that most of his Sardarji jokes were "pro-Sardarji".[24]		In December 2007, India's second biggest mobile operator Reliance Communications and its head Anil Ambani were charged by Lucknow police with "insulting a religion or faith", after Reliance sent a Sardarji joke as its "joke of the day".[25] Many Sikhs in Meerut staged violent protests.[10] The joke originated from the website santabanta.com, and was supplied to Reliance by OnMobile, a third party supplier. Reliance stated that the it was not responsible for content provided by OnMobile, but apologised its subscribers and the Sikh community in Uttar Pradesh. OnMobile also issued a public apology.[10]		In 2013, Atul Kumar was arrested in Jalandhar for texting offensive Santa-Banta jokes, under Section 295 (A) and IT Act.[26][27]		
Sneferu (also read Snefru or Snofru)[citation needed], well known under his Hellenized name Soris (by Manetho), was the founding monarch of the 4th dynasty during the Old Kingdom. Estimates of his reign vary, with for instance The Oxford History of Ancient Egypt suggesting a reign from around 2613 BC to 2589 BC,[4] a reign of 24 years, while Rolf Krauss suggests a 30-year reign,[5] and Stadelmann a 48-year reign.[6] He built at least three pyramids that survive to this day and introduced major innovations in the design and construction of pyramids in Ancient Egypt.						The 24-year Turin Canon figure for Sneferu's reign is considered today to be an underestimate since this king's highest-known date is an inscription discovered at the Red Pyramid of Dahshur and mentioning Sneferu's 24th cattle count, corresponding to at least 24 full years.[7] Snefru, however, was known to have a minimum of at least three years after the cattle count dates: his years after the 10th, the 13th and the 18th count are attested at his Meidum pyramid.[8] This would mean that Sneferu ruled Egypt a minimum of 27 full years.		However, in the Palermo Stone, recto 6 at the bottom of the fragment shows the year of the 7th count of Sneferu while recto 7 on the same following row shows the year of the 8th count of Sneferu.[9] Significantly, there is a previous mostly intact column for Sneferu in recto 5 which also mentions events in this king's reign in a specific year but does not mention the previous (6th) year.[10] This column must, therefore, be dated to the year after the 6th count of Sneferu. Hence, Sneferu's reign would be a minimum of 28 years. Since there are many periods in Snefru's reigns for which Egyptologists have few dates—only the years of the 2nd, 7th, 8th, 12th, 13th, 14th, 15th, 16th, 17th, 18th, 23rd and 24th count are known for Sneferu before one considers the years after his cattle counts[11]—this pharaoh is most likely to have had a reign in excess of 30 years to manage to build three pyramids in his long rule but not 48 years since the cattle count was not regularly biannual during his kingship. (There are fewer years after the count dates known for Sneferu compared to year of the count or census dates.)		Sneferu was the first king of the fourth dynasty of Ancient Egypt, who according to Manetho reigned for 24 years (2613-2589 BC).		Manetho was an Egyptian priest, living in the third century BC, who categorized the pharaohs of dynastic Egypt into 31 dynasties.[12] Manetho’s schematic has its flaws; nevertheless, modern scholars conventionally follow his method of grouping. The Papyrus Prisse, a Middle Kingdom source, supports the fact that King Huni was indeed Sneferu’s predecessor. It states that "the majesty of the king of Upper and Lower Egypt, Huni, came to the landing place (i.e., died), and the majesty of the king of Upper and Lower Egypt, Sneferu, was raised up as a beneficent king in this entire land..."[13] Aside from Sneferu’s succession, we learn from this text that later generations considered him to be a "beneficent" ruler. This idea may stem from the etymology of the king’s name, for it can be interpreted as the infinitive "to make beautiful."[14] It is uncertain whether Huni was Sneferu’s father; however, the Cairo Annals Stone denotes that his mother may have been a woman named Meresankh.[15]		Hetepheres I was Sneferu’s main wife and the mother of Khufu,[16] the builder of the Great Pyramid on the Giza Plateau.		Sons of Sneferu:		Daughters of Sneferu:		The most well known monuments from Sneferu’s reign are the three pyramids he is considered to have built in Dahshur: the Bent Pyramid, the Red Pyramid and Meidum (the Meidum pyramid). Under Sneferu, there was a major evolution in monumental pyramid structures, which would lead to Khufu’s Great Pyramid, which would be seen as the pinnacle of the Egyptian Old Kingdom’s majesty and splendour, and as one of the Seven Wonders of the Ancient World.		The first of Sneferu’s massive undertakings is the Pyramid at Meidum. There is some debate among scholars as to Sneferu’s claim to the Meidum pyramid, and many credit its origin to King Huni. Nonetheless, the pyramid is a remarkable example of the progression of technology and ideology surrounding the king’s burial site.		The immense stone structure serves as physical testimony to the transition from the stepped pyramid structure to that of a "true" pyramid structure. Archaeological investigations of the pyramid show that it was first conceived as a seven-stepped structure, built in a similar manner to the Djoser complex at Saqqara. Modifications later were made to add another platform, and at an even later stage limestone facing was added to create the smooth, angled finish characteristic of a "true" pyramid.[23] Complete with a descending northern passage, two underground chambers, and a burial vault, the pyramid mainly follows the conventions of previous tombs in most aspects other than one: instead of being situated underneath the colossal structure, the burial chamber is built directly within the main body albeit very near ground level.[24]		The Bent Pyramid, also known as the Rhomboidal or Blunted Pyramid, attests to an even greater increase in architectural innovations. As the name suggests, the angle of the inclination changes from 55° to about 43° in the upper levels of the pyramid. It is likely that the pyramid initially was not designed to be built this way, but was modified during construction due to unstable accretion layers. As a means of stabilising the structure, the top layers were laid horizontally, marking the abandonment of the step pyramid concept.[25] The internal components of the Rhomboidal pyramid have also evolved. There are two entrances, one from the north and another from the west. The subterranean chambers are much larger, and distinguished by corbel walls and ceilings with more complex diagonal portcullis systems in place. J.P Lepre asserts:		It is apparent that with the interior design of the Bent Pyramid the architect was groping and experimenting, taking maximum advantage of the huge volume of the monument (50 million cubic feet), the largest pyramid constructed to that date.[26]		The satellite pyramid complementing Sneferu’s Bent Pyramid introduces more change in the architecture of the time, when the passageway is built ascending westward (as opposed to the conventionally descending northward direction of the passages of previously build pyramids) towards the burial chambers.[27]		With the increase of innovation in Sneferu's building projects, one expects that his last pyramid, the Red Pyramid, will show the greatest complexity and change in architecture yet. Upon first glance, one may be disappointed seeing that the construction of the Red Pyramid seemingly is simpler than its predecessor. Lepre points out that some of the internal innovations that the previous pyramids boast seem to be missing in the king’s last monument. Although the chambers and burial vaults are all present in the monument’s main body, no ascending passageway has been excavated, nor is there evidence of a western entrance or diagonal portcullis. Although the absence of these features have dissuaded many archaeologists from further studying the Red Pyramid, Lepre is convinced that there are secret chambers waiting to be uncovered within the stone superstructure. Considering that the remains of King Sneferu have not yet been found, it still may be possible that his sarcophagus and mummy lie hidden in his mysterious last structure. Lepre claims: "the Red pyramid remains one of the chief pyramids that may possibly contain secret chambers, not the least of which may be the true burial chamber of King Sneferu himself." [28] Whether or not this conjecture is true is left to modern archaeologists to determine.		Sneferu’s architectural innovations served as a catalyst for later pyramid builders to build on. The first king of the fourth dynasty set a challenging precedent for his successors to match, and only Khufu’s Great Pyramid can rival Sneferu’s accomplishments. As time progressed and ideology changed in Ancient Egypt, the monuments of the kings decreased greatly in size. As the Pyramid of Menkaure is only a fraction of the size of the previous pyramids, the focus of Egyptian ideology might have shifted from the worship of the king to the direct worship of the sun god, Ra.[29]		To enable Sneferu to undertake such massive building projects, he would have had to secure an extensive store of labour and materials. According to Guillemette Andreu, this is where the king’s foreign policy played a large part. Sneferu’s conquests into Libya and Nubia served two purposes: the first goal was to establish an extensive labour force, and the second goal was to gain access to the raw materials and special products that were available in these countries.[29] This is alluded to in the Palermo stone:		According to this inscription, Sneferu was able to capture large numbers of people from other nations, make them his prisoners and then add them into his labour force. During his raids into Nubia and Libya, he also captured cattle for the sustenance of his massive labour force. Such incursions must have been incredibly devastating to the populations of the raided countries, and it is suggested that the campaigns into Nubia may have contributed to the dissemination of the Nubian A-Group culture of that region. Sneferu's military efforts in Libya led to the capture of 11,000 prisoners and 13,100 head of cattle.[31] Aside from the extensive import of cedar (most likely from Lebanon) described above, there is evidence of activity in the turquoise mines on the Sinai Peninsula.[32] There would also have been large-scale quarrying projects to provide Sneferu with the stone he needed for his pyramids.		Sneferu's ancient cedar wood ship Praise of the Two Lands is the first known instance of a ship being referred to by name.[33]		
Martin E. P. "Marty" Seligman (born August 12, 1942) is an American psychologist, educator, and author of self-help books. Since the late 90s, Seligman has been an avid promoter within the scientific community for the field of positive psychology.[1] His theory of learned helplessness is popular among scientific and clinical psychologists.[2] A Review of General Psychology survey, published in 2002, ranked Seligman as the 31st most cited psychologist of the 20th century.[3]		Seligman is the Zellerbach Family Professor of Psychology in the University of Pennsylvania's Department of Psychology. He was previously the Director of the Clinical Training Program in the department, and earlier taught at Cornell University.[4] He is the director of the university's Positive Psychology Center.[1] Seligman was elected President of the American Psychological Association for 1998.[5] He is the founding editor-in-chief of Prevention and Treatment (the APA electronic journal) and is on the board of advisers of Parents magazine.		Seligman has written about positive psychology topics in books such as The Optimistic Child, Child's Play, Learned Optimism, and Authentic Happiness. His most recent book, Flourish, was published in 2011.						Seligman was born in Albany, New York. He was educated at a public school and at The Albany Academy. He earned a bachelor's degree in philosophy at Princeton University in 1964, graduating Summa Cum Laude. Seligman turned down a scholarship to study analytic philosophy at Oxford University, or animal experimental psychology at the University of Pennsylvania and accepted an offer to attend the University of Pennsylvania to study psychology.[6] He earned his Ph.D. in psychology at University of Pennsylvania in 1967. On June 2, 1989 Seligman received an honorary doctorate from the Faculty of Social Sciences at Uppsala University, Sweden [7]		Seligman's foundational experiments and theory of "learned helplessness" began at University of Pennsylvania in 1967, as an extension of his interest in depression. Quite by accident, Seligman and colleagues discovered that the experimental conditioning protocol they used with dogs led to behaviors which were unexpected, in that under the experimental conditions, the recently conditioned dogs did not respond to opportunities to learn to escape from an unpleasant situation.[8] Seligman developed the theory further, finding learned helplessness to be a psychological condition in which a human being or an animal has learned to act or behave helplessly in a particular situation — usually after experiencing some inability to avoid an adverse situation — even when it actually has the power to change its unpleasant or even harmful circumstance. Seligman saw a similarity with severely depressed patients, and argued that clinical depression and related mental illnesses result in part from a perceived absence of control over the outcome of a situation.[9] In later years, alongside Abramson, Seligman reformulated his theory of learned helplessness to include attributional style.[10]		James Elmer Mitchell was involved in the development of enhanced interrogation techniques. Mitchell attended a meeting at Seligman's home regarding the September 11 attacks and the psychology of capitulation in December 2001. Mitchell also attended a three-hour talk from Seligman sponsored by the JPRA on learned helplessness and torture resistance at Naval Base San Diego in May 2002. The Senate Intelligence Committee report on CIA torture stated that the enhanced interrogation techniques were based on the theory of learned helplessness. Seligman has stated that his involvement does not extend beyond those two events, he does not support torture, and he is grieved and horrified that good science may have been used for such a bad and dubious purpose as torture.[11][12][13][14][15][16][17]		Seligman worked with Christopher Peterson to create what they describe as a 'positive' counterpart to the Diagnostic and Statistical Manual of Mental Disorders (DSM). While the DSM focuses on what can go wrong, Character Strengths and Virtues is designed to look at what can go right. In their research they looked across cultures and across millennia to attempt to distill a manageable list of virtues that have been highly valued from ancient China and India, through Greece and Rome, to contemporary Western cultures. Their list includes six character strengths: wisdom/knowledge, courage, humanity, justice, temperance, and transcendence. Each of these has three to five sub-entries; for instance, temperance includes forgiveness, humility, prudence, and self-regulation.[18] The authors do not believe that there is a hierarchy for the six virtues; no one is more fundamental than or a precursor to the others.		In July 2011, Seligman encouraged British Prime Minister David Cameron to look into well-being as well as financial wealth in ways of assessing the prosperity of a nation. On July 6, 2011, Seligman appeared on Newsnight and was interviewed by Jeremy Paxman about his ideas and his interest in the concept of well-being.		In his latest book, Flourish, Seligman articulated an account of how he measures well-being, and titled this work, "Well-Being Theory".[19] He concludes that there are five elements to "well-being", which fall under the mnemonic PERMA:[19]		From Martin Seligmans book:		"Each element of well-being must itself have three properties to count as an element:		These theories have not been empirically validated.		The Master of Applied Positive Psychology (MAPP) program at the University of Pennsylvania was established under the leadership of Seligman as the first educational initiative of the Positive Psychology Center in 2003.[20]		Seligman plays bridge and finished second in the 1998 installment of one of the three major North American pair championships, the Blue Ribbon Pairs, as well as having won over 50 regional championships.[21]		Seligman has seven children, four grandchildren, and two dogs. He and his second wife, Mandy, live in a house that was once occupied by Eugene Ormandy. They have home-schooled five of their seven children.[22]		Seligman was inspired by the work of the psychiatrist Aaron T. Beck at the University of Pennsylvania in refining his own cognitive techniques and exercises.[23]		
Near East (c. 3300–1200 BC)		South Asia (c. 3000–1200 BC)		Europe (c. 3200–600 BC)		China (c. 2000–700 BC)		arsenical bronze writing, literature sword, chariot		Ancient Egypt was a civilization of ancient Northeastern Africa, concentrated along the lower reaches of the Nile River in the place that is now the country Egypt. It is one of six historic civilizations to arise independently. Egyptian civilization followed prehistoric Egypt and coalesced around 3150 BC (according to conventional Egyptian chronology)[1] with the political unification of Upper and Lower Egypt under Menes (often identified with Narmer).[2] The history of ancient Egypt occurred as a series of stable kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.		Egypt reached the pinnacle of its power in the New Kingdom, during the Ramesside period, where it rivalled the Hittite Empire, Assyrian Empire and Mitanni Empire, after which it entered a period of slow decline. Egypt was invaded or conquered by a succession of foreign powers, such as the Canaanites/Hyksos, Libyans, the Nubians, the Assyrians, Babylonians, the Achaemenid Persians, and the Macedonians in the Third Intermediate Period and the Late Period of Egypt. In the aftermath of Alexander the Great's death, one of his generals, Ptolemy Soter, established himself as the new ruler of Egypt. This Greek Ptolemaic Kingdom ruled Egypt until 30 BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province.[3]		The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley for agriculture. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to defeat foreign enemies and assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.[4][5]		The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known planked boats,[6] Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites.[7] Egypt left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for centuries. A new-found respect for antiquities and excavations in the early modern period by Europeans and Egyptians led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.[8]						The Nile has been the lifeline of its region for much of human history.[9] The fertile floodplain of the Nile gave humans the opportunity to develop a settled agricultural economy and a more sophisticated, centralized society that became a cornerstone in the history of human civilization.[10] Nomadic modern human hunter-gatherers began living in the Nile valley through the end of the Middle Pleistocene some 120,000 years ago. By the late Paleolithic period, the arid climate of Northern Africa became increasingly hot and dry, forcing the populations of the area to concentrate along the river region.		In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were covered in treed savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were first domesticated.[11]		By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was the Badari, which probably originated in the Western Desert; it was known for its high quality ceramics, stone tools, and its use of copper.[12]		The Badari was followed by the Amratian (Naqada I) and Gerzeh (Naqada II) cultures,[13] which brought a number of technological improvements. As early as the Naqada I Period, predynastic Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes.[14] In Naqada II times, early evidence exists of contact with the Near East, particularly Canaan and the Byblos coast.[15] Over a period of about 1,000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley.[16] Establishing a power center at Hierakonpolis, and later at Abydos, Naqada III leaders expanded their control of Egypt northwards along the Nile.[17] They also traded with Nubia to the south, the oases of the western desert to the west, and the cultures of the eastern Mediterranean and Near East to the east.[17] Royal Nubian burials at Qustul produced artifacts bearing the oldest-known examples of Egyptian dynastic symbols, such as the white crown of Egypt and falcon.[18][19]		The Naqada culture manufactured a diverse selection of material goods, reflective of the increasing power and wealth of the elite, as well as societal personal-use items, which included combs, small statuary, painted pottery, high quality decorative stone vases, cosmetic palettes, and jewelry made of gold, lapis, and ivory. They also developed a ceramic glaze known as faience, which was used well into the Roman Period to decorate cups, amulets, and figurines.[20] During the last predynastic phase, the Naqada culture began using written symbols that eventually were developed into a full system of hieroglyphs for writing the ancient Egyptian language.[21]		The Early Dynastic Period was approximately contemporary to the early Sumerian-Akkadian civilisation of Mesopotamia and of ancient Elam. The third-century BC Egyptian priest Manetho grouped the long line of pharaohs from Menes to his own time into 30 dynasties, a system still used today.[22] He chose to begin his official history with the king named "Meni" (or Menes in Greek) who was believed to have united the two kingdoms of Upper and Lower Egypt (around 3100 BC).[23]		The transition to a unified state happened more gradually than ancient Egyptian writers represented, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have been the pharaoh Narmer, who is depicted wearing royal regalia on the ceremonial Narmer Palette, in a symbolic act of unification.[24] In the Early Dynastic Period about 3150 BC, the first of the Dynastic pharaohs solidified control over lower Egypt by establishing a capital at Memphis, from which he could control the labour force and agriculture of the fertile delta region, as well as the lucrative and critical trade routes to the Levant. The increasing power and wealth of the pharaohs during the early dynastic period was reflected in their elaborate mastaba tombs and mortuary cult structures at Abydos, which were used to celebrate the deified pharaoh after his death.[25] The strong institution of kingship developed by the pharaohs served to legitimize state control over the land, labour, and resources that were essential to the survival and growth of ancient Egyptian civilization.[26]		Major advances in architecture, art, and technology were made during the Old Kingdom, fueled by the increased agricultural productivity and resulting population, made possible by a well-developed central administration.[28] Some of ancient Egypt's crowning achievements, the Giza pyramids and Great Sphinx, were constructed during the Old Kingdom. Under the direction of the vizier, state officials collected taxes, coordinated irrigation projects to improve crop yield, drafted peasants to work on construction projects, and established a justice system to maintain peace and order.[29]		Along with the rising importance of a central administration arose a new class of educated scribes and officials who were granted estates by the pharaoh in payment for their services. Pharaohs also made land grants to their mortuary cults and local temples, to ensure that these institutions had the resources to worship the pharaoh after his death. Scholars believe that five centuries of these practices slowly eroded the economic power of the pharaoh, and that the economy could no longer afford to support a large centralized administration.[30] As the power of the pharaoh diminished, regional governors called nomarchs began to challenge the supremacy of the pharaoh. This, coupled with severe droughts between 2200 and 2150 BC,[31] is assumed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.[32]		After Egypt's central government collapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. Regional governors could not rely on the king for help in times of crisis, and the ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the pharaoh, used their new-found independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer—which was demonstrated by larger and better burials among all social classes.[33] In bursts of creativity, provincial artisans adopted and adapted cultural motifs formerly restricted to the royalty of the Old Kingdom, and scribes developed literary styles that expressed the optimism and originality of the period.[34]		Free from their loyalties to the pharaoh, local rulers began competing with each other for territorial control and political power. By 2160 BC, rulers in Herakleopolis controlled Lower Egypt in the north, while a rival clan based in Thebes, the Intef family, took control of Upper Egypt in the south. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055 BC the northern Theban forces under Nebhepetre Mentuhotep II finally defeated the Herakleopolitan rulers, reuniting the Two Lands. They inaugurated a period of economic and cultural renaissance known as the Middle Kingdom.[35]		The pharaohs of the Middle Kingdom restored the country's prosperity and stability, thereby stimulating a resurgence of art, literature, and monumental building projects.[36] Mentuhotep II and his Eleventh Dynasty successors ruled from Thebes, but the vizier Amenemhat I, upon assuming kingship at the beginning of the Twelfth Dynasty around 1985 BC, shifted the nation's capital to the city of Itjtawy, located in Faiyum.[37] From Itjtawy, the pharaohs of the Twelfth Dynasty undertook a far-sighted land reclamation and irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory in Nubia that was rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls-of-the-Ruler", to defend against foreign attack.[38]		With the pharaohs' having secured military and political security and vast agricultural and mineral wealth, the nation's population, arts, and religion flourished. In contrast to elitist Old Kingdom attitudes towards the gods, the Middle Kingdom experienced an increase in expressions of personal piety and what could be called a democratization of the afterlife, in which all people possessed a soul and could be welcomed into the company of the gods after death.[39] Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style.[34] The relief and portrait sculpture of the period captured subtle, individual details that reached new heights of technical perfection.[40]		The last great ruler of the Middle Kingdom, Amenemhat III, allowed Semitic-speaking Canaanite settlers from the Near East into the delta region to provide a sufficient labour force for his especially active mining and building campaigns. These ambitious building and mining activities, however, combined with severe Nile floods later in his reign, strained the economy and precipitated the slow decline into the Second Intermediate Period during the later Thirteenth and Fourteenth dynasties. During this decline, the Canaanite settlers began to seize control of the delta region, eventually coming to power in Egypt as the Hyksos.[41]		Around 1785 BC, as the power of the Middle Kingdom pharaohs weakened, a Western Asian people called the Hyksos had already settled in the Eastern Delta town of Avaris, seized control of Egypt, and forced the central government to retreat to Thebes. The pharaoh was treated as a vassal and expected to pay tribute.[42] The Hyksos ("foreign rulers") retained Egyptian models of government and identified as pharaohs, thus integrating Egyptian elements into their culture. They and other invaders introduced new tools of warfare into Egypt, most notably the composite bow and the horse-drawn chariot.[43]		After their retreat, the native Theban kings found themselves trapped between the Canaanite Hyksos ruling the north and the Hyksos' Nubian allies, the Kushites, to the south of Egypt. After years of vassalage, Thebes gathered enough strength to challenge the Hyksos in a conflict that lasted more than 30 years, until 1555 BC.[42] The pharaohs Seqenenre Tao II and Kamose were ultimately able to defeat the Nubians to the south of Egypt, but failed to defeat the Hyksos. That task fell to Kamose's successor, Ahmose I, who successfully waged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. He established a new dynasty. In the New Kingdom that followed, the military became a central priority for the pharaohs seeking to expand Egypt's borders and attempting to gain mastery of the Near East.[44]		The New Kingdom pharaohs established a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbours, including the Mitanni Empire, Assyria, and Canaan. Military campaigns waged under Tuthmosis I and his grandson Tuthmosis III extended the influence of the pharaohs to the largest empire Egypt had ever seen. Between their reigns, Hatshepsut generally promoted peace and restored trade routes lost during the Hyksos occupation, as well as expanding to new regions. When Tuthmosis III died in 1425 BC, Egypt had an empire extending from Niya in north west Syria to the fourth waterfall of the Nile in Nubia, cementing loyalties and opening access to critical imports such as bronze and wood.[45]		The New Kingdom pharaohs began a large-scale building campaign to promote the god Amun, whose growing cult was based in Karnak. They also constructed monuments to glorify their own achievements, both real and imagined. The Karnak temple is the largest Egyptian temple ever built.[46] The pharaoh Hatshepsut used such hyperbole and grandeur during her reign of almost twenty-two years.[47] Her reign was very successful, marked by an extended period of peace and wealth-building, trading expeditions to Punt, restoration of foreign trade networks, and great building projects, including an elegant mortuary temple that rivaled the Greek architecture of a thousand years later, a colossal pair of obelisks, and a chapel at Karnak. Despite her achievements, Amenhotep II, the heir to Hatshepsut's nephew-stepson Tuthmosis III, sought to erase her legacy near the end of his father's reign and throughout his, touting many of her accomplishments as his.[48] He also tried to change many established traditions that had developed over the centuries, which some suggest was a futile attempt to prevent other women from becoming pharaoh and to curb their influence in the kingdom.		Around 1350 BC, the stability of the New Kingdom seemed threatened further when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name to Akhenaten, he touted the previously obscure sun deity Aten as the supreme deity, suppressed the worship of most other deities, and attacked the power of the temple that had become dominated by the priests of Amun in Thebes, whom he saw as corrupt.[49] Moving the capital to the new city of Akhetaten (modern-day Amarna), Akhenaten turned a deaf ear to events in the Near East (where the Hittites, Mitanni, and Assyrians were vying for control). He was devoted to his new religion and artistic style. After his death, the cult of the Aten was quickly abandoned, the priests of Amun soon regained power and returned the capital to Thebes. Under their influence the subsequent pharaohs Tutankhamun, Ay, and Horemheb worked to erase all mention of Akhenaten's heresy, now known as the Amarna Period.[50]		Around 1279 BC, Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history.[51] A bold military leader, Ramesses II led his army against the Hittites in the Battle of Kadesh (in modern Syria) and, after fighting to a stalemate, finally agreed to the first recorded peace treaty, around 1258 BC.[52] With both the Egyptians and Hittite Empire proving unable to gain the upper hand over one another, and both powers also fearful of the expanding Middle Assyrian Empire, Egypt withdrew from much of the Near East. The Hittites were thus left to compete unsuccessfully with the powerful Assyrians and the newly arrived Phrygians.		Egypt's wealth, however, made it a tempting target for invasion, particularly by the Libyan Berbers to the west, and the Sea Peoples, a conjectured[53][54] confederation of seafarers from the Aegean Sea. Initially, the military was able to repel these invasions, but Egypt eventually lost control of its remaining territories in southern Canaan, much of it falling to the Assyrians. The effects of external threats were exacerbated by internal problems such as corruption, tomb robbery, and civil unrest. After regaining their power, the high priests at the temple of Amun in Thebes accumulated vast tracts of land and wealth, and their expanded power splintered the country during the Third Intermediate Period.[55]		Following the death of Ramesses XI in 1078 BC, Smendes assumed authority over the northern part of Egypt, ruling from the city of Tanis. The south was effectively controlled by the High Priests of Amun at Thebes, who recognized Smendes in name only.[56] During this time, Berber tribes from what was later to be called Libya had been settling in the western delta, and the chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta under Shoshenq I in 945 BC, founding the Libyan Berber, or Bubastite, dynasty that ruled for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions.		In the mid-ninth century BC, Egypt made a failed attempt to once more gain a foothold in Western Asia. Osorkon II of Egypt, along with a large alliance of nations and peoples, including Persia, Israel, Hamath, Phoenicia/Canaan, the Arabs, Arameans, and neo Hittites among others, engaged in the Battle of Karkar against the powerful Assyrian king Shalmaneser III in 853 BC. However, this coalition of powers failed and the Neo Assyrian Empire continued to dominate Western Asia.		Libyan Berber control began to erode as a rival native dynasty in the delta arose under Leontopolis. Also, the Nubians of the Kushites threatened Egypt from the lands to the south.[57]		Drawing on millennia of interaction (trade, acculturation, occupation, assimilation, and war[58]) with Egypt,[59] the Kushite king Piye left his Nubian capital of Napata and invaded Egypt around 727 BC. Piye easily seized control of Thebes and eventually the Nile Delta.[60] He recorded the episode on his stela of victory. Piye set the stage for subsequent Twenty-fifth dynasty pharaohs,[61] such as Taharqa, to reunite the "Two lands" of Northern and Southern Egypt. The Nile valley empire was as large as it had been since the New Kingdom.		The Twenty-fifth dynasty ushered in a renaissance period for ancient Egypt.[62] Religion, the arts, and architecture were restored to their glorious Old, Middle, and New Kingdom forms. Pharaohs, such as Taharqa, built or restored temples and monuments throughout the Nile valley, including at Memphis, Karnak, Kawa, Jebel Barkal, etc.[63] It was during the Twenty-fifth dynasty that there was the first widespread construction of pyramids (many in modern Sudan) in the Nile Valley since the Middle Kingdom.[64][65][66]		Piye made various unsuccessful attempts to extend Egyptian influence in the Near East, then controlled by Assyria. In 720 BC, he sent an army in support of a rebellion against Assyria, which was taking place in Philistia and Gaza. However, Piye was defeated by Sargon II and the rebellion failed. In 711 BC, Piye again supported a revolt against Assyria by the Israelites of Ashdod and was once again defeated by the Assyrian king Sargon II. Subsequently, Piye was forced from the Near East.[67]		From the 10th century BC onwards, Assyria fought for control of the southern Levant. Frequently, cities and kingdoms of the southern Levant appealed to Egypt for aid in their struggles against the powerful Assyrian army. Taharqa enjoyed some initial success in his attempts to regain a foothold in the Near East. Taharqa aided the Judean King Hezekiah when Hezekiah and Jerusalem was besieged by the Assyrian king, Sennacherib. Scholars disagree on the primary reason for Assyria's abandonment of their siege on Jerusalem. Reasons for the Assyrian withdrawal range from conflict with the Egyptian/Kushite army to divine intervention to surrender to disease.[68] Henry Aubin argues that the Kushite/Egyptian army saved Jerusalem from the Assyrians and prevented the Assyrians from returning to capture Jerusalem for the remainder of Sennacherib's life (20 years).[69] Some argue that disease was the primary reason for failing to actually take the city; however, Senacherib's annals claim Judah was forced into tribute regardless.[70]		Sennacherib had been murdered by his own sons for destroying the rebellious city of Babylon, a city sacred to all Mesopotamians, the Assyrians included. In 674 BC Esarhaddon launched a preliminary incursion into Egypt; however, this attempt was repelled by Taharqa.[71] However, in 671 BC, Esarhaddon launched a full-scale invasion. Part of his army stayed behind to deal with rebellions in Phoenicia, and Israel. The remainder went south to Rapihu, then crossed the Sinai, and entered Egypt. Esarhaddon decisively defeated Taharqa, took Memphis, Thebes and all the major cities of Egypt, and Taharqa was chased back to his Nubian homeland. Esarhaddon now called himself "king of Egypt, Patros, and Kush", and returned with rich booty from the cities of the delta; he erected a victory stele at this time, and paraded the captive Prince Ushankhuru, the son of Taharqa in Nineveh. Esarhaddon stationed a small army in northern Egypt and describes how "All Ethiopians (read Nubians/Kushites) I deported from Egypt, leaving not one left to do homage to me".[72] He installed native Egyptian princes throughout the land to rule on his behalf.[73] The conquest by Esarhaddon effectively marked the end of the short lived Kushite Empire.		However, the native Egyptian rulers installed by Esarhaddon were unable to retain full control of the whole country for long. Two years later, Taharqa returned from Nubia and seized control of a section of southern Egypt as far north as Memphis. Esarhaddon prepared to return to Egypt and once more eject Taharqa; however, he fell ill and died in his capital, Nineveh, before he left Assyria. His successor, Ashurbanipal, sent an Assyrian general named Sha-Nabu-shu with a small, but well trained army, which conclusively defeated Taharqa at Memphis and once more drove him from Egypt. Taharqa died in Nubia two years later.		His successor, Tanutamun, also made a failed attempt to regain Egypt for Nubia. He successfully defeated Necho, the native Egyptian puppet ruler installed by Ashurbanipal, taking Thebes in the process. The Assyrians then sent a large army southwards. Tantamani (Tanutamun) was heavily routed and fled back to Nubia. The Assyrian army sacked Thebes to such an extent it never truly recovered. A native ruler, Psammetichus I was placed on the throne, as a vassal of Ashurbanipal, and the Nubians were never again to pose a threat to either Assyria or Egypt.[74]		With no permanent plans for conquest, the Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of the Twenty-sixth Dynasty. By 653 BC, the Saite king Psamtik I (taking advantage of the fact that Assyria was involved in a fierce war conquering Elam and that few Assyrian troops were stationed in Egypt) was able to free Egypt relatively peacefully from Assyrian vassalage with the help of Lydian and Greek mercenaries, the latter of whom were recruited to form Egypt's first navy. Psamtik and his successors however were careful to maintain peaceful relations with Assyria. Greek influence expanded greatly as the city of Naukratis became the home of Greeks in the delta.		In 609 BC Necho II went to war with Babylonia, the Chaldeans, the Medians and the Scythians in an attempt to save Assyria, which after a brutal civil war was being overrun by this coalition of powers. However, the attempt to save Egypt's former masters failed. The Egyptians delayed intervening too long, and Nineveh had already fallen and King Sin-shar-ishkun was dead by the time Necho II sent his armies northwards. However, Necho easily brushed aside the Israelite army under King Josiah but he and the Assyrians then lost a battle at Harran to the Babylonians, Medes and Scythians. Necho II and Ashur-uballit II of Assyria were finally defeated at Carchemish in Aramea (modern Syria) in 605 BC. The Egyptians remained in the area for some decades, struggling with the Babylonian kings Nabopolassar and Nebuchadnezzar II for control of portions of the former Assyrian Empire in The Levant. However, they were eventually driven back into Egypt, and Nebuchadnezzar II even briefly invaded Egypt itself in 567 BC.[70] The Saite kings based in the new capital of Sais witnessed a brief but spirited resurgence in the economy and culture, but in 525 BC, the powerful Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa in Persia (modern Iran), leaving Egypt under the control of a satrapy. A few temporarily successful revolts against the Persians marked the fifth century BC, but Egypt was never able to permanently overthrow the Persians.[75]		Following its annexation by Persia, Egypt was joined with Cyprus and Phoenicia (modern Lebanon) in the sixth satrapy of the Achaemenid Persian Empire. This first period of Persian rule over Egypt, also known as the Twenty-seventh dynasty, ended after more than one-hundred years in 402 BC, and from 380 to 343 BC the Thirtieth Dynasty ruled as the last native royal house of dynastic Egypt, which ended with the kingship of Nectanebo II. A brief restoration of Persian rule, sometimes known as the Thirty-first Dynasty, began in 343 BC, but shortly after, in 332 BC, the Persian ruler Mazaces handed Egypt over to the Macedonian ruler Alexander the Great without a fight.[76]		In 332 BC, Alexander the Great conquered Egypt with little resistance from the Persians and was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, the Macedonian Ptolemaic Kingdom, was based on an Egyptian model and based in the new capital city of Alexandria. The city showcased the power and prestige of Hellenistic rule, and became a seat of learning and culture, centered at the famous Library of Alexandria.[77] The Lighthouse of Alexandria lit the way for the many ships that kept trade flowing through the city—as the Ptolemies made commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority.[78]		Hellenistic culture did not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek and Egyptian gods were syncretized into composite deities, such as Serapis, and classical Greek forms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and the powerful mob of Alexandria that formed after the death of Ptolemy IV.[79] In addition, as Rome relied more heavily on imports of grain from Egypt, the Romans took great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful Syriac opponents from the Near East made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.[80]		Egypt became a province of the Roman Empire in 30 BC, following the defeat of Marc Antony and Ptolemaic Queen Cleopatra VII by Octavian (later Emperor Augustus) in the Battle of Actium. The Romans relied heavily on grain shipments from Egypt, and the Roman army, under the control of a prefect appointed by the Emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period.[81] Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome.[82]		Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued.[83] The art of mummy portraiture flourished, and some Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians.[83]		From the mid-first century AD, Christianity took root in Egypt and it was originally seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from Egyptian Religion and Greco-Roman religion and threatened popular religious traditions. This led to the persecution of converts to Christianity, culminating in the great purges of Diocletian starting in 303, but eventually Christianity won out.[84] In 391 the Christian Emperor Theodosius introduced legislation that banned pagan rites and closed temples.[85] Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed.[86] As a consequence, Egypt's native religious culture was continually in decline. While the native population certainly continued to speak their language, the ability to read hieroglyphic writing slowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted to churches or abandoned to the desert.[87]		The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the supreme military commander and head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, the vizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and the archives.[88] At a regional level, the country was divided into as many as 42 administrative regions called nomes each governed by a nomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were they houses of worship, but were also responsible for collecting and storing the nation's wealth in a system of granaries and treasuries administered by overseers, who redistributed grain and goods.[89]		Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not use coinage until the Late period,[90] they did use a type of money-barter system,[91] with standard sacks of grain and the deben, a weight of roughly 91 grams (3 oz) of copper or silver, forming a common denominator.[92] Workers were paid in grain; a simple laborer might earn 5½ sacks (200 kg or 400 lb) of grain per month, while a foreman might earn 7½ sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140 deben.[92] Grain could be traded for other goods, according to the fixed price list.[92] During the fifth century BC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces of precious metal rather than true money, but in the following centuries international traders came to rely on coinage.[93]		Egyptian society was highly stratified, and social status was expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, or noble family that owned the land.[94] Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in a corvée system.[95] Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, known as the "white kilt class" in reference to the bleached linen garments that served as a mark of their rank.[96] The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. Slavery was known in ancient Egypt, but the extent and prevalence of its practice are unclear.[97]		The ancient Egyptians viewed men and women, including people from all social classes except slaves, as essentially equal under the law, and even the lowliest peasant was entitled to petition the vizier and his court for redress.[98] Although slaves were mostly used as indentured servants, they were able to buy and sell their servitude, work their way to freedom or nobility, and were usually treated by doctors in the workplace.[99] Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices and opportunities for achievement. Women such as Hatshepsut and Cleopatra VII even became pharaohs, while others wielded power as Divine Wives of Amun. Despite these freedoms, ancient Egyptian women did not often take part in official roles in the administration, served only secondary roles in the temples, and were not as likely to be as educated as men.[98]		The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to as Ma'at.[88] Although no legal codes from ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes.[98] Local councils of elders, known as Kenbet in the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes.[88] More serious cases involving murder, major land transactions, and tomb robbery were referred to the Great Kenbet, over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference.[100]		Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, or impaling the criminal on a stake. Punishment could also be extended to the criminal's family.[88] Beginning in the New Kingdom, oracles played a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgment by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or an ostracon.[101]		A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the rich fertile soil resulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits. Land management was crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned.[102]		Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons: Akhet (flooding), Peret (planting), and Shemu (harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, the growing season lasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops.[103] From March to May, farmers used sickles to harvest their crops, which were then threshed with a flail to separate the straw from the grain. Winnowing removed the chaff from the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use.[104]		The ancient Egyptians cultivated emmer and barley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer.[105] Flax plants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets of linen and to make clothing. Papyrus growing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.[106]		The Egyptians believed that a balanced relationship between people and animals was an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole.[107] Animals, both domesticated and wild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians. Cattle were the most important livestock; the administration collected taxes on livestock in regular censuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs. Poultry, such as ducks, geese, and pigeons, were captured in nets and bred on farms, where they were force-fed with dough to fatten them.[108] The Nile provided a plentiful source of fish. Bees were also domesticated from at least the Old Kingdom, and provided both honey and wax.[109]		The ancient Egyptians used donkeys and oxen as beasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual.[108] Horses were introduced by the Hyksos in the Second Intermediate Period. Camels, although known from the New Kingdom, were not used as beasts of burden until the Late Period. There is also evidence to suggest that elephants were briefly utilized in the Late Period but largely abandoned due to lack of grazing land.[108] Dogs, cats, and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such as Sub-Saharan African lions,[110] were reserved for royalty. Herodotus observed that the Egyptians were the only people to keep their animals with them in their houses.[107] During the Predynastic and Late periods, the worship of the gods in their animal form was extremely popular, such as the cat goddess Bastet and the ibis god Thoth, and these animals were bred in large numbers on farms for the purpose of ritual sacrifice.[111]		Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. These natural resources allowed the ancient Egyptians to build monuments, sculpt statues, make tools, and fashion jewelry.[112] Embalmers used salts from the Wadi Natrun for mummification, which also provided the gypsum needed to make plaster.[113] Ore-bearing rock formations were found in distant, inhospitable wadis in the eastern desert and the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensive gold mines in Nubia, and one of the first maps known is of a gold mine in this region. The Wadi Hammamat was a notable source of granite, greywacke, and gold. Flint was the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose.[114] Ancient Egyptians were among the first to use minerals such as sulfur as cosmetic substances.[115]		The Egyptians worked deposits of the lead ore galena at Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces from malachite ore mined in the Sinai.[116] Workers collected gold by washing the nuggets out of sediment in alluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were utilized in the Late Period.[117] High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the eastern desert. Deposits of decorative stones such as porphyry, greywacke, alabaster, and carnelian dotted the eastern desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits of emeralds in Wadi Sikait and amethyst in Wadi el-Hudi.[118]		The ancient Egyptians engaged in trade with their foreign neighbors to obtain rare, exotic goods not found in Egypt. In the Predynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs.[119] An Egyptian colony stationed in southern Canaan dates to slightly before the First Dynasty.[120] Narmer had Egyptian pottery produced in Canaan and exported back to Egypt.[121]		By the Second Dynasty at latest, ancient Egyptian trade with Byblos yielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade with Punt provided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons.[122] Egypt relied on trade with Anatolia for essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stone lapis lazuli, which had to be imported from far-away Afghanistan. Egypt's Mediterranean trade partners also included Greece and Crete, which provided, among other goods, supplies of olive oil.[123] In exchange for its luxury imports and raw materials, Egypt mainly exported grain, gold, linen, and papyrus, in addition to other finished goods including glass and stone objects.[124]		The Egyptian language is a northern Afro-Asiatic language closely related to the Berber and Semitic languages.[125] It has the second longest history of any language (after Sumerian), having been written from c. 3200 BC to the Middle Ages and remaining as a spoken language for longer. The phases of ancient Egyptian are Old Egyptian, Middle Egyptian (Classical Egyptian), Late Egyptian, Demotic and Coptic.[126] Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes.[127]		Ancient Egyptian was a synthetic language, but it became more analytic later on. Late Egyptian developed prefixal definite and indefinite articles, which replaced the older inflectional suffixes. There was a change from the older verb–subject–object word order to subject–verb–object.[128] The Egyptian hieroglyphic, hieratic, and demotic scripts were eventually replaced by the more phonetic Coptic alphabet. Coptic is still used in the liturgy of the Egyptian Orthodox Church, and traces of it are found in modern Egyptian Arabic.[129]		Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These include pharyngeal and emphatic consonants, voiced and voiceless stops, voiceless fricatives and voiced and voiceless affricates. It has three long and three short vowels, which expanded in Later Egyptian to about nine.[130] The basic word in Egyptian, similar to Semitic and Berber, is a triliteral or biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to the person. For example, the triconsonantal skeleton S-Ḏ-M is the semantic core of the word 'hear'; its basic conjugation is sḏm, 'he hears'. If the subject is a noun, suffixes are not added to the verb:[131] sḏm ḥmt, 'the woman hears'.		Adjectives are derived from nouns through a process that Egyptologists call nisbation because of its similarity with Arabic.[132] The word order is predicate–subject in verbal and adjectival sentences, and subject–predicate in nominal and adverbial sentences.[133] The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun.[134] Verbs and nouns are negated by the particle n, but nn is used for adverbial and adjectival sentences. Stress falls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).[135]		Hieroglyphic writing dates from c. 3000 BC, and is composed of hundreds of symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, called hieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing, Demotic, became the prevalent writing style, and it is this form of writing—along with formal hieroglyphs—that accompany the Greek text on the Rosetta Stone.[137]		Around the first century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modified Greek alphabet with the addition of some Demotic signs.[138] Although formal hieroglyphs were used in a ceremonial role until the fourth century, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine[139] and Islamic periods in Egypt,[140] but only in 1822, after the discovery of the Rosetta stone and years of research by Thomas Young and Jean-François Champollion, were hieroglyphs almost fully deciphered.[141]		Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of the Per Ankh institution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories.[142] Some of the best-known pieces of ancient Egyptian literature, such as the Pyramid and Coffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300 BC. Later Egyptian was spoken from the New Kingdom onward and is represented in Ramesside administrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those of Harkhuf and Weni. The genre known as Sebayt ("instructions") was developed to communicate teachings and guidance from famous nobles; the Ipuwer papyrus, a poem of lamentations describing natural disasters and social upheaval, is a famous example.		The Story of Sinuhe, written in Middle Egyptian, might be the classic of Egyptian literature.[143] Also written at this time was the Westcar Papyrus, a set of stories told to Khufu by his sons relating the marvels performed by priests.[144] The Instruction of Amenemope is considered a masterpiece of near-eastern literature.[145] Towards the end of the New Kingdom, the vernacular language was more often employed to write popular pieces like the Story of Wenamun and the Instruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700 BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in the demotic script and phase of Egyptian. Many stories written in demotic during the Greco-Roman period were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such as Ramesses II.[146]		Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed of mud-brick designed to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling grain and a small oven for baking the bread.[147] Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture.[148]		The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made from animal fat and chalk. Men shaved their entire bodies for cleanliness; perfumes and aromatic ointments covered bad odors and soothed skin.[149] Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, and cosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family's income.[150]		Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, drums, and imported lutes and lyres from Asia.[151] The sistrum was a rattle-like musical instrument that was especially important in religious ceremonies.		The ancient Egyptians enjoyed a variety of leisure activities, including games and music. Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game was mehen, which had a circular gaming board. Juggling and ball games were popular with children, and wrestling is also documented in a tomb at Beni Hasan.[152] The wealthy members of ancient Egyptian society enjoyed hunting and boating as well.		The excavation of the workers' village of Deir el-Madinah has resulted in one of the most thoroughly documented accounts of community life in the ancient world that spans almost four hundred years. There is no comparable site in which the organization, social interactions, working and living conditions of a community were studied in such detail.[153]		Egyptian cuisine remained remarkably stable over time; indeed, the cuisine of modern Egypt retains some striking similarities to the cuisine of the ancients. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill.[154]		The architecture of ancient Egypt includes some of the most famous structures in the world: the Great Pyramids of Giza and the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the wide-ranging power of the pharaoh. The ancient Egyptians were skilled builders; using only simple but effective tools and sighting instruments, architects could build large stone structures with great accuracy and precision that is still envied today.[155]		The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mud bricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite and the pharaoh were more elaborate structures. A few surviving New Kingdom palaces, such as those in Malkata and Amarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs.[156] Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of mud bricks. The architectural elements used in the world's first large-scale stone building, Djoser's mortuary complex, include post and lintel supports in the papyrus and lotus motif.		The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added the pylon, the open courtyard, and the enclosed hypostyle hall to the front of the temple's sanctuary, a style that was standard until the Greco-Roman period.[157] The earliest and most popular tomb architecture in the Old Kingdom was the mastaba, a flat-roofed rectangular structure of mudbrick or stone built over an underground burial chamber. The step pyramid of Djoser is a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but most later rulers abandoned them in favor of less conspicuous rock-cut tombs.[158] The Twenty-fifth dynasty was a notable exception, as all Twenty-fifth dynasty pharaohs constructed pyramids.[64][65][66]		The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change.[159] These artistic standards—simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth—created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. The Narmer Palette, for example, displays figures that can also be read as hieroglyphs.[160] Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity.[161]		Ancient Egyptian artisans used stone to carve statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed with gum arabic as a binder and pressed into cakes, which could be moistened with water when needed.[162]		Pharaohs used reliefs to record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces of funerary art, such as shabti statues and books of the dead, which they believed would protect them in the afterlife.[163] During the Middle Kingdom, wooden or clay models depicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in the afterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife.[164]		Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period, Minoan-style frescoes were found in Avaris.[165] The most striking example of a politically driven change in artistic forms comes from the Amarna period, where figures were radically altered to conform to Akhenaten's revolutionary religious ideas.[166] This style, known as Amarna art, was quickly and thoroughly erased after Akhenaten's death and replaced by the traditional forms.[167]		Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on the divine right of kings. The Egyptian pantheon was populated by gods who had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of this pantheon changed continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflicting myths and stories into a coherent system.[168] These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality.[169]		Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos.[170] After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system of oracles to communicate the will of the gods directly to the people.[171]		The Egyptians believed that every human being was composed of physical and spiritual parts or aspects. In addition to the body, each person had a šwt (shadow), a ba (personality or soul), a ka (life-force), and a name.[172] The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin his ka and ba and become one of the "blessed dead", living on as an akh, or "effective one". For this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth". If deemed worthy, the deceased could continue their existence on earth in spiritual form.[173]		The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body by mummification, performing burial ceremonies, and interring with the body goods the deceased would use in the afterlife.[163] Before the Old Kingdom, bodies buried in desert pits were naturally preserved by desiccation. The arid, desert conditions were a boon throughout the history of ancient Egypt for burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and use artificial mummification, which involved removing the internal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately in canopic jars.[174]		By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts called natron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in painted cartonnage mummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated.[175]		Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased. Beginning in the New Kingdom, books of the dead were included in the grave, along with shabti statues that were believed to perform manual labor for them in the afterlife.[176] Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.[177]		The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in the ancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city of Buhen on the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to the Levant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquer Kush and parts of the Levant.[178]		Typical military equipment included bows and arrows, spears, and round-topped shields made by stretching animal skin over a wooden frame. In the New Kingdom, the military began using chariots that had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and the Khopesh was adopted from Asiatic soldiers.[179] The pharaoh was usually depicted in art and literature riding at the head of the army; it has been suggested that at least a few pharaohs, such as Seqenenre Tao II and his sons, did do so.[180] However, it has also been argued that "kings of this period did not personally act as frontline war leaders, fighting alongside their troops."[181] Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.[182]		In technology, medicine, and mathematics, ancient Egypt achieved a relatively high standard of productivity and sophistication. Traditional empiricism, as evidenced by the Edwin Smith and Ebers papyri (c. 1600 BC), is first credited to Egypt. The Egyptians created their own alphabet and decimal system.		Even before the Old Kingdom, the ancient Egyptians had developed a glassy material known as faience, which they treated as a type of artificial semi-precious stone. Faience is a non-clay ceramic made of silica, small amounts of lime and soda, and a colorant, typically copper.[183] The material was used to make beads, tiles, figurines, and small wares. Several methods can be used to create faience, but typically production involved application of the powdered materials in the form of a paste over a clay core, which was then fired. By a related technique, the ancient Egyptians produced a pigment known as Egyptian Blue, also called blue frit, which is produced by fusing (or sintering) silica, copper, lime, and an alkali such as natron. The product can be ground up and used as a pigment.[184]		The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently.[185] It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as adding trace elements to control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.[186]		The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards from malaria and debilitating schistosomiasis parasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The lifelong labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible to abscesses (though caries were rare).[187]		The diets of the wealthy were rich in sugars, which promoted periodontal disease.[188] Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence.[189] Adult life expectancy was about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy.[190]		Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, such as Imhotep, remained famous long after their deaths.[191] Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists.[192] Training of physicians took place at the Per Ankh or "House of Life" institution, most notably those headquartered in Per-Bastet during the New Kingdom and at Abydos and Saïs in the Late period. Medical papyri show empirical knowledge of anatomy, injuries, and practical treatments.[193]		Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads, and swabs soaked with honey to prevent infection,[194] while opium thyme and belladona were used to relieve pain. The earliest records of burn treatment describe burn dressings that use the milk from mothers of male babies. Prayers were made to the goddess Isis. Moldy bread, honey and copper salts were also used to prevent infection from dirt in burns.[195] Garlic and onions were used regularly to promote good health and were thought to relieve asthma symptoms. Ancient Egyptian surgeons stitched wounds, set broken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until death occurred.[196]		Early Egyptians knew how to assemble planks of wood into a ship hull and had mastered advanced forms of shipbuilding as early as 3000 BC. The Archaeological Institute of America reports that the oldest planked ships known are the Abydos boats.[6] A group of 14 discovered ships in Abydos were constructed of wooden planks "sewn" together. Discovered by Egyptologist David O'Connor of New York University,[197] woven straps were found to have been used to lash the planks together,[6] and reeds or grass stuffed between the planks helped to seal the seams.[6] Because the ships are all buried together and near a mortuary belonging to Pharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000 BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000 BC was 75 feet (23 m) long and is now thought to perhaps have belonged to an earlier pharaoh. According to professor O'Connor, the 5,000-year-old ship may have even belonged to Pharaoh Aha.[197]		Early Egyptians also knew how to assemble planks of wood with treenails to fasten them together, using pitch for caulking the seams. The "Khufu ship", a 43.6-metre (143 ft) vessel sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza in the Fourth Dynasty around 2500 BC, is a full-size surviving example that may have filled the symbolic function of a solar barque. Early Egyptians also knew how to fasten the planks of this ship together with mortise and tenon joints.[6]		Large seagoing ships are known to have been heavily used by the Egyptians in their trade with the city states of the eastern Mediterranean, especially Byblos (on the coast of modern-day Lebanon), and in several expeditions down the Red Sea to the Land of Punt.[198] In fact one of the earliest Egyptian words for a seagoing ship is a "Byblos Ship", which originally defined a class of Egyptian seagoing ships used on the Byblos run; however, by the end of the Old Kingdom, the term had come to include large seagoing ships, whatever their destination.[198]		In 2011 archaeologists from Italy, the United States, and Egypt excavating a dried-up lagoon known as Mersa Gawasis have unearthed traces of an ancient harbor that once launched early voyages like Hatshepsut's Punt expedition onto the open ocean.[199] Some of the site's most evocative evidence for the ancient Egyptians' seafaring prowess include large ship timbers and hundreds of feet of ropes, made from papyrus, coiled in huge bundles.[199] And in 2013 a team of Franco-Egyptian archaeologists discovered what is believed to be the world's oldest port, dating back about 4500 years, from the time of King Cheops on the Red Sea coast near Wadi el-Jarf (about 110 miles south of Suez).[200]		In 1977, an ancient north-south canal dating to the Middle Kingdom of Egypt was discovered extending from Lake Timsah to the Ballah Lakes.[201] It was dated to the Middle Kingdom of Egypt by extrapolating dates of ancient sites constructed along its course.[201][202]		The earliest attested examples of mathematical calculations date to the predynastic Naqada period, and show a fully developed numeral system.[204] The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor, and grain.[205] Texts such as the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus show that the ancient Egyptians could perform the four basic mathematical operations—addition, subtraction, multiplication, and division—use fractions, compute the volumes of boxes and pyramids, and calculate the surface areas of rectangles, triangles, and circles. They understood basic concepts of algebra and geometry, and could solve simple sets of simultaneous equations.[206]		Mathematical notation was decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively.[207] Because their methods of calculation could not handle most fractions with a numerator greater than one, they had to write fractions as the sum of several fractions. For example, they resolved the fraction two-fifths into the sum of one-third + one-fifteenth. Standard tables of values facilitated this.[208] Some common fractions, however, were written with a special glyph—the equivalent of the modern two-thirds is shown on the right.[209]		Ancient Egyptian mathematicians had a grasp of the principles underlying the Pythagorean theorem, knowing, for example, that a triangle had a right angle opposite the hypotenuse when its sides were in a 3–4–5 ratio.[210] They were able to estimate the area of a circle by subtracting one-ninth from its diameter and squaring the result:		a reasonable approximation of the formula πr 2.[210][211]		The golden ratio seems to be reflected in many Egyptian constructions, including the pyramids, but its use may have been an unintended consequence of the ancient Egyptian practice of combining the use of knotted ropes with an intuitive sense of proportion and harmony.[212]		Greek historian Herodotus claimed that ancient Egyptians looked like the people in Colchis (modern-day Georgia). This claim has been largely discredited as fictional by modern-day scholars.[213][214][215]		For the fact is as I soon came to realise myself, and then heard from others later, that the Colchians are obviously Egyptian. When the notion occurred to me, I asked both the Colchians and the Egyptians about it, and found that the Colchians had better recall of the Egyptians than the Egyptians did of them. Some Egyptians said that they thought the Colchians originated with Sesostris’ army, but I myself guessed their Egyptian origin not only because the Colchians are dark-skinned and curly-haired (which does not count for much by itself, because these features are common in others too) but more importantly because Colchians, Egyptians and Ethiopians are the only peoples in the world who practise circumcision and who have always done so.[216]		A team lead by Johannes Krause managed the first reliable sequencing of the genomes of 90 mummified individuals in 2017. Whilst not conclusive, because of the non-exhaustive time frame and restricted location that the mummies represent, their study nevertheless showed that these Ancient Egyptians "closely resembled ancient and modern Near Eastern populations, especially those in the Levant, and had almost no DNA from sub-Saharan Africa. What's more, the genetics of the mummies remained remarkably consistent even as different powers—including Nubians, Greeks, and Romans—conquered the empire." Later, however, something did alter the genomes of Egyptians. Although the mummies contain almost no DNA from sub-Saharan Africa, some 15% to 20% of modern Egyptians’ DNA reflects sub-Saharan ancestry.[217]		The culture and monuments of ancient Egypt have left a lasting legacy on the world. The cult of the goddess Isis, for example, became popular in the Roman Empire, as obelisks and other relics were transported back to Rome.[218] The Romans also imported building materials from Egypt to erect Egyptian-style structures. Early historians such as Herodotus, Strabo, and Diodorus Siculus studied and wrote about the land, which Romans came to view as a place of mystery.[219]		During the Middle Ages and the Renaissance, Egyptian pagan culture was in decline after the rise of Christianity and later Islam, but interest in Egyptian antiquity continued in the writings of medieval scholars such as Dhul-Nun al-Misri and al-Maqrizi.[220] In the seventeenth and eighteenth centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave of Egyptomania across Europe. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities.[221]		Although the European colonial occupation of Egypt destroyed a significant portion of the country's historical legacy, some foreigners left more positive marks. Napoleon, for example, arranged the first studies in Egyptology when he brought some 150 scientists and artists to study and document Egypt's natural history, which was published in the Description de l'Égypte.[222]		In the 20th century, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. The Supreme Council of Antiquities now approves and oversees all excavations, which are aimed at finding information rather than treasure. The council also supervises museums and monument reconstruction programs designed to preserve the historical legacy of Egypt.		Tourists riding a camel in front of Giza pyramids		Frontispiece of Description de l'Égypte, published in 38 volumes between 1809 and 1829.		Relief from interior of the Temple of Rameses II.		
A proverb (from Latin: proverbium) is a simple and concrete saying, popularly known and repeated, that expresses a truth based on common sense or experience. They are often metaphorical. A proverb that describes a basic rule of conduct may also be known as a maxim. Proverbs fall into the category of formulaic language.		Proverbs are often borrowed from similar languages and cultures, and sometimes come down to the present through more than one language. Both the Bible (including, but not limited to the Book of Proverbs) and medieval Latin (aided by the work of Erasmus) have played a considerable role in distributing proverbs across Europe. Mieder has concluded that cultures that treat the Bible as their "major spiritual book contain between three hundred and five hundred proverbs that stem from the Bible".[1] However, almost every culture has examples of its own unique proverbs.						Defining a "proverb" is a difficult task. Proverb scholars often quote Archer Taylor's classic "The definition of a proverb is too difficult to repay the undertaking... An incommunicable quality tells us this sentence is proverbial and that one is not. Hence no definition will enable us to identify positively a sentence as proverbial".[2] Another common definition is from Lord John Russell (c. 1850) "A proverb is the wit of one, and the wisdom of many."[3]		More constructively, Mieder has proposed the following definition, "A proverb is a short, generally known sentence of the folk which contains wisdom, truth, morals, and traditional views in a metaphorical, fixed, and memorizable form and which is handed down from generation to generation".[4] Norrick created a table of distinctive features to distinguish proverbs from idioms, cliches, etc.[5] Prahlad distinguishes proverbs from some other, closely related types of sayings, "True proverbs must further be distinguished from other types of proverbial speech, e.g. proverbial phrases, Wellerisms, maxims, quotations, and proverbial comparisons."[6] Based on Persian proverbs, Zolfaghari and Ameri propose the following definition: "A proverb is a short sentence, which is well-known and at times rhythmic, including advice, sage themes and ethnic experiences, comprising simile, metaphor or irony which is well-known among people for its fluent wording, clarity of expression, simplicity, expansiveness and generality and is used either with or without change".[7]		There are many sayings in English that are commonly referred to as "proverbs", such as weather sayings. Alan Dundes, however, rejects including such sayings among truly proverbs: "Are weather proverbs proverbs? I would say emphatically 'No!'"[8] The definition of "proverb" has also changed over the years. For example, the following was labeled "A Yorkshire proverb" in 1883, but would not be categorized as a proverb by most today, "as throng as Throp's wife when she hanged herself with a dish-cloth".[9] The changing of the definition of "proverb" is also noted in Turkish.[10]		In other languages and cultures, the definition of "proverb" also differs from English. In the Chumburung language of Ghana, "aŋase are literal proverbs and akpare are metaphoric ones".[11] Among the Bini of Nigeria, there are three words that are used to translate "proverb": ere, ivbe, and itan. The first relates to historical events, the second relates to current events, and the third was "linguistic ornamentation in formal discourse".[12] Among the Balochi of Pakistan and Afghanistan, there is a word batal for ordinary proverbs and bassīttuks for "proverbs with background stories".[13]		There are also language communities that combine proverbs and riddles in some sayings, leading some scholars to create the label "proverb riddles".[14][15][16]		All of this makes it difficult to come up with a definition of "proverb" that is universally applicable, which brings us back to Taylor's observation, "An incommunicable quality tells us this sentence is proverbial and that one is not.".		The study of proverbs is called paremiology which has a variety of uses in the study of such topics as philosophy, linguistics, and folklore. There are several types and styles of proverbs which are analyzed within Paremiology as is the use and misuse of familiar expressions which are not strictly 'proverbial' in the dictionary definition of being fixed sentences.		Proverbs in various languages are found with a wide variety of grammatical structures.[17] In English, for example, we find the following structures (in addition to others):		However, people will often quote only a fraction of a proverb to invoke an entire proverb, e.g. "All is fair" instead of "All is fair in love and war", and "A rolling stone" for "A rolling stone gathers no moss."		The grammar of proverbs is not always the typical grammar of the spoken language, often elements are moved around, to achieve rhyme or focus.[18]		Another type of grammatical structure in proverbs is a short dialogue:		Proverbs are used in conversation by adults more than children, partially because adults have learned more proverbs than children. Also, using proverbs well is a skill that is developed over years. Additionally, children have not mastered the patterns of metaphorical expression that are invoked in proverb use. Proverbs, because they are indirect, allow a speaker to disagree or give advice in a way that may be less offensive. Studying actual proverb use in conversation, however, is difficult since the researcher must wait for proverbs to happen.[23] An Ethiopian researcher, Tadesse Jaleta Jirata, made headway in such research by attending and taking notes at events where he knew proverbs were expected to be part of the conversations.[24]		Many authors have used proverbs in their writings. Probably the most famous user of proverbs in novels is J. R. R. Tolkien in his The Hobbit and The Lord of the Rings series.[25][26][27] Herman Melville is noted for creating proverbs in Moby Dick[28] and in his poetry.[29][30] Also, C. S. Lewis created a dozen proverbs in The Horse and His Boy,[31] and Mercedes Lackey created dozens for her invented Shin'a'in and Tale'edras cultures;[32] Lackey's proverbs are notable in that they are reminiscent to those of Ancient Asia - e.g. "Just because you feel certain an enemy is lurking behind every bush, it doesn't follow that you are wrong" is like to "Before telling secrets on the road, look in the bushes." These authors are notable for not only using proverbs as integral to the development of the characters and the story line, but also for creating proverbs.[citation needed]		Among medieval literary texts, Geoffrey Chaucer's Troilus and Criseyde plays a special role because Chaucer's usage seems to challenge the truth value of proverbs by exposing their epistemological unreliability.[33] Rabelais used proverbs to write an entire chapter of Gargantua.[34]		The patterns of using proverbs in literature can change over time. A study of "classical Chinese novels" found proverb use as frequently as one proverb every 3,500 words in Water Margin (Sui-hu chuan) and one proverb every 4,000 words in Wen Jou-hsiang. But modern Chinese novels have fewer proverbs by far.[35]		Proverbs (or portions of them) have been the inspiration for titles of books: The Bigger they Come by Erle Stanley Gardner, and Birds of a Feather (several books with this title), Devil in the Details (multiple books with this title). Sometimes a title alludes to a proverb, but does not actually quote it, such as The Gift Horse's Mouth by Robert Campbell. Some stories have been written with a proverb overtly as an opening, such as "A stitch in time saves nine" at the beginning of "Kitty's Class Day", one of Louisa May Alcott's Proverb Stories. Other times, a proverb appears at the end of a story, summing up a moral to the story, frequently found in Aesop's Fables, such as "Heaven helps those who help themselves" from Hercules and the Wagoner.		Proverbs have also been used strategically by poets.[36] Sometimes proverbs (or portions of them or anti-proverbs) are used for titles, such as "A bird in the bush" by Lord Kennet and his stepson Peter Scott and "The blind leading the blind" by Lisa Mueller. Sometimes, multiple proverbs are important parts of poems, such as Paul Muldoon's "Symposium", which begins "You can lead a horse to water but you can't make it hold its nose to the grindstone and hunt with the hounds. Every dog has a stitch in time..." The Turkish poet Refiki wrote an entire poem by stringing proverbs together, which has been translated into English poetically yielding such verses as "Be watchful and be wary, / But seldom grant a boon; / The man who calls the piper / Will also call the tune."[37]		Because proverbs are familiar and often pointed, they have been used by a number of hip-hop poets. This has been true not only in the USA, birthplace of hip-hop, but also in Nigeria. Since Nigeria is so multilingual, hip-hop poets there use proverbs from various languages, mixing them in as it fits their need, sometimes translating the original. For example, "They forget say ogbon ju agbaralo They forget that wisdom is greater than power"[38]		Some authors have bent and twisted proverbs, creating anti-proverbs, for a variety of literary effects. For example, in the Harry Potter novels, J. K. Rowling reshapes a standard English proverb into "It's no good crying over spilt potion" and Dumbledore advises Harry not to "count your owls before they are delivered".[39] In a slightly different use of reshaping proverbs, in the Aubrey–Maturin series of historical naval novels by Patrick O'Brian, Capt. Jack Aubrey humorously mangles and mis-splices proverbs, such as "Never count the bear's skin before it is hatched" and "There's a good deal to be said for making hay while the iron is hot."[40] Earlier than O'Brian's Aubrey, Beatrice Grimshaw also used repeated splicings of proverbs in the mouth of an eccentric marquis to create a memorable character in The Sorcerer's Stone, such as "The proof of the pudding sweeps clean" (p. 109) and "A stitch in time is as good as a mile" (p. 97).[41]		Because proverbs are so much a part of the language and culture, authors have sometimes used proverbs in historical fiction effectively, but anachronistically, before the proverb was actually known. For example, the novel Ramage and the Rebels, by Dudley Pope is set in approximately 1800. Captain Ramage reminds his adversary "You are supposed to know that it is dangerous to change horses in midstream" (p. 259), with another allusion to the same proverb three pages later. However, the proverb about changing horses in midstream is reliably dated to 1864,[42] so the proverb could not have been known or used by a character from that period.		Some authors have used so many proverbs that there have been entire books written cataloging their proverb usage, such as Charles Dickens,[43] Agatha Christie,[44] George Bernard Shaw,[45] Cervantes,[46] and Friedrich Nietzsche.[47]		On the non-fiction side, proverbs have also been used by authors. Some have been used as the basis for book titles, e.g. I Shop, Therefore I Am: Compulsive Buying and the Search for Self by April Lane Benson. Some proverbs been used as the basis for article titles, "All our eggs in a broken basket: How the Human Terrain System is undermining sustainable military cultural competence."[48] Proverbs have been noted as common in subtitles of articles[49] such as "Discontinued intergenerational transmission of Czech in Texas: 'Hindsight is better than foresight'."[50] Many authors have cited proverbs as epigrams at the beginning of their articles, e.g. "'If you want to dismantle a hedge, remove one thorn at a time' Somali proverb" in an article on peacemaking in Somalia.[51] An article about research among the Māori used a Māori proverb as a title, then began the article with the Māori form of the proverb as an epigram "Set the overgrown bush alight and the new flax shoots will spring up", followed by three paragraphs about how the proverb served as a metaphor for the research and the present context.[52]		Interpreting proverbs is often complex, but is best done in a context.[53] Interpreting proverbs from other cultures is much more difficult than interpreting proverbs in one's own culture. Even within English-speaking cultures, there is difference of opinion on how to interpret the proverb A rolling stone gathers no moss. Some see it as condemning a person that keeps moving, seeing moss as a positive thing, such as profit; others see it the proverb as praising people that keep moving and developing, seeing moss as a negative thing, such as negative habits.		Similarly, among Tajik speakers, the proverb "One hand cannot clap" has two significantly different interpretations. Most see the proverb as promoting teamwork. Others understand it to mean that an argument requires two people.[54]		In an extreme example, one researcher working in Ghana found that for a single Akan proverb, twelve different interpretations were given.[55] Though this is extreme, proverbs can often have multiple interpretations.		Children will sometimes interpret proverbs in a literal sense, not yet knowing how to understand the conventionalized metaphor. Interpretation of proverbs is also affected by injuries and diseases of the brain, "A hallmark of schizophrenia is impaired proverb interpretation."[56]		There are often proverbs that contradict each other, such as "Look before you leap" and "He who hesitates is lost", or "Many hands make light work" and "Too many cooks spoil the broth". These have been labeled "counter proverbs"[57] or "antonymous proverbs".[58] When there are such counter proverbs, each can be used in its own appropriate situation, and neither is intended to be a universal truth.		The concept of "counter proverb" is more about pairs of contradictory proverbs than about the use of proverbs to counter each other in an argument. For example, from the Tafi language of Ghana, the following pair of proverbs are counter to each other but are each used in appropriate contexts, "A co-wife who is too powerful for you, you address her as your mother" and "Do not call your mother's co-wife your mother..."[59] In Nepali, there is a set of totally contradictory proverbs: "Religion is victorious and sin erodes" and "Religion erodes and sin is victorious".[60] Also, the following pair are counter proverbs from the Kasena of Ghana: "It is the patient person who will milk a barren cow" and "The person who would milk a barren cow must prepare for a kick on the forehead".[61] The two contradict each other, whether they are used in an argument or not (though indeed they were used in an argument). But the same work contains an appendix with many examples of proverbs used in arguing for contrary positions, but proverbs that are not inherently contradictory,[62] such as "One is better off with hope of a cow's return than news of its death" countered by "If you don't know a goat [before its death] you mock at its skin". Though this pair was used in a contradictory way in a conversation, they are not a set of "counter proverbs".		Discussing counter proverbs in the Badaga language, Hockings explained that in his large collection "a few proverbs are mutually contradictory... we can be sure that the Badagas do not see the matter that way, and would explain such apparent contradictions by reasoning that proverb x is used in one context, while y is used in quite another."[63] Comparing Korean proverbs, "when you compare two proverbs, often they will be contradictory." They are used for "a particular situation".[64]		"Counter proverbs" are not the same as a "paradoxical proverb", a proverb that contains a seeming paradox.[65]		In many cultures, proverbs are so important and so prominent that there are proverbs about proverbs, that is, "metaproverbs". The most famous one is from Yoruba of Nigeria, "Proverbs are the horses of speech, if communication is lost we use proverbs to find it," used by Wole Soyinka in Death and the King's Horsemen. In Mieder's bibliography of proverb studies, there are twelve publications listed as describing metaproverbs.[66] Other metaproverbs include:		This list contains mostly examples from Africa, though there are likely more examples from elsewhere, also.		Similarly to other forms of literature, proverbs have also been used as important units of language in drama and films. This is true from the days of classical Greek works[83] to old French[84] to Shakespeare,[85] to 19th Century Spanish,[86] to today. The use of proverbs in drama and film today is still found in languages around the world, such as Yorùbá.[87]		A film that makes rich use of proverbs is Forrest Gump, known for both using and creating proverbs.[88][89] Other studies of the use of proverbs in film include work by Kevin McKenna on the Russian film Aleksandr Nevsky,[90] Haase's study of an adaptation of Little Red Riding Hood,[91] Elias Dominguez Barajas on the film Viva Zapata!,[92] and Aboneh Ashagrie on The Athlete (a movie in Amharic about Abebe Bikila).[93]		In the case of Forrest Gump, the screenplay by Eric Roth had more proverbs than the novel by Winston Groom, but for The Harder They Come, the reverse is true, where the novel derived from the movie by Michael Thelwell has many more proverbs than the movie.[94]		Éric Rohmer, the French film director, directed a series of films, the "Comedies and Proverbs", where each film was based on a proverb: The Aviator's Wife, The Perfect Marriage, Pauline at the Beach, Full Moon in Paris (the film's proverb was invented by Rohmer himself: "The one who has two wives loses his soul, the one who has two houses loses his mind."), The Green Ray, Boyfriends and Girlfriends.[95]		Movie titles based on proverbs include Murder Will Out (1939 film), Try, Try Again, and The Harder They Fall. The title of an award-winning Turkish film, Three Monkeys, also invokes a proverb, though the title does not fully quote it.		They have also been used as the titles of plays:[96] Baby with the Bathwater by Christopher Durang, Dog Eat Dog by Mary Gallagher, and The Dog in the Manger by Charles Hale Hoyt. The use of proverbs as titles for plays is not, of course, limited to English plays: Il faut qu'une porte soit ouverte ou fermée (A door must be open or closed) by Paul de Musset. Proverbs have also been used in musical dramas, such as The Full Monty, which has been shown to use proverbs in clever ways.[97]		Proverbs are often poetic in and of themselves, making them ideally suited for adapting into songs. Proverbs have been used in music from opera to country to hip-hop. Proverbs have also been used in music in other languages, such as the Akan language[98] the Igede language,[99] and Spanish.[100]		English examples of using proverbs in music[101] include Elvis Presley's Easy come, easy go, Harold Robe's Never swap horses when you're crossing a stream, Arthur Gillespie's Absence makes the heart grow fonder, Bob Dylan's Like a rolling stone, Cher's Apples don't fall far from the tree. Lynn Anderson made famous a song full of proverbs, I never promised you a rose garden (written by Joe South). In choral music, we find Michael Torke's Proverbs for female voice and ensemble. A number of Blues musicians have also used proverbs extensively.[102][103] The frequent use of proverbs in Country music has led to published studies of proverbs in this genre.[104][105] The Reggae artist Jahdan Blakkamoore has recorded a piece titled Proverbs Remix. The opera Maldobrìe contains careful use of proverbs.[106] An extreme example of many proverbs used in composing songs is a song consisting almost entirely of proverbs performed by Bruce Springsteen, "My best was never good enough".[107] The Mighty Diamonds recorded a song called simply "Proverbs".		The band Fleet Foxes used the proverb painting Netherlandish Proverbs for the cover of their eponymous album Fleet Foxes.		In addition to proverbs being used in songs themselves, some rock bands have used parts of proverbs as their names, such as the Rolling Stones, Bad Company, The Mothers of Invention, Feast or Famine, Of Mice and Men. There have been at least two groups that called themselves "The Proverbs", and there is a hip-hop performer in South Africa known as "Proverb". In addition, many albums have been named with allusions to proverbs, such as Spilt milk (a title used by Jellyfish and also Kristina Train), The more things change by Machine Head, Silk purse by Linda Ronstadt, Another day, another dollar by DJ Scream Roccett, The blind leading the naked by Violent Femmes, What's good for the goose is good for the gander by Bobby Rush, Resistance is Futile by Steve Coleman, Murder will out by Fan the Fury. The proverb Feast or famine has been used as an album title by Chuck Ragan, Reef the Lost Cauze, Indiginus, and DaVinci. Whitehorse mixed two proverbs for the name of their album Leave no bridge unburned. The band Splinter Group released an album titled When in Rome, Eat Lions. The band Downcount used a proverb for the name of their tour, Come and take it.		Proverbs come from a variety of sources. Some are, indeed, the result of people pondering and crafting language, such as some by Confucius, Plato, Baltasar Gracián, etc. Others are taken from such diverse sources as poetry,[108] stories,[109] songs, commercials, advertisements, movies, literature, etc.[110] A number of the well known sayings of Jesus, Shakespeare, and others have become proverbs, though they were original at the time of their creation, and many of these sayings were not seen as proverbs when they were first coined. Many proverbs are also based on stories, often the end of a story. For example, the proverb "Who will bell the cat?" is from the end of a story about the mice planning how to be safe from the cat.		Some authors have created proverbs in their writings, such a J.R.R. Tolkien,[25][26] and some of these proverbs have made their way into broader society, such as the bumper sticker pictured here. Similarly, C.S. Lewis' created proverb about a lobster in a pot, from the Chronicles of Narnia, has also gained currency.[111] In cases like this, deliberately created proverbs for fictional societies have become proverbs in real societies. In a fictional story set in a real society, the movie Forrest Gump introduced "Life is like a box of chocolates" into broad society.		The proverb with "a longer history than any other recorded proverb in the world", going back to "around 1800 BC"[112] is in a Sumerian clay tablet, "The bitch by her acting too hastily brought forth the blind".[113] Though many proverbs are ancient, they were all newly created at some point by somebody. Sometimes it is easy to detect that a proverb is newly coined by a reference to something recent, such as the Haitian proverb "The fish that is being microwaved doesn't fear the lightning".[114] Also, there is a proverb in the Kafa language of Ethiopia that refers to the forced military conscription of the 1980s, "...the one who hid himself lived to have children."[115] A Mongolian proverb also shows evidence of recent origin, "A beggar who sits on gold; Foam rubber piled on edge."[116] A political candidate in Kenya popularised a new proverb in his 1995 campaign, Chuth ber "Immediacy is best". "The proverb has since been used in other contexts to prompt quick action."[117] Over 1,400 new English proverbs are said to have been coined and gained currency in the 20th century.[118] This process of creating proverbs is always ongoing, so that possible new proverbs are being created constantly. Those sayings that are adopted and used by an adequate number of people become proverbs in that society.		Grigorii Permjakov[119] developed the concept of the core set of proverbs that full members of society know, what he called the "paremiological minimum" (1979). For example, an adult American is expected to be familiar with "Birds of a feather flock together", part of the American paremiological minimum. However, an average adult American is not expected to know "Fair in the cradle, foul in the saddle", an old English proverb that is not part of the current American paremiological minimum. Thinking more widely than merely proverbs, Permjakov observed "every adult Russian language speaker (over 20 years of age) knows no fewer than 800 proverbs, proverbial expressions, popular literary quotations and other forms of cliches".[120] Studies of the paremiological minimum have been done for a limited number of languages, including Russian,[121] Hungarian,[122][123] Czech,[124] Somali,[125] Nepali,[126] Gujarati,[127] Spanish,[128] Esperanto,[129] Polish,[130] Ukrainian,[131] Two noted examples of attempts to establish a paremiological minimum in America are by Haas (2008) and Hirsch, Kett, and Trefil (1988), the latter more prescriptive than descriptive. There is not yet a recognized standard method for calculating the paremiological minimum, as seen by comparing the various efforts to establish the paremiological minimum in a number of languages.		From ancient times, people around the world have recorded proverbs in visual form. This has been done in two ways. First, proverbs have been written to be displayed, often in a decorative manner, such as on pottery, cross-stitch, murals,[132][133] kangas (East African women's wraps),[134] and quilts.[135]		Secondly, proverbs have often been visually depicted in a variety of media, including paintings, etchings, and sculpture. Jakob Jordaens painted a plaque with a proverb about drunkenness above a drunk man wearing a crown, titled The King Drinks. Probably the most famous examples of depicting proverbs are the different versions of the paintings Netherlandish Proverbs by the father and son Pieter Bruegel the Elder and Pieter Brueghel the Younger, the proverbial meanings of these paintings being the subject of a 2004 conference, which led to a published volume of studies (Mieder 2004a). The same father and son also painted versions of The Blind Leading the Blind, a Biblical proverb. These and similar paintings inspired another famous painting depicting some proverbs and also idioms (leading to a series of additional paintings), such as Proverbidioms by T. E. Breitenbach. Another painting inspired by Bruegel's work is by the Chinese artist, Ah To, who created a painting illustrating 81 Cantonese sayings.[136] Corey Barksdale has produced a book of paintings with specific proverbs and pithy quotations.[137] The British artist Chris Gollon has painted a major work entitled "Big Fish Eat Little Fish", a title echoing Bruegel's painting Big Fishes Eat Little Fishes.		Sometimes well-known proverbs are pictured on objects, without a text actually quoting the proverb, such as the three wise monkeys who remind us "Hear no evil, see no evil, speak no evil". When the proverb is well known, viewers are able to recognize the proverb and understand the image appropriately, but if viewers do not recognize the proverb, much of the effect of the image is lost. For example, there is a Japanese painting in the Bonsai museum in Saitama city that depicted flowers on a dead tree, but only when the curator learned the ancient (and no longer current) proverb "Flowers on a dead tree" did the curator understand the deeper meaning of the painting.[138]		A bibliography on proverbs in visual form has been prepared by Mieder and Sobieski (1999). Interpreting visual images of proverbs is subjective, but familiarity with the depicted proverb helps.[139]		In an abstract non-representational visual work, sculptor Mark di Suvero has created a sculpture titled "Proverb", which is located in Dallas, TX, near the Morton H. Meyerson Symphony Center.		Some artists have used proverbs and anti-proverbs for titles of their paintings, alluding to a proverb rather than picturing it. For example, Vivienne LeWitt painted a piece titled "If the shoe doesn't fit, must we change the foot?", which shows neither foot nor shoe, but a woman counting her money as she contemplates different options when buying vegetables.[140]		Cartoonists, both editorial and pure humorists, have often used proverbs, sometimes primarily building on the text, sometimes primarily on the situation visually, the best cartoons combining both. Not surprisingly, cartoonists often twist proverbs, such as visually depicting a proverb literally or twisting the text as an anti-proverb.[141] An example with all of these traits is a cartoon showing a waitress delivering two plates with worms on them, telling the customers, "Two early bird specials... here ya go."[142]		The traditional Three wise monkeys were depicted in Bizarro with different labels. Instead of the negative imperatives, the one with ears covered bore the sign "See and speak evil", the one with eyes covered bore the sign "See and hear evil", etc. The caption at the bottom read "The power of positive thinking."[143] Another cartoon showed a customer in a pharmacy telling a pharmacist, "I'll have an ounce of prevention."[144] The comic strip The Argyle Sweater showed an Egyptian archeologist loading a mummy on the roof of a vehicle, refusing the offer of a rope to tie it on, with the caption "A fool and his mummy are soon parted."[145] The comic One Big Happy showed a conversation where one person repeatedly posed part of various proverb and the other tried to complete each one, resulting in such humorous results as "Don't change horses... unless you can lift those heavy diapers."[146]		Editorial cartoons can use proverbs to make their points with extra force as they can invoke the wisdom of society, not just the opinion of the editors.[147] In an example that invoked a proverb only visually, when a US government agency (GSA) was caught spending money extravagantly, a cartoon showed a black pot labeled "Congress" telling a black kettle labeled "GSA", "Stop wasting the taxpayers' money!"[148] It may have taken some readers a moment of pondering to understand it, but the impact of the message was the stronger for it.		Cartoons with proverbs are so common that Wolfgang Mieder has published a collected volume of them, many of them editorial cartoons. For example, a German editorial cartoon linked a current politician to the Nazis, showing him with a bottle of swastika-labeled wine and the caption "In vino veritas".[149]		One cartoonist very self-consciously drew and wrote cartoons based on proverbs for the University of Vermont student newspaper The Water Tower, under the title "Proverb place".[150]		There is a growing interest in deliberately using proverbs to achieve goals, usually to support and promote changes in society. On the negative side, this was deliberately done by the Nazis.[151] On the more positive side, proverbs have also been used for constructive purposes. For example, proverbs have been used for teaching foreign languages at various levels.[152][153] In addition, proverbs have been used for public health promotion, such as promoting breast feeding with a shawl bearing a Swahili proverb "Mother's milk is sweet".[154] Proverbs have also been applied for helping people manage diabetes,[155] to combat prostitution,[156] and for community development.,[157] to resolve conflicts,[158] and to slow the transmission of HIV.[159]		The most active field deliberately using proverbs is Christian ministry, where Joseph G. Healey and others have deliberately worked to catalyze the collection of proverbs from smaller languages and the application of them in a wide variety of church-related ministries, resulting in publications of collections[160] and applications.[161][162] This attention to proverbs by those in Christian ministries is not new, many pioneering proverb collections having been collected and published by Christian workers.[163][164][165][166]		U.S. Navy Captain Edward Zellem pioneered the use of Afghan proverbs as a positive relationship-building tool during the war in Afghanistan, and in 2012 he published two bilingual collections[167][168] of Afghan proverbs in Dari and English, part of an effort of nationbuilding, followed by a volume of Pashto proverbs in 2014.[169]		Proverbs are often and easily translated and transferred from one language into another. "There is nothing so uncertain as the derivation of proverbs, the same proverb being often found in all nations, and it is impossible to assign its paternity."[170]		Proverbs are often borrowed across lines of language, religion, and even time. For example, a proverb of the approximate form "No flies enter a mouth that is shut" is currently found in Spain, France, Ethiopia, and many countries in between. It is embraced as a true local proverb in many places and should not be excluded in any collection of proverbs because it is shared by the neighbors. However, though it has gone through multiple languages and millennia, the proverb can be traced back to an ancient Babylonian proverb (Pritchard 1958:146).		Another example of a widely spread proverb is "A drowning person clutches at [frogs] foam", found in Peshai of Afghanistan[171] and Orma of Kenya,[172] and presumably places in between.		Proverbs about one hand clapping are common across Asia,[173] from Dari in Afghanistan[174] to Japan.[175]		Some studies have been done devoted to the spread of proverbs in certain regions, such as India and her neighbors[176] and Europe.[177]		An extreme example of the borrowing and spread of proverbs was the work done to create a corpus of proverbs for Esperanto, where all the proverbs were translated from other languages.[178]		It is often not possible to trace the direction of borrowing a proverb between languages. This is complicated by the fact that the borrowing may have been through plural languages. In some cases, it is possible to make a strong case for discerning the direction of the borrowing based on an artistic form of the proverb in one language, but a prosaic form in another language. For example, in Ethiopia there is a proverb "Of mothers and water, there is none evil." It is found in Amharic, Alaaba language, and Oromo, three languages of Ethiopia:		The Oromo version uses poetic features, such as the initial ha in both clauses with the final -aa in the same word, and both clauses ending with -an. Also, both clauses are built with the vowel a in the first and last words, but the vowel i in the one syllable central word. In contrast, the Amharic and Alaaba versions of the proverb show little evidence of sound-based art. Based on the verbal artistry of the Oromo, it appears that the Oromo form is prior to the Alaaba or Amharic, though it could be borrowed from yet another language.		There is a longstanding debate among proverb scholars as to whether the cultural values of specific language communities are reflected (to varying degree) in their proverbs. Many claim that the proverbs of a particular culture reflect the values of that specific culture, at least to some degree. Many writers have asserted that the proverbs of their cultures reflect their culture and values; this can be seen in such titles as the following: An introduction to Kasena society and culture through their proverbs,[180] Prejudice, power, and poverty in Haiti: a study of a nation's culture as seen through its proverbs,[181] Proverbiality and worldview in Maltese and Arabic proverbs,[182] Fatalistic traits in Finnish proverbs,[183] Vietnamese cultural patterns and values as expressed in proverbs,[184] The Wisdom and Philosophy of the Gikuyu proverbs: The Kihooto worldview,[185] Spanish Grammar and Culture through Proverbs,[186] and "How Russian Proverbs Present the Russian National Character".[187] Kohistani has written a thesis to show how understanding Afghan Dari proverbs will help Europeans understand Afghan culture.[188]		However, a number of scholars argue that such claims are not valid. They have used a variety of arguments. Grauberg argues that since many proverbs are so widely circulated they are reflections of broad human experience, not any one culture's unique viewpoint.[189] Related to this line of argument, from a collection of 199 American proverbs, Jente showed that only 10 were coined in the USA, so that most of these proverbs would not reflect uniquely American values.[190] Giving another line of reasoning that proverbs should not be trusted as a simplistic guide to cultural values, Mieder once observed "proverbs come and go, that is, antiquated proverbs with messages and images we no longer relate to are dropped from our proverb repertoire, while new proverbs are created to reflect the mores and values of our time",[191] so old proverbs still in circulation might reflect past values of a culture more than its current values. Also, within any language's proverb repertoire, there may be "counter proverbs", proverbs that contradict each other on the surface[57] (see section above). When examining such counter proverbs, it is difficult to discern an underlying cultural value. With so many barriers to a simple calculation of values directly from proverbs, some feel "one cannot draw conclusions about values of speakers simply from the texts of proverbs".[192]		Many outsiders have studied proverbs to discern and understand cultural values and world view of cultural communities.[193] These outsider scholars are confident that they have gained insights into the local cultures by studying proverbs, but this is not universally accepted.		Seeking empirical evidence to evaluate the question of whether proverbs reflect a culture's values, some have counted the proverbs that support various values. For example, Moon lists what he sees as the top ten core cultural values of the Builsa society of Ghana, as exemplified by proverbs. He found that 18% of the proverbs he analyzed supported the value of being a member of the community, rather than being independent.[194] This was corroboration to other evidence that collective community membership is an important value among the Builsa. In studying Tajik proverbs, Bell notes that the proverbs in his corpus "Consistently illustrate Tajik values" and "The most often observed proverbs reflect the focal and specific values" discerned in the thesis.[195]		A study of English proverbs created since 1900 showed in the 1960s a sudden and significant increase in proverbs that reflected more casual attitudes toward sex.[196] Since the 1960s was also the decade of the Sexual revolution, this shows a strong statistical link between the changed values of the decades and a change in the proverbs coined and used. Another study mining the same volume counted Anglo-American proverbs about religion to show that proverbs indicate attitudes toward religion are going downhill.[197]		There are many examples where cultural values have been explained and illustrated by proverbs. For example, from India, the concept that birth determines one's nature "is illustrated in the oft-repeated proverb: there can be no friendship between grass-eaters and meat-eaters, between a food and its eater".[198] Proverbs have been used to explain and illustrate the Fulani cultural value of pulaaku.[199] But using proverbs to illustrate a cultural value is not the same as using a collection of proverbs to discern cultural values. In a comparative study between Spanish and Jordanian proverbs it is defined the social imagination for the mother as an archetype in the context of role transformation and in contrast with the roles of husband, son and brother, in two societies which might be occasionally associated with sexist and /or rural ideologies.[200]		Some scholars have adopted a cautious approach, acknowledging at least a genuine, though limited, link between cultural values and proverbs: "The cultural portrait painted by proverbs may be fragmented, contradictory, or otherwise at variance with reality... but must be regarded not as accurate renderings but rather as tantalizing shadows of the culture which spawned them."[201] There is not yet agreement on the issue of whether, and how much, cultural values are reflected in a culture's proverbs.		It is clear that the Soviet Union believed that proverbs had a direct link to the values of a culture, as they used them to try to create changes in the values of cultures within their sphere of domination. Sometimes they took old Russian proverbs and altered them into socialist forms.[202] These new proverbs promoted Socialism and its attendant values, such as atheism and collectivism, e.g. "Bread is given to us not by Christ, but by machines and collective farms" and "A good harvest is had only by a collective farm." They did not limit their efforts to Russian, but also produced "newly coined proverbs that conformed to socialist thought" in Tajik and other languages of the USSR.[203]		Many proverbs from around the world address matters of ethics and expected of behavior. Therefore, it is not surprising that proverbs are often important texts in religions. The most obvious example is the Book of Proverbs in the Bible. Additional proverbs have also been coined to support religious values, such as the following from Dari of Afghanistan:[204] "In childhood you're playful, In youth you're lustful, In old age you're feeble, So when will you before God be worshipful?"		Clearly proverbs in religion are not limited to monotheists; among the Badaga of India (Sahivite Hindus), there is a traditional proverb "Catch hold of and join with the man who has placed sacred ash [on himself]."[205] Proverbs are widely associated with large religions that draw from sacred books, but they are also used for religious purposes among groups with their own traditional religions, such as the Guji Oromo.[24] The broadest comparative study of proverbs across religions is The eleven religions and their proverbial lore, a comparative study. A reference book to the eleven surviving major religions of the world by Selwyn Gurney Champion, from 1945. Some sayings from sacred books also become proverbs, even if they were not obviously proverbs in the original passage of the sacred book.[206] For example, many quote "Be sure your sin will find you out" as a proverb from the Bible, but there is no evidence it was proverbial in its original usage (Numbers 32:23).		Not all religious references in proverbs are positive, some are cynical, such as the Tajik, "Do as the mullah says, not as he does."[207] Also, note the Italian proverb, "One barrel of wine can work more miracles than a church full of saints". An Indian proverb is cynical about devotees of Hinduism, "[Only] When in distress, a man calls on Rama".[208] In the context of Tibetan Buddhism, some Ladakhi proverbs mock the lamas, e.g. "If the lama's own head does not come out cleanly, how will he do the drawing upwards of the dead?... used for deriding the immoral life of the lamas."[209]		Dammann thought "The influence of Islam manifests itself in African proverbs... Christian influences, on the contrary, are rare."[210] If widely true in Africa, this is likely due to the longer presence of Islam in many parts of Africa. Reflection of Christian values is common in Amharic proverbs of Ethiopia, an area that has had a presence of Christianity for well over 1,000 years. The Islamic proverbial reproduction may also be shown in the image of some animals such as the dog. Although dog is portrayed in many European proverbs as the most faithful friend of man, it is represented in some Islamic countries as impure, dirty, vile, cowardly, ungrateful and treacherous, in addition to links to negative human superstitions such as loneliness, indifference and bad luck.[211]		Though much proverb scholarship is done by literary scholars, those studying the human mind have used proverbs in a variety of studies. One of the earliest studies in this field is the Proverbs Test by Gorham, developed in 1956. A similar test is being prepared in German.[212] Proverbs have been used to evaluate dementia,[213][214][215] study the cognitive development of children,[216] measure the results of brain injuries,[217] and study how the mind processes figurative language.[56][218]		Proverbs are frequently used in advertising, often in slightly modified form.[219][220][221] Ford once advertised its Thunderbird with, "One drive is worth a thousand words" (Mieder 2004b: 84). This is doubly interesting since the underlying proverb behind this, "One picture is worth a thousand words," was originally introduced into the English proverb repertoire in an ad for televisions (Mieder 2004b: 83).		A few of the many proverbs adapted and used in advertising include:		The GEICO company has created a series of television ads that are built around proverbs, such as "A bird in the hand is worth two in the bush",[222] and "The pen is mightier than the sword",[223] "Pigs may fly/When pigs fly",[224] "If a tree falls in the forest...",[225] and "Words can never hurt you".[226] Doritos made a commercial based on the proverb, "When pigs fly."[227]		Use of proverbs in advertising is not limited to the English language. Seda Başer Çoban has studied the use of proverbs in Turkish advertising.[228] Tatira has given a number of examples of proverbs used in advertising in Zimbabwe.[229] However, unlike the examples given above in English, all of which are anti-proverbs, Tatira's examples are standard proverbs. Where the English proverbs above are meant to make a potential customer smile, in one of the Zimbabwean examples "both the content of the proverb and the fact that it is phrased as a proverb secure the idea of a secure time-honored relationship between the company and the individuals". When newer buses were imported, owners of older buses compensated by painting a traditional proverb on the sides of their buses, "Going fast does not assure safe arrival".		Because many proverbs are both poetic and traditional, they are often passed down in fixed forms. Though spoken language may change, many proverbs are often preserved in conservative, even archaic, form. In English, for example, "betwixt" is not used by many, but a form of it is still heard (or read) in the proverb "There is many a slip 'twixt the cup and the lip." The conservative form preserves the meter and the rhyme. This conservative nature of proverbs can result in archaic words and grammatical structures being preserved in individual proverbs, as has been documented in Amharic,[230] Greek,[231] Nsenga,[232] and Polish.[233]		In addition, proverbs may still be used in languages which were once more widely known in a society, but are now no longer so widely known. For example, English speakers use some non-English proverbs that are drawn from languages that used to be widely understood by the educated class, e.g. "C'est la vie" from French and "Carpe diem" from Latin.		Proverbs are often handed down through generations. Therefore, "many proverbs refer to old measurements, obscure professions, outdated weapons, unknown plants, animals, names, and various other traditional matters."[234] Therefore, it is common that they preserve words that become less common and archaic in broader society.[235] For example, English has a proverb "The cobbler's children have no shoes". The word "cobbler", meaning a maker of shoes, is now unknown among many English speakers, but it is preserved in the proverb.		A seminal work in the study of proverbs is Archer Taylor's The Proverb (1931), later republished by Wolfgang Mieder with Taylor's Index included (1985/1934). A good introduction to the study of proverbs is Mieder's 2004 volume, Proverbs: A Handbook. Mieder has also published a series of bibliography volumes on proverb research, as well as a large number of articles and other books in the field. Stan Nussbaum has edited a large collection on proverbs of Africa, published on a CD, including reprints of out-of-print collections, original collections, and works on analysis, bibliography, and application of proverbs to Christian ministry (1998).[236] Paczolay has compared proverbs across Europe and published a collection of similar proverbs in 55 languages (1997). Mieder edits an academic journal of proverb study, Proverbium (ISSN 0743-782X), many back issues of which are available online.[237] A volume containing articles on a wide variety of topics touching on proverbs was edited by Mieder and Alan Dundes (1994/1981). Paremia is a Spanish-language journal on proverbs, with articles available online.[238] There are also papers on proverbs published in conference proceedings volumes from the annual Interdisciplinary Colloquium on Proverbs[239] in Tavira, Portugal. Mieder has published a two-volume International Bibliography of Paremiology and Phraseology, with a topical, language, and author index.[240] Mieder has published a bibliography of collections of proverbs from around the world.[241] A broad introduction to proverb study, Introduction to Paremiology, edited by Hrisztalina Hrisztova-Gotthardt and Melita Aleksa Varga has been published in both hardcover and free open access, with articles by a dozen different authors.[242]		Serious websites related to the study of proverbs, and some that list regional proverbs:		
Irony (from Ancient Greek εἰρωνεία eirōneía, meaning 'dissimulation, feigned ignorance'[1]), in its broadest sense, is a rhetorical device, literary technique, or event in which what appears, on the surface, to be the case, differs radically from what is actually the case. Irony may be divided into categories such as verbal, dramatic, and situational.		Verbal, dramatic, and situational irony are often used for emphasis in the assertion of a truth. The ironic form of simile, used in sarcasm, and some forms of litotes can emphasize one's meaning by the deliberate use of language which states the opposite of the truth, denies the contrary of the truth, or drastically and obviously understates a factual connection.[2]		Other forms, as identified by historian Connop Thirlwall, include dialectic and practical irony.[3]						Henry Watson Fowler, in The King's English, says, "any definition of irony—though hundreds might be given, and very few of them would be accepted—must include this, that the surface meaning and the underlying meaning of what is said are not the same." Also, Eric Partridge, in Usage and Abusage, writes that "Irony consists in stating the contrary of what is meant."		The use of irony may require the concept of a double audience. Fowler's A Dictionary of Modern English Usage says:		Irony is a form of utterance that postulates a double audience, consisting of one party that hearing shall hear & shall not understand, & another party that, when more is meant than meets the ear, is aware both of that more & of the outsiders' incomprehension.[4]		The term is sometimes used as a synonym for incongruous and applied to "every trivial oddity" in situations where there is no double audience.[4] An example of such usage is:		Sullivan, whose real interest was, ironically, serious music, which he composed with varying degrees of success, achieved fame for his comic opera scores rather than for his more earnest efforts.[5]		The American Heritage Dictionary's secondary meaning for irony: "incongruity between what might be expected and what actually occurs".[6] This sense, however, is not synonymous with "incongruous" but merely a definition of dramatic or situational irony. It is often included in definitions of irony not only that incongruity is present but also that the incongruity must reveal some aspect of human vanity or folly. Thus the majority of American Heritage Dictionary's usage panel found it unacceptable to use the word ironic to describe mere unfortunate coincidences or surprising disappointments that "suggest no particular lessons about human vanity or folly."[7]		On this aspect, the Oxford English Dictionary (OED) has also:		A condition of affairs or events of a character opposite to what was, or might naturally be, expected; a contradictory outcome of events as if in mockery of the promise and fitness of things. (In French, ironie du sort).[8]		According to the Encyclopædia Britannica,		The term irony has its roots in the Greek comic character Eiron, a clever underdog who by his wit repeatedly triumphs over the boastful character Alazon. The Socratic irony of the Platonic dialogues derives from this comic origin.[9]		According to Richard Whately:		Aristotle mentions Eironeia, which in his time was commonly employed to signify, not according to the modern use of 'Irony, saying the contrary to what is meant', but, what later writers usually express by Litotes, i.e. 'saying less than is meant'.[10]		The word came into English as a figure of speech in the 16th century as similar to the French ironie. It derives from the Latin ironia and ultimately from the Greek εἰρωνεία eirōneía, meaning dissimulation, ignorance purposely affected.[11]		The New Princeton Encyclopedia of Poetry and Poetics distinguishes between the following types of irony:[3]		According to A glossary of literary terms by Abrams and Hartman,		Verbal irony is a statement in which the meaning that a speaker employs is sharply different from the meaning that is ostensibly expressed. An ironic statement usually involves the explicit expression of one attitude or evaluation, but with indications in the overall speech-situation that the speaker intends a very different, and often opposite, attitude or evaluation.[12]		Verbal irony is distinguished from situational irony and dramatic irony in that it is produced intentionally by speakers. For instance, if a man exclaims, "I'm not upset!" but reveals an upset emotional state through his voice while truly trying to claim he's not upset, it would not be verbal irony by virtue of its verbal manifestation (it would, however, be situational irony). But if the same speaker said the same words and intended to communicate that he was upset by claiming he was not, the utterance would be verbal irony. This distinction illustrates an important aspect of verbal irony—speakers communicate implied propositions that are intentionally contradictory to the propositions contained in the words themselves. There are, however, examples of verbal irony that do not rely on saying the opposite of what one means, and there are cases where all the traditional criteria of irony exist and the utterance is not ironic.		In a clear example from literature, in Shakespeare's Julius Caesar, Mark Antony's speech after the assassination of Caesar appears to praise the assassins, particularly Brutus ("But Brutus says he was ambitious; / And Brutus is an honourable man"), while actually condemning them. "We're left in no doubt as to who's ambitious and who's honourable. The literal truth of what's written clashes with the perceived truth of what's meant to revealing effect, which is irony in a nutshell". [13]		Ironic similes are a form of verbal irony where a speaker intends to communicate the opposite of what they mean. For instance, the following explicit similes begin with the deceptive formation of a statement that means A but that eventually conveys the meaning not A:		The irony is recognizable in each case only by using knowledge of the source concepts (e.g., that mud is opaque, that root canal surgery is painful) to detect an incongruity.		A fair amount of confusion has surrounded the issue of the relationship between verbal irony and sarcasm.		Fowler's A Dictionary of Modern English Usage states:		Sarcasm does not necessarily involve irony and irony has often no touch of sarcasm.		This suggests that the two concepts are linked but may be considered separately. The OED entry for sarcasm does not mention irony, but the irony entry reads:		A figure of speech in which the intended meaning is the opposite of that expressed by the words used; usually taking the form of sarcasm or ridicule in which laudatory expressions are used to imply condemnation or contempt.		The Encyclopædia Britannica has "Non-literary irony is often called sarcasm"; while the Webster's Dictionary entry is:		Sarcasm: 1 : a sharp and often satirical or ironic utterance designed to cut or give pain. 2 a : a mode of satirical wit depending for its effect on bitter, caustic, and often ironic language that is usually directed against an individual.		Partridge in Usage and Abusage would separate the two forms of speech completely:		Irony must not be confused with sarcasm, which is direct: sarcasm means precisely what it says, but in a sharp, caustic, ... manner.		The psychologist Martin, in The Psychology of Humour, is quite clear that irony is where "the literal meaning is opposite to the intended" and sarcasm is "aggressive humor that pokes fun".[14] He has the following examples: for irony he uses the statement "What a nice day" when it is raining. For sarcasm, he cites Winston Churchill, who is supposed to have said, when told by Bessie Braddock that he was drunk, "But I shall be sober in the morning, and you will still be ugly", as being sarcastic, while not saying the opposite of what is intended.		Psychology researchers Lee and Katz (1998) have addressed the issue directly. They found that ridicule is an important aspect of sarcasm, but not of verbal irony in general. By this account, sarcasm is a particular kind of personal criticism levelled against a person or group of persons that incorporates verbal irony. For example, a woman reports to her friend that rather than going to a medical doctor to treat her cancer, she has decided to see a spiritual healer instead. In response her friend says sarcastically, "Oh, brilliant, what an ingenious idea, that's really going to cure you." The friend could have also replied with any number of ironic expressions that should not be labeled as sarcasm exactly, but still have many shared elements with sarcasm.		Most instances of verbal irony are labeled by research subjects as sarcastic, suggesting that the term sarcasm is more widely used than its technical definition suggests it should be (Bryant & Fox Tree, 2002; Gibbs, 2000). Some psycholinguistic theorists (e.g., Gibbs, 2000) suggest that sarcasm ("Great idea!", "I hear they do fine work."), hyperbole ("That's the best idea I have heard in years!"), understatement ("Sure, what the hell, it's only cancer..."), rhetorical questions ("What, does your spirit have cancer?"), double entendre ("I'll bet if you do that, you'll be communing with spirits in no time...") and jocularity ("Get them to fix your bad back while you're at it.") should all be considered forms of verbal irony. The differences between these rhetorical devices (tropes) can be quite subtle and relate to typical emotional reactions of listeners, and the rhetorical goals of the speakers. Regardless of the various ways theorists categorize figurative language types, people in conversation who are attempting to interpret speaker intentions and discourse goals do not generally identify, by name, the kinds of tropes used (Leggitt & Gibbs, 2000).		Echoic allusion is the main component involved in conveying verbally ironic meaning. It is best described as a speech act by which the speaker simultaneously represents a thought, belief or idea, and implicitly attributes this idea to someone else who is wrong or deluded. In this way, the speaker intentionally dissociates themselves from the idea and conveys their tacit dissent, thereby providing a different meaning to their utterance. In some cases, the speaker can provide stronger dissociation from the represented thought by also implying derision toward the idea or outwardly making fun of the person or people they attribute it to. [15]		Echoic allusion, like other forms of verbal irony, relies on semantically disambiguating cues to be interpreted correctly. These cues often come in the form of paralinguistic markers such as prosody, tone, or pitch, [16] as well as nonverbal cues like hand gesture, facial expression and eye gaze. [17]		An example of echoic allusion and its disambiguating paralinguistic markers is as follows:		From simple semantic analysis, Person 2 appears to believe Person 1. However, if this conversation is given the context of Person 2 walking in on Person 1 about to eat some cake, and Person 2 speaking their sentence in a significantly decreased rate of speech and lowered tone, the interpretation of "I just must have been mistaken" changes. Instead of being taken as Person 2 believing Person 1, the utterance calls to mind someone who would believe Person 1, while also conveying Person 2's implication that said individual would be considered gullible. From this, Person 2 negates the possible interpretation that they believe Person 1.		Dramatic irony exploits the device of giving the spectator an item of information that at least one of the characters in the narrative is unaware of (at least consciously), thus placing the spectator a step ahead of at least one of the characters. Connop Thirlwall in his 1833 article On the Irony of Sophocles originally highlighted the role of irony in drama.[18][19] The OED defines dramatic irony as:		the incongruity created when the (tragic) significance of a character's speech or actions is revealed to the audience but unknown to the character concerned; the literary device so used, orig. in Greek tragedy.[20]		According to Stanton,[21] dramatic irony has three stages—installation, exploitation, and resolution (often also called preparation, suspension, and resolution) —producing dramatic conflict in what one character relies or appears to rely upon, the contrary of which is known by observers (especially the audience; sometimes to other characters within the drama) to be true. In summary, it means that the reader/watcher/listener knows something that one or more of the characters in the piece is not aware of.		For example:		Tragic irony is a special category of dramatic irony. In tragic irony, the words and actions of the characters contradict the real situation, which the spectators fully realize. The Oxford English Dictionary defines this as:		the incongruity created when the (tragic) significance of a character's speech or actions is revealed to the audience but unknown to the character concerned, the literary device so used, orig. in Greek tragedy.[20]		Ancient Greek drama was especially characterized by tragic irony because the audiences were so familiar with the legends that most of the plays dramatized. Sophocles' Oedipus Rex provides a classic example of tragic irony at its fullest. Colebrook writes:		Tragic irony is exemplified in ancient drama ... The audience watched a drama unfold, already knowing its destined outcome. ... In Sophocles' Oedipus the King, for example, 'we' (the audience) can see what Oedipus is blind to. The man he murders is his father, but he does not know it.[29]		Further, Oedipus vows to find the murderer and curses him for the plague that he has caused, not knowing that the murderer he has cursed and vowed to find is himself. Irony has some of its foundation in the onlooker's perception of paradox that arises from insoluble problems. For example, in the William Shakespeare play Romeo and Juliet, when Romeo finds Juliet in a drugged deathlike sleep, he assumes her to be dead and kills himself. Upon awakening to find her dead lover beside her, Juliet stabs herself with a dagger thus killing herself.		Situational irony is a relatively modern use of the term, and describes a sharp discrepancy between the expected result and actual results in a certain situation.		Lars Elleström writes:		Situational irony ... is most broadly defined as a situation where the outcome is incongruous with what was expected, but it is also more generally understood as a situation that includes contradictions or sharp contrasts.[30]		For example:		The expression cosmic irony or "irony of fate" stems from the notion that the gods (or the Fates) are amusing themselves by toying with the minds of mortals with deliberate ironic intent. Closely connected with situational irony, it arises from sharp contrasts between reality and human ideals, or between human intentions and actual results. The resulting situation is poignantly contrary to what was expected or intended.		According to Sudhir Dixit, "Cosmic irony is a term that is usually associated with [Thomas] Hardy. ... There is a strong feeling of a hostile deus ex machina in Hardy's novels." In Tess of the d'Urbervilles "there are several instances of this type of irony."[37] One example follows:		"Justice" was done, and the President of the Immortals (in Æschylean phrase) had ended his sport with Tess.[38]		When history is seen through modern eyes, there often appear sharp contrasts between the way historical figures see their world's future and what actually transpires. For example, during the 1920s The New York Times repeatedly scorned crossword puzzles. In 1924, it lamented "the sinful waste in the utterly futile finding of words the letters of which will fit into a prearranged pattern." In 1925 it said "the question of whether the puzzles are beneficial or harmful is in no urgent need of an answer. The craze evidently is dying out fast." Today, no U.S. newspaper is more closely identified with the crossword than The New York Times.[39]		In a more tragic example of historical irony, what people now refer to as the "First World War" was called by H.G. Wells "The war that will end war",[40] which soon became "The war to end war" and "The War to End All Wars", and this became a widespread truism, almost a cliché. Historical irony is therefore a subset of cosmic irony, but one in which the element of time is bound to play a role. Another example could be that of the Vietnam War, where in the 1960s the U.S. attempted to stop the Viet Cong (Viet Minh) taking over South Vietnam. However, it is an often ignored fact that, in 1941, the U.S. originally supported the Viet Minh in its fight against Japanese occupation.[41]		In the introduction to The Irony of American History, Andrew Bacevich writes:		After 9/11, the Bush administration announced its intention of bringing freedom and democracy to the people of the Middle East. Ideologues within the Bush administration persuaded themselves that American power, adroitly employed, could transform that region ... The results speak for themselves.[42]		Gunpowder was, according to prevailing academic consensus, discovered in the 9th century by Chinese alchemists searching for an elixir of immortality.[43]		Historical irony also includes inventors killed by their own creations, such as William Bullock—unless, due to the nature of the invention, the risk of death was always known and accepted, as in the case of Otto Lilienthal, who was killed by flying a glider of his own devising.		In certain kinds of situational or historical irony, a factual truth is highlighted by some person's complete ignorance of it or his belief in its opposite. However, this state of affairs does not occur by human design. In some religious contexts, such situations have been seen as the deliberate work of Divine Providence to emphasize truths and to taunt humans for not being aware of them when they could easily have been enlightened (this is similar to human use of irony). Such ironies are often more evident, or more striking, when viewed retrospectively in the light of later developments which make the truth of past situations obvious to all.		Other prominent examples of outcomes now seen as poignantly contrary to expectation include:		Irony is often used in literature to produce a comic effect. This may also be combined with satire. For instance, an author may facetiously state something as a well-known fact and then demonstrate through the narrative that the fact is untrue.		Jane Austen's Pride and Prejudice begins with the proposition "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife." In fact, it soon becomes clear that Austen means the opposite: women (or their mothers) are always in search of, and desperately on the lookout for, a rich single man to make a husband. The irony deepens as the story promotes this romance and ends in a double marriage proposal. "Austen's comic irony emerges out of the disjunction between Elizabeth's overconfidence (or pride) in her perceptions of Darcy and the narrator's indications that her views are in fact partial and prejudicial."[49]		"The Third Man is a film that features any number of eccentricities, each of which contributes to the film's perspective of comic irony as well as its overall cinematic self-consciousness."[50]		Writing about performances of Shakespeare's Othello in apartheid South Africa, Robert Gordon suggests: "Could it be that black people in the audience ... may have viewed as a comic irony his audacity and naïvety in thinking he could pass for white."[51]		Romantic irony is "an attitude of detached scepticism adopted by an author towards his or her work, typically manifesting in literary self-consciousness and self-reflection". This conception of irony originated with the German Romantic writer and critic Karl Wilhelm Friedrich Schlegel.[52]		Joseph Dane writes "From a twentieth-century perspective, the most crucial area in the history of irony is that described by the term romantic irony." He discusses the difficulty of defining romantic irony: "But what is romantic irony? A universal type of irony? The irony used by romantics? or an irony envisioned by the romantics and romanticists?" He also describes the arguments for and against its use.[53]		Referring to earlier self-conscious works such as Don Quixote and Tristram Shandy, Douglas Muecke points particularly to Peter Weiss's 1964 play, "Marat/Sade". This work is a play within a play set in a lunatic asylum, in which it is difficult to tell whether the players are speaking only to other players or also directly to the audience. When The Herald says, "The regrettable incident you've just seen was unavoidable indeed foreseen by our playwright", there is confusion as to who is being addressed, the "audience" on the stage or the audience in the theatre. Also, since the play within the play is performed by the inmates of a lunatic asylum, the theatre audience cannot tell whether the paranoia displayed before them is that of the players, or the people they are portraying. Muecke notes that, "in America, Romantic irony has had a bad press", while "in England ... [it] is almost unknown."[54]		However, in a book entitled English Romantic Irony, Anne Mellor, referring to Byron, Keats, Carlyle, Coleridge and Lewis Carroll, writes, "Romantic irony is both a philosophical conception of the universe and an artistic program. Ontologically, it sees the world as fundamentally chaotic. No order, no far goal of time, ordained by God or right reason, determines the progression of human or natural events." Furthermore,		Of course, romantic irony itself has more than one mode. The style of romantic irony varies from writer to writer. ... But however distinctive the voice, a writer is a romantic ironist if and when his or her work commits itself enthusiastically both in content and form to a hovering or unresolved debate between a world of merely man-made being and a world of ontological becoming.[55]		Similarly, metafiction is "Fiction in which the author self-consciously alludes to the artificiality or literariness of a work by parodying or departing from novelistic conventions (esp. naturalism) and narrative techniques."[56] It is a type of fiction that self-consciously addresses the devices of fiction, thereby exposing the fictional illusion.		Gesa Giesing writes that "the most common form of metafiction is particularly frequent in Romantic literature. The phenomenon is then referred to as Romantic Irony." Giesing notes that "There has obviously been an increased interest in metafiction again after World War II."[57]		For examples, Patricia Waugh quotes from several works at the top of her chapter headed "What is metafiction?". These include:		"The thing is this./ That of all the several ways of beginning a book ... I am confident my own way of doing it is best" - Tristram Shandy		"Since I've started this story, I've gotten boils ..." - The death of the novel and other stories by Ronald Sukenick[58]		Additionally, The Cambridge Introduction to Postmodern Fiction refers to John Fowles's The French Lieutenant's Woman:[59]		For the first twelve chapters ... the reader has been able to immerse him or herself in the story, enjoying the kind of 'suspension of disbelief ' required of realist novels ... what follows is a remarkable act of metafictional 'frame-breaking'. Chapter 13 notoriously begins:		I do not know. This story I am telling is all imagination. These characters I create never existed outside my own mind. ... if this is a novel, it cannot be a novel in the modern sense.		Socratic irony is "the dissimulation of ignorance practised by Socrates as a means of confuting an adversary".[60] Socrates would pretend to be ignorant of the topic under discussion, to draw out the inherent nonsense in the arguments of his interlocutors. The Chambers Dictionary defines it as "a means by which a questioner pretends to know less than a respondent, when actually he knows more".		Zoe Williams of The Guardian wrote: "The technique [of Socratic irony], demonstrated in the Platonic dialogues, was to pretend ignorance and, more sneakily, to feign credence in your opponent's power of thought, in order to tie him in knots."[61]		A more modern example of Socratic irony can be seen on the American crime fiction television series, Columbo. The character Lt. Columbo is seemingly naïve and incompetent. His untidy appearance adds to this fumbling illusion. As a result, he is underestimated by the suspects in murder cases he is investigating. With their guard down and their false sense of confidence, Lt. Columbo is able to solve the cases, leaving the murderers feeling duped and outwitted.[62]		Danish philosopher Søren Kierkegaard, and others, see irony, such as that used by Socrates, as a disruptive force with the power to undo texts and readers alike.[63] The phrase itself is taken from Hegel's Lectures on Aesthetics, and is applied by Kierkegaard to the irony of Socrates. This tradition includes 19th-century German critic and novelist Friedrich Schlegel ("On Incomprehensibility"), Charles Baudelaire, Stendhal, and the 20th century deconstructionist Paul de Man ("The Concept of Irony"). In Kierkegaard's words, from On the Concept of Irony with Continual Reference to Socrates:		[Socratic] irony [is] the infinite absolute negativity. It is negativity, because it only negates; it is infinite, because it does not negate this or that phenomenon; it is absolute, because that by virtue of which it negates is a higher something that still is not. The irony established nothing, because that which is to be established lies behind it...[64]		Where much of philosophy attempts to reconcile opposites into a larger positive project, Kierkegaard and others insist that irony—whether expressed in complex games of authorship or simple litotes—must, in Kierkegaard's words, "swallow its own stomach". Irony entails endless reflection and violent reversals, and ensures incomprehensibility at the moment it compels speech. Similarly, among other literary critics, writer David Foster Wallace viewed the pervasiveness of ironic and other postmodern tropes as the cause of "great despair and stasis in U.S. culture, and that for aspiring fictionists [ironies] pose terrifically vexing problems."[65]		The '90s saw an expansion of the definition of irony from "saying what one doesn't mean" into a "general stance of detachment from life in general",[66] this detachment serving as a shield against the awkwardness of everyday life. Humor from that era (most notably, Seinfeld) relies on the audience watching the show with some detachment from the show's typical signature awkward situations.		The generation of people in the United States who grew up in the 90s, (Millennials), are seen as having this same sort of detachment from serious or awkward situations in life, as well. Hipsters are thought to use irony as a shield against those same serious or genuine confrontations.[67]		Some speakers of English complain that the words irony and ironic are often misused,[68] though the more general casual usage of a contradiction between circumstance and expectation originates in the 1640s.[69][examples needed]		Dan Shaughnessy wrote:		We were always kidding about the use of irony. I maintained that it was best never to use the word because it was too often substituted for coincidence. (Alanis Morissette's song "Isn't it Ironic?" cites multiple examples of things that are patently not ironic.)[70]		Tim Conley cites the following: "Philip Howard assembled a list of seven implied meanings for the word "ironically", as it opens a sentence:		No agreed-upon method for indicating irony in written text exists, though many ideas have been suggested. For instance, an irony punctuation mark was proposed in the 1580s, when Henry Denham introduced a rhetorical question mark or percontation point, which resembles a reversed question mark. This mark was also advocated by the French poet Marcel Bernhardt at the end of the 19th century, to indicate irony or sarcasm. French writer Hervé Bazin suggested another pointe d'ironie: the Greek letter psi Ψ with a dot below it, while Tom Driberg recommended that ironic statements should be printed in italics that lean the other way from conventional italics.[72]		
Neuroscience (or neurobiology) is the scientific study of the nervous system.[1] It is a multidisciplinary branch of biology,[2] that deals with the anatomy, biochemistry, molecular biology, and physiology of neurons and neural circuits. It also draws upon other fields, with the most obvious being pharmacology, psychology, and medicine.[3][4][5][6][7][8]		The scope of neuroscience has broadened over time to include different approaches used to study the molecular, cellular, developmental, structural, functional, evolutionary, computational, psychosocial and medical aspects of the nervous system. Neuroscience has also given rise to such other disciplines as neuroeducation,[9] neuroethics, and neurolaw. The techniques used by neuroscientists have also expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory and motor tasks in the brain. Recent theoretical advances in neuroscience have also been aided by the study of neural networks.		As a result of the increasing number of scientists who study the nervous system, several prominent neuroscience organizations have been formed to provide a forum to all neuroscientists and educators. For example, the International Brain Research Organization was founded in 1960,[10] the International Society for Neurochemistry in 1963,[11] the European Brain and Behaviour Society in 1968,[12] and the Society for Neuroscience in 1969.[13]						The earliest study of the nervous system dates to ancient Egypt. Trepanation, the surgical practice of either drilling or scraping a hole into the skull for the purpose of curing headaches or mental disorders, or relieving cranial pressure, was first recorded during the Neolithic period. Manuscripts dating to 1700 BC indicate that the Egyptians had some knowledge about symptoms of brain damage.[14]		Early views on the function of the brain regarded it to be a "cranial stuffing" of sorts. In Egypt, from the late Middle Kingdom onwards, the brain was regularly removed in preparation for mummification. It was believed at the time that the heart was the seat of intelligence. According to Herodotus, the first step of mummification was to "take a crooked piece of iron, and with it draw out the brain through the nostrils, thus getting rid of a portion, while the skull is cleared of the rest by rinsing with drugs."[15]		The view that the heart was the source of consciousness was not challenged until the time of the Greek physician Hippocrates. He believed that the brain was not only involved with sensation—since most specialized organs (e.g., eyes, ears, tongue) are located in the head near the brain—but was also the seat of intelligence. Plato also speculated that the brain was the seat of the rational part of the soul.[16] Aristotle, however, believed the heart was the center of intelligence and that the brain regulated the amount of heat from the heart.[17] This view was generally accepted until the Roman physician Galen, a follower of Hippocrates and physician to Roman gladiators, observed that his patients lost their mental faculties when they had sustained damage to their brains.		Abulcasis, Averroes, Avicenna, Avenzoar, and Maimonides, active in the Medieval Muslim world, described a number of medical problems related to the brain. In Renaissance Europe, Vesalius (1514–1564), René Descartes (1596–1650), and Thomas Willis (1621–1675) also made several contributions to neuroscience.		In the first half of the 19th century, Jean Pierre Flourens pioneered the experimental method of carrying out localized lesions of the brain in living animals describing their effects on motricity, sensibility and behavior. Studies of the brain became more sophisticated after the invention of the microscope and the development of a staining procedure by Camillo Golgi during the late 1890s. The procedure used a silver chromate salt to reveal the intricate structures of individual neurons. His technique was used by Santiago Ramón y Cajal and led to the formation of the neuron doctrine, the hypothesis that the functional unit of the brain is the neuron.[18] Golgi and Ramón y Cajal shared the Nobel Prize in Physiology or Medicine in 1906 for their extensive observations, descriptions, and categorizations of neurons throughout the brain. While Luigi Galvani's pioneering work in the late 1700s had set the stage for studying the electrical excitability of muscles and neurons, it was in the late 19th century that Emil du Bois-Reymond, Johannes Peter Müller, and Hermann von Helmholtz demonstrated that the electrical excitation of neurons predictably affected the electrical states of adjacent neurons,[19][citation needed] and Richard Caton found electrical phenomena in the cerebral hemispheres of rabbits and monkeys.		In parallel with this research, work with brain-damaged patients by Paul Broca suggested that certain regions of the brain were responsible for certain functions. At the time, Broca's findings were seen as a confirmation of Franz Joseph Gall's theory that language was localized and that certain psychological functions were localized in specific areas of the cerebral cortex.[20][21] The localization of function hypothesis was supported by observations of epileptic patients conducted by John Hughlings Jackson, who correctly inferred the organization of the motor cortex by watching the progression of seizures through the body. Carl Wernicke further developed the theory of the specialization of specific brain structures in language comprehension and production. Modern research through neuroimaging techniques, still uses the Brodmann cerebral cytoarchitectonic map (referring to study of cell structure) anatomical definitions from this era in continuing to show that distinct areas of the cortex are activated in the execution of specific tasks.[22]		During the 20th century, neuroscience began to be recognized as a distinct academic discipline in its own right, rather than as studies of the nervous system within other disciplines. Eric Kandel and collaborators have cited David Rioch, Francis O. Schmitt, and Stephen Kuffler as having played critical roles in establishing the field.[23] Rioch originated the integration of basic anatomical and physiological research with clinical psychiatry at the Walter Reed Army Institute of Research, starting in the 1950s. During the same period, Schmitt established a neuroscience research program within the Biology Department at the Massachusetts Institute of Technology, bringing together biology, chemistry, physics, and mathematics. The first freestanding neuroscience department (then called Psychobiology) was founded in 1964 at the University of California, Irvine by James L. McGaugh.[citation needed] This was followed by the Department of Neurobiology at Harvard Medical School which was founded in 1966 by Stephen Kuffler.[citation needed]		The understanding of neurons and of nervous system function became increasingly precise and molecular during the 20th century. For example, in 1952, Alan Lloyd Hodgkin and Andrew Huxley presented a mathematical model for transmission of electrical signals in neurons of the giant axon of a squid, which they called "action potentials", and how they are initiated and propagated, known as the Hodgkin–Huxley model. In 1961–1962, Richard FitzHugh and J. Nagumo simplified Hodgkin–Huxley, in what is called the FitzHugh–Nagumo model. In 1962, Bernard Katz modeled neurotransmission across the space between neurons known as synapses. Beginning in 1966, Eric Kandel and collaborators examined biochemical changes in neurons associated with learning and memory storage in Aplysia. In 1981 Catherine Morris and Harold Lecar combined these models in the Morris–Lecar model. Such increasingly quantitative work gave rise to numerous biological neuron models.		The scientific study of the nervous system has increased significantly during the second half of the twentieth century, principally due to advances in molecular biology, electrophysiology, and computational neuroscience. This has allowed neuroscientists to study the nervous system in all its aspects: how it is structured, how it works, how it develops, how it malfunctions, and how it can be changed. For example, it has become possible to understand, in much detail, the complex processes occurring within a single neuron. Neurons are cells specialized for communication. They are able to communicate with neurons and other cell types through specialized junctions called synapses, at which electrical or electrochemical signals can be transmitted from one cell to another. Many neurons extrude long thin filaments of protoplasm called axons, which may extend to distant parts of the body and are capable of rapidly carrying electrical signals, influencing the activity of other neurons, muscles, or glands at their termination points. A nervous system emerges from the assemblage of neurons that are connected to each other.		In vertebrates, the nervous system can be split into two parts, the central nervous system (brain and spinal cord), and the peripheral nervous system. In many species — including all vertebrates — the nervous system is the most complex organ system in the body, with most of the complexity residing in the brain. The human brain alone contains around one hundred billion neurons and one hundred trillion synapses; it consists of thousands of distinguishable substructures, connected to each other in synaptic networks whose intricacies have only begun to be unraveled. The majority of the approximately 20,000–25,000 genes belonging to the human genome are expressed specifically in the brain. Due to the plasticity of the human brain, the structure of its synapses and their resulting functions change throughout life.[24] Thus the challenge of making sense of all this complexity is formidable.		The study of the nervous system can be done at multiple levels, ranging from the molecular and cellular levels to the systems and cognitive levels. At the molecular level, the basic questions addressed in molecular neuroscience include the mechanisms by which neurons express and respond to molecular signals and how axons form complex connectivity patterns. At this level, tools from molecular biology and genetics are used to understand how neurons develop and how genetic changes affect biological functions. The morphology, molecular identity, and physiological characteristics of neurons and how they relate to different types of behavior are also of considerable interest.		The fundamental questions addressed in cellular neuroscience include the mechanisms of how neurons process signals physiologically and electrochemically. These questions include how signals are processed by neurites – thin extensions from a neuronal cell body, consisting of dendrites (specialized to receive synaptic inputs from other neurons) and axons (specialized to conduct nerve impulses called action potentials) – and somas (the cell bodies of the neurons containing the nucleus), and how neurotransmitters and electrical signals are used to process information in a neuron. Another major area of neuroscience is directed at investigations of the development of the nervous system. These questions include the patterning and regionalization of the nervous system, neural stem cells, differentiation of neurons and glia, neuronal migration, axonal and dendritic development, trophic interactions, and synapse formation.		Computational neurogenetic modeling is concerned with the study and development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes.		At the systems level, the questions addressed in systems neuroscience include how neural circuits are formed and used anatomically and physiologically to produce functions such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory. In other words, they address how these neural circuits function and the mechanisms through which behaviors are generated. For example, systems level analysis addresses questions concerning specific sensory and motor modalities: how does vision work? How do songbirds learn new songs and bats localize with ultrasound? How does the somatosensory system process tactile information? The related fields of neuroethology and neuropsychology address the question of how neural substrates underlie specific animal and human behaviors. Neuroendocrinology and psychoneuroimmunology examine interactions between the nervous system and the endocrine and immune systems, respectively. Despite many advancements, the way networks of neurons perform complex cognitive processes and behaviors is still poorly understood.		At the cognitive level, cognitive neuroscience addresses the questions of how psychological functions are produced by neural circuitry. The emergence of powerful new measurement techniques such as neuroimaging (e.g., fMRI, PET, SPECT), electrophysiology, and human genetic analysis combined with sophisticated experimental techniques from cognitive psychology allows neuroscientists and psychologists to address abstract questions such as how human cognition and emotion are mapped to specific neural substrates. Although many studies still hold a reductionist stance looking for the neurobiological basis of cognitive phenomena, recent research shows that there is an interesting interplay between neuroscientific findings and conceptual research, soliciting and integrating both perspectives. For example, the neuroscience research on empathy solicited an interesting interdisciplinary debate involving philosophy, psychology and psychopathology.[25] Moreover, the neuroscientific identification of multiple memory systems related to different brain areas has challenged the idea of memory as a literal reproduction of the past, supporting a view of memory as a generative, constructive and dynamic process.[26]		Neuroscience is also allied with the social and behavioral sciences as well as nascent interdisciplinary fields such as neuroeconomics, decision theory, and social neuroscience to address complex questions about interactions of the brain with its environment.		Ultimately neuroscientists would like to understand every aspect of the nervous system, including how it works, how it develops, how it malfunctions, and how it can be altered or repaired. The specific topics that form the main foci of research change over time, driven by an ever-expanding base of knowledge and the availability of increasingly sophisticated technical methods. Over the long term, improvements in technology have been the primary drivers of progress. Developments in electron microscopy, computers, electronics, functional brain imaging, and most recently genetics and genomics, have all been major drivers of progress.		Most studies in neurology have too few test subjects to be scientifically sure. Those insufficient size studies are the basis for all domain-specific diagnoses in neuropsychiatry, since the few large enough studies there are always find individuals with the brain changes thought to be associated with a mental condition but without any of the symptoms. The only diagnoses that can be validated through large enough brain studies are those on serious brain damages and neurodegenerative diseases that destroy most of the brain.[27][28]		Neurology, psychiatry, neurosurgery, psychosurgery, anesthesiology and pain medicine, neuropathology, neuroradiology, ophthalmology, otolaryngology, clinical neurophysiology, addiction medicine, and sleep medicine are some medical specialties that specifically address the diseases of the nervous system. These terms also refer to clinical disciplines involving diagnosis and treatment of these diseases. Neurology works with diseases of the central and peripheral nervous systems, such as amyotrophic lateral sclerosis (ALS) and stroke, and their medical treatment. Psychiatry focuses on affective, behavioral, cognitive, and perceptual disorders. Anesthesiology focuses on perception of pain, and pharmacologic alteration of consciousness. Neuropathology focuses upon the classification and underlying pathogenic mechanisms of central and peripheral nervous system and muscle diseases, with an emphasis on morphologic, microscopic, and chemically observable alterations. Neurosurgery and psychosurgery work primarily with surgical treatment of diseases of the central and peripheral nervous systems. The boundaries between these specialties have been blurring recently as they are all influenced by basic research in neuroscience. Brain imaging also enables objective, biological insights into mental illness, which can lead to faster diagnosis, more accurate prognosis, and help assess patient progress over time.[29]		Integrative neuroscience makes connections across these specialized areas of focus.		Modern neuroscience education and research activities can be very roughly categorized into the following major branches, based on the subject and scale of the system in examination as well as distinct experimental or curricular approaches. Individual neuroscientists, however, often work on questions that span several distinct subfields.		The largest professional neuroscience organization is the Society for Neuroscience (SFN), which is based in the United States but includes many members from other countries. Since its founding in 1969 the SFN has grown steadily: as of 2010 it recorded 40,290 members from 83 different countries.[33] Annual meetings, held each year in a different American city, draw attendance from researchers, postdoctoral fellows, graduate students, and undergraduates, as well as educational institutions, funding agencies, publishers, and hundreds of businesses that supply products used in research.		Other major organizations devoted to neuroscience include the International Brain Research Organization (IBRO), which holds its meetings in a country from a different part of the world each year, and the Federation of European Neuroscience Societies (FENS), which holds a meeting in a different European city every two years. FENS comprises a set of 32 national-level organizations, including the British Neuroscience Association, the German Neuroscience Society (Neurowissenschaftliche Gesellschaft), and the French Société des Neurosciences. The first National Honor Society in Neuroscience, Nu Rho Psi, was founded in 2006.		In 2013, the BRAIN Initiative was announced in the US.		In addition to conducting traditional research in laboratory settings, neuroscientists have also been involved in the promotion of awareness and knowledge about the nervous system among the general public and government officials. Such promotions have been done by both individual neuroscientists and large organizations. For example, individual neuroscientists have promoted neuroscience education among young students by organizing the International Brain Bee, which is an academic competition for high school or secondary school students worldwide.[34] In the United States, large organizations such as the Society for Neuroscience have promoted neuroscience education by developing a primer called Brain Facts,[35] collaborating with public school teachers to develop Neuroscience Core Concepts for K-12 teachers and students,[36] and cosponsoring a campaign with the Dana Foundation called Brain Awareness Week to increase public awareness about the progress and benefits of brain research.[37] In Canada, the CIHR Canadian National Brain Bee is held annually at McMaster University.[38]		Finally, neuroscientists have also collaborated with other education experts to study and refine educational techniques to optimize learning among students, an emerging field called educational neuroscience.[39] Federal agencies in the United States, such as the National Institute of Health (NIH)[40] and National Science Foundation (NSF),[41] have also funded research that pertains to best practices in teaching and learning of neuroscience concepts!		
The VIA Inventory of Strengths (VIA-IS), formerly known as the "Values in Action Inventory," is a psychological assessment measure designed to identify an individual’s profile of character strengths.		It was created by Christopher Peterson and Martin Seligman, well-known researchers in the field of positive psychology, in order to operationalize their Character Strengths and Virtues Handbook (CSV).[1] The CSV is the positive psychology counterpart to the Diagnostic and Statistical Manual of Mental Disorders (DSM) used in traditional psychology.[1] Unlike the DSM, which scientifically categorizes human deficits and disorders, the CSV classifies positive human strengths.[2] Moreover, the CSV is centered on helping people recognize and build upon their strengths. This aligned with the overall goal of the positive psychology movement, which aims to make people’s lives more fulfilling, rather than simply treating mental illness.[2] Notably, the VIA-IS is the tool by which people can identify their own positive strengths and learn how to capitalize on them.[2]						The VIA-IS is composed of a 240 item measure of 24 character strengths (10 items per strength).[1] On average, an individual will complete the VIA-IS in 30 to 40 minutes.[1] Since 2001, the survey is available online for free at www.viacharacter.org and over 400,000 people have participated so far.[4] Participants are instructed to answer each item on the VIA-IS in terms of “whether the statement describes what you are like”.[4] Participants respond according to a 5-point Likert scale ranging from (1= very much unlike me, 5= very much like me).[4] Sample items include “I find the world a very interesting place”, which gauges curiosity, and “I always let bygone be bygones”, which gauges forgiveness.[1] People can score anywhere from 10 to 50 points for each of the 24 strengths. Moreover, a higher score on a scale indicates that the participant more strongly identifies with that scale's associated strength. Score reports are delivered to each participant at the completion of the survey. Feedback is provided for the signature strengths, but not for the lesser strengths. The results rank order the participant’s strengths from 1-24, with the top 4-7 strengths considered “signature strengths”.		As a relatively new field of research, positive psychology lacked a common vocabulary for discussing measurable positive traits before 2004.[1] Traditional psychology benefited from the creation of DSM, as it provided researchers and clinicians with the same set of language from which they could talk about the negative. As a first step in remedying this disparity between tradition and positive psychology, Peterson and Seligman set out to identify, organize and measure character.		Peterson & Seligman began by defining the notion of character as traits that are possessed by an individual and are stable over time, but can still be impacted by setting and thus are subject to change.[1] The researchers then started the process of identifying character strengths and virtues by brainstorming with a group of noted positive psychology scholars. Then, Peterson & Seligman examined ancient cultures (including their religions, politics, education and philosophies) for information about how people in the past construed human virtue. The researchers looked for virtues that were present across cultures and time. Six core virtues emerged from their analysis: courage, justice, humanity, temperance, transcendence and wisdom.		Next, Peterson and Seligman proposed a model of classification which includes horizontal and vertical components. The hierarchical system is modeled after the Linnaean classification of species, which ranges from a specific species to more general and broad categories. The scientists stated the six core values are the broadest category and are, “core characteristics valued by moral philosophers and religious thinkers” (p. 13).[1] Peterson and Seligman then moved down the hierarchy to identifying character strengths, which are, “the psychological processes or mechanisms that define the virtues” (p. 13).[1]		The researchers began the process of identifying individual character strengths by brainstorming with a group of noted positive psychology scholars.[1] This exercise generated a list of human strengths, which were helpful when consulting with Gallup Organization. Peterson and Seligman then performed an exhaustive literature search for work that directly addresses good character in the domains of, “psychiatry, youth development, philosophy and psychology” (p. 15). Some individuals who influenced Peterson and Seligman’s choice of strengths include: Abraham Maslow, Erik Erikson, Ellen Greenberger, Marie Jahoda, Carol Ryff, Michael Cawley, Howard Gardner, Shalom Schwartz. In an effort to leave no stone unturned, the researchers also looked for virtue-laden messages in popular culture. For example, the researchers examined Hallmark greeting cards, personal ads, graffiti, bumper stickers and profiles of Pokémon characters.		After identifying dozens of ‘candidate strengths’, the researchers needed to find a way to further refine their list. Therefore, Peterson & Seligman developed a list of 10 criteria (e.g., strengths must contribute to a sense of a fulfilling life, must be intrinsically valuable) to help them select the final 24 strengths for the CSV (see CSV for complete list of criteria). Approximately half of the strengths included in the CSV meet all 10 criteria, and half do not.[1] By looking for similarities between candidate strengths, the researchers distributed 24 character strengths between six virtue categories. Only after creating this a priori organization of traits, the researchers performed, “an exploratory factor analysis of scale scores using varimax rotation,” (p. 632) from which five factors emerged.[1] Peterson & Seligman state that they are not as concerned with how the 24 strengths are grouped into virtue clusters because, in the end, these traits are mixed together to form the character of a person.		Peterson and Seligman state that all character strengths must be measurable.[1] Of the 24 strengths, most can be assessed using self-report questionnaires, behavioral observation, peer-report methods and clinical interviews. Three strengths, however, have yet to be reliably assessed: humility, modesty and bravery.[1] The researchers acknowledge that some strengths are more difficult to assess than others, therefore methods of assessing these strengths are still in-progress.		For each strength, there are typically several measures that could be administered in order to assess a person's trait level for that strength.[1] Time and energy, however, prohibit administering all of the measures for the 24 strengths in one testing session. To solve this problem, Peterson & Seligman designed a new measure, the VIA-IS, to assess all 24 strengths in relatively brief amount of time. Beginning in the fall of 2000, the researchers pilot tested the VIA-IS with a group of 250 adults.[1] The researchers removed items that correlated poorly with the rest of the items in the same scale of interest. Peterson & Seligman repeated this process until Cronbach's alpha for all scales exceeded .70. Along the way, the researchers added in 3 reverse-scored items in each of the 24 scales as well. For the current version of the VIA-IS, test-retest correlations for all scales during a 4-month period are > .70.[1]		Peterson & Seligman (2004) provide limited data on the validity and reliability of the VIA-IS. In fact, the only published statistics are stated above. The researchers say that they will provide the full statistical results of their analysis of the VIA-IS in a future publication.[1] However, other researchers have published studies that challenge the validity of this 6 factor structure (Shryack, Steger, Krueger & Kallie, 2010; Singh & Choubisa, 2010).		Although researchers have not yet examined the validity and reliability of the VIA-IS, they are beginning to look at how the 24 character strengths are distributed within the United States and international populations. Researchers found that, within the United States, the most commonly endorsed strengths are kindness, fairness, honesty, gratitude and judgment.[5] The lesser strengths demonstrated consistency across states and regions as well: prudence, modesty and self-regulation.[5] The researchers did not find regional differences in the rank-order of strengths, with the exception of the South demonstrating slightly higher scores for religiousness.		When the rank order of strengths in the U.S. is compared to that of 53 other countries, scientists found the relative pattern of rank ordering did not differ.[6] This finding provides evidence to support Peterson & Seligman’s (2004) assertion that their classification system is composed of universally acknowledged strengths.		The results of this study do have limitations. More specifically, respondents to the survey must speak English, as the VIA-IS was not translated into each respondent’s native language. This may restrict the extension of these results to non-English speakers.		In an earlier study, researchers administered the English-language version of the VIA-IS to individuals in 40 countries (Steen, Park & Peterson, 2005; in Park, Peterson & Seligman, 2005). Worldwide, the following strengths were most associated with positive life satisfaction: hope, zest, gratitude and love. The researchers called these strengths of the heart'. Moreover, strengths associated with knowledge, such as love of learning and curiosity, were least correlated with life satisfaction.		Scientists have also performed more in-depth analyses of the VIA-IS when it is applied to populations outside of the United States. Unlike Park, Peterson & Seligman (2006), Linley and colleagues (2007) did not simply compare the rank-order of strengths of the U.S. to other countries. Linley and colleagues (2007) administered the VIA-IS to 17,056 individuals living in the United Kingdom between 2002 & 2005. Compared to the entire U.K. population, the study’s sample was better educated, composed of more women and fewer elderly individuals.		The researchers found that as people aged, strength scores tended to increase. Using Pearson’s correlations, researchers looked for associations between age and strengths. The following strengths showed the strongest correlations: love of learning, curiosity, forgiveness, self-regulation and fairness.[4] Humor, however, did not follow this pattern and was negatively correlated with age.		In terms of statistically significant gender differences, women demonstrated higher scores for interpersonal strengths (kindness, love and social intelligence) and appreciation of beauty and gratitude.[4] Men scored significantly higher than women on creativity. For men and women, four of the top five signature strengths were the same: open-mindedness, fairness, curiosity and love of learning.		When the means and standard deviations were broken down by gender and age, they were consistent with those reported by U.S. samples.[7] The rank ordering of strengths were comparable to the patterns found in the U.S. and other international samples.[5] Once again, research supports Peterson & Seligman’s (2004) assertion that the strengths listed in the CSV and VIA-IS are present in the majority of cultures.		An important limitation of this study, as with all studies that collect data via the internet, is that the samples tend to be more educated and from higher socioeconomic background because these individuals are more likely to have access and knowledge of the internet.		Unlike previous studies, Shimai and colleagues (2007) tested the applicability of a translated version of the VIA-IS to a sample in Japan. The researchers administered the VIA-IS to 308 young adults from Japan and 1099 young adults from the U.S. The scientists translated the VIA-IS into Japanese and then back to English in order to be examined by the original creators of the VIA-IS. They confirmed that the Japanese version of the VIA-IS demonstrated face validity, test-retest reliability and internal consistency before administering it to young adults.[8]		The researchers found that top-ranked strengths (in terms of prevalence) for young adults in Japan, were similar to those of young adults in the U.S. The percentage of people who scored high or low on each character strength were similar between the two countries.[8] Moreover, the scientists did not find a significant variation in the pattern of gender differences between the United States and Japan. Women in both countries were more likely than men to score highly on the strengths of kindness, love, gratitude, teamwork and appreciation of beauty, whereas men in both countries were more likely score highly on the strengths of open-mindedness, perspective, creativity, self-regulation and bravery. The correlations between specific strengths and happiness outcomes were consistent as well.[8] More specifically, the strengths of zest, curiosity, gratitude, and hope were significantly positively correlated with subjective measures of happiness for both populations.		Differences between the young adults in Japan and the U.S. emerged as well.[8] The rank-order of religiousness was the biggest difference between the cultures. For American young adults, religiousness was on average, the 14th most prevalent strength. For Japanese young adults, religiousness was, on average, the 19th most prevalent strength. The researchers attributed this finding to the fact that some of the items on the VIA-IS that assess religiousness were based on Western connotations of religiosity (e.g. monotheistic traditions).		A notable limitation of this study is that the researchers examined young adults, rather than the population at-large. According to the researchers, young adults in Japan are more active participants in a more global, Americanized culture than the older generations. This could explain the commonalities found between young adults in Japan and the US.		Overall, Shimai and colleagues demonstrated that the VIA-IS can be successfully and accurately translated into other languages. When this is done, however, researchers will need to ensure that the items on the scale are not culturally biased toward Western concepts.[8]		One of the major goals of positive psychology is to help people “cultivate and sustain the good life” (p. 640).[1] The creation of the VIA-IS provides a practical measure that can be used to evaluate the efficacy of these positive interventions. As one example, consider the thousands of people participate in life coaching and character education programs every year (Eccles & Gootman, 2002; in Peterson & Seligman, 2004). Strengths of character are often the outcome of interest, yet these programs do not employ a rigorous outcome measure in order to gauge efficacy.[1] Researchers propose that if these programs used the VIA-IS, then they may discover unanticipated benefits of their interventions and would facilitate objective evaluation of its outcome.		Peterson & Seligman (2004) suggest that the VIA-IS could be used as a way to help people identify their signature strengths. With this knowledge, people could then begin to capitalize and build upon their signature strengths. Positive psychologists argue that the VIA-IS should not be used as a way to identify your ‘lesser strengths’ or weaknesses.[2] Their approach departs from the medical model of traditional psychology, which focuses on fixing deficits. In contrast, positive psychologists emphasize that people should focus and build upon what they are doing well.		Only 3 studies have checked the factor structure of the CSV, on which the VIA-IS is based.[1][9][10]		Using a second order factor analysis, Macdonald & colleagues (2008) found that the 24 strengths did not fit into the 6 higher order virtues model proposed in the CSV. None of the clusters of characters strengths that they found resembled the structure of the 6 virtue clusters of strengths. The researchers noted that many of the VIA character strengths cross-loaded onto multiple factors. Rather, the strengths were best represented by a one and four factor model. A one factor model would mean that the strengths are best accounted for by, “one overarching factor,” such as a global trait of character (p. 797).[9] A four factor model more closely resembles the 'Big Five' model of personality. The character strengths in the four factor model could be organized into the following four groups: Niceness, Positivity, Intellect and Conscientiousness.[citation needed]		Peterson and Seligman (2004) conducted a factor analysis and found that a five factor model, rather than their 6 hierarchical virtues model, best organized the strengths. Their study, however, did not include five of the character strengths in the results of their analysis. The researchers most likely did this because their results were plagued by the problem of strengths cross-loading on to multiple factors, similar to what occurred in Macdonald and colleagues (2008) study.[10] Clearly, empirical evidence casts doubt on the link proposed by Peterson & Seligman (2004) between the 24 strengths and associated 6 higher order virtues.		Brdar & Kashdan (2009) used more precise statistical tools to build upon the findings of the two earlier studies. They found that a four factor model (Interpersonal Strengths, Vitality, Fortitude and Cautiousness) explained 60% of the variance. One large, overarching factor explained 50% of the variance. The four factors found by Brdar and Kashdan (2009) are similar to the four factors found by Macdonald and colleagues (2008). Once again, the Brdar and Kashdan found that the 24 strengths did not fall into the 6 higher order virtues proposed by Peterson and Seligman (2004). The correlations found between many of the strengths demonstrates that each strength is not distinct, which contradicts the claims made by the creators of the VIA-IS.		Caution should be taken in interpreting the results from these three studies as their samples differ in age and country of origin.[10]		Shryack, J., Steger, M. F., Krueger, R. F., & Kallie, C. S. (2010). The structure of virtue: An empirical investigation of the dimensionality of the virtues in action inventory of strengths. Personality and Individual Differences 48, 714–719. Singh, K., & Choubisa, R. (2010). Empirical validation of values in Action-Inventory of Strengths (VIA-IS) in Indian context. Psychological Studies, 55, 151–158.		
A maternal insult (also referred to as a "yo mama" joke) is a reference to a person's mother through the use of phrases such as "your mother" or other regional variants, frequently used to insult the target by way of their mother.[1] Used as an insult, "your mother ..." preys on widespread sentiments of filial piety, making the insult particularly and globally offensive. "Your mother" can be combined with most types of insults, although suggestions of promiscuity are particularly common.[2] Insults based on obesity, incest, age, race, poverty, poor hygiene, unattractiveness, or stupidity may also be used. Compared to other types of insults, "your mother" insults are especially likely to incite violence.[3] Slang variants such as "yo mama", "yo momma", "yer ma", "ya mum", "your mum" or "your mom" are sometimes used, depending on the local dialect. Insults involving "your mother" are commonly used when playing the dozens.		Although the phrase has a long history of including a description portion (such as the old "your mother wears combat boots", which implied that one's mother worked as a prostitute in the military[citation needed]), the phrase "yo mama" by itself, without any qualifiers, has become commonly used as an all-purpose insult[1] or an expression of defiance.						"Your mom", sometimes also "yo momma", generally depicts an obese, old, short, hairy, permissive, poor, unintelligent and ugly person. The phrase is usually not literally meant as a direct insult to a person's mother, but is supposed to describe a type of imaginative fantasy person with the said characteristics. When visually depicted, "your mom" is usually ugly, neglected or absurdly obese. These attributes are also seen in the intro of a maternal insult, e.g. "Your mom is so fat,...", but sometimes occur later within the joke or even deliver the punchline itself.		Sometimes, even the gender of the "mom" is doubted, which is reflected in a lacking femininity or excessive masculinity.		In the Bible, King Joram is greeted by the rebel Jehu with a hostile expression concerning Joram's mother:		When Joram saw Jehu, he said, "Is it peace, Jehu?" And he answered, "What peace, so long as the harlotries of your mother Jezebel and her witchcrafts are so many?"[4]		William Shakespeare used such a device in Act I Scene 1 of Timon of Athens, implying that a character's mother is a "bitch":		Painter: "Y'are a dog." Apemantus: "Thy mother's of my generation. What's she, if I be a dog?"		Also in Act IV, Scene II of Titus Andronicus, Aaron taunts his lover's sons:		Demetrius: "Villain, what hast thou done?" Aaron: "That which thou canst not undo." Chiron: "Thou hast undone our mother." Aaron: "Villain, I have done thy mother."		
Slapstick is a style of humor involving exaggerated physical activity which exceeds the boundaries of normal physical comedy.[1][2][3] The term arises from a device developed during the broad, physical comedy style known as Commedia dell'arte in 16th Century Italy. The 'slap stick' consists of two thin slats of wood made from splitting a single long stick, which make a 'slap' when striking another actor, with little force needed to make a loud - and comical - sound. The physical slap stick remains a key component of the plot in the traditional and popular Punch and Judy puppet show.						The name "slapstick" originates from the Italian language word batacchio or bataccio — called the "slap stick" in English — a club-like object composed of two wooden slats used in commedia dell'arte. When struck, the batacchio produces a loud smacking noise, though little force transfers from the object to the person being struck. Actors may thus hit one another repeatedly with great audible effect while causing no damage and only very minor, if any, pain. Along with the inflatable bladder (of which the whoopee cushion is a modern variant), it was among the earliest special effects.		Slapstick comedy's history is measured in centuries. Shakespeare incorporated many chase scenes and beatings into his comedies, such as in his play The Comedy of Errors. In early 19th century England, pantomime acquired its present form which includes slapstick comedy, while comedy routines also featured heavily in British music hall theatre which became popular in the 1850s.[4][5]		In Punch and Judy shows, a large slapstick is wielded by Punch against the other characters.		British comedians who honed their skills at pantomime and music hall sketches include Charlie Chaplin, Stan Laurel, George Formby and Dan Leno.[6][7] The influential English music hall comedian and theatre impresario Fred Karno developed a form of sketch comedy without dialogue in the 1890s, and Chaplin and Laurel were among the young comedians who worked for him as part of "Fred Karno's Army".[6] American producer Hal Roach described Fred Karno as "not only a genius, he is the man who originated slapstick comedy. We in Hollywood owe much to him."[8]		Building on its later popularity in the 19th and early 20th-century ethnic routines of the American vaudeville house, the style was explored extensively during the "golden era" of black and white, silent movies directed by figures Mack Sennett and Hal Roach and featuring such notables as Charlie Chaplin, Laurel and Hardy, the Marx Brothers, the Keystone Cops, the Three Stooges, and Chespirito. Slapstick is also common in Disney's Goofy shorts, MGM's Tom and Jerry, Warner Bros. Looney Tunes/Merrie Melodies, MGM's Barney Bear, and Tex Avery's Screwy Squirrel. Silent slapstick comedy was also popular in early French films and included films by Max Linder and Charles Prince.		Slapstick continues to maintain a presence in modern comedy that draws upon its lineage, running in film from Buster Keaton and Louis de Funès to Mel Brooks to the television series Jackass and comedy movies by the Farrelly Brothers, and in live performance from Weber and Fields to Jackie Gleason to Rowan Atkinson. In England, slapstick was a main element of the Monty Python comedy troupe and in television series such as Fawlty Towers and The Benny Hill Show. Slapstick has remained a popular art form to the present day.		
On January 28, 1986, the NASA shuttle orbiter mission STS-51-L and the tenth flight of Space Shuttle Challenger (OV-99) broke apart 73 seconds into its flight, killing all seven crew members, which consisted of five NASA astronauts and two payload specialists. The spacecraft disintegrated over the Atlantic Ocean, off the coast of Cape Canaveral, Florida, at 11:39 EST (16:39 UTC). Disintegration of the vehicle began after an O-ring seal in its right solid rocket booster (SRB) failed at liftoff. The O-ring was not designed to fly under unusually cold conditions as in this launch. Its failure caused a breach in the SRB joint it sealed, allowing pressurized burning gas from within the solid rocket motor to reach the outside and impinge upon the adjacent SRB aft field joint attachment hardware and external fuel tank. This led to the separation of the right-hand SRB's aft field joint attachment and the structural failure of the external tank. Aerodynamic forces broke up the orbiter.		The crew compartment and many other vehicle fragments were eventually recovered from the ocean floor after a lengthy search and recovery operation. The exact timing of the death of the crew is unknown; several crew members are known to have survived the initial breakup of the spacecraft. The shuttle had no escape system,[1][2] and the impact of the crew compartment with the ocean surface was too violent to be survivable.[3]		The disaster resulted in a 32-month hiatus in the shuttle program and the formation of the Rogers Commission, a special commission appointed by United States President Ronald Reagan to investigate the accident. The Rogers Commission found NASA's organizational culture and decision-making processes had been key contributing factors to the accident,[4] with the agency violating its own safety rules. NASA managers had known since 1977 that contractor Morton Thiokol's design of the SRBs contained a potentially catastrophic flaw in the O-rings, but they had failed to address this problem properly. NASA managers also disregarded warnings (an example of "go fever") from engineers about the dangers of launching posed by the low temperatures of that morning, and failed to adequately report these technical concerns to their superiors.		As a result of the disaster, the Air Force decided to cancel its plans to use the Shuttle for classified military satellite launches from Vandenberg Air Force Base in California, deciding to use the Titan IV instead.		Approximately 17 percent of Americans witnessed the launch live because of the presence of Payload Specialist Christa McAuliffe, who would have been the first teacher in space. Media coverage of the accident was extensive: one study reported that 85 percent of Americans surveyed had heard the news within an hour of the accident.[5] The Challenger disaster has been used as a case study in many discussions of engineering safety and workplace ethics.						Each of the Space Shuttle's two Solid Rocket Boosters (SRBs) was constructed of seven sections, six of which were permanently joined in pairs at the factory. For each flight, the four resulting segments were then assembled in the Vehicle Assembly Building at Kennedy Space Center (KSC), with three field joints. The factory joints were sealed with asbestos-silica insulation applied over the joint, while each field joint was sealed with two rubber O-rings. (After the destruction of Challenger, the number of O-rings per field joint was increased to three.)[6] The seals of all of the SRB joints were required to contain the hot, high-pressure gases produced by the burning solid propellant inside, thus forcing them out of the nozzle at the aft end of each rocket.		During the Space Shuttle design process, a McDonnell Douglas report in September 1971 discussed the safety record of solid rockets. While a safe abort was possible after most types of failures, one was especially dangerous: a burnthrough by hot gases of the rocket's casing. The report stated that "if burnthrough occurs adjacent to [liquid hydrogen/oxygen] tank or orbiter, timely sensing may not be feasible and abort not possible", accurately foreshadowing the Challenger accident.[7] Morton Thiokol was the contractor responsible for the construction and maintenance of the shuttle's SRBs. As originally designed by Thiokol, the O-ring joints in the SRBs were supposed to close more tightly due to forces generated at ignition, but a 1977 test showed that when pressurized water was used to simulate the effects of booster combustion, the metal parts bent away from each other, opening a gap through which gases could leak. This phenomenon, known as "joint rotation," caused a momentary drop in air pressure. This made it possible for combustion gases to erode the O-rings. In the event of widespread erosion, a flame path could develop, causing the joint to burst—which would have destroyed the booster and the shuttle.[8]		Engineers at the Marshall Space Flight Center wrote to the manager of the Solid Rocket Booster project, George Hardy, on several occasions suggesting that Thiokol's field joint design was unacceptable. For example, one engineer suggested that joint rotation would render the secondary O-ring useless, but Hardy did not forward these memos to Thiokol, and the field joints were accepted for flight in 1980.[9]		Evidence of serious O-ring erosion was present as early as the second space shuttle mission, STS-2, which was flown by Columbia. Contrary to NASA regulations, the Marshall Center did not report this problem to senior management at NASA, but opted to keep the problem within their reporting channels with Thiokol. Even after the O-rings were redesignated as "Criticality 1"—meaning that their failure would result in the destruction of the Orbiter—no one at Marshall suggested that the shuttles be grounded until the flaw could be fixed.[9]		After the 1984 launch of STS-41-D, flown by Discovery, the first occurrence of hot gas "blow-by" was discovered beyond the primary O-ring. In the post-flight analysis, Thiokol engineers found that the amount of blow-by was relatively small and had not impinged upon the secondary O-ring, and concluded that for future flights, the damage was an acceptable risk. However, after the Challenger disaster, Thiokol engineer Brian Russell identified this event as the first "big red flag" regarding O-ring safety.[10]		By 1985, with seven of nine shuttle launches that year using boosters displaying O-ring erosion and/or hot gas blow-by,[11] Marshall and Thiokol realized that they had a potentially catastrophic problem on their hands. Perhaps most concerning was the launch of STS-51-B in April 1985, flown by Challenger, in which the worst O-ring damage to date was discovered in post-flight analysis. The primary O-ring of the left nozzle had been eroded so extensively that it had failed to seal, and for the first time hot gases had eroded the secondary O-ring.[12] They began the process of redesigning the joint with three inches (76 mm) of additional steel around the tang. This tang would grip the inner face of the joint and prevent it from rotating. They did not call for a halt to shuttle flights until the joints could be redesigned, but rather treated the problem as an acceptable flight risk. For example, Lawrence Mulloy, Marshall's manager for the SRB project since 1982, issued and waived launch constraints for six consecutive flights. Thiokol even went as far as to persuade NASA to declare the O-ring problem "closed".[9] Donald Kutyna, a member of the Rogers Commission, later likened this situation to an airline permitting one of its planes to continue to fly despite evidence that one of its wings was about to fall off.		Challenger was originally set to launch from KSC in Florida at 14:42 Eastern Standard Time (EST) on January 22. Delays in the previous mission, STS-61-C, caused the launch date to be moved to January 23 and then to January 24. The launch was then rescheduled to January 25 due to bad weather at the Transoceanic Abort Landing (TAL) site in Dakar, Senegal. NASA decided to use Casablanca as the TAL site, but because it was not equipped for night landings, the launch had to be moved to the morning (Florida time). Predictions of unacceptable weather at KSC on January 26, caused the launch to be rescheduled for 09:37 EST on January 27.[13]		The launch was delayed the next day, due to problems with the exterior access hatch. First, one of the micro-switch indicators, used to verify that the hatch was safely locked, malfunctioned.[14] Then, a stripped bolt prevented the closeout crew from removing a closing fixture from the orbiter's hatch.[15] By the time repair personnel had sawed the fixture off, crosswinds at the Shuttle Landing Facility exceeded the limits for a Return to Launch Site (RTLS) abort.[16] While the crew waited for winds to die down, the launch window expired, forcing yet another scrub.		Forecasts for January 28 predicted an unusually cold morning, with temperatures close to −1 °C (30 °F), the minimum temperature permitted for launch. The Shuttle was never certified to operate in temperatures that low. The O-rings, as well as many other critical components, had no test data to support any expectation of a successful launch in such conditions.[17][18]		By mid-1985 Thiokol engineers worried that others did not share their concerns about low temperatures' effects on the boosters. Bob Ebeling in October 1985 wrote a memo—titled "Help!" so others would read it—of concerns regarding low temperatures and O-rings. After the weather forecast, NASA personnel remembered Thiokol's warnings and contacted the company. When a Thiokol manager asked Ebeling about the possibility of a launch at 18 degrees, he answered "[W]e're only qualified to 40 degrees ...'what business does anyone even have thinking about 18 degrees, we're in no-man's land.'" After his team agreed that a launch risked disaster, Thiokol immediately called NASA recommending a postponement until temperatures rose in the afternoon. NASA manager Jud Lovingood responded that Thiokol could not make the recommendation without providing a safe temperature. The company prepared for a teleconference two hours later during which it would have to justify a no-launch recommendation.[17][18]		At the teleconference on the evening of January 27, Thiokol engineers and managers discussed the weather conditions with NASA managers from Kennedy Space Center and Marshall Space Flight Center. Several engineers (most notably Ebeling and Roger Boisjoly) reiterated their concerns about the effect of low temperatures on the resilience of the rubber O-rings that sealed the joints of the SRBs, and recommended a launch postponement.[18] They argued that they did not have enough data to determine whether the joints would properly seal if the O-rings were colder than 12 °C (54 °F). This was an important consideration, since the SRB O-rings had been designated as a "Criticality 1" component, meaning that there was no backup if both the primary and secondary O-rings failed, and their failure could destroy the Orbiter and kill its crew.		Thiokol management initially supported its engineers' recommendation to postpone the launch, but NASA staff opposed a delay. During the conference call, Hardy told Thiokol, "I am appalled. I am appalled by your recommendation." Mulloy said, "My God, Thiokol, when do you want me to launch — next April?"[18] NASA believed that Thiokol's hastily prepared presentation's quality was too poor to support such a statement on flight safety.[17] One argument by NASA personnel contesting Thiokol's concerns was that if the primary O-ring failed, the secondary O-ring would still seal. This was unproven, and was in any case an argument that did not apply to a "Criticality 1" component. As astronaut Sally Ride stated when questioning NASA managers before the Rogers Commission, it is forbidden to rely on a backup for a "Criticality 1" component.		NASA claimed that it did not know of Thiokol's earlier concerns about the effects of the cold on the O-rings, and did not understand that Rockwell International, the shuttle's prime contractor, viewed the large amount of ice present on the pad as a constraint to launch. For reasons that are unclear, Thiokol management reversed itself and recommended that the launch proceed as scheduled;[18][19] NASA did not ask why.[17] Ebeling told his wife that night that Challenger would blow up.[20]		Ken Iliff, a former NASA Chief Scientist who had worked on the Space Shuttle Program since its first mission (and the X-15 program before that) stated in 2004, "Violating a couple of mission rules was the primary cause of the Challenger accident."[21]		The Thiokol engineers had also argued that the low overnight temperatures (−8 °C (18 °F) the evening prior to launch) would almost certainly result in SRB temperatures below their redline of 4 °C (39 °F). Ice had accumulated all over the launch pad, raising concerns that ice could damage the shuttle upon lift-off. The Kennedy Ice Team inadvertently pointed an infrared camera at the aft field joint of the right SRB and found the temperature to be only −13 °C (9 °F). This was believed to be the result of supercooled air blowing on the joint from the liquid oxygen tank vent. It was much lower than the air temperature and far below the design specifications for the O-rings. The low reading was later determined to be erroneous, the error caused by not following the temperature probe manufacturer's instructions. Tests and adjusted calculations later confirmed that the temperature of the joint was not substantially different from the ambient temperature.		The temperature on the day of the launch was far lower than had been the case with previous launches: below freezing at −2.2 to −1.7 °C (28.0 to 28.9 °F); previously, the coldest launch had been at 12 °C (54 °F). Although the Ice Team had worked through the night removing ice, engineers at Rockwell still expressed concern. Rockwell engineers watching the pad from their headquarters in Downey, California, were horrified when they saw the amount of ice. They feared that during launch, ice might be shaken loose and strike the shuttle's thermal protection tiles, possibly due to the aspiration induced by the jet of exhaust gas from the SRBs. Rocco Petrone, the head of Rockwell's space transportation division, and his colleagues viewed this situation as a launch constraint, and told Rockwell's managers at the Cape that Rockwell could not support a launch. Rockwell's managers at the Cape voiced their concerns in a manner that led Houston-based mission manager Arnold Aldrich to go ahead with the launch. Aldrich decided to postpone the shuttle launch by an hour to give the Ice Team time to perform another inspection. After that last inspection, during which the ice appeared to be melting, Challenger was cleared to launch at 11:38 am EST.[19]		The following account of the accident is derived from real time telemetry data and photographic analysis, as well as from transcripts of air-to-ground and mission control voice communications.[22] All times are given in seconds after launch and correspond to the telemetry time-codes from the closest instrumented event to each described event.[23]		The Space Shuttle main engines (SSMEs) were ignited at T-6.6 seconds. The SSMEs were liquid-fueled and could be safely shut down (and the launch aborted if necessary) until the Solid Rocket Boosters ignited at T=0 (which was at 11:38:00.010 EST) and the hold-down bolts were released with explosives, freeing the vehicle from the pad. At lift off, the three SSMEs were at 100% of their original rated performance, and began throttling up to 104% under computer control. With the first vertical motion of the vehicle, the gaseous hydrogen vent arm retracted from the External Tank (ET) but failed to latch back. Review of film shot by pad cameras showed that the arm did not re-contact the vehicle, and thus it was ruled out as a contributing factor in the accident.[23] The post-launch inspection of the pad also revealed that kick springs on four of the hold-down bolts were missing, but they were similarly ruled out as a possible cause.[24]		Later review of launch film showed that at T+0.678, strong puffs of dark gray smoke were emitted from the right-hand SRB near the aft strut that attaches the booster to the ET. The last smoke puff occurred at about T+2.733. The last view of smoke around the strut was at T+3.375. It was later determined that these smoke puffs were caused by the opening and closing of the aft field joint of the right-hand SRB. The booster's casing had ballooned under the stress of ignition. As a result of this ballooning, the metal parts of the casing bent away from each other, opening a gap through which hot gases—above 2,760 °C (5,000 °F)—leaked. This had occurred in previous launches, but each time the primary O-ring had shifted out of its groove and formed a seal. Although the SRB was not designed to function this way, it appeared to work well enough, and Morton-Thiokol changed the design specs to accommodate this process, known as extrusion.		While extrusion was taking place, hot gases leaked past (a process called "blow-by"), damaging the O-rings until a seal was made. Investigations by Morton-Thiokol engineers determined that the amount of damage to the O-rings was directly related to the time it took for extrusion to occur, and that cold weather, by causing the O-rings to harden, lengthened the time of extrusion. (The redesigned SRB field joint used subsequent to the Challenger accident used an additional interlocking mortise and tang with a third O-ring, mitigating blow-by.)		On the morning of the disaster, the primary O-ring had become so hard due to the cold that it could not seal in time. The secondary O-ring was not in its seated position due to the metal bending. There was now no barrier to the gases, and both O-rings were vaporized across 70 degrees of arc. Aluminum oxides from the burned solid propellant sealed the damaged joint, temporarily replacing the O-ring seal before flame passed through the joint.		As the vehicle cleared the tower, the SSMEs were operating at 104% of their rated maximum thrust, and control switched from the Launch Control Center (LCC) at Kennedy to the Mission Control Center (MCC) at Johnson Space Center in Houston, Texas. To prevent aerodynamic forces from structurally overloading the orbiter, at T+28 the SSMEs began throttling down to limit the velocity of the shuttle in the dense lower atmosphere, per normal operating procedure. At T+35.379, the SSMEs throttled back further to the planned 65%. Five seconds later, at about 5,800 metres (19,000 ft), Challenger passed through Mach 1. At T+51.860, the SSMEs began throttling back up to 104% as the vehicle passed beyond Max Q, the period of maximum aerodynamic pressure on the vehicle.		Beginning at about T+37 and for 27 seconds, the shuttle experienced a series of wind shear events that were stronger than on any previous flight.[25]		At T+58.788, a tracking film camera captured the beginnings of a plume near the aft attach strut on the right SRB. Unknown to those on Challenger or in Houston, hot gas had begun to leak through a growing hole in one of the right-hand SRBs joints. The force of the wind shear shattered the temporary oxide seal that had taken the place of the damaged O-rings, removing the last barrier to flame passing through the joint. Had it not been for the wind shear, the fortuitous oxide seal might have held through booster burnout.		Within a second, the plume became well defined and intense. Internal pressure in the right SRB began to drop because of the rapidly enlarging hole in the failed joint, and at T+60.238 there was visual evidence of flame burning through the joint and impinging on the external tank.[22]		At T+64.660, the plume suddenly changed shape, indicating that a leak had begun in the liquid hydrogen tank, located in the aft portion of the external tank. The nozzles of the main engines pivoted under computer control to compensate for the unbalanced thrust produced by the booster burn-through. The pressure in the shuttle's external liquid hydrogen tank began to drop at T+66.764, indicating the effect of the leak.[22]		At this stage the situation still seemed normal both to the crew and to flight controllers. At T+68, the CAPCOM Richard O. Covey informed the crew that they were "go at throttle up", and Commander Dick Scobee confirmed, "Roger, go at throttle up"; this was the last communication from Challenger on the air-to-ground loop.[22]		At T+72.284, the right SRB pulled away from the aft strut attaching it to the external tank. Later analysis of telemetry data showed a sudden lateral acceleration to the right at T+72.525, which may have been felt by the crew. The last statement captured by the crew cabin recorder came just half a second after this acceleration, when Pilot Michael J. Smith said "Uh-oh."[26] Smith may also have been responding to onboard indications of main engine performance, or to falling pressures in the external fuel tank.		At T+73.124, the aft dome of the liquid hydrogen tank failed, producing a propulsive force that rammed the hydrogen tank into the liquid oxygen tank in the forward part of the ET. At the same time, the right SRB rotated about the forward attach strut, and struck the intertank structure. The external tank at this point suffered a complete structural failure, the LH2 and LOX tanks rupturing, mixing, and igniting, creating a fireball that enveloped the whole stack.[27]		The breakup of the vehicle began at T+73.162 seconds and at an altitude of 48,000 feet (15 km).[28] With the external tank disintegrating (and with the semi-detached right SRB contributing its thrust on an anomalous vector), Challenger veered from its correct attitude with respect to the local airflow, resulting in a load factor of up to 20 (or 20 g), well over its design limit of 5 g and was quickly ripped apart by abnormal aerodynamic forces (contrary to popular belief, the orbiter did not explode as the force of the external tank breakup was well within its structural limits). The two SRBs, which could withstand greater aerodynamic loads, separated from the ET and continued in uncontrolled powered flight. The SRB casings were made of half-inch (12.7 mm) thick steel and were much stronger than the orbiter and ET; thus, both SRBs survived the breakup of the space shuttle stack, even though the right SRB was still suffering the effects of the joint burn-through that had set the destruction of Challenger in motion.[24]		The more robustly constructed crew cabin also survived the breakup of the launch vehicle, as it was designed to survive 20 psi while the estimated pressure it had been subjected to during orbiter breakup was only about 4-5 psi.; while the SRBs were subsequently destroyed remotely by the Range Safety Officer, the detached cabin continued along a ballistic trajectory and was observed exiting the cloud of gases at T+75.237.[24] Twenty-five seconds after the breakup of the vehicle, the altitude of the crew compartment peaked at a height of 65,000 feet (20 km).[28]The cabin was stabilized during descent by the large mass of electrical wires trailing behind it.		The Thiokol engineers who had opposed the decision to launch were watching the events on television. They had believed that any O-ring failure would have occurred at liftoff, and thus were happy to see the shuttle successfully leave the launch pad. At about one minute after liftoff, a friend of Boisjoly said to him "Oh God. We made it. We made it!" Boisjoly recalled that when the shuttle was destroyed a few seconds later, "we all knew exactly what happened."[18]		In Mission Control, there was a burst of static on the air-to-ground loop as Challenger disintegrated. Television screens showed a cloud of smoke and water vapor (the product of hydrogen+oxygen combustion) where Challenger had been, with pieces of debris falling toward the ocean. At about T+89, flight director Jay Greene prompted his Flight Dynamics Officer (FIDO) for information. FIDO responded that "the [radar] filter has discreting sources", a further indication that Challenger had broken into multiple pieces. Moments later, the ground controller reported "negative contact (and) loss of downlink" of radio and telemetry data from Challenger. Greene ordered his team to "watch your data carefully" and look for any sign that the Orbiter had escaped.		At T+110.250, the Range Safety Officer (RSO) at the Cape Canaveral Air Force Station sent radio signals that activated the range safety system's "destruct" packages on board both solid rocket boosters. This was a normal contingency procedure, undertaken because the RSO judged the free-flying SRBs a possible threat to land or sea. The same destruct signal would have destroyed the External Tank had it not already disintegrated.[29] The SRBs were close to the end of their scheduled burn (T+110 seconds after launch) and had nearly exhausted their propellants when the destruct command was sent, so very little, if any explosive force was generated by this event.		Public affairs officer Steve Nesbitt reported: "Flight controllers here are looking very carefully at the situation. Obviously a major malfunction. We have no downlink."[22]		On the Mission Control loop, Greene ordered that contingency procedures be put into effect; these procedures included locking the doors of the control center, shutting down telephone communications with the outside world, and following checklists that ensured that the relevant data were correctly recorded and preserved.[30]		Nesbitt relayed this information to the public: "We have a report from the Flight Dynamics Officer that the vehicle has exploded. The flight director confirms that. We are looking at checking with the recovery forces to see what can be done at this point."[22]		The crew cabin, made of reinforced aluminum, was a particularly robust section of the orbiter.[31] During vehicle breakup, it detached in one piece and slowly tumbled into a ballistic arc. NASA estimated the load factor at separation to be between 12 and 20 g; within two seconds it had already dropped to below 4 g and within 10 seconds the cabin was in free fall. The forces involved at this stage were probably insufficient to cause major injury.		At least some of the crew were probably alive and at least briefly conscious after the breakup, as three of the four recovered Personal Egress Air Packs (PEAPs) on the flight deck were found to have been activated.[32] Investigators found their remaining unused air supply consistent with the expected consumption during the 2 minute 45 second post-breakup trajectory.		While analyzing the wreckage, investigators discovered that several electrical system switches on Pilot Mike Smith's right-hand panel had been moved from their usual launch positions. Fellow astronaut Richard Mullane wrote, "These switches were protected with lever locks that required them to be pulled outward against a spring force before they could be moved to a new position." Later tests established that neither force of the explosion nor the impact with the ocean could have moved them, indicating that Smith made the switch changes, presumably in a futile attempt to restore electrical power to the cockpit after the crew cabin detached from the rest of the orbiter.[33]		Whether the crew members remained conscious long after the breakup is unknown, and largely depends on whether the detached crew cabin maintained pressure integrity. If it did not, the time of useful consciousness at that altitude is just a few seconds; the PEAPs supplied only unpressurized air, and hence would not have helped the crew to retain consciousness. If, on the other hand, the cabin was not depressurized or only slowly depressurizing, they may have been conscious for the entire fall until impact. Recovery of the cabin found that the middeck floor had not suffered buckling or tearing, as would result from a rapid decompression, thus providing some evidence that the depressurization may have not happened all at once.		NASA routinely trained shuttle crews for splashdown events, but the cabin hit the ocean surface at roughly 207 mph (333 km/h), with an estimated deceleration at impact of well over 200 g, far beyond the structural limits of the crew compartment or crew survivability levels, and far greater than almost any automobile, aircraft, or train accident. The crew would have been torn from their seats and killed instantly by the extreme impact force.[28]		On July 28, 1986, NASA's Associate Administrator for Space Flight, former astronaut Richard H. Truly, released a report on the deaths of the crew from the director of Space and Life Sciences at the Johnson Space Center, Joseph P. Kerwin. A medical doctor and former astronaut, Kerwin was a veteran of the 1973 Skylab 2 mission. According to the Kerwin Report:		The findings are inconclusive. The impact of the crew compartment with the ocean surface was so violent that evidence of damage occurring in the seconds which followed the disintegration was masked. Our final conclusions are:		Some experts believe most if not all of the crew were alive and possibly conscious during the entire descent until impact with the ocean. Astronaut and NASA lead accident investigator Robert Overmyer said, "I not only flew with Dick Scobee, we owned a plane together, and I know Scob did everything he could to save his crew. Scob fought for any and every edge to survive. He flew that ship without wings all the way down... they were alive."[31]		During powered flight of the space shuttle, crew escape was not possible. Launch escape systems were considered several times during shuttle development, but NASA's conclusion was that the shuttle's expected high reliability would preclude the need for one. Modified SR-71 Blackbird ejection seats and full pressure suits were used for the two-man crews on the first four shuttle orbital missions, which were considered test flights, but they were removed for the "operational" missions that followed. (The Columbia Accident Investigation Board later declared, after the 2003 Columbia re-entry disaster, that the space shuttle system should never have been declared operational because it is experimental by nature due to the limited number of flights as compared to certified commercial aircraft.) The multi-deck design of the crew cabin precluded use of such ejection seats for larger crews. Providing some sort of launch escape system had been considered, but deemed impractical due to "limited utility, technical complexity and excessive cost in dollars, weight or schedule delays."[29]		After the loss of Challenger, the question was re-opened, and NASA considered several different options, including ejector seats, tractor rockets and emergency egress through the bottom of the orbiter. NASA once again concluded that all of the launch escape systems considered would be impractical due to the sweeping vehicle modifications that would have been necessary and the resultant limitations on crew size. A system was designed to give the crew the option to leave the shuttle during gliding flight, but this system would not have been usable in the Challenger situation.[34]		On the night of the disaster, President Ronald Reagan had been scheduled to give his annual State of the Union address. He initially announced that the address would go on as scheduled, but then postponed the State of the Union address for a week and instead gave a national address on the Challenger disaster from the Oval Office of the White House. It was written by Peggy Noonan, and was listed as one of the most significant speeches of the 20th century in a survey of 137 communication scholars.[35][36] It finished with the following statement, which quoted from the poem "High Flight" by John Gillespie Magee, Jr.:		We will never forget them, nor the last time we saw them, this morning, as they prepared for their journey and waved goodbye and 'slipped the surly bonds of Earth' to 'touch the face of God.'[37]		Three days later, Reagan and his wife Nancy traveled to the Johnson Space Center to speak at a memorial service honoring the crew members, where he stated:		Sometimes, when we reach for the stars, we fall short. But we must pick ourselves up again and press on despite the pain.[38]		It was attended by 6,000 NASA employees and 4,000 guests,[39][40] as well as by the families of the crew.[41] During the ceremony, an Air Force band led the singing of "God Bless America" as NASA T-38 Talon jets flew directly over the scene, in the traditional missing-man formation.[39][40] All activities were broadcast live by the national television networks.[39]		President Reagan would further mention the Challenger crew members at the beginning of his State of the Union address on February 4.		In the first minutes after the accident, recovery efforts were begun by NASA's Launch Recovery Director, who ordered the ships normally used by NASA for recovery of the solid rocket boosters to be sent to the location of the water impact. Search and rescue aircraft were also dispatched. At this stage debris was still falling, and the Range Safety Officer (RSO) held both aircraft and ships out of the impact area until it was considered safe for them to enter. It was about an hour until the RSO allowed the recovery forces to begin their work.[42]		The search and rescue operations that took place in the first week after the Challenger accident were managed by the Department of Defense on behalf of NASA, with assistance from the United States Coast Guard, and mostly involved surface searches. According to the Coast Guard, "the operation was the largest surface search in which they had participated."[42] This phase of operations lasted until February 7. In order to discourage scavengers, NASA did not disclose the exact location of the debris field, instead referring to it by the cryptic code name "Target 67". This was impossible to keep secret for any length of time and Radio Shacks in the Cape Canaveral area were soon completely sold out of radios that could tune into the frequency used by Coast Guard vessels. Thereafter, recovery efforts were managed by a Search, Recovery, and Reconstruction team; its aim was to salvage debris that would help in determining the cause of the accident. Sonar, divers, remotely operated submersibles and manned submersibles were all used during the search, which covered an area of 486 square nautical miles (1,670 km2), and took place at water depths between 70 feet (21 m) and 1,200 feet (370 m).[43] On March 7, divers from the USS Preserver identified what might be the crew compartment on the ocean floor.[44][45] The finding, along with discovery of the remains of all seven crew members, was confirmed the next day and on March 9, NASA announced the finding to the press.[46] The crew cabin was severely crushed and fragmented from the extreme impact forces; one member of the search team described it as "largely a pile of rubble with wires protruding from it". The largest intact section was the rear wall containing the two payload bay windows and the airlock. All windows in the cabin had been destroyed, with only small bits of glass still attached to the frames. Impact forces appeared to be greatest on the left side, indicating that it had struck the water in a nose-down, left-end first position.		Inside the twisted debris of the crew cabin were the bodies of the astronauts, which after weeks of immersion in salt water and exposure to scavenging marine life were in a semi-liquefied state that bore little resemblance to anything living, although according to John Devlin, the skipper of the USS Preserve, they "were not as badly mangled as you'd see in some aircraft accidents". Lt. Cmdr James Simpson of the Coast Guard reported finding a helmet with ears and a scalp in it.[47] Judy Resnik was the first to be removed followed by Christa McAuliffe with more human remains retrieved over several hours. Due to the hazardous nature of the recovery operation (the cabin was filled with large pieces of protruding jagged metal), the Navy divers protested that they would not go on with the work unless the cabin was hauled onto the ship's deck. Despite the desire to be respectful to the dead, the bodies were said to have been handled less gingerly than the recovered memory core units and tape recorders, whose contents were vital in the accident investigation.		During the recovery of the remains of the crew, Gregory Jarvis's body floated out of the shattered crew compartment and was lost to the diving team. A day later, his body was seen floating on the ocean's surface. It sank as a team prepared to pull him from the water. Determined to not end the recovery operations without retrieving Jarvis, astronaut Robert Crippen rented a fishing boat at his own expense and went searching for the body. On April 15, near the end of the salvage operations, the Navy divers found Jarvis. His body had settled 101.2 feet below the water on the sea floor, some 0.7 nautical miles from the final resting place of the crew compartment. He was recovered and brought to the surface before being processed with the other crew members and then prepared for release to his family.		Navy pathologists performed autopsies on the crew members but due to the poor condition of the bodies, no exact cause of death could be determined for any of them.		The crew transfer took place on April 29, 1986, three months and one day after the accident. Seven hearses carried the crew's remains from the Life Sciences Facility on Cape Canaveral, to a waiting MAC C-141 aircraft. Their caskets were each draped with an American flag and carried past an honor guard and followed by an astronaut escort. The astronaut escorts for the Challenger crew were: Dan Brandenstein, Jim Buckley, Norm Thagard, Charles Bolden, Tammy Jernigan, Dick Richards, and Loren Shriver. Once the crew's remains were aboard the jet, they were flown to Dover Air Force Base in Delaware to be processed and then released to their relatives.		It had been suggested early in the investigation that the accident was caused by inadvertent detonation of the Range Safety destruct charges on the external tank, but the charges were recovered mostly intact and a quick overview of telemetry data immediately ruled out that theory.		The three shuttle main engines were found largely intact and still attached to the thrust assembly despite extensive damage from impact with the ocean, marine life, and immersion in salt water. They had considerable heat damage due to a LOX-rich shutdown caused by the drop in hydrogen fuel pressure as the external tank began to fail. The memory units from Engines 1 and 2 were recovered, cleaned, and their contents analyzed, which confirmed normal engine operation until LH2 starvation began starting at T+72 seconds. Loss of fuel pressure and rising combustion chamber temperatures caused the computers to shut off the engines. Since there was no evidence of abnormal SSME behavior until 72 seconds, the engines were ruled out as a contributing factor in the accident.		Other recovered orbiter components showed no indication of pre-breakup malfunction. Recovered parts of the TDRSS satellite also did not disclose any abnormalities other than damage caused by vehicle breakup, impact, and immersion in salt water. The solid rocket motor boost stage for the payload had not ignited either and was quickly ruled out as a cause of the accident.		The solid rocket booster debris had no signs of explosion (other than the Range Safety charges splitting the casings open), or propellant debonding/cracking. There was no question about the RSO manually destroying the SRBs following vehicle breakup, so the idea of the destruct charges accidentally detonating was ruled out. Premature separation of the SRBs from the stack or inadvertent activation of the recovery system was also considered, but telemetry data quickly disproved that idea. Nor was there any evidence of in-flight structural failure since visual and telemetry evidence showed that the SRBs remained structurally intact up to and beyond vehicle breakup. The aft field joint on the right SRB did show extensive burn damage.		Telemetry proved that the right SRB, after the failure of the lower struts, had come loose and struck the external tank. The exact point where the struts broke could not be determined from film of the launch, nor were the struts or the adjacent section of the external tank recovered during salvage operations. Based on the location of the rupture in the right SRB, the P12 strut most likely failed first. The SRB's nose cone also exhibited some impact damage from this behavior (for comparison, the left SRB nose cone had no damage at all) and the intertank region had signs of impact damage as well. In addition, the orbiter's right wing had impact and burn damage from the right SRB colliding with it following vehicle breakup.		Most of the initially-considered failure modes were soon ruled out and by May 1, enough of the right solid rocket booster had been recovered to determine the original cause of the accident, and the major salvage operations were concluded. While some shallow-water recovery efforts continued, this was unconnected with the accident investigation; it aimed to recover debris for use in NASA's studies of the properties of materials used in spacecraft and launch vehicles.[42] The recovery operation was able to pull 15 short tons (14 t) of debris from the ocean; 55% of Challenger, 5% of the crew cabin and 65% of the satellite cargo is still missing.[48] Some of the missing debris continued to wash up on Florida shores for some years, such as on December 17, 1996, nearly 11 years after the incident, when two large pieces of the shuttle were found at Cocoa Beach.[49] Under 18 U.S.C. § 641 it is against the law to be in possession of Challenger debris, and any newly discovered pieces must be turned over to NASA.[50]		On board Challenger was an American flag, dubbed the Challenger flag, that was sponsored by Boy Scout Troop 514 of Monument, Colorado. It was recovered intact, still sealed in its plastic container.[51]		All recovered non-organic debris from Challenger was ultimately buried in a former missile silo at Cape Canaveral Air Force Station Launch Complex 31.		The remains of the crew that were identifiable were returned to their families on April 29, 1986. Three of the crew members, Judith Resnik, Dick Scobee, and Capt. Michael J. Smith, were buried by their families at Arlington National Cemetery at individual grave sites. Mission Specialist Lt Col Ellison Onizuka was buried at the National Memorial Cemetery of the Pacific in Honolulu, Hawaii. Christa McAuliffe's remains are buried at Calvary Cemetery in her hometown of Concord, New Hampshire.[52] Unidentified crew remains were buried communally at the Space Shuttle Challenger Memorial in Arlington on May 20, 1986.[53]		Several National Reconnaissance Office (NRO) satellites that only the shuttle could launch were grounded because of the accident, a dilemma NRO had feared since the 1970s when the shuttle was designated as the United States' primary launch system for all government and commercial payloads.[54][55] NASA had difficulties with its own Titan rocket and Delta rocket programs, due to other unexpected rocket failures occurring before and after the Challenger disaster. On August 28, 1985, a Titan 34D[56] carrying a KH-11 Kennan satellite exploded after liftoff over Vandenberg Air Force Base, when the first stage propellant feed system failed. It was the first failure of a Titan missile since 1978. On April 18, 1986, another Titan 34D-9[56][57] carrying a classified payload,[57] said to be a Big Bird spy satellite, exploded at about 830 feet above the pad after liftoff over Vandenberg AFB, when a burnthrough occurred on one of the rocket boosters. On May 3, 1986, a Delta 3914[56] carrying the GOES-G weather satellite[58] exploded 71 seconds after liftoff over Cape Canaveral Air Force Station due to an electrical malfunction on the Delta's first stage, which prompted the range safety officer on the ground to decide to destroy the rocket, just as a few of the rocket's boosters were jettisoned. As a result of these three failures, NASA decided to cancel all Titan and Delta launches from Cape Canaveral and Vandenberg for four months until the problem in the rockets' designs were solved.		Due to the shuttle fleet being grounded, excess ammonium perchlorate that was manufactured as rocket fuel was kept on site at the Pacific Engineering and Production Company of Nevada (PEPCON) plant in Henderson, Nevada. This excess ammonium perchlorate later caught fire and the resulting explosion destroyed the PEPCON facility and the neighboring Kidd & Co marshmallow factory.[59]		In the aftermath of the accident, NASA was criticized for its lack of openness with the press. The New York Times noted on the day after the accident that "neither Jay Greene, flight director for the ascent, nor any other person in the control room, was made available to the press by the space agency."[60] In the absence of reliable sources, the press turned to speculation; both The New York Times and United Press International ran stories suggesting that a fault with the space shuttle external tank had caused the accident, despite the fact that NASA's internal investigation had quickly focused in on the solid rocket boosters.[61][62] "The space agency," wrote space reporter William Harwood, "stuck to its policy of strict secrecy about the details of the investigation, an uncharacteristic stance for an agency that long prided itself on openness."[61]		The Presidential Commission on the Space Shuttle Challenger Accident, also known as the Rogers Commission after its chairman, was formed to investigate the disaster. The commission members were Chairman William P. Rogers, Vice Chairman Neil Armstrong, David Acheson, Eugene Covert, Richard Feynman, Robert Hotz, Donald Kutyna, Sally Ride, Robert Rummel, Joseph Sutter, Arthur Walker, Albert Wheelon, and Chuck Yeager. The commission worked for several months and published a report of its findings. It found that the Challenger accident was caused by a failure in the O-rings sealing a joint on the right solid rocket booster, which allowed pressurized hot gases and eventually flame to "blow by" the O-ring and make contact with the adjacent external tank, causing structural failure. The failure of the O-rings was attributed to a faulty design, whose performance could be too easily compromised by factors including the low temperature on the day of launch.[63]		More broadly, the report also considered the contributing causes of the accident. Most salient was the failure of both NASA and Morton Thiokol to respond adequately to the danger posed by the deficient joint design. Rather than redesigning the joint, they came to define the problem as an acceptable flight risk. The report found that managers at Marshall had known about the flawed design since 1977, but never discussed the problem outside their reporting channels with Thiokol—a flagrant violation of NASA regulations. Even when it became more apparent how serious the flaw was, no one at Marshall considered grounding the shuttles until a fix could be implemented. On the contrary, Marshall managers went as far as to issue and waive six launch constraints related to the O-rings.[9] The report also strongly criticized the decision-making process that led to the launch of Challenger, saying that it was seriously flawed:[19]		failures in communication ... resulted in a decision to launch 51-L based on incomplete and sometimes misleading information, a conflict between engineering data and management judgments, and a NASA management structure that permitted internal flight safety problems to bypass key Shuttle managers.		One of the commission's members was theoretical physicist Richard Feynman. Feynman, who was then seriously ill with cancer, was reluctant to undertake the job. He did so to find the root cause of the disaster, and to speak plainly to the public about his findings.[64] At the start of investigation, fellow members Dr. Sally Ride and General Kutyna gave Feynman a hint that the O-rings were not tested at temperatures below 10 °C (50 °F).[65] During a televised hearing, Feynman demonstrated how the O-rings became less resilient and subject to seal failures at ice-cold temperatures by immersing a sample of the material in a glass of ice water. While other members of the Commission met with NASA and supplier top management, Feynman sought out the engineers and technicians for the answers.[66] He was critical of flaws in NASA's "safety culture", so much so that he threatened to remove his name from the report unless it included his personal observations on the reliability of the shuttle, which appeared as Appendix F.[66] In the appendix, he argued that the estimates of reliability offered by NASA management were wildly unrealistic, differing as much as a thousandfold from the estimates of working engineers. "For a successful technology," he concluded, "reality must take precedence over public relations, for nature cannot be fooled."[67]		The U.S. House Committee on Science and Technology also conducted hearings, and on October 29, 1986, released its own report on the Challenger accident.[68] The committee reviewed the findings of the Rogers Commission as part of its investigation, and agreed with the Rogers Commission as to the technical causes of the accident. It differed from the committee in its assessment of the accident's contributing causes:		the Committee feels that the underlying problem which led to the Challenger accident was not poor communication or underlying procedures as implied by the Rogers Commission conclusion. Rather, the fundamental problem was poor technical decision-making over a period of several years by top NASA and contractor personnel, who failed to act decisively to solve the increasingly serious anomalies in the Solid Rocket Booster joints.[68]		After the Challenger accident, further shuttle flights were suspended, pending the results of the Rogers Commission investigation. Whereas NASA had held an internal inquiry into the Apollo 1 fire in 1967, its actions after Challenger were more constrained by the judgment of outside bodies. The Rogers Commission offered nine recommendations on improving safety in the space shuttle program, and NASA was directed by President Reagan to report back within thirty days as to how it planned to implement those recommendations.[69]		When the disaster occurred, the Air Force had performed extensive modifications of its Space Launch Complex 6 (SLC-6, pronounced as "Slick Six") at Vandenberg Air Force Base in California, for launch and landing operations of classified Shuttle launches of satellites in polar orbit, and was planning its first polar flight for October 15, 1986. Originally built for the Manned Orbital Laboratory project cancelled in 1969, the modifications were proving problematic and expensive,[70] costing over $4 billion. The Challenger loss motivated the Air Force to set in motion a chain of events that finally led to the May 13, 1988 decision to cancel its Vandenberg Shuttle launch plans, in favor of the Titan IV unmanned launch vehicle.		In response to the commission's recommendation, NASA initiated a total redesign of the space shuttle's solid rocket boosters, which was watched over by an independent oversight group as stipulated by the commission.[69] NASA's contract with Morton Thiokol, the contractor responsible for the solid rocket boosters, included a clause stating that in the event of a failure leading to "loss of life or mission," Thiokol would forfeit $10 million of its incentive fee and formally accept legal liability for the failure. After the Challenger accident, Thiokol agreed to "voluntarily accept" the monetary penalty in exchange for not being forced to accept liability.[71]		NASA also created a new Office of Safety, Reliability and Quality Assurance, headed as the commission had specified by a NASA associate administrator who reported directly to the NASA administrator. George Martin, formerly of Martin Marietta, was appointed to this position.[72] Former Challenger flight director Jay Greene became chief of the Safety Division of the directorate.[73]		The unrealistically optimistic launch schedule pursued by NASA had been criticized by the Rogers Commission as a possible contributing cause to the accident. After the accident, NASA attempted to aim at a more realistic shuttle flight rate: it added another orbiter, Endeavour, to the space shuttle fleet to replace Challenger, and it worked with the Department of Defense to put more satellites in orbit using expendable launch vehicles rather than the shuttle.[74] In August 1986, President Reagan also announced that the shuttle would no longer carry commercial satellite payloads.[74] After a 32-month hiatus, the next shuttle mission, STS-26, was launched on September 29, 1988.		Although changes were made by NASA after the Challenger accident, many commentators have argued that the changes in its management structure and organizational culture were neither deep nor long-lasting.		After the Space Shuttle Columbia disaster in 2003, attention once again focused on the attitude of NASA management towards safety issues. The Columbia Accident Investigation Board (CAIB) concluded that NASA had failed to learn many of the lessons of Challenger. In particular, the agency had not set up a truly independent office for safety oversight; the CAIB felt that in this area, "NASA's response to the Rogers Commission did not meet the Commission's intent".[75] The CAIB believed that "the causes of the institutional failure responsible for Challenger have not been fixed," saying that the same "flawed decision making process" that had resulted in the Challenger accident was responsible for Columbia's destruction seventeen years later.[76]		While the presence of New Hampshire's Christa McAuliffe, a member of the Teacher in Space program, on the Challenger crew had provoked some media interest, there was little live broadcast coverage of the launch. The only live national TV coverage available publicly was provided by CNN;.[77] Los Angeles station KNBC also carried the launch with anchor Kent Shocknek describing the tragedy as it happened.[78] Live radio coverage of the launch and explosion was heard on ABC Radio anchored by Vic Ratner and Bob Walker.[79] CBS Radio News carried the launch live but cut out of coverage seconds before the explosion necessitating anchor Christopher Glenn to hastily scramble back on the air to report what had happened.[80]		NBC, CBS and ABC all broke into regular programing shortly after the accident; NBC's John Palmer announced there had been "a major problem" with the launch. Both Palmer and CBS anchor Dan Rather reacted to cameras catching live video of something descending by parachute into the area where Challenger debris was falling with confusion and speculation that a crew member may have ejected from the shuttle and survived. The shuttle had no individual ejection seats or a crew escape capsule. Mission control identified the parachute as a paramedic parachuting into the area but this was also incorrect based on internal speculation at mission control. The chute was the parachute and nose cone from one of the solid rocket boosters which had been destroyed by the range safety officer after the explosion.[81] Due to McAuliffe's presence on the mission, NASA arranged for many US public schools to view the launch live on NASA TV.[82] As a result, many who were schoolchildren in the US in 1986 had the opportunity to view the launch live. After the accident, 17 percent of respondents in one study reported that they had seen the shuttle launch, while 85 percent said that they had learned of the accident within an hour. As the authors of the paper reported, "only two studies have revealed more rapid dissemination [of news]." (One of those studies was of the spread of news in Dallas after President John F. Kennedy's assassination, while the other was the spread of news among students at Kent State regarding President Franklin D. Roosevelt's death.)[83] Another study noted that "even those who were not watching television at the time of the disaster were almost certain to see the graphic pictures of the accident replayed as the television networks reported the story almost continuously for the rest of the day."[84] Children were even more likely than adults to have seen the accident live, since many children—48 percent of nine to thirteen-year-olds, according to a New York Times poll—watched the launch at school.[84]		Following the day of the accident, press interest remained high. While only 535 reporters were accredited to cover the launch, three days later there were 1,467 reporters at Kennedy Space Center and another 1,040 at the Johnson Space Center. The event made headlines in newspapers worldwide.[61]		The Challenger accident has frequently been used as a case study in the study of subjects such as engineering safety, the ethics of whistle-blowing, communications, group decision-making, and the dangers of groupthink. It is part of the required readings for engineers seeking a professional license in Canada and other countries.[85] Roger Boisjoly, the engineer who had warned about the effect of cold weather on the O-rings, left his job at Morton Thiokol and became a speaker on workplace ethics.[86] He argues that the caucus called by Morton Thiokol managers, which resulted in a recommendation to launch, "constituted the unethical decision-making forum resulting from intense customer intimidation."[87] For his honesty and integrity leading up to and directly following the shuttle disaster, Roger Boisjoly was awarded the Prize for Scientific Freedom and Responsibility from the American Association for the Advancement of Science. Many colleges and universities have also used the accident in classes on the ethics of engineering.[88][89]		Information designer Edward Tufte has claimed that the Challenger accident is an example of the problems that can occur from the lack of clarity in the presentation of information. He argues that if Morton Thiokol engineers had more clearly presented the data that they had on the relationship between low temperatures and burn-through in the solid rocket booster joints, they might have succeeded in persuading NASA managers to cancel the launch. To demonstrate this, he took all of the data he claimed the engineers had presented during the briefing, and reformatted it onto a single graph of O-ring damage versus external launch temperature, showing the effects of cold on the degree of O-ring damage. Tufte then placed the proposed launch of Challenger on the graph according to its predicted temperature at launch. According to Tufte, the launch temperature of Challenger was so far below the coldest launch, with the worst damage seen to date, that even a casual observer could have determined that the risk of disaster was severe.[90]		Tufte has also argued that poor presentation of information may have also affected NASA decisions during the last flight of the space shuttle Columbia.[91]		Boisjoly, Wade Robison, a Rochester Institute of Technology professor, and their colleagues have vigorously repudiated Tufte's conclusions about the Morton Thiokol engineers' role in the loss of Challenger. First, they argue that the engineers didn't have the information available as Tufte claimed: "But they did not know the temperatures even though they did try to obtain that information. Tufte has not gotten the facts right even though the information was available to him had he looked for it."[92][93] They further argue that Tufte "misunderstands thoroughly the argument and evidence the engineers gave."[92] They also criticized Tufte's diagram as "fatally flawed by Tufte's own criteria. The vertical axis tracks the wrong effect, and the horizontal axis cites temperatures not available to the engineers and, in addition, mixes O-ring temperatures and ambient air temperature as though the two were the same."[92]		The Challenger disaster also provided a chance to see how traumatic events affected children's psyches. At least one psychological study has found that memories of the Challenger explosion were similar to memories of experiencing single, unrepeated traumas. The majority of children's memories of Challenger were often clear and consistent, and even things like personal placement such as who they were with or what they were doing when they heard the news were remembered well. In one U.S. study, children's memories were recorded and tested again. Children on the East Coast recalled the event more easily than children on the West Coast, due to the time difference. Children on the East Coast either saw the explosion on TV while in school, or heard people talking about it. On the other side of the country, most children were either on their way to school, or just beginning their morning classes. Researchers found that those children who saw the explosion on TV had a more emotional connection to the event, and thus had an easier time remembering it. After one year the children's memories were tested, and those on the East Coast recalled the event better than their West Coast counterparts. Regardless of where they were when it happened, the Challenger explosion was still an important event that many children easily remembered.[94]		After the accident, NASA's Space Shuttle fleet was grounded for almost three years while the investigation, hearings, engineering redesign of the SRBs, and other behind-the-scenes technical and management reviews, changes, and preparations were taking place. At 11:37 on September 29, 1988, Space Shuttle Discovery lifted off with a crew of five[95] from Kennedy Space Center pad 39-B. It carried a Tracking and Data Relay Satellite, TDRS-C (named TDRS-3 after deployment), which replaced TDRS-B, the satellite that was launched and lost on Challenger. The "Return to Flight" launch of Discovery also represented a test of the redesigned boosters, a shift to a more conservative stance on safety (e.g., it was the first time the crew had launched in pressure suits since STS-4, the last of the four initial Shuttle test flights), and a chance to restore national pride in the American space program, especially manned space flight. The mission, STS-26, was a success (with only two minor system failures, one of a cabin cooling system and one of a Ku-band antenna), and a regular schedule of STS flights followed, continuing without extended interruption until the 2003 Columbia disaster.		Barbara Morgan, the backup for McAuliffe who trained with her in the Teacher in Space program and was at KSC watching her launch on January 28, 1986, flew on STS-118 as a Mission Specialist in August 2007.		The families of the Challenger crew organized the Challenger Center for Space Science Education as a permanent memorial to the crew. Fifty-two learning centers have been established by this non-profit organization.[citation needed]		The final episode of the second season of Punky Brewster was notable for centering on the very recent, real-life Space Shuttle Challenger disaster. Punky and her classmates watched the live coverage of the shuttle launch in Mike Fulton's class. After the accident occurred, Punky is traumatized, and finds her dreams to become an astronaut are crushed. She writes a letter to NASA, and is visited by special guest star Buzz Aldrin.[96] Although the episode received high ratings, NBC would, in the following weeks, decide to cancel the show.[97]		On the evening of April 5, 1986, the Rendez-vous Houston concert commemorated and celebrated the crew of the Challenger. It featured a live performance by musician Jean Michel Jarre, a friend of crew member Ron McNair. McNair was supposed to play the saxophone from space during the track "Last Rendez-Vous". It was to have become the first musical piece professionally recorded in space.[citation needed] His substitute for the concert was Houston native Kirk Whalum.[citation needed]		In June 1986, singer-songwriter John Denver, a pilot with a deep interest in going to space himself, released the album One World which included the song Flying For Me, a tribute to the Challenger crew.		Star Trek IV: The Voyage Home was dedicated to the crew of the Challenger. Principal photography for The Voyage Home began four weeks after Challenger and her crew were lost.		In 1988, seven craters on the far side of the moon, within the Apollo Basin, were named after the fallen astronauts by the IAU.[98]		The Squadron "Challenger" 17 is an Air Force unit in the Texas A&M Corps of Cadets that emphasizes athletic and academic success in honor of the Challenger crew.[99] The unit was established in 1992.[100]		In Huntsville, Alabama, home of Marshall Space Flight Center, Challenger Elementary School, Challenger Middle School, and the Ronald E. McNair Junior High School are all named in memory of the crew. (Huntsville has also named new schools posthumously in memory of each of the Apollo 1 astronauts and the final Space Shuttle Columbia crew.) Streets in a neighborhood established in the late-1980s in nearby Decatur are named in memory of each of the Challenger crew members (Onizuka excluded), as well as the three deceased Apollo 1 astronauts.[citation needed] Julian Harris Elementary School is located on McAuliffe Drive, and its mascot is the Challengers.		In San Antonio, Texas, Scobee Elementary School opened in 1987, the year after the disaster. Students at the school are referred to as "Challengers." An elementary school in Nogales, Arizona, commemorates the accident in name, Challenger Elementary School, and their school motto, "Reach for the sky". The suburbs of Seattle, Washington are home to Challenger Elementary School in Issaquah, Washington[101] and Christa McAuliffe Elementary School in Sammamish, Washington.[102] and Dick Scobee Elementary in Auburn, Washington. In San Diego, California, the next-opened public middle school in the San Diego Unified School District was named Challenger Middle School.[103] The City of Palmdale, the birthplace of the entire shuttle fleet, and its neighbor City of Lancaster, California, both renamed 10th Street East, from Avenue M to Edwards Air Force Base, to Challenger Way in honor of the lost shuttle and its crew.[citation needed] This was the road that the Challenger, Enterprise, and Columbia all were towed along in their initial move from U.S. Air Force Plant 42 to Edwards AFB after completion since Palmdale airport had not yet installed the shuttle crane for placement of an orbiter on the 747 Shuttle Carrier Aircraft.[citation needed] In addition, the City of Lancaster has built Challenger Middle School, and Challenger Memorial Hall at the former site of the Antelope Valley Fairgrounds, all in tribute to the Challenger shuttle and crew.[citation needed] Another school was opened in Chicago, IL as the Sharon Christa McAuliffe Elementary school.[104] The public Peers Park in Palo Alto, California features a "Challenger Memorial Grove" that includes redwood trees grown from seeds carried aboard Challenger in 1985.[105] In Boise, ID, Boise High School has a memorial to the Challenger astrounauts. In 1986 in Webster, Texas, the "Challenger Seven Memorial Park" was also dedicated in remembrance of the event.[106]		In Port Saint John, Florida within Brevard County the same county that the Kennedy Space Center resides in is the Challenger 7 Elementary School which is named in memory of the seven crew members of STS-51-L.[107] There is also a middle school in neighboring Rockledge, McNair Magnet School, named after astronaut Ronald McNair.[108] A middle school (formerly high school) in Mohawk, New York is named after Payload Specialist Gregory Jarvis. Another middle school in Boynton Beach, Florida, is named after deceased teacher Christa McAuliffe. There are also schools in Denver, Colorado, Saratoga, California, Lowell, Massachusetts, Houston, Texas, and Lenexa, Kansas, named in honor of Christa McAuliffe. The McAuliffe-Shepard Discovery Center, a science museum and planetarium in Concord, New Hampshire, is also partly named in her honor. There is also an elementary school in Germantown, Maryland, named after Christa McAuliffe as well as in Green Bay, Wisconsin and Hastings, Minnesota.[109][110][111] The draw bridge over the barge canal on State Rd.3 on Merritt Island, Florida, is named the Christa McAuliffe Memorial Bridge.[112] In Oxnard, Ca, McAuliffe Elementary School is named after Christa McAuliffe, and bears tribute to the crew of the Challenger flight in its logo, with an image of the Shuttle and the motto "We Meet The Challenge." They crew and mission are also tributed by the schools mascot, The Challengers, and their saying "We Reach for the Stars."[113]		The 1996 science fiction television series Space Cases is set on a spaceship known as the Christa, named in honor of Christa McAuliffe, described in the series as "an Earth teacher who died during the early days of space exploration."		In 1997, playwright Jane Anderson wrote a play inspired by the Challenger incident entitled Defying Gravity.[114]		In 2004, President George W. Bush conferred posthumous Congressional Space Medals of Honor to all 14 crew members lost in the Challenger and Columbia accidents.[115]		In 2009, Allan J. McDonald, former director of the Space Shuttle Solid Motor Rocket Project for Morton Thiokol, Inc. published his book Truth, Lies, and O-Rings: Inside the Space Shuttle Challenger Disaster. Up to that point, no one directly involved in the decision to launch Challenger had published a memoir about the experience.[116]		In June 14, 2011, Christian electronic/dance pop singer Adam Young, through his electronica project, released a song about the Challenger incident on his third studio album All Things Bright and Beautiful.		In December 2013, Beyoncé released a song titled "XO", which begins with a sample of former NASA public affairs officer Steve Nesbitt, recorded moments after the disaster: "Flight controllers here looking very carefully at the situation. Obviously a major malfunction."[117] The song raised controversy, with former NASA astronauts and families labelling Knowles' sample as "insensitive."[118] Hardeep Phull of New York Post described the sample's presence as "tasteless,"[119] and Keith Cowing of NASA Watch suggested that the usage of the clip ranged from "negligence" to "repugnant."[120] On December 31, 2013, NASA criticized the use of the sample, stating that "The Challenger accident is an important part of our history; a tragic reminder that space exploration is risky and should never be trivialized. NASA works everyday to honor the legacy of our fallen astronauts as we carry out our mission to reach for new heights and explore the universe."[117][120] On December 30, 2013, Knowles issued a statement to ABC News, saying: "My heart goes out to the families of those lost in the Challenger disaster. The song 'XO' was recorded with the sincerest intention to help heal those who have lost loved ones and to remind us that unexpected things happen, so love and appreciate every minute that you have with those who mean the most to you. The songwriters included the audio in tribute to the unselfish work of the Challenger crew with hope that they will never be forgotten."[121]		On June 16, 2015, post-metal band Vattnet Viskar released a full-length album titled Settler which was largely inspired by the Challenger accident and Christa McAuliffe in particular. The album was released in Europe on June 29. Guitarist Chris Alfieri stated in a June 17, 2015 interview with Decibel Magazine that, "Christa was from Concord, New Hampshire, the town that I live in. One of my first memories is the Challenger mission’s demise, so it’s a personal thing for me. But the album isn’t about the explosion, it's about everything else. Pushing to become something else, something better. A transformation, and touching the divine."[122]		On June 27, 2015, the "Forever Remembered" exhibit at the Kennedy Space Center Visitor Complex, Florida, opened and includes a display of a section of Challenger's recovered fuselage to memorialize and honor the fallen astronauts. The exhibit was opened by NASA Administrator Charles Bolden along with family members of the crew.[123]		On August 7, 2015 English singer-songwriter Frank Turner released his sixth album Positive Songs for Negative People which includes the song "Silent Key".[124]		The mountain range Challenger Colles on Pluto was named in honor of the victims of the Challenger disaster.		The Challenger Columbia Stadium in League City, Texas is named in honor of the victims of both the Challenger disaster as well as the Columbia disaster in 2003.		Until 2010, the live broadcast of the launch and subsequent disaster by CNN was the only known on-location video footage from within range of the launch site. More recently, as of March 15, 2014, seven other motion picture recordings of the event have become publicly available:		An ABC television movie titled Challenger was broadcast on February 24, 1990. It starred Barry Bostwick as Scobee, Brian Kerwin as Smith, Joe Morton as McNair, Keone Young as Onizuka, Julie Fulton as Resnik, Richard Jenkins as Jarvis and Karen Allen as McAuliffe.[133][134][135]		A BBC docudrama titled The Challenger was broadcast on March 18, 2013, based on the last of Richard Feynman's autobiographical works, What Do You Care What Other People Think?. It stars William Hurt as Feynman.[136][137]		In the Sega Saturn version of the video game The House of the Dead, the words "Challenger, go at throttle up", spoken by Richard O. Covey from the mission control room only seconds before the explosion, can be heard in the soundtrack of Stage 2, several times.[138]		 This article incorporates public domain material from websites or documents of the National Aeronautics and Space Administration.		Coordinates: 28°38′24″N 80°16′48″W﻿ / ﻿28.64000°N 80.28000°W﻿ / 28.64000; -80.28000		
A semantic network, or frame network, is a network that represents semantic relations between concepts. This is often used as a form of knowledge representation. It is a directed or undirected graph consisting of vertices, which represent concepts, and edges, which represent semantic relations between concepts.[1]		Typical standardized semantic networks are expressed as semantic triples.						"Semantic Nets" were first invented for computers by Richard H. Richens of the Cambridge Language Research Unit in 1956 as an "interlingua" for machine translation of natural languages.[2]		They were independently developed by Robert F. Simmons,[3] Sheldon Klein, Karen McConologue, M. Ross Quillian[4] and others at System Development Corporation in the early 1960s as part of the SYNTHEX project. It later featured prominently in the work of Allan M. Collins and Quillian (e.g., Collins and Quillian;[5][6] Collins and Loftus[7] Quillian[8][9][10][11])		In the late 1980s, two Netherlands universities, Groningen and Twente, jointly began a project called Knowledge Graphs, which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations, to facilitate algebras on the graph.[12] In the subsequent decades, the distinction between semantic networks and knowledge graphs was blurred.[13][14] In 2012, Google gave their knowledge graph the name Knowledge Graph.		A semantic network is used when one has knowledge that is best understood as a set of concepts that are related to one another.		Most semantic networks are cognitively based. They also consist of arcs and nodes which can be organized into a taxonomic hierarchy. Semantic networks contributed ideas of spreading activation, inheritance, and nodes as proto-objects.		Using an association list.		You would use the "assoc" function with a key of "canary" to extract all the information about the "canary" type.[15]		An example of a semantic network is WordNet, a lexical database of English. It groups English words into sets of synonyms called synsets, provides short, general definitions, and records the various semantic relations between these synonym sets. Some of the most common semantic relations defined are meronymy (A is part of B, i.e. B has A as a part of itself), holonymy (B is part of A, i.e. A has B as a part of itself), hyponymy (or troponymy) (A is subordinate of B; A is kind of B), hypernymy (A is superordinate of B), synonymy (A denotes the same as B) and antonymy (A denotes the opposite of B).		WordNet properties have been studied from a network theory perspective and compared to other semantic networks created from Roget's Thesaurus and word association tasks. From this perspective the three of them are a small world structure.[16]		It is also possible to represent logical descriptions using semantic networks such as the existential graphs of Charles Sanders Peirce or the related conceptual graphs of John F. Sowa.[1] These have expressive power equal to or exceeding standard first-order predicate logic. Unlike WordNet or other lexical or browsing networks, semantic networks using these representations can be used for reliable automated logical deduction. Some automated reasoners exploit the graph-theoretic features of the networks during processing.		Other examples of semantic networks are Gellish models. Gellish English with its Gellish English dictionary, is a formal language that is defined as a network of relations between concepts and names of concepts. Gellish English is a formal subset of natural English, just as Gellish Dutch is a formal subset of Dutch, whereas multiple languages share the same concepts. Other Gellish networks consist of knowledge models and information models that are expressed in the Gellish language. A Gellish network is a network of (binary) relations between things. Each relation in the network is an expression of a fact that is classified by a relation type. Each relation type itself is a concept that is defined in the Gellish language dictionary. Each related thing is either a concept or an individual thing that is classified by a concept. The definitions of concepts are created in the form of definition models (definition networks) that together form a Gellish Dictionary. A Gellish network can be documented in a Gellish database and is computer interpretable.		SciCrunch is a collaboratively edited knowledge base for scientific resources. It provides unambiguous identifiers (Research Resource IDentifiers or RRIDs) for software, lab tools etc. and it also provides options to create links between RRIDs and from communities.		Another example of semantic networks, based on category theory, is ologs. Here each type is an object, representing a set of things, and each arrow is a morphism, representing a function. Commutative diagrams also are prescribed to constrain the semantics.		In the social sciences people sometimes use the term semantic network to refer to co-occurrence networks.[17] The basic idea is that words that co-occur in a unit of text, e.g. a sentence, are semantically related to one another. Ties based on co-occurrence can then be used to construct semantic networks.		There are also elaborate types of semantic networks connected with corresponding sets of software tools used for lexical knowledge engineering, like the Semantic Network Processing System (SNePS) of Stuart C. Shapiro[18] or the MultiNet paradigm of Hermann Helbig,[19] especially suited for the semantic representation of natural language expressions and used in several NLP applications.		Semantic networks are used in specialized information retrieval tasks, such as plagiarism detection. They provide information on hierarchical relations in order to employ semantic compression to reduce language diversity and enable the system to match word meanings, independently from sets of words used.		
In German humour, a Manta joke (German: Mantawitz) is a joke cycle about the Mantafahrer ("Manta driver"), the male driver of an Opel Manta, who is an aggressive driver, dull, lower class, macho, and infatuated with both his car and his blonde hairdresser girlfriend.[1]		The joke pokes fun at a stereotype of working class owner of a second-tier muscle car, typified by the Opel Manta.[2] Mantas were targeted at buyers who yearned for a sports car but could not afford a status car such as a BMW or Mercedes. Proud Manta owners often decorated them with chrome, G-T stripes, muscle tyre, high-beam, etc., to mimic the exclusiveness of race cars.[3][1][4]		The jokes poke fun at his vanity, dullness, etc.		The popularity of such jokes spawned two successful movies (Manta – Der Film (de) and Manta, Manta, the latter starring Til Schweiger as the Mantafahrer).[3]		
Facial Action Coding System (FACS) is a system to taxonomize human facial movements by their appearance on the face, based on a system originally developed by a Swedish anatomist named Carl-Herman Hjortsjö.[1] It was later adopted by Paul Ekman and Wallace V. Friesen, and published in 1978.[2] Ekman, Friesen, and Joseph C. Hager published a significant update to FACS in 2002.[3] Movements of individual facial muscles are encoded by FACS from slight different instant changes in facial appearance.[4] It is a common standard to systematically categorize the physical expression of emotions, and it has proven useful to psychologists and to animators. Due to subjectivity and time consumption issues, FACS has been established as a computed automated system that detects faces in videos, extracts the geometrical features of the faces, and then produces temporal profiles of each facial movement.[4] The pioneer F-M Facial Action Coding System 2.0 (F-M FACS 2.0) [5] was created in 2017 by Dr. Freitas-Magalhães, and presents 2,000 segments in 4K, using 3D technology and automatic and real-time recognition.						Using FACS,[6] human coders can manually code nearly any anatomically possible facial expression, deconstructing it into the specific Action Units (AU) and their temporal segments that produced the expression. As AUs are independent of any interpretation, they can be used for any higher order decision making process including recognition of basic emotions, or pre-programmed commands for an ambient intelligent environment. The FACS Manual is over 500 pages in length and provides the AUs, as well as Ekman's interpretation of their meaning.		FACS defines AUs, which are a contraction or relaxation of one or more muscles. It also defines a number of Action Descriptors, which differ from AUs in that the authors of FACS have not specified the muscular basis for the action and have not distinguished specific behaviors as precisely as they have for the AUs.		For example, FACS can be used to distinguish two types of smiles as follows:[7]		Although the labeling of expressions currently requires trained experts, researchers have had some success in using computers to automatically identify FACS codes, and thus quickly identify emotions.[8] Computer graphical face models, such as CANDIDE or Artnatomy, allow expressions to be artificially posed by setting the desired action units.		The use of FACS has been proposed for use in the analysis of depression,[9] and the measurement of pain in patients unable to express themselves verbally.[10]		FACS is designed to be self-instructional. People can learn the technique from a number of sources including manuals and workshops,[11] and obtain certification through testing.[12] The original FACS has been modified to analyze facial movements in several non-human primates, namely chimpanzees,[13] rhesus macaques,[14] gibbons and siamangs,[15] and orangutans.[16] More recently, it was adapted for a domestic species, the dog.[17]		Thus, FACS can be used to compare facial repertoires across species due to its anatomical basis. A study conducted by Vick and others (2006) suggests that FACS can be modified by taking differences in underlying morphology into account. Such considerations enable a comparison of the homologous facial movements present in humans and chimpanzees, to show that the facial expressions of both species result from extremely notable appearance changes. The development of FACS tools for different species allows the objective and anatomical study of facial expressions in communicative and emotional contexts. Furthermore, a cross-species analysis of facial expressions can help to answer interesting questions, such as which emotions are uniquely human.[18]		EMFACS (Emotional Facial Action Coding System)[19] and FACSAID (Facial Action Coding System Affect Interpretation Dictionary)[20] consider only emotion-related facial actions. Examples of these are:		For clarification, FACS is an index of facial expressions, but does not actually provide any bio-mechanical information about the degree of muscle activation. Though muscle activation is not part of FACS, the main muscles involved in the facial expression have been added here for the benefit of the reader.		Action Units (AUs) are the fundamental actions of individual muscles or groups of muscles.		Action Descriptors (ADs) are unitary movements that may involve the actions of several muscle groups (e.g., a forward‐thrusting movement of the jaw). The muscular basis for these actions hasn't been specified and specific behaviors haven't been distinguished as precisely as for the AUs.		For most accurate annotation, FACS suggests agreement from at least two independent certified FACS encoders.		Intensities of FACS are annotated by appending letters A–E (for minimal-maximal intensity) to the Action Unit number (e.g. AU 1A is the weakest trace of AU 1 and AU 1E is the maximum intensity possible for the individual person).		There are other modifiers present in FACS codes for emotional expressions, such as "R" which represents an action that occurs on the right side of the face and "L" for actions which occur on the left. An action which is unilateral (occurs on only one side of the face) but has no specific side is indicated with a "U" and an action which is unilateral but has a stronger side is indicated with an "A."		These codes are reserved for recording information about gross behaviors that may be relevant to the facial actions that are scored.		
Harvey Sacks (July 19, 1935 – November 14, 1975) was an American sociologist influenced by the ethnomethodology tradition. He pioneered extremely detailed studies of the way people use language in everyday life. Despite his early death in a car crash and the fact that he did not publish widely, he founded the discipline of conversation analysis. His work has had significant influence on fields such as linguistics, discourse analysis, and discursive psychology.						Sacks received his doctoral degree in sociology at the University of California, Berkeley (1966),[2] an LL.B. at Yale Law School (1959),[3] and a B.A. at Columbia College (1955).[3] He lectured at the University of California, Los Angeles and Irvine from 1964-1975.		Sacks became interested in the structure of conversation while working at a suicide counseling hotline in Los Angeles in the 1960s.[4] The calls to the hotline were recorded, and Sacks was able to gain access to the tapes and study them. In the 1960s, prominent linguists like Noam Chomsky believed that conversation was too disorganized to be worthy of any kind of in-depth structural analysis[citation needed] . Sacks strongly disagreed, since he saw structure in every conversation, and developed conversation analysis as a result.		Sacks's recorded lectures were transcribed (by Gail Jefferson who also edited them posthumously) but the tapes were not saved. The duplicated copies of the transcribed lectures were made freely available by Sacks and achieved international circulation and recognition during his lifetime and subsequently[citation needed] .		He treated such topics as: the organization of person-reference; topic organization and stories in conversation; speaker selection preferences; pre-sequences; the organization of turn-taking; conversational openings and closings; and puns, jokes, stories and repairs in conversation among many others.[5]		Emanuel Schegloff, one of Sacks's close collaborators, colleagues and co-authors, became his literary executor. The subsequent handling of the literary estate (Nachlass, to use the academic term) has attracted some controversy.[citation needed]		Sacks's major work, Lectures on Conversation, is composed of edited revisions of transcribed lectures held from Spring 1964 through to 1972, and comprises about 1200 pages in a two-volume work published by Basil Blackwel in 1992. This publication project was instigated largely by David Sudnow and Gail Jefferson, colleagues and students of Sacks at Berkeley, UCLA and Irvine, and includes an introduction by Emanuel Schegloff. In her acknowledgements in these volumes, Jefferson mentioned the help of Sudnow in dealing with Sacks's literary estate. The Harvey Sacks Memorial Association, registered as a not-for-profit Association, was formed by Sudnow.[citation needed]		These Lectures have been important for Sacks's later influence and for the field of Conversation Analysis.		Sudnow was a follower of Alfred Schutz in phenomenology, and Harold Garfinkel in ethnomethodology. Sudnow regards the work of Sacks as outside the ethnomethodological mainstream.[citation needed] By contrast Garfinkel lists Sacks as one of 'Ethnomethodology's Authors' [6]		
LOL or lol, an acronym for laugh(ing) out loud[1][2][3] or lots of laughs,[4][5][6] is a popular element of Internet slang. It was first used almost exclusively on Usenet, but has since become widespread in other forms of computer-mediated communication and even face-to-face communication. It is one of many initialisms for expressing bodily reactions, in particular laughter, as text, including initialisms for more emphatic expressions of laughter such as LMAO[7] ("laugh(ing) my ass off") and ROFL[8][9][10] (or its older form ROTFL;[11][12] "roll(ing) on the floor laughing"). Other unrelated expansions include the now mostly obsolete "lots of luck" or "lots of love" used in letter-writing.[13]		The list of acronyms "grows by the month",[9] and they are collected along with emoticons and smileys into folk dictionaries that are circulated informally amongst users of Usenet, IRC, and other forms of (textual) computer-mediated communication.[14] These initialisms are controversial, and several authors[15][16][17][18] recommend against their use, either in general or in specific contexts such as business communications.		LOL was first documented in the Oxford English Dictionary in March 2011.[19]						Laccetti (professor of humanities at Stevens Institute of Technology) and Molski, in their essay entitled The Lost Art of Writing, are critical of the terms, predicting reduced chances of employment for students who use such slang, stating that, "Unfortunately for these students, their bosses will not be 'lol' when they read a report that lacks proper punctuation and grammar, has numerous misspellings, various made-up words, and silly acronyms."[15][16] Fondiller and Nerone in their style manual assert that "professional or business communication should never be careless or poorly constructed" whether one is writing an electronic mail message or an article for publication, and warn against the use of smileys and abbreviations, stating that they are "no more than e-mail slang and have no place in business communication".[17]		Yunker and Barry in a study of online courses and how they can be improved through podcasting have found that these slang terms, and emoticons as well, are "often misunderstood" by students and are "difficult to decipher" unless their meanings are explained in advance. They single out the example of "ROFL" as not obviously being the abbreviation of "rolling on the floor laughing" (emphasis added).[18] Haig singles out LOL as one of the three most popular initialisms in Internet slang, alongside BFN[dubious – discuss] ("bye for now") and IMHO ("in my honest/humble opinion"). He describes the various initialisms of Internet slang as convenient, but warns that "as ever more obscure acronyms emerge they can also be rather confusing".[1] Bidgoli likewise states that these initialisms "save keystrokes for the sender but [...] might make comprehension of the message more difficult for the receiver" and that "[s]lang may hold different meanings and lead to misunderstandings especially in international settings"; he advises that they be used "only when you are sure that the other person knows the meaning".[20]		Shortis observes that ROFL is a means of "annotating text with stage directions".[10] Hershock, in discussing these terms in the context of performative utterances, points out the difference between telling someone that one is laughing out loud and actually laughing out loud: "The latter response is a straightforward action. The former is a self-reflexive representation of an action: I not only do something but also show you that I am doing it. Or indeed, I may not actually laugh out loud but may use the locution 'LOL' to communicate my appreciation of your attempt at humor."[9]		David Crystal notes that use of LOL is not necessarily genuine, just as the use of smiley faces or grins is not necessarily genuine, posing the rhetorical question "How many people are actually 'laughing out loud' when they send LOL?".[21] Franzini concurs, stating that there is as yet no research that has determined the percentage of people who are actually laughing out loud when they write LOL.[2]		Victoria Clarke, in her analysis of telnet talkers, states that capitalization is important when people write LOL, and that "a user who types LOL may well be laughing louder than one who types lol", and opines that "these standard expressions of laughter are losing force through overuse".[22] Egan describes LOL, ROFL, and other initialisms as helpful so long as they are not overused. He recommends against their use in business correspondence because the recipient may not be aware of their meanings, and because in general neither they nor emoticons are (in his view) appropriate in such correspondence.[3] June Hines Moore shares that view.[23] So, too, does Lindsell-Roberts, who gives the same advice of not using them in business correspondence, "or you won't be LOL".[24]		LOL, ROFL, and other initialisms have crossed from computer-mediated communication to face-to-face communication. David Crystal—likening the introduction of LOL, ROFL, and others into spoken language in magnitude to the revolution of Johannes Gutenberg's invention of movable type in the 15th century—states that this is "a brand new variety of language evolving", invented by young people within five years, that "extend[s] the range of the language, the expressiveness [and] the richness of the language".[25][26]		Geoffrey K. Pullum points out that even if interjections such as LOL and ROFL were to become very common in spoken English, their "total effect on language" would be "utterly trivial".[27]		Conversely, a 2003 study of college students by Naomi Baron found that the use of these initialisms in computer-mediated communication (CMC), specifically in instant messaging, was actually lower than she had expected. The students "used few abbreviations, acronyms, and emoticons". The spelling was "reasonably good" and contractions were "not ubiquitous". Out of 2,185 transmissions, there were 90 initialisms in total, only 31 CMC-style abbreviations, and 49 emoticons.[26] Out of the 90 initialisms, 76 were occurrences of LOL.[28]		While LOL and similar acronyms are used a lot in real life speech, a lot of people are annoyed with this, say it accidentally or use it ironically.[29]		On March 24, 2011, LOL, along with other acronyms, was formally recognized in an update of the Oxford English Dictionary.[19][30] In their research, it was determined that the earliest recorded use of LOL as an initialism was for "little old lady" in the 1960s.[31] They also discovered that the oldest written record of the use of LOL in the contemporary meaning of "Laughing Out Loud" was from a message typed by Wayne Pearson in the 1980s, from the archives of Usenet.[32]		Gabriella Coleman references "lulz" extensively in her anthropological studies of Anonymous.[33][34]		The past tense of lol is lolled. The participle form is lolling.		Most of these variants are usually found in lowercase.		In some languages with a non-Latin script, the abbreviation LOL itself is also often transliterated. See for example Arabic لول and Russian лол.[citation needed]		Pre-dating the Internet and phone texting by a century, the way to express laughter in morse code is "hi hi". The sound of this in morse ('di-di-di-dit di-dit, di-di-di-dit di-dit') is thought to represent chuckling.[49][50]		
The Brothers Grimm (die Brüder Grimm or die Gebrüder Grimm), Jacob (1785–1863) and Wilhelm Grimm (1786–1859), were German academics, philologists, cultural researchers, lexicographers and authors who together collected and published folklore during the 19th century. They were among the best-known storytellers of folk tales, and popularized stories such as "Cinderella" ("Aschenputtel"), "The Frog Prince" ("Der Froschkönig"), "The Goose-Girl" ("Die Gänsemagd"), "Hansel and Gretel" ("Hänsel und Gretel"), "Rapunzel", "Rumpelstiltskin" ("Rumpelstilzchen"), "Sleeping Beauty" ("Dornröschen"), and "Snow White" ("Schneewittchen"). Their first collection of folk tales, Children's hand Household Tales (Kinder- und Hausmärchen), was published in 1812.		The brothers spent their formative years in the German town of Hanau. Their father's death in 1796 impoverished the family and affected the brothers for many years after. They attended the University of Marburg where they developed a curiosity about German folklore, which grew into a lifelong dedication to collecting German folk tales. The rise of Romanticism during the 19th century revived interest in traditional folk stories, which to the brothers represented a pure form of national literature and culture. With the goal of researching a scholarly treatise on folk tales, they established a methodology for collecting and recording folk stories that became the basis for folklore studies. Between 1812 and 1857, their first collection was revised and republished many times, growing from 86 stories to more than 200. In addition to writing and modifying folk tales, the brothers wrote collections of well-respected German and Scandinavian mythologies, and in 1838 they began writing a definitive German dictionary (Deutsches Wörterbuch), which they were unable to finish during their lifetimes.		Many of the Grimms' folk tales have enjoyed enduring popularity. The tales are available in more than 100 languages and have been adapted by filmmakers including Lotte Reiniger and Walt Disney, with films such as Snow White and the Seven Dwarfs and Sleeping Beauty. During the 1930s and 40s, the tales were used as propaganda by the Third Reich; later in the 20th century psychologists such as Bruno Bettelheim reaffirmed the value of the work, in spite of the cruelty and violence in original versions of some of the tales, which the Grimms eventually sanitized.						Jacob Ludwig Carl Grimm was born on 4 January 1785, and his brother Wilhelm Carl Grimm was born on 24 February 1786. Both were born in Hanau, in the Landgraviate of Hesse-Kassel within the Holy Roman Empire (present-day Germany), to Philipp Wilhelm Grimm, a jurist, and Dorothea Grimm née Zimmer, daughter of a Kassel city councilman.[1] They were the second- and third-eldest surviving siblings in a family of nine children, three of whom died in infancy.[2][3][4] In 1791, the family moved to the countryside town of Steinau, when Philipp was employed there as district magistrate (Amtmann). The family became prominent members of the community, residing in a large home surrounded by fields. Biographer Jack Zipes writes that the brothers were happy in Steinau and "clearly fond of country life".[1] The children were educated at home by private tutors, receiving strict instruction as Lutherans that instilled in both a lifelong religious faith.[5] Later, they attended local schools.[1]		In 1796, Philipp Grimm died of pneumonia, plunging his family into poverty, and they were forced to relinquish their servants and large house. Dorothea depended on financial support from her father and sister, first lady-in-waiting at the court of William I, Elector of Hesse. Jacob was the eldest living son, and he was forced at age 11 to assume adult responsibilities (shared with Wilhelm) for the next two years. The two boys adhered to the advice of their grandfather, who continually exhorted them to be industrious.[1]		The brothers left Steinau and their family in 1798 to attend the Friedrichsgymnasium in Kassel, which had been arranged and paid for by their aunt. By then, they were without a male provider (their grandfather died that year), forcing them to rely entirely on each other, and they became exceptionally close. The two brothers differed in temperament; Jacob was introspective and Wilhelm was outgoing (although he often suffered from ill-health). Sharing a strong work ethic, they excelled in their studies. In Kassel, they became acutely aware of their inferior social status relative to "high-born" students who received more attention. Still, each brother graduated at the head of his class: Jacob in 1803 and Wilhelm in 1804.[1][6]		After graduation from the Friedrichsgymnasium, the brothers attended the University of Marburg. The university was small with about 200 students and there they became painfully aware that students of lower social status were not treated equally. They were disqualified from admission because of their social standing and had to request dispensation to study law. Wealthier students received stipends, but the brothers were excluded even from tuition aid. Their poverty kept them from student activities or university social life; ironically, however, their outsider status worked in their favor, and they pursued their studies with extra vigor.[6]		The brothers were inspired by their law professor Friedrich von Savigny, who awakened in them an interest in history and philology, and they turned to studying medieval German literature.[7] They shared Savigny's desire to see unification of the 200 German principalities into a single state. Through Savigny and his circle of friends—German romantics such as Clemens Brentano and Ludwig Achim von Arnim—the Grimms were introduced to the ideas of Johann Gottfried Herder, who thought that German literature should revert to simpler forms, which he defined as Volkspoesie (natural poetry) as opposed to Kunstpoesie (artistic poetry).[8] The brothers dedicated themselves with great enthusiasm to their studies, about which Wilhelm wrote in his autobiography, "the ardor with which we studied Old German helped us overcome the spiritual depression of those days."[9]		Jacob was still financially responsible for his mother, brother, and younger siblings in 1805, so he accepted a post in Paris as research assistant to von Savigny. On his return to Marburg, he was forced to abandon his studies to support the family, whose poverty was so extreme that food was often scarce. He took a job with the Hessian War Commission. In a letter written to his aunt at this time, Wilhelm wrote of their circumstances, "We five people eat only three portions and only once a day".[7]		Jacob found full-time employment in 1808 when he was appointed court librarian to the King of Westphalia and went on to become librarian in Kassel.[2] After their mother's death that year, he became fully responsible for his younger siblings. He arranged and paid for his brother Ludwig's studies at art school and for Wilhelm's extended visit to Halle to seek treatment for heart and respiratory ailments, following which Wilhelm joined Jacob as librarian in Kassel.[1] The brothers also began collecting folk tales at about this time, in a cursory manner and on Brentano's request. According to Jack Zipes, at this point "the Grimms were unable to devote all their energies to their research and did not have a clear idea about the significance of collecting folk tales in this initial phase."[1]		During their employment as librarians—which paid little but afforded them ample time for research—the brothers experienced a productive period of scholarship, publishing a number of books between 1812 and 1830.[10] In 1812, they published their first volume of 86 folk tales, Kinder- und Hausmärchen, followed quickly by two volumes of German legends and a volume of early literary history.[2] They went on to publish works about Danish and Irish folk tales and Norse mythology, while continuing to edit the German folk tale collection. These works became so widely recognized that the brothers received honorary doctorates from universities in Marburg, Berlin, and Breslau (now Wrocław).[10]		In 1825, Wilhelm married Henriette Dorothea (Dortchen) Wild, a long-time family friend and one of a group who supplied them with stories. Jacob never married but continued to live in the household with Wilhelm and Dortchen.[11] In 1830, both brothers were overlooked when the post of chief librarian came available, which disappointed them greatly.[10] They moved the household to Göttingen in the Kingdom of Hanover where they took employment at the University of Göttingen, Jacob as a professor and head librarian and Wilhelm as professor.[2]		During the next seven years, the brothers continued to research, write, and publish. In 1835, Jacob published the well-regarded German Mythology (Deutsche Mythologie); Wilhelm continued to edit and prepare the third edition of Kinder- und Hausmärchen for publication. The two brothers taught German studies at the university, becoming well-respected in the newly established discipline.[11]		In 1837, they lost their university posts after joining in protest with the Göttingen Seven. The 1830s were a period of political upheaval and peasant revolt in Germany, leading to the movement for democratic reform known as Young Germany. The Grimm brothers were not directly aligned with the Young Germans, but five of their colleagues reacted against the demands of King Ernest Augustus I, who dissolved the parliament of Hanover in 1837 and demanded oaths of allegiance from civil servants—including professors at the University of Göttingen. For refusing to sign the oath, the seven professors were dismissed and three were deported from Hanover, including Jacob who went to Kassel. He was later joined there by Wilhelm, Dortchen, and their four children.[11]		The brothers were without income in 1838 and again in extreme financial difficulty, so they began what became a lifelong project: the writing of a definitive dictionary. The first volume of their German Dictionary (Deutsches Wörterbuch) was not published until 1854. The brothers again depended on friends and supporters for financial assistance and influence in finding employment.[11]		In 1840, von Savigny and Bettina von Arnim appealed successfully to Frederick William IV of Prussia on behalf of the brothers who were offered posts at the University of Berlin. In addition to teaching posts, the Academy of Sciences offered them stipends to continue their research. Once they had established their household in Berlin, they directed their efforts towards the work on the German dictionary and continued to publish their research. Jacob turned his attention to researching German legal traditions and the history of the German language, which was published in the late 1840s and early 1850s; meanwhile, Wilhelm began researching medieval literature while editing new editions of Hausmärchen.[10]		After the Revolutions of 1848 in the German states, the brothers were elected to the civil parliament. Jacob became a prominent member of the National Assembly at Mainz.[11] Their political activities were short-lived, as their hope dwindled for a unified Germany and their disenchantment grew. In the late 1840s, Jacob resigned his university position and saw the publication of The History of the German Language (Geschichte der deutschen Sprache). Wilhelm continued at his university post until 1852. After retiring from teaching, the brothers devoted themselves to the German Dictionary for the rest of their lives.[11] Wilhelm died of an infection in Berlin in 1859,[12] and Jacob became increasingly reclusive, deeply upset at his brother's death. He continued work on the dictionary until his own death in 1863. Zipes writes of the Grimm brothers' dictionary and of their very large body of work: "Symbolically the last word was Frucht (fruit)."[11]		The rise of romanticism, Romantic nationalism, and trends in valuing popular culture in the early 19th century revived interest in fairy tales, which had declined since their late-17th-century peak.[13] Johann Karl August Musäus published a popular collection of tales between 1782 and 1787;[14] the Grimms aided the revival with their folklore collection, built on the conviction that a national identity could be found in popular culture and with the common folk (Volk). They collected and published tales as a reflection of German cultural identity. In the first collection, though, they included Charles Perrault's tales, published in Paris in 1697 and written for the literary salons of an aristocratic French audience. Scholar Lydie Jean explains that a myth was created that Perrault's tales came from the common people and reflected existing folklore in order to justify their inclusion—even though many of them were original.[13]		The brothers were directly influenced by Brentano and von Arnim, who edited and adapted the folk songs of Des Knaben Wunderhorn (The Boy's Magic Horn or cornucopia).[14] They began the collection with the purpose of creating a scholarly treatise of traditional stories and of preserving the stories as they had been handed from generation to generation—a practice that was threatened by increased industrialization.[16] Maria Tatar, professor of German studies at Harvard University, explains that it is precisely the handing from generation to generation and the genesis in the oral tradition that gives folk tales an important mutability. Versions of tales differ from region to region, "picking up bits and pieces of local culture and lore, drawing a turn of phrase from a song or another story and fleshing out characters with features taken from the audience witnessing their performance."[17]		However, as Tatar explains, the Grimms appropriated stories as being uniquely German, such as "Little Red Riding Hood", which had existed in many versions and regions throughout Europe, because they believed that such stories were reflections of Germanic culture.[15] Furthermore, the brothers saw fragments of old religions and faiths reflected in the stories which they thought continued to exist and survive through the telling of stories.[18]		When Jacob returned to Marburg from Paris in 1806, their friend Brentano sought the brothers' help in adding to his collection of folk tales, at which time the brothers began to gather tales in an organized fashion.[1] By 1810, they had produced a manuscript collection of several dozen tales, written after inviting storytellers to their home and transcribing what they heard. These tales were heavily modified in transcription, and many had roots in previously written sources.[19] At Brentano's request, they printed and sent him copies of the 53 tales that they collected for inclusion in his third volume of Des Knaben Wunderhorn.[2] Brentano either ignored or forgot about the tales, leaving the copies in a church in Alsace where they were found in 1920 and became known as the Ölenberg manuscript. It is the earliest extant version of the Grimms' collection and has become a valuable source to scholars studying the development of the Grimms' collection from the time of its inception. The manuscript was published in 1927 and again in 1975.[20]		The brothers gained a reputation for collecting tales from peasants, although many tales came from middle-class or aristocratic acquaintances. Wilhelm's wife Dortchen Wild and her family, with their nursery maid, told the brothers some of the more well-known tales, such as "Hansel and Gretel" and "Sleeping Beauty".[21] Wilhelm collected a number of tales after befriending August von Haxthausen, whom he visited in 1811 in Westphalia where he heard stories from von Haxthausen's circle of friends.[22] Several of the storytellers were of Huguenot ancestry, telling tales of French origin such as those told to the Grimms by Marie Hassenpflug, an educated woman of French Huguenot ancestry,[19] and it is probable that these informants were familiar with Perrault's Histoires ou contes du temps passé (Stories from Past Times).[13] Other tales were collected from Dorothea Viehmann, the wife of a middle-class tailor and also of French descent. Despite her middle-class background, in the first English translation she was characterized as a peasant and given the name Gammer Gretel.[16]		According to scholars such as Ruth Bottigheimer and Maria Tatar, some of the tales probably originated in written form during the medieval period with writers such as Straparola and Boccaccio, but were modified in the 17th century and again rewritten by the Grimms. Moreover, Tatar writes that the brothers' goal of preserving and shaping the tales as something uniquely German at a time of French occupation was a form of "intellectual resistance" and, in so doing, they established a methodology for collecting and preserving folklore that set the model to be followed later by writers throughout Europe during periods of occupation.[16][23]		From 1807 onward, the brothers added to the collection. Jacob established the framework, maintained through many iterations; from 1815 until his death, Wilhelm assumed sole responsibility for editing and rewriting the tales. He made the tales stylistically similar, added dialogue, removed pieces "that might detract from a rustic tone", improved the plots, and incorporated psychological motifs.[22] Ronald Murphy writes in The Owl, the Raven and the Dove that the brothers—and in particular Wilhelm—also added religious and spiritual motifs to the tales. He believes that Wilhelm "gleaned" bits from old Germanic faiths, Norse mythology, Roman and Greek mythology, and biblical stories that he reshaped.[18]		Over the years, Wilhelm worked extensively on the prose, expanded and added detail to the stories to the point that many grew to be twice the length of those in the earliest published editions.[24] In the later editions, Wilhelm polished the language to make it more enticing to a bourgeois audience, eliminated sexual elements, and added Christian elements. After 1819, he began writing for children (children were not initially considered the primary audience), adding entirely new tales or adding new elements to existing tales, elements that were often strongly didactic.[22]		Some changes were made in light of unfavorable reviews, particularly from those who objected that not all the tales were suitable for children because of scenes of violence and sexuality.[25] He worked to modify plots for many stories; for example, "Rapunzel" in the first edition of Kinder- und Hausmärchen clearly shows a sexual relationship between the prince and the girl in the tower, which he edited out in subsequent editions.[24] Tatar writes that morals were added (in the second edition, a king's regret was added to the scene in which his wife is to be burned at the stake) and often the characters in the tale were amended to appear more German: "every fairy (Fee), prince (Prinz) and princess (Prinzessin)—all words of French origin—was transformed into a more Teutonic-sounding enchantress (Zauberin) or wise woman (weise Frau), king's son (Königssohn), king's daughter (Königstochter)."[26]		The Grimms' legacy contains legends, novellas, and folk stories, the vast majority of which were not intended as children's tales. Von Armin was deeply concerned about the content of some of the tales, such as those which showed children being eaten, and suggested that they be removed. Instead, the brothers added an introduction with cautionary advice that parents steer children toward age-appropriate stories. Despite von Armin's unease, none of the tales were eliminated from the collection, in the brothers' belief that all the tales were of value and reflected inherent cultural qualities. Furthermore, the stories were didactic in nature at a time when discipline relied on fear, according to scholar Linda Dégh, who explains that tales such as "Little Red Riding Hood" and "Hansel and Gretel" were written to be "warning tales" for children.[27]		The stories in Kinder- und Hausmärchen include scenes of violence that have since been sanitized. For example, in the Grimms' original version of "Snow White", the Queen is Little Snow White's mother, not her stepmother, yet even so she orders her Huntsman to kill Snow White (her biological daughter) and bring home the child's lungs and liver so that she can eat them. The story ends with the Queen mother dancing at Snow White's wedding wearing a pair of red-hot iron shoes that kill her.[28] Another story ("The Goose Girl") has a servant being stripped naked and pushed into a barrel "studded with sharp nails" pointing inwards and then rolled down the street.[12] The Grimms' version of "The Frog Prince" describes the princess throwing the frog against a wall instead of kissing him. To some extent, the cruelty and violence may have been a reflection of medieval culture from which the tales originated, such as scenes of witches burning, as described in "The Six Swans".[12]		Tales with a spinning motif are broadly represented in the collection. In her essay "Tale Spinners: Submerged Voices in Grimms' Fairy Tales", children's literature scholar Bottigheimer explains that these stories reflect the degree to which spinning was crucial in the life of women in the 19th century and earlier. Spinning, and particularly the spinning of flax, was commonly performed in the home by women. Many stories begin by describing the occupation of a main character, as in "There once was a miller", yet spinning is never mentioned as an occupation, probably because the brothers did not consider it an occupation. Instead, spinning was a communal activity, frequently performed in a Spinnstube (spinning room), a place where women most likely kept the oral traditions alive by telling stories while engaged in tedious work.[29] In the stories, a woman's personality is often represented by her attitude toward spinning; a wise woman might be a spinster and Bottigheimer explains that the spindle was the symbol of a "diligent, well-ordered womanhood."[30] In some stories, such as "Rumpelstiltskin", spinning is associated with a threat; in others, spinning might be avoided by a character who is either too lazy or not accustomed to spinning because of her high social status.[29]		The tales were also criticized for being insufficiently German, which influenced the tales that the brothers included as well as their use of language. Scholars such as Heinz Rölleke say that the stories are an accurate depiction of German culture, showing "rustic simplicity [and] sexual modesty".[12] German culture is deeply rooted in the forest (wald), a dark dangerous place to be avoided, most particularly the old forests with large oak trees and yet a place to which Little Red Riding Hood's mother sent her daughter to deliver food to grandmother's house.[12]		Some critics such as Alistair Hauke use Jungian analysis to say that the deaths of the brothers' father and grandfather are the reason for the Grimms' tendency to idealize and excuse fathers, as well as the predominance of female villains in the tales, such as the wicked stepmother and stepsisters in "Cinderella", but this disregards the fact that they were collectors, not authors of the tales.[31] Another possible influence is found in stories such as "The Twelve Brothers", which mirrors the brothers' family structure of several brothers facing and overcoming opposition.[32] Autobiographical elements exist in some of the tales, and according to Zipes the work may have been a "quest" to replace the family life lost after their father died. The collection includes 41 tales about siblings, which Zipes says are representative of Jacob and Wilhelm. Many of the sibling stories follow a simple plot where the characters lose a home, work industriously at a specific task and, in the end, find a new home.[33]		Between 1812 and 1864, Kinder- und Hausmärchen was published 17 times: seven of the "Large edition" (Große Ausgabe) and ten of the "Small edition" (Kleine Ausgabe). The Large editions contained all the tales collected to date, extensive annotations, and scholarly notes written by the brothers; the Small editions had only 50 tales and were intended for children. Jacob and Wilhelm's younger brother Emil Grimm illustrated the Small editions, adding Christian symbolism to the drawings, such as depicting Cinderella's mother as an angel, and adding a Bible to the bedside table of Little Red Riding Hood's grandmother.[10]		The first volume was published in 1812 with 86 folk tales,[21] and a second volume with 70 additional tales was published late in 1814 (dated 1815 on the title page); together, the two volumes and their 156 tales are considered the first of the Large (annotated) editions.[34][35] A second expanded edition with 170 tales was published in 1819, followed in 1822 by a volume of scholarly commentary and annotations.[2][25] Five more Large editions were published in 1837, 1840, 1843, 1850, and 1857. The seventh and final edition of 1857 contained 211 tales—200 numbered folk tales and eleven legends.[2][25][35]		In Germany, Kinder- und Hausmärchen was also released in a "popular poster-sized Bilderbogen (broadsides)"[35] format and in single story formats for the more popular tales, such as "Hansel and Gretel". The stories were often added to collections by other authors without respect to copyright as the tales became a focus of interest for children's book illustrators,[35] with well-known artists such as Arthur Rackham, Walter Crane, and Edmund Dulac illustrating the tales. A popular edition that sold well was released in the mid-19th century and included elaborate etchings by George Cruikshank.[36] At the deaths of the brothers, the copyright went to Hermann Grimm (Wilhelm's son) who continued the practice of printing the volumes in expensive and complete editions; however, the copyright lapsed after 1893 and the stories began to be published in many formats and editions.[35] In the 21st century, Kinder- und Hausmärchen is a universally recognized text, commonly called Grimms' Fairy Tales in English. Jacob and Wilhelm's collection of stories has been translated to more than 160 languages with 120 different editions of the text available for sale in the US alone.[12]		While at the University of Marburg, the brothers came to see culture as tied to language and regarded the purest cultural expression in the grammar of a language. They moved away from Brentano's practice—and that of the other romanticists—who frequently changed original oral styles of folk tale to a more literary style, which the brothers considered artificial. They thought that the style of the people (the volk) reflected a natural and divinely inspired poetry (naturpoesie) as opposed to the kunstpoesie (art poetry), which they saw as artificially constructed.[37][38] As literary historians and scholars, they delved into the origins of stories and attempted to retrieve them from the oral tradition without loss of the original traits of oral language.[37]		The brothers strongly believed that the dream of national unity and independence relied on a full knowledge of the cultural past that was reflected in folklore.[38] They worked to discover and crystallize a kind of Germanness in the stories that they collected because they believed that folklore contained kernels of ancient mythologies and beliefs which were crucial to understanding the essence of German culture.[16] By examining culture from a philological point of view, they sought to establish connections between German law and culture and local beliefs.[37]		The Grimms considered the tales to have origins in traditional Germanic folklore, which they thought had been "contaminated" by later literary tradition.[16] In the shift from the oral tradition to the printed book, tales were translated from regional dialects to Standard German (Hochdeutsch or High German).[39] Over the course of the many modifications and revisions, however, the Grimms sought to reintroduce regionalisms, dialects, and Low German to the tales—to re-introduce the language of the original form of the oral tale.[40]		As early as 1812, they published Die beiden ältesten deutschen Gedichte aus dem achten Jahrhundert: Das Lied von Hildebrand und Hadubrand und das Weißenbrunner Gebet (The Two Oldest German Poems of the Eighth Century: The Song of Hildebrand and Hadubrand and the Wessobrunn Prayer). The Song of Hildebrand and Hadubrand is a 9th-century German heroic song, while the Wessobrunn Prayer is the earliest known German heroic song.[41]		Between 1816 and 1818, the brothers published a two-volume work titled Deutsche Sagen (German Legends) consisting of 585 German legends.[34] Jacob undertook most of the work of collecting and editing the legends that he organized according to region and historical (ancient) legends,[42] and which were about real people or events.[41] It was meant to be a scholarly work, yet the historical legends were often taken from secondary sources, interpreted, modified, and rewritten, resulting in works "that were regarded as trademarks".[42] Some scholars criticized the Grimm's methodology in collecting and rewriting the legends, yet conceptually they set an example for legend collections that was to be followed by others throughout Europe. Unlike the collection of folk tales, Deutsche Sagen sold poorly,[42] but Zipes says that the collection is a "vital source for folklorists and critics alike".[43]		Less well known in the English-speaking world is the brothers' monumental scholarly work on a German dictionary, the Deutsches Wörterbuch, which they began in 1838. Not until 1852 did they begin publishing the dictionary in installments.[42] The work on the dictionary could not be finished in their lifetime because in it they gave a history and analysis of each word.[41]		"And now the old king gathered together his court, and asked all his kingdom to come and celebrate the wedding of his son and the princess. And young and old, noble and squire, gentle and simple, came at once on the summons; and among the rest came the friendly dwarf, with the sugarloaf hat, and a new scarlet cloak.		"The day came that had been fixed for the marriage. The bridegroom arrived and also a large company of guests, for the miller had taken care to invite all his friends and relations. As they sat at the feast, each guest in turn was asked to tell a tale; the bride sat still and did not say a word. ‘And you, my love,’ said the bridegroom, turning to her, ‘is there no tale you know? Tell us something.’ ‘I will tell you a dream, then,’ said the bride. ‘I went alone through a forest and came at last to a house; not a soul could I find within, but a bird that was hanging in a cage on the wall cried:		And again a second time it said these words.’ ‘My darling, this is only a dream.’ ‘I went on through the house from room to room, but they were all empty, and everything was so grim and mysterious. At last I went down to the cellar, and there sat a very, very old woman, who could not keep her head still. I asked her if my betrothed lived here, and she answered, “Ah, you poor child, you are come to a murderers’ den; your betrothed does indeed live here, but he will kill you without mercy and afterwards cook and eat you.”’" - The Robber Bridegroom [45]		"A poor woodman sat in his cottage one night, smoking his pipe by the fireside, while his wife sat by his side spinning. ‘How lonely it is, wife,’ said he, as he puffed out a long curl of smoke, ‘for you and me to sit here by ourselves, without any children to play about and amuse us while other people seem so happy and merry with their children!’ ‘What you say is very true,’ said the wife, sighing, and turning round her wheel; ‘how happy should I be if I had but one child! If it were ever so small—nay, if it were no bigger than my thumb—I should be very happy, and love it dearly.’ Now—odd as you may think it—it came to pass that this good woman’s wish was fulfilled, just in the very way she had wished it; for, not long afterwards, she had a little boy, who was quite healthy and strong, but was not much bigger than my thumb. So they said, ‘Well, we cannot say we have not got what we wished for, and, little as he is, we will love him dearly.’ And they called him Thomas Thumb." -Intro to Tom Thumb [46]		Kinder- und Hausmärchen was not an immediate bestseller, but its popularity grew with each edition.[48] The early editions attracted lukewarm critical reviews, generally on the basis that the stories were unappealing to children. The brothers responded with modifications and rewrites in order to increase the book's market appeal to that demographic.[16] By the 1870s, the tales had increased greatly in popularity, to the point that they were added to the teaching curriculum in Prussia. In the 20th century, the work has maintained status as second only to the Bible as the most popular book in Germany. Its sales generated a mini-industry of criticism which analyzed the tales' folkloric content in the context of literary history, socialism, and psychological elements often along Freudian and Jungian lines.[48]		In their research, the brothers made a science of the study of folklore (see folkloristics), generating a model of research that "launched general fieldwork in most European countries",[49] and setting standards for research and analysis of stories and legends that made them pioneers in the field of folklore in the 19th century.[50]		During the Third Reich, the Grimms' stories were used to foster nationalism, and the Nazi Party decreed that Kinder- und Hausmärchen was a book which each household should own; later, in Allied-occupied Germany, the book was banned for a period.[51] In the US, the 1937 release of Walt Disney's Snow White and the Seven Dwarfs shows the triumph of good over evil, innocence over oppression, according to Zipes—a popular theme that Disney repeated in 1959 during the Cold War with the production of Sleeping Beauty.[52] The Grimms' tales have provided much of the early foundation on which the Disney empire was built.[12] In film, the Cinderella motif, the story of a poor girl finding love and success, continues to be repeated in movies such as Pretty Woman, The Princess Diaries, The Princess Diaries II, Ever After, Maid in Manhattan, and Ella Enchanted. In 1962, the lives of both brothers were the subject of the film The Wonderful World of the Brothers Grimm featuring an all star cast, including Laurence Harvey and Karlheinz Böhm in the title roles.		Twentieth-century educators debated the value and influence of teaching stories that include brutality and violence, causing some of the more gruesome details to be sanitized.[48] Dégh writes that some educators believe that children should be shielded from cruelty of any form; that stories with a happy ending are fine to teach, whereas those that are darker, particularly the legends, might pose more harm. On the other hand, some educators and psychologists believe that children easily discern the difference between what is a story and what is not and that the tales continue to have value for children.[53] The publication of Bruno Bettelheim's 1976 The Uses of Enchantment brought a new wave of interest in the stories as children's literature, with an emphasis on the "therapeutic value for children".[54] More popular stories such as "Hansel and Gretel" and "Little Red Riding Hood" have become staples of modern childhood, presented in coloring books, puppet shows, and cartoons. Other stories, however, have been considered too gruesome and have not made a popular transition.[51]		Regardless of the debate, the Grimms' stories have continued to be resilient and popular around the world,[53] although a recent study in England appears to suggest that parents consider the stories to be overly violent and inappropriate for young children, writes Libby Copeland for Slate.[55] Nevertheless, children remain enamored with grim stories as the recent success of The Grimm Series by Adam Gidwitz and The Sisters Grimm by Michael Buckley illustrates.		The university library at the Humboldt University of Berlin is housed in the Jacob and Wilhelm Grimm Center (Jakob-und-Wilhelm-Grimm-Zentrum).[56] Among its collections is a large portion of the Grimm Brothers' private library.[57]		
The University of Wolverhampton is an English university located on four campuses across the West Midlands, Shropshire and Staffordshire.		The city campus is located in Wolverhampton city centre, with a second campus at Walsall and a third in Telford. There is an additional fourth campus in Wolverhampton at the University of Wolverhampton Science Park. The university also operates a Health Education Centre in Burton-upon-Trent for nursing students.		The university has seven academic schools/faculties and several cross-disciplinary research centres and institutes. It has 19,790 students and currently offers over 380 undergraduate and postgraduate courses.[7]		The University is currently second in the UK for graduate employability - 96% of students who graduated from the University of Wolverhampton in 2015 were in work or further study six months after they had left - for universities of its size (with 2,000-3,000 full-time undergraduate graduating students), according to the Destinations of Leavers From Higher Education (DHLE) survey.[8]		In addition, the university was commended with the highest level of commendation by the Quality Assurance Agency in 2015 for the 'enhancement of student learning opportunities',[9] whilst the 2014 Research Excellence Framework (REF) exercise rated all submitted Research Centres as having 'world-leading' elements.[10]						The university's roots lie in the Wolverhampton Tradesmen's and Mechanics' Institute founded in 1827 and the 19th-century growth of the Wolverhampton Free Library (1870) whose evening classes were formalised as the Science, Technical and Commercial School in 1899, and grew into the Wolverhampton and Staffordshire Technical College in 1926. It was renamed Wolverhampton and Staffordshire College of Technology in 1951 and became Wolverhampton College of Technology in 1966 following county boundary changes. Wolverhampton School of Art was founded in 1851, becoming the Municipal School of Art in 1878, and finally Wolverhampton College of Art in 1950. Wolverhampton College of Technology merged with Wolverhampton College of Art in 1969 to form The Polytechnic, Wolverhampton in 1969. The Polytechnic changed its name to Wolverhampton Polytechnic in 1988 and gained university status as the University of Wolverhampton in 1992.[4][11][12][13][14][2][15][3][16][17][18]		The university has four faculties, comprising twenty-two schools and institutes.[19] It has 19,790 students and currently offers over 500 undergraduate and postgraduate courses.[7] The university is noted for its success in encouraging wider participation in higher education.[20]		The roots of the University of Wolverhampton lie in the 19th-century growth of the Wolverhampton Free Library (1870), which developed technical, scientific, commercial and general classes, and the Municipal School of Art, founded in 1878.[4][13][14][16][21]		In 1931 His Royal Highness Prince George laid the foundation stone for the new Wolverhampton and Staffordshire Technical College.		By 1945, the creation of the Music Department allowed the College to capitalize on the growing demand for a variety of subject areas. Enrolment in the first year totalled 135, and by 1950 HM Inspectors stated that "it was unique among technical colleges". The composer Vaughan Williams attended a performance of his Riders to the Sea in early 1950.[22] In 1951 the College's name was changed to the Wolverhampton and Staffordshire College of Technology, and the work of the High School of Commerce was partially transferred to the College. In 1956 the Joint Education Committee of the college noted: "Research is an essential feature of any institution of higher learning. Very good work is being done in applied science, and mechanical engineering is bringing to fruition negotiation with a local firm for sponsored research into problems at heat exchangers".[22] By 1957–58 the student numbers grew to 6,236. This included trainee teachers being enrolled into the College. Parallel developments with Wulfrun College set the foundations for the creation of the Faculty of Education created in 1977.[22]		The first computers also arrived in 1957, the WITCH (Wolverhampton Instrument for Teaching Computing from Harwell). The annual report for 1956–57 records: "Following a visit of a member of staff to Harwell, the college in competition with eight other colleges was offered the gift of an Electronic Digital Computer." A number of local firms donated sums of money to cover the cost of maintenance and operation.[22][23] The WITCH is now considered to be the "oldest original functioning electronic stored program computer in the world"[24] and from September 2009 began restoration at The National Museum of Computing, Bletchley Park.[25]		By 1964, with the further expansion of Higher Education the college began to provide BA degrees with options in English, Geography, History, Music, and Economics among others. By 1965 the college was offering a degree in Computer Technology. The college was renamed Wolverhampton College of Technology following county boundary changes.[15][22][26][27]		In 1969 the College of Technology and the College of Art amalgamated to become Wolverhampton Polytechnic. The formal opening ceremony took place on 14 January 1970. Wolverhampton Polytechnic was operational by the creation of five faculties; Applied Science, Art and Design, Arts, Engineering and Social Sciences. The functional units were operated by committees such as the Academic Board, Faculty Boards, Planning and Standing Committees, Committee of Deans.[17][18][28]		1970 saw the opening of the New School of Art and Design, opened by Sir Charles Wheeler. Mergers with Teacher Training Colleges in Wolverhampton and Dudley in the 1970s added to the expansion of the Polytechnic, with additional growth in 1989 on Walsall Campus when the Polytechnic acquired the Teacher Training College ( West Midlands College of Higher Education ) site.[29]		In 1992 the Polytechnic was granted university status and became the University of Wolverhampton.		The university was further expanded by the construction of the Telford Campus, completed in 1994, which includes in its grounds the 18th Century, Grade II listed Priorslee Hall; the oldest building under the University of Wolverhampton's banner. Telford Campus opened its doors to students from the Business School and the Faculty of Science and Engineering.		1994 also saw Wolverhampton become the first UK university to be awarded the Charter Mark for excellence in customer service.[29]		In 1995 the Wolverhampton Science Park opened (renamed the University of Wolverhampton Science Park in November 2012[30]); a collaboration between the university and the local council, with its main aim being to forge links between local businesses and the university's research departments. The Science Park housed The Creative Industries Centre, The Technology Centre, The Development Centre and other business and technology support services.		Also in 1995, two local nursing colleges – the United Midlands College for Nursing and Midwifery and the Sister Dora School of Nursing – amalgamated to form the School of Nursing and Midwifery at the Walsall campus, formerly West Midlands College of Higher Education.		In 1997 the university was one of the first to establish a virtual learning environment: WOLF (Wolverhampton Online Learning Framework) a system used by students and staff to support learning in most subject areas. It provides online space for tutors to make reference materials, notes, videos and documents related to a subject available. In 2008 an upgraded version "WOLF2" was launched.		Two new learning centres were opened at the Telford and City campuses in 1998. These learning centres were a fusion of traditional libraries with high-tech facilities, aimed at providing a greater range of accessible materials for students. The following year the university opened the Arena Theatre, Wolverhampton on the City campus along with the new SC building in Telford.		2000 saw the launch of a multimillion-pound refurbishment programme.[31] From 2000–2010 £115 million was invested in campus developments. Highlights include the £26 million 'Millennium City' building opening by the then Chancellor of the Exchequer, the Rt Hon Gordon Brown MP in February 2003.[32]		This was followed in 2004 by a teaching building called the 'Technology Centre' (now the Alan Turing building), home of the School of Computing and IT (later to become the School of Mathematics and Computer Science[33]), with in excess of 400 high-specification PCs running the very latest software for multimedia, games development and databases.[34] The same year a £4 million extension to the Harrison Learning Centre was completed.		In October 2005 Professor Caroline Gipps became Vice-Chancellor – the university's first female VC.		In 2006 the City Campus North Administration and Teaching Building was erected, providing space for a 120-seat lecture theatre, 4 elliptical 35-seat learning pods and the bringing together of many administration departments to work all under the one roof. In 2007, a new building at Walsall Campus was established to accommodate over 1,100 students over four floors and providing a combination of specialist and open access IT facilities and office accommodation for the School of Education.		2009 saw the formation and launch of two new Schools: the School of Law, Social Sciences and Communications[35] and the School of Health and Wellbeing, as well as the launch of the research group Centre for Developmental and Applied Research in Education (CeDARE).[36]		The new School of Technology launched on 1 September 2010.[37] In 2011, the university in partnership with Walsall College opened the Black Country University Technical College, one of the first University Technical Colleges in England.[38]		The current Vice-Chancellor, Professor Geoff Layer, joined the university on 1 August 2011. September 2011 saw the opening of the Performance Hub at Walsall Campus; a multimillion-pound teaching, learning, rehearsal and performance space for performing arts.[39]		Plans for a further £45 million investment in City Campus were announced in December 2012, with redevelopments including a new Business School building opposite the Molineux Stadium.[40] In 2013, the university celebrated its 21st anniversary since being granted university status on 17 June 1992.[41]		In 2015, the university announced its biggest ever investment plan, 'Our Vision, Your Opportunity', to generate £250 million of investment by 2020 to enhance the student experience and help to drive economic growth in the region.[42] Key projects include the new Rosalind Franklin Science Centre (which opened to the public in 2014), the completion of the Lord Swraj Paul Building (new home to the University of Wolverhampton Business School), £10 million investment in engineering at Telford Innovation Campus, a new courtyard and catering facilities at City Campus, and the development of the new Springfield Campus, a national centre for excellence for construction and the built environment.[43]		In 2015, Lord Paul, The Chancellor, donated £1m to the University which is the largest donation ever received.[44]		In June 2008 the university gained official Fairtrade status,[45] with Fairtrade products being sold in University food and drink outlets across its campuses. Each year activities take place across the University to mark the annual national Fairtrade fortnight.[46]		Since April 2009 the university has been one of eleven universities participating in the Carbon Trust's fifth HE Carbon Management programme which helps Universities to access and reduce their carbon footprint.[47]		The University of Wolverhampton is located across four campuses across the West Midlands and Shropshire.		A free student and staff bus service operates between each of its campuses and campus towns, running between Wolverhampton city centre, Walsall and Telford.		The university provides one of the largest wireless networks in UK Higher Education, allowing students and staff remote access to the Internet across all its campuses.[48]		City Campus is the main site for the university, and is situated in the heart of Wolverhampton city centre, right opposite Molineux Stadium, home of Wolverhampton Wanderers F.C., and approximately 16 miles (26 km) from Birmingham. Divided into City Campus Wulfruna and City Campus Molineux, it is home to several academic schools/faculties; administration departments; the Students' Union and student support facilities. In addition, over 1000 students live in three separate Halls of Residence on this campus: North Road, Lomas Street and Randall Lines.		The £26 million Millennium City Building – opened in 2003 by the then Chancellor of the Excheqeur, the Rt Hon Gordon Brown – provides over 10,000 square metres of teaching space, audio-visual equipment in all rooms, 300-seat lecture theatre, exhibition gallery, campus restaurant, and an informal Social Learning Space.		The Alan Turing Building (formerly the Technology Centre) on City Campus contains an open plan workspace with over 400 PCs, as well as prototyping equipment and industry-standard software packages for 3D modelling and product design. The Centre includes two TV studios with remote-controlled cameras and a full lighting rig, plus a radio studio with digital editing suites.		The Harrison Learning Centre has traditional and electronic-based library facilities over four floors. It provides electronic auto-service and online cataloguing facilities, and academic librarians manage, monitor and update the available information.		The Wolverhampton School of Art, established in 1851, is housed in the George Wallis building, which was formally opened by Sir Charles Wheeler in 1970 in its former guise as the MK building. It provides specialist equipment, facilities and expertise for students studying one of the various art and design specialisms available to study at the School of Art.[49]		MX building, opened by Sarah Brown, wife of the then Prime Minister Gordon Brown, was opened in 2006 and brought together many administration departments to work all under the one roof.		Based on the City Campus in Wolverhampton, the Arena Theatre contains an auditorium seating 150, a studio seating 100 and a seminar room for up to 50 people. Its programme includes professional companies, celebrating drama, dance and music, as well as showcasing work by local schools, colleges, students, amateur companies and community events.[50][51]		The purpose-built Telford Innovation Campus opened in 1994. 18 miles (29 km) from Wolverhampton and 26 miles (42 km) from Birmingham, the campus is on a greenfield site in the grounds of Priorslee Hall – a grade-II listed 18th Century redbrick mansion.		The campus houses facilities for engineering, built environment, business, computing and social work. Halls of residence for just under 500 students are located on campus together with a Learning Centre, a Students' Union bar, a floodlit tennis and basketball court, and		The campus is home to the e-Innovation Centre which provides startup companies and small and medium enterprises with business accommodation and funded support from a team of IT consultants, giving them access to the university's IT facilities, expertise and resources. It has hi-tech meeting rooms, social meeting areas, "hot-desking" provision, fully furnished offices, "incubation" units, and "grow-on" space for businesses who need to expand.[52]		The Walsall campus is based a mile from Walsall town centre and near both junction 7 and junction 9 of the M6 motorway. Students studying sport, music, dance, education, health, events management, tourism and hospitality are based here. Opened in 2005, the Student Village provides over 300 individual en-suite study rooms.		A multimillion-pound sports centre houses a 12-court, multi-activity sports hall, a six-lane floodlit athletics track, an all-weather floodlit pitch, a dance studio and swimming pool. There are also fully equipped physiology, psychology and biomechanics.		A new teaching building contains a flexible IT teaching and learning area, three advanced lecture theatres, and specialist teaching rooms, ranging from primary science laboratories to specialist design and technology teaching facilities. It's also home to the Institute for Learning Enhancement which leads innovative practice in learning and teaching for the university.		Refurbished facilities at Boundary House allow trainee nurses and other healthcare professionals to follow the academic part of their course.		The University of Wolverhampton's Walsall Campus Sports Centre was named as an official training base for the 2012 Olympics. It is included in the Guide for National Olympic Committees (NOCs) for the Olympic sports of Basketball, Judo and Taekwondo. The Guide will be used by countries organising their training programmes in the run-up to the Olympics.[53]		The Performance Hub houses state-of-the-art performing arts facilities and opened in September 2011.		Walsall Campus was named as the location of a new judo Centre of Excellence in England by the British Judo Association. The Centre became operational in September 2013.[54]		The University of Wolverhampton Science Park is home to around 80 innovative businesses working in science, technology, knowledge-based and creative sectors. As well as business support services, it offers office accommodation and workshop/laboratory areas for companies, as well as conference and meeting facilities.		The Science Park was formed in 1993 as a joint venture between the University of Wolverhampton and Wolverhampton City Council.		The School of Health and Wellbeing has a presence at Burton Health Education Centre, which specialises in nursing. The campus has a Learning Centre[55] (open 5 days a week) which provides a wide range of books, leaflets, and electronic and paper journals for staff, students and external members. There is also a common room and IT facilities.		The campus is located on the Outwoods site, opposite Queens Hospital on Belvedere Road. The site can be accessed from Burton upon Trent town centre on the Arriva 3, 3A and 3B bus routes.		The university's Arms show supporters on either side of the shield. These represent Lady Wulfrun often regarded as the founder of what is now the City of Wolverhampton in CE circa 980 (a settlement described as Wulfruna's Heantun in the Saxon Chronicles) and Thomas Telford the renowned Engineer who, in 1787 became surveyor of public works for Shropshire and whose works and structures can be seen across the region and the nation and after whom the Shropshire New Town was named.[1]		The motto of the university is "Innovation and Opportunity".[1]		The University of Wolverhampton is led by the Board of Governors and Offices of the Vice-Chancellor. It has four faculties,[56] 22 academic schools/institutes, 14 research institutes and centres,[57] and a range of other departments.[58]		The Honorary position of Chancellor is the figurehead of the University and presides over the University's ceremonial occasions and acts as its Ambassador. The role of Chancellor was created following the grant of University title in 1992.[59]		The Board of Governors is responsible for the oversight of the University's activities and for the effective and efficient use of resources and the safeguarding of assets. It has 18 members including nine independent members and a representative of the student body.[60]		The Offices of the Vice-Chancellor has responsibility for the overall management of the university. The Offices of the Vice-Chancellor are led by the Vice-Chancellor assisted by three Deputy Vice-Chancellors, the University Registrar and Secretary, and Finance Director. The Offices of the Vice-Chancellor are also responsible for implementing corporate strategy and operational policy decisions from Academic Board and the Board of Governors.[61]		Each Academic School/Faculty is managed by a Dean[62] aided by Associate Deans. The academic provision in the Schools is supported by support departments each managed by a head or director.		Wolverhampton's current Chancellor is The Rt Hon Lord Paul of Marylebone, PC,[59] and its current Vice-Chancellor is Professor Geoff Layer, who took up the position in 2011.[63]		According to the Times Higher Education's league tables for the RAE of 2008, Wolverhampton was ranked at equal 93rd from 132 institutions for research. Wolverhampton was the joint fourth best university in the UK for linguistics and is the highest-rated new university in that subject area.[64] The Statistical Cybermetrics Research Group was joint second in the country for library and information management.[64] Also in 2008, a University of Wolverhampton academic, Mike 'Rodney' Thelwall, was ranked number one in the world in a list of leading researchers in the field of informetrics.[65] The Higher Education Funding Council for England (HEFCE) announced a 1,290% increase in funding allocation for Wolverhampton's Quality Research (QR). The QR allocation of £1.905 million for Wolverhampton was the highest amount for a new university in the West Midlands.[66]		The university achieved its best ever results in the most recent Research Excellence Framework (REF) in 2014, with all Research Centres that submitted rated as having 'world-leading' elements.[10]		The mathematicians and information scientists in the Statistical Cybermetrics Research Group were rated world number 1 for research quality in the 2017 Shanghai Rankings for Library and Information Science [67].		The university is noted for its success in encouraging wider participation in higher education.[20] A third of the places are filled by mature students.[68]		Strongly regional in outlook, the university draws two thirds of its students from the West Midlands,[20] although there are also 2500 overseas students studying at the university which has offices in China, India, Poland, Malaysia and Nigeria, with affiliations in Singapore.[69][70]		Between 2005 and 2009 five staff were awarded National Teaching Fellowships.[71][72][73]		The University of Wolverhampton won two Lord Stafford Awards in 2007, recognising its excellence in innovative work with businesses. Rachel Westwood of SkinScientists Ltd won the "Entrepreneurial Spirit Award" for her innovative brand of "cosmeceuticals" especially formulated for men. Robert Harris, Principal Lecturer Corporate Programmes, University of Wolverhampton Business School, won the "West Midlands Knowledge Transfer Champion Award" for his contribution to knowledge transfer activities between the university and companies in the West Midlands.[74]		In May 2008 the university was awarded an unprecedented seven Knowledge Transfer Partnerships, securing its top position in the West Midlands. In September 2009 it was awarded £24.3 million for knowledge transfer, bringing it to 2nd place nationally for the number of KTPs it runs. The university will lead a consortium of all 12 of the universities in its region to increase the number of partnerships from 70 to 210 over the next three years.[75]		In April 2009, the Quality Assurance Agency (QAA) institutional audit found that confidence can reasonably be placed in the soundness of the institution's present and likely future management of both the academic standards and the quality of the learning opportunities available to students.[76][77] Following this, the university was commended with the highest level of commendation by the Quality Assurance Agency in 2015 for the 'enhancement of student learning opportunities'.[78]		The results of the ninth National Student Survey in 2013 revealed an overall student satisfaction rate at Wolverhampton of 83%, compared to 80% in 2012. Satisfaction with the learning resources (which includes IT and library facilities) also went up three per cent, with 88% of students saying they were satisfied. In addition, 83% of students reported that they were satisfied with the teaching on their course.[79][80] An improved satisfaction rate was reported when the National Student Survey 2016 found that 84% of students at the University of Wolverhampton were satisfied overall with their course.[81]		The university has pursued a policy of non-participation in the league table rankings produced by British newspapers, such that rankings which would compare its performance to that of other British universities are unavailable. The university takes the view that league tables disadvantage universities such as Wolverhampton as they are constructed using a methodology that does not accurately reflect the positive impact on the communities they serve or represent a fair picture of their strengths.[82][83]		In June 2013, a university team won a Times Higher Education Leadership and Management Award (THELMA) in the category of Knowledge Exchange/Transfer Initiative of the Year for its "one-stop shop" approach to promoting services to businesses.[84]		In May 2016, the university was awarded 'Business of the Year' at the Express & Star Business Awards, where its contribution to the region's economy was hailed as 'truly outstanding'.[85]		In 1984/85, the Faculty of Art and Design was closed without warning over the Christmas period and remained closed for many months whilst contractors stripped asbestos from the building. Students were given no warning of the closure and many lost hundreds of pounds worth of equipment stored in lockers in the building. The Local Authority shifted blame to the contractor, and vice versa. The student union engaged solicitors on the students' behalf but no compensation was ever awarded.		In a Times column dated 29 February 1988, the writer Bernard Levin cited the then Wolverhampton Polytechnic as an example of how student unions were allegedly dominated by the political hard left.		In 2002, the university paid out £30,000 in an out-of-court settlement to Mike Austen, a dissatisfied law student, who sued on the grounds of multiple misrepresentations and multiple breaches of the student contract.[86]		In July 2006, in a swimming pool at the university's Walsall campus, a disabled rugby player drowned whilst not being supervised properly by lifeguards and managers, an inquest jury ruled.[87]		In 1998, Dr. Ian Connell left the university after being found guilty of academic misconduct. A few weeks later he committed suicide, apparently depressed by his situation.[88]		In 2001, the university was investigated by the Health and Safety Executive after local doctors reported an unusually large number of staff seeking their help for stress and bullying.[89]		In 2009 the university Executive announced that the university was in financial difficulties, needing to make savings of £8 million.[90] This followed reports in the media that it had understated student non-completion rates to HEFCE.[91] The University announced it was taking steps to reduce expenditure on staff pay and launched a voluntary redundancy exercise on 1 October 2009.[90] This concluded with the loss of 150 posts through voluntary redundancy.		In 2015, despite 2014 REF successes, the Vice Chancellor announced that four areas of research would be cut back and some professors selected for compulsory redundancy while all professors would be subject to more rigorous annual appraisal which would lead to demotion to senior lecturer in three years if they failed to sustain their target levels of outputs, income and PhD students.[92]		The Union is run by an Executive Committee, which consists of a President and three full-time Vice Presidents (all four are also trustees of the organisation) and up to nine part-time, Non-Sabbatical Officers. These posts are elected annually by a cross-campus ballot. The Union organises one-off events such as Freshers' Fayre.		In partnership with the university the Union runs the Student Voice to ensure that students have the opportunity to express their views and participate in decisions that affect them.[93]		The Union runs an Advice and Support Centre (ASC) which offers advice on university life and on specific issues such as housing, finance, international and academic concerns. The ASC is affiliated to, among others, Citizens Advice.[94]		There are over 50 societies which are run by students.		The Athletic Union is the department of the Students' Union that represents, co-ordinates, administers and promotes sporting and recreational activities for students. It includes over twenty sports clubs ranging from football, rugby and hockey to martial arts, squash and volleyball. It is a member of the British Universities & Colleges Sport.[95]		The university offers over 1600 places in Halls of Residence across three campuses, including over 1000 rooms with en-suite facilities.[96]		Accommodation at City, Telford and Walsall campuses have wireless access in the bedrooms and communal areas. Across all campuses facilities for students with any impairments are offered, whether this is hearing, sight, mobility or any other. These include wheelchair adapted rooms and facilities for deaf/hard of hearing students.		Flats are available to rent for couples or students coming to university with their partner or spouse. The university has many links with local accredited landlords that have properties around the campuses. All landlords are required to be a member of the Midlands Landlord Accreditation Scheme (MLAS).		There are numerous opportunities for students to get involved with volunteering and work with the local community. These are co-ordinated by Active Volunteers, the university's volunteering agency. All student volunteers are eligible to register for the University of Wolverhampton Volunteering Certificate. Registered students are then able to be nominated for the university's Volunteer of the Year Award which recognises outstanding contribution to volunteering.[97]		The Students' Union also offers opportunities including the Volunteer Squad and BestMates.[98]		96% of students who graduated from the University of Wolverhampton in 2015 were in work or further study six months after they had left[99] – positioning the University as second in the UK for universities with 2,000-3,000 graduating students[100] (full-time undergraduate), according to the Destinations of Leavers From Higher Education (DHLE) survey.		Students also have a variety of opportunities to gain work experience while they are studying and on graduation. These include graduate placements such as Knowledge Transfer Partnerships (KTP). The university is leading on the £5.2M national Student Placements for Entrepreneurs in Education West Midlands (SPEED WM) project[101] involving 13 UK universities, to help students create their own businesses whilst they are studying. 'Erasmus for Young Entrepreneurs' is aimed at helping new entrepreneurs to acquire relevant skills for managing a small or medium-sized enterprise by spending time working in another EU country with an experienced entrepreneur in his/her company. And SP/ARK provides facilities, accommodation, training and mentoring for business start-ups and freelancers in new media and design.		In 2013 the university won a Times Higher Education Leadership and Management Award for Knowledge Exchange/Transfer Initiative of the Year.[102]		Notable alumni in the field of government and politics include: Steven Linares, MP in the Gibraltar Parliament and Minister for Sport, Culture, Heritage & Youth in the Government of Gibraltar; Nando Bodha, former Minister of Tourism & Leisure and former Minister of Agriculture for Mauritius; Juhar Mahiruddin, Governor of Sabah, Malaysia, and Chancellor of University Malaysia Sabah; Michael John Foster, former Labour MP and Parliamentary Under-Secretary of State at the Department for International Development; David Wright, former Labour MP; Chris Heaton-Harris, Conservative MP; Brian Jenkins, former Labour MP; Jenny Jones, former Labour MP; Ken Purchase, former Labour MP; Chauhdry Abdul Rashid, former Lord Mayor of Birmingham and former Chancellor of Birmingham City University; Bill Etheridge, UKIP MEP; and Yatindra Nath Varma, former Attorney General of the Government of Mauritius.[103]		Other notable alumni include: Sir Terence Beckett, former director-general of the Confederation of British Industry; Sir Charles Wheeler, sculptor and President of the Royal Academy; Suzi Perry, television presenter and journalist; Maggie Gee, novelist; Trevor Beattie; advertising executive; Peter Bebb, special effect artist;[104] Vernie Bennett, singer, formerly of Eternal; Scott Boswell, former professional cricketer; David Carruthers, former online gambling executive; Major Peter Cottrell, soldier, author and military historian; Matt Hayes; television angler; Mil Millington, author; Magnus Mills, author; Mark O'Shea, zoologist and television presenter; Cornelia Parker, artist/sculptor; Julian Peedle-Calloo, television presenter; Robert Priseman, artist; Anne Schwegmann-Fielding, artist; Michael Salu, graphic artist and creative director; Ged Simmons, television actor; Gillian Small, University Dean for Research, City University of New York; Clare Teal, jazz singer and broadcaster; Andy Thompson, footballer; Patrick Trollope, editor of UK's first online-only regional newspaper; Annemarie Wright, artist; Ben Stewart, head of media at Greenpeace; and His Honour Judge Jonathan Gosling, Circuit Judge.[105]		Notable academics include the broadcaster and journalist Jeff Randall; sculptor Sir Anish Kapoor;[106] artist Roy Ascott; and the author Howard Jacobson. Jacobson's experience formed the basis of his novel Coming from Behind, set at a "fictional" polytechnic in the Midlands.[107][108]		Coordinates: 52°35′14″N 2°07′38″W﻿ / ﻿52.58722°N 2.12722°W﻿ / 52.58722; -2.12722		Mike Haynes and Lib Meakin, Opening Doors in the Heartlands: A History of the University of Wolverhampton, Wolverhampton: University of Wolverhampton, 2013, 184 pages ISBN 978-0-9576636-0-2.		
Stand-up comedy is a comic style in which a comedian performs in front of a live audience, usually speaking directly to them. The performer is commonly known as a comic, stand-up comic, stand-up comedian, or simply a stand-up.[1] In stand-up comedy, the comedian usually recites a grouping of humorous stories, jokes and one-liners typically called a monologue, routine, or act. Some stand-up comedians use props, music, or magic tricks to "enhance" their acts. Stand-up comedy is often performed in comedy clubs, bars and pubs, nightclubs, neo-burlesques, colleges and theatres. Outside of live performance, stand-up is often distributed commercially via television, DVD, CD and the internet.[1][2]						In stand-up comedy, the feedback of the audience is instant and crucial for the comedian's act. Audiences expect a stand-up comic to provide a steady stream of laughs, and a performer is always under pressure to deliver. Will Ferrell has called stand-up comedy "hard, lonely and vicious".[2]		A stand-up comedy show may involve only one comedian, or feature a "headline" or a "showcase" format.[citation needed] A headline format typically features an opening act known as a host, compère (UK), or master of ceremonies (MC), who usually warms up the crowd, interacts with the audience members, makes announcements, and introduces the other performers. This is followed by one or two "middle" or "featured" acts, who perform 15- to 20-minute sets, followed by a headliner who performs for longer. The "showcase" format consists of several acts who perform for roughly equal lengths of time, typical in smaller clubs such as the Comedy Cellar, or Jongleurs, or at large events where the billing of several names allows for a larger venue than the individual comedians could draw. A showcase format may still feature an MC.		Many smaller venues hold "open mic" events, where anyone can take the stage and perform for the audience, offering a way for amateur performers to hone their craft and possibly break into the profession, or for established professionals to work on their material.		"Bringer shows" are another opportunity for amateur performers. The performer must bring a specified number of paying guests in order to get stage time. The guests usually have to pay a cover charge and there is often a minimum number of drinks that must be ordered. These shows usually have a "showcase" format. This type of show gives comedians better exposure than open mics because there is usually better audience turnout and industry professionals sometimes go to watch. Different comedy clubs have different requirements for their bringer shows. Gotham Comedy Club in New York City, for example, usually has ten-person bringers, while Broadway Comedy Club in New York City usually has six-person bringers.		As the name implies, "stand-up" comedians usually perform their material while standing, though this is not mandatory.[citation needed]		Stand-up comedy has its origin in classic Parrhesia in 400 BC used for cynics and epicureans in order to tell the reality without censorship.[3]		Stand-up comedy in the United Kingdom began in the music halls of the 18th and 19th centuries. Notable performers who rose through the 20th century music hall circuit were Morecambe and Wise, Arthur Askey, Ken Dodd and Max Miller, who was considered to be the quintessential music-hall comedian. The heavy censorship regime of the Lord Chamberlain's Office required all comedians to submit their acts for censorship. The act would be returned with unacceptable sections underlined in blue pencil (possibly giving rise to the term "blue" for a comedian whose act is considered bawdy or smutty). The comedian was then obliged not to deviate from the act in its edited form.[1]		The rise of the post-war comedians coincided with the rise of television and radio, and the traditional music hall circuit suffered greatly as a result. Whereas a music hall performer could work for years using just one act, television exposure created a constant demand for new material, although this may have also been responsible for the cessation of theatrical censorship in 1968.[citation needed]		By the 1970s, music hall entertainment was virtually dead. Alternative circuits had evolved, such as working men's clubs.[1] Some of the more successful comedians on the working men's club circuit—including Bernard Manning, Bobby Thompson, Frank Carson and Stan Boardman — eventually made their way to television via such shows as The Wheeltappers and Shunters Social Club. The "alternative" comedy scene also began to evolve. Some of the earliest successes came from folk clubs, where performers such as Billy Connolly, Mike Harding and Jasper Carrott started as relatively straight musical acts whose between-song banter developed into complete comedy routines. The 1960s had also seen the satire boom, including the creation of the club, the Establishment, which, amongst other things, gave British audiences their first taste of extreme American stand-up comedy from Lenny Bruce.[4] Victoria Wood launched her stand-up career in the early 1980s, which included observational conversation mixed with comedy songs. Wood was to become one of the country's most successful comedians, in 2001 selling out the Royal Albert Hall for 15 nights in a row.		In 1979, the first American-style stand-up comedy club, the Comedy Store was opened in London by Peter Rosengard, where many alternative comedy stars of the 1980s, such as Dawn French and Jennifer Saunders, Alexei Sayle, Craig Ferguson, Rik Mayall and Ade Edmondson began their careers.[5] The stand-up comedy circuit rapidly expanded from London across the UK. The present British stand-up comedy circuit arose from the 'alternative' comedy revolution of the 1980s, with political and observational humor being the prominent styles to flourish. In 1983, young drama teacher Maria Kempinska created Jongleurs Comedy Clubs, now the largest stand-up comedy chain in Europe. Stand up comedy is believed to have been performed originally as a one-man show. Lately, this type of show started to involve a group of young comedians, especially in Europe.		Stand-up comedy in the United States has its roots in various traditions of popular entertainment of the late 19th century, including vaudeville, English music hall, burlesque or early variety shows; minstrel shows, humorist monologues by personalities such as Mark Twain, and circus clown antics. With the turn of the century and ubiquitousness of urban and industrial living, the structure, pacing and timing, and material of American humor began to change. Comedians of this era often depended on fast-paced joke delivery, slapstick, outrageous or lewd innuendo, and donned an ethnic persona—African, Scottish, German, Jewish—and built a routine based on popular stereotypes. Jokes were generally broad and material was widely shared, or in some cases, stolen. Industrialized American audiences sought entertainment as a way to escape and confront city living.		The founders of modern American stand-up comedy include Moms Mabley, Jack Benny, Bob Hope, George Burns, Fred Allen, Milton Berle and Frank Fay all of whom came from vaudeville or the Chitlin' Circuit.[6] They spoke directly to the audience as themselves, in front of the curtain, known as performing "in one". Frank Fay gained acclaim as a "master of ceremonies" at New York's Palace Theater. Vaudevillian Charlie Case (also spelled Charley Case) is often credited with the first form of stand-up comedy; performing humorous monologues without props or costumes. This had not been done before during a vaudeville show.		Nightclubs and resorts became the new breeding ground for stand-ups. Acts such as Alan King, Danny Thomas, Martin and Lewis, Don Rickles, Joan Rivers and Jack E. Leonard flourished in these new arenas.		In the 1950s and into the 1960s, stand-ups such as Mort Sahl began developing their acts in small folk clubs like San Francisco's hungry i (owned by impresario Enrico Banducci and origin of the ubiquitous "brick wall" behind comedians)[7] or New York's Bitter End. These comedians added an element of social satire and expanded both the language and boundaries of stand-up, venturing into politics, race relations, and sexual humor. Lenny Bruce became known as 'the' obscene comic when he used language that usually led to his arrest.[8] After Lenny Bruce, arrests for obscene language on stage nearly disappeared until George Carlin was arrested on 21 July 1972 at Milwaukee's Summerfest after performing the routine "Seven Words You Can Never Say on Television"[9] (the case against Carlin was eventually dismissed).		Other notable comics from this era include Woody Allen, Shelley Berman, Phyllis Diller, and Bob Newhart. Some Black American comedians such as Redd Foxx, George Kirby, Bill Cosby, and Dick Gregory began to cross over to white audiences during this time.		In the 1970s, several entertainers became major stars based on stand-up comedy performances. Richard Pryor and George Carlin followed Lenny Bruce's acerbic style to become icons. Stand-up expanded from clubs, resorts, and coffee houses into major concerts in sports arenas and amphitheaters. Steve Martin and Bill Cosby had levels of success with gentler comic routines. The older style of stand-up comedy (no social satire) was kept alive by Rodney Dangerfield and Buddy Hackett, who enjoyed revived careers late in life. Television programs such as Saturday Night Live and The Tonight Show helped publicize the careers of other stand-up comedians, including Janeane Garofalo, Bill Maher and Jay Leno.		From the 1970s to the '90s, different styles of comedy began to emerge, from the madcap stylings of Robin Williams, to the odd observations of Jerry Seinfeld and Ellen DeGeneres, the ironic musings of Steven Wright, to the mimicry of Whoopi Goldberg, and Eddie Murphy. These comedians would serve to influence the next generation of comedians, including Chris Rock, Martin Lawrence, Dave Chappelle, Dave Attell, Patrice O'Neal, Greg Giraldo, Doug Benson, Bill Hicks, Margaret Cho, Bill Burr, David Cross, Louis C.K., Mitch Hedberg, Maria Bamford, Jim Norton, Dave Foley, Todd Glass, Kevin Hart, Sammy Obeid, Joe Rogan, Doug Stanhope, and Sarah Silverman.		Stand-up comedy in India is a young artform. Even though the history of live comedy performances in India traces its early roots back to 1980s, for a long time stand-up comedians were only given supporting/filler acts in various performances (dance or music).		In 1986, India's Johnny Lever performed in a charity show called "Hope 86", in front of the whole Hindi film industry as a filler and was loved by audience. His talent was recognized.[10][11] He started to perform in musical shows (orchestras) and after earning fame, joined the group of Kalyanji-Anandji, a legendary music direction duo.[citation needed] Even before joining Hindustan Lever, he was giving stage performances. As because of his growing absenteeisms and since he was earning well from stage shows, he quit HLL in the year 1981. He did a lot of shows and world tours with them, one of his first big tours being with Amitabh Bachchan in 1982.		It was not until 2005, when the TV show The Great Indian Laughter Challenge garnered huge popularity and stand-up comedy in itself started getting recognised. Thus, a lot more comedians became popular and started performing various live and TV shows. The demand for comedy content continues to increase. Some popular comedians around 2005-2008 include Raju Srivastav, Kapil Sharma, Sunil Pal etc. Most of them performed their acts in Hindi.		Raju Srivastav first appeared on the comedy talent show The Great Indian Laughter Challenge. He finished as second runner-up and then took part in the spin-off, The Great Indian Laughter Challenge — Champions, in which he won the title of "The King of Comedy".[12] Srivastava was a participant on season 3 of Bigg Boss. He has participated in the comedy show Comedy Ka Maha Muqabla.[13]		Kapil Sharma is ranked no. 3 at the most admired Indian personality list by The Economic Times in 2015.[14] Currently he is hosting the most popular Indian comedy show "The Kapil Sharma Show" after "Comedy Nights with Kapil".[15] Sharma had been working in the comedy show Hasde Hasande Raho on MH One, until he got his first break in The Great Indian Laughter Challenge, one of the nine reality television shows he has won. He became the winner of the show in 2007 for which he won 10 lakhs as prize money.[15]		Sharma participated in Sony Entertainment Television’s Comedy Circus.[16] He became the winner of all six seasons of "Comedy Circus" he participated in.[17] He has hosted dance reality show Jhalak Dikhhla Jaa Season 6[18] and also hosted comedy show Chhote Miyan.[19][20] Sharma also participated in the show Ustaadon Ka Ustaad.		Around the 2008-2009, two other popular comedians Papa CJ and Vir Das returned to India and started making their marks on Indian comedy scene. Both of them were exposed to UK and US comedy routines and they performed mostly in English. At the same time, a few more youngsters got inspired and started taking plunge into stand-up comedy.		Since 2011, the stand-up comedy has been getting substantial appreciation. The Comedy Store from London opened an outlet in Mumbai's Palladium Mall where people would regularly enjoy comedians from UK. The Comedy Story also supported local comedians and helped them grow. This outlet eventually become Canvas Laugh Club in Mumbai.		Around 2011, people started organizing different comedy open mic events in Mumbai, Delhi (and Gurgaon), Bangalore. All of this happened in association with growth of a counterculture in Indian cities which catered to the appetite of younger generations for live events for comedy, poetry, storytelling, and music. Various stand up events were covered by popular news channels such NDTV / Aajtak etc. and were appreciated by millions of viewers.		As a result of these developments, plus the increasing penetration of YouTube (along with Internet), Indian stand up comedy started reaching further masses. While the established comedians such as Vir Das, Papa CJ were independently growing through various corporate / international performances, other comedians such Vipul Goyal, Biswa Kalyan Rath, Kenny Sebastian, Kanan Gill grew popular through YouTube videos. 2015 also saw the rise of Zakir Khan as an extremely likable popular comedian in Indian stand-up scene.		The industry, still in its early stages, now sees a lot more influx of aspiring comedians as it transforms the ecosystem around it.		Umer Shareef was born in Liaquatabad, Karachi[when?] as Mohammad Umer. (He changed his name to Omer Sharif when he joined the theater). He started his career in entertainment in 1974, when at age 14 he became a stage performer in Karachi.		Early on, Sharif worked as a background musician with a group of friends, playing at local parties and functions. Sharif became one of the best-known stage performers in Pakistan after his extremely popular 1989 comedy stage plays Bakra Qistoon Pay and Buddha Ghar Pe Hai. In both he starred with Moin Akhter, another well-known Pakistani actor. Sharif was one of the first actors who started to record his shows for video rentals, which played a major part in his success[citation needed]. Yes Sir Eid, No Sir Eid and Bakra Qistoon Pay were the first two-stage shows to come out on video[where?], respectively.		Sharif attained considerable fame and is sometimes called[by whom?] "the King of Comedy" in South Asia.[citation needed] His videos are sold at stores across India.		Sharif was a guest judge for one of the episodes of the Indian comedy show The Great Indian Laughter Challenge, alongside Navjot Singh Siddhu and Shekhar Suman. Sharif also hosts The Sharif Show, where he interviews film and television actors, entertainers, musicians, and politicians. He has also served as Master of Ceremonies for local and overseas events[examples needed].[21]		For the 50-year anniversary of Pakistan's independence, Sharif performed a play called Umer Sharif Haazir Ho. In the play, a representative from every occupation was called into court and asked what they had done for Pakistan in the past 50 years. The Lawyer's Association stated a case against Sharif as a result.[22]		Stand-up comedy is the focus of four major international festivals: the Edinburgh Festival Fringe in Edinburgh, Scotland; Just for Laughs in Montreal and Toronto, Canada; HBO's U.S. Comedy Arts Festival in Aspen, CO, and the Melbourne International Comedy Festival in Melbourne, Australia. A number of other festivals operate around the world, including The Comedy Festival in Las Vegas, the Vancouver Comedy Festival, the New York Comedy Festival, the Boston Comedy and Film Festival, the New York Underground Film Festival, the Sydney Comedy Festival, and the Cat Laughs Comedy Festival in Kilkenny, Ireland. Radio hosts Opie and Anthony also produce a comedy tour called Opie and Anthony's Traveling Virus Comedy Tour, featuring their own co-host, Jim Norton as well as several other stand-up comedians regularly featured on their radio show. There is also a festival in Hong Kong called the HK International Comedy Festival.		The festival format has proven quite successful at attracting attention to the art of stand-up, and is often used as a scouting and proving ground by industry professionals seeking new comedy talent. In the US, performances at colleges are an important market for stand-up comedians, with many tour bookings made at the annual convention of the National Association for Campus Activities (NACA).[23] However, in the mid-2010s performers Chris Rock and Jerry Seinfeld as well as journalists reported college campuses as having become a repressive environment for stand-up comedy, with expectations of political correctness precluding much material that is successful in more open venues.[23]		There are comedy schools that work with new comics to workshop material. Comics work to overcome stage fright and better their writing skills by helping their classmates improve their sets.[24] Some schools, like Manhattan Comedy School, offer improvisation classes for comics so that they are more comfortable with straying from written material. Improv also helps comics think more quickly when dealing with hecklers. Hecklers are people who interrupt sets, usually with negative comments or gibes. Improv is also key when doing crowd work, which is when comics interact directly with the audience.		Many of the earliest vaudeville-era stand-ups gained their greater recognition on radio. They often opened their programs with topical monologues, characterized by ad-libs and discussions about anything from the latest films to a missed birthday. Each program tended to be divided into the opening monologue, musical number, followed by a skit or story routine. A "feud" between Fred Allen and Jack Benny was used as comic material for nearly a decade.		HBO presented comedians uncensored for the first time, beginning with Robert Klein in 1975, and was instrumental in reaching larger audiences. George Carlin was a perennial favorite, who appeared in 14 HBO comedy specials.		Continuing that tradition, most modern stand-up comedians use television or motion pictures to reach a level of success and recognition unattainable in the comedy-club circuit alone.		Since the mid-2000s, online video-sharing sites such as YouTube have also provided a venue for stand-up comedians, and many comedians' performances can be viewed online.[25]		{https://www.youtube.com/channel/UCogq1DrdtA8OgZ2mw9Sb_DA |title: Stand-Up Comedy}		
Essex girl, as a pejorative stereotype in the United Kingdom, applies to a female viewed as promiscuous and unintelligent, characteristics jocularly attributed to women from Essex. It is applied widely throughout the country and has gained popularity over time, dating from the 1980s and 1990s.[1]						The stereotypical image formed as a variation of the dumb blonde/bimbo persona, with references to the Estuary English accent, white stiletto heels, mini skirt, silicone-augmented breasts, peroxide blonde hair, over-indulgent use of fake tan (lending an orange appearance), promiscuity, loud verbal vulgarity, and socializing at downmarket nightclubs.		Time magazine recorded:		In the typology of the British, there is a special place reserved for Essex Girl, a lady from London's eastern suburbs who dresses in white strappy sandals and suntan oil, streaks her hair blond, has a command of Spanish that runs only to the word Ibiza, and perfects an air of tarty prettiness. Victoria Beckham – Posh Spice, as she was – is the acknowledged queen of that realm ...[2]		Essex girl jokes are primarily variations of dumb blonde jokes, and often sexually explicit. ("How does an Essex girl turn the light on after sex? — She opens the car door.")[1]		In 2004, Bob Russell, Liberal Democrat MP for Colchester in Essex, appealed for debate in the House of Commons on the issue, encouraging a boycott of The People tabloid, which has printed several derogatory references to girls from Essex.[3]		The Essex Women’s Advisory Group was set up in 2010[4] to combat the negative stereotyping of girls living in Essex by supporting Essex-based women's charities helping those in need, as well as by funding projects that promote women and girl's learning and success in science, technology, the arts, sports and business. The charitable fund is administered by the Essex Community Foundation.		On 6 October 2016, Juliet Thomas and Natasha Sawkins of The Mother Hub launched a campaign on social media to draw attention to the negative definition of Essex Girl in the Oxford English Dictionary and Collins Dictionary.[5] Their main goal was to raise awareness and to open a dialogue around the derogatory "Essex girl" stereotype. Their campaign centred around changing the definition of "Essex girl" to "a girl from or living in Essex" by encouraging women to use the hashtag #IAmAnEssexGirl and included a petition to change or remove the dictionary definitions. The campaign reached the national press.[6][7][8]		The Essex Girl was referenced in 2017 in regard to a misogynistic "debate" on Twitter about the ethnic diversity of Roman Britain, specifically noting "the wonderful couple from South Shields, Barates and Queenie (‘Regina’), he from Palmyra, she an Essex girl. There is no doubt about that." [9][10]		
The parable of the blind men and an elephant originated in ancient Indian subcontinent, from where it has widely diffused. It is a story of a group of blind men who have never come across an elephant before, learn and conceptualize what the elephant is like by touching it. Each blind man feels a different part of the elephant body, but only one part, such as the side or the tusk. They then describe the elephant based on their partial experience and their descriptions are in complete disagreement on what an elephant is. In some versions they come to suspect that the other person is dishonest and they come to blows. The moral of the parable is that humans have a tendency to project their partial experiences as the whole truth, ignore other people's partial experiences, and one should consider that one may be partially right and may have partial information.[1][2]		The earliest mentions of this premise occurs in the Rigveda,[3] and a complete version of the story is traceable to the Buddhist text Udana 6.4, dated to about mid 1st millennium BCE. According to John Ireland, the parable is likely older than the Buddhist text.[4]		An alternate version of the parable describes normal men, experiencing a large statue on a dark night, or with blindfold masking their eyes experiencing a large object by feeling it. They then describe what it is they experienced. In its various versions, it is a parable that has crossed between many religious traditions and is part of Buddhist, Hindu and Jain texts of 1st millennium CE or before.[3][4] The story also appears in 2nd millennium Sufi and Bahá’í lore. The tale later became well known in Europe, with 19th century American poet John Godfrey Saxe creating his own version as a poem.[5] The story has been published in many books for adults and children, and interpreted in a variety of ways.						The earliest versions of the parable of blind men and elephant is found in Buddhist, Hindu and Jain texts, as they discuss the limits of perception and the importance of complete context. The parable has several Indian variations, but broadly goes as follows:[6][7]		A group of blind men heard that a strange animal, called an elephant, had been brought to the town, but none of them were aware of its shape and form. Out of curiosity, they said: "We must inspect and know it by touch, of which we are capable". So, they sought it out, and when they found it they groped about it. In the case of the first person, whose hand landed on the trunk, said "This being is like a thick snake". For another one whose hand reached its ear, it seemed like a kind of fan. As for another person, whose hand was upon its leg, said, the elephant is a pillar like a tree-trunk. The blind man who placed his hand upon its side said, "elephant is a wall". Another who felt its tail, described it as a rope. The last felt its tusk, stating the elephant is that which is hard, smooth and like a spear.		In some versions, the blind men then discover their disagreements, suspect the others to be not telling the truth and come to blows. The stories also differ primarily in how the elephant's body parts are described, how violent the conflict becomes and how (or if) the conflict among the men and their perspectives is resolved. In some versions, they stop talking, start listening and collaborate to "see" the full elephant. In another, a sighted man enters the parable and describes the entire elephant from various perspectives, the blind men then learn that they were all partially correct and partially wrong. While one's subjective experience is true, it may not be the totality of truth.[4][6]		The parable has been used to illustrate a range of truths and fallacies; broadly, the parable implies that one's subjective experience can be true, but that such experience is inherently limited by its failure to account for other truths or a totality of truth. At various times the parable has provided insight into the relativism, opaqueness or inexpressible nature of truth, the behavior of experts in fields where there is a deficit or inaccessibility of information, the need for communication, and respect for different perspectives.		The Rigveda, dated to have been composed between 1500 to 1200 BCE, states "Reality is one, though wise men speak of it variously." According to Paul J. Griffiths, this premise is the foundation of universalist perspective behind the parable of the blind men and an elephant. The hymn asserts that the same Reality is subject to interpretations and described in various ways by the wise.[3] In the oldest version, four blind men walk into a forest where they meet an elephant. In this version, they do not fight with each other, but conclude that they each must have perceived a different beast although they experienced the same elephant.[3] The expanded version of the parable occurs in various ancient and Hindu texts. Many scholars refer to it as a Hindu parable.[6][7][8]		The parable or references appear in bhasya (commentaries, secondary literature) in the Hindu traditions. For example, Adi Shankara mentions it in his bhasya on verse 5.18.1 of the Chandogya Upanishad as follows:		etaddhasti darshana iva jatyandhah		Translation: That is like people blind by birth in/when viewing an elephant.		The medieval era Jain texts explain the concepts of anekāntavāda (or "many-sidedness") and syādvāda ("conditioned viewpoints") with the parable of the blind men and an elephant (Andhgajanyāyah), which addresses the manifold nature of truth. For example, this parable are found in Tattvarthaslokavatika of Vidyanandi (9th century) and Syādvādamanjari of Ācārya Mallisena (13th century). Mallisena uses the parable to argue that immature people deny various aspects of truth; deluded by the aspects they do understand, they deny the aspects they don't understand. "Due to extreme delusion produced on account of a partial viewpoint, the immature deny one aspect and try to establish another. This is the maxim of the blind (men) and the elephant."[10] Mallisena also cites the parable when noting the importance of considering all viewpoints in obtaining a full picture of reality. "It is impossible to properly understand an entity consisting of infinite properties without the method of modal description consisting of all viewpoints, since it will otherwise lead to a situation of seizing mere sprouts (i.e., a superficial, inadequate cognition), on the maxim of the blind (men) and the elephant."[11]		The Buddha twice uses the simile of blind men led astray. The earliest known version occurs in the text Udana 6.4.[4][12]		In the Canki Sutta he describes a row of blind men holding on to each other as an example of those who follow an old text that has passed down from generation to generation.[13] In the Udana (68–69)[14] he uses the elephant parable to describe sectarian quarrels. A king has the blind men of the capital brought to the palace, where an elephant is brought in and they are asked to describe it.		When the blind men had each felt a part of the elephant, the king went to each of them and said to each: 'Well, blind man, have you seen the elephant? Tell me, what sort of thing is an elephant?'		The men assert the elephant is either like a pot (the blind man who felt the elephant's head), a winnowing basket (ear), a plowshare (tusk), a plow (trunk), a granary (body), a pillar (foot), a mortar (back), a pestle (tail) or a brush (tip of the tail).		The men cannot agree with one another and come to blows over the question of what it is like and their dispute delights the king. The Buddha ends the story by comparing the blind men to preachers and scholars who are blind and ignorant and hold to their own views: "Just so are these preachers and scholars holding various views blind and unseeing.... In their ignorance they are by nature quarrelsome, wrangling, and disputatious, each maintaining reality is thus and thus." The Buddha then speaks the following verse:		O how they cling and wrangle, some who claim For preacher and monk the honored name! For, quarreling, each to his view they cling. Such folk see only one side of a thing.[15]		The Persian Sufi poet Sanai of Ghazni (currently, Afghanistan) presented this teaching story in his The Walled Garden of Truth.[16]		Rumi, the 13th Century Persian poet and teacher of Sufism, included it in his Masnavi. In his retelling, "The Elephant in the Dark", some Hindus bring an elephant to be exhibited in a dark room. A number of men touch and feel the elephant in the dark and, depending upon where they touch it, they believe the elephant to be like a water spout (trunk), a fan (ear), a pillar (leg) and a throne (back). Rumi uses this story as an example of the limits of individual perception:		The sensual eye is just like the palm of the hand. The palm has not the means of covering the whole of the beast.[17]		Rumi does not present a resolution to the conflict in his version, but states:		The eye of the Sea is one thing and the foam another. Let the foam go, and gaze with the eye of the Sea. Day and night foam-flecks are flung from the sea: oh amazing! You behold the foam but not the Sea. We are like boats dashing together; our eyes are darkened, yet we are in clear water.[17]		Rumi ends his poem by stating "If each had a candle and they went in together the differences would disappear." [18]		One of the most famous versions of the 19th century was the poem "The Blind Men and the Elephant" by John Godfrey Saxe (1816–1887).		The poem begins:		It was six men of Indostan     To learning much inclined, Who went to see the Elephant     (Though all of them were blind), That each by observation     Might satisfy his mind[19]		Each in his own opinion concludes that the elephant is like a wall, snake, spear, tree, fan or rope, depending upon where they had touched. Their heated debate comes short of physical violence, but the conflict is never resolved.		    Moral: So oft in theologic wars,     The disputants, I ween, Rail on in utter ignorance     Of what each other mean, And prate about an Elephant     Not one of them has seen!		Natalie Merchant sang this poem in full on her Leave Your Sleep album (Disc 1, track 13).		The story is seen as a metaphor in many disciplines, being pressed into service as an analogy in fields well beyond the traditional. In physics, it has been seen as an analogy for the wave–particle duality.[20] In biology, the way the blind men hold onto different parts of the elephant has been seen as a good analogy for the Polyclonal B cell response.[21]		The fable is one of a number of tales that cast light on the response of hearers or readers to the story itself. Idries Shah has commented on this element of self-reference in the many interpretations of the story, and its function as a teaching story:		...people address themselves to this story in one or more [...] interpretations. They then accept or reject them. Now they can feel happy; they have arrived at an opinion about the matter. According to their conditioning they produce the answer. Now look at their answers. Some will say that this is a fascinating and touching allegory of the presence of God. Others will say that it is showing people how stupid mankind can be. Some say it is anti-scholastic. Others that it is just a tale copied by Rumi from Sanai – and so on.[22]		Shah adapted the tale in his book The Dermis Probe. This version begins with a conference of scientists, from different fields of expertise, presenting their conflicting conclusions on the material upon which a camera is focused. As the camera slowly zooms out it gradually becomes clear that the material under examination is the hide of an African elephant. The words 'The Parts Are Greater Than The Whole' then appear on the screen. This retelling formed the script for a short four-minute film by the animator Richard Williams. The film was chosen as an Outstanding Film of the Year and was exhibited at the London and New York film festivals.[23]		The story enjoys a continuing appeal, as shown by the number of illustrated children's books of the fable; there is one for instance by Paul Galdone and another, Seven Blind Mice, by Ed Young (1992).		In the title cartoon of one of his books, cartoonist Sam Gross postulated that one of the blind men, encountering a pile of the elephant's fewmets, concluded that "An elephant is soft and mushy."		An elephant joke inverts the story in the following way:		Six blind elephants were discussing what men were like. After arguing they decided to find one and determine what it was like by direct experience. The first blind elephant felt the man and declared, 'Men are flat.' After the other blind elephants felt the man, they agreed.		Moral:		"We have to remember that what we observe is not nature in itself, but nature exposed to our method of questioning." - Werner Heisenberg		
The Life of Lazarillo de Tormes and of His Fortunes and Adversities (Spanish: La vida de Lazarillo de Tormes y de sus fortunas y adversidades [la ˈβiða ðe laθaˈɾiʎo ðe ˈtormes i ðe sus forˈtunas i aðβersiˈðaðes]) is a Spanish novella, published anonymously because of its anticlerical content. It was published simultaneously in three cities in 1554: Alcalá de Henares, Burgos and Antwerp. The Alcalá de Henares edition adds some episodes which were probably written by a second author.						Lázaro is a boy of humble origins from Salamanca. After his stepfather is accused of thievery, his mother asks a wily blind beggar to take on Lazarillo (little Lázaro) as his apprentice. Lázaro develops his cunning while serving the blind beggar and several other masters, while also learning to take on his father's practice.		Table of contents:		Besides its importance in the Spanish literature of the Golden Age, Lazarillo de Tormes is credited with founding a literary genre, the picaresque novel, from the Spanish word pícaro, meaning "rogue" or "rascal." In novels of this type, the adventures of the pícaro expose injustice while amusing the reader. This extensive genre includes Cervantes' Rinconete y Cortadillo and El coloquio de los perros, Henry Fielding's Tom Jones and Mark Twain's Adventures of Huckleberry Finn. Its influence extends to twentieth century novels, dramas and films featuring the "anti-hero".		Lazarillo de Tormes was banned by the Spanish Crown and included in the Index of Forbidden Books of the Spanish Inquisition; this was at least in part due to the book's anti-clerical flavour. In 1573, the Crown allowed circulation of a version which omitted Chapters 4 and 5 and assorted paragraphs from other parts of the book. An unabridged version did not appear in Spain until the nineteenth century. It was the Antwerp version that circulated throughout Europe, translated into French (1560), English (1576), Dutch (after the northern, largely Protestant Seven Provinces of the Low Countries revolted against Spain in 1579), German (1617), and Italian (1622).		Spanish first edition title pages in 1554 of Lazarillo de Tormes.		Burgos, Juan de Junta		Medina del Campo, Hermanos Del Canto		Alcalá de Henares, Salcedo		Antwerp, Martín Nucio		The primary objections to Lazarillo had to do with its vivid and realistic descriptions of the world of the pauper and the petty thief. The "worm's eye view" of society contrasted sharply with the more conventional literary focus on superhuman exploits recounted in chivalric romances such as the hugely popular Amadís de Gaula. In Antwerp, it followed the tradition of the impudent trickster figure Till Eulenspiegel.		Lazarillo introduced the picaresque device of delineating various professions and levels of society. A young boy or young man or woman describes masters or "betters" with ingenuously presented realistic details. But Lazarillo speaks of "the blind man," "the squire," "the pardoner," presenting these characters as types.		Significantly, the only named characters are Lazarillo and his family: his mother Antoña Pérez, his father Tomé Gonzáles, and his stepfather El Zayde. The surname de Tormes comes from the river Tormes. In the narrative, Lazarillo explains that his father ran a mill on the river, where he was literally born on the river. The Tormes runs through Lazarillo's home town, Salamanca, a Castilian-Leonese university city. (There is an old mill on the river, and a statue of Lazarillo and the blind man next to the Roman bridge [puente romano] in the city.)		Lazarillo is the diminutive of the Spanish name Lázaro. There are two appearances of the name Lazarus in the Bible, and not all critics agree as to which story the author was referring when he chose the name. The more well-known tale is in John 11:41–44, in which Jesus raises Lazarus from the dead. The second is in Luke 16:19–31, a parable about a beggar named Lazarus at the gate of a stingy rich man's house.		In contrast to the fancifully poetic language devoted to fantastic and supernatural events about unbelievable creatures and chivalric knights, the realistic prose of Lazarillo described suppliants purchasing indulgences from the Church, servants forced to die with their masters on the battlefield (as Lazarillo's father did), thousands of refugees wandering from town to town, poor beggars flogged away by whips because of the lack of food. The anonymous author included many popular sayings and ironically interpreted popular stories.		The Prologue with Lázaro's extensive protest against injustice is addressed to a high-level cleric, and five of his eight masters in the novel serve the church. Lazarillo attacked the appearance of the church and its hypocrisy, though not its essential beliefs, a balance not often present in following picaresque novels.		Besides creating a new genre, Lazarillo de Tormes was critically innovative in world literature in several aspects:		In his book Don Quixote, Cervantes introduces a gypsy thief called Ginés de Pasamonte who claims to be a writer (and who later in Part II masquerades as a puppeteer while on the run). Don Quixote interrogates this writer about his book;		"So good is it," replied Gines, "that a fig for 'Lazarillo de Tormes,' and all of that kind that have been written, or shall be written compared with it: all I will say about it is that it deals with facts, and facts so neat and diverting that no lies could match them."		"And how is the book entitled?" asked Don Quixote.		"The 'Life of Gines de Pasamonte,'" replied the subject of it.		"And is it finished?" asked Don Quixote.		"How can it be finished," said the other, "when my life is not yet finished?		The author criticises many organisations and groups in his book, most notably the Catholic Church and the Spanish aristocracy.		These two groups are clearly criticised through the different masters that Lazarillo serves. Characters such as the Cleric, the Friar, the Pardoner, the Priest and the Archbishop all have something wrong either with them as a person or with their character. The self-indulgent cleric concentrates on feeding himself, and when he does decide to give the "crumbs from his table" to Lazarillo, he says, "toma, come, triunfa, para tí es el mundo" "take, eat, triumph – the world is yours" a clear parody of a key communion statement.		In the final chapter, Lazarillo works for an Archpriest, who arranges his marriage to the Archpriest's maid. It is clear that Lazarillo's wife cheats on him with the Archpriest, and all vows of celibacy are forgotten.		In Chapter 3, Lazarillo becomes the servant of a Squire. The Squire openly flaunts his wealth despite not being able to feed himself, let alone Lázaro. This is a parody of the importance of having a strong image among the nobility.		The identity of the author of Lazarillo has been a puzzle for nearly four hundred years. Given the subversive nature of Lazarillo and its open criticism of the Catholic Church, it is likely that the author chose to remain anonymous out of fear of religious persecution.		Neither the author nor the date and place of the first appearance of the work is known. It appeared anonymously; and no author's name was accredited to it until 1605, when the Hieronymite monk José de Sigüenza named as its author Fray Juan de Ortega. Two years later, it was accredited by the Belgian Valère André to Diego Hurtado de Mendoza. In 1608, André Schott repeated this assertion, although less categorically. Despite these claims, the assignment of the work to Diego Hurtado de Mendoza was generally accepted, until Alfred Paul Victor Morel-Fatio, in 1888, demonstrated the untenability of that candidate[clarification needed]. The earliest known editions are the four of Alcalá de Henares, Antwerp, Medina del Campo, and Burgos, all of which appeared in 1554. Two continuations (or second parts) appeared – one, anonymously, in 1555, and the other, accredited to H. Luna, in 1620.		There has been some suggestion that the author was originally of Jewish extraction, who in 1492 had to convert to Catholicism to avoid being expelled from Spain; it could be used to explain the animosity towards the Catholic Church displayed in the book.[citation needed] Apart from the chronological difficulties this hypothesis presents, it should be noted that Catholic criticism of Catholic clergy, including the pope, was common; by then, such criticism had had a long and even reputable tradition that can be seen in the works of famous Catholic writers such as Chaucer, Dante and Erasmus.		Documents recently discovered by the Spanish paleographer Mercedes Agulló support the hypothesis that the author was, in fact, Diego Hurtado de Mendoza.[3]		In 1555, only a year after the first edition of the book, a sequel by another anonymous author was attached to the original Lazarillo in an edition printed in Antwerp, Low Countries. This sequel is known as El Lazarillo de Amberes, Amberes being the Spanish name for Antwerp.		Lázaro leaves his wife and child with the priest, in Toledo, and joins the Spanish army in their campaign against the Moors. The ship carrying the soldiers sinks, but before it does, Lázaro drinks as much wine as he can. His body is so full of wine that there is no place for the water to enter him, and by that means he survives under the sea. Threatened by the tuna fish there, Lázaro prays for mercy and is eventually metamorphosized into a tuna himself. Most of the book tells about how Lázaro struggles to find his place in tuna society.		In 1620, another sequel, by Juan de Luna, appeared in Paris. In the prologue, the narrator (not Lázaro himself but someone who claims to have a copy of Lázaro's writings) tells the reader that he was moved to publish the second part of Lázaro's adventures after hearing about a book which, he alleges, had falsely told of Lázaro being transformed into a tuna (obviously a disparaging reference to Lazarillo de Amberes).		Because of Lazarillo's first adventures, the Spanish word lazarillo has taken on the meaning "guide", as to a blind person. Consequently, in Spanish a guide dog is still informally called a perro lazarillo, as it was called before perro guía became common.		
Sumer (/ˈsuːmər/)[note 1] was the first urban civilization in the historical region of southern Mesopotamia, modern-day southern Iraq, during the Chalcolithic and Early Bronze ages, and arguably the first civilization in the world with Ancient Egypt and the Indus Valley.[1] Living along the valleys of the Tigris and Euphrates, Sumerian farmers were able to grow an abundance of grain and other crops, the surplus of which enabled them to settle in one place.		Proto-writing in the prehistory dates back to c. 3000 BC. The earliest texts come from the cities of Uruk and Jemdet Nasr and date back to 3300 BC; early cuneiform script writing emerged in 3000 BC.[2]		Modern historians have suggested that Sumer was first permanently settled between c. 5500 and 4000 BC by a West Asian people who spoke the Sumerian language (pointing to the names of cities, rivers, basic occupations, etc., as evidence), an agglutinative language isolate.[3][4][5][6]		These conjectured, prehistoric people are now called "proto-Euphrateans" or "Ubaidians",[7] and are theorized to have evolved from the Samarra culture of northern Mesopotamia.[8][9][10][11] The Ubaidians (though never mentioned by the Sumerians themselves) are assumed by modern-day scholars to have been the first civilizing force in Sumer, draining the marshes for agriculture, developing trade, and establishing industries, including weaving, leatherwork, metalwork, masonry, and pottery.[7]		Some scholars contest the idea of a Proto-Euphratean language or one substrate language. It has been suggested by them, and others, that the Sumerian language was originally that of the hunter and fisher peoples, who lived in the marshland and the Eastern Arabia littoral region, and were part of the Arabian bifacial culture.[12] Reliable historical records begin much later; there are none in Sumer of any kind that have been dated before Enmebaragesi (c. 26th century BC). Juris Zarins believes the Sumerians lived along the coast of Eastern Arabia, today's Persian Gulf region, before it flooded at the end of the Ice Age.[13]		Sumerian civilization took form in the Uruk period (4th millennium BC), continuing into the Jemdet Nasr and Early Dynastic periods. During the 3rd millennium BC, a close cultural symbiosis developed between the Sumerians, who spoke a language isolate, and Akkadian-speakers, which included widespread bilingualism.[14] The influence of Sumerian on Akkadian (and vice versa) is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence.[14] This has prompted scholars to refer to Sumerian and Akkadian in the 3rd millennium BC as a Sprachbund.[14] Sumer was conquered by the Semitic-speaking kings of the Akkadian Empire around 2270 BC (short chronology), but Sumerian continued as a sacred language.		Native Sumerian rule re-emerged for about a century in the Third Dynasty of Ur at approximately 2100–2000 BC, but the Akkadian language also remained in use. The Sumerian city of Eridu, on the coast of the Persian Gulf, is considered to have been the world's first city, where three separate cultures may have fused: that of peasant Ubaidian farmers, living in mud-brick huts and practicing irrigation; that of mobile nomadic Semitic pastoralists living in black tents and following herds of sheep and goats; and that of fisher folk, living in reed huts in the marshlands, who may have been the ancestors of the Sumerians.[15]						The term Sumerian is the common name given to the ancient non-Semitic-speaking inhabitants of Mesopotamia, Sumer, by the East Semitic-speaking Akkadians. The Sumerians referred to themselves as ùĝ saĝ gíg ga (cuneiform: 𒌦 𒊕 𒈪 𒂵), phonetically /uŋ saŋ gi ga/, literally meaning "the black-headed people", and to their land as ki-en-gi(-r) (cuneiform: 𒆠𒂗𒄀) ('place' + 'lords' + 'noble'), meaning "place of the noble lords".[16] The Akkadian word Shumer may represent the geographical name in dialect, but the phonological development leading to the Akkadian term šumerû is uncertain.[17] Hebrew Shinar, Egyptian Sngr, and Hittite Šanhar(a), all referring to southern Mesopotamia, could be western variants of Shumer.[17]		In the late 4th millennium BC, Sumer was divided into many independent city-states, which were divided by canals and boundary stones. Each was centered on a temple dedicated to the particular patron god or goddess of the city and ruled over by a priestly governor (ensi) or by a king (lugal) who was intimately tied to the city's religious rites.		The five "first" cities, said to have exercised pre-dynastic kingship "before the flood":   		Other principal cities:		(1location uncertain) (2an outlying city in northern Mesopotamia)		Minor cities (from south to north):		(2an outlying city in northern Mesopotamia)		Apart from Mari, which lies full 330 kilometres (205 miles) north-west of Agade, but which is credited in the king list as having “exercised kingship” in the Early Dynastic II period, and Nagar, an outpost, these cities are all in the Euphrates-Tigris alluvial plain, south of Baghdad in what are now the Bābil, Diyala, Wāsit, Dhi Qar, Basra, Al-Muthannā and Al-Qādisiyyah governorates of Iraq.		The Sumerian city-states rose to power during the prehistoric Ubaid and Uruk periods. Sumerian written history reaches back to the 27th century BC and before, but the historical record remains obscure until the Early Dynastic III period, c. the 23rd century BC, when a now deciphered syllabary writing system was developed, which has allowed archaeologists to read contemporary records and inscriptions. Classical Sumer ends with the rise of the Akkadian Empire in the 23rd century BC. Following the Gutian period, there was a brief Sumerian Renaissance in the 21st century BC, cut short in the 20th century BC by invasions by the Amorites. The Amorite "dynasty of Isin" persisted until c. 1700 BC, when Mesopotamia was united under Babylonian rule. The Sumerians were eventually absorbed into the Akkadian (Assyro-Babylonian) population.		The Ubaid period is marked by a distinctive style of fine quality painted pottery which spread throughout Mesopotamia and the Persian Gulf. During this time, the first settlement in southern Mesopotamia was established at Eridu (Cuneiform: nun.ki), c. 6500 BC, by farmers who brought with them the Hadji Muhammed culture, which first pioneered irrigation agriculture. It appears that this culture was derived from the Samarran culture from northern Mesopotamia. It is not known whether or not these were the actual Sumerians who are identified with the later Uruk culture. The rise of the city of Uruk may be reflected in the story of the passing of the gifts of civilization (me) to Inanna, goddess of Uruk and of love and war, by Enki, god of wisdom and chief god of Eridu, may reflect the transition from Eridu to Uruk.[19]:174		The archaeological transition from the Ubaid period to the Uruk period is marked by a gradual shift from painted pottery domestically produced on a slow wheel to a great variety of unpainted pottery mass-produced by specialists on fast wheels. The Uruk period is a continuation and an outgrowth of Ubaid with pottery being the main visible change.[20][21]		By the time of the Uruk period (c. 4100–2900 BC calibrated), the volume of trade goods transported along the canals and rivers of southern Mesopotamia facilitated the rise of many large, stratified, temple-centered cities (with populations of over 10,000 people) where centralized administrations employed specialized workers. It is fairly certain that it was during the Uruk period that Sumerian cities began to make use of slave labor captured from the hill country, and there is ample evidence for captured slaves as workers in the earliest texts. Artifacts, and even colonies of this Uruk civilization have been found over a wide area—from the Taurus Mountains in Turkey, to the Mediterranean Sea in the west, and as far east as central Iran.[22]		The Uruk period civilization, exported by Sumerian traders and colonists (like that found at Tell Brak), had an effect on all surrounding peoples, who gradually evolved their own comparable, competing economies and cultures. The cities of Sumer could not maintain remote, long-distance colonies by military force.[22]		Sumerian cities during the Uruk period were probably theocratic and were most likely headed by a priest-king (ensi), assisted by a council of elders, including both men and women.[23] It is quite possible that the later Sumerian pantheon was modeled upon this political structure. There was little evidence of organized warfare or professional soldiers during the Uruk period, and towns were generally unwalled. During this period Uruk became the most urbanized city in the world, surpassing for the first time 50,000 inhabitants.		The ancient Sumerian king list includes the early dynasties of several prominent cities from this period. The first set of names on the list is of kings said to have reigned before a major flood occurred. These early names may be fictional, and include some legendary and mythological figures, such as Alulim and Dumizid.[23]		The end of the Uruk period coincided with the Piora oscillation, a dry period from c. 3200 – 2900 BC that marked the end of a long wetter, warmer climate period from about 9,000 to 5,000 years ago, called the Holocene climatic optimum.[24]		The dynastic period begins c. 2900 BC and was associated with a shift from the temple establishment headed by council of elders led by a priestly "En" (a male figure when it was a temple for a goddess, or a female figure when headed by a male god)[25] towards a more secular Lugal (Lu = man, Gal = great) and includes such legendary patriarchal figures as Enmerkar, Lugalbanda and Gilgamesh—who are supposed to have reigned shortly before the historic record opens c. 2700 BC, when the now deciphered syllabic writing started to develop from the early pictograms. The center of Sumerian culture remained in southern Mesopotamia, even though rulers soon began expanding into neighboring areas, and neighboring Semitic groups adopted much of Sumerian culture for their own.		The earliest dynastic king on the Sumerian king list whose name is known from any other legendary source is Etana, 13th king of the first dynasty of Kish. The earliest king authenticated through archaeological evidence is Enmebaragesi of Kish (c. 26th century BC), whose name is also mentioned in the Gilgamesh epic—leading to the suggestion that Gilgamesh himself might have been a historical king of Uruk. As the Epic of Gilgamesh shows, this period was associated with increased war. Cities became walled, and increased in size as undefended villages in southern Mesopotamia disappeared. (Both Enmerkar and Gilgamesh are credited with having built the walls of Uruk[26]).		c. 2500–2270 BC		The dynasty of Lagash, though omitted from the king list, is well attested through several important monuments and many archaeological finds.		Although short-lived, one of the first empires known to history was that of Eannatum of Lagash, who annexed practically all of Sumer, including Kish, Uruk, Ur, and Larsa, and reduced to tribute the city-state of Umma, arch-rival of Lagash. In addition, his realm extended to parts of Elam and along the Persian Gulf. He seems to have used terror as a matter of policy.[27] Eannatum's Stele of the Vultures depicts vultures pecking at the severed heads and other body parts of his enemies. His empire collapsed shortly after his death.		Later, Lugal-Zage-Si, the priest-king of Umma, overthrew the primacy of the Lagash dynasty in the area, then conquered Uruk, making it his capital, and claimed an empire extending from the Persian Gulf to the Mediterranean. He was the last ethnically Sumerian king before Sargon of Akkad.[15]		c. 2270–2083 BC (short chronology)		The Eastern Semitic Akkadian language is first attested in proper names of the kings of Kish c. 2800 BC,[27] preserved in later king lists. There are texts written entirely in Old Akkadian dating from c. 2500 BC. Use of Old Akkadian was at its peak during the rule of Sargon the Great (c. 2270–2215 BC), but even then most administrative tablets continued to be written in Sumerian, the language used by the scribes. Gelb and Westenholz differentiate three stages of Old Akkadian: that of the pre-Sargonic era, that of the Akkadian empire, and that of the "Neo-Sumerian Renaissance" that followed it. Akkadian and Sumerian coexisted as vernacular languages for about one thousand years, but by around 1800 BC, Sumerian was becoming more of a literary language familiar mainly only to scholars and scribes. Thorkild Jacobsen has argued that there is little break in historical continuity between the pre- and post-Sargon periods, and that too much emphasis has been placed on the perception of a "Semitic vs. Sumerian" conflict.[28] However, it is certain that Akkadian was also briefly imposed on neighboring parts of Elam that were previously conquered, by Sargon.		c. 2083–2050 BC (short chronology)		c. 2093–2046 BC (short chronology)		Following the downfall of the Akkadian Empire at the hands of Gutians, another native Sumerian ruler, Gudea of Lagash, rose to local prominence and continued the practices of the Sargonid kings' claims to divinity. The previous Lagash dynasty, Gudea and his descendants also promoted artistic development and left a large number of archaeological artifacts.		c. 2047–1940 BC (short chronology)		Later, the 3rd dynasty of Ur under Ur-Nammu and Shulgi, whose power extended as far as southern Assyria, was the last great "Sumerian renaissance", but already the region was becoming more Semitic than Sumerian, with the resurgence of the Akkadian speaking Semites in Assyria and elsewhere, and the influx of waves of Semitic Martu (Amorites) who were to found several competing local powers in the south, including Isin, Larsa, Eshnunna and some time later Babylonia. The last of these eventually came to briefly dominate the south of Mesopotamia as the Babylonian Empire, just as the Old Assyrian Empire had already done so in the north from the late 21st century BC. The Sumerian language continued as a sacerdotal language taught in schools in Babylonia and Assyria, much as Latin was used in the Medieval period, for as long as cuneiform was utilized.		This period is generally taken to coincide with a major shift in population from southern Mesopotamia toward the north. Ecologically, the agricultural productivity of the Sumerian lands was being compromised as a result of rising salinity. Soil salinity in this region had been long recognized as a major problem. Poorly drained irrigated soils, in an arid climate with high levels of evaporation, led to the buildup of dissolved salts in the soil, eventually reducing agricultural yields severely. During the Akkadian and Ur III phases, there was a shift from the cultivation of wheat to the more salt-tolerant barley, but this was insufficient, and during the period from 2100 BC to 1700 BC, it is estimated that the population in this area declined by nearly three fifths.[29] This greatly upset the balance of power within the region, weakening the areas where Sumerian was spoken, and comparatively strengthening those where Akkadian was the major language. Henceforth, Sumerian would remain only a literary and liturgical language, similar to the position occupied by Latin in medieval Europe.		Following an Elamite invasion and sack of Ur during the rule of Ibbi-Sin (c. 1940 BC)[citation needed], Sumer came under Amorites rule (taken to introduce the Middle Bronze Age). The independent Amorite states of the 20th to 18th centuries are summarized as the "Dynasty of Isin" in the Sumerian king list, ending with the rise of Babylonia under Hammurabi c. 1700 BC.		Later rulers who dominated Assyria and Babylonia occasionally assumed the old Sargonic title "King of Sumer and Akkad", such as Tukulti-Ninurta I of Assyria after c. 1225 BC.		Uruk, one of Sumer's largest cities, has been estimated to have had a population of 50,000-80,000 at its height;[30] given the other cities in Sumer, and the large agricultural population, a rough estimate for Sumer's population might be 0.8 million to 1.5 million. The world population at this time has been estimated at about 27 million.[31]		The Sumerians spoke a language isolate, but a number of linguists have claimed to be able to detect a substrate language of unknown classification beneath Sumerian because names of some of Sumer's major cities are not Sumerian, revealing influences of earlier inhabitants.[32] However, the archaeological record shows clear uninterrupted cultural continuity from the time of the early Ubaid period (5300 – 4700 BC C-14) settlements in southern Mesopotamia. The Sumerian people who settled here farmed the lands in this region that were made fertile by silt deposited by the Tigris and the Euphrates.		Some archaeologists have speculated that the original speakers of ancient Sumerian may have been farmers, who moved down from the north of Mesopotamia after perfecting irrigation agriculture there. The Ubaid period pottery of southern Mesopotamia has been connected via Choga Mami transitional ware to the pottery of the Samarra period culture (c. 5700 – 4900 BC C-14) in the north, who were the first to practice a primitive form of irrigation agriculture along the middle Tigris River and its tributaries. The connection is most clearly seen at Tell Awayli (Oueilli, Oueili) near Larsa, excavated by the French in the 1980s, where eight levels yielded pre-Ubaid pottery resembling Samarran ware. According to this theory, farming peoples spread down into southern Mesopotamia because they had developed a temple-centered social organization for mobilizing labor and technology for water control, enabling them to survive and prosper in a difficult environment.[citation needed]		Others have suggested a continuity of Sumerians, from the indigenous hunter-fisherfolk traditions, associated with the bifacial assemblages found on the Arabian littoral. Juris Zarins believes the Sumerians may have been the people living in the Persian Gulf region before it flooded at the end of the last Ice Age.[33]		Sumerian myths suggest a prohibition against premarital sex.[34] Marriages were often arranged by the parents of the bride and groom; engagements were usually completed through the approval of contracts recorded on clay tablets. These marriages became legal as soon as the groom delivered a bridal gift to his bride's father. Nonetheless, evidence suggests that premarital sex was a common, but surreptitious, occurrence.[35]:78		In the early Sumerian period, the primitive pictograms suggest[36] that		There is considerable evidence concerning Sumerian music. Lyres and flutes were played, among the best-known examples being the Lyres of Ur.[37]		Inscriptions describing the reforms of king Urukagina of Lagash (c. 2300 BC) say that he abolished the former custom of polyandry in his country, prescribing that a woman who took multiple husbands be stoned with rocks upon which her crime had been written.[38]		Sumerian culture was male-dominated and stratified. The Code of Ur-Nammu, the oldest such codification yet discovered, dating to the Ur III, reveals a glimpse at societal structure in late Sumerian law. Beneath the lu-gal ("great man" or king), all members of society belonged to one of two basic strata: The "lu" or free person, and the slave (male, arad; female geme). The son of a lu was called a dumu-nita until he married. A woman (munus) went from being a daughter (dumu-mi), to a wife (dam), then if she outlived her husband, a widow (numasu) and she could then remarry.		Prostitution existed but it is not clear if sacred prostitution did.[39]:151		Anthropological evidence suggests that most societies before Sumer, along with most contemporary civilizations, were relatively egalitarian. The early periods of Sumer were also very egalitarian by nature, but that started to change with the rise of the Early Dynastic Period. By the time the Akkadian Empire rose to power, Patriarchy was a well-established cultural norm.[40][41]		The most important archaeological discoveries in Sumer are a large number of clay tablets written in cuneiform script. Sumerian writing, while proven to not be the oldest example of writing on earth, is considered to be a great milestone in the development of humanity's ability to not only create historical records but also in creating pieces of literature, both in the form of poetic epics and stories as well as prayers and laws. Although pictures — that is, hieroglyphs — were used first, cuneiform and then ideograms (where symbols were made to represent ideas) soon followed. Triangular or wedge-shaped reeds were used to write on moist clay. A large body of hundreds of thousands of texts in the Sumerian language have survived, such as personal and business letters, receipts, lexical lists, laws, hymns, prayers, stories, and daily records. Full libraries of clay tablets have been found. Monumental inscriptions and texts on different objects, like statues or bricks, are also very common. Many texts survive in multiple copies because they were repeatedly transcribed by scribes in training. Sumerian continued to be the language of religion and law in Mesopotamia long after Semitic speakers had become dominant.		A prime example of cuneiform writing would be a lengthy poem that was discovered in the ruins of Uruk. The Epic of Gilgamesh was written in the standard Sumerian cuneiform. It tells of a king from the early Dynastic II period named Gilgamesh or "Bilgamesh" in Sumerian. The story is based around the fictional adventures of Gilgamesh and his companion, Enkidu. It was laid out on several clay tablets and is claimed to be the earliest example of a fictional, written piece of literature discovered so far.		The Sumerian language is generally regarded as a language isolate in linguistics because it belongs to no known language family; Akkadian, by contrast, belongs to the Semitic branch of the Afroasiatic languages. There have been many failed attempts to connect Sumerian to other language families. It is an agglutinative language; in other words, morphemes ("units of meaning") are added together to create words, unlike analytic languages where morphemes are purely added together to create sentences. Some authors have proposed that there may be evidence of a substratum or adstratum language for geographic features and various crafts and agricultural activities, called variously Proto-Euphratean or Proto Tigrean, but this is disputed by others.		Understanding Sumerian texts today can be problematic even for experts.[citation needed] Most difficult are the earliest texts, which in many cases do not give the full grammatical structure of the language and seem to have been used as an "aide-mémoire" for knowledgeable scribes.		During the 3rd millennium BC a cultural symbiosis developed between the Sumerians and the Akkadians, which included widespread bilingualism.[14] The influences between Sumerian on Akkadian are evident in all areas including lexical borrowing on a massive scale—and syntactic, morphological, and phonological convergence.[14] This mutual influence has prompted scholars to refer to Sumerian and Akkadian of the 3rd millennium BC as a Sprachbund.[14]		Akkadian gradually replaced Sumerian as a spoken language somewhere around the turn of the 3rd and the 2nd millennium BC,[42] but Sumerian continued to be used as a sacred, ceremonial, literary, and scientific language in Babylonia and Assyria until the 1st century AD.[43]		The Sumerians credited their divinities for all matters pertaining to them and exhibited humility in the face of cosmic forces, such as death and divine wrath.[35]:3-4		Sumerian religion seems to have been founded upon two separate cosmogenic myths. The first saw creation as the result of a series of hieroi gamoi or sacred marriages, involving the reconciliation of opposites, postulated as a coming together of male and female divine beings; the gods. This continued to influence the whole Mesopotamian mythos. Thus, in the later Akkadian Enuma Elish, the creation was seen as the union of fresh and salt water; as male Abzu, and female Tiamat. The products of that union, Lahm and Lahmu, "the muddy ones", were titles given to the gate keepers of the E-Abzu temple of Enki, in Eridu, the first Sumerian city. Describing the way that muddy islands emerge from the confluence of fresh and salty water at the mouth of the Euphrates, where the river deposited its load of silt, a second hieros gamos supposedly created Anshar and Kishar, the "sky-pivot" or axle, and the "earth pivot", parents in turn of Anu (the sky) and Ki (the earth). Another important Sumerian hieros gamos was that between Ki, here known as Ninhursag or "Lady of the Mountains", and Enki of Eridu, the god of fresh water which brought forth greenery and pasture.		At an early stage, following the dawn of recorded history, Nippur, in central Mesopotamia, replaced Eridu in the south as the primary temple city, whose priests exercised political hegemony on the other city-states. Nippur retained this status throughout the Sumerian period.		Sumerians believed in an anthropomorphic polytheism, or the belief in many gods in human form. There was no common set of gods; each city-state had its own patrons, temples, and priest-kings. Nonetheless, these were not exclusive; the gods of one city were often acknowledged elsewhere. Sumerian speakers were among the earliest people to record their beliefs in writing, and were a major inspiration in later Mesopotamian mythology, religion, and astrology.		The Sumerians worshiped:		These deities formed a core pantheon; there were additionally hundreds of minor ones. Sumerian gods could thus have associations with different cities, and their religious importance often waxed and waned with those cities' political power. The gods were said to have created human beings from clay for the purpose of serving them. The temples organized the mass labour projects needed for irrigation agriculture. Citizens had a labor duty to the temple, though they could avoid it by a payment of silver.		Sumerians believed that the universe consisted of a flat disk enclosed by a dome. The Sumerian afterlife involved a descent into a gloomy netherworld to spend eternity in a wretched existence as a Gidim (ghost).[44]		The universe was divided into four quarters:		Their known world extended from The Upper Sea or Mediterranean coastline, to The Lower Sea, the Persian Gulf and the land of Meluhha (probably the Indus Valley) and Magan (Oman), famed for its copper ores.		Ziggurats (Sumerian temples) each had an individual name and consisted of a forecourt, with a central pond for purification.[45] The temple itself had a central nave with aisles along either side. Flanking the aisles would be rooms for the priests. At one end would stand the podium and a mudbrick table for animal and vegetable sacrifices. Granaries and storehouses were usually located near the temples. After a time the Sumerians began to place the temples on top of multi-layered square constructions built as a series of rising terraces, giving rise to the Ziggurat style.[46]		It was believed that when people died, they would be confined to a gloomy world of Ereshkigal, whose realm was guarded by gateways with various monsters designed to prevent people entering or leaving. The dead were buried outside the city walls in graveyards where a small mound covered the corpse, along with offerings to monsters and a small amount of food. Those who could afford it sought burial at Dilmun.[47] Human sacrifice was found in the death pits at the Ur royal cemetery where Queen Puabi was accompanied in death by her servants.		The Sumerians adopted an agricultural lifestyle perhaps as early as c. 5000 BC – 4500 BC. The region demonstrated a number of core agricultural techniques, including organized irrigation, large-scale intensive cultivation of land, mono-cropping involving the use of plough agriculture, and the use of an agricultural specialized labour force under bureaucratic control. The necessity to manage temple accounts with this organization led to the development of writing (c. 3500 BC).		In the early Sumerian Uruk period, the primitive pictograms suggest that sheep, goats, cattle, and pigs were domesticated. They used oxen as their primary beasts of burden and donkeys or equids as their primary transport animal and "woollen clothing as well as rugs were made from the wool or hair of the animals. ... By the side of the house was an enclosed garden planted with trees and other plants; wheat and probably other cereals were sown in the fields, and the shaduf was already employed for the purpose of irrigation. Plants were also grown in pots or vases."[36]		The Sumerians were one of the first known beer drinking societies. Cereals were plentiful and were the key ingredient in their early brew. They brewed multiple kinds of beer consisting of wheat, barley, and mixed grain beers. Beer brewing was very important to the Sumerians. It was referenced in the Epic of Gilgamesh when Enkidu was introduced to the food and beer of Gilgamesh's people: "Drink the beer, as is the custom of the land... He drank the beer-seven jugs! and became expansive and sang with joy!"[48]		The Sumerians practiced similar irrigation techniques as those used in Egypt.[49] American anthropologist Robert McCormick Adams says that irrigation development was associated with urbanization,[50] and that 89% of the population lived in the cities.		They grew barley, chickpeas, lentils, wheat, dates, onions, garlic, lettuce, leeks and mustard. Sumerians caught many fish and hunted fowl and gazelle.[51]		Sumerian agriculture depended heavily on irrigation. The irrigation was accomplished by the use of shaduf, canals, channels, dykes, weirs, and reservoirs. The frequent violent floods of the Tigris, and less so, of the Euphrates, meant that canals required frequent repair and continual removal of silt, and survey markers and boundary stones needed to be continually replaced. The government required individuals to work on the canals in a corvee, although the rich were able to exempt themselves.		As is known from the "Sumerian Farmer's Almanac", after the flood season and after the Spring Equinox and the Akitu or New Year Festival, using the canals, farmers would flood their fields and then drain the water. Next they made oxen stomp the ground and kill weeds. They then dragged the fields with pickaxes. After drying, they plowed, harrowed, and raked the ground three times, and pulverized it with a mattock, before planting seed. Unfortunately, the high evaporation rate resulted in a gradual increase in the salinity of the fields. By the Ur III period, farmers had switched from wheat to the more salt-tolerant barley as their principal crop.		Sumerians harvested during the spring in three-person teams consisting of a reaper, a binder, and a sheaf handler.[52] The farmers would use threshing wagons, driven by oxen, to separate the cereal heads from the stalks and then use threshing sleds to disengage the grain. They then winnowed the grain/chaff mixture.		The Tigris-Euphrates plain lacked minerals and trees. Sumerian structures were made of plano-convex mudbrick, not fixed with mortar or cement. Mud-brick buildings eventually deteriorate, so they were periodically destroyed, leveled, and rebuilt on the same spot. This constant rebuilding gradually raised the level of cities, which thus came to be elevated above the surrounding plain. The resultant hills, known as tells, are found throughout the ancient Near East.		According to Archibald Sayce, the primitive pictograms of the early Sumerian (i.e. Uruk) era suggest that "Stone was scarce, but was already cut into blocks and seals. Brick was the ordinary building material, and with it cities, forts, temples and houses were constructed. The city was provided with towers and stood on an artificial platform; the house also had a tower-like appearance. It was provided with a door which turned on a hinge, and could be opened with a sort of key; the city gate was on a larger scale, and seems to have been double. The foundation stones — or rather bricks — of a house were consecrated by certain objects that were deposited under them."[36]		The most impressive and famous of Sumerian buildings are the ziggurats, large layered platforms that supported temples. Sumerian cylinder seals also depict houses built from reeds not unlike those built by the Marsh Arabs of Southern Iraq until as recently as 400 CE. The Sumerians also developed the arch, which enabled them to develop a strong type of dome. They built this by constructing and linking several arches. Sumerian temples and palaces made use of more advanced materials and techniques,[citation needed] such as buttresses, recesses, half columns, and clay nails.		The Sumerians developed a complex system of metrology c. 4000 BC. This advanced metrology resulted in the creation of arithmetic, geometry, and algebra. From c. 2600 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.[53] The period c. 2700 – 2300 BC saw the first appearance of the abacus, and a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.[54] The Sumerians were the first to use a place value numeral system. There is also anecdotal evidence the Sumerians may have used a type of slide rule in astronomical calculations. They were the first to find the area of a triangle and the volume of a cube.[55]		Discoveries of obsidian from far-away locations in Anatolia and lapis lazuli from Badakhshan in northeastern Afghanistan, beads from Dilmun (modern Bahrain), and several seals inscribed with the Indus Valley script suggest a remarkably wide-ranging network of ancient trade centered on the Persian Gulf. For example, Imports to Ur came from many parts of the world. In particular, the metals of all types had to be imported.		The Epic of Gilgamesh refers to trade with far lands for goods such as wood that were scarce in Mesopotamia. In particular, cedar from Lebanon was prized. The finding of resin in the tomb of Queen Puabi at Ur, indicates it was traded from as far away as Mozambique.		The Sumerians used slaves, although they were not a major part of the economy. Slave women worked as weavers, pressers, millers, and porters.		Sumerian potters decorated pots with cedar oil paints. The potters used a bow drill to produce the fire needed for baking the pottery. Sumerian masons and jewelers knew and made use of alabaster (calcite), ivory, iron, gold, silver, carnelian, and lapis lazuli.[56]		Large institutions kept their accounts in barley and silver, often with a fixed rate between them. The obligations, loans and prices in general were usually denominated in one of them. Many transactions involved debt, for example goods consigned to merchants by temple and beer advanced by "ale women".[57]		Commercial credit and agricultural consumer loans were the main types of loans. The trade credit was usually extended by temples in order to finance trade expeditions and was nominated in silver. The interest rate was set at 1/60 a month (one shekel per mina) some time before 2000 BC and it remained at that level for about two thousand years.[57] Rural loans commonly arose as a result of unpaid obligations due to an institution (such as a temple), in this case the arrears were considered to be lent to the debtor.[58] They were denominated in barley or other crops and the interest rate was typically much higher than for commercial loans and could amount to 1/3 to 1/2 of the loan principal.[57]		Periodically, rulers signed "clean slate" decrees that cancelled all the rural (but not commercial) debt and allowed bondservants to return to their homes. Customarily, rulers did it at the beginning of the first full year of their reign, but they could also be proclaimed at times of military conflict or crop failure. The first known ones were made by Enmetena and Urukagina of Lagash in 2400-2350 BC. According to Hudson, the purpose of these decrees was to prevent debts mounting to a degree that they threatened the fighting force, which could happen if peasants lost the subsistence land or became bondservants due to the inability to repay the debt.[57]		The almost constant wars among the Sumerian city-states for 2000 years helped to develop the military technology and techniques of Sumer to a high level.[59] The first war recorded in any detail was between Lagash and Umma in c. 2525 BC on a stele called the Stele of the Vultures. It shows the king of Lagash leading a Sumerian army consisting mostly of infantry. The infantry carried spears, wore copper helmets, and carried rectangular shields. The spearmen are shown arranged in what resembles the phalanx formation, which requires training and discipline; this implies that the Sumerians may have made use of professional soldiers.[60]		The Sumerian military used carts harnessed to onagers. These early chariots functioned less effectively in combat than did later designs, and some have suggested that these chariots served primarily as transports, though the crew carried battle-axes and lances. The Sumerian chariot comprised a four or two-wheeled device manned by a crew of two and harnessed to four onagers. The cart was composed of a woven basket and the wheels had a solid three-piece design.		Sumerian cities were surrounded by defensive walls. The Sumerians engaged in siege warfare between their cities, but the mudbrick walls were able to deter some foes.		Examples of Sumerian technology include: the wheel, cuneiform script, arithmetic and geometry, irrigation systems, Sumerian boats, lunisolar calendar, bronze, leather, saws, chisels, hammers, braces, bits, nails, pins, rings, hoes, axes, knives, lancepoints, arrowheads, swords, glue, daggers, waterskins, bags, harnesses, armor, quivers, war chariots, scabbards, boots, sandals, harpoons and beer. The Sumerians had three main types of boats:		Evidence of wheeled vehicles appeared in the mid 4th millennium BC, near-simultaneously in Mesopotamia, the Northern Caucasus (Maykop culture) and Central Europe. The wheel initially took the form of the potter's wheel. The new concept quickly led to wheeled vehicles and mill wheels. The Sumerians' cuneiform script is the oldest (or second oldest after the Egyptian hieroglyphs) which has been deciphered (the status of even older inscriptions such as the Jiahu symbols and Tartaria tablets is controversial). The Sumerians were among the first astronomers, mapping the stars into sets of constellations, many of which survived in the zodiac and were also recognized by the ancient Greeks.[61] They were also aware of the five planets that are easily visible to the naked eye.[62]		They invented and developed arithmetic by using several different number systems including a mixed radix system with an alternating base 10 and base 6. This sexagesimal system became the standard number system in Sumer and Babylonia. They may have invented military formations and introduced the basic divisions between infantry, cavalry, and archers. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The first true city-states arose in Sumer, roughly contemporaneously with similar entities in what are now Syria and Lebanon. Several centuries after the invention of cuneiform, the use of writing expanded beyond debt/payment certificates and inventory lists to be applied for the first time, about 2600 BC, to messages and mail delivery, history, legend, mathematics, astronomical records, and other pursuits. Conjointly with the spread of writing, the first formal schools were established, usually under the auspices of a city-state's primary temple.		Finally, the Sumerians ushered in domestication with intensive agriculture and irrigation. Emmer wheat, barley, sheep (starting as mouflon), and cattle (starting as aurochs) were foremost among the species cultivated and raised for the first time on a grand scale.		Geography		Language		Coordinates: 32°00′N 45°30′E﻿ / ﻿32.0°N 45.5°E﻿ / 32.0; 45.5		
A punch line ("punch-line" or punchline) concludes a joke; it is intended to make people laugh. It is the third and final part of the typical joke structure. It follows the introductory framing of the joke and the narrative which sets up for the punch line.		In a broader sense punch line can also refer to the unexpected and funny conclusion of any performance, situation or story.						The origin of the term is unknown. Even though the comedic formula using the classic "set-up, premise, punch line" format was well-established in Vaudeville by the beginning of the 20th century, the actual term “punch line” is first documented in the 1920s; the Merriam-Webster dictionary pegs the first use in 1921.[1] Some people argue the term's origin is related to the British weekly magazine Punch.		A linguistic interpretation of the mechanics of the punch line / response is posited by Victor Raskin in his Script-based Semantic Theory of Humor. Humor is evoked when a trigger, contained in the punch line, causes the audience to abruptly shift its understanding of the story from the primary (or more obvious) interpretation to a secondary, opposing interpretation. "The punch line is the pivot on which the joke text turns as it signals the shift between the [semantic] scripts necessary to interpret [re-interpret] the joke text."[2] To produce the humor in the verbal joke, the two interpretations (i.e. scripts) need to be both compatible with the joke text AND opposite or incompatible with each other.[3] Thomas R. Shultz, a psychologist, independently expands Raskin’s linguistic theory to include "two stages of incongruity: perception and resolution." He explains that "… incongruity alone is insufficient to account for the structure of humour. […] Within this framework, humour appreciation is conceptualized as a biphasic sequence involving first the discovery of incongruity followed by a resolution of the incongruity."[4][5] Resolution generates laughter.		There are many folk theories of how people deliver punchlines, such as punchlines being louder and of a higher pitch than the speech preceding it, and a dramatic pause before the punchline is delivered.[6] In laboratory settings, however, none of these changes are employed at a statistically significant level in the production of humorous narratives.[7] Rather, the pitch and loudness of the punchline are comparable to those of the ending of any narrative, humorous or not.[8]		In order to better elucidate the structure and function of the punch line it is useful to look at some joke forms which purposely remove or avoid the punch line in their narrative. Shaggy dog stories are long-winded anti-jokes in which the punch line is deliberately anti-climactic. The humor here lies in fooling the audience into expecting a typical joke with a punch line. Instead they listen and listen to nothing funny, and end up themselves as the butt of the joke.		Another type of anti-joke is the nonsense joke, defined as having “a surprising or incongruous punch line” which provides either no resolution at all, or only a partial, unsatisfactory resolution.[9] One example of this is the No soap radio punch line. Here the anticipated resolution to the joke is absent and the audience becomes the butt of the joke.		A joke contains a single story with a single punch line at the end. In the analysis of longer humorous texts, an expanded model is needed to map the narratological structure. With this in mind, the General Theory of Verbal Humor (GTVH) was expanded to include longer humorous texts together with jokes, using the GTVH narrative structure to categorize them. A new term “jab line” was introduced to designate humor within the body of a text, as opposed to the “punch line” which is always placed at the end. The jab line is functionally identical to the punch line except it can be positioned anywhere within the text, not just at the end. “Jab and punch lines are semantically indistinguishable (…), but they differ at a narratological level.”[10] Additionally, “jab lines are humorous elements fully integrated in the narrative in which they appear (i.e. they do not disrupt the flow of the narrative, because they either are indispensable to the development of the “plot” or of the text, or they are not antagonistic to it)”.[11][12]		Using the expanded Narrative Structure of the GTVH and this new terminology of jab lines, literature and humor researchers now have a single theoretical framework with which they can analyze and map any kind of verbal humor, including novels, short stories, TV sitcoms, plays, movies as well as jokes.[13]		Felicitous jokes are often formatted in a style called AAB,[14] where a joke is made up of a set of 3, the first two of which share some common attribute, and the third represents a deviation from that attribute. Under these conditions, the third item in the set— the B— is the punchline.[15]		Rozin gives the following example as exemplifying this structure:		A Some men are about to be executed. The guard brings the first man forward, and the executioner asks if he has any last requests. He says no, and the executioner shouts, “Ready! Aim!” Suddenly the man yells, “Earthquake!” Everyone is startled and looks around. In all the confusion, the first man escapes.		A The guard brings the second man forward, and the executioner asks if he has any last requests. He says no, and the executioner shouts, “Ready! Aim!” Suddenly the man yells, “Tornado!” In the confusion, the second man escapes.		B By now the last man has it all figured out. The guard brings him forward, and the executioner asks if he has any last requests. He says no, and the executioner shouts, “Ready! Aim!” and the last man yells, “Fire!” [16]		According to this theory, the punchline is always the deviation, and it does not matter how many instances of A occur for there to be a punchline. However, jokes following the AAB structure are consistently rated as being funnier than their AB or AAAB counterparts.[17]		
Off-color humor (also known as vulgar humor or crude humor) is humor that the political correct current describes as dealing with topics that may be considered to be in poor taste or overly vulgar. Many comedic genres (including jokes, prose, poems, black comedy, blue comedy, insult comedy, cringe comedy and skits) can incorporate vulgar elements.		Most commonly labeled as "off-color" are acts concerned with sex, a particular ethnic group, or gender. Other off-color topics include violence, particularly domestic abuse; excessive swearing or profanity; "toilet humor" / scatological humor; national superiority or inferiority, pedophilic content, and any topics generally considered impolite or indecent. Generally, the point of off-color humor is to induce laughter by evoking a feeling of shock and surprise in the comedian's audience. In this way, off-color humor is related to other forms of postmodern humor, such as the anti-joke.						Off-color humor was used in Ancient Greek comedy, primarily by its most famous contributor and representative, Aristophanes. His work parodied some of the great tragedians of his time, especially Euripides, using sexual and excremental jokes that received great popularity among his contemporaries.		William Shakespeare, the 16th-century playwright and poet, is well known for his ribald humor. Almost every one of his plays contains suggestive jokes and innuendo.		Jonathan Swift, an Irish satirist in the 17th century, used scatological humor in some of his pieces, including his famous essay A Modest Proposal and his rather crude poem "The Lady's Dressing Room", in which the speaker comments on the goings-on in a 17th-century woman's room, including her business in her chamber pot.		Dirty jokes were once considered subversive and underground, and rarely heard in public. Comedian Lenny Bruce was tried, convicted, and jailed for obscenity after a stand up performance that included off-color humor in New York City in 1964. Comedian and actor Redd Foxx was well known in nightclubs in the 1960s and 1970s for his raunchy stand-up act, but toned it down for the television shows Sanford and Son and The Redd Foxx Comedy Hour, stating in the first monologue of the latter show that the only similarity between the show and his nightclub act was that "I'm smoking".[1] American society has become increasingly tolerant of off-color humor since that time. Such forms of humor have become widely distributed and more socially acceptable, in part due to the mainstream success in the 1970s and 1980s of comedians like Peter Cook and Dudley Moore's alter-egos Derek and Clive, Dolemite, and Andrew Dice Clay. George Carlin and Richard Pryor have used it as an effective tool for social commentary.		In the 1990s and modern era, comedians such as Bill Hicks, Doug Stanhope, and Dave Chappelle have used shocking content to draw attention to their criticism of social issues, especially censorship and the socioeconomic divide. Dave Attell and Louis C.K. are recognised[by whom?] as the modern masters of off-color humor that focuses on absurd topics.The cartoon Beavis and Butt-head was especially off-color in its early episodes, which included numerous depictions of animal cruelty. The highly praised television show South Park also popularized the use of offensive humor, for which the show has become infamous. The Aristocrats is perhaps the most famous dirty joke in the US and is certainly one of the best-known and most oft-repeated among comedians themselves. Tom Green has used shock humor in The Tom Green Show and the film Freddy Got Fingered, using outrageous stunts and jokes to draw an audience in.		In British humor, the genre of "sick jokes" is often used to shock by poking fun at taboo or as a reaction against political correctness. Examples include the website B3ta and its accompanying book The Bumper B3ta Book of Sick Jokes,[2] the humor wiki Sickipedia and the adult comic Viz.		In some parts of the world, sexual humor in particular is known as "non-veg" humor, contrasted with the "veg" jokes that are more acceptable in polite company. The use of the term "non-veg" is probably a reference to the carnal nature of sexual humor, and can be viewed in the context of the prevalence of both vegetarian and non-vegetarian dietary preferences in India.[3][4]		
The absent-minded professor is a stock character of popular fiction, usually portrayed as a talented academic whose academic brilliance is accompanied by below-par functioning in other areas, leading to forgetfulness and mistakes. One explanation of this is that highly talented individuals often have unevenly distributed capabilities, being brilliant in their field of choice but below average on other measures of ability. Alternatively, they are considered to be so engrossed in their field of study that they forget their surroundings.		The phrase "absent-minded professor" is also commonly used more generally in English to describe people who are so engrossed in their "own world" that they fail to keep track of their surroundings. It is a common stereotype that professors get so obsessed with their research that they pay little attention to anything else.		The stereotype is very old: the ancient Greek biographer Diogenes Laërtius wrote that the philosopher Thales walked at night with his eyes focused on the heavens and, as a result, fell down a well.[1]						Isaac Newton,[2] Adam Smith, André-Marie Ampère, Jacques Hadamard, Sewall Wright, Nikola Tesla, Norbert Wiener, Archimedes, Pierre Curie[3] and Albert Einstein[2] were all scholars considered to be absent-minded – their attention absorbed by their academic studies. William Archibald Spooner, who gave his name to the spoonerism, was known for his absent-mindedness and eccentricity.		"Doc" Emmett Brown from Back to the Future is an example of an absent-minded professor in film. Another example is the title character in the film The Absent-Minded Professor and its less successful film remakes, all based on the short story "A Situation of Gravity" by Samuel W. Taylor.		Examples in television include Professor Farnsworth in Futurama, Professor Frink in The Simpsons, Walter Bishop in the Fox television series Fringe, and Professor Von Schlemmer in Adventures of Sonic the Hedgehog. Multo, one of the characters in the children's series The Zula Patrol, is another example of an absent-minded professor.		Professor Kokintz in The Mouse That Roared by Leonard Wibberley is an example from literature. Professor Branestawm, created in the 1930s by Norman Hunter, is an earlier example of the archetype, and Jacques Paganel from the Jules Verne's 1867 novel In Search of the Castaways is probably the codifier of the archetype in the modern literature. Professor Caractacus Potts in the story of Chitty Chitty Bang Bang qualifies as an absent-minded inventor.		Comic strip examples include Professor Calculus in The Adventures of Tintin; Eli Eon in Little Orphan Annie; and Professor Edgewise, a minor recurring character in Marvel Family stories.		Isaac Kleiner from the Half-Life saga and Professor Harold MacDougal from Red Dead Redemption are examples in videogames.		The archetype is sometimes mixed with that of the mad scientist, often for comic effect, as in the Jerry Lewis film The Nutty Professor or the Profesor Bacterio in the Mortadelo y Filemón comics and movies. However, the mad scientist archetype usually has malevolent connotations, while the absent-minded professor is typically characterized as benevolent.		The fictional absent-minded professor is often a college professor of science or engineering; in the fantasy genre, a similar character may appear as a wizard. Examples of this include the characterisation of Merlin in The Sword in the Stone (particularly in the Disney adaptation) and Albus Dumbledore in the Harry Potter series.		
Telecommunication is the transmission of signs, signals, messages, words, writings, images and sounds or intelligence of any nature by wire, radio, optical or other electromagnetic systems.[1][2] Telecommunication occurs when the exchange of information between communication participants includes the use of technology. It is transmitted either electrically over physical media, such as cables, or via electromagnetic radiation.[3][4][5][6][7][8] Such transmission paths are often divided into communication channels which afford the advantages of multiplexing. Since the Latin term communicatio is considered the social process of information exchange, the term, telecommunications, is often used in its plural form because it involves many different technologies.[9]		Early means of communicating over a distance included visual signals, such as beacons, smoke signals, semaphore telegraphs, signal flags, and optical heliographs.[10] Other examples of pre-modern long-distance communication included audio messages such as coded drumbeats, lung-blown horns, and loud whistles. 20th and 21st century technologies for long-distance communication usually involve electrical and electromagnetic technologies, such as telegraph, telephone, and teleprinter, networks, radio, microwave transmission, fiber optics, and communications satellites.		A revolution in wireless communication began in the first decade of the 20th century with the pioneering developments in radio communications by Guglielmo Marconi, who won the Nobel Prize in Physics in 1909. Other notable pioneering inventors and developers in the field of electrical and electronic telecommunications include Charles Wheatstone and Samuel Morse (inventors of the telegraph), Alexander Graham Bell (inventor of the telephone), Edwin Armstrong and Lee de Forest (inventors of radio), as well as Vladimir K. Zworykin, John Logie Baird and Philo Farnsworth (some of the inventors of television).						The word telecommunication is a compound of the Greek prefix tele (τηλε), meaning distant, far off, or afar,[11] and the Latin communicare, meaning to share. Its modern use is adapted from the French,[7] because its written use was recorded in 1904 by the French engineer and novelist Édouard Estaunié.[12][13] Communication was first used as an English word in the late 14th century. It comes from Old French comunicacion (14c., Modern French communication), from Latin communicationem (nominative communicatio), noun of action from past participle stem of communicare "to share, divide out; communicate, impart, inform; join, unite, participate in," literally "to make common," from communis".[14]		Homing pigeons have occasionally been used throughout history by different cultures. Pigeon post had Persian roots, and was later used by the Romans to aid their military. Frontinus said that Julius Caesar used pigeons as messengers in his conquest of Gaul.[15] The Greeks also conveyed the names of the victors at the Olympic Games to various cities using homing pigeons.[16] In the early 19th century, the Dutch government used the system in Java and Sumatra. And in 1849, Paul Julius Reuter started a pigeon service to fly stock prices between Aachen and Brussels, a service that operated for a year until the gap in the telegraph link was closed.[17]		In the Middle Ages, chains of beacons were commonly used on hilltops as a means of relaying a signal. Beacon chains suffered the drawback that they could only pass a single bit of information, so the meaning of the message such as "the enemy has been sighted" had to be agreed upon in advance. One notable instance of their use was during the Spanish Armada, when a beacon chain relayed a signal from Plymouth to London.[18]		In 1792, Claude Chappe, a French engineer, built the first fixed visual telegraphy system (or semaphore line) between Lille and Paris.[19] However semaphore suffered from the need for skilled operators and expensive towers at intervals of ten to thirty kilometres (six to nineteen miles). As a result of competition from the electrical telegraph, the last commercial line was abandoned in 1880.[20]		Sir Charles Wheatstone and Sir William Fothergill Cooke invented the electric telegraph in 1837.[21] On July 25, 1837 the first commercial electrical telegraph was demonstrated by William Fothergill Cooke, an English inventor, and Charles Wheastone, an English scientist.[22] Both inventors viewed their device as "an improvement to the [existing] electromagnetic telegraph" not as a new device.[23]		Samuel Morse independently developed a version of the electrical telegraph that he unsuccessfully demonstrated on 2 September 1837. His code was an important advance over Wheatstone's signaling method. The first transatlantic telegraph cable was successfully completed on 27 July 1866, allowing transatlantic telecommunication for the first time.[24]		The conventional telephone was invented independently by Alexander Bell and Elisha Gray in 1876.[25] Antonio Meucci invented the first device that allowed the electrical transmission of voice over a line in 1849. However Meucci's device was of little practical value because it relied upon the electrophonic effect and thus required users to place the receiver in their mouth to "hear" what was being said.[26] The first commercial telephone services were set-up in 1878 and 1879 on both sides of the Atlantic in the cities of New Haven and London.[27][28]		In 1832, James Lindsay gave a classroom demonstration of wireless telegraphy to his students. By 1854, he was able to demonstrate a transmission across the Firth of Tay from Dundee, Scotland to Woodhaven, a distance of two miles (3 km), using water as the transmission medium.[29] In December 1901, Guglielmo Marconi established wireless communication between St. John's, Newfoundland (Canada) and Poldhu, Cornwall (England), earning him the 1909 Nobel Prize in physics (which he shared with Karl Braun).[30] However small-scale radio communication had already been demonstrated in 1893 by Nikola Tesla in a presentation to the National Electric Light Association.[31]		On 25 March 1925, John Logie Baird was able to demonstrate the transmission of moving pictures at the London department store Selfridges. Baird's device relied upon the Nipkow disk and thus became known as the mechanical television. It formed the basis of experimental broadcasts done by the British Broadcasting Corporation beginning 30 September 1929.[32] However, for most of the twentieth century televisions depended upon the cathode ray tube invented by Karl Braun. The first version of such a television to show promise was produced by Philo Farnsworth and demonstrated to his family on 7 September 1927.[33]		On 11 September 1940, George Stibitz transmitted problems for his Complex Number Calculator in New York using a teletype, and received the computed results back at Dartmouth College in New Hampshire.[34] This configuration of a centralized computer (mainframe) with remote dumb terminals remained popular well into the 1970s. However, already in the 1960s, researchers started to investigate packet switching, a technology that sends a message in portions to its destination asynchronously without passing it through a centralized mainframe. A four-node network emerged on 5 December 1969, constituting the beginnings of the ARPANET, which by 1981 had grown to 213 nodes.[35] ARPANET eventually merged with other networks to form the Internet. While Internet development was a focus of the Internet Engineering Task Force (IETF) who published a series of Request for Comment documents, other networking advancement occurred in industrial laboratories, such as the local area network (LAN) developments of Ethernet (1983) and the token ring protocol (1984).		Modern telecommunication is founded on a series of key concepts that experienced progressive development and refinement in a period of well over a century.		Telecommunication technologies may primarily be divided into wired and wireless methods. Overall though, a basic telecommunication system consists of three main parts that are always present in some form or another:		For example, in a radio broadcasting station the station's large power amplifier is the transmitter; and the broadcasting antenna is the interface between the power amplifier and the "free space channel". The free space channel is the transmission medium; and the receiver's antenna is the interface between the free space channel and the receiver. Next, the radio receiver is the destination of the radio signal, and this is where it is converted from electricity to sound for people to listen to.		Sometimes, telecommunication systems are "duplex" (two-way systems) with a single box of electronics working as both the transmitter and a receiver, or a transceiver. For example, a cellular telephone is a transceiver.[36] The transmission electronics and the receiver electronics within a transceiver are actually quite independent of each other. This can be readily explained by the fact that radio transmitters contain power amplifiers that operate with electrical powers measured in watts or kilowatts, but radio receivers deal with radio powers that are measured in the microwatts or nanowatts. Hence, transceivers have to be carefully designed and built to isolate their high-power circuitry and their low-power circuitry from each other, as to not cause interference.		Telecommunication over fixed lines is called point-to-point communication because it is between one transmitter and one receiver. Telecommunication through radio broadcasts is called broadcast communication because it is between one powerful transmitter and numerous low-power but sensitive radio receivers.[36]		Telecommunications in which multiple transmitters and multiple receivers have been designed to cooperate and to share the same physical channel are called multiplex systems. The sharing of physical channels using multiplexing often gives very large reductions in costs. Multiplexed systems are laid out in telecommunication networks, and the multiplexed signals are switched at nodes through to the correct destination terminal receiver.		Communications signals can be sent either by analog signals or digital signals. There are analog communication systems and digital communication systems. For an analog signal, the signal is varied continuously with respect to the information. In a digital signal, the information is encoded as a set of discrete values (for example, a set of ones and zeros). During the propagation and reception, the information contained in analog signals will inevitably be degraded by undesirable physical noise. (The output of a transmitter is noise-free for all practical purposes.) Commonly, the noise in a communication system can be expressed as adding or subtracting from the desirable signal in a completely random way. This form of noise is called additive noise, with the understanding that the noise can be negative or positive at different instants of time. Noise that is not additive noise is a much more difficult situation to describe or analyze, and these other kinds of noise will be omitted here.		On the other hand, unless the additive noise disturbance exceeds a certain threshold, the information contained in digital signals will remain intact. Their resistance to noise represents a key advantage of digital signals over analog signals.[37]		A telecommunications network is a collection of transmitters, receivers, and communications channels that send messages to one another. Some digital communications networks contain one or more routers that work together to transmit information to the correct user. An analog communications network consists of one or more switches that establish a connection between two or more users. For both types of network, repeaters may be necessary to amplify or recreate the signal when it is being transmitted over long distances. This is to combat attenuation that can render the signal indistinguishable from the noise.[38] Another advantage of digital systems over analog is that their output is easier to store in memory, i.e. two voltage states (high and low) are easier to store than a continuous range of states.		The term "channel" has two different meanings. In one meaning, a channel is the physical medium that carries a signal between the transmitter and the receiver. Examples of this include the atmosphere for sound communications, glass optical fibers for some kinds of optical communications, coaxial cables for communications by way of the voltages and electric currents in them, and free space for communications using visible light, infrared waves, ultraviolet light, and radio waves. Coaxial cable types are classified by RG type or "radio guide", terminology derived from World War II. The various RG designations are used to classify the specific signal transmission applications.[39] This last channel is called the "free space channel". The sending of radio waves from one place to another has nothing to do with the presence or absence of an atmosphere between the two. Radio waves travel through a perfect vacuum just as easily as they travel through air, fog, clouds, or any other kind of gas.		The other meaning of the term "channel" in telecommunications is seen in the phrase communications channel, which is a subdivision of a transmission medium so that it can be used to send multiple streams of information simultaneously. For example, one radio station can broadcast radio waves into free space at frequencies in the neighborhood of 94.5 MHz (megahertz) while another radio station can simultaneously broadcast radio waves at frequencies in the neighborhood of 96.1 MHz. Each radio station would transmit radio waves over a frequency bandwidth of about 180 kHz (kilohertz), centered at frequencies such as the above, which are called the "carrier frequencies". Each station in this example is separated from its adjacent stations by 200 kHz, and the difference between 200 kHz and 180 kHz (20 kHz) is an engineering allowance for the imperfections in the communication system.		In the example above, the "free space channel" has been divided into communications channels according to frequencies, and each channel is assigned a separate frequency bandwidth in which to broadcast radio waves. This system of dividing the medium into channels according to frequency is called "frequency-division multiplexing". Another term for the same concept is "wavelength-division multiplexing", which is more commonly used in optical communications when multiple transmitters share the same physical medium.		Another way of dividing a communications medium into channels is to allocate each sender a recurring segment of time (a "time slot", for example, 20 milliseconds out of each second), and to allow each sender to send messages only within its own time slot. This method of dividing the medium into communication channels is called "time-division multiplexing" (TDM), and is used in optical fiber communication. Some radio communication systems use TDM within an allocated FDM channel. Hence, these systems use a hybrid of TDM and FDM.		The shaping of a signal to convey information is known as modulation. Modulation can be used to represent a digital message as an analog waveform. This is commonly called "keying" – a term derived from the older use of Morse Code in telecommunications – and several keying techniques exist (these include phase-shift keying, frequency-shift keying, and amplitude-shift keying). The "Bluetooth" system, for example, uses phase-shift keying to exchange information between various devices.[40][41] In addition, there are combinations of phase-shift keying and amplitude-shift keying which is called (in the jargon of the field) "quadrature amplitude modulation" (QAM) that are used in high-capacity digital radio communication systems.		Modulation can also be used to transmit the information of low-frequency analog signals at higher frequencies. This is helpful because low-frequency analog signals cannot be effectively transmitted over free space. Hence the information from a low-frequency analog signal must be impressed into a higher-frequency signal (known as the "carrier wave") before transmission. There are several different modulation schemes available to achieve this [two of the most basic being amplitude modulation (AM) and frequency modulation (FM)]. An example of this process is a disc jockey's voice being impressed into a 96 MHz carrier wave using frequency modulation (the voice would then be received on a radio as the channel "96 FM").[42] In addition, modulation has the advantage that it may use frequency division multiplexing (FDM).		Telecommunication has a significant social, cultural and economic impact on modern society. In 2008, estimates placed the telecommunication industry's revenue at $4.7 trillion or just under 3 percent of the gross world product (official exchange rate).[43] Several following sections discuss the impact of telecommunication on society.		On the microeconomic scale, companies have used telecommunications to help build global business empires. This is self-evident in the case of online retailer Amazon.com but, according to academic Edward Lenert, even the conventional retailer Walmart has benefited from better telecommunication infrastructure compared to its competitors.[44] In cities throughout the world, home owners use their telephones to order and arrange a variety of home services ranging from pizza deliveries to electricians. Even relatively poor communities have been noted to use telecommunication to their advantage. In Bangladesh's Narshingdi district, isolated villagers use cellular phones to speak directly to wholesalers and arrange a better price for their goods. In Côte d'Ivoire, coffee growers share mobile phones to follow hourly variations in coffee prices and sell at the best price.[45]		On the macroeconomic scale, Lars-Hendrik Röller and Leonard Waverman suggested a causal link between good telecommunication infrastructure and economic growth.[46][47] Few dispute the existence of a correlation although some argue it is wrong to view the relationship as causal.[48]		Because of the economic benefits of good telecommunication infrastructure, there is increasing worry about the inequitable access to telecommunication services amongst various countries of the world—this is known as the digital divide. A 2003 survey by the International Telecommunication Union (ITU) revealed that roughly a third of countries have fewer than one mobile subscription for every 20 people and one-third of countries have fewer than one land-line telephone subscription for every 20 people. In terms of Internet access, roughly half of all countries have fewer than one out of 20 people with Internet access. From this information, as well as educational data, the ITU was able to compile an index that measures the overall ability of citizens to access and use information and communication technologies.[49] Using this measure, Sweden, Denmark and Iceland received the highest ranking while the African countries Nigeria, Burkina Faso and Mali received the lowest.[50]		Telecommunication has played a significant role in social relationships. Nevertheless, devices like the telephone system were originally advertised with an emphasis on the practical dimensions of the device (such as the ability to conduct business or order home services) as opposed to the social dimensions. It was not until the late 1920s and 1930s that the social dimensions of the device became a prominent theme in telephone advertisements. New promotions started appealing to consumers' emotions, stressing the importance of social conversations and staying connected to family and friends.[51]		Since then the role that telecommunications has played in social relations has become increasingly important. In recent years, the popularity of social networking sites has increased dramatically. These sites allow users to communicate with each other as well as post photographs, events and profiles for others to see. The profiles can list a person's age, interests, sexual preference and relationship status. In this way, these sites can play important role in everything from organising social engagements to courtship.[52]		Prior to social networking sites, technologies like short message service (SMS) and the telephone also had a significant impact on social interactions. In 2000, market research group Ipsos MORI reported that 81% of 15- to 24-year-old SMS users in the United Kingdom had used the service to coordinate social arrangements and 42% to flirt.[53]		In cultural terms, telecommunication has increased the public's ability to access music and film. With television, people can watch films they have not seen before in their own home without having to travel to the video store or cinema. With radio and the Internet, people can listen to music they have not heard before without having to travel to the music store.		Telecommunication has also transformed the way people receive their news. A 2006 survey (right table) of slightly more than 3,000 Americans by the non-profit Pew Internet and American Life Project in the United States the majority specified television or radio over newspapers.		Telecommunication has had an equally significant impact on advertising. TNS Media Intelligence reported that in 2007, 58% of advertising expenditure in the United States was spent on media that depend upon telecommunication.[55]		Many countries have enacted legislation which conforms to the International Telecommunication Regulations established by the International Telecommunication Union (ITU), which is the "leading UN agency for information and communication technology issues."[56] In 1947, at the Atlantic City Conference, the ITU decided to "afford international protection to all frequencies registered in a new international frequency list and used in conformity with the Radio Regulation." According to the ITU's Radio Regulations adopted in Atlantic City, all frequencies referenced in the International Frequency Registration Board, examined by the board and registered on the International Frequency List "shall have the right to international protection from harmful interference."[57]		From a global perspective, there have been political debates and legislation regarding the management of telecommunication and broadcasting. The history of broadcasting discusses some debates in relation to balancing conventional communication such as printing and telecommunication such as radio broadcasting.[58] The onset of World War II brought on the first explosion of international broadcasting propaganda.[58] Countries, their governments, insurgents, terrorists, and militiamen have all used telecommunication and broadcasting techniques to promote propaganda.[58][59] Patriotic propaganda for political movements and colonization started the mid-1930s. In 1936, the BBC broadcast propaganda to the Arab World to partly counter similar broadcasts from Italy, which also had colonial interests in North Africa.[58]		Modern insurgents, such as those in the latest Iraq war, often use intimidating telephone calls, SMSs and the distribution of sophisticated videos of an attack on coalition troops within hours of the operation. "The Sunni insurgents even have their own television station, Al-Zawraa, which while banned by the Iraqi government, still broadcasts from Erbil, Iraqi Kurdistan, even as coalition pressure has forced it to switch satellite hosts several times."[59]		On 10 November 2014, President Obama recommended the Federal Communications Commission reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality.[60][61]		According to data collected by Gartner[62][63] and Ars Technica[64] sales of main consumer's telecommunication equipment worldwide in millions of units was:		In a telephone network, the caller is connected to the person they want to talk to by switches at various telephone exchanges. The switches form an electrical connection between the two users and the setting of these switches is determined electronically when the caller dials the number. Once the connection is made, the caller's voice is transformed to an electrical signal using a small microphone in the caller's handset. This electrical signal is then sent through the network to the user at the other end where it is transformed back into sound by a small speaker in that person's handset.		The landline telephones in most residential homes are analog—that is, the speaker's voice directly determines the signal's voltage.[citation needed] Although short-distance calls may be handled from end-to-end as analog signals, increasingly telephone service providers are transparently converting the signals to digital signals for transmission. The advantage of this is that digitized voice data can travel side-by-side with data from the Internet and can be perfectly reproduced in long distance communication (as opposed to analog signals that are inevitably impacted by noise).		Mobile phones have had a significant impact on telephone networks. Mobile phone subscriptions now outnumber fixed-line subscriptions in many markets. Sales of mobile phones in 2005 totalled 816.6 million with that figure being almost equally shared amongst the markets of Asia/Pacific (204 m), Western Europe (164 m), CEMEA (Central Europe, the Middle East and Africa) (153.5 m), North America (148 m) and Latin America (102 m).[65] In terms of new subscriptions over the five years from 1999, Africa has outpaced other markets with 58.2% growth.[66] Increasingly these phones are being serviced by systems where the voice content is transmitted digitally such as GSM or W-CDMA with many markets choosing to deprecate analog systems such as AMPS.[67]		There have also been dramatic changes in telephone communication behind the scenes. Starting with the operation of TAT-8 in 1988, the 1990s saw the widespread adoption of systems based on optical fibers. The benefit of communicating with optic fibers is that they offer a drastic increase in data capacity. TAT-8 itself was able to carry 10 times as many telephone calls as the last copper cable laid at that time and today's optic fibre cables are able to carry 25 times as many telephone calls as TAT-8.[68] This increase in data capacity is due to several factors: First, optic fibres are physically much smaller than competing technologies. Second, they do not suffer from crosstalk which means several hundred of them can be easily bundled together in a single cable.[69] Lastly, improvements in multiplexing have led to an exponential growth in the data capacity of a single fibre.[70][71]		Assisting communication across many modern optic fibre networks is a protocol known as Asynchronous Transfer Mode (ATM). The ATM protocol allows for the side-by-side data transmission mentioned in the second paragraph. It is suitable for public telephone networks because it establishes a pathway for data through the network and associates a traffic contract with that pathway. The traffic contract is essentially an agreement between the client and the network about how the network is to handle the data; if the network cannot meet the conditions of the traffic contract it does not accept the connection. This is important because telephone calls can negotiate a contract so as to guarantee themselves a constant bit rate, something that will ensure a caller's voice is not delayed in parts or cut off completely.[72] There are competitors to ATM, such as Multiprotocol Label Switching (MPLS), that perform a similar task and are expected to supplant ATM in the future.[73][74]		In a broadcast system, the central high-powered broadcast tower transmits a high-frequency electromagnetic wave to numerous low-powered receivers. The high-frequency wave sent by the tower is modulated with a signal containing visual or audio information. The receiver is then tuned so as to pick up the high-frequency wave and a demodulator is used to retrieve the signal containing the visual or audio information. The broadcast signal can be either analog (signal is varied continuously with respect to the information) or digital (information is encoded as a set of discrete values).[36][75]		The broadcast media industry is at a critical turning point in its development, with many countries moving from analog to digital broadcasts. This move is made possible by the production of cheaper, faster and more capable integrated circuits. The chief advantage of digital broadcasts is that they prevent a number of complaints common to traditional analog broadcasts. For television, this includes the elimination of problems such as snowy pictures, ghosting and other distortion. These occur because of the nature of analog transmission, which means that perturbations due to noise will be evident in the final output. Digital transmission overcomes this problem because digital signals are reduced to discrete values upon reception and hence small perturbations do not affect the final output. In a simplified example, if a binary message 1011 was transmitted with signal amplitudes [1.0 0.0 1.0 1.0] and received with signal amplitudes [0.9 0.2 1.1 0.9] it would still decode to the binary message 1011 — a perfect reproduction of what was sent. From this example, a problem with digital transmissions can also be seen in that if the noise is great enough it can significantly alter the decoded message. Using forward error correction a receiver can correct a handful of bit errors in the resulting message but too much noise will lead to incomprehensible output and hence a breakdown of the transmission.[76][77]		In digital television broadcasting, there are three competing standards that are likely to be adopted worldwide. These are the ATSC, DVB and ISDB standards; the adoption of these standards thus far is presented in the captioned map. All three standards use MPEG-2 for video compression. ATSC uses Dolby Digital AC-3 for audio compression, ISDB uses Advanced Audio Coding (MPEG-2 Part 7) and DVB has no standard for audio compression but typically uses MPEG-1 Part 3 Layer 2.[78][79] The choice of modulation also varies between the schemes. In digital audio broadcasting, standards are much more unified with practically all countries choosing to adopt the Digital Audio Broadcasting standard (also known as the Eureka 147 standard). The exception is the United States which has chosen to adopt HD Radio. HD Radio, unlike Eureka 147, is based upon a transmission method known as in-band on-channel transmission that allows digital information to "piggyback" on normal AM or FM analog transmissions.[80]		However, despite the pending switch to digital, analog television remains being transmitted in most countries. An exception is the United States that ended analog television transmission (by all but the very low-power TV stations) on 12 June 2009[81] after twice delaying the switchover deadline,Kenya also ended analog television transmission in December 2014 after multiple delays. For analog television, there are three standards in use for broadcasting color TV (see a map on adoption here). These are known as PAL (German designed), NTSC (North American designed), and SECAM (French designed). (It is important to understand that these are the ways of sending color TV, and they do not have anything to do with the standards for black & white TV, which also vary from country to country.) For analog radio, the switch to digital radio is made more difficult by the fact that analog receivers are sold at a small fraction of the price of digital receivers.[82][83] The choice of modulation for analog radio is typically between amplitude (AM) or frequency modulation (FM). To achieve stereo playback, an amplitude modulated subcarrier is used for stereo FM.		The Internet is a worldwide network of computers and computer networks that communicate with each other using the Internet Protocol.[84] Any computer on the Internet has a unique IP address that can be used by other computers to route information to it. Hence, any computer on the Internet can send a message to any other computer using its IP address. These messages carry with them the originating computer's IP address allowing for two-way communication. The Internet is thus an exchange of messages between computers.[85]		It is estimated that the 51% of the information flowing through two-way telecommunications networks in the year 2000 were flowing through the Internet (most of the rest (42%) through the landline telephone). By the year 2007 the Internet clearly dominated and captured 97% of all the information in telecommunication networks (most of the rest (2%) through mobile phones).[86] As of 2008[update], an estimated 21.9% of the world population has access to the Internet with the highest access rates (measured as a percentage of the population) in North America (73.6%), Oceania/Australia (59.5%) and Europe (48.1%).[87] In terms of broadband access, Iceland (26.7%), South Korea (25.4%) and the Netherlands (25.3%) led the world.[88]		The Internet works in part because of protocols that govern how the computers and routers communicate with each other. The nature of computer network communication lends itself to a layered approach where individual protocols in the protocol stack run more-or-less independently of other protocols. This allows lower-level protocols to be customized for the network situation while not changing the way higher-level protocols operate. A practical example of why this is important is because it allows an Internet browser to run the same code regardless of whether the computer it is running on is connected to the Internet through an Ethernet or Wi-Fi connection. Protocols are often talked about in terms of their place in the OSI reference model (pictured on the right), which emerged in 1983 as the first step in an unsuccessful attempt to build a universally adopted networking protocol suite.[89]		For the Internet, the physical medium and data link protocol can vary several times as packets traverse the globe. This is because the Internet places no constraints on what physical medium or data link protocol is used. This leads to the adoption of media and protocols that best suit the local network situation. In practice, most intercontinental communication will use the Asynchronous Transfer Mode (ATM) protocol (or a modern equivalent) on top of optic fiber. This is because for most intercontinental communication the Internet shares the same infrastructure as the public switched telephone network.		At the network layer, things become standardized with the Internet Protocol (IP) being adopted for logical addressing. For the World Wide Web, these "IP addresses" are derived from the human readable form using the Domain Name System (e.g. 72.14.207.99 is derived from www.google.com). At the moment, the most widely used version of the Internet Protocol is version four but a move to version six is imminent.[90]		At the transport layer, most communication adopts either the Transmission Control Protocol (TCP) or the User Datagram Protocol (UDP). TCP is used when it is essential every message sent is received by the other computer whereas UDP is used when it is merely desirable. With TCP, packets are retransmitted if they are lost and placed in order before they are presented to higher layers. With UDP, packets are not ordered or retransmitted if lost. Both TCP and UDP packets carry port numbers with them to specify what application or process the packet should be handled by.[91] Because certain application-level protocols use certain ports, network administrators can manipulate traffic to suit particular requirements. Examples are to restrict Internet access by blocking the traffic destined for a particular port or to affect the performance of certain applications by assigning priority.		Above the transport layer, there are certain protocols that are sometimes used and loosely fit in the session and presentation layers, most notably the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. These protocols ensure that data transferred between two parties remains completely confidential.[92] Finally, at the application layer, are many of the protocols Internet users would be familiar with such as HTTP (web browsing), POP3 (e-mail), FTP (file transfer), IRC (Internet chat), BitTorrent (file sharing) and XMPP (instant messaging).		Voice over Internet Protocol (VoIP) allows data packets to be used for synchronous voice communications. The data packets are marked as voice type packets and can be prioritized by the network administrators so that the real-time, synchronous conversation is less subject to contention with other types of data traffic which can be delayed (i.e. file transfer or email) or buffered in advance (i.e. audio and video) without detriment. That prioritization is fine when the network has sufficient capacity for all the VoIP calls taking place at the same time and the network is enabled for prioritization i.e. a private corporate style network, but the Internet is not generally managed in this way and so there can be a big difference in the quality of VoIP calls over a private network and over the public Internet.[93]		Despite the growth of the Internet, the characteristics of local area networks (LANs)--computer networks that do not extend beyond a few kilometers—remain distinct. This is because networks on this scale do not require all the features associated with larger networks and are often more cost-effective and efficient without them. When they are not connected with the Internet, they also have the advantages of privacy and security. However, purposefully lacking a direct connection to the Internet does not provide assured protection from hackers, military forces, or economic powers. These threats exist if there are any methods for connecting remotely to the LAN.		Wide area networks (WANs) are private computer networks that may extend for thousands of kilometers. Once again, some of their advantages include privacy and security. Prime users of private LANs and WANs include armed forces and intelligence agencies that must keep their information secure and secret.		In the mid-1980s, several sets of communication protocols emerged to fill the gaps between the data-link layer and the application layer of the OSI reference model. These included Appletalk, IPX, and NetBIOS with the dominant protocol set during the early 1990s being IPX due to its popularity with MS-DOS users. TCP/IP existed at this point, but it was typically only used by large government and research facilities.[94]		As the Internet grew in popularity and its traffic was required to be routed into private networks, the TCP/IP protocols replaced existing local area network technologies. Additional technologies, such as DHCP, allowed TCP/IP-based computers to self-configure in the network. Such functions also existed in the AppleTalk/ IPX/ NetBIOS protocol sets.[95]		Whereas Asynchronous Transfer Mode (ATM) or Multiprotocol Label Switching (MPLS) are typical data-link protocols for larger networks such as WANs; Ethernet and Token Ring are typical data-link protocols for LANs. These protocols differ from the former protocols in that they are simpler, e.g., they omit features such as quality of service guarantees, and offer collision prevention. Both of these differences allow for more economical systems.[96]		Despite the modest popularity of IBM Token Ring in the 1980s and 1990s, virtually all LANs now use either wired or wireless Ethernet facilities. At the physical layer, most wired Ethernet implementations use copper twisted-pair cables (including the common 10BASE-T networks). However, some early implementations used heavier coaxial cables and some recent implementations (especially high-speed ones) use optical fibers.[97] When optic fibers are used, the distinction must be made between multimode fibers and single-mode fibers. Multimode fibers can be thought of as thicker optical fibers that are cheaper to manufacture devices for, but that suffers from less usable bandwidth and worse attenuation – implying poorer long-distance performance.[98]		The effective capacity to exchange information worldwide through two-way telecommunication networks grew from 281 petabytes of (optimally compressed) information in 1986, to 471 petabytes in 1993, to 2.2 (optimally compressed) exabytes in 2000, and to 65 (optimally compressed) exabytes in 2007.[86] This is the informational equivalent of two newspaper pages per person per day in 1986, and six entire newspapers per person per day by 2007.[99] Given this growth, telecommunications play an increasingly important role in the world economy and the global telecommunications industry was about a $4.7 trillion sector in 2012.[43][100] The service revenue of the global telecommunications industry was estimated to be $1.5 trillion in 2010, corresponding to 2.4% of the world’s gross domestic product (GDP).[43]		
Gelotology (from the Greek γέλως gelos "laughter")[1] is the study of laughter and its effects on the body, from a psychological and physiological perspective. Its proponents often advocate induction of laughter on therapeutic grounds in alternative medicine. The field of study was pioneered by William F. Fry of Stanford University.[2]						Gelotology was first studied by psychiatrists, although some doctors in antiquity recommended laughter as a form of medicine. It was initially deprecated by most other physicians, who doubted that laughter possessed analgesic qualities. One early study that demonstrated the effectiveness of laughter in a clinical setting showed that laughter could help patients with atopic dermatitis respond less to allergens.[3] Other studies have shown that laughter can help alleviate stress and pain, and can assist cardiopulmonary rehabilitation.[4]		Several types of therapy have emerged which use laughter to help patients.		
The Westcar Papyrus (inventory-designation: P. Berlin 3033) is an ancient Egyptian text containing five stories about miracles performed by priests and magicians. In the text, each of these tales are told at the royal court of Pharaoh Cheops (4th dynasty) by his sons. The story in the papyrus is usually rendered in English as "King Cheops and the Magicians"[1] and "The Tale of King Cheops' Court".[2] In German, in which the text of the Westcar Papyrus was first translated, it is rendered as Die Märchen des Papyrus Westcar ("the fairy tales of Papyrus Westcar").[3][4]		The surviving material of the Westcar Papyrus consists of twelve columns written in hieratic script. Miriam Lichtheim dates the document to the Hyksos period (18th to 16th century BC) and states that it is written in classical Middle Egyptian.[5] Linguist and Egyptologist Verena Lepper thinks it may be possible that the Westcar Papyrus had already been written during the 13th dynasty. The papyrus has been used by historians as a literary resource for reconstituting the history of the 4th dynasty.		The papyrus is now on display under low-light conditions in the Egyptian Museum of Berlin.[4]						In 1823 or 1824, a British adventurer, Henry Westcar, apparently discovered the papyrus during travels in Egypt. For unknown reasons he didn't note the exact circumstances under which he obtained the artifact.		In 1838 or 1839, German Egyptologist Karl Richard Lepsius claimed to have received the papyrus from Westcar's niece. As Lepsius was able to read some signs of Hieratic, he recognized some of the royal cartouche names of the kings and dated the text to the old kingdom.		There are inconsistencies about the true nature of the acquisition and the subsequent whereabouts of the Westcar Papyrus. Lepsius writes that the document was on display in the Oxford Bodleian Library, but public exhibitions have been documented there since the early 1860s and Lepsius' name does not appear in any lists or documents. Furthermore, Lepsius never made the text of the Westcar Papyrus public; he stored the papyrus at home in the attic, where it was found later after his death. These inconsistencies led to wide speculation; Many British historians speculate that Lepsius may have stolen the papyrus.[6]		In 1886, German Egyptologist Adolf Erman purchased the papyrus from Lepsius' son and left it to the Museum of Berlin. As the hieratic signs were still insufficiently investigated and translated, the Westcar Papyrus was displayed as some kind of curiosity. Since Erman's first attempt at a complete translation in 1890, the Westcar Papyrus has been translated numerous times, resulting in different outcomes. The dating of the text also varies.[7]		Papyrus Westcar is a reused papyrus made of the plant Cyperus papyrus. The scroll of Westcar has been separated into three parts. During the life of Lepsius and Erman it was in two parts; it is not known when and why the scroll was separated into three fragments. The text written on the papyrus includes twelve columns in all. The first part contains on the recto (the front) columns one to three, the second part contains on its recto columns four and five and the third part contains on the verso (the back) columns six to nine and on the recto, the final columns, ten to twelve. The papyrus textile is grainy, of greyish-yellowish colour and very fragile. Part one was fixed onto linen and placed between two glass panes. At five spots the papyrus was fixed to the glass with methyl cellulose. Part two was fixed to a cardboard and wooden plate and is covered by a glass pane. Part three was simply placed between two glass panes and was completely glued to them. The adhesive used for this has partially lost its transparency and a whitish haze has appeared. The edges of all three parts were left free for air circulation. Because of the paper lamination during the 18h century, all the papyrus fragments are partially damaged; at several spots the material is torn, distorted and squashed. Some of the fibres are now lying over the inscription. All of the artifact shows large gaps and the rim of the scrolls is badly frayed. Because of the gaps, many parts of the text are now missing.		The text itself is completely written in black iron gall ink and carbon black ink and divided by rubra into ten paragraphs. Between the neatly written sentences red traces of an older text are visible. It looks like Papyrus Westcar is a Palimpsest; the unknown ancient Egyptian author obviously tried but partially failed to wipe the older text off. The clean and calligraphical handwriting shows that the author was a highly educated professional.[8]		The first story, told by an unknown son of Khufu (possibly Djedefra), is missing everything but the conclusion, in which Khufu orders blessed offerings to king Djoser. It seems to have been a text detailing a miracle performed by a lector priest in the reign of king Djoser, possibly the famous Imhotep himself.[3][4][9]		The second story, told by Khafra, is set during the reign of one of Khufu's predecessors. King Nebka's chief lector Ubaoner finds that his wife is having a love affair with a townsman of Memphis, and he fashions a crocodile in wax. Upon learning that his unfaithful wife is meeting her lover, he casts a spell for the figurine to come to life upon contact with water, and sets his caretaker to throw it in the stream by which the townsman enters and leaves the lector's estate undiscovered. Upon catching the townsman, the crocodile takes him to the bottom of the lake, where they remain for seven days as the lector entertains the visiting pharaoh. When he tells Nebka the story, and calls the crocodile up again, the king orders the crocodile to devour the townsman once and for all. Then he has the adulterous wife brought forth, set on fire and thrown in the river.[3][4][9]		The third story, told by another son named Baufra, is set during the reign of his grandfather Sneferu. The king is bored and his chief lector Djadjaemankh advises him to gather twenty young women and use them to sail him around the palace lake. Sneferu orders twenty beautiful oars made, and gives the women nets to drape around them as they sail. However, one of the girls loses an amulet - a fish pendant made of malachite so dear to her that she will not even accept a substitute from the royal treasury, and until it is returned to her neither she nor any of the other girls will row. The king laments this, and the chief lector folds aside the water to allow the retrieval of the amulet, then folds the water back.[3][4][9]		The fourth story, told by Hordjedef, concerns a miracle set within Khufu's own reign. A townsman named Dedi apparently has the power to reattach a severed head onto an animal, to tame wild lions, and knows the number of secret rooms in the shrine of Thoth. Khufu, intrigued, sends his son to invite this wise man to the court, and upon Dedi's arrival he orders a goose, an undefined waterbird, and a bull beheaded. Dedi reattaches the heads. Khufu then questions him on his knowledge on the shrine of Thoth, and Dedi answers that he does not know the number of rooms, but he knows where they are. When Khufu asks for the wheres and hows, Dedi answers that the one who can give Khufu access is not him, but the first of the three future kings in the womb of the woman Rededjet. This is a prophecy detailing the beginnings of the Fifth dynasty, starting with Userkaf.[3][4][9]		The final story breaks from the format and moves the focus to Rededjet giving birth to her three sons. Upon the day of her children's birth, Ra orders Isis, Nephthys, Meskhenet, Heket and Khnum to aid her. They disguise themselves as musicians and hurry to Reddedet's house to help her with the difficult birth. The three children are born, each described as strong and healthy, with limbs covered in gold and wearing headdresses of lapis lazuli. The maid servant of Rededjet later has an argument with her mistress, receives a beating and flees, vowing to tell king Khufu what had happened. But on the way, she meets her brother and tells the story to him. Displeased, he beats her too, and sends her path to the water's edge where a crocodile catches her. The brother then goes to see Rededjet, who is crying over the loss of the girl. The brother starts to confess what has happened and at this point the papyrus ends.[3][4][9]		Papyrus Westcar is of the highest interest to historians and Egyptologists, since it is one of the oldest Egyptian documents that contains such a complex novel. Unfortunately, the name of the author has been lost. The most recent translations and linguistic investigations by Miriam Lichtheim and Verena Lepper reveal interesting writing and spelling elements hidden in the text of the Papyrus, which has led them to a new evaluation of the individual stories.[4][9]		The first story is lost due to the damage to the papyrus. The preserved sentences merely reveal the main protagonist of the story, King Djoser. The name of the hero, who is said to have performed the miracle, is completely lost, but Liechtheim and Lepper think it possible that the Papyrus was talking about the famous architect and high lector priest Imhotep.[4][9]		The second and third story are written in a conspicuous, flowery, old-fashioned style, and the author has obviously tried to make the novels sound as if handed down from a long time ago but fantastic at the same time. He uses quaint phrases and makes the heroes' acting stilted and ceremonious. All three initial stories are written in past tense and all the kings are addressed with the salutation "justified" (Egyptian: maa´-cheru), which was typical in Ancient Egypt when talking about a deceased king. The heroes are addressed in the preserved stories II. and III. alike. Curiously, all kings are addressed with their birth name, notwithstanding that this was actually unusual in the author's lifetime. Deceased kings were normally always called by their personal name; living kings were called by their horus name. King Khufu is nevertheless called by his birth name yet in story IV., where he is treated as being still alive and being himself the main actor. And even the future kings Userkaf, Sahure and Neferirkare-Kakai are called by their birthname. Verena Lepper thinks, that the reason may be some kind of spelling reform which occurred in the lifetime of the author: he tried to fix the spelling rule for naming a deceased king at all costs to show that even the future kings are long since dead during his own lifetime. For this reason Verena Lepper doubts that the novel of Westcar is based on documents originating from the Old Kingdom itself.[10]		Story IV. and V. are written in present tense. The unknown author moves the timeline and also changes his mode of expression from "old-fashioned" into a contemporary term. He clearly distinguishes "long time passed" from "most recently" without cutting the timeline too quickly. The speech of prince Hordjedef builds the decisive transition: Hordjedef is sick of hearing old, dusty tales that cannot be proven in any way. He explains that a now-happening wonder would be richer in content and more instructive, and so he brings up the story of Dedi. The last sequel of the fourth story, in which the magician Dedi gives a prophecy to king Khufu, moves to the future tense for a short time, before moving back to present tense again. This present tense is maintained until the end of the Westcar-story.[4][9]		Papyrus Westcar contains hidden allusions and puns to the characters of the kings Nebka, Sneferu and Khufu. An evaluation of the character description of Djoser is impossible due to the great deterioration of his story.		In story II. king Nebka plays the key-role. He is depicted there as a strict, but lawful judge, who doesn't allow mischief and misbehaviour to occur. The adulterous wife of the story's hero is punished by being burnt alive and her secret lover, revealed thanks to the loyal caretaker, is eaten alive by a summoned crocodile. Caretaker and crocodile are playing the role of justice, whilst king Nebka plays the role of the destiny. Lepper and Liechtheim evaluate the depiction of king Nebka as being fairly positive. A strict but lawful pharaoh was ideal for the people of the author's lifetime.[4][9]		In story III. king Sneferu becomes a victim of the author's courage to criticize the monarchy. He depicts Sneferu as a fatuous fool, who is easily pleased with superficial entertainment and who is unable to resolve a dispute with a little rowing maid. Sneferu must go to the extent of having a priest solve the problem. With this narration and embarrassing depiction of a pharaoh, the author of Westcar dares to criticise the pharaohs of Egypt as such and makes story III. a sort of satire. Lepper points out that the critics are hidden cleverly throughout. It is no wonder, since the author had to be careful—the Westcar Papyrus was possibly made available for public entertainment, or at least, for public study.[4][9]		In story IV. king Khufu is difficult to assess. At one side he is depicted as ruthless: deciding to have a condemned prisoner become decapitated to test the alleged magical powers of the magician Dedi. On the other side Khufu is depicted as inquisitive, reasonable and generous: He accepts the outrage and alternative offer of Dedi for the prisoner, questions the circumstances and contents of Dedi's prophecy; he rewards the magician generously. The contradictory depiction of Khufu is an object of controversy between Egyptologists and historians up to this day. Earlier Egyptologists and historians in particular, such as Adolf Erman, Kurt Heinrich Sethe and Wolfgang Helck evaluated Khufu's character as heartless and sacrilegious. They lean on the ancient Greek traditions of Herodot and Diodor, who described an exaggerated negative character image of Khufu, ignoring the paradoxical (because positive) traditions the Egyptians themselves always taught. But other Egyptologists such as Dietrich Wildung see Khufu's order as an act of mercy: the prisoner would have received his life back if Dedi had actually had performed his magical trick. Wildung thinks that Dedi's refusal was an allusion to the respect Egyptians showed to human life. The ancient Egyptians were of the opinion that human life should not be misused for dark magic or similar evil things. Lepper and Liechtheim suspect that a difficult-to-assess depiction of Khufu was exactly what the author had planned. He wanted to create a mysterious character.[3][4][9][11][12]		The fifth and last story tells about the female hero Rededjet (her name is also often read as Ruddedet) and her difficult birth of three sons. The sun god Ra orders his companions Isis, Meskhenet, Hekhet, Nephthys and Khnum to help Rededjet, to ensure the birth of the triplets and the beginning of a new dynasty. Lepper and Liechtheim both evaluate the story as some kind of narrated moral which deals with the theme of justice and what happens to traitors. Lepper points out, that the story of Rededjet might have been inspired by the historical figure of queen Khentkaus I, who lived and maybe ruled at the end of the Fourth dynasty.[13] Khentkaus I is demonstrably entitled as the "mother of two kings" and for a long time it has been thought that she may have borne Userkaf and Sahure. But new evidence shows that at least Sahure had a different mother (Queen Neferhetepes), the implication of the Westcar Papyrus that the first three kings of the fifth dynasty had been siblings, is therefore incorrect.[14] Since in the Westcar Papyrus Rededjet was concerned with the role of a future king's mother, the parallels between the biographies of the two ladies aroused special attention. The role of the maidservant is evaluated as being a key figure for a modern phrasing of indoctrinations about morality and betrayal. The maidservant wants to run her mistress down and is punished by destiny. The destiny is depicted here as a crocodile which snatches the traitor. The whole purpose would be to ensure the beginning of a new dynasty and by making the only danger disappear. The author of the Westcar Papyrus artfully creates some kind of happy ending.[13]		Since the first translations of the Westcar-Papyrus historians and Egyptologists dispute whether the story was finished or unfinished. Earlier evaluations seemed to show an abrupt ending after the death of the traitorous maid servant. But more recently linguistic investigations made by Verena Lepper and Miriam Liechtheim (especially by the first one) strengthen the theory that the novel of pWestcar is definitely over after the story of the maid servant's death. Lepper points out that the crocodile sequence is repeated several times, like a kind of refrain, a typical writing element in similar stories and documents. Furthermore, Lepper argues that the papyrus leaves a lot of free space after the ending, enough for a further short story.[3][9][13]		Verena Lepper and Miriam Lichtheim postulate that the tales of Papyrus Westcar inspired later authors to compose and write down similar novels. They refer to multiple and somewhat later ancient Egyptian writings in which magicians perform very similar magic tricks and make prophecies to a king. Descriptive examples are the papyri pAthen and The prophecy of Neferti. These novels show the popular theme of prophesying used during the Old Kingdom – just like in the story of the Westcar Papyrus. They also both talk about subalterns with magical powers similar to those of Dedi's. The Papyrus pBerlin 3023 contains the novel The Eloquent Peasant, in which the following phrase appears: "See, these are artists who create the existing anew, who even replace a severed head", which could be interpreted as an allusion to the Westcar Papyrus. pBerlin 3023 contains another reference which strengthens the idea that many ancient Egyptian novels were influenced by Westcar Papyrus: column 232 contains the phrase "sleeping until dawn", which appears nearly word-by-word in the Westcar Papyrus.		A further descriptive example appears in The prophecy of Neferti. Like in pWestcar, a subaltern is addressed by a king with "my brother" and the king himself is depicted as being accostable and simple-minded. Furthermore, both novels talk about the same king: pharaoh Sneferu. The Papyrus pAthen contains the phrase: "...for these are the wise who can move waters and make a river flow at their mere will and want...", which clearly refers to the wonder that the magicians Djadjaemankh and Dedi had performed in pWestcar.		Since pAthen, pBerlin 3023 and The prophecy of Neferti use the same manner of speaking and quaint phrases, equipped with numerous allusions to the wonders of Papyrus Westcar, Lepper and Lichtheim hold that Dedi, Ubaoner and Djadjaemankh must have been known to Egyptian authors for a long time.[4][9]		
John Skelton, also known as John Shelton (c. 1463 – 21 June 1529), possibly born in Diss, Norfolk, was an English poet and tutor to King Henry VIII of England. Skelton died in Westminster and was buried in St. Margaret's Church, although no trace of the tomb remains.[1]						Skelton is said to have been educated at Oxford,[2] though it is documented that he studied at Cambridge.[3] He could be the "one Scheklton" mentioned by William Cole as taking his M.A. degree at Cambridge in 1484, but this is unconfirmed. In 1490, William Caxton, in the preface to The Boke of Eneydos compyled by Vyrgyle, refers to him as though Skelton already had a scholarly reputation when the book was published. "But I pray mayster John Skelton," he says, "late created poete laureate in the unyversite of Oxenforde, to oversee and correct this sayd booke ... for him I know for suffycyent to expowne and englysshe every dyffyculte that is therin. For he hath late translated the epystlys of Tulle, and the boke of dyodorus siculus, and diverse other works... in polysshed and ornate termes craftely... suppose he hath drunken of Elycons well."		The laureateship referred to was a degree in rhetoric. As well as Oxford, in 1493 Skelton received the same honour at Cambridge, and also at Leuven.[2] He found a patron in the pious and learned Countess of Richmond, Henry VII's mother, for whom he wrote Of Mannes Lyfe the Peregrynacioun, a lost translation of Guillaume de Diguileville's "Pèlerinage de la vie humaine."[4] An elegy "Of the death of the noble prince Kynge Edwarde the forth," included in some of the editions of the Mirror for Magistrates, and another (1489) on the death of Henry Percy, fourth earl of Northumberland, are among his earliest poems.		In the last decade of the 15th century he was appointed tutor to Prince Henry (afterwards King Henry VIII of England).[2] He wrote for his pupil a lost Speculum principis, and Erasmus, in 1500, dedicated an ode to the prince speaking of Skelton as "unum Britannicarum literarum lumen ac decus." This Latin phrase roughly translates as "the one light and glory of British letters."[citation needed] In 1498 he was successively ordained sub-deacon, deacon and priest.[2] He seems to have been imprisoned in 1502, but no reason is known for his disgrace. It has been said[by whom?] that he offended Wolsey but this would be impossible if the date is correct, given Wolsey was not yet an influential figure at court – Wolsey's rise began in 1508. Two years later he retired from regular court attendance to become rector of Diss, a benefice he retained nominally until his death.[2]		Skelton frequently signed himself "regius orator" and poet-laureate, but there is no record of any emoluments paid in connection with these dignities, although the Abbé du Resnel, author of "Recherches sur les poètes couronnez," asserts that he had seen a patent (1513–1514) in which Skelton was appointed poet-laureate to Henry VIII. As rector of Diss he caused great scandal among his parishioners, who thought him, says Anthony Wood,[citation needed] more fit for the stage than the pew or the pulpit. He was secretly married to a woman who lived in his house, and earned the hatred of the Dominican monks by his fierce satire. He consequently came under the formal censure of Richard Nix, the bishop of the diocese, and appears to have been temporarily suspended. After his death a collection of farcical tales, no doubt chiefly, if not entirely, apocryphal, gathered round his name—The Merie Tales of Skelton.		During the rest of the century he figured in the popular imagination as an incorrigible practical joker. His sarcastic wit made him enemies, among them: Sir Christopher Garnesche or Garneys, Alexander Barclay, William Lilly and the French scholar, Robert Gaguin (c. 1425–1502). With Garneys he engaged in a regular "flyting," undertaken, he says,[citation needed] at the king's command, but Skelton's four poems read as if the abuse in them were dictated by genuine anger. Earlier in his career he found a friend and patron in Cardinal Wolsey, and the dedication to the cardinal of his Replycacion is couched in the most flattering terms. But in 1522, when Wolsey in his capacity of Papal legate dissolved convocation at St Paul's, Skelton put in circulation the couplet:		Gentle Paul, laie doune thy sweard For Peter of Westminster hath shaven thy beard.		In Colyn Cloute he incidentally attacked Wolsey in a general satire on the clergy. Speke, Parrot and Why Come Ye nat to Courte? are direct and fierce invectives against the cardinal who is said[by whom?] to have more than once imprisoned the author. To avoid another arrest Skelton took sanctuary in Westminster Abbey. He was kindly received by the abbot, John Islip, who continued to protect him until his death. According to his biographer, Edward Braynewood, Skelton was buried before the high altar of Saint Margaret's Church with this inscription on alabaster: Joannes Skeltonus vates pierius hic situs est (Here lies John Skelton, Pierian bard).[4] It is thought[by whom?] that Skelton wrote Why Come Ye nat to Courte? having been inspired by Sir Thomas Spring, a merchant in Suffolk who had fallen out with Wolsey over tax.		In his Garlande of Laurell Skelton gives a long list of his works, only a few of which are extant. The garland in question was worked for him in silks, gold and pearls by the ladies of the Countess of Surrey at Sheriff Hutton Castle, where he was the guest of the duke of Norfolk. The composition includes complimentary verses to the various ladies, and a good deal of information about himself. But it is as a satirist that Skelton merits attention. The Bowge of Court is directed against the vices and dangers of court life. He had already in his Boke of the Thre Foles drawn on Alexander Barclay's version of the Narrenschijf of Sebastian Brant, and this more elaborate, imaginative poem belongs to the same class.		Skelton, falling into a dream at Harwich, sees a stately ship in the harbour called the Bowge of Court, the owner of which is the "Dame Saunce Pere." Her merchandise is Favour; the helmsman Fortune; and the poet, who figures as Drede (modesty), finds on board F'avell (the flatterer), Suspect, Harvy Hafter (the clever thief), Dysdayne, Ryotte, Dyssymuler and Subtylte. These figures explain themselves in turn, until at last Drede, who finds they are secretly his enemies, is about to save his life by jumping overboard, when he wakes with a start. Both poems are written in the seven-lined Rhyme Royal, a Continental verse-form first used in English by Chaucer, but it is in an irregular metre of his own—known as "Skeltonics" —that his most characteristic work was accomplished.[citation needed]		The Boke of Phyllyp Sparowe, the lament of Jane Scroop, a schoolgirl in the Benedictine convent of Carrow near Norwich, for her dead bird, was no doubt inspired by Catullus.[2] It is a poem of some 1,400 lines and takes many liberties with the formularies of the church. The digressions are considerable. It depicts Jane as having a wide reading in the romances of Charlemagne, of the Round Table, The Four Sons of Aymon and the "Trojan cycle." Skelton finds space to give an opinion of Geoffrey Chaucer, John Gower and John Lydgate. Whether we can equate this opinion, voiced by the character of Jane, with Skelton's own is contentious. It would appear that he seems to have realised Chaucer's value as a master of the English language. Gower's matter was, Jane tells us, "worth gold," but his English she regards as antiquated. The verse in which the poem is written, called from its inventor "Skeltonical," is here turned entirely to whimsical use. The lines are usually six-syllabled but vary in length, and rhyme in groups of two, three, four and even more. It is not far removed from the old alliterative English verse, and well fitted to be chanted by the minstrels who had sung the old ballads. For its comic admixture of Latin Skelton had abundant example in French and Low Latin macaronic verse. He makes frequent use of Latin and French words to carry out his exacting system of frequently recurring rhymes. This breathless, voluble measure was in Skelton's energetic hands an admirable vehicle for invective, but it easily degenerated into doggerel.		By the end of the 16th century he was a "rude rayling rimer" (Puttenham, Arte of English Poesie), and at the hands of Pope and Warton he fared even worse. His own criticism is a just one:		For though my ryme be ragged, Tattered and jagged, Rudely rayne beaten, Rusty and moughte eaten, It hath in it some pyth.		Colyn Cloute represents the average country man who gives his opinions on the state of the church. It is an indictment of the sins of the clergy before the Reformation.[2][5][6] He exposes their greed and ignorance, the ostentation of the bishops and the common practice of simony, taking care to explain the accusations do not include all and that he writes in defence of the church. He repeatedly, indirectly hits at Wolsey in this satire. Speke, Parrot has only been preserved in a fragmentary form, and is very obscure. It was apparently composed at different times, but in the latter part of the composition he openly attacks Wolsey. In Why Come Ye nat to Courte? there is no attempt at disguise. The wonder is not that Skelton had to seek sanctuary, but that he had any opportunity of doing so. He rails at Wolsey's ostentation, at his almost royal authority, his overbearing manner to suitors high and low, and taunts him with his mean extraction. This scathing invective was not allowed to be printed in the cardinal's lifetime, but no doubt widely circulated in manuscript and by repetition. The charge of coarseness regularly brought against Skelton is based chiefly on The Tunnynge of Elynoare Rummynge, a realistic description in the same metre of the drunken women who gathered at a well-known ale-house kept by Elynour Rummynge at Leatherhead, not far from the royal palace of Nonsuch.		"Skelton Laureate against the Scottes" is a fierce song of triumph celebrating the victory of Flodden. "Jemmy is ded And closed in led, That was theyr owne Kynge," says the poem; but there was an earlier version written before the news of James IV's death had reached London. This, the earliest singly printed ballad in the language, was entitled A Ballade of the Scottysshe Kynge, and was rescued in 1878 from the wooden covers of a copy of Huon de Bordeaux. "Howe the douty Duke of Albany, lyke a cowarde knight" deals with the Campaign of 1523, and contains a panegyric of Henry VIII. To this is attached an envoi to Wolsey, but it surely was misplaced, for both satires on the cardinal are of earlier date.[2]		Skelton also wrote three plays, only one of which survives. Magnificence is one of the best examples of the morality play. It deals with the same topic as his satires - the evils of ambition. The play's moral, namely "how suddenly worldly wealth doth decay," was a favourite with him. Thomas Warton in his History of English Poetry described another piece titled Nigramansir, printed by Wynkyn de Worde in 1504. It deals with simony and the love of money in the church; but no copy is known to exist, and suspicion has been cast on Warton's statement.[2]		Illustration of Skelton's hold on public imagination is supplied from the stage. A play (1600) called Scogan and Shelton, by Richard Hathwaye and William Rankins, is mentioned by Henslowe. In Anthony Munday's Downfall of Robert, Earl of Huntingdon, Skelton acts the part of Friar Tuck, and Ben Jonson in his masque, The Fortunate Isles, introduced Skogan and Skelton in like habits as they lived.		Very few of Skelton productions are dated; their titles are here necessarily abbreviated. De Worde printed the Bowge of Court twice. Divers Batettys and dyties salacious devysed by Master Shelton Laureat, and Shelton Laureate agaynste a comely Coystroune have no date or printer's name, but are evidently from the press of Richard Pynson, who also printed Replycacion against certain yang scalers, dedicated to Wolsey. The Garlande or Chapelet of Laurell was printed by Richard Faukes (1523); Magnificence, A goodly interlude, probably by John Rastell about 1533, reprinted (1821) for the Roxburghe Club. Hereafter foloweth the Boke of Phyllyp Sparowe was printed by Richard Kele (1550?), Robert Toy, Antony Kitson (1560?), Abraham Veale (1570?), John Walley, John Wyght (1560?). Hereafter foloweth certaine bokes compyled by mayster Shelton ... including "Speke, Parrot", "Ware the Hawke", "Elynoure Rumpiynge and others", was printed by Richard Lant (1550?), John King and Thomas March (1565?), and by John Day (1560). Hereafter foloweth a title boke called Colyn Cloute and Hereafter ... Why Come Ye nat to Courte? were printed by Richard Kele (1550?) and in numerous subsequent editions. Pithy, plesaunt and profitable workes of maister Shelton, Poete Laureate. Nowe collected and newly published was printed in 1568, and reprinted in 1736. A scarce reprint of Filnour Rummin by Samuel Rand appeared in 1624.		Five of Skelton's "Tudor Portraits", including The Tunnying of Elynour Rummyng were set to music by Ralph Vaughan Williams in or around 1935. Although he changed the text to suit his music, the sentiments are well expressed. The four others are "My pretty Bess", "Epitaph of John Jayberd of Diss", "Jane Scroop (her lament for Philip Sparrow)", and "Jolly Rutterkin." The music is rarely performed, although it is considered funny,[by whom?] and captures the coarseness of Skelton in an inspired way.		See The Poetical Works of John Shelton; with Notes and some account of the author and his writings, by the Rev. Alexander Dyce (2 vols., 1843). A selection of his works was edited by WH Williams (London, 1902). See also Zur Charakteristik John Skeltons by Dr Arthur Koelbing (Stuttgart, 1904); F Brie, "Skelton Studien" in Englische Studien, vol. 38 (Heilbronn, 1877, etc.); A Rey, Skelton's Satirical Poems... (Berne, 1899); A Thummel, Studien über John Skelton (Leipzig-Reudnitz, 1905); G Saintsbury, Hist. of Eng. Prosody (vol. i, 1906); and A Kolbing in the Cambridge History of English Literature (vol. iii, 1909).		John Skelton's lineage is difficult to prove.[citation needed] Some scholars have thought he may have been related to Sir John Shelton and his children, who also came from Norfolk.[7] Sir John's daughter, Mary Shelton, was a mistress of Henry VIII's during the reign of her cousin, Anne Boleyn. Mary Shelton was the main editor and contributor to the Devonshire MS, a collection of poems written by various members of the court.		It is said that several of Skelton's works were inspired by women who were to become mothers to two of Henry VIII's six wives.[8] Elizabeth Boleyn (Howard), Countess of Wiltshire and Ormonde, was said to be so beautiful that Skelton compared her to Cressida. This comparison may have been a double entendre, because Cressida, as depicted by Chaucer in his work Troilus and Criseyde, was notable as a symbol of female inconstancy.[9] A popular but unverifiable legend suggests several poems were inspired by Margery Wentworth; she is noted as one of the women portrayed in Skelton's Garland of Laurels. She also is reported as having an eponymous poem written in her honour by Skelton.[10] Elizabeth was the mother of Anne Boleyn, Henry's second wife; Margery was the mother of his third, Jane Seymour.		
Jokes and Their Relation to the Unconscious (German: Der Witz und seine Beziehung zum Unbewußten)[1] is a book on the psychoanalysis of jokes and humour by Sigmund Freud (1856-1939), first published in 1905 (translated into English in 1960).[2] In this work, Freud described the psychological processes and techniques of jokes, which he likened as similar to the processes and techniques of dreamwork and the Unconscious.[3]		The book is referenced by the character Alvy in the opening seconds of Annie Hall, which won an Oscar for Best Original Screenplay.[4]						In Jokes and Their Relation to the Unconscious, Freud claimed that "our enjoyment of the joke" indicates what is being repressed in more serious talk.[5] Freud argues that the success of the joke depends upon a psychic economy, whereby the joke allows one to overcome inhibitions.[6]		According to Freud, understanding of joke technique is essential for understanding jokes and their relation to the unconscious, however, these techniques are what make a joke a joke.[7] Freud also noted that the listener laughing really heartily at the joke will typically not be in the mood for investigating its technique.[8]		The book is divided into three sections: "analytic," "synthetic" and "theoretical."		The book's first section includes a discussion on the techniques and tendencies of jokes.		The second section includes a discussion on the psychological origins and motives of the joke and the joke as a social process.		The book's final section discusses the joke's relation to dreams and the Unconscious.		
The Aarne–Thompson classification systems are indices used to classify folktales: the Aarne–Thompson Motif-Index (catalogued by alphabetical letters followed by numerals), the Aarne–Thompson Tale Type Index (cataloged by AT or AaTh numbers), and the Aarne–Thompson–Uther classification system (developed in 2004 and cataloged by ATU numbers). The indices are used in folkloristics to organize, classify, and analyze folklore narratives and are essential tools for folklorists because, as Alan Dundes explained in 1997 about the first two indices, "the identification of folk narratives through motif and/or tale type numbers has become an international sine qua non among bona fide folklorists".[1]						The Aarne–Thompson Tale Type Index divides tales into sections with an AT number for each entry. The names given are typical, but usage varies; the same tale type number may be referred to by its central motif or by one of the variant folktales of that type, which can also vary, especially when used in different countries and cultures. The tale type does not have to be accurate for every folktale. For example, The Cat as Helper (545B) also includes tales where a fox helps the hero. Closely related folktales are often grouped within a type. For example, tale types 400–424 all feature brides or wives as the primary protagonist, for instance The Quest for a Lost Bride (400) or the Animal Bride (402). Subtypes within a tale type are designated by the addition of a letter to the AT number, for instance: the Persecuted Heroine (510) has subtypes 510A, Cinderella, and 510B, Catskin (see other examples of tale types in the online resource links at the end of this article).		The Aarne–Thompson Motif-Index organizes thousands of motifs into a similar system. Entries are first organized by an umbrella topic (for example, category S is "Unnatural Cruelty"). Entries are then divided into more specific subcategories. For example, entry S50 "Cruel relatives-in-law", under which is the more specific entry S51.1 "Cruel mother-in-law plans death of daughter-in-law".		The AT-number system was updated and expanded in 2004 with the publication of The Types of International Folktales: A Classification and Bibliography by Hans-Jörg Uther. Uther noted that many of the earlier descriptions were cursory and often imprecise, that many "irregular types" are in fact old and widespread, and that "emphasis on oral tradition" often obscured "older, written versions of the tale types".[2] To remedy these shortcomings Uther developed the Aarne–Thompson–Uther classification (ATU) system[3] and included more tales from eastern and southern Europe as well as "smaller narrative forms" in this expanded listing.[2]		In his essay "The Motif-Index and the Tale Type Index: A Critique", Alan Dundes explains that the Aarne–Thompson indexes are some of the "most valuable tools in the professional folklorist's arsenal of aids for analysis".[1] Antti Aarne was a student of Julius Krohn and his son Kaarle Krohn. Aarne further developed their historic-geographic method of comparative folkloristics, and developed the initial version of what became the Aarne–Thompson tale type index for classifying folktales, first published in 1910.		The American folklorist Stith Thompson translated Aarne's motif-based classification system in 1928, enlarging its scope. With Thompson's second revisions to Aarne's catalogue in 1961, he created the "AT number system" (also referred to as "AaTh system"), which is often used today, more commonly in its updated "ATU number" form. (See § Hans-Jörg Uther, above.) According to D. L. Ashliman, "The Aarne–Thompson system catalogues some 2500 basic plots from which, for countless generations, European and Near Eastern storytellers have built their tales".[4]		A quantitative study, published by folklorist Sara Graça da Silva and anthropologist Jamshid J. Tehrani in 2016, tried to evaluate the time of emergence for the "Tales of Magic" (ATU 300–ATU 749), based on a phylogenetic model.[5] They found four of them to belong to the Proto-Indo-European (PIE) stratum of magic tales, namely:		Ten more magic tales were found to be current throughout the Western branch of the Indo-European languages, comprising the main European language families derived from PIE (i. e. Balto-Slavic, Germanic and Italo-Celtic):		The tale type index was criticized by Vladimir Propp of the Formalist school of the 1920s for ignoring the functions of the motifs by which they are classified. Furthermore, Propp contended that using a "macro-level" analysis means that the stories that share motifs might not be classified together, while stories with wide divergences may be grouped under one tale type because the index must select some features as salient.[6] He also observed that while the distinction between animal tales and tales of the fantastic was basically correct—no one would classify Tsarevitch Ivan, the Fire Bird and the Gray Wolf an animal tale because of the wolf—it did raise questions because animal tales often contained fantastic elements, and tales of the fantastic often contained animals; indeed a tale could shift categories if a peasant deceived a bear rather than a devil.[7]		In describing the motivation for his work,[2] Uther presents several criticisms of the original index. He points out that Thompson's focus on oral tradition sometimes neglects older versions of stories, even when written records exist, that the distribution of stories is uneven (with Eastern and Southern European as well as many other regions' folktale types being under-represented), and that some included folktale types have dubious importance. Similarly, Thompson had noted that the tale type index might well be called The Types of the Folk-Tales of Europe, West Asia, and the Lands Settled by these Peoples.[2] However, Alan Dundes notes that in spite of the flaws of tale type indexes (e. g., typos, redundancies, censorship, etc.; p. 198),[1] "they represent the keystones for the comparative method in folkloristics, a method which despite postmodern naysayers ... continues to be the hallmark of international folkloristics" (p. 200).[1]		
Ed Wynn (born Isaiah Edwin Leopold on November 9, 1886 – June 19, 1966) was an American actor and comedian noted for his Perfect Fool comedy character, his pioneering radio show of the 1930s, and his later career as a dramatic actor.[1]						Ed Wynn was an American comedian who was born Isaiah Edwin Leopold in Philadelphia, Pennsylvania. His father, who manufactured and sold women's hats, was born in Bohemia. His mother, of Romanian and Turkish ancestry, came from Istanbul.[2] Wynn attended Central High School in Philadelphia until age 15.[3] He ran away from home in his teens, worked as a hat salesman and as a utility boy,[3] and eventually adapted his middle name "Edwin" into his new stage name, "Ed Wynn", to save his family the embarrassment of having a lowly comedian as a relative.		Wynn began his career in vaudeville in 1903[4][5] and was a star of the Ziegfeld Follies starting in 1914. During The Follies of 1915, W. C. Fields allegedly caught Wynn mugging for the audience under the table during his "Pool Room" routine and knocked him unconscious with his cue.[6] Wynn wrote, directed, and produced many Broadway shows in the subsequent decades, and was known for his silly costumes and props as well as for the giggly, wavering voice he developed for the 1921 musical review, The Perfect Fool.		Although many gag writers later provided material for Wynn's performances in radio, television and movies, he was proud to boast that he had written every line he ever spoke during his early career as a stage performer.[citation needed]		In the early 1930s Wynn hosted the popular radio show The Fire Chief, heard in North America on Tuesday nights, sponsored by Texaco gasoline. Like many former vaudeville performers who turned to radio in the same decade, the stage-trained Wynn insisted on playing for a live studio audience, doing each program as an actual stage show, using visual bits to augment his written material, and in his case, wearing a colorful costume with a red fireman's helmet. He usually bounced his gags off announcer/straight man Graham McNamee; Wynn's customary opening, "Tonight, Graham, the show's gonna be different," became one of the most familiar tag-lines of its time; a sample joke: "Graham, my uncle just bought a new second-handed car... he calls it Baby! I don't know, it won't go anyplace without a rattle!"		Wynn reprised his Fire Chief radio character in two movies, Follow the Leader (1930) and The Chief (1933). Near the height of his radio fame (1933) he founded his own short-lived radio network the Amalgamated Broadcasting System, which lasted only five weeks, nearly destroying the comedian. According to radio historian Elizabeth McLeod, the failed venture left Wynn deep in debt, divorced and finally, suffering a nervous breakdown.[7]		Wynn was offered the title role in MGM's 1939 screen adaptation of The Wizard of Oz, but turned it down, as did his Ziegfeld contemporary W. C. Fields. The part went to Frank Morgan.		Ed Wynn first appeared on television on July 7, 1936 in a brief, ad-libbed spot with Graham McNamee during an NBC experimental television broadcast. In the 1949–50 season, Ed Wynn hosted one of the first network, comedy-variety television shows, on CBS, and won both a Peabody Award and an Emmy Award in 1949. Buster Keaton, Lucille Ball, and The Three Stooges all made guest appearances with Wynn. This was the first CBS variety television show to originate from Los Angeles, which was seen live on the west coast, but filmed via kinescope for distribution in the Midwest and East, as the national coaxial cable had yet to be completed.[8] Wynn was also a rotating host of NBC's Four Star Revue from 1950 through 1952.		After the end of Wynn's third television series, The Ed Wynn Show (a short-lived situation comedy on NBC's 1958–59 schedule), his son, actor Keenan Wynn, encouraged him to make a career change rather than retire. The comedian reluctantly began a career as a dramatic actor in television and movies. Father and son appeared in three productions, the first of which was the 1956 Playhouse 90 broadcast of Rod Serling's play Requiem for a Heavyweight. Ed was terrified of straight acting and kept goofing his lines in rehearsal. When the producers wanted to fire him, star Jack Palance said he would quit if they fired Ed. (However, unbeknownst to Wynn, supporting player Ned Glass was his secret understudy in case something did happen before air time.) On live broadcast night, Wynn surprised everyone with his pitch-perfect performance, and his quick ad libs to cover his mistakes. A dramatization of what happened during the production was later staged as an April 1960 Westinghouse Desilu Playhouse episode, "The Man in the Funny Suit", starring both senior and junior Wynns, with key figures involved in the original production also portraying themselves. Ed and his son also worked together in the Jose Ferrer film The Great Man, with Ed again proving his unexpected skills in drama.		Requiem established Wynn as serious dramatic actor who could easily hold his own with the best. His role in The Diary of Anne Frank (1959) won him an Academy Award nomination for Best Supporting Actor.		Also in 1959, Wynn appeared on Serling's TV series The Twilight Zone in "One for the Angels". Serling, a longtime admirer, had written that episode especially for him, and Wynn later in 1963 starred in the episode "Ninety Years Without Slumbering". For the rest of his life, Wynn skillfully moved between comic and dramatic roles. He appeared in feature films and anthology television, endearing himself to new generations of fans.		Wynn was caricatured in the Merrie Melodies cartoon shorts Shuffle Off to Buffalo (1933) and I've Got to Sing a Torch Song (1933), and as a pot of jam in the Betty Boop short Betty in Blunderland (1934).		He appeared as the Fairy Godfather in Jerry Lewis' Cinderfella. His performance as Paul Beaseley in the 1958 film The Great Man earned him nominations for a Golden Globe Award for "Best Supporting Actor" and a BAFTA Award for "Best Foreign Actor". The following year he received his first (and only) nomination for an Academy Award for Best Supporting Actor for his role as Mr. Dussell in The Diary of Anne Frank (1959). Six years later he appeared in the Bible epic The Greatest Story Ever Told.		Wynn provided the voice of the Mad Hatter in Walt Disney's film, Alice in Wonderland and played The Toymaker alongside Annette Funicello and Tommy Sands in Walt Disney's Babes in Toyland released in 1961.		Possibly his best-remembered film appearance was in Walt Disney's Mary Poppins (1964), in which he played eccentric man Uncle Albert floating around just beneath the ceiling in uncontrollable mirth, singing "I Love to Laugh".		Re-teaming with the Disney team the following year, in That Darn Cat! (1965) featuring Dean Jones and Hayley Mills, Wynn filled out the character of Mr. Hofstedder, the watch jeweler with his bumbling charm. He also had brief roles in The Absent Minded Professor (as the fire chief, in a scene alongside his son Keenan Wynn, who played the film's antagonist) and Son of Flubber (as county agricultural agent A.J. Allen). His final performance, as Rufus in Walt Disney's The Gnome-Mobile, was released a few months after his death.		In addition to Disney films, Wynn was also an actor in the Disneyland production The Golden Horseshoe Revue.		Wynn died June 19, 1966 in Beverly Hills, California of throat cancer,[3] aged 79. He is interred at Forest Lawn Memorial Park in Glendale, in The Great Mausoleum, Daffodil Corridor, Columbarium of the Dawn, alongside his son Keenan Wynn, his granddaughter Emily Wynn (February 13, 1960 – November 27, 1980), who died from lupus and his older sister Blanche Leopold (May 18, 1880 – December 26, 1973). His bronze grave marker reads "Dear God, Thanks... Ed Wynn". According to his granddaughter Hilda Levine, Walt Disney, who would die just a few months later, served as one of his casket bearers. Red Skelton, who was discovered by Wynn, stated: "His death is the first time he ever made anyone sad."[9]		Wynn's distinct voice was deliberately emulated by Alan Tudyk for the character King Candy in Disney's animated film Wreck-It Ralph.[10]		Wynn was posthumously named a Disney Legend on August 10, 2013.[11]		
The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier.		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.		The initial ISBN configuration of recognition was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).		Occasionally, a book may appear without a printed ISBN if it is printed privately or the author does not follow the usual ISBN procedure; however, this can be rectified later.[1]		Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.						The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[2] for the booksellers and stationers WHSmith and others in 1965.[3] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[4] (regarded as the "Father of the ISBN"[5]) and in 1968 in the US by Emery Koltay[4] (who later became director of the U.S. ISBN agency R.R. Bowker).[5][6][7]		The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[3][4] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[8]		An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.		Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[9]		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[10] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):		A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN number is complicated, because most of the parts do not use a fixed number of digits.[13]		ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[14] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.		Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[15] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[16]		Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [17]		Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Bibliothèque et Archives nationales du Québec.		Colombia: Cámara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.		Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[18]		India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[19] There is no fee associated in getting ISBN in India.[20]		Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[21] The original national prefix 978-88 is reserved for publishing companies, starting at €49 for a ten-codes block[22] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of €25 for a single code.		Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]		Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[23][24][25]		Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.		New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[26]		Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.		South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.		United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from £120 (plus VAT) for the smallest block on a standard turnaround of ten days.[27]		United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[4] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125.00 for a single number. Access is immediate when requested via their website.[28]		Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.		The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[11] Registration group identifiers have primarily been allocated within the 978 prefix element.[29] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[30] Books published in rare languages typically have longer group identifiers.[31]		Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[11] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[32]		The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.		The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]		A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[33] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.		Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.		By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[34] Here are some sample ISBN-10 codes, illustrating block length variations.		English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[35]		A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".		The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[36] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.		For example, for an ISBN-10 of 0-306-40615-2:		Formally, using modular arithmetic, we can say:		It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:		Formally, we can say:		The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[37]		In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN number (although it is still unlikely).		Modular arithmetic is convenient for calculating the check digit using modulus 11. Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by a number in a sequence from 10 to 2, and the remainder of the sum, with respect to 11, is computed. The resulting remainder, plus the check digit, must equal a multiple of 11 (either 0 or 11). Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)		For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:		Thus the check digit is 2, and the complete sequence is ISBN 0-306-40615-2. The value x 10 {\displaystyle x_{10}} required to satisfy this condition might be 10; if so, an 'X' should be used.		It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:		The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.		The 2005 edition of the International ISBN Agency's official manual[38] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.		Formally, using modular arithmetic, we can say:		The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.		For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:		Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.		In general, the ISBN-13 check digit is calculated as follows.		Let		Then		This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.		Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).		The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.		Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[39] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden® : a novel based on the best-selling game by Tecmo (1990) and Wacky Laws (1997), both published by Scholastic.		Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[40] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.		Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[41]		Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[42] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).		Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[43] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.		Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[44]		Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.		
In its original sense, a shaggy dog story or yarn is an extremely long-winded anecdote characterized by extensive narration of typically irrelevant incidents and terminated by an anticlimax or a pointless punchline.		Shaggy dog stories play upon the audience's preconceptions of joke-telling. The audience listens to the story with certain expectations, which are either simply not met or met in some entirely unexpected manner.[1] A lengthy shaggy dog story derives its humour from the fact that the joke-teller held the attention of the listeners for a long time (such jokes can take five minutes or more to tell) for no reason at all, as the end resolution is essentially meaningless.[2] The nature of their delivery is reflected in the English idiom spin a yarn, by way of analogy with the production of yarn.						The commonly believed archetype of the shaggy dog story is a story that concerns a shaggy dog. The story builds up, repeatedly emphasizing how shaggy the dog is. At the climax of the story, someone in the story reacts with, "That dog's not so shaggy." The expectations of the audience that have been built up by the presentation of the story, that the story will end with a punchline, are thus disappointed. Ted Cohen gives the following example of this story:[1]		A boy owned a dog that was uncommonly shaggy. Many people remarked upon its considerable shagginess. When the boy learned that there are contests for shaggy dogs, he entered his dog. The dog won first prize for shagginess in both the local and the regional competitions. The boy entered the dog in ever-larger contests, until finally he entered it in the world championship for shaggy dogs. When the judges had inspected all of the competing dogs, they remarked about the boy's dog: "He's not that shaggy."		However, authorities disagree as to whether this particular story is the archetype after which the category is named. Eric Partridge, for example, provides a very different story, as do William and Mary Morris in The Morris Dictionary of Word and Phrase Origins.		According to Partridge and the Morrises, the archetypical shaggy dog story involves an advertisement placed in the Times announcing a search for a shaggy dog. In the Partridge story, an aristocratic family living in Park Lane is searching for a lost dog, and an American answers the advertisement with a shaggy dog that he has found and personally brought across the Atlantic, only to be received by the butler at the end of the story who takes one look at the dog and shuts the door in his face, saying, "But not so shaggy as that, sir!" In the Morris story, the advertiser is organizing a competition to find the shaggiest dog in the world, and after a lengthy exposition of the search for such a dog, a winner is presented to the aristocratic instigator of the competition, who says, "I don't think he's so shaggy."[3][4]		A typical shaggy dog story occurs in Mark Twain's book about his travels west, Roughing It. Twain's friends encourage him to go find a man called Jim Blaine when he is properly drunk, and ask him to tell "the stirring story about his grandfather's old ram".[5] Twain, encouraged by his friends who have already heard the story, finally finds Blaine, an old silver miner, who sets out to tell Twain and his friends the tale. Blaine starts out with the ram ("There never was a bullier old ram than what he was"), and goes on for four more mostly dull but occasionally hilarious unparagraphed pages. Along the way, Blaine tells many stories, each of which connects back to the one before by some tenuous thread, and none of which has to do with the old ram. Among these stories are: a tale of boiled missionaries; of a lady who borrows a false eye, a peg leg, and the wig of a coffin-salesman's wife; and a final tale of a man who gets caught in machinery at a carpet factory and whose "widder bought the piece of carpet that had his remains wove in..." As Blaine tells the story of the carpet man's funeral, he begins to fall asleep, and Twain, looking around, sees his friends "suffocating with suppressed laughter." They now inform him that "at a certain stage of intoxication, no human power could keep [Blaine] from setting out, with impressive unction, to tell about a wonderful adventure which he had once had with his grandfather's old ram — and the mention of the ram in the first sentence was as far as any man had heard him get, concerning it."		Buy Jupiter and Other Stories, a collection of stories by Isaac Asimov, contains a tale whose title is "Shah Guido G."[6] In his background notes, Asimov defines the tale as a shaggy dog story, and explains that the title is a play on "shaggy dog".		Arlo Guthrie's classic anti-war story-song "Alice's Restaurant Massacree" is a shaggy dog story about the military draft, hippies, and improper disposal of garbage.[7]		David Bromberg's version of "Bullfrog Blues" (on "How Late'll Ya Play 'Til?") is a rambling shaggy dog story performed as a talking blues song.[8][9]		
A redneck joke is a joke about rednecks — working-class, rural or southern white Americans.[1] For example,[2]		Jeff Foxworthy is a comedian that specializes in telling redneck jokes.[3] For example:		
Low culture is a derogatory term for forms of popular culture that have mass appeal. Its contrast is high culture which can also be derogatory. It has been said by culture theorists that both high culture and low culture are subcultures.		The boundaries of low culture and high culture blur, through convergence. Many people are "omnivores", making cultural choices from different menus.						In his book Popular Culture and High Culture, Herbert J. Gans gives a definition of how to identify and create low culture:		Aesthetic standards of low culture stress substance, form and being totally subservient; there is no explicit concern with abstract ideas or even with fictional forms of contemporary social problems and issues. ... Low culture emphasizes morality but limits itself to familial and individual problems and [the] values, which apply to such problems. Low culture is content to depict traditional working class values winning out over the temptation to give into conflicting impulses and behavior patterns.		When applying that lens to mass media, it often includes shows that do not go too deeply into abstract ideas, or that do not address head-on contemporary social problems.		Herbert Gans states in his book Popular Culture and High Culture that the different classes of culture are linked correspondingly to socio-economic and educational classes.[2] For any given socio-economic class, there is a culture for that class. Hence the terms high and low culture and the manifestation of those terms as they appeal to their respective constituents.		All cultural products (especially high culture) have a certain demographic to which they appeal most. Low culture appeals to very simple and basic human needs plus offers a perceived return to innocence,[3] the escape from real world problems, or the experience of living vicariously through viewing someone else’s life on television.[4] for examples.		Low culture can be formulaic, employing trope conventions, stock characters and character archetypes in a manner that can be perceived as more simplistic, crude, emotive, unbalanced, or blunt compared to high culture's implementations—which may be perceived as more subtle, balanced, or refined and open for interpretations.		
The Bellman joke is a type of simple joke popular among Swedish schoolchildren, always including a person named Bellman as the main character.		The jokes first became popular in the 19th century, and were originally inspired by the life of the poet and composer Carl Michael Bellman. The first known Bellman jokes survive in a book from 1835, which quotes a Bellman joke in a letter written 1808 by a contemporary of Bellman. 19th century Bellman jokes tended to focus on C. M. Bellman's life at court, and often contained sexual humour. Since then, however, the Bellman character of the jokes has changed into a generic swede, rather than the historical figure. The shift from jokes told by adults to jokes told mainly by young schoolchildren up to 10 years of age probably happened in the first half of the 20th century.[1]		The modern versions of the Bellman jokes often include Bellman and two other characters of different nationalities, with the former coming out victorious from a tricky situation. However, in many Bellman jokes, Bellman is portrayed as something of an anti-hero, who may cheat, lie or even smell very bad in order to get the last laugh. Another common theme is that Bellman fools or makes fun of a priest, policeman or other authority figure. He can thus be seen as a modern sort of a trickster. The jokes tend to involve bodily functions such as urinating or defecating.		The ubiquitous character of the stories and the fact that they have been told in various forms for so many years have made them subject to study by ethnologists such as Bengt af Klintberg[2] and researchers in children's culture.		A Russian, a German and Bellman wanted to see who could swim the fastest across the Atlantic. First out was the German. He swam one kilometer and drowned. Next came the Russian. He swam 10 kilometers and then he drowned. Now it was Bellman's turn. He swam and swam until he almost reached the coast of America – then he got tired and swam back.		A Dane, a Norwegian and Bellman made a wager on who could remain inside a goat pen the longest. First out was the Dane, who came out after just 10 minutes yelling "Damn! The goat stinks!" After him the Norwegian went in, and after half an hour he came out yelling, "Damn! The goat stinks!" Finally Bellman went in. After two hours the goat came rushing out yelling "Damn! Bellman stinks!"		
Discourse analysis (DA), or discourse studies, is a general term for a number of approaches to analyze written, vocal, or sign language use, or any significant semiotic event.		The objects of discourse analysis (discourse, writing, conversation, communicative event) are variously defined in terms of coherent sequences of sentences, propositions, speech, or turns-at-talk. Contrary to much of traditional linguistics, discourse analysts not only study language use 'beyond the sentence boundary' but also prefer to analyze 'naturally occurring' language use, not invented examples.[1] Text linguistics is a closely related field. The essential difference between discourse analysis and text linguistics is that discourse analysis aims at revealing socio-psychological characteristics of a person/persons rather than text structure.[2]		Discourse analysis has been taken up in a variety of disciplines in the humanities and social sciences, including linguistics, education, sociology, anthropology, social work, cognitive psychology, social psychology, area studies, cultural studies, international relations, human geography, communication studies, biblical studies, and translation studies, each of which is subject to its own assumptions, dimensions of analysis, and methodologies.						Topics of discourse analysis include:[3]		Political discourse analysis is a field of discourse analysis which focuses on discourse in political forums (such as debates, speeches, and hearings) as the phenomenon of interest. Policy analysis requires discourse analysis to be effective from the post-positivist perspective.[citation needed]		Political discourse is the informal exchange of reasoned views as to which of several alternative courses of action should be taken to solve a societal problem.[4]		An example of an analysis of political discourse is Roffee's 2016 examination into speech acts surrounding the justification of the legislative processes concerning the Australian federal government's intervening in the Northern Territory Aboriginal communities. The intervention was a hasty reaction to a social problem. Through this analysis, Roffee established that there was in fact an unwillingness to respond on behalf of the government, and the intervention was, in fact, no more than another attempt to control the Indigenous population. However, due to the political rhetoric used, this was largely unidentified.[5]		Although the ancient Greeks (among others) had much to say on discourse, some scholars[which?] consider Austria-born Leo Spitzer's Stilstudien (Style Studies) of 1928 the earliest example of discourse analysis (DA). It was translated into French by Michel Foucault.		However, the term first came into general use following the publication of a series of papers by Zellig Harris from 1952 reporting on work from which he developed transformational grammar in the late 1930s. Formal equivalence relations among the sentences of a coherent discourse are made explicit by using sentence transformations to put the text in a canonical form. Words and sentences with equivalent information then appear in the same column of an array. This work progressed over the next four decades (see references) into a science of sublanguage analysis (Kittredge & Lehrberger 1982), culminating in a demonstration of the informational structures in texts of a sublanguage of science, that of immunology, (Harris et al. 1989) and a fully articulated theory of linguistic informational content (Harris 1991). During this time, however, most linguists ignored such developments in favor of a succession of elaborate theories of sentence-level syntax and semantics.[6]		In January 1953, a linguist working for the American Bible Society, James A. Lauriault/Loriot, needed to find answers to some fundamental errors in translating Quechua, in the Cuzco area of Peru. Following Harris's 1952 publications, he worked over the meaning and placement of each word in a collection of Quechua legends with a native speaker of Quechua and was able to formulate discourse rules that transcended the simple sentence structure. He then applied the process to Shipibo, another language of Eastern Peru. He taught the theory at the Summer Institute of Linguistics in Norman, Oklahoma, in the summers of 1956 and 1957 and entered the University of Pennsylvania to study with Harris in the interim year. He tried to publish a paper Shipibo Paragraph Structure, but it was delayed until 1970 (Loriot & Hollenbach 1970).[citation needed] In the meantime, Kenneth Lee Pike, a professor at University of Michigan, Ann Arbor, taught the theory, and one of his students, Robert E. Longacre developed it in his writings.		Harris's methodology disclosing the correlation of form with meaning was developed into a system for the computer-aided analysis of natural language by a team led by Naomi Sager at NYU, which has been applied to a number of sublanguage domains, most notably to medical informatics. The software for the Medical Language Processor is publicly available on SourceForge.		In the late 1960s and 1970s, and without reference to this prior work, a variety of other approaches to a new cross-discipline of DA began to develop in most of the humanities and social sciences concurrently with, and related to, other disciplines, such as semiotics, psycholinguistics, sociolinguistics, and pragmatics. Many of these approaches, especially those influenced by the social sciences, favor a more dynamic study of oral talk-in-interaction. An example is "conversational analysis", which was influenced by the Sociologist Harold Garfinkel, the founder of Ethnomethodology.		In Europe, Michel Foucault became one of the key theorists of the subject, especially of discourse, and wrote The Archaeology of Knowledge. In this context, the term 'discourse' no longer refers to formal linguistic aspects, but to institutionalized patterns of knowledge that become manifest in disciplinary structures and operate by the connection of knowledge and power. Since the 1970s, Foucault´s works have had an increasing impact especially on discourse analysis in the social sciences. Thus, in modern European social sciences, one can find a wide range of different approaches working with Foucault´s definition of discourse and his theoretical concepts. Apart from the original context in France, there is, at least since 2005, a broad discussion on socio-scientific discourse analysis in Germany. Here, for example, the sociologist Reiner Keller developed his widely recognized 'Sociology of Knowledge Approach to Discourse (SKAD)'.[7] Following the sociology of knowledge by Peter L. Berger and Thomas Luckmann, Keller argues, that our sense of reality in everyday life and thus the meaning of every objects, actions and events are the product of a permanent, routinized interaction. In this context, SKAD has been developed as a scientific perspective that is able to understand the processes of 'The Social Construction of Reality' on all levels of social life by combining Michel Foucault's theories of discourse and power with the theory of knowledge by Berger/Luckmann. Whereas the latter primarily focus on the constitution and stabilisation of knowledge on the level of interaction, Foucault's perspective concentrates on institutional contexts of the production and integration of knowledge, where the subject mainly appears to be determined by knowledge and power. Therefore, the 'Sociology of Knowledge Approach to Discourse' can also be seen as an approach to deal with the vividly discussed micro–macro problem in sociology.		The following are some of the specific theoretical perspectives and analytical approaches used in linguistic discourse analysis:		Although these approaches emphasize different aspects of language use, they all view language as social interaction, and are concerned with the social contexts in which discourse is embedded.		Often a distinction is made between 'local' structures of discourse (such as relations among sentences, propositions, and turns) and 'global' structures, such as overall topics and the schematic organization of discourses and conversations. For instance, many types of discourse begin with some kind of global 'summary', in titles, headlines, leads, abstracts, and so on.		A problem for the discourse analyst is to decide when a particular feature is relevant to the specification is required. A question many linguist ask is: "Are there general principles which will determine the relevance or nature of the specification?"		
Monty Python (also known as The Pythons)[2][3] were a British surreal comedy group who created their sketch comedy show Monty Python's Flying Circus, which first aired on the BBC in 1969. Forty-five episodes were made over four series. The Python phenomenon developed from the television series into something larger in scope and impact, including touring stage shows, films, numerous albums, several books, and musicals. The Pythons' influence on comedy has been compared to the Beatles' influence on music.[4][5][6] Their sketch show has been referred to as "not only one of the more enduring icons of 1970s British popular culture, but also an important moment in the evolution of television comedy."[7]		Broadcast by the BBC between 1969 and 1974, Monty Python's Flying Circus was conceived, written, and performed by its members Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. Loosely structured as a sketch show, but with an innovative stream-of-consciousness approach (aided by Gilliam's animation), it pushed the boundaries of what was acceptable in style and content.[8][9] A self-contained comedy team responsible for both writing and performing their work, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. Following their television work, they began making films, which include Holy Grail (1975), Life of Brian (1979) and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while in North America, it has coloured the work of cult performers from the early editions of Saturday Night Live through to more recent absurdist trends in television comedy. "Pythonesque" has entered the English lexicon as a result.		In a 2005 poll of over 300 comics, comedy writers, producers and directors throughout the English-speaking world to find "The Comedian's Comedian", three of the six Pythons members were voted by fellow comedians and comedy insiders to be among the top 50 greatest comedians ever: Cleese at No. 2, Idle at No. 21, and Palin at No. 30.[10][11]						Jones and Palin met at Oxford University, where they performed together with the Oxford Revue. Chapman and Cleese met at Cambridge University. Idle was also at Cambridge, but started a year after Chapman and Cleese. Cleese met Gilliam in New York City while on tour with the Cambridge University Footlights revue Cambridge Circus (originally entitled A Clump of Plinths). Chapman, Cleese, and Idle were members of the Footlights, which at that time also included the future Goodies (Tim Brooke-Taylor, Bill Oddie, and Graeme Garden), and Jonathan Lynn (co-writer of Yes Minister and Yes, Prime Minister). During Idle's presidency of the club, feminist writer Germaine Greer and broadcaster Clive James were members. Recordings of Footlights' revues (called "Smokers") at Pembroke College include sketches and performances by Cleese and Idle, which, along with tapes of Idle's performances in some of the drama society's theatrical productions, are kept in the archives of the Pembroke Players.		The six Python members appeared in or wrote these shows before Flying Circus:		The Frost Report is credited as first uniting the British Pythons and providing an environment in which they could develop their particular styles.		Several other important British comedy writers or future performers who were featured included Marty Feldman, Jonathan Lynn, David Jason, and David Frost, as well as members of other future comedy teams such as Ronnie Corbett and Ronnie Barker (the Two Ronnies), and Tim Brooke-Taylor, Graeme Garden, and Bill Oddie (the Goodies).		Following the success of Do Not Adjust Your Set, a tea-time children's programme, ITV offered Gilliam, Idle, Jones, and Palin their own late-night adult comedy series together. At the same time, Chapman and Cleese were offered a show by the BBC, which had been impressed by their work on The Frost Report and At Last the 1948 Show. Cleese was reluctant to do a two-man show for various reasons, including Chapman's supposedly difficult and erratic personality. Cleese had fond memories of working with Palin on How To Irritate People and invited him to join the team. With no studio available at ITV until summer 1970 for the late-night show, Palin agreed to join Cleese and Chapman, and suggested the involvement of his writing partner Jones and colleague Idle—who in turn wanted Gilliam to provide animations for the projected series. Much has been made of the fact that the Monty Python troupe is the result of Cleese's desire to work with Palin and the chance circumstances that brought the other four members into the fold.[12]		By contrast, according to John Cleese's autobiography,[13] the origins of Monty Python lay in the admiration that writing partners Cleese and Chapman had for the new type of comedy being done on Do Not Adjust Your Set; as a result, a meeting was initiated by Cleese between Chapman, Idle, Jones, Palin, and himself at which it was agreed to pool their writing and performing efforts and jointly seek production sponsorship.		The Pythons had a definite idea about what they wanted to do with the series. They were admirers of the work of Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore on Beyond the Fringe, and had worked on Frost, which was similar in style.[14] They enjoyed Cook and Moore's sketch show Not Only... But Also. One problem the Pythons perceived with these programmes was that though the body of the sketch would be strong, the writers would often struggle to then find a punchline funny enough to end on, and this would detract from the overall sketch quality. They decided that they would simply not bother to "cap" their sketches in the traditional manner, and early episodes of the Flying Circus series make great play of this abandonment of the punchline (one scene has Cleese turn to Idle, as the sketch descends into chaos, and remark that "This is the silliest sketch I've ever been in"—they all resolve not to carry on and simply walk off the set).[15] However, as they began assembling material for the show, the Pythons watched one of their collective heroes, Spike Milligan, recording his groundbreaking series Q5 (1969). Not only was the programme more irreverent and anarchic than any previous television comedy, but Milligan also would often "give up" on sketches halfway through and wander off set (often muttering "Did I write this?"). It was clear that their new series would now seem less original, and Jones in particular became determined the Pythons should innovate.		After much debate, Jones remembered an animation Gilliam had created for Do Not Adjust Your Set called "Beware of the Elephants", which had intrigued him with its stream-of-consciousness style. Jones felt it would be a good concept to apply to the series: allowing sketches to blend into one another. Palin had been equally fascinated by another of Gilliam's efforts, entitled "Christmas Cards", and agreed that it represented "a way of doing things differently". Since Cleese, Chapman, and Idle were less concerned with the overall flow of the programme, Jones, Palin, and Gilliam became largely responsible for the presentation style of the Flying Circus series, in which disparate sketches are linked to give each episode the appearance of a single stream-of-consciousness (often using a Gilliam animation to move from the closing image of one sketch to the opening scene of another).		Writing started at 9 am and finished at 5 pm. Typically, Cleese and Chapman worked as one pair isolated from the others, as did Jones and Palin, while Idle wrote alone. After a few days, they would join together with Gilliam, critique their scripts, and exchange ideas. Their approach to writing was democratic. If the majority found an idea humorous, it was included in the show. The casting of roles for the sketches was a similarly unselfish process, since each member viewed himself primarily as a "writer", rather than an actor eager for screen time. When the themes for sketches were chosen, Gilliam had a free hand in bridging them with animations, using a camera, scissors, and airbrush.		While the show was a collaborative process, different factions within Python were responsible for elements of the team's humour. In general, the work of the Oxford-educated members (Jones and Palin) was more visual, and more fanciful conceptually (e.g., the arrival of the Spanish Inquisition in a suburban front room), while the Cambridge graduates' sketches tended to be more verbal and more aggressive (for example, Cleese and Chapman's many "confrontation" sketches, where one character intimidates or hurls abuse, or Idle's characters with bizarre verbal quirks, such as "The Man Who Speaks In Anagrams"). Cleese confirmed that "most of the sketches with heavy abuse were Graham's and mine, anything that started with a slow pan across countryside and impressive music was Mike and Terry's, and anything that got utterly involved with words and disappeared up any personal orifice was Eric's".[16] Gilliam's animations, meanwhile, ranged from the whimsical to the savage (the cartoon format allowing him to create some astonishingly violent scenes without fear of censorship).		Several names for the show were considered before Monty Python's Flying Circus was settled upon. Some were Owl Stretching Time, The Toad Elevating Moment, A Horse, a Spoon and a Bucket, Vaseline Review, and Bun, Wackett, Buzzard, Stubble and Boot. Flying Circus stuck when the BBC explained it had printed that name in its schedules and was not prepared to amend it. Many variations on the name in front of this title then came and went (popular legend holds that the BBC considered Monty Python's Flying Circus to be a ridiculous name, at which point the group threatened to change their name every week until the BBC relented). Gwen Dibley's Flying Circus was named after a woman Palin had read about in the newspaper, thinking it would be amusing if she were to discover she had her own TV show. Baron Von Took's Flying Circus was considered as an affectionate tribute to Barry Took, the man who had brought them together. Arthur Megapode's Flying Circus was suggested, then discarded. The name Baron Von Took's Flying Circus had the form of Baron Manfred von Richthofen's Flying Circus of WWI fame, and the new group was forming in a time when the Royal Guardsmen's 1966 song "Snoopy vs. the Red Baron" had peaked. The term 'flying circus' was also another name for the popular entertainment of the 1920s known as barnstorming, where multiple performers collaborated with their stunts to perform a combined set of acts.		Differing, somewhat confusing accounts are given of the origins of the Python name, although the members agree that its only "significance" was that they thought it sounded funny. In the 1998 documentary Live at Aspen during the US Comedy Arts Festival, where the troupe was awarded the AFI Star Award by the American Film Institute, the group implied that "Monty" was selected (Eric Idle's idea) as a gently mocking tribute to Field Marshal Lord Montgomery, a legendary British general of World War II; requiring a "slippery-sounding" surname, they settled on "Python". On other occasions, Idle has claimed that the name "Monty" was that of a popular and rotund fellow who drank in his local pub; people would often walk in and ask the barman, "Has Monty been in yet?", forcing the name to become stuck in his mind. The name Monty Python was later described by the BBC as being "envisaged by the team as the perfect name for a sleazy entertainment agent".[17]		Flying Circus popularised innovative formal techniques, such as the cold open, in which an episode began without the traditional opening titles or announcements.[18] An example of this is the "It's" man: Palin, outfitted in Robinson Crusoe garb, making a tortuous journey across various terrains, before finally approaching the camera to state, "It's ...", only to be then cut off by the title sequence and theme music.		On several occasions, the cold open lasted until mid-show, after which the regular opening titles ran. Occasionally, the Pythons tricked viewers by rolling the closing credits halfway through the show, usually continuing the joke by fading to the familiar globe logo used for BBC continuity, over which Cleese would parody the clipped tones of a BBC announcer. On one occasion, the credits ran directly after the opening titles.		Because of their dislike of finishing with punchlines, they experimented with ending the sketches by cutting abruptly to another scene or animation, walking offstage, addressing the camera (breaking the fourth wall), or introducing a totally unrelated event or character. A classic example of this approach was the use of Chapman's "anti-silliness" character of "the Colonel", who walked into several sketches and ordered them to be stopped because things were becoming "far too silly".		Another favourite way of ending sketches was to drop a cartoonish "16-ton weight" prop on one of the characters when the sketch seemed to be losing momentum, or a knight in full armour (played by Terry Gilliam) would wander on-set and hit characters over the head with a rubber chicken,[19] before cutting to the next scene. Yet another way of changing scenes was when John Cleese, usually outfitted in a dinner suit, would come in as a radio commentator and, in a rather pompous manner, make the formal and determined announcement "And now for something completely different.", which later became the title of the first Monty Python film.		The Python theme music is The Liberty Bell, a march by John Philip Sousa, which was chosen, among other reasons, because the recording was in the public domain.[18]		The use of Gilliam's surreal, collage stop motion animations was another innovative intertextual element of the Python style. Many of the images Gilliam used were lifted from famous works of art, and from Victorian illustrations and engravings. The giant foot which crushes the show's title at the end of the opening credits is in fact the foot of Cupid, cut from a reproduction of the Renaissance masterpiece Venus, Cupid, Folly and Time by Bronzino. This foot, and Gilliam's style in general, are visual trademarks of the programme.		The Pythons used the British tradition of cross-dressing comedy by donning frocks and makeup and playing female roles themselves while speaking in falsetto. Jones specialised in playing the working-class housewife, with Palin and Idle in being generally more posh. The other members played female roles more sparsely. Generally speaking, female roles were played by women only when the scene specifically required that the character be sexually attractive (although sometimes they used Idle for this). The troupe later turned to Carol Cleveland, who co-starred in numerous episodes after 1970. In some episodes and later in Monty Python's Life of Brian, they took the idea one step further by playing women who impersonated men (in the stoning scene).		Many sketches are well-known and widely quoted. "Dead Parrot sketch", "The Lumberjack Song", "Spam" (which led to the coining of the term email spam),[20] "Nudge Nudge", "The Spanish Inquisition", "Upper Class Twit of the Year", "Cheese Shop", and "The Ministry of Silly Walks" are just a few examples.		The Canadian Broadcasting Corporation (CBC) added Monty Python's Flying Circus to its national September 1970 fall line-up.[21] They aired the 13 episodes of series 1, which had first run on the BBC the previous fall (October 1969 to January 1970), as well as the first six episodes of series 2 only a few weeks after they first appeared on the BBC (September to November 1970).[21] The CBC dropped the show when it returned to regular programming after the Christmas 1970 break, choosing to not place the remaining seven episodes of series 2 on the January 1971 CBC schedule.[21] Within a week, the CBC received hundreds of calls complaining of the cancellation, and more than 100 people staged a demonstration at the CBC's Montreal studios. The show eventually returned, becoming a fixture on the network during the first half of the 1970s.[21]		Time-Life Films had the rights to distribute all BBC-TV programmes in the United States; however, they decided that British comedy simply would not work in America, so it would not be worth the investment to convert the Python episodes from the European PAL standard to the American NTSC standard.		Sketches from Monty Python's Flying Circus were introduced to American audiences in August 1972, with the release of the Python film And Now for Something Completely Different, featuring sketches from series 1 and 2 of the television show. This 1972 release met limited box office success. Sketches like "Bicycle Repairman" and "The Dull Life of a Stockbroker" aired in the summer of 1972 on Comedyworld, a summer replacement series for NBC's The Dean Martin Show.		In the summer of 1974, Ron Devillier, the programme director for nonprofit PBS television station KERA in Dallas, Texas, started airing episodes of Monty Python's Flying Circus. Ratings shot through the roof, providing an encouraging sign to the other 100 PBS stations that had signed up to begin airing the show in October 1974—exactly five years after their BBC debut. There was also cross-promotion from FM Radio stations across the country, whose airing of tracks from the Python LPs had already introduced American audiences to this bizarre brand of comedy. The popularity on PBS resulted in the 1974 re-release of the 1972 ...Completely Different film, with much greater box office success.		The ability to show Monty Python's Flying Circus under the American NTSC standard had been made possible by the commercial actions of American television producer Greg Garrison. Garrison produced the NBC series The Dean Martin Comedy World, which ran during the summer of 1974. The concept was to show clips from comedy shows produced in other countries, including tape of the Python sketches "Bicycle Repairman" and "The Dull Life of a Stockbroker". Payment for use of these two sketches was enough to allow Time-Life Films to convert the entire Python library to NTSC standard, allowing for the sale to the PBS network stations which then brought the entire show to US audiences.		In 1975, ABC broadcast two 90-minute Monty Python specials, each with three shows, but cut out a total of 24 minutes from each, in part to make time for commercials, and in part to avoid upsetting their audience. As the judge observed in Gilliam v. American Broadcasting Companies, Inc., where Monty Python sued for damages caused by broadcast of the mutilated version, "According to the network, appellants should have anticipated that most of the excised material contained scatological references inappropriate for American television and that these scenes would be replaced with commercials, which presumably are more palatable to the American public." Monty Python won the case.[22]		With the popularity of Python throughout the rest of the 1970s and through most of the 1980s, PBS stations looked at other British comedies, leading to UK shows such as Are You Being Served? gaining a US audience, and leading, over time, to many PBS stations having a "British Comedy Night" which airs many popular UK comedies.[23]		Having considered the possibility at the end of the second season, Cleese left the Flying Circus at the end of the third. He later explained that he felt he no longer had anything fresh to offer the show, and claimed that only two Cleese- and Chapman-penned sketches in the third series ("Dennis Moore" and the "Cheese Shop") were truly original, and that the others were bits and pieces from previous work cobbled together in slightly different contexts.[12] He was also finding Chapman, who was at that point in the full throes of alcoholism, difficult to work with. According to an interview with Idle, "It was on an Air Canada flight on the way to Toronto, when John (Cleese) turned to all of us and said 'I want out.' Why? I don't know. He gets bored more easily than the rest of us. He's a difficult man, not easy to be friendly with. He's so funny because he never wanted to be liked. That gives him a certain fascinating, arrogant freedom."[24]		The rest of the group carried on for one more "half" season before calling a halt to the programme in 1974. The name Monty Python's Flying Circus appears in the opening animation for season four, but in the end credits, the show is listed as simply "Monty Python". Although Cleese left the show, he was credited as a writer for three of the six episodes, largely concentrated in the "Michael Ellis" episode, which had begun life as one of the many drafts of the "Holy Grail" motion picture. When a new direction for "Grail" was decided upon, the subplot of Arthur and his knights wandering around a strange department store in modern times was lifted out and recycled as the aforementioned TV episode.		While the first three seasons contained 13 episodes each, the fourth ended after just six. Extremely keen to keep the now massively popular show going, the BBC had offered the troupe a full 13 episodes, but the truncated troupe (now under the unspoken 'leadership' of Terry Jones) had come to a common agreement while writing the fourth series that there was only enough material, and more importantly only enough enthusiasm, to shoot the six that were made.		The Pythons' first feature film was directed by Ian MacNaughton, reprising his role from the television series. It was composed of sketches from the first two seasons of the Flying Circus, reshot on a low budget (and often slightly edited) for cinema release. Material selected for the film includes: "Dead Parrot", "The Lumberjack Song", "Upper Class Twit of the Year", "Hell's Grannies", "Self-Defence Class", "How Not To Be Seen", and "Nudge Nudge". Financed by Playboy's UK executive Victor Lownes, it was intended as a way of breaking Monty Python into America, and although it was ultimately unsuccessful in this, the film did good business in the UK, this being in the era before home video would make the original material much more accessible. The group did not consider the film a success.		In 1974, between production on the third and fourth seasons, the group decided to embark on their first "proper" feature film, containing entirely new material. Monty Python and the Holy Grail was based on Arthurian legend and was directed by Jones and Gilliam. Again, the latter also contributed linking animations (and put together the opening credits). Along with the rest of the Pythons, Jones and Gilliam performed several roles in the film, but Chapman took the lead as King Arthur. Cleese returned to the group for the film, feeling that they were once again breaking new ground. Holy Grail was filmed on location, in picturesque rural areas of Scotland, with a budget of only £229,000; the money was raised in part with investments from rock groups such as Pink Floyd, Jethro Tull, and Led Zeppelin—and UK music industry entrepreneur Tony Stratton-Smith (founder and owner of the Charisma Records label, for which the Pythons recorded their comedy albums).		The backers of the film wanted to cut the famous Black Knight scene (in which the Black Knight loses his limbs in a duel), but it was eventually kept in the movie.[25]		Following the success of Holy Grail, reporters asked for the title of the next Python film, despite the fact that the team had not even begun to consider a third one. Eventually, Idle flippantly replied "Jesus Christ – Lust for Glory", which became the group's stock answer once they realised that it shut reporters up. However, they soon began to seriously consider a film lampooning the New Testament era in the same way Holy Grail had lampooned Arthurian legend. Despite them all sharing a distrust of organised religion, they agreed not to mock Jesus or his teachings directly. They also mentioned that they could not think of anything legitimate to make fun of about him. Instead, they decided to write a satire on credulity and hypocrisy among the followers of someone who had been mistaken for the "Messiah", but who had no desire to be followed as such. Chapman was cast in the lead role of Brian.		The focus therefore shifted to a separate individual born at the same time, in a neighbouring stable. When Jesus appears in the film (first, as a baby in the stable, and then later on the Mount, speaking the Beatitudes), he is played straight (by actor Kenneth Colley) and portrayed with respect. The comedy begins when members of the crowd mishear his statements of peace, love, and tolerance ("I think he said, 'Blessed are the cheesemakers'").		Directing duties were handled solely by Jones, having amicably agreed with Gilliam that Jones' approach to film-making was better suited for Python's general performing style. Holy Grail's production had often been stilted by their differences behind the camera. Gilliam again contributed two animated sequences (one being the opening credits) and took charge of set design. The film was shot on location in Tunisia, the finances being provided this time by former Beatle George Harrison, who together with Denis O'Brien formed the production company Hand-Made Films for the movie. Harrison had a cameo role as the 'owner of the Mount'.		Despite its subject matter attracting controversy, particularly upon its initial release, it has (together with its predecessor) been ranked among the greatest comedy films. A Channel 4 poll in 2005 ranked Holy Grail in sixth place, with Life of Brian at the top.[26]		Filmed at the Hollywood Bowl in Los Angeles during preparations for The Meaning of Life, this was a concert film (directed by Terry Hughes) in which the Pythons performed sketches from the television series in front of an audience. The released film also incorporated footage from the German television specials (the inclusion of which gives Ian MacNaughton his first on-screen credit for Python since the end of Flying Circus) and live performances of several songs from the troupe's then-current Monty Python's Contractual Obligation Album.		Python's final film returned to something structurally closer to the style of Flying Circus. A series of sketches loosely follows the ages of man from birth to death. Directed again by Jones solo, The Meaning of Life is embellished with some of Python's most bizarre and disturbing moments, as well as various elaborate musical numbers. The film is by far their darkest work, containing a great deal of black humour, garnished by some spectacular violence (including an operation to remove a liver from a living patient without anaesthetic and the morbidly obese Mr. Creosote exploding over several restaurant patrons). At the time of its release, the Pythons confessed their aim was to offend "absolutely everyone".		Besides the opening credits and the fish sequence, Gilliam, by now an established live-action director, no longer wanted to produce any linking cartoons, offering instead to direct one sketch—"The Crimson Permanent Assurance". Under his helm, though, the segment grew so ambitious and tangential that it was cut from the movie and used as a supporting feature in its own right. (Television screenings also use it as a prologue.) Crucially, this was the last project on which all six Pythons would collaborate, except for the 1989 compilation Parrot Sketch Not Included, where they are all seen sitting in a closet for four seconds. This was the last time Chapman appeared on screen with the Pythons.		Members of Python contributed their services to charitable endeavours and causes—sometimes as an ensemble, at other times as individuals. The cause that has been the most frequent and consistent beneficiary has been the human rights work of Amnesty International. Between 1976 and 1981, the troupe or its members appeared in four major fund-raisers for Amnesty—known collectively as the Secret Policeman's Ball shows—which were turned into multiple films, TV shows, videos, record albums, and books. These benefit shows and their many spin-offs raised considerable sums of money for Amnesty, raised public and media awareness of the human rights cause, and influenced many other members of the entertainment community (especially rock musicians) to become involved in political and social issues.[27] Among the many musicians who have publicly attributed their activism—and the organisation of their own benefit events—to the inspiration of the work in this field of Monty Python are U2, Bob Geldof, Pete Townshend, and Sting.[27] The shows are credited by Amnesty with helping the organisation develop public awareness in the US, where one of the spin-off films was a major success.		Cleese and Jones had an involvement (as performer, writer or director) in all four Amnesty benefit shows, Palin in three, Chapman in two, and Gilliam in one. Idle did not participate in the Amnesty shows. Notwithstanding Idle's lack of participation, the other five members (together with "Associate Pythons" Carol Cleveland and Neil Innes) all appeared together in the first Secret Policeman's Ball benefit—the 1976 A Poke in the Eye—where they performed several Python sketches. In this first show, they were collectively billed as Monty Python. (Peter Cook deputised for the errant Idle in a courtroom sketch.) In the next three shows, the participating Python members performed many Python sketches, but were billed under their individual names rather than under the collective Python banner. After a six-year break, Amnesty resumed producing Secret Policeman's Ball benefit shows in 1987 (sometimes with, and sometimes without, variants of the iconic title) and by 2006 had presented a total of 12 such shows. The shows since 1987 have featured newer generations of British comedic performers, including many who have attributed their participation in the show to their desire to emulate the Python's pioneering work for Amnesty. (Cleese and Palin made a brief cameo appearance in the 1989 Amnesty show; apart from that, the Pythons have not appeared in shows after the first four.)		Each member has pursued various film, television, and stage projects since the break-up of the group, but often continued to work with one another. Many of these collaborations were very successful, most notably A Fish Called Wanda (1988), written by Cleese, in which he starred along with Palin. The pair also appeared in Time Bandits (1981), a film directed by Gilliam, who wrote it together with Palin. Gilliam directed Jabberwocky (1977), and also directed and co-wrote Brazil (1985), which featured Palin, and The Adventures of Baron Munchausen (1988), which featured Idle. Yellowbeard (1983) was co-written by Chapman and featured Chapman, Idle, and Cleese, as well as many other English comedians including Peter Cook, Spike Milligan, and Marty Feldman.		Palin and Jones wrote the comedic TV series Ripping Yarns (1976–79), starring Palin. Jones also appeared in the pilot episode and Cleese appeared in a nonspeaking part in the episode "Golden Gordon". Jones' film Erik the Viking also has Cleese playing a small part.		In 1996, Terry Jones wrote and directed an adaptation of Kenneth Grahame's novel The Wind in the Willows. It featured four members of Monty Python: Jones as Mr. Toad, Idle as Ratty, Cleese as Mr. Toad's lawyer, and Palin as the Sun. Gilliam was considered for the voice of the river.		In terms of numbers of productions, Cleese has the most prolific solo career, having appeared in dozens of films, several TV shows or series (including Cheers, 3rd Rock from the Sun, Q's assistant in the James Bond movies, and Will & Grace), many direct-to-video productions, some video games, and a number of commercials.[28] His BBC sitcom Fawlty Towers (written by and starring Cleese together with his then-wife Connie Booth) is the only comedy series to rank higher than the Flying Circus on the BFI TV 100's list, topping the whole poll.		Idle enjoyed critical success with Rutland Weekend Television in the mid-1970s, out of which came the Beatles parody the Rutles (responsible for the cult mockumentary All You Need Is Cash), and as an actor in Nuns on the Run (1990) with Robbie Coltrane. In 1976, Idle directed music videos for George Harrison songs "This Song" and "Crackerbox Palace", the latter of which also featured cameo appearances from Neil Innes and John Cleese. Idle has had success with Python songs: "Always Look on the Bright Side of Life" went to no. 3 in the UK singles chart in 1991. The song had been revived by Simon Mayo on BBC Radio 1, and was consequently released as a single that year. The theatrical phenomenon of the Python musical Spamalot has made Idle the most financially successful of the troupe after Python. Written by Idle, it has proved to be an enormous hit on Broadway, London's West End, and Las Vegas.[29] This was followed by Not the Messiah, which repurposes The Life of Brian as an oratorio. For the work's 2007 premiere at the Luminato festival in Toronto (which commissioned the work), Idle himself sang the "baritone-ish" part.		Since The Meaning of Life, their last project as a team, the Pythons have often been the subject of reunion rumours.[29] The final reunion of all six members occurred during the Parrot Sketch Not Included – 20 Years of Monty Python special. The death of Chapman in 1989 (on the eve of their 20th anniversary) put an end to the speculation of any further reunions. Several occasions since 1989 have occurred when the surviving five members have gathered together for appearances—albeit not formal reunions. In 1996, Jones, Idle, Cleese, and Palin were featured in a film adaptation of The Wind in the Willows, which was later renamed Mr. Toad's Wild Ride.		In 1998 during the US Comedy Arts Festival, where the troupe was awarded the AFI Star Award by the American Film Institute, the five remaining members, along with what was purported to be Chapman's ashes, were reunited on stage for the first time in 18 years.[30] The occasion was in the form of an interview called Monty Python Live at Aspen, (hosted by Robert Klein, with an appearance by Eddie Izzard) in which the team looked back at some of their work and performed a few new sketches.		On 9 October 1999, to commemorate 30 years since the first Flying Circus television broadcast, BBC2 devoted an evening to Python programmes, including a documentary charting the history of the team, interspersed with new sketches by the Monty Python team filmed especially for the event. The program appears, with a few omissions, on the DVD The Life of Python. Idle's involvement in the special is limited, yet the final sketch marks the only time since 1989 that all surviving members of the troupe appear in one sketch, albeit not in the same room.		The surviving Pythons had agreed in principle to perform a live tour of America in 1999. Several shows were to be linked with Q&A meetings in various cities. Although all had said yes, Palin later changed his mind, much to the annoyance of Idle, who had begun work organising the tour. This led to Idle refusing to take part in the new material shot for the BBC anniversary evening.		In 2002, four of the surviving members, bar Cleese, performed "The Lumberjack Song" and "Sit on My Face" for George Harrison's memorial concert. The reunion also included regular supporting contributors Neil Innes and Carol Cleveland, with a special appearance from Tom Hanks.		In an interview to publicise the DVD release of The Meaning of Life, Cleese said a further reunion was unlikely. "It is absolutely impossible to get even a majority of us together in a room, and I'm not joking," Cleese said. He said that the problem was one of busyness rather than one of bad feelings.[31] A sketch appears on the same DVD spoofing the impossibility of a full reunion, bringing the members "together" in a deliberately unconvincing fashion with modern bluescreen/greenscreen techniques.		Idle has responded to queries about a Python reunion by adapting a line used by George Harrison in response to queries about a possible Beatles reunion. When asked in November 1989 about such a possibility, Harrison responded: "As far as I'm concerned, there won't be a Beatles reunion as long as John Lennon remains dead."[32] Idle's version of this was that he expected to see a proper Python reunion, "just as soon as Graham Chapman comes back from the dead", but added, "we're talking to his agent about terms."[33]		The Pythons Autobiography By the Pythons (2003), compiled from interviews with the surviving members, reveals that a series of disputes in 1998, over a possible sequel to Holy Grail that had been conceived by Idle, may have resulted in the group's permanent split. Cleese's feeling was that The Meaning of Life had been personally difficult and ultimately mediocre, and did not wish to be involved in another Python project for a variety of reasons (not least amongst them was the absence of Chapman, whose straight man-like central roles in the Grail and Brian films had been considered to be an essential anchoring performance). Apparently, Idle was angry with Cleese for refusing to do the film, which most of the remaining Pythons thought reasonably promising (the basic plot would have taken on a self-referential tone, featuring them in their main 'knight' guises from Holy Grail, mulling over the possibilities of reforming their posse). The book also reveals that a secondary option around this point was the possibility of revitalising the Python brand with a new stage tour, perhaps with the promise of new material. This idea had also met with Cleese's refusal, this time with the backing of other members.		March 2005 had a full, if nonperforming, reunion of the surviving cast members at the premiere of Idle's musical Spamalot, based on Monty Python and the Holy Grail. It opened in Chicago and has since played in New York on Broadway, London, and numerous other major cities across the world. In 2004, it was nominated for 14 Tony Awards and won three: Best Musical, Best Direction of a Musical for Mike Nichols, and Best Performance by a Featured Actress in a Musical for Sara Ramirez, who played the Lady of the Lake, a character specially added for the musical. Cleese played the voice of God, played in the film by Chapman.		Owing in part to the success of Spamalot, PBS announced on 13 July 2005 that it would begin to re-air the entire run of Monty Python's Flying Circus and new one-hour specials focusing on each member of the group, called Monty Python's Personal Best.[34] Each episode was written and produced by the individual being honoured, with the five remaining Pythons collaborating on Chapman's programme, the only one of the editions to take on a serious tone with its new material.		In 2009, to commemorate the 40th anniversary of the first episode of Monty Python's Flying Circus, a six-part documentary entitled Monty Python: Almost the Truth (Lawyers Cut) was released, featuring interviews with the surviving members of the team, as well as archive interviews with Graham Chapman and numerous excerpts from the television series and films.		Also in commemoration of the 40th anniversary, Idle, Palin, Jones, and Gilliam appeared in a production of Not the Messiah at the Royal Albert Hall. The European premiere was held on 23 October 2009.[35] An official 40th anniversary Monty Python reunion event took place in New York City on 15 October 2009, where the team received a Special Award from the British Academy of Film and Television Arts.[36]		In June 2011, it was announced that A Liar's Autobiography, an animated 3D movie based on the memoir of Graham Chapman, was in the making. The book A Liar's Autobiography was published in 1980 and details Chapman's journey through medical school, alcoholism, acknowledgement of his gay identity, and the tolls of surreal comedy. Asked what was true in a deliberately fanciful account by Chapman of his life, Terry Jones joked: "Nothing ... it's all a downright, absolute, blackguardly lie."		The film uses Chapman's own voice – from a reading of his autobiography shortly before he died of cancer – and entertainment channel Epix announced that the film will be released in early 2012 in both 2D and 3D formats. Produced and directed by London-based Bill Jones, Ben Timlett, and Jeff Simpson, the new film has 15 animation companies working on chapters that will range from three to 12 minutes in length, each in a different style.		John Cleese recorded dialogue which was matched with Chapman's voice. Michael Palin voiced Chapman's father and Terry Jones voiced his mother. Terry Gilliam voiced Graham's psychiatrist. They all play various other roles. Among the original Python group, only Eric Idle was not involved.[37]		On 26 January 2012, Terry Jones announced that the five surviving Pythons would reunite in a sci-fi comedy film called Absolutely Anything.[38] The film would combine computer-generated imagery and live action. It would be directed by Jones based on a script by Jones and Gavin Scott. The plot revolves around a teacher who discovers aliens (voiced by the Pythons) have given him magical powers to do "absolutely anything".[39] Eric Idle responded via Twitter that he would not, in fact, be participating,[40] although he was later added to the cast.[41]		In 2013, the Pythons lost a legal case to Mark Forstater, the film producer of Monty Python and the Holy Grail, over royalties for the derivative work Spamalot. They owed a combined £800,000 in legal fees and back royalties to Forstater. They proposed a reunion show to pay their legal bill.[42]		On 19 November 2013, a new reunion was reported, following months of "secret talks".[43] The original plan was for a live, one-off stage show at the O2 Arena in London on 1 July 2014, with "some of Monty Python's greatest hits, with modern, topical, Pythonesque twists" according to a press release.[44][45][46] The tickets for this show went on sale in November 2013 and sold out in just 43 seconds.[47] Nine additional shows were added, all of them at the O2, the last on 20 July. They have said that their reunion was inspired by South Park creators Trey Parker and Matt Stone, who are massive Monty Python fans.[48]		Michael Palin stated that the final reunion show on 20 July would be the last time that the troupe would perform together. The event was first shown live from the UK nationwide and was titled Monty Python Live (Mostly) and was later reshown at select theatres in recorded form in August.[49][50]		Graham Chapman was originally a medical student, joining the Footlights at Cambridge. He completed his medical training and was legally entitled to practice as a doctor. Chapman is best remembered for the lead roles in Holy Grail, as King Arthur, and Life of Brian, as Brian Cohen. He died of spinal and throat cancer on 4 October 1989. At Chapman's memorial service, Cleese delivered an irreverent eulogy that included all the euphemisms for being dead from the "Dead Parrot" sketch, which they had written. Chapman's comedic fictional memoir, A Liar's Autobiography, was adapted into an animated 3D movie in 2012.		John Cleese is the oldest Python. He met his future Python writing partner, Chapman, in Cambridge. Outside of Python, he is best known for setting up the Video Arts group and for the sitcom Fawlty Towers (co-written with Connie Booth, whom Cleese met during work on Python and to whom he was married for a decade). Cleese has also co-authored several books on psychology and wrote the screenplay for the award-winning A Fish Called Wanda, in which he starred with Michael Palin.		Terry Gilliam, an American by birth, is the only member of the troupe of non-British origin.[51] He started off as an animator and strip cartoonist for Harvey Kurtzman's Help! magazine, one issue of which featured Cleese. Moving from the US to England, he animated features for Do Not Adjust Your Set and was then asked by its makers to join them on their next project: Monty Python's Flying Circus. He co-directed Monty Python and the Holy Grail and directed short segments of other Python films (for instance "The Crimson Permanent Assurance", the short film that appears before The Meaning of Life).		When Monty Python was first formed, two writing partnerships were already in place: Cleese and Chapman, Jones and Palin. That left two in their own corners: Gilliam, operating solo due to the nature of his work, and Eric Idle. Regular themes in Idle's contributions were elaborate wordplay and musical numbers. After Flying Circus, he hosted Saturday Night Live four times in the first five seasons. Idle's initially successful solo career faltered in the 1990s with the failures of his 1993 film Splitting Heirs (written, produced by, and starring him) and 1998's An Alan Smithee Film: Burn Hollywood Burn (in which he starred), which was awarded five Razzies, including 'Worst Picture of the Year'. He revived his career by returning to the source of his worldwide fame, adapting Monty Python material for other media. He also wrote the Broadway musical Spamalot, based on the Holy Grail movie. He also wrote Not the Messiah, an oratorio derived from the Life of Brian.		Terry Jones has been described by other members of the team as the "heart" of the operation. Jones had a lead role in maintaining the group's unity and creative independence. Python biographer George Perry has commented that should "[you] speak to him on subjects as diverse as fossil fuels, or Rupert Bear, or mercenaries in the Middle Ages or Modern China ... in a moment you will find yourself hopelessly out of your depth, floored by his knowledge." Many others agree that Jones is characterised by his irrepressible, good-natured enthusiasm. However, Jones' passion often led to prolonged arguments with other group members—in particular Cleese—with Jones often unwilling to back down. Since his major contributions were largely behind the scenes (direction, writing), and he often deferred to the other members of the group as an actor, Jones' importance to Python was often under-rated. However, he does have the legacy of delivering possibly the most famous line in all of Python, as Brian's mother Mandy in Life of Brian, "He's not the Messiah, he's a very naughty boy!", a line voted the funniest in film history on two occasions.[52][53]		Michael Palin attended Oxford, where he met his Python writing partner Jones. The two also wrote the series Ripping Yarns together. Palin and Jones originally wrote face-to-face, but soon found it was more productive to write apart and then come together to review what the other had written. Therefore, Jones and Palin's sketches tended to be more focused than that of the others, taking one bizarre situation, sticking to it, and building on it. After Flying Circus, Palin hosted Saturday Night Live four times in the first 10 seasons. His comedy output began to decrease in amount following the increasing success of his travel documentaries for the BBC. Palin released a book of diaries from the Python years entitled Michael Palin Diaries 1969–1979, published in 2007.		Several people have been accorded unofficial "Associate Python" status over the years. Occasionally such people have been referred to as the 'seventh Python', in a style reminiscent of George Martin (or other associates of the Beatles) being dubbed "the Fifth Beatle." The two collaborators with the most meaningful and plentiful contributions have been Neil Innes and Carol Cleveland. Both were present and presented as Associate Pythons at the official Monty Python 25th-anniversary celebrations held in Los Angeles in July 1994.		Neil Innes is the only non-Python besides Douglas Adams to be credited with writing material for Flying Circus. He appeared in sketches and the Python films, as well as performing some of his songs in Monty Python Live at the Hollywood Bowl. He was also a regular stand-in for absent team members on the rare occasions when they recreated sketches. For example, he took the place of Cleese at the Concert for George. Gilliam once noted that if anyone qualified for the title of the seventh Python, it would certainly be Innes. He was one of the creative talents in the off-beat Bonzo Dog Band. He would later portray Ron Nasty of the Rutles and write all of the Rutles' compositions for All You Need Is Cash (1978). By 2005, a falling out had occurred between Idle and Innes over additional Rutles projects, the results being Innes' critically acclaimed Rutles "reunion" album The Rutles: Archaeology and Idle's straight-to-DVD The Rutles 2: Can't Buy Me Lunch, each undertaken without the other's participation. According to an interview with Idle in the Chicago Tribune in May 2005, his attitude is that Innes and he go back "too far. And no further." Innes has remained silent on the dispute.		Carol Cleveland was the most important female performer in the Monty Python ensemble, commonly referred to as "the female Python". She was originally hired by producer/director John Howard Davies for just the first five episodes of the Flying Circus. The Pythons then pushed to make Cleveland a permanent recurring performer after producer/director Ian MacNaughton brought in several other actresses who were not as good as she was.[54] Cleveland went on to appear in about two-thirds of the episodes, as well as in all of the Python films, and in most of their stage shows, as well. Her common portrayal as the stereotypical "blonde bimbo" eventually earned her the sobriquet "Carol Cleavage" from the other Pythons, but she felt that the variety of her roles should not be described in such a pejorative way.		Cleese's first wife, Connie Booth, appeared as various characters in all four series of Flying Circus. Her most significant role was the "best girl" of the eponymous Lumberjack in "The Lumberjack Song", though this role was sometimes played by Carol Cleveland. Booth appeared in a total of six sketches and also played one-off characters in Python feature films And Now for Something Completely Different and Monty Python and the Holy Grail.		Douglas Adams was "discovered" by Chapman when a version of Footlights Revue (a 1974 BBC2 television show featuring some of Adams' early work) was performed live in London's West End. In Cleese's absence from the final TV series, the two formed a brief writing partnership, with Adams earning a writing credit in one episode for a sketch called "Patient Abuse". In the sketch, a man who had been stabbed by a nurse arrives at his doctor's office bleeding profusely from the stomach, when the doctor makes him fill in numerous senseless forms before he can administer treatment. He also had two cameo appearances in this season. Firstly, in the episode "The Light Entertainment War", Adams shows up in a surgeon's mask (as Dr. Emile Koning, according to the on-screen captions), pulling on gloves, while Palin narrates a sketch that introduces one person after another, and never actually gets started. Secondly, at the beginning of "Mr. Neutron", Adams is dressed in a "pepperpot" outfit and loads a missile onto a cart being driven by Terry Jones, who is calling out for scrap metal ("Any old iron ..."). Adams and Chapman also subsequently attempted a few non-Python projects, including Out of the Trees. He also contributed to a sketch on the soundtrack album for Monty Python and the Holy Grail.		Other than Carol Cleveland, the only other non-Python to make a significant number of appearances in the Flying Circus was Ian Davidson. He appeared in the first two series of the show, and played over 10 roles. While Davidson is primarily known as a scriptwriter, it is not known if he had any contribution toward the writing of the sketches, as he is only credited as a performer. In total, Davidson is credited as appearing in eight episodes of the show, which is more than any other male actor who was not a Python. Despite this, Davidson did not appear in any Python-related media subsequent to series 2, though footage of him was shown on the documentary Python Night – 30 Years of Monty Python.		Stand-up comedian Eddie Izzard, a devoted fan of the group, has occasionally stood in for absent members. When the BBC held a "Python Night" in 1999 to celebrate 30 years of the first broadcast of Flying Circus, the Pythons recorded some new material with Izzard standing in for Idle, who had declined to partake in person (he taped a solo contribution from the US). Izzard hosted The Life of Python (1999), a history of the group that was part of Python Night and appeared with them at a festival/tribute in Aspen, Colorado, in 1998 (released on DVD as Live at Aspen). Izzard has said that Monty Python was a significant influence on his style of comedy and Cleese has referred to him as "the lost Python".[55]		Series director of Flying Cirus, Ian MacNaughton, is also regularly associated with the group and made a few on-screen appearances in the show and in the film And Now for Something Completely Different. Apart from Neil Innes, others to contribute musically included Fred Tomlinson and the Fred Tomlinson Singers. They made appearances in songs such as "The Lumberjack Song" as a backup choir. In addition, various other contributors and performers for the Pythons included John Howard Davies, John Hughman, Lyn Ashley, Bob Raymond, John Young, Rita Davies, Stanley Mason, Maureen Flanagan, and David Ballantyne.		Monty Python in films		Monty Python live		Monty Python reunions		By the time of Monty Python's 25th anniversary, in 1994, the point was already being made that "the five surviving members had with the passing years begun to occupy an institutional position in the edifice of British social culture that they had once had so much fun trying to demolish".[56] A similar point is made in a 2006 book on the relationship between Monty and philosophy: "It is remarkable, after all, not only that the utterly bizarre Monty Python's Flying Circus was sponsored by the BBC in the first place, but that Monty Python itself grew into an institution of enormous cultural influence."[57]		Monty Python has been named as being influential to the comedy stylings of a great many additional people including: Sacha Baron Cohen,[59] David Cross,[60] Noel Fielding, Seth MacFarlane,[61] Seth Meyers,[62] Trey Parker,[63] Matt Stone,[63] and Vic and Bob.[64]		Amongst the more visible cultural influences of Monty Python is the inclusion of terms either directly from, or derived from, Monty Python, into the lexicon of the English language. *The most obvious of these is the term 'pythonesque', which has become a byword in surreal humour, and is included in standard dictionaries.[67] Terry Jones commented on his disappointment at the existence of such a term, claiming the initial aim of Monty Python was to create something new and impossible to categorise, and "the fact that Pythonesque is now a word in the Oxford English Dictionary shows the extent to which we failed".[68]		The Japanese anime series, Girls und Panzer, featured the special episode, "Survival War!", which referenced the 'Spam' sketch.[71]		Beyond a dictionary definition, Python terms have entered the lexicon in other ways.		On St George's Day, 23 April 2007, the cast and creators of Spamalot gathered in Trafalgar Square under the tutelage of the two Terrys (Jones and Gilliam) to set a new record for the world's largest coconut orchestra. They led 5,567 people "clip-clopping" in time to the Python classic, "Always Look on the Bright Side of Life", for the Guinness World Records attempt.[80]		Five Monty Python productions were released as theatrical films:		
Adab or Udab (Sumerian: 𒌓𒉣𒆠 Adabki,[1] spelled UD.NUNKI[2]) was an ancient Sumerian city between Telloh and Nippur. It was located at the site of modern Bismaya or Bismya in the Wasit Province of Iraq.						Initial examinations of the site of Bismaya were by William Hayes Ward of the Wolfe Expedition in 1885 and by John Punnett Peters of the University of Pennsylvania in 1890, each spending a day there and find one cuneiform table and a few fragments.[3] Walter Andrae visited Bismaya in 1902, found a table fragment and produced a sketch map of the site.[4]		Excavations conducted there for six months, from Christmas of 1903 to June 1904, for the University of Chicago, by Dr. Edgar James Banks, proved that these mounds covered the site of the ancient city of Adab (Ud-Nun), hitherto known only from the Sumerian king list and a brief mention of its name in the introduction to the Hammurabi Code. The city was divided into two parts by a canal, on an island in which stood the temple, E-mach, with a ziggurat, or stepped tower. It was evidently once a city of considerable importance, but deserted at a very early period, since the ruins found close to the surface of the mounds belong to Shulgi and Ur-Nammu, kings of the Third Dynasty of Ur in the latter part of the third millennium BC, based on inscribed bricks excavated at Bismaya. Immediately below these, as at Nippur, were found artifacts dating to the reign of Naram-Suen and Sargon of Akkad, c. 2300 BC. Below these there were still 10.5 metres (34 ft) of stratified remains, constituting seven-eighths of the total depth of the ruins. Besides the remains of buildings, walls and graves, Dr. Banks discovered a large number of inscribed clay tablets of a very early period, bronze and stone tablets, bronze implements and the like.[5]		Of the tablets, 543 went to the Oriental Institute and roughly 1100, mostly purchased from the locals rather than excavated, went to the Istanbul Museum. The latter are still apparently unpublished.[6] But the two most notable discoveries were a complete statue in white marble, apparently the earliest yet found in Mesopotamia, now in the Istanbul Archaeology Museums, bearing the inscription, translated by Banks as "E-mach, King Da-udu, King of, Ud-Nun";[7] and a temple refuse heap, consisting of great quantities of fragments of vases in marble, alabaster, onyx, porphyry and granite, some of which were inscribed, and others engraved and inlaid with ivory and precious stones.[5]		Of the Adab tablets that ended up at the University of Chicago, sponsor of the excavations, all have been published and also made available in digital form online.[8] Of the purchased tablets sold piecemeal to various owners, a few have also made their way into publication.[9]		Though the Banks expedition to Bismaya was well documented by the standards of the time and many objects photographed, no final report was ever produced due to personal disputes. Recently, the Oriental Institute has re-examined the records and objects returned to the institute by Banks and produced a report.[10]		There is a Sumerian comic tale of the Three Ox-drivers from Adab.[11]		A group of tells or settlement mounds are what remains of the ancient city. The mounds are about 1.5 kilometres (0.93 mi) long and 3 kilometres (1.9 mi) wide, consisting of a number of low ridges, nowhere exceeding 12 metres (39 ft) in height, lying somewhat nearer to the Tigris than the Euphrates, about a day's journey to the south-east of Nippur.		Adab was occupied from at least the Early Dynastic period. According to Sumerian text Inanna's descent to the netherworld, there was a temple of Inanna named E-shar at Adab during the reign of Dumuzid of Uruk. In another text in the same series, Dumuzid's dream, Dumuzid of Uruk is toppled from his opulence by a hungry mob composed of men from the major cities of Sumer, including Adab.		A king of Kish, Mesilim, appears to have ruled at Adab, based on inscriptions found at Bismaya. One king of Adab, Lugal-Anne-Mundu, appearing in the Sumerian King List, is mentioned in few contemporary inscriptions; some that are much later copies claim that he established a vast, but brief empire stretching from Elam all the way to Lebanon and the Amorite territories along the Jordan. Adab is also mentioned in some of the Ebla tablets from roughly the same era as a trading partner of Ebla in northern Syria, shortly before Ebla was destroyed by unknown forces.[12]		A marble statue was found at Bismaya inscribed with the name of another king of Adab, variously translated as Lugal-daudu, Da-udu, Lugaldalu, and Esar.[13] Brick stamps, found by Banks during his excavation of Adab state that the Akkadian ruler Naram-Suen built a temple to Inanna at Adab, but the temple was not found during the dig, and is not known for certain to be E-shar.		Several governors of the city under Ur III are also known. While no later archaeological evidence was found at Bismaya, the excavations there were brief, and there were later epigraphic references to Adab, such as in the Code of Hammurabi.		
The "Dead Parrot Sketch", alternatively and originally known as the "Pet Shop Sketch" or "Parrot Sketch", is a sketch from Monty Python's Flying Circus. It was written by John Cleese and Graham Chapman and initially performed in the show's first series, in the eighth episode ("Full Frontal Nudity", which first aired 7 December 1969).[1]		The sketch portrays a conflict between disgruntled customer Mr Praline (played by Cleese) and a shopkeeper (Michael Palin), who argue whether or not a "Norwegian Blue" parrot is dead. It pokes fun at the many euphemisms for death used in British culture.		The "Dead Parrot" sketch was inspired by a "Car Salesman" sketch that Palin and Chapman had done in How to Irritate People. In it, Palin played a car salesman who repeatedly refused to admit that there was anything wrong with his customer's (Chapman) car, even as it fell apart in front of him. That sketch was based on an actual incident between Palin and a car salesman.[2] In Monty Python Live at Aspen, Palin said that this salesman "had an excuse for everything". John Cleese said on the same show that he and Chapman "believed that there was something very funny there, if we could find the right context for it". In early drafts of what would become the Dead Parrot Sketch, the frustrated customer was trying to return a faulty toaster to a shop. Chapman realised that it needed to be "madder", and came up with the parrot idea.[3][dead link]		Over the years, Cleese and Palin have done many versions of the "Dead Parrot" sketch for various television shows, record albums, and live performances.		"Dead Parrot" was voted the top alternative comedy sketch in a Radio Times poll.[4]				John Cleese enters the pet shop to register a complaint about the dead Norwegian Blue parrot just as the shopkeeper is preparing to close the establishment for lunch. Despite being told that the bird is deceased and that it had been nailed to its perch, the proprietor insists that it is "pining for the fjords" or simply "stunned".[5]		As the exasperated Cleese attempts to wake up the parrot, the shopkeeper tries to make the bird move by hitting the cage, and Cleese erupts into a rage after banging "Polly Parrot" on the counter. After listing off several euphemisms for death ("is no more", "has ceased to be", "bereft of life, it rests in peace", and "this is an ex-parrot") he is told to go to the pet shop run by the shopkeeper's brother in Bolton for a refund. That proves difficult, however, as the proprietor of that store (who is really the shopkeeper, save for a fake moustache) claims this is Ipswich, whereas the railway station attendant (Terry Jones) claims he is in fact in Bolton after all.[5]		Confronting the shopkeeper's "brother" for lying, the shopkeeper claims he was playing a prank on Cleese by sending him to Ipswich, which was a palindrome for Bolton; Cleese points out that the shopkeeper was wrong because a palindrome for Bolton would have been "Notlob".		Just as Cleese has decided that "this is getting too silly", Graham Chapman's no-nonsense Colonel bursts in and orders the sketch stopped.[5]		In the 1971 film And Now For Something Completely Different, the sketch ends with the shopkeeper explaining that he always wanted to be a lumberjack and, ignoring Mr Praline's protests of that being irrelevant, subsequently begins singing "The Lumberjack Song".		The Monty Python Live at Drury Lane album features a live version of the sketch, which is slightly different from the TV version. Praline's rant about the deceased parrot includes "He fucking snuffed it!" Also, the sketch ends with the shopkeeper saying that he has a slug that does talk. Cleese, after a brief pause, says, "Right, I'll have that one, then!" According to Michael Palin's published diary, Palin changed his response in order to throw Cleese off. During this performance something occurs on stage that does not translate into audio, but causes the audience to break into hysterics upon Cleese's follow-up line "Now that's what I call a dead parrot".		The 1976 Monty Python Live at City Center performance ended with the slug lines, followed by:		On the Rhino Records' compilation Dead Parrot Society, a live performance from The Secret Policeman's Ball in 1976 has Palin cracking up while Cleese declares "Pining for the fjords? What kind of talk is that?" The audience cheers this bit of breaking character, but Palin quickly composes himself and Cleese declares "Now, look! This is nothing to laugh at!" before proceeding with the sketch. This version is included in the book and CD set The Best British Stand-Up and Comedy Routines, along with a transcript of the sketch and the Four Yorkshiremen sketch.		In his appearance on The Muppet Show, Cleese appears as a pirate attempting to take over a spaceship during a "Pigs In Space" sketch. At the end of the sketch, he demands of the smart-mouthed talking parrot on his shoulder, "Do you want to be an ex-parrot?"		In 1989's The Secret Policeman's Biggest Ball, a benefit for Amnesty International, the sketch opens similarly, but ends very differently:		In a 1997 Saturday Night Live performance of the sketch, Cleese added a line to the rant: "Its metabolic processes are a matter of interest only to historians!" In an interview on NPR's Fresh Air, Palin attributed an almost dead audience to his seeing guests reverently mouthing the words of the sketch, rather than laughing at it. To end the sketch, Palin asked Cleese, "Do you want to come back to my place?" to which Cleese said, "I thought you'd never ask!"		In his published diary, Michael Palin recalls that during the filming of Monty Python's Life of Brian in Tunisia, Spike Milligan (who happened to be there on holiday) regaled the Pythons with his own version of the Dead Parrot sketch, but changed "Norwegian Blue" to "Arctic Grey".		In a 2002 interview with Michael Parkinson, John Cleese said that when he and Palin were performing the sketch on Drury Lane, Palin made him laugh by saying, when asked if his slug could talk, "It mutters a bit" instead of "Not really." When Cleese eventually stopped laughing, he couldn't remember where they were in the sketch. He turned to the audience and asked them what the next line was, and people shouted it at him, causing him to wonder, "What is the point of this?" He also says that when he and Palin were asked to do the sketch for Saturday Night Live they sat down together to try to remember the lines, and when they got stuck they considered just going out and stopping somebody on the street to ask how it went, since everybody seemed to have it memorised.		Margaret Thatcher famously used the sketch in a speech at the Conservative Party Conference in 1990, referring to the Liberal Democrats and their symbol being a dove, before ending the speech by commenting, "And now for something completely different."[6]		In 1998, The Sun ran the front-page headline "This party is no more...it has ceased to be...this is an EX-party" for an article about a Conservative Party conference. This turned out to be overly pessimistic.		Trey Parker and Matt Stone made a South Park version of the sketch depicting Cartman angrily returning a dead Kenny to Kyle's shop. Most of the lines are the same in the original sketch. It ends when Terry Gilliam's animations play around with Cartman and everything is crushed by the giant foot.		Cleese and Palin acted out the sketch during the Python's reunion in The O2 in July 2014, Monty Python Live (Mostly). The sketch ended with the shopkeeper saying he has a selection of cheeses, transitioning into the Cheese Shop Sketch. The entire sketch ended like the City Centre performance, with the shopkeeper offering Mr Praline to come back to his place, and Mr Praline replying "I thought you'd never ask." In their final performance on 20 July (which was broadcast live to many theatres all over the world), whilst listing the metaphors for the parrot's death, Cleese added the line "it had expired and gone to meet Dr. Chapman" after which both Cleese and Palin did a thumbs-up to the sky.		In the episode of The Late Show with Stephen Colbert from 13 November 2015, John Cleese is a guest on the show. At the end of the big furry hat segment (where Colbert and in this specific instance Cleese, create nonsensical rules), Cleese says, "Do you want to come back to my place?" and Stephen answers, "I thought you'd never ask."[7]		At Graham Chapman's memorial service, John Cleese began his eulogy by reprising euphemisms from the sketch.[8]		In 1998 there was a follow up with their Live at Aspen, with the supposed ashes of Chapman. Midway through the interview, Terry Gilliam put his feet up on the table and knocked the urn off, spilling the ashes and prompting a frantic, slapstick attempt to clean him up with a dustpan and brush, and subsequently a dustbuster.		In an anniversary concert Jasper Carrot had allowed his fans to pick the material. He opened the performance by announcing this then saying "I've had some funny requests; somebody from Newark wanted me to do the dead parrot sketch".[citation needed]		A joke dated c. CE 400, recently translated from Greek, shows similarities to the Parrot sketch. It was written by Hierocles and Philagrius and was included in a compilation of 265 jokes titled Philogelos: The Laugh Addict. In the Greek version, a man complains to a slave-merchant that his new slave has died. The slave-merchant replies, "When he was with me, he never did any such thing!"[9]		In Mark Twain's humorous short story "A Nevada Funeral", two characters use a series of euphemisms for death including 'kicked the bucket' and 'departed to that mysterious country from whose bourne no traveller returns'.[10]		In 1959, Tony Hancock and Sid James performed a similar sketch involving a dead tortoise.[11]		In the 1960s Freddie "Parrot Face" Davies included an obviously-stuffed caged parrot as part of his stage routine, occasionally complaining that he had been swindled by the seller.[12]		
Humour (British English) or humor (American English; see spelling differences) is the tendency of particular cognitive experiences to provoke laughter and provide amusement. The term derives from the humoral medicine of the ancient Greeks, which taught that the balance of fluids in the human body, known as humours (Latin: humor, "body fluid"), controlled human health and emotion.		People of all ages and cultures respond to humour. Most people are able to experience humour—be amused, smile or laugh at something funny—and thus are considered to have a sense of humour. The hypothetical person lacking a sense of humour would likely find the behaviour inducing it to be inexplicable, strange, or even irrational. Though ultimately decided by personal taste, the extent to which a person finds something humorous depends on a host of variables, including geographical location, culture, maturity, level of education, intelligence and context. For example, young children may favour slapstick such as Punch and Judy puppet shows or the Tom and Jerry cartoons, whose physical nature makes it accessible to them. By contrast, more sophisticated forms of humour such as satire require an understanding of its social meaning and context, and thus tend to appeal to a more mature audience.						Many theories exist about what humour is and what social function it serves. The prevailing types of theories attempting to account for the existence of humour include psychological theories, the vast majority of which consider humour-induced behaviour to be very healthy; spiritual theories, which may, for instance, consider humour to be a "gift from God"; and theories which consider humour to be an unexplainable mystery, very much like a mystical experience.[1]		The benign-violation theory, endorsed by Peter McGraw, attempts to explain humour's existence. The theory says 'humour only occurs when something seems wrong, unsettling, or threatening, but simultaneously seems okay, acceptable or safe'.[2] Humour can be used as a method to easily engage in social interaction by taking away that awkward, uncomfortable, or uneasy feeling of social interactions.		Others believe that 'the appropriate use of humour can facilitate social interactions'.[3]		Some claim that humour cannot or should not be explained. Author E.B. White once said, "Humor can be dissected as a frog can, but the thing dies in the process and the innards are discouraging to any but the pure scientific mind."[4] Counter to this argument, protests against "offensive" cartoons invite the dissection of humour or its lack by aggrieved individuals and communities. This process of dissecting humour does not necessarily banish a sense of humour but begs attention towards its politics and assumed universality (Khanduri 2014).[5]		Arthur Schopenhauer lamented the misuse of humour (a German loanword from English) to mean any type of comedy. However, both humour and comic are often used when theorising about the subject. The connotations of humour as opposed to comic are said to be that of response versus stimulus. Additionally, humour was thought to include a combination of ridiculousness and wit in an individual; the paradigmatic case being Shakespeare's Sir John Falstaff. The French were slow to adopt the term humour; in French, humeur and humour are still two different words, the former referring to a person's mood or to the archaic concept of the four humours.[citation needed]		Non-satirical humour can be specifically termed droll humour or recreational drollery.[6][7]		As with any art form, the acceptance of a particular style or incidence of humour depends on sociological factors and varies from person to person. Throughout history, comedy has been used as a form of entertainment all over the world, whether in the courts of the Western kings or the villages of the Far East. Both a social etiquette and a certain intelligence can be displayed through forms of wit and sarcasm. Eighteenth-century German author Georg Lichtenberg said that "the more you know humour, the more you become demanding in fineness."[citation needed]		Western humour theory begins with Plato, who attributed to Socrates (as a semi-historical dialogue character) in the Philebus (p. 49b) the view that the essence of the ridiculous is an ignorance in the weak, who are thus unable to retaliate when ridiculed. Later, in Greek philosophy, Aristotle, in the Poetics (1449a, pp. 34–35), suggested that an ugliness that does not disgust is fundamental to humour.		In ancient Sanskrit drama, Bharata Muni's Natya Shastra defined humour (hāsyam) as one of the nine nava rasas, or principle rasas (emotional responses), which can be inspired in the audience by bhavas, the imitations of emotions that the actors perform. Each rasa was associated with a specific bhavas portrayed on stage.		The terms comedy and satire became synonymous after Aristotle's Poetics was translated into Arabic in the medieval Islamic world, where it was elaborated upon by Arabic writers and Islamic philosophers such as Abu Bischr, his pupil Al-Farabi, Persian Avicenna, and Averroes. Due to cultural differences, they disassociated comedy from Greek dramatic representation, and instead identified it with Arabic poetic themes and forms, such as hija (satirical poetry). They viewed comedy as simply the "art of reprehension" and made no reference to light and cheerful events or troublesome beginnings and happy endings associated with classical Greek comedy. After the Latin translations of the 12th century, the term comedy thus gained a new semantic meaning in Medieval literature.[8]		Mento star Lord Flea, stated in an 1957 interview that he thought that: "West Indians have the best sense of humour in the world. Even in the most solemn song, like Las Kean Fine ["Lost and Can Not Be Found"], which tells of a boiler explosion on a sugar plantation that killed several of the workers, their natural wit and humour shine though."[9]		Confucianist Neo-Confucian orthodoxy, with its emphasis on ritual and propriety, has traditionally looked down upon humour as subversive or unseemly. The Confucian "Analects" itself, however, depicts the Master as fond of humorous self-deprecation, once comparing his wanderings to the existence of a homeless dog.[10] Early Daoist philosophical texts such as "Zhuangzi" pointedly make fun of Confucian seriousness and make Confucius himself a slow-witted figure of fun.[11] Joke books containing a mix of wordplay, puns, situational humor, and play with taboo subjects like sex and scatology, remained popular over the centuries. Local performing arts, storytelling, vernacular fiction, and poetry offer a wide variety of humorous styles and sensibilities.		Famous Chinese humorists include the ancient jesters Chunyu Kun and Dongfang Shuo; writers of the Ming and Qing dynasties such as Feng Menglong, Li Yu,[12] and Wu Jingzi; and modern comic writers such as Lu Xun, Lin Yutang, Lao She, Qian Zhongshu, Wang Xiaobo, and Wang Shuo, and performers such as Ge You, Guo Degang, and Zhou Libo.		Modern Chinese humor has been heavily influenced not only by indigenous traditions, but also by foreign humor, circulated via print culture, cinema, television, and the internet.[13] During the 1930s, the transliteration "youmo" (humor) caught on as a new term for humor, sparking a fad for humor literature, as well as impassioned debate about what type of humorous sensibility best suited China, a poor, weak country under partial foreign occupation.[14][15][16] While some types of comedy were officially sanctioned during the rule of Mao Zedong, the Party-state's approach towards humor was generally repressive.[17] Social liberalisation in the 1980s, commercialisation of the cultural market in the 1990s, and the advent of the internet have each—despite an invasive state-sponsored censorship apparatus—enabled new forms of humor to flourish in China in recent decades.[18]		The social transformation model of humour predicts that specific characteristics, such as physical attractiveness, interact with humour.[19] This model involves linkages between the humorist, an audience, and the subject matter of the humour.[19] The two transformations associated with this particular model involves the subject matter of the humour, and the change in the audiences perception of the humorous person, therefore establishing a relationship between the humorous speaker and the audience.[19] The social transformation model views humour as adaptive because it communicates the present desire to be humorous as well as future intentions of being humorous.[19] This model is used with deliberate self-deprecating humour where one is communicating with desires to be accepted into someone else’s specific social group.[19] Although self-deprecating humour communicates weakness and fallibility in the bid to gain another's affection, it can be concluded from the model that this type of humour can increase romantic attraction towards the humorist when other variables are also favourable.[19] The social transformation model can also be followed in teaching and lecturing where humour is used to improve the cognitive capabilities of the students. Humour could create a positive and informal classroom environment that triggers students’ enthusiasm and interest.[20]		90% of men and 81% of women, all college students, report having a sense of humour is a crucial characteristic looked for in a romantic partner.[21] Humour and honesty were ranked as the two most important attributes in a significant other.[22] It has since been recorded that humour becomes more evident and significantly more important as the level of commitment in a romantic relationship increases.[23] Recent research suggests expressions of humour in relation to physical attractiveness are two major factors in the desire for future interaction.[19] Women regard physical attractiveness less highly compared to men when it came to dating, a serious relationship, and sexual intercourse.[19] However, women rate humorous men more desirable than nonhumorous individuals for a serious relationship or marriage, but only when these men were physically attractive.[19]		Furthermore, humorous people are perceived by others to be more cheerful but less intellectual than nonhumorous people. Self-deprecating humour has been found to increase the desirability of physically attractive others for committed relationships.[19] The results of a study conducted by McMaster University suggest humour can positively affect one’s desirability for a specific relationship partner, but this effect is only most likely to occur when men use humour and are evaluated by women.[24] No evidence was found to suggest men prefer women with a sense of humour as partners, nor women preferring other women with a sense of humour as potential partners.[24] When women were given the forced-choice design in the study, they chose funny men as potential relationship partners even though they rated them as being less honest and intelligent.[24] Post-Hoc analysis showed no relationship between humour quality and favourable judgments.[24]		It is generally known that humour contributes to higher subjective wellbeing (both physical and psychological).[25] Previous research on humour and psychological well-being show that humour is in fact a major factor in achieving, and sustaining, higher psychological wellbeing.[25][26] This hypothesis is known as general facilitative hypothesis for humour.[25] That is, positive humour leads to positive health. Not all contemporary research, however, supports the previous assertion that humour is in fact a cause for healthier psychological wellbeing.[27] Some of the previous researches’ limitations is that they tend to use a unidimensional approach to humour because it was always inferred that humour was deemed positive. They did not consider the types of humour, or humour styles. For example, self-defeating or aggressive humour.[28] Research has proposed 2 types of humour that each consist of 2 styles, making 4 styles in total. The two types are adaptive versus maladaptive humour.[28] Adaptive humour consist of facilitative and self-enhancing humour, and maladaptive is self-defeating and aggressive humour. Each of these styles can have a different impact on psychological and individuals’ overall subjective wellbeing.[28]		In the study on humour and psychological well-being, research has concluded that high levels of adaptive type humour (affiliative and self-enhancing) is associated with better self-esteem, positive affect, greater self-competency, as well as anxiety control and social interactions.[29] All of which are constituents of psychological wellbeing. Additionally, adaptive humour styles may enable people to preserve their sense of wellbeing despite psychological problems.[26] In contrast, maladaptive humour types (aggressive and self-defeating) are associated with poorer overall psychological wellbeing,[29] emphasis on higher levels of anxiety and depression. Therefore, humour may have detrimental effects on psychological wellbeing, only if that humour is of negative characteristics.[29]		Humour is often used to make light of difficult or stressful situations and to brighten up a social atmosphere in general. It is regarded by many as an enjoyable and positive experience, so it would be reasonable to assume that it humour might have some positive physiological effects on the body.		A study designed to test the positive physiological effects of humour, the relationship between being exposed to humour and pain tolerance in particular, was conducted in 1994 by Karen Zwyer, Barbara Velker, and Willibald Ruch. To test the effects of humour on pain tolerance the test subjects were first exposed to a short humorous video clip and then exposed to the Cold Press Test. To identify the aspects of humour which might contribute to an increase in pain tolerance the study separated its fifty six female participants into three groups, cheerfulness, exhilaration and humour production. The subjects were further separated into two groups, high Trait-Cheerfulness and high Trait-Seriousness according to the State-Trait-Cheerfulness-Inventory. The instructions for the three groups were as follows: the cheerfulness group were told to get excited about the movie without laughing or smiling, the exhilaration group was told to laugh and smile excessively, exaggerating their natural reactions, the humour production group was told to make humorous comments about the video clip as they watched. To ensure that the participants actually found the movie humorous and that it produced the desired effects the participants took a survey on the topic which resulted in a mean score of 3.64 out of 5. The results of the Cold Press Test showed that the participants in all three groups experienced a higher pain threshold, a higher pain tolerance and a lower pain tolerance than previous to the film. The results did not show a significant difference between the three groups.[30]		There are also potential relationships between humour and having a healthy immune system. SIgA is a type of antibody that protects the body from infections. In a method similar to the previous experiment, the participants were shown a short humorous video clip and then tested for the effects. The participants showed a significant increase in SIgA levels.[31]		There have been claims that laughter can be a supplement for cardiovascular exercise and might increase muscle tone.[32] However an early study by Paskind J. showed that laughter can lead to a decrease in skeletal muscle tone because the short intense muscle contractions caused by laughter are followed by longer periods of muscle relaxation. The cardiovascular benefits of laughter also seem to be just a figment of imagination as a study that was designed to test oxygen saturation levels produced by laughter, showed that even though laughter creates sporadic episodes of deep breathing, oxygen saturation levels are not affected.[33]		As humour is often used to ease tension, it might make sense that the same would be true for anxiety. A study by Yovetich N, Dale A, Hudak M. was designed to test the effects humour might have on relieving anxiety. The study subject were told that they would be given to an electric shock after a certain period of time. One group was exposed to humorous content, while the other was not. The anxiety levels were measured through self-report measures as well as the heart rate. Subjects which rated high on sense of humour reported less anxiety in both groups, while subjects which rated lower on sense of humour reported less anxiety in the group which was exposed to the humorous material. However, there was not a significant difference in the heart rate between the subjects.[34]		Humour is a ubiquitous, highly ingrained, and largely meaningful aspect of human experience and is therefore decidedly relevant in organisational contexts, such as the workplace.[35]		The significant role that laughter and fun play in organisational life has been seen as a sociological phenomenon and has increasingly been recognised as also creating a sense of involvement among workers.[36] Sharing humour at work not only offers a relief from boredom, but can also build relationships, improve camaraderie between colleagues and create positive affect.[35] Humour in the workplace may also relieve tension and can be used as a coping strategy.[35] In fact, one of the most agreed upon key impacts that workplace humour has on people’s well being, is the use of humour as a coping strategy to aid in dealing with daily stresses, adversity or other difficult situations.[35] Sharing a laugh with a few colleagues may improve moods, which is pleasurable, and people perceive this as positively affecting their ability to cope.[35] Fun and enjoyment are critical in people's lives and the ability for colleagues to be able to laugh during work, through banter or other, promotes harmony and a sense of cohesiveness.[35]		Humour may also be used to offset negative feelings about a workplace task or to mitigate the use of profanity, or other coping strategies, that may not be otherwise tolerated.[35] Not only can humour in the workplace assist with defusing negative emotions, but it may also be used as an outlet to discuss personal painful events, in a lighter context, thus ultimately reducing anxiety and allowing more happy, positive emotions to surface.[35] Additionally, humour may be used as a tool to mitigate the authoritative tone by managers when giving directives to subordinates. Managers may use self-deprecating humour as a way to be perceived as more human and "real" by their employees.[35] Furthermore, ethnography studies, carried out in a variety of workplace settings, confirmed the importance of a fun space in the workplace.[36] The attachment to the notion of fun by contemporary companies has resulted in workplace management coming to recognise the potentially positive effects of "workplay" and realise that it does not necessarily undermine workers’ performance.[36]		Laughter and play can unleash creativity, thus raising morale, so in the interest of encouraging employee consent to the rigours of the labour process, management often ignore, tolerate and even actively encourage playful practices, with the purpose of furthering organisational goals.[36] Essentially, fun in the workplace is no longer being seen as frivolous.[36] The most current approach of managed fun and laughter in the workplace originated in North America, where it has taken off to such a degree, that it has humour consultants flourishing, as some states have introduced an official "fun at work" day.[36] The results have carried claims of well-being benefits to workers, improved customer experiences and an increase in productivity that organisations can enjoy, as a result.[36] Others examined results of this movement while focusing around the science of happiness – concerned with mental health, motivation, community building and national well-being – and drew attention to the ability to achieve "flow" through playfulness and stimulate "outside the box" thinking.[36] Parallel to this movement is the "positive" scholarship that has emerged in psychology which seeks to empirically theorise the optimisation of human potential.[36] This happiness movement suggests that investing in fun at the workplace, by allowing for laughter and play, will not only create enjoyment and a greater sense of well-being, but it will also enhance energy, performance and commitment in workers.[36]		One of the main focuses of modern psychological humour theory and research is to establish and clarify the correlation between humour and laughter. The major empirical findings here are that laughter and humour do not always have a one-to-one association. While most previous theories assumed the connection between the two almost to the point of them being synonymous, psychology has been able to scientifically and empirically investigate the supposed connection, its implications, and significance.		In 2009, Diana Szameitat conducted a study to examine the differentiation of emotions in laughter. They hired actors and told them to laugh with one of four different emotional associations by using auto-induction, where they would focus exclusively on the internal emotion and not on the expression of laughter itself. They found an overall recognition rate of 44%, with joy correctly classified at 44%, tickle 45%, schadenfreude 37%, and taunt 50%.[37]:399 Their second experiment tested the behavioural recognition of laughter during an induced emotional state and they found that different laughter types did differ with respect to emotional dimensions.[37]:401–402 In addition, the four emotional states displayed a full range of high and low sender arousal and valence.[37]:403 This study showed that laughter can be correlated with both positive (joy and tickle) and negative (schadenfreude and taunt) emotions with varying degrees of arousal in the subject.		This brings into question the definition of humour, then. If it is to be defined by the cognitive processes which display laughter, then humour itself can encompass a variety of negative as well as positive emotions. However, if humour is limited to positive emotions and things which cause positive affect, it must be delimited from laughter and their relationship should be further defined.		Humour has shown to be effective for increasing resilience in dealing with distress and also effective in undoing negative affects.		Madeljin Strick, Rob Holland, Rick van Baaren, and Ad van Knippenberg (2009) of Radboud University conducted a study that showed the distracting nature of a joke on bereaved individuals.[38]:574–578 Subjects were presented with a wide range of negative pictures and sentences. Their findings showed that humorous therapy attenuated the negative emotions elicited after negative pictures and sentences were presented. In addition, the humour therapy was more effective in reducing negative affect as the degree of affect increased in intensity.[38]:575–576 Humour was immediately effective in helping to deal with distress. The escapist nature of humour as a coping mechanism suggests that it is most useful in dealing with momentary stresses. Stronger negative stimuli requires a different therapeutic approach.[citation needed]		Humour is an underlying character trait associated with the positive emotions used in the broaden-and-build theory of cognitive development.		Studies, such as those testing the undoing hypothesis,[39]:313 have shown several positive outcomes of humour as an underlying positive trait in amusement and playfulness. Several studies have shown that positive emotions can restore autonomic quiescence after negative affect. For example, Frederickson and Levinson showed that individuals who expressed Duchenne smiles during the negative arousal of a sad and troubling event recovered from the negative affect approximately 20% faster than individuals who didn’t smile.[39]:314		Using humour judiciously can have a positive influence on cancer treatment.[40]		Humour can serve as a strong distancing mechanism in coping with adversity. In 1997 Kelter and Bonanno found that Duchenne laughter correlated with reduced awareness of distress.[41] Positive emotion is able to loosen the grip of negative emotions on peoples’ thinking. A distancing of thought leads to a distancing of the unilateral responses people often have to negative arousal. In parallel with the distancing role plays in coping with distress, it supports the broaden and build theory that positive emotions lead to increased multilateral cognitive pathway and social resource building.		Humour has been shown to improve and help the ageing process in three areas. The areas are improving physical health, improving social communications, and helping to achieve a sense of satisfaction in life.		Studies have shown that constant humour in the ageing process gives health benefits to individuals. Such benefits as higher self-esteem, lower levels of depression, anxiety, and perceived stress, and a more positive self-concept as well as other health benefits which have been recorded and acknowledged through various studies.[42][43] Even patients with specific diseases have shown improvement with ageing using humour.[44] Overall there is a strong correlation through constant humour in ageing and better health in the individuals.		Another way that research indicates that humour helps with the ageing process, is through helping the individual to create and maintain strong social relationship during transitory periods in their lives.[44] One such example is when people are moved into nursing homes or other facilities of care. With this transition certain social interactions with friend and family may be limited forcing the individual to look else where for these social interactions. Humour has been shown to make transitions easier, as humour is shown reduce stress and facilitate socialisation and serves as a social bonding function.[45] Humour may also help the transition in helping the individual to maintain positive feelings towards those who are enforcing the changes in their lives. These new social interactions can be critical for these transitions in their lives and humour will help these new social interactions to take place making these transitions easier.		Humour can also help ageing individuals maintain a sense of satisfaction in their lives. Through the ageing process many changes will occur, such as losing the right to drive a car. This can cause a decrease in satisfaction in the lives of the individual. Humour helps to alleviate this decrease of satisfaction by allowing the humour to release stress and anxiety caused by changes in the individuals life.[44] Laughing and humour can be a substitute for the decrease in satisfaction by allowing individuals to feel better about their situations by alleviating the stress.[42] This, in turn, can help them to maintain a sense of satisfaction towards their new and changing life style.		In an article published in Nature Reviews Neuroscience, it is reported that a study's results indicate that humour is rooted in the frontal lobe of the cerebral cortex. The study states, in part:		"Humour seems to engage a core network of cortical and subcortical structures, including temporo-occipito-parietal areas involved in detecting and resolving incongruity (mismatch between expected and presented stimuli); and the mesocorticolimbic dopaminergic system and the amygdala, key structures for reward and salience processing."[46]		Humour can be verbal, visual, or physical. Non-verbal forms of communication–for example, music or visual art–can also be humorous.		Rowan Atkinson explains in his lecture in the documentary Funny Business[47] that an object or a person can become funny in three ways:		Most sight gags fit into one or more of these categories.		Some theoreticians of the comic consider exaggeration to be a universal comic device.[48] It may take different forms in different genres, but all rely on the fact that the easiest way to make things laughable is to exaggerate to the point of absurdity their salient traits.[49]		Different cultures have different typical expectations of humour so comedy shows are not always successful when transplanted into another culture. For example, a 2004 BBC News article discusses a stereotype among British comedians that Americans and Germans do not understand irony, and therefore UK sitcoms are not appreciated by them.[50]		
A chapbook is an early type of popular literature printed in early modern Europe. Produced cheaply, chapbooks were commonly small, paper-covered booklets, usually printed on a single sheet folded into books of 8, 12, 16 and 24 pages. They were often illustrated with crude woodcuts, which sometimes bore no relation to the text. When illustrations were included in chapbooks, they were considered popular prints.		The tradition of chapbooks arose in the 16th century, as soon as printed books became affordable, and rose to its height during the 17th and 18th centuries. Many different kinds of ephemera and popular or folk literature were published as chapbooks, such as almanacs, children's literature, folk tales, ballads, nursery rhymes, pamphlets, poetry, and political and religious tracts.		The term "chapbook" for this type of literature was coined in the 19th century. The corresponding French and German terms are bibliothèque bleue (blue book) and Volksbuch, respectively. In Spain they were known as pliegos de cordel.[1][2][3]		The term "chapbook" is also in use for present-day publications, commonly short, inexpensive booklets.[4]						Chapbook is first attested in English in 1824, and seems to derive from the word for the itinerant salesmen who would sell such books: chapman. The first element of chapman comes in turn from Old English cēap ('barter, business, dealing').[5]		Broadside ballads were popular songs, sold for a penny or halfpenny in the streets of towns and villages around Britain between the 16th century and early 20th centuries. They preceded chapbooks, but had similar content, marketing and distribution systems. There are records from Cambridgeshire as early as in 1553 of a man offering a scurrilous ballad "maistres mass" at an alehouse, and a pedlar selling "lytle books" to people, including a patcher of old clothes in 1578. These sales are probably characteristic of the market for chapbooks.		Chapbooks gradually disappeared from the mid-19th century in the face of competition from cheap newspapers and, especially in Scotland, religious tract societies that regarded them as "ungodly." Although the form originated in Britain, many were made in the U.S. during the same period.		Because of their flimsy nature such ephemera rarely survive as individual items. They were aimed at buyers without formal libraries, and, in an era when paper was expensive, were used for wrapping or baking. Paper has also always had hygienic uses and there are contemporary references to the use of chapbooks as bum fodder (i.e. toilet paper). Many of the surviving chapbooks come from the collections of Samuel Pepys between 1661 and 1688 which are now held at Magdalene College, Cambridge. Anthony Wood also collected 65 chapbooks, (including 20 from before 1660), which are now at The Bodleian Library. There are also significant Scottish collections, such as those held by the University of Glasgow.[6]		Modern collectors, such as Peter Opie, have chiefly a scholarly interest in the form.		Chapbooks are mostly small paper-covered booklets, usually printed on a single sheet folded into books of 8, 12, 16 and 24 pages, often illustrated with crude woodcuts, which sometimes bear no relation to the text. They were produced cheaply. One collector, Harry Weiss, wrote: "the printing in many cases was execrable, the paper even worse, and the woodcut illustrations, some of which did duty for various tales regardless of their fitness, were sometimes worse than the paper and presswork combined". However, the category has no real limits: some chapbooks were long, some well produced, and some even historically accurate.		The centre of chapbook and ballad production was London, and until the Great Fire of London the printers were based around London Bridge. However, a feature of chapbooks is the proliferation of provincial printers, especially in Scotland and Newcastle upon Tyne.[7]		Chapbooks were an important medium for the dissemination of popular culture to the common people, especially in rural areas. They were a medium of entertainment, information and (generally unreliable) history. In general, the content of chapbooks has been criticized, though, for their unsophisticated narratives which were heavily loaded with repetition and emphasized adventure through mostly anecdotal structures.[8] However, they are nonetheless valued as a record of popular culture, preserving cultural artifacts that may not survive in any other form.		Chapbooks were priced for sales to workers, although their market was not limited to the working classes. Broadside ballads were sold for a halfpenny, or a few pence. Prices of chapbooks were from 2d. to 6d., when agricultural labourers wages were 12d. per day. The literacy rate in England in the 1640s was around 30 percent for males and rose to 60 percent in the mid-18th century (see Education in the Age of Enlightenment). Many working people were readers, if not writers, and pre-industrial working patterns provided periods during which they could read. Chapbooks were undoubtedly used for reading to family groups or groups in alehouses.		They even contributed to the development of literacy. Francis Kirkman, the author and publisher, wrote about how they fired his imagination and his love of books. There is other evidence of their use by autodidacts.		Nevertheless, the numbers printed are astonishing. In the 1660s as many as 400,000 almanacs were printed annually, enough for one family in three in England. One 17th-century publisher of chapbooks in London had in stock one book for every 15 families in the country. In the 1520s the Oxford bookseller, John Dorne, noted in his day-book selling up to 190 ballads a day at a halfpenny each. The probate inventory of the stock of Charles Tias, of The sign of the Three Bibles on London Bridge, in 1664 included books and printed sheets to make c.90,000 chapbooks (inc. 400 reams of paper) and 37,500 ballad sheets. Tias was not regarded as an outstanding figure in the trade. The inventory of Josiah Blare, of The Sign of the Looking Glass on London Bridge, in 1707 listed 31,000 books, plus 257 reams of printed sheets. A conservative estimate of their sales in Scotland alone in the second half of the 18th century was over 200,000 per year.		These printers provided chapbooks to chapmen on credit, who carried them around the country, selling from door to door, at markets and fairs, and returning to pay for the stock they sold. This facilitated wide distribution and large sales with minimum outlay, and also provided the printers with feedback about what titles were most popular. Popular works were reprinted, pirated, edited, and produced in different editions. Francis Kirkman, whose eye was always on the market, wrote two sequels to the popular Don Bellianus of Greece, first printed in 1598.		Publishers also issued catalogues, and chapbooks are found in the libraries of provincial yeomen and gentry. John Whiting, a Quaker yeoman imprisoned at Ilchester, Somerset in the 1680s had books sent by carrier from London, and left for him at an inn.		Pepys had a collection of ballads bound into volumes, under the following classifications, into which could fit the subject matter of most chapbooks:		The stories in many of the popular chapbooks can be traced back to much earlier origins. Bevis of Hampton was an Anglo-Norman romance of the 13th century, which probably drew on earlier themes. The structure of The Seven Sages of Rome was from the orient, and was used by Chaucer. Many jests about ignorant and greedy clergy in chapbooks were taken from The Friar and the Boy printed about 1500 by Wynkyn de Worde, and The Sackfull of News (1557).		Historical stories set in a mythical and fantastical past were popular. The selection is interesting. Charles I, and Oliver Cromwell do not appear as historical figures in the Pepys collection, and Elizabeth I only once. The Wars of the Roses and the English Civil War do not appear at all. Henry VIII and Henry II appear in disguise, standing up for the right with cobblers and millers and then inviting them to Court and rewarding them. There was a pattern of high born heroes overcoming reduced circumstances by valour, such as St George, Guy of Warwick, Robin Hood (who at this stage has yet to give to the poor what he was stealing from the rich), and heroes of low birth who achieve status through force of arms, such as Clim of Clough, and William of Cloudesley. Clergy often appear as figures of fun, and stupid countrymen were also popular (e.g., The Wise Men of Gotham). Other works were aimed at regional and rural audience (e.g., The Country Mouse and the Town Mouse).		From 1597 works appeared aimed at specific trades, such as clothiers, weavers and shoemakers. The latter were commonly literate. Thomas Deloney, a weaver, wrote Thomas of Reading, about six clothiers from Reading, Gloucester, Worcester, Exeter, Salisbury and Southampton, travelling together and meeting at Basingstoke their fellows from Kendal, Manchester and Halifax. In his, Jack of Newbury, 1600, set in Henry VIII's time, an apprentice to a broadcloth weaver takes over his business and marries his widow on his death. On achieving success, he is liberal to the poor and refuses a knighthood for his substantial services to the king.		Other examples from the Pepys collection include The Countryman's Counsellor, or Everyman his own Lawyer, and Sports and Pastimes, written for schoolboys, including magic tricks, like how to "fetch a shilling out of a handkerchief", write invisibly, make roses out of paper, snare wild duck, and make a maid-servant fart uncontrollably.		The provinces and Scotland had their own local heroes. Robert Burns commented that one of the first two books he read in private was "the history of Sir William Wallace ... poured a Scottish prejudice in my veins which will boil along there till the flood-gates of life shut in eternal rest".		They had a wide and continuing influence. Eighty percent of English folk songs collected by early-20th-century collectors have been linked to printed broadsides, including over 90 of which could only be derived from those printed before 1700. It has been suggested the majority of surviving ballads can be traced to 1550–1600 by internal evidence.		One of the most popular and influential chapbooks was Richard Johnson's Seven Champions of Christendom (1596), believed to be the source for the introduction of the character St George into English folk plays.		Robert Greene's novel, Dorastus and Fawnia, (originally Pandosto) (1588), the basis of Shakespeare's The Winter's Tale was still being published in cheap editions in the 1680s. Some stories were still being published in the 19th century, (e.g., Jack of Newbury, Friar Bacon, Dr Faustus and The Seven Champions of Christendom).		Chapbook is also a term currently used to denote publications of up to about 40 pages, usually poetry bound with some form of saddle stitch, though many are perfect bound, folded, or wrapped. These publications range from low-cost productions to finely produced, hand-made editions that may sell to collectors for hundreds of dollars. More recently, the popularity of fiction chapbooks has also increased. In the UK they are more often referred to as pamphlets.		The genre has been revitalized in the past 40 years by the widespread availability of first mimeograph technology, then low-cost copy centers and digital printing, and by the cultural revolutions spurred by both zines and poetry slams, the latter generating hundreds upon hundreds of self-published chapbooks that are used to fund tours. In New York, a joint effort of the Center for the Humanities at the Graduate Center, CUNY and their sponsors has come up with the NYC/CUNY Chapbook festival where, as it states in their about page, “The NYC/CUNY Chapbook Festival celebrates the chapbook as a work of art, and as a medium for alternative and emerging writers and publishers… the festival features a day-long bookfair with chapbook publishers from around the country, workshops, panels, a chapbook exhibition, and a reading of prize-winning Chapbook Fellows.”[9]		With the recent popularity of blogs, online literary journals, and other online publishers, short collections of poetry published online are frequently referred to as "online chapbooks", "electronic chapbooks", "e-chapbooks", or "e-chaps".[citation needed]		Stephen King wrote a few parts of an early draft of The Plant and sent them out as chapbooks to his friends, instead of Christmas cards, in 1982, 1983, and 1985. Philtrum Press produced just three installments before the story was shelved, and the original editions have been hotly sought-after collector's items.[citation needed]		
PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.		From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]						In addition to MEDLINE, PubMed provides access to:		Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]		Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]		As of 11 July 2017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed's records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950's to 16% in 2016.[8] Other significant proportion of records correspond to “Chemistry” (8.69%), “Therapy” (8.39%) and "Infection" (5%).				In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]		Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window.		PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms.		The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:		Likewise,		The new PubMed interface, launched in October 2009, encourages the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11]		For comprehensive, optimal searches in PubMed, it is necessary to have a thorough understanding of its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features, and are best carried out by PubMed search specialists or librarians,[12] who are able to select the right type of search and carefully adjust it for precision and recall.[13]		When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).		Publication type parameter enables many special features. A special feature of PubMed is its "Clinical Queries" section, where "Clinical Categories", "Systematic Reviews", and "Medical Genetics" subjects can be searched, with study-type 'filters' automatically applied to identify substantial, robust studies.[14] As these 'clinical girish' can generate small sets of robust studies with considerable precision, it has been suggested that this PubMed section can be used as a 'point-of-care' resource.[15]		Since July 2005, the MEDLINE article indexing process extracts important identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]		A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the 'Find related data' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The 'related articles' function has been judged to be so precise that some researchers suggest it can be used instead of a full search.[18]		A strong feature of PubMed is its ability to automatically link to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This important feature makes PubMed searches automatically more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.		The PubMed optional facility "My NCBI" (with free registration) provides tools for		and a wide range of other options.[19] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[20]		LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[21] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March 2010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[22] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.		In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[23]		PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[24] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[25]		askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[26]		A PMID (PubMed identifier or PubMed unique identifier)[27] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[28]		The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.		The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[29] As of October 2008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[30] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.		Lu[30] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:		As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[30] Without the need to store about 90 GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[44]		Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950's to 16% in 2016.[45]		
Cultural anthropology is a branch of anthropology focused on the study of cultural variation among humans. It is in contrast to social anthropology, which perceives cultural variation as a subset of the anthropological constant.		A variety of methods are involved in cultural anthropological, including participant observation (often called fieldwork because it requires the anthropologist spending an extended period of time at the research location), interviews, and surveys.[1]		One of the earliest articulations of the anthropological meaning of the term "culture" came from Sir Edward Tylor who writes on the first page of his 1871 book: "Culture, or civilization, taken in its broad, ethnographic sense, is that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society."[2] The term "civilization" later gave way to definitions given by V. Gordon Childe, with culture forming an umbrella term and civilization becoming a particular kind of culture.[3]		The anthropological concept of "culture" reflects in part a reaction against earlier Western discourses based on an opposition between "culture" and "nature", according to which some human beings lived in a "state of nature".[citation needed] Anthropologists have argued that culture is "human nature", and that all people have a capacity to classify experiences, encode classifications symbolically (i.e. in language), and teach such abstractions to others.		Since humans acquire culture through the learning processes of enculturation and socialization, people living in different places or different circumstances develop different cultures. Anthropologists have also pointed out that through culture people can adapt to their environment in non-genetic ways, so people living in different environments will often have different cultures. Much of anthropological theory has originated in an appreciation of and interest in the tension between the local (particular cultures) and the global (a universal human nature, or the web of connections between people in distinct places/circumstances).[4]		The rise of cultural anthropology took place within the context of the late 19th century, when questions regarding which cultures were "primitive" and which were "civilized" occupied the minds of not only Marx and Freud, but many others. Colonialism and its processes increasingly brought European thinkers into direct or indirect contact with "primitive others."[5] The relative status of various humans, some of whom had modern advanced technologies that included engines and telegraphs, while others lacked anything but face-to-face communication techniques and still lived a Paleolithic lifestyle, was of interest to the first generation of cultural anthropologists.		Parallel with the rise of cultural anthropology in the United States, social anthropology, in which sociality is the central concept and which focuses on the study of social statuses and roles, groups, institutions, and the relations among them—developed as an academic discipline in Britain and in France.[6] The umbrella term socio-cultural anthropology draws upon both cultural and social anthropology traditions.[7]								Anthropology is with the lives of people within different parts of the world, particularly in relation to the discourse of beliefs and practices. In addressing this question, ethnologists in the 19th century divided into two schools of thought. Some, like Grafton Elliot Smith, argued that different groups must have learned from one another somehow, however indirectly; in other words, they argued that cultural traits spread from one place to another, or "diffused".		Other ethnologists argued that different groups had the capability of creating similar beliefs and practices independently. Some of those who advocated "independent invention", like Lewis Henry Morgan, additionally supposed that similarities meant that different groups had passed through the same stages of cultural evolution (See also classical social evolutionism). Morgan, in particular, acknowledged that certain forms of society and culture could not possibly have arisen before others. For example, industrial farming could not have been invented before simple farming, and metallurgy could not have developed without previous non-smelting processes involving metals (such as simple ground collection or mining). Morgan, like other 19th century social evolutionists, believed there was a more or less orderly progression from the primitive to the civilized.		20th-century anthropologists largely reject the notion that all human societies must pass through the same stages in the same order, on the grounds that such a notion does not fit the empirical facts. Some 20th-century ethnologists, like Julian Steward, have instead argued that such similarities reflected similar adaptations to similar environments. Although 19th-century ethnologists saw "diffusion" and "independent invention" as mutually exclusive and competing theories, most ethnographers quickly reached a consensus that both processes occur, and that both can plausibly account for cross-cultural similarities. But these ethnographers also pointed out the superficiality of many such similarities. They noted that even traits that spread through diffusion often were given different meanings and function from one society to another. Analyses of large human concentrations in big cities, in multidisciplinary studies by Ronald Daus, show how new methods may be applied to the understanding of man living in a global world and how it was caused by the action of extra-European nations, so highlighting the role of Ethics in modern anthropology.		Accordingly, most of these anthropologists showed less interest in comparing cultures, generalizing about human nature, or discovering universal laws of cultural development, than in understanding particular cultures in those cultures' own terms. Such ethnographers and their students promoted the idea of "cultural relativism", the view that one can only understand another person's beliefs and behaviors in the context of the culture in which he or she lived or lives.		Others, such as Claude Lévi-Strauss (who was influenced both by American cultural anthropology and by French Durkheimian sociology), have argued that apparently similar patterns of development reflect fundamental similarities in the structure of human thought (see structuralism). By the mid-20th century, the number of examples of people skipping stages, such as going from hunter-gatherers to post-industrial service occupations in one generation, were so numerous that 19th-century evolutionism was effectively disproved.[8]		Cultural relativism is a principle that was established as axiomatic in anthropological research by Franz Boas and later popularized by his students. Boas first articulated the idea in 1887: "...civilization is not something absolute, but ... is relative, and ... our ideas and conceptions are true only so far as our civilization goes."[9] Although Boas did not coin the term, it became common among anthropologists after Boas' death in 1942, to express their synthesis of a number of ideas Boas had developed. Boas believed that the sweep of cultures, to be found in connection with any sub-species, is so vast and pervasive that there cannot be a relationship between culture and race.[10] Cultural relativism involves specific epistemological and methodological claims. Whether or not these claims require a specific ethical stance is a matter of debate. This principle should not be confused with moral relativism.		Cultural relativism was in part a response to Western ethnocentrism. Ethnocentrism may take obvious forms, in which one consciously believes that one's people's arts are the most beautiful, values the most virtuous, and beliefs the most truthful. Boas, originally trained in physics and geography, and heavily influenced by the thought of Kant, Herder, and von Humboldt, argued that one's culture may mediate and thus limit one's perceptions in less obvious ways. This understanding of culture confronts anthropologists with two problems: first, how to escape the unconscious bonds of one's own culture, which inevitably bias our perceptions of and reactions to the world, and second, how to make sense of an unfamiliar culture. The principle of cultural relativism thus forced anthropologists to develop innovative methods and heuristic strategies.		Boas and his students realized that if they were to conduct scientific research in other cultures, they would need to employ methods that would help them escape the limits of their own ethnocentrism. One such method is that of ethnography: basically, they advocated living with people of another culture for an extended period of time, so that they could learn the local language and be enculturated, at least partially, into that culture. In this context, cultural relativism is of fundamental methodological importance, because it calls attention to the importance of the local context in understanding the meaning of particular human beliefs and activities. Thus, in 1948 Virginia Heyer wrote, "Cultural relativity, to phrase it in starkest abstraction, states the relativity of the part to the whole. The part gains its cultural significance by its place in the whole, and cannot retain its integrity in a different situation."[11]		Lewis Henry Morgan (1818–1881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from savagery, to barbarism, to civilization. Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.		Franz Boas established academic anthropology in the United States in opposition to Morgan's evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.		Influenced by the German tradition, Boas argued that the world was full of distinct cultures, rather than societies whose evolution could be measured by how much or how little "civilization" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.		In doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas.[12] Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called "Four Field Approach" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.		Boas used his positions at Columbia University and the American Museum of Natural History to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.		The publication of Alfred Kroeber's textbook, Anthropology, marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.		Though such works as Coming of Age in Samoa and The Chrysanthemum and the Sword remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined by Ralph Linton, and Mead was limited to her offices at the AMNH.		In the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche—an approach popularized by Marvin Harris.		Economic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors, and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated Lévi-Strauss's structuralism into their work. Structuralism also influenced a number of developments in 1960s and 1970s, including cognitive anthropology and componential analysis.		In keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War;[13] Marxism became an increasingly popular theoretical approach in the discipline.[14] By the 1970s the authors of volumes such as Reinventing Anthropology worried about anthropology's relevance.		Since the 1980s issues of power, such as those examined in Eric Wolf's Europe and the People Without History, have been central to the discipline. In the 1980s books like Anthropology and the Colonial Encounter pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins (again), who drew on Lévi-Strauss and Fernand Braudel to examine the relationship between symbolic meaning, sociocultural structure, and individual agency in the processes of historical transformation. Jean and John Comaroff produced a whole generation of anthropologists at the University of Chicago that focused on these themes. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan.[15]		Many anthropologists reacted against the renewed emphasis on materialism and scientific modelling derived from Marx by emphasizing the importance of the concept of culture. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. Geertz was to state:		"Believing, with Max Weber, that man is an animal suspended in webs of significance he himself has spun, I take culture to be those webs, and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning."		Geertz's interpretive method involved what he called "thick description." The cultural symbols of rituals, political and economic action, and of kinship, are "read" by the anthropologist as if they are a document in a foreign language. The interpretation of those symbols must be re-framed for their anthropological audience, i.e. transformed from the "experience-near" but foreign concepts of the other culture, into the "experience-distant" theoretical concepts of the anthropologist. These interpretations must then be reflected back to its originators, and its adequacy as a translation fine-tuned in a repeated way, a process called the hermeneutic circle. Geertz applied his method in a number of areas, creating programs of study that were very productive. His analysis of "religion as a cultural system" was particularly influential outside of anthropology. David Schnieder's cultural analysis of American kinship has proven equally influential.[17] Schneider demonstrated that the American folk-cultural emphasis on "blood connections" had an undue influence on anthropological kinship theories, and that kinship is not a biological characteristic but a cultural relationship established on very different terms in different societies.[18]		Prominent British symbolic anthropologists include Victor Turner and Mary Douglas.		In the late 1980s and 1990s authors such as James Clifford pondered ethnographic authority, in particular how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics.[19] Nevertheless, key aspects of feminist theory and methods became de rigueur as part of the 'post-modern moment' in anthropology: Ethnographies became more interpretative and reflexive,[20] explicitly addressing the author's methodology, cultural, gender and racial positioning, and their influence on his or her ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously.[21] Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.		Modern cultural anthropology has its origins in, and developed in reaction to, 19th century "ethnology", which involves the organized comparison of human societies. Scholars like E.B. Tylor and J.G. Frazer in England worked mostly with materials collected by others – usually missionaries, traders, explorers, or colonial officials – earning them the moniker of "arm-chair anthropologists".		Participant observation is one of the principle research methods of cultural anthropology. It relies on the assumption that the best way to understand a group of people is to interact with them closely over a long period of time.[22] The method originated in the field research of social anthropologists, especially Bronislaw Malinowski in Britain, the students of Franz Boas in the United States, and in the later urban research of the Chicago School of Sociology. Historically, the group of people being studied was a small, non-Western society. However, today it may be a specific corporation, a church group, a sports team, or a small town.[22] There are no restrictions as to what the subject of participant observation can be, as long as the group of people is studied intimately by the observing anthropologist over a long period of time. This allows the anthropologist to develop trusting relationships with the subjects of study and receive an inside perspective on the culture, which helps him or her to give a richer description when writing about the culture later. Observable details (like daily time allotment) and more hidden details (like taboo behavior) are more easily observed and interpreted over a longer period of time, and researchers can discover discrepancies between what participants say—and often believe—should happen (the formal system) and what actually does happen, or between different aspects of the formal system; in contrast, a one-time survey of people's answers to a set of questions might be quite consistent, but is less likely to show conflicts between different aspects of the social system or between conscious representations and behavior.[23]		Interactions between an ethnographer and a cultural informant must go both ways.[24] Just as an ethnographer may be naive or curious about a culture, the members of that culture may be curious about the ethnographer. To establish connections that will eventually lead to a better understanding of the cultural context of a situation, an anthropologist must be open to becoming part of the group, and willing to develop meaningful relationships with its members.[22] One way to do this is to find a small area of common experience between an anthropologist and his or her subjects, and then to expand from this common ground into the larger area of difference.[25] Once a single connection has been established, it becomes easier to integrate into the community, and more likely that accurate and complete information is being shared with the anthropologist.		Before participant observation can begin, an anthropologist must choose both a location and a focus of study.[22] This focus may change once the anthropologist is actively observing the chosen group of people, but having an idea of what one wants to study before beginning fieldwork allows an anthropologist to spend time researching background information on their topic. It can also be helpful to know what previous research has been conducted in one's chosen location or on similar topics, and if the participant observation takes place in a location where the spoken language is not one the anthropologist is familiar with, he or she will usually also learn that language. This allows the anthropologist to become better established in the community. The lack of need for a translator makes communication more direct, and allows the anthropologist to give a richer, more contextualized representation of what they witness. In addition, participant observation often requires permits from governments and research institutions in the area of study, and always needs some form of funding.[22]		The majority of participant observation is based on conversation. This can take the form of casual, friendly dialogue, or can also be a series of more structured interviews. A combination of the two is often used, sometimes along with photography, mapping, artifact collection, and various other methods.[22] In some cases, ethnographers also turn to structured observation, in which an anthropologist's observations are directed by a specific set of questions he or she is trying to answer.[26] In the case of structured observation, an observer might be required to record the order of a series of events, or describe a certain part of the surrounding environment.[26] While the anthropologist still makes an effort to become integrated into the group they are studying, and still participates in the events as they observe, structured observation is more directed and specific than participant observation in general. This helps to standardize the method of study when ethnographic data is being compared across several groups or is needed to fulfill a specific purpose, such as research for a governmental policy decision.		One common criticism of participant observation is its lack of objectivity.[22] Because each anthropologist has his or her own background and set of experiences, each individual is likely to interpret the same culture in a different way. Who the ethnographer is has a lot to do with what he or she will eventually write about a culture, because each researcher is influenced by his or her own perspective.[27] This is considered a problem especially when anthropologists write in the ethnographic present, a present tense which makes a culture seem stuck in time, and ignores the fact that it may have interacted with other cultures or gradually evolved since the anthropologist made observations.[22] To avoid this, past ethnographers have advocated for strict training, or for anthropologists working in teams. However, these approaches have not generally been successful, and modern ethnographers often choose to include their personal experiences and possible biases in their writing instead.[22]		Participant observation has also raised ethical questions, since an anthropologist is in control of what he or she reports about a culture. In terms of representation, an anthropologist has greater power than his or her subjects of study, and this has drawn criticism of participant observation in general.[22] Additionally, anthropologists have struggled with the effect their presence has on a culture. Simply by being present, a researcher causes changes in a culture, and anthropologists continue to question whether or not it is appropriate to influence the cultures they study, or possible to avoid having influence.[22]		In the 20th century, most cultural and social anthropologists turned to the crafting of ethnographies. An ethnography is a piece of writing about a people, at a particular place and time. Typically, the anthropologist lives among people in another society for a period of time, simultaneously participating in and observing the social and cultural life of the group.		Numerous other ethnographic techniques have resulted in ethnographic writing or details being preserved, as cultural anthropologists also curate materials, spend long hours in libraries, churches and schools poring over records, investigate graveyards, and decipher ancient scripts. A typical ethnography will also include information about physical geography, climate and habitat. It is meant to be a holistic piece of writing about the people in question, and today often includes the longest possible timeline of past events that the ethnographer can obtain through primary and secondary research.		Bronisław Malinowski developed the ethnographic method, and Franz Boas taught it in the United States. Boas' students such as Alfred L. Kroeber, Ruth Benedict and Margaret Mead drew on his conception of culture and cultural relativism to develop cultural anthropology in the United States. Simultaneously, Malinowski and A.R. Radcliffe Brown´s students were developing social anthropology in the United Kingdom. Whereas cultural anthropology focused on symbols and values, social anthropology focused on social groups and institutions. Today socio-cultural anthropologists attend to all these elements.		In the early 20th century, socio-cultural anthropology developed in different forms in Europe and in the United States. European "social anthropologists" focused on observed social behaviors and on "social structure", that is, on relationships among social roles (for example, husband and wife, or parent and child) and social institutions (for example, religion, economy, and politics).		American "cultural anthropologists" focused on the ways people expressed their view of themselves and their world, especially in symbolic forms, such as art and myths. These two approaches frequently converged and generally complemented one another. For example, kinship and leadership function both as symbolic systems and as social institutions. Today almost all socio-cultural anthropologists refer to the work of both sets of predecessors, and have an equal interest in what people do and in what people say.		One means by which anthropologists combat ethnocentrism is to engage in the process of cross-cultural comparison. It is important to test so-called "human universals" against the ethnographic record. Monogamy, for example, is frequently touted as a universal human trait, yet comparative study shows that it is not. The Human Relations Area Files, Inc. (HRAF) is a research agency based at Yale University. Since 1949, its mission has been to encourage and facilitate worldwide comparative studies of human culture, society, and behavior in the past and present. The name came from the Institute of Human Relations, an interdisciplinary program/building at Yale at the time. The Institute of Human Relations had sponsored HRAF's precursor, the Cross-Cultural Survey (see George Peter Murdock), as part of an effort to develop an integrated science of human behavior and culture. The two eHRAF databases on the Web are expanded and updated annually. eHRAF World Cultures includes materials on cultures, past and present, and covers nearly 400 cultures. The second database, eHRAF Archaeology, covers major archaeological traditions and many more sub-traditions and sites around the world.		Comparison across cultures includies the industrialized (or de-industrialized) West. Cultures in the more traditional standard cross-cultural sample of small scale societies are:		Ethnography dominates socio-cultural anthropology. Nevertheless, many contemporary socio-cultural anthropologists have rejected earlier models of ethnography as treating local cultures as bounded and isolated. These anthropologists continue to concern themselves with the distinct ways people in different locales experience and understand their lives, but they often argue that one cannot understand these particular ways of life solely from a local perspective; they instead combine a focus on the local with an effort to grasp larger political, economic, and cultural frameworks that impact local lived realities. Notable proponents of this approach include Arjun Appadurai, James Clifford, George Marcus, Sidney Mintz, Michael Taussig, Eric Wolf and Ronald Daus.		A growing trend in anthropological research and analysis is the use of multi-sited ethnography, discussed in George Marcus' article, "Ethnography In/Of the World System: the Emergence of Multi-Sited Ethnography". Looking at culture as embedded in macro-constructions of a global social order, multi-sited ethnography uses traditional methodology in various locations both spatially and temporally. Through this methodology, greater insight can be gained when examining the impact of world-systems on local and global communities.		Also emerging in multi-sited ethnography are greater interdisciplinary approaches to fieldwork, bringing in methods from cultural studies, media studies, science and technology studies, and others. In multi-sited ethnography, research tracks a subject across spatial and temporal boundaries. For example, a multi-sited ethnography may follow a "thing," such as a particular commodity, as it is transported through the networks of global capitalism.		Multi-sited ethnography may also follow ethnic groups in diaspora, stories or rumours that appear in multiple locations and in multiple time periods, metaphors that appear in multiple ethnographic locations, or the biographies of individual people or groups as they move through space and time. It may also follow conflicts that transcend boundaries. An example of multi-sited ethnography is Nancy Scheper-Hughes' work on the international black market for the trade of human organs. In this research, she follows organs as they are transferred through various legal and illegal networks of capitalism, as well as the rumours and urban legends that circulate in impoverished communities about child kidnapping and organ theft.		Sociocultural anthropologists have increasingly turned their investigative eye on to "Western" culture. For example, Philippe Bourgois won the Margaret Mead Award in 1997 for In Search of Respect, a study of the entrepreneurs in a Harlem crack-den. Also growing more popular are ethnographies of professional communities, such as laboratory researchers, Wall Street investors, law firms, or information technology (IT) computer employees.[28]		Kinship refers to the anthropological study of the ways in which humans form and maintain relationships with one another, and further, how those relationships operate within and define social organization.[29]		Research in kinship studies often crosses over into different anthropological subfields including medical, feminist, and public anthropology. This is likely due to its fundamental concepts, as articulated by linguistic anthropologist Patrick McConvell:		Kinship is the bedrock of all human societies that we know. All humans recognize fathers and mothers, sons and daughters, brothers and sisters, uncles and aunts, husbands and wives, grandparents, cousins, and often many more complex types of relationships in the terminologies that they use. That is the matrix into which human children are born in the great majority of cases, and their first words are often kinship terms.[30]		Throughout history, kinship studies have primarily focused on the topics of marriage, descent, and procreation.[31] Anthropologists have written extensively on the variations within marriage across cultures and its legitimacy as a human institution. There are stark differences between communities in terms of marital practice and value, leaving much room for anthropological fieldwork. For instance, the Nuer of Sudan and the Brahmans of Nepal practice polygyny, where one man has several marriages to two or more women. The Nyar of India and Nyimba of Tibet and Nepal practice polyandry, where one woman is often married to two or more men. The marital practice found in most cultures, however, is monogamy, where one woman is married to one man. Anthropologists also study different marital taboos across cultures, most commonly the incest taboo of marriage within sibling and parent-child relationships. It has been found that all cultures have an incest taboo to some degree, but the taboo shifts between cultures when the marriage extends beyond the nuclear family unit.[29]		There are similar foundational differences where the act of procreation is concerned. Although anthropologists have found that biology is acknowledged in every cultural relationship to procreation, there are differences in the ways in which cultures assess the constructs of parenthood. For example, in the Nuyoo municipality of Oaxaca, Mexico, it is believed that a child can have partible maternity and partible paternity. In this case, a child would have multiple biological mothers in the case that it is born of one woman and then breastfed by another. A child would have multiple biological fathers in the case that the mother had sex with multiple men, following the commonplace belief in Nuyoo culture that pregnancy must be preceded by sex with multiple men in order have the necessary accumulation of semen.[32]		In the twenty-first century, Western ideas of kinship have evolved beyond the traditional assumptions of the nuclear family, raising anthropological questions of consanguinity, lineage, and normative marital expectation. The shift can be traced back to the 1960s, with the reassessment of kinship's basic principles offered by Edmund Leach, Rodney Neeham, David Schneider, and others.[31] Instead of relying on narrow ideas of Western normalcy, kinship studies increasingly catered to "more ethnographic voices, human agency, intersecting power structures, and historical contex".[33] The study of kinship evolved to accommodate for the fact that it cannot be separated from its institutional roots and must pay respect to the society in which it lives, including that society's contradictions, hierarchies, and individual experiences of those within it. This shift was progressed further by the emergence of second-wave feminism in the early 1970s, which introduced ideas of martial oppression, sexual autonomy, and domestic subordination. Other themes that emerged during this time included the frequent comparisons between Eastern and Western kinship systems and the increasing amount of attention paid to anthropologists' own societies, a swift turn from the focus that had traditionally been paid to largely "foreign", non-Western communities.[31]		Kinship studies began to gain mainstream recognition in the late 1990s with the surging popularity of feminist anthropology, particularly with its work related to biological anthropology and the intersectional critique of gender relations. At this time, there was the arrival of "Third World feminism", a movement that argued kinship studies could not examine the gender relations of developing countries in isolation, and must pay respect to racial and economic nuance as well. This critique became relevant, for instance, in the anthropological study of Jamaica: race and class were seen as the primary obstacles to Jamaican liberation from economic imperialism, and gender as an identity was largely ignored. Third World feminism aimed to combat this in the early twenty-first century by promoting these categories as coexisting factors. In Jamaica, marriage as an institution is often substituted for a series of partners, as poor women cannot rely on regular financial contributions in a climate of economic instability. In addition, there is a common practice of Jamaican women artificially lightening their skin tones in order to secure economic survival. These anthropological findings, according to Third World feminism, cannot see gender, racial, or class differences as separate entities, and instead must acknowledge that they interact together to produce unique individual experiences.[33]		Kinship studies have also experienced a rise in the interest of reproductive anthropology with the advancement of assisted reproductive technologies (ARTs), including in vitro fertilization (IVF). These advancements have led to new dimensions of anthropological research, as they challenge the Western standard of biogenetically based kinship, relatedness, and parenthood. According to anthropologists Maria C. Inhorn and Daphna Birenbaum-Carmeli, "ARTs have pluralized notions of relatedness and led to a more dynamic notion of "kinning" namely, kinship as a process, as something under construction, rather than a natural given".[34] With this technology, questions of kinship have emerged over the difference between biological and genetic relatedness, as gestational surrogates can provide a biological environment for the embryo while the genetic ties remain with a third party.[35] If genetic, surrogate, and adoptive maternities are involved, anthropologists have acknowledged that there can be the possibility for three "biological" mothers to a single child.[34] With ARTs, there are also anthropological questions concerning the intersections between wealth and fertility: ARTs are generally only available to those in the highest income bracket, meaning the infertile poor are inherently devalued in the system. There have also been issues of reproductive tourism and bodily commodification, as individuals seek economic security through hormonal stimulation and egg harvesting, which are potentially harmful procedures. With IVF, specifically, there have been many questions of embryotic value and the status of life, particularly as it relates to the manufacturing of stem cells, testing, and research.[34]		Current issues in kinship studies, such as adoption, have revealed and challenged the Western cultural disposition towards the genetic, "blood" tie.[36] Western biases against single parent homes have also been explored through similar anthropological research, uncovering that a household with a single parent experiences "greater levels of scrutiny and [is] routinely seen as the 'other' of the nuclear, patriarchal family".[37] The power dynamics in reproduction, when explored through a comparative analysis of "conventional" and "unconventional" families, have been used to dissect the Western assumptions of child bearing and child rearing in contemporary kinship studies.		Kinship, as an anthropological field of inquiry, has been heavily criticized across the discipline. One critique is that, as its inception, the framework of kinship studies was far too structured and formulaic, relying on dense language and stringent rules.[33] Another critique, explored at length by American anthropologist David Schneider, argues that kinship has been limited by its inherent Western ethnocentrism. Schneider proposes that kinship is not a field that can be applied cross-culturally, as the theory itself relies on European assumptions of normalcy. He states in the widely circulated 1984 book A critique of the study of kinship that "[K]inship has been defined by European social scientists, and European social scientists use their own folk culture as the source of many, if not all of their ways of formulating and understanding the world about them".[38] However, this critique has been challenged by the argument that it is linguistics, not cultural divergence, that has allowed for a European bias, and that the bias can be lifted by centering the methodology on fundamental human concepts. Polish anthropologist Anna Wierzbicka argues that "mother" and "father" are examples of such fundamental human concepts, and can only be Westernized when conflated with English concepts such as "parent" and "sibling".[39]		A more recent critique of kinship studies is its solipsistic focus on privileged, Western human relations and its promotion of normative ideals of human exceptionalism. In "Critical Kinship Studies", social psychologists Elizabeth Peel and Damien Riggs argue for a move beyond this human-centered framework, opting instead to explore kinship through a "posthumanist" vantage point where anthropologists focus on the intersecting relationships of human animals, non-human animals, technologies and practices.[40]		The role of anthropology in institutions has expanded significantly since the end of the 20th century.[41] Much of this development can be attributed to the rise in anthropologists working outside of academia and the increasing importance of globalization in both institutions and the field of anthropology.[41] Anthropologists can be employed by institutions such as for-profit business, nonprofit organizations, and governments.[41] For instance, cultural anthropologists are commonly employed by the United States federal government.[41]		The two types of institutions defined in the field of anthropology are total institutions and social institutions.[42] Total institutions are places that comprehensively coordinate the actions of people within them, and examples of total institutions include prisons, convents, and hospitals.[42] Social institutions, on the other hand, are constructs that regulate individuals' day-to-day lives, such as kinship, religion, and economics.[42] Anthropology of institutions may analyze labor unions, businesses ranging from small enterprises to corporations, government, medical organizations,[41] education,[3] prisons,[4][5] and financial institutions.[6] Nongovernmental organizations have garnered particular interest in the field of institutional anthropology because of they are capable of fulfilling roles previously ignored by governments,[43] or previously realized by families or local groups, in an attempt to mitigate social problems.[41]		The types and methods of scholarship performed in the anthropology of institutions can take a number of forms. Institutional anthropologists may study the relationship between organizations or between an organization and other parts of society.[41] Institutional anthropology may also focus on the inner workings of an institution, such as the relationships, hierarchies and cultures formed,[41] and the ways that these elements are transmitted and maintained, transformed, or abandoned over time.[44] Additionally, some anthropology of institutions examines the specific design of institutions and their corresponding strength.[9] More specifically, anthropologists may analyze specific events within an institution, perform semiotic investigations, or analyze the mechanisms by which knowledge and culture are organized and dispersed.[41]		In all manifestations of institutional anthropology, participant observation is critical to understanding the intricacies of the way an institution works and the consequences of actions taken by individuals within it.[45] Simultaneously, anthropology of institutions extends beyond examination of the commonplace involvement of individuals in institutions to discover how and why the organizational principles evolved in the manner that they did.[44]		Common considerations taken by anthropologists in studying institutions include the physical location at which a researcher places themselves, as important interactions often take place in private, and the fact that the members of an institution are often being examined in their workplace and may not have much idle time to discuss the details of their everyday endeavors.[46] The ability of individuals to present the workings of an institution in a particular light or frame must additionally be taken into account when using interviews and document analysis to understand an institution,[45] as the involvement of an anthropologist may be met with distrust when information being released to the public isn't directly controlled by the institution and could potentially be damaging.[46]		Within anthropology's "two cultures"—the positivist/objectivist style of comparative anthropology versus a reflexive/interpretative anthropology—Mead has been characterized as a "humanist" heir to Franz Boas's historical particularism—hence, associated with the practices of interpretation and reflexivity [...]		
Ephemera (singular: ephemeron) are any transitory written or printed matter not meant to be retained or preserved. The word derives from the Greek ephemeros, meaning "lasting only one day, short-lived".[1] Some collectible ephemera are advertising trade cards, airsickness bags, bookmarks, catalogues, greeting cards, letters, pamphlets, postcards, posters, prospectuses, defunct stock certificates or tickets, and zines.						Ephemera (ἐφήμερα) is a noun, the plural neuter of ephemeron and ephemeros, Greek and New Latin for ἐπί – epi "on, for" and ἡμέρα – hemera "day" with the ancient sense extending to the mayfly and other short lived insects and flowers and for something which lasts a day or a short period of time.[2]		In library and information science, the term ephemera also describes the class of published single-sheet or single page documents which are meant to be thrown away after one use. This classification excludes simple letters and photographs with no printing on them, which are considered manuscripts or typescripts. Large academic and national libraries and museums may collect, organize, and preserve ephemera as history. A particularly large and important example of such an archive is the John Johnson Collection of Printed Ephemera[3] at the Bodleian Library, Oxford. Over 2,000 images from the John Johnson Collection are available to search online for free at VADS[4] and more than 65,000 items are available online.[5] The extensive Laura Seddon Greeting Card Collection from the Manchester Metropolitan University gathers 32,000 Victorian and Edwardian greeting cards and 450 Valentine's Day cards dating from the early nineteenth century, printed by the major publishers of the day.[6] The Ephemera Kabinett at the Los-Angeles–based Institute of Cultural Inquiry contains 'first' items from cultural turning points of the last two decades, such as a copy of the first Marvel comic in which a lead character comes out of the closet and one of the first AIDS red ribbons.[7]		By extension, video ephemera and audio ephemera refer to transitory audiovisual matter not intended to be retained or preserved. Surprisingly, the great bulk of video and audio expression has, until recently, been ephemeral. Early TV broadcasts were not preserved (indeed, the technology to preserve them postdates the invention of television). Even if radio and television stations preserve archives of their broadcasts, those backcatalogs are inaccessible in practice to the general public, leaving it to a small number of underground tape traders to exchange the rare, lucky moments when something unexpected or historical came across the air.		An article on the Ephemera Society of America website notes		Printed ephemera gave way to audio and video ephemera in the twentieth century. ... These present even more of a preservation problem than printed materials. Although seldom made available for libraries, when videotapes are acquired for archival preservation they are found to be made on low quality tape, poorly processed, and damaged from abuse by users.[8]		The large capacity and reach provided by resources such as the Internet Archive and YouTube have made finding and sharing video ephemera (past and present) dramatically easier.		
JSTOR (/ˈdʒeɪstɔːr/ JAY-stor;[3] short for Journal Storage) is a digital library founded in 1995. Originally containing digitized back issues of academic journals, it now also includes books and primary sources, and current issues of journals.[4] It provides full-text searches of almost 2,000 journals.[5] As of 2013, more than 8,000 institutions in more than 160 countries had access to JSTOR;[5] most access is by subscription, but some older public domain content is freely available to anyone.[6] JSTOR's revenue was $69 million in 2014.[7]						William G. Bowen, president of Princeton University from 1972 to 1988, founded JSTOR.[8] JSTOR originally was conceived as a solution to one of the problems faced by libraries, especially research and university libraries, due to the increasing number of academic journals in existence. Most libraries found it prohibitively expensive in terms of cost and space to maintain a comprehensive collection of journals. By digitizing many journal titles, JSTOR allowed libraries to outsource the storage of journals with the confidence that they would remain available long-term. Online access and full-text search ability improved access dramatically.		Bowen initially considered using CD-ROMs for distribution.[9] However, Ira Fuchs, Princeton University's vice-president for Computing and Information Technology, convinced Bowen that CD-ROM was an increasingly outdated technology and that network distribution could eliminate redundancy and increase accessibility. (For example, all Princeton's administrative and academic buildings were networked by 1989; the student dormitory network was completed in 1994; and campus networks like the one at Princeton were, in turn, linked to larger networks such as BITNET and the Internet.) JSTOR was initiated in 1995 at seven different library sites, and originally encompassed ten economics and history journals. JSTOR access improved based on feedback from its initial sites, and it became a fully searchable index accessible from any ordinary web browser. Special software was put in place[where?] to make pictures and graphs clear and readable.[10]		With the success of this limited project, Bowen and Kevin Guthrie, then-president of JSTOR, wanted to expand the number of participating journals. They met with representatives of the Royal Society of London and an agreement was made[by whom?] to digitize the Philosophical Transactions of the Royal Society dating from its beginning in 1665. The work of adding these volumes to JSTOR was completed by December 2000.[10]		The Andrew W. Mellon Foundation funded JSTOR initially. Until January 2009 JSTOR operated as an independent, self-sustaining nonprofit organization with offices in New York City and in Ann Arbor, Michigan. Then JSTOR merged with the nonprofit Ithaka Harbors, Inc.[11] - a nonprofit organization founded in 2003 and "dedicated to helping the academic community take full advantage of rapidly advancing information and networking technologies."[1]		JSTOR content is provided by more than 900 publishers.[5] The database contains more than 1,900 journal titles,[5] in more than 50 disciplines. Each object is uniquely identified by an integer value, starting at 1.		In addition to the main site, the JSTOR labs group operates an open service that allows access to the contents of the archives for the purposes of corpus analysis at its Data for Research service.[12] This site offers a search facility with graphical indication of the article coverage and loose integration into the main JSTOR site. Users may create focused sets of articles and then request a dataset containing word and n-gram frequencies and basic metadata. They are notified when the dataset is ready and may download it in either XML or CSV formats. The service does not offer full-text, although academics may request that from JSTOR, subject to a non-disclosure agreement.		JSTOR Plant Science[13] is available in addition to the main site. JSTOR Plant Science provides access to content such as plant type specimens, taxonomic structures, scientific literature, and related materials and aimed at those researching, teaching, or studying botany, biology, ecology, environmental, and conservation studies. The materials on JSTOR Plant Science are contributed through the Global Plants Initiative (GPI)[14] and are accessible only to JSTOR and GPI members. Two partner networks are contributing to this: the African Plants Initiative, which focuses on plants from Africa, and the Latin American Plants Initiative, which contributes plants from Latin America.		JSTOR launched its Books at JSTOR program in November 2012, adding 15,000 current and backlist books to its site. The books are linked with reviews and from citations in journal articles.[15]		JSTOR is licensed mainly to academic institutions, public libraries, research institutions, museums, and schools. More than 7,000 institutions in more than 150 countries have access.[4] JSTOR has been running a pilot program of allowing subscribing institutions to provide access to their alumni, in addition to current students and staff. The Alumni Access Program officially launched in January 2013.[16] Individual subscriptions also are available to certain journal titles through the journal publisher.[17] Every year, JSTOR blocks 150 million attempts by non-subscribers to read articles.[18]		Inquiries have been made about the possibility of making JSTOR open access. According to Harvard Law professor Lawrence Lessig, JSTOR had been asked "how much would it cost to make this available to the whole world, how much would we need to pay you? The answer was $250 million".[19]		In late 2010 and early 2011, Internet activist Aaron Swartz used MIT's data network to bulk-download a substantial portion of JSTOR's collection of academic journal articles.[20][21] When the bulk-download was discovered, a video camera was placed in the room to film the mysterious visitor and the relevant computer was left untouched. Once video was captured of the visitor, the download was stopped and Swartz identified. Rather than pursue a civil lawsuit against him, in June 2011 they reached a settlement wherein he surrendered the downloaded data.[20][21]		The following month, federal authorities charged Swartz with several "data theft"-related crimes, including wire fraud, computer fraud, unlawfully obtaining information from a protected computer, and recklessly damaging a protected computer.[22][23] Prosecutors in the case claimed that Swartz acted with the intention of making the papers available on P2P file-sharing sites.[21][24]		Swartz surrendered to authorities, pleaded not guilty to all counts, and was released on $100,000 bail. In September 2012, U.S. attorneys increased the number of charges against Swartz from four to thirteen, with a possible penalty of 35 years in prison and $1 million in fines.[25][26] The case still was pending when Swartz committed suicide in January 2013.[27] Prosecutors dropped the charges after his death.[28]		The availability of most journals on JSTOR is controlled by a "moving wall," which is an agreed-upon delay between the current volume of the journal and the latest volume available on JSTOR. This time period is specified by agreement between JSTOR and the publisher of the journal, which usually is three to five years. Publishers may request that the period of a "moving wall" be changed or request discontinuation of coverage. Formerly, publishers also could request that the "moving wall" be changed to a "fixed wall"—a specified date after which JSTOR would not add new volumes to its database. As of November 2010[update], "fixed wall" agreements were still in effect with three publishers of 29 journals made available online through sites controlled by the publishers.[29]		In 2010, JSTOR started adding current issues of certain journals through its Current Scholarship Program.[30]		Beginning September 6, 2011, JSTOR made public domain content freely available to the public.[31][32] This "Early Journal Content" program constitutes about 6% of JSTOR's total content, and includes over 500,000 documents from more than 200 journals that were published before 1923 in the United States, and before 1870 in other countries.[31][32][33] JSTOR stated that it had been working on making this material free for some time. The Swartz controversy and Greg Maxwell's protest torrent of the same content led JSTOR to "press ahead" with the initiative.[31][32] As of 2017, JSTOR does not have plans to extend it to other public domain content, stating that "We do not believe that just because something is in the public domain, it can always be provided for free".[34]		In January 2012, JSTOR started a pilot program, "Register & Read," offering limited no-cost access (not open access) to archived articles for individuals who register for the service. At the conclusion of the pilot, in January 2013, JSTOR expanded Register & Read from an initial 76 publishers to include about 1,200 journals from over 700 publishers.[35] Registered readers may read up to three articles online every two weeks, but may not print or download PDFs.[36]		This is done by placing up to 3 items on a "shelf". The "Shelf" is under "My JSTOR" below "My Profile". The 3 works can then be read online at any time. An item cannot be removed from the shelf until it has been there for 14 days. Removing an old work from the shelf creates space for a new one, but doing so means the old work can no longer be accessed until it is shelved again.		JSTOR is conducting a pilot program with Wikipedia, whereby established editors are given reading privileges through the Wikipedia Library, as with a university library.[37][38]		In 2012, JSTOR users performed nearly 152 million searches, with more than 113 million article views and 73.5 million article downloads.[5] JSTOR has been used as a resource for linguistics research to investigate trends in language use over time and also to analyze gender differences in scholarly publishing.[39][40]		
The Leningrad première of Shostakovich's Symphony No. 7 took place on 9 August 1942 during the Second World War, while the city (now Saint Petersburg) was under siege by Nazi German forces. Dmitri Shostakovich (pictured) had intended for the piece to be premièred by the Leningrad Philharmonic Orchestra, but they had been evacuated because of the siege, along with the composer, and the world première was instead held in Kuybyshev. The Leningrad première was performed by the surviving musicians of the Leningrad Radio Orchestra, supplemented with military performers. Most of the musicians were starving, and three died during rehearsals. Supported by a Soviet military offensive intended to silence German forces, the performance was a success, prompting an hour-long ovation. The symphony was broadcast to the German lines by loudspeaker as a form of psychological warfare. The Leningrad première was considered by music critics to be one of the most important artistic performances of the war because of its psychological and political effects. Reunion concerts featuring surviving musicians were convened in 1964 and 1992 to commemorate the event. (Full article...)		August 9: International Day of the World's Indigenous Peoples; National Women's Day in South Africa		Hieronymus Bosch (d. 1516) · Elizabeth Schuyler Hamilton (b. 1757) · Gillian Anderson (b. 1968)		Marina City is a mixed-use residential/commercial building complex in Chicago, Illinois. It occupies almost an entire city block on State Street and sits on the north bank of the Chicago River in downtown Chicago, directly across from the Loop. The complex consists of two corncob-shaped, 587-foot (179 m), 65-story towers, as well as a saddle-shaped auditorium building and a mid-rise hotel building. Designed by Bertrand Goldberg, Marina City was the first building in the United States to be constructed with tower cranes.		Photograph: Diego Delso		Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:		This Wikipedia is written in English. Started in 2001 (2001), it currently contains 5,456,467 articles. Many other Wikipedias are available; some of the largest are listed below.		
Wind-up doll jokes (or simply doll jokes) is a series of jokes in which an imagined wind-up doll of a well known person (a show business or sports celebrity or a politician) acts in a way supposedly peculiar to this person.[1][2] An example is given in the biography of Miles Davis by John Szwed. Miles had a habit to walk to the back of the band after finishing his solo, which was called "turning his back on the audience" by press. George Crater of the Down Beat magazine cracked the following joke: "Question: What does a Miles Davis doll do if you wind it up? - Answer: It turns its back on you!"[3]		These jokes are among relatively few examples of American folklore recorded by folklorists close to the time of their origination.[1] They started to be transmitted orally in fall 1960 in the Los Angeles area, and in two years they have found their way to major popular periodicals as a new fad. They were variously called "Living Dolls", "Topical Dolls", but the "wind-up doll" was the most common term.[2]		This fad produced a book, Dolls My Mother Never Gave Me, by Jack Wohl & Stan Rice (1962).[4]		
Computational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective.		Traditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among others.		Computational linguistics has theoretical and applied components. Theoretical computational linguistics focuses on issues in theoretical linguistics and cognitive science, and applied computational linguistics focuses on the practical outcome of modeling human language use.[1]		The Association for Computational Linguistics defines computational linguistics as:						Computational linguistics is often grouped within the field of artificial intelligence, but actually was present before the development of artificial intelligence. Computational linguistics originated with efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English.[3] Since computers can make arithmetic calculations much faster and more accurately than humans, it was thought to be only a short matter of time before they could also begin to process language.[4] Computational and quantitative methods are also used historically in attempted reconstruction of earlier forms of modern languages and subgrouping modern languages into language families. Earlier methods such as lexicostatistics and glottochronology have been proven to be premature and inaccurate. However, recent interdisciplinary studies which borrow concepts from biological studies, especially gene mapping, have proved to produce more sophisticated analytical tools and more trustful results.[5]		When machine translation (also known as mechanical translation) failed to yield accurate translations right away, automated processing of human languages was recognized as far more complex than had originally been assumed. Computational linguistics was born as the name of the new field of study devoted to developing algorithms and software for intelligently processing language data. When artificial intelligence came into existence in the 1960s, the field of computational linguistics became that sub-division of artificial intelligence dealing with human-level comprehension and production of natural languages.[citation needed]		In order to translate one language into another, it was observed that one had to understand the grammar of both languages, including both morphology (the grammar of word forms) and syntax (the grammar of sentence structure). In order to understand syntax, one had to also understand the semantics and the lexicon (or 'vocabulary'), and even something of the pragmatics of language use. Thus, what started as an effort to translate between languages evolved into an entire discipline devoted to understanding how to represent and process natural languages using computers.[6]		Nowadays research within the scope of computational linguistics is done at computational linguistics departments,[7] computational linguistics laboratories,[8] computer science departments,[9] and linguistics departments.[10][11] Some research in the field of computational linguistics aims to create working speech or text processing systems while others aim to create a system allowing human-machine interaction. Programs meant for human-machine communication are called conversational agents.[12]		Just as computational linguistics can be performed by experts in a variety of fields and through a wide assortment of departments, so too can the research fields broach a diverse range of topics. The following sections discuss some of the literature available across the entire field broken into four main area of discourse: developmental linguistics, structural linguistics, linguistic production, and linguistic comprehension.		Language is a cognitive skill which develops throughout the life of an individual. This developmental process has been examined using a number of techniques, and a computational approach is one of them. Human language development does provide some constraints which make it harder to apply a computational method to understanding it. For instance, during language acquisition, human children are largely only exposed to positive evidence.[13] This means that during the linguistic development of an individual, only evidence for what is a correct form is provided, and not evidence for what is not correct. This is insufficient information for a simple hypothesis testing procedure for information as complex as language,[14] and so provides certain boundaries for a computational approach to modeling language development and acquisition in an individual.		Attempts have been made to model the developmental process of language acquisition in children from a computational angle, leading to both statistical grammars and connectionist models.[15] Work in this realm has also been proposed as a method to explain the evolution of language through history. Using models, it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span.[16] This was simultaneously posed as a reason for the long developmental period of human children.[16] Both conclusions were drawn because of the strength of the neural network which the project created.		The ability of infants to develop language has also been modeled using robots[17] in order to test linguistic theories. Enabled to learn as children might, a model was created based on an affordance model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure, vastly simplifying the learning process and shedding light on information which furthers the current understanding of linguistic development. It is important to note that this information could only have been empirically tested using a computational approach.		As our understanding of the linguistic development of an individual within a lifetime is continually improved using neural networks and learning robotic systems, it is also important to keep in mind that languages themselves change and develop through time. Computational approaches to understanding this phenomenon have unearthed very interesting information. Using the Price Equation and Pólya urn dynamics, researchers have created a system which not only predicts future linguistic evolution, but also gives insight into the evolutionary history of modern-day languages.[18] This modeling effort achieved, through computational linguistics, what would otherwise have been impossible.		It is clear that the understanding of linguistic development in humans as well as throughout evolutionary time has been fantastically improved because of advances in computational linguistics. The ability to model and modify systems at will affords science an ethical method of testing hypotheses that would otherwise be intractable.		In order to create better computational models of language, an understanding of language’s structure is crucial. To this end, the English language has been meticulously studied using computational approaches to better understand how the language works on a structural level. One of the most important pieces of being able to study linguistic structure is the availability of large linguistic corpora, or samples. This grants computational linguists the raw data necessary to run their models and gain a better understanding of the underlying structures present in the vast amount of data which is contained in any single language. One of the most cited English linguistic corpora is the Penn Treebank.[19] Derived from widely-different sources, such as IBM computer manuals and transcribed telephone conversations, this corpus contains over 4.5 million words of American English. This corpus has been primarily annotated using part-of-speech tagging and syntactic bracketing and has yielded substantial empirical observations related to language structure.[20]		Theoretical approaches to the structure of languages have also been developed. These works allow computational linguistics to have a framework within which to work out hypotheses that will further the understanding of the language in a myriad of ways. One of the original theoretical theses on internalization of grammar and structure of language proposed two types of models.[14] In these models, rules or patterns learned increase in strength with the frequency of their encounter.[14] The work also created a question for computational linguists to answer: how does an infant learn a specific and non-normal grammar (Chomsky Normal Form) without learning an overgeneralized version and getting stuck?[14] Theoretical efforts like these set the direction for research to go early in the lifetime of a field of study, and are crucial to the growth of the field.		Structural information about languages allows for the discovery and implementation of similarity recognition between pairs of text utterances.[21] For instance, it has recently been proven that based on the structural information present in patterns of human discourse, conceptual recurrence plots can be used to model and visualize trends in data and create reliable measures of similarity between natural textual utterances.[21] This technique is a strong tool for further probing the structure of human discourse. Without the computational approach to this question, the vastly complex information present in discourse data would have remained inaccessible to scientists.		Information regarding the structural data of a language is available for English as well as other languages, such as Japanese.[22] Using computational methods, Japanese sentence corpora were analyzed and a pattern of log-normality was found in relation to sentence length.[22] Though the exact cause of this lognormality remains unknown, it is precisely this sort of intriguing information which computational linguistics is designed to uncover. This information could lead to further important discoveries regarding the underlying structure of Japanese, and could have any number of effects on the understanding of Japanese as a language. Computational linguistics allows for very exciting additions to the scientific knowledge base to happen quickly and with very little room for doubt.		Without a computational approach to the structure of linguistic data, much of the information that is available now would still be hidden under the vastness of data within any single language. Computational linguistics allows scientists to parse huge amounts of data reliably and efficiently, creating the possibility for discoveries unlike any seen in most other approaches.		The production of language is equally as complex in the information it provides and the necessary skills which a fluent producer must have. That is to say, comprehension is only half the problem of communication. The other half is how a system produces language, and computational linguistics has made some very interesting discoveries in this area.		In a now famous paper published in 1950 Alan Turing proposed the possibility that machines might one day have the ability to "think". As a thought experiment for what might define the concept of thought in machines, he proposed an "imitation test" in which a human subject has two text-only conversations, one with a fellow human and another with a machine attempting to respond like a human. Turing proposes that if the subject cannot tell the difference between the human and the machine, it may be concluded that the machine is capable of thought.[23] Today this test is known as the Turing test and it remains an influential idea in the area of artificial intelligence.		One of the earliest and best known examples of a computer program designed to converse naturally with humans is the ELIZA program developed by Joseph Weizenbaum at MIT in 1966. The program emulated a Rogerian psychotherapist when responding to written statements and questions posed by a user. It appeared capable of understanding what was said to it and responding intelligently, but in truth it simply followed a pattern matching routine that relied on only understanding a few keywords in each sentence. Its responses were generated by recombining the unknown parts of the sentence around properly translated versions of the known words. For example, in the phrase "It seems that you hate me" ELIZA understands "you" and "me" which matches the general pattern "you [some words] me", allowing ELIZA to update the words "you" and "me" to "I" and "you" and replying "What makes you think I hate you?". In this example ELIZA has no understanding of the word "hate", but it is not required for a logical response in the context of this type of psychotherapy.[24]		Some projects are still trying to solve the problem which first started computational linguistics off as its own field in the first place. However, the methods have become more refined and clever, and consequently the results generated by computational linguists have become more enlightening. In an effort to improve computer translation, several models have been compared, including hidden Markov models, smoothing techniques, and the specific refinements of those to apply them to verb translation.[25] The model which was found to produce the most natural translations of German and French words was a refined alignment model with a first-order dependence and a fertility model[16]. They also provide efficient training algorithms for the models presented, which can give other scientists the ability to improve further on their results. This type of work is specific to computational linguistics, and has applications which could vastly improve understanding of how language is produced and comprehended by computers.		Work has also been done in making computers produce language in a more naturalistic manner. Using linguistic input from humans, algorithms have been constructed which are able to modify a system's style of production based on a factor such as linguistic input from a human, or more abstract factors like politeness or any of the five main dimensions of personality.[26] This work takes a computational approach via parameter estimation models to categorize the vast array of linguistic styles we see across individuals and simplify it for a computer to work in the same way, making human-computer interaction much more natural.		Many of the earliest and simplest models of human-computer interaction, such as ELIZA for example, involve a text-based input from the user to generate a response from the computer. By this method, words typed by a user trigger the computer to recognize specific patterns and reply accordingly, through a process known as keyword spotting.		Recent technologies have placed more of an emphasis on speech-based interactive systems. These systems, such as Siri of the iOS operating system, operate on a similar pattern-recognizing technique as that of text-based systems, but with the former, the user input is conducted through speech recognition. This branch of linguistics involves the processing of the user's speech as sound waves and the interpreting of the acoustics and language patterns in order for the computer to recognize the input.[27]		Much of the focus of modern computational linguistics is on comprehension. With the proliferation of the internet and the abundance of easily accessible written human language, the ability to create a program capable of understanding human language would have many broad and exciting possibilities, including improved search engines, automated customer service, and online education.		Early work in comprehension included applying Bayesian statistics to the task of optical character recognition, as illustrated by Bledsoe and Browing in 1959 in which a large dictionary of possible letters were generated by "learning" from example letters and then the probability that any one of those learned examples matched the new input was combined to make a final decision.[28] Other attempts at applying Bayesian statistics to language analysis included the work of Mosteller and Wallace (1963) in which an analysis of the words used in The Federalist Papers was used to attempt to determine their authorship (concluding that Madison most likely authored the majority of the papers).[29]		In 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule governed environment. The primary language parsing program in this project was called SHRDLU, which was capable of carrying out a somewhat natural conversation with the user giving it commands, but only within the scope of the toy environment designed for the task. This environment consisted of different shaped and colored blocks, and SHRDLU was capable of interpreting commands such as "Find a block which is taller than the one you are holding and put it into the box." and asking questions such as "I don't understand which pyramid you mean." in response to the user's input.[30] While impressive, this kind of natural language processing has proven much more difficult outside the limited scope of the toy environment. Similarly a project developed by NASA called LUNAR was designed to provide answers to naturally written questions about the geological analysis of lunar rocks returned by the Apollo missions.[31] These kinds of problems are referred to as question answering.		Initial attempts at understanding spoken language were based on work done in the 1960s and 1970s in signal modeling where an unknown signal is analyzed to look for patterns and to make predictions based on its history. An initial and somewhat successful approach to applying this kind of signal modeling to language was achieved with the use of hidden Markov models as detailed by Rabiner in 1989.[32] This approach attempts to determine probabilities for the arbitrary number of models that could be being used in generating speech as well as modeling the probabilities for various words generated from each of these possible models. Similar approaches were employed in early speech recognition attempts starting in the late 70s at IBM using word/part-of-speech pair probabilities.[33]		More recently these kinds of statistical approaches have been applied to more difficult tasks such as topic identification using Bayesian parameter estimation to infer topic probabilities in text documents.[34]		Modern computational linguistics is often a combination of studies in computer science and programming, math, particularly statistics, language structures, and natural language processing. Combined, these fields most often lead to the development of systems that can recognize speech and perform some task based on that speech. Examples include speech recognition software, such as Apple's Siri feature, spellcheck tools, speech synthesis programs, which are often used to demonstrate pronunciation or help the disabled, and machine translation programs and websites, such as Google Translate and Word Reference.[35]		Computational linguistics can be especially helpful in situations involving social media and the Internet. For example, filters in chatrooms or on website searches require computational linguistics. Chat operators often use filters to identify certain words or phrases and deem them inappropriate so that users cannot submit them.[35] Another example of using filters is on websites. Schools use filters so that websites with certain keywords are blocked from children to view. There are also many programs in which parents use Parental controls to put content filters in place. Computational linguists can also develop programs that group and organize content through Social media mining. An example of this is Twitter, in which programs can group tweets by subject or keywords.[36] Computational linguistics is also used for document retrieval and clustering. When you do an online search, documents and websites are retrieved based on the frequency of unique labels related to what you typed into a search engine. For instance, if you search "red, large, four-wheeled vehicle," with the intention of finding pictures of a red truck, the search engine will still find the information desired by matching words such as "four-wheeled" with "car".[37]		Computational linguistics can be divided into major areas depending upon the medium of the language being processed, whether spoken or textual; and upon the task being performed, whether analyzing language (recognition) or synthesizing language (generation).		Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers. Parsing and generation are sub-divisions of computational linguistics dealing respectively with taking language apart and putting it together. Machine translation remains the sub-division of computational linguistics dealing with having computers translate between languages. The possibility of automatic language translation, however, has yet to be realized and remains a notoriously hard branch of computational linguistics.[38]		Some of the areas of research that are studied by computational linguistics include:		The subject of computational linguistics has had a recurring impact on popular culture:		
A comedian or comic is a person who seeks to entertain an audience by making them laugh. This might be through jokes or amusing situations, or acting foolish (as in slapstick) or employing prop comedy. A comedian who addresses an audience directly is called a stand-up comedian.		A popular saying, variously quoted but generally attributed to Ed Wynn,[1] is, "A comic says funny things; a comedian says things funny", which draws a distinction between how much of the comedy can be attributed to verbal content and how much to acting and persona.		Since the 1980s, a new wave of comedy, called alternative comedy, has grown in popularity with its more offbeat and experimental style. This normally involves more experiential, or observational reporting, e.g. Alexei Sayle, Daniel Tosh, Louis C.K. and Malcolm Hardee. As far as content is concerned, comedians such as Tommy Tiernan, Des Bishop, and Joan Rivers draw on their background to poke fun at themselves, while others such as Jon Stewart, and Ben Elton have very strong political and cultural undertones.		Many comics achieve a cult following while touring famous comedy hubs such as the Just for Laughs festival in Montreal, the Edinburgh Fringe, and Melbourne Comedy Festival in Australia. Often a comic's career advances significantly when they win a notable comedy award, such as the Edinburgh Comedy Award (formerly the Perrier comedy award). Comics sometimes foray into other areas of entertainment, such as film and television, where they become more widely known; e.g., Eddie Izzard or Ricky Gervais. However, a comic's stand-up success does not guarantee a film's critical or box office success.						Comedians can be dated back to 425 BC, when Aristophanes, a comic author and playwright, wrote ancient comedic plays. He wrote 40 comedies, 11 of which survive and are still being performed. Aristophanes' comedy style took the form of satyr plays.[2]		The English poet and playwright William Shakespeare wrote many comedies. A Shakespearean comedy is one that has a happy ending, usually involving marriages between the unmarried characters, and a tone and style that is more light-hearted than Shakespeare's other plays.		Charles Chaplin was the most popular screen comedian of the first half of the 20th century. He wrote comedic silent films such as Modern Times and The Kid. His films still have a major impact on comedy in films today.[3]		One of the most popular forms of modern-day comedy is stand-up comedy. Stand-up comedy is a comic monologue performed by one or more people standing on a stage.[4] Bob Hope was the most popular stand-up comedian of the 20th century, and also starred in numerous comedy films over a five-decade span. Other noted stand-up comedians include George Carlin, Jerry Seinfeld, Lenny Bruce, Mort Sahl, Louis CK and Chris Rock.		Another popular form of modern-day comedy is talk shows where comedians make fun of current news or popular topics. Such comedians include Jay Leno, Conan O'Brien, Daniel Tosh, Chris Hardwick, Jimmy Fallon, David Letterman, and Chelsea Handler.		A third form of modern-day comedy is television programs in which many comedians band together to make skits, such as Saturday Night Live. These shows often receive high ratings, likely because many comedians band together to create jokes, rather than one comedian creating his own jokes.[5]		One of the most successful comedians[citation needed] is Ellen DeGeneres, who has parlayed her comic career into film, television shows, and hosting major media events. In 1986, Ellen DeGeneres appeared for the first time on The Tonight Show Starring Johnny Carson since she began gaining popularity as a stand-up comic in the 1980s.[6] Johnny Carson, who launched many contemporary comics careers, would sometimes invite them to join him on the couch for one-on-one conversation after their set. It was Carson's stamp of approval, and because of the show's immense popularity, and the lack of other national spotlights for comics, Carson, "until the early '90s, was the biggest influence on whether a stand-up comedian's career took off."[7] He likened DeGeneres to Bob Newhart, and invited her for an onscreen chat after her performance, she became the first comedian to have been offered that opportunity.[6]		In a January 2014 study, conducted in the British Journal of Psychiatry, scientists found that comedians tend to have high levels of psychotic personality traits. In the study, researchers analyzed 404 male and 119 female comedians from Australia, Britain, and the United States. The participants were asked to complete an online questionnaire designed to measure psychotic traits in healthy people. They found that comedians scored "significantly higher on four types of psychotic characteristics compared to a control group of people who had non-creative jobs." Gordon Claridge, a professor of experimental psychology at the University of Oxford and leader of the study claimed, "the creative elements needed to produce humor are strikingly similar to those characterizing the cognitive style of people with psychosis - both schizophrenia and bipolar disorder."[8] However, labeling comedians' personality traits as "psychotic" does not mean that individual is a psychopath,[9][10] since psychopathy is distinct from psychosis, and neither does it mean their behavior is necessarily pathological.		Forbes publishes an annual list of the most financially successful comedians in the world, similarly to their Celebrity 100 list. Their data sources include Nielsen Media Research, Pollstar, Box Office Mojo and IMDb.[11] The list was topped by Jerry Seinfeld from 2006 until 2015, who lost the title to Kevin Hart in 2016.[12] In that year, the eight highest paid comedians were from the United States, including Amy Schumer, who became the first woman to be listed in the top ten.[13] The top ten of 2016 are as follows:[a]		
Rhythm (from Greek ῥυθμός, rhythmos, "any regular recurring motion, symmetry" (Liddell and Scott 1996)) generally means a "movement marked by the regulated succession of strong and weak elements, or of opposite or different conditions" (Anon. 1971, 2537). This general meaning of regular recurrence or pattern in time can apply to a wide variety of cyclical natural phenomena having a periodicity or frequency of anything from microseconds to several seconds (as with the riff in a rock music song); to several minutes or hours, or, at the most extreme, even over many years.		In the performance arts, rhythm is the timing of events on a human scale; of musical sounds and silences that occur over time, of the steps of a dance, or the meter of spoken language and poetry. In some performing arts, such as hip hop music, the rhythmic delivery of the lyrics is one of the most important elements of the style. Rhythm may also refer to visual presentation, as "timed movement through space" (Jirousek 1995,[page needed]) and a common language of pattern unites rhythm with geometry. In recent years, rhythm and meter have become an important area of research among music scholars. Recent work in these areas includes books by Maury Yeston (1976), Fred Lerdahl and Ray Jackendoff (Lerdahl and Jackendoff 1983), Jonathan Kramer, Christopher Hasty (1997), Godfried Toussaint (2005), William Rothstein (1989), and Joel Lester (Lester 1986).		In Thinking and Destiny, Harold W. Percival defined rhythm as the character and meaning of thought expressed through the measure or movement in sound or form, or by written signs or words (Percival 1946, 1006).						In his television series How Music Works, Howard Goodall presents theories that human rhythm recalls the regularity with which we walk and the heartbeat (Goodall 2006, 0:03:10). Other research suggests that it does not relate to the heartbeat directly, but rather the speed of emotional affect, which also influences heartbeat. Yet other researchers suggest that since certain features of human music are widespread, it is "reasonable to suspect that beat-based rhythmic processing has ancient evolutionary roots" (Patel 2014, 1). Justin London writes that musical metre "involves our initial perception as well as subsequent anticipation of a series of beats that we abstract from the rhythm surface of the music as it unfolds in time" (London 2004, 4). The "perception" and "abstraction" of rhythmic measure is the foundation of human instinctive musical participation, as when we divide a series of identical clock-ticks into "tick-tock-tick-tock" (Scholes 1977b; Scholes 1977c).		Joseph Jordania recently suggested that the sense of rhythm was developed in the early stages of hominid evolution by the forces of natural selection (Jordania, 2011 & pg.99-101). Plenty of animals walk rhythmically and hear the sounds of the heartbeat in the womb, but only humans have the ability to be engaged (entrained) in rhythmically coordinated vocalizations and other activities. According to Jordania, development of the sense of rhythm was central for the achievement of the specific neurological state of the battle trance, crucial for the development of the effective defense system of early hominids. Rhythmic war cry, rhythmic drumming by shamans, rhythmic drilling of the soldiers and contemporary professional combat forces listening to the heavy rhythmic rock music (Pieslak 2009,[page needed]) all use the ability of rhythm to unite human individuals into a shared collective identity where group members put the interests of the group above their individual interests and safety.		Some types of parrots can know rhythm (Anon. 2009). Neurologist Oliver Sacks states that chimpanzees and other animals show no similar appreciation of rhythm yet posits that human affinity for rhythm is fundamental, so that a person's sense of rhythm cannot be lost (e.g. by stroke). "There is not a single report of an animal being trained to tap, peck, or move in synchrony with an auditory beat" (Patel 2006, cited in Sacks 2007, 239–40, who adds, "No doubt many pet lovers will dispute this notion, and indeed many animals, from the Lippizaner horses of the Spanish Riding School of Vienna to performing circus animals appear to 'dance' to music. It is not clear whether they are doing so or are responding to subtle visual or tactile cues from the humans around them.") Human rhythmic arts are possibly to some extent rooted in courtship ritual (Mithen 2005,[page needed]).		The establishment of a basic beat requires the perception of a regular sequence of distinct short-duration pulses and, as a subjective perception of loudness is relative to background noise levels, a pulse must decay to silence before the next occurs if it is to be really distinct. For this reason, the fast-transient sounds of percussion instruments lend themselves to the definition of rhythm. Musical cultures that rely upon such instruments may develop multi-layered polyrhythm and simultaneous rhythms in more than one time signature, called polymeter. Such are the cross-rhythms of Sub-Saharan Africa and the interlocking kotekan rhythms of the gamelan.		For information on rhythm in Indian music see Tala (music). For other Asian approaches to rhythm see Rhythm in Persian music, Rhythm in Arabian music and Usul—Rhythm in Turkish music and Dumbek rhythms.		(See main articles; Pulse (music), Beat (music))		Most music, dance and oral poetry establishes and maintains an underlying "metric level", a basic unit of time that may be audible or implied, the pulse or tactus of the mensural level (Berry 1987, 349; Lerdahl and Jackendoff 1983; Fitch and Rosenfeld 2007, 44), or beat level, sometimes simply called the beat. This consists of a (repeating) series of identical yet distinct periodic short-duration stimuli perceived as points in time (Winold 1975, 213). The "beat" pulse is not necessarily the fastest or the slowest component of the rhythm but the one that is perceived as fundamental: it has a tempo to which listeners entrain as they tap their foot or dance to a piece of music (Handel 1989). It is currently most often designated as a crotchet or quarter note in western notation (see time signature). Faster levels are division levels, and slower levels are multiple levels (Winold 1975, 213). "Rhythms of recurrence" arise from the interaction of two levels of motion, the faster providing the pulse and the slower organizing the beats into repetitive groups (Yeston 1976, 50–52). "Once a metric hierarchy has been established, we, as listeners, will maintain that organization as long as minimal evidence is present" (Lester 1986, 77).		A durational pattern that synchronises with a pulse or pulses on the underlying metric level may be called a rhythmic unit. These may be classified as; metric—even patterns, such as steady eighth notes or pulses—intrametric—confirming patterns, such as dotted eighth-sixteenth note and swing patterns—contrametric—non-confirming, or syncopated patterns and extrametric—irregular patterns, such as tuplets.		A rhythmic gesture is any durational pattern that, in contrast to the rhythmic unit, does not occupy a period of time equivalent to a pulse or pulses on an underlying metric level. It may be described according to its beginning and ending or by the rhythmic units it contains. Beginnings on a strong pulse are thetic, a weak pulse, anacrustic and those beginning after a rest or tied-over note are called initial rest. Endings on a strong pulse are strong, a weak pulse, weak and those that end on a strong or weak upbeat are upbeat (Winold 1975, 239).		Rhythm is marked by the regulated succession of opposite elements, the dynamics of the strong and weak beat, the played beat and the inaudible but implied rest beat, the long and short note. As well as perceiving rhythm we must be able to anticipate it. This depends on repetition of a pattern that is short enough to memorize.		The alternation of the strong and weak beat is fundamental to the ancient language of poetry, dance and music. The common poetic term "foot" refers, as in dance, to the lifting and tapping of the foot in time. In a similar way musicians speak of an upbeat and a downbeat and of the "on" and "off" beat. These contrasts naturally facilitate a dual hierarchy of rhythm and depend on repeating patterns of duration, accent and rest forming a "pulse-group" that corresponds to the poetic foot. Normally such pulse-groups are defined by taking the most accented beat as the first and counting the pulses until the next accent (MacPherson 1930, 5; Scholes 1977b). A rhythm that accents another beat and de-emphasises the downbeat as established or assumed from the melody or from a preceding rhythm is called syncopated rhythm.		Normally, even the most complex of meters may be broken down into a chain of duple and triple pulses (MacPherson 1930, 5; Scholes 1977b) either by addition or division. According to Pierre Boulez, beat structures beyond four, in western music, are "simply not natural" (Slatkin n.d., at 5:05).		(See main articles; Duration (music), Tempo)		The tempo of the piece is the speed or frequency of the tactus, a measure of how quickly the beat flows. This is often measured in 'beats per minute' (bpm): 60 bpm means a speed of one beat per second, a frequency of 1 Hz. A rhythmic unit is a durational pattern that has a period equivalent to a pulse or several pulses (Winold 1975, 237). The duration of any such unit is inversely related to its tempo.		Musical sound may be analyzed on five different time scales, which Moravscik has arranged in order of increasing duration (Moravcsik 2002, 114).		Curtis Roads (Roads 2001) takes a wider view by distinguishing nine-time scales, this time in order of decreasing duration. The first two, the infinite and the supra musical, encompass natural periodicities of months, years, decades, centuries, and greater, while the last three, the sample and subsample, which take account of digital and electronic rates "too brief to be properly recorded or perceived", measured in millionths of seconds (microseconds), and finally the infinitesimal or infinitely brief, are again in the extra-musical domain. Roads' Macro level, encompassing "overall musical architecture or form" roughly corresponds to Moravcsik's "very long" division while his Meso level, the level of "divisions of form" including movements, sections, phrases taking seconds or minutes, is likewise similar to Moravcsik's "long" category. Roads' Sound object (Schaeffer 1959; Schaeffer 1977): "a basic unit of musical structure" and a generalization of note (Xenakis' mini structural time scale); fraction of a second to several seconds, and his Microsound (see granular synthesis) down to the threshold of audible perception; thousands to millionths of seconds, are similarly comparable to Moravcsik's "short" and "supershort" levels of duration.		(See main articles; Metre (music), Bar (music), Metre (poetry))		The study of rhythm, stress, and pitch in speech is called prosody: it is a topic in linguistics and poetics, where it means the number of lines in a verse, the number of syllables in each line and the arrangement of those syllables as long or short, accented or unaccented. Music inherited the term "meter or metre" from the terminology of poetry (Scholes 1977b; Scholes 1977c; Latham 2002).		The metric structure of music includes meter, tempo and all other rhythmic aspects that produce temporal regularity against which the foreground details or durational patterns of the music are projected (Winold 1975, 209-10). The terminology of western music is notoriously imprecise in this area (Scholes 1977b). MacPherson 1930, 3 preferred to speak of "time" and "rhythmic shape", Imogen Holst (Holst 1963, 17) of "measured rhythm".		Dance music has instantly recognizable patterns of beats built upon a characteristic tempo and measure. The Imperial Society of Teachers of Dancing defines the tango, for example, as to be danced in 2 4 time at approximately 66 beats per minute. The basic slow step forwards or backwards, lasting for one beat, is called a "slow", so that a full "right–left" step is equal to one 2 4 measure (Imperial Society of Teachers of Dancing 1977,[page needed]) (See Rhythm and dance).		The general classifications of metrical rhythm, measured rhythm, and free rhythm may be distinguished (Cooper 1973, 30). Metrical or divisive rhythm, by far the most common in Western music calculates each time value as a multiple or fraction of the beat. Normal accents re-occur regularly providing systematical grouping (measures). Measured rhythm (additive rhythm) also calculates each time value as a multiple or fraction of a specified time unit but the accents do not recur regularly within the cycle. Free rhythm is where there is neither (Cooper 1973, 30), such as in Christian chant, which has a basic pulse but a freer rhythm, like the rhythm of prose compared to that of verse (Scholes 1977c). See Free time (music).		Finally some music, such as some graphically scored works since the 1950s and non-European music such as Honkyoku repertoire for shakuhachi, may be considered ametric (Karpinski 2000, 19). Senza misura is an Italian musical term for "without meter", meaning to play without a beat, using time to measure how long it will take to play the bar (Forney and Machlis 2007[page needed]).		A composite rhythm is the durations and patterns (rhythm) produced by amalgamating all sounding parts of a musical texture. In music of the common practice period, the composite rhythm usually confirms the meter, often in metric or even-note patterns identical to the pulse on a specific metric level. White defines composite rhythm as, "the resultant overall rhythmic articulation among all the voices of a contrapuntal texture" (White 1976, 136.).		Worldwide there are many different approaches to passing on rhythmic phrases and patterns, as they exist in traditional music, from generation to generation.		In the western academic world several schools and training courses have been developed since the 1960s according to national tradition and educational policy. Amongst the few methods and learning programmes for music colleges, universities and conservatoires that are employed on an international scale it is worthy mentioning the following books:		In the Griot tradition of Africa everything related to music has been passed on orally. Babatunde Olatunji (1927–2003) developed a simple series of spoken sounds for teaching the rhythms of the hand-drum, using six vocal sounds, "Goon, Doon, Go, Do, Pa, Ta", for three basic sounds on the drum, each played with either the left or the right hand.[citation needed] The debate about the appropriateness of staff notation for African music is a subject of particular interest to outsiders while African scholars from Kyagambiddwa to Kongo have, for the most part, accepted the conventions and limitations of staff notation, and produced transcriptions to inform and enable discussion and debate (Agawu 2003, 52)		John Miller Chernoff 1979 has argued that West African music is based on the tension between rhythms, polyrhythms created by the simultaneous sounding of two or more different rhythms, generally one dominant rhythm interacting with one or more independent competing rhythms. These often oppose or complement each other and the dominant rhythm. Moral values underpin a musical system based on repetition of relatively simple patterns that meet at distant cross-rhythmic intervals and on call-and-response form. Collective utterances such as proverbs or lineages appear either in phrases translated into "drum talk" or in the words of songs. People expect musicians to stimulate participation by reacting to people dancing. Appreciation of musicians is related to the effectiveness of their upholding community values (Chernoff 1979)[page needed].		Indian music has also been passed on orally. Tabla players would learn to speak complex rhythm patterns and phrases before attempting to play them. Sheila Chandra, an English pop singer of Indian descent, made performances based on her singing these patterns. In Indian Classical music, the Tala of a composition is the rhythmic pattern over which the whole piece is structured.		In the 20th century, composers like Igor Stravinsky, Béla Bartók, Philip Glass, and Steve Reich wrote more rhythmically complex music using odd meters, and techniques such as phasing and additive rhythm. At the same time, modernists such as Olivier Messiaen and his pupils used increased complexity to disrupt the sense of a regular beat, leading eventually to the widespread use of irrational rhythms in New Complexity. This use may be explained by a comment of John Cage's where he notes that regular rhythms cause sounds to be heard as a group rather than individually; the irregular rhythms highlight the rapidly changing pitch relationships that would otherwise be subsumed into irrelevant rhythmic groupings (Sandow 2004, 257). LaMonte Young also wrote music in which the sense of a regular beat is absent because the music consists only of long sustained tones (drones). In the 1930s, Henry Cowell wrote music involving multiple simultaneous periodic rhythms and collaborated with Léon Thérémin to invent the Rhythmicon, the first electronic rhythm machine, in order to perform them. Similarly, Conlon Nancarrow wrote for the player piano.		In linguistics, rhythm or isochrony is one of the three aspects of prosody, along with stress and intonation. Languages can be categorized according to whether they are syllable-timed, mora-timed, or stress-timed. Speakers of syllable-timed languages such as Spanish and Cantonese put roughly equal time on each syllable; in contrast, speakers of stressed-timed languages such as English and Mandarin Chinese put roughly equal time lags between stressed syllables, with the timing of the unstressed syllables in between them being adjusted to accommodate the stress timing.		Narmour 1977 (cited in Winold 1975,[page needed]) describes three categories of prosodic rules that create rhythmic successions that are additive (same duration repeated), cumulative (short-long), or countercumulative (long-short). Cumulation is associated with closure or relaxation, countercumulation with openness or tension, while additive rhythms are open-ended and repetitive. Richard Middleton points out this method cannot account for syncopation and suggests the concept of transformation (Middleton 1990,[page needed]).		In day-to-day, figurative language, there are several non-music related uses of the word "rhythm". For example, a person may describe the rhythm of the workday in a certain company or occupation, or the rhythm of life in a certain country or region. In these contexts, the meaning of rhythm is often confused with the concept of "tempo", with people erroneously referring to a certain rhythm as being "slow" or "fast". Speed (tempo) cannot be a specification for a certain rhythm, as the same rhythm can occur at any tempo. A certain tempo compares to other tempi by the difference in speed, whereas a certain rhythm compares to other rhythms by the difference in structure.		
Neurolinguistics is the study of the neural mechanisms in the human brain that control the comprehension, production, and acquisition of language. As an interdisciplinary field, neurolinguistics draws methods and theories from fields such as neuroscience, linguistics, cognitive science, neurobiology, communication disorders, neuropsychology, and computer science. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modeling.		Neurolinguistics is historically rooted in the development in the 19th century of aphasiology, the study of linguistic deficits (aphasias) occurring as the result of brain damage.[1] Aphasiology attempts to correlate structure to function by analyzing the effect of brain injuries on language processing.[2] One of the first people to draw a connection between a particular brain area and language processing was Paul Broca,[1] a French surgeon who conducted autopsies on numerous individuals who had speaking deficiencies, and found that most of them had brain damage (or lesions) on the left frontal lobe, in an area now known as Broca's area. Phrenologists had made the claim in the early 19th century that different brain regions carried out different functions and that language was mostly controlled by the frontal regions of the brain, but Broca's research was possibly the first to offer empirical evidence for such a relationship,[3][4] and has been described as "epoch-making"[5] and "pivotal"[3] to the fields of neurolinguistics and cognitive science. Later, Carl Wernicke, after whom Wernicke's area is named, proposed that different areas of the brain were specialized for different linguistic tasks, with Broca's area handling the motor production of speech, and Wernicke's area handling auditory speech comprehension.[1][2] The work of Broca and Wernicke established the field of aphasiology and the idea that language can be studied through examining physical characteristics of the brain.[4] Early work in aphasiology also benefited from the early twentieth-century work of Korbinian Brodmann, who "mapped" the surface of the brain, dividing it up into numbered areas based on each area's cytoarchitecture (cell structure) and function;[6] these areas, known as Brodmann areas, are still widely used in neuroscience today.[7]		The coining of the term "neurolinguistics" is attributed to Edith Crowell Trager, Henri Hecaen and Alexandr Luria, in the late 1940s and 1950s; Luria's book "Problems in Neurolinguistics" is likely the first book with Neurolinguistics in the title. Harry Whitaker popularized neurolinguistics in the United States in the 1970s, founding the journal "Brain and Language" in 1974.[8]		Although aphasiology is the historical core of neurolinguistics, in recent years the field has broadened considerably, thanks in part to the emergence of new brain imaging technologies (such as PET and fMRI) and time-sensitive electrophysiological techniques (EEG and MEG), which can highlight patterns of brain activation as people engage in various language tasks;[1][9][10] electrophysiological techniques, in particular, emerged as a viable method for the study of language in 1980 with the discovery of the N400, a brain response shown to be sensitive to semantic issues in language comprehension.[11][12] The N400 was the first language-relevant brain response to be identified, and since its discovery EEG and MEG have become increasingly widely used for conducting language research.[13]		Neurolinguistics is closely related to the field of psycholinguistics, which seeks to elucidate the cognitive mechanisms of language by employing the traditional techniques of experimental psychology; today, psycholinguistic and neurolinguistic theories often inform one another, and there is much collaboration between the two fields.[12][14]		Much work in neurolinguistics involves testing and evaluating theories put forth by psycholinguists and theoretical linguists. In general, theoretical linguists propose models to explain the structure of language and how language information is organized, psycholinguists propose models and algorithms to explain how language information is processed in the mind, and neurolinguists analyze brain activity to infer how biological structures (populations and networks of neurons) carry out those psycholinguistic processing algorithms.[15] For example, experiments in sentence processing have used the ELAN, N400, and P600 brain responses to examine how physiological brain responses reflect the different predictions of sentence processing models put forth by psycholinguists, such as Janet Fodor and Lyn Frazier's "serial" model,[16] and Theo Vosse and Gerard Kempen's "unification model".[14] Neurolinguists can also make new predictions about the structure and organization of language based on insights about the physiology of the brain, by "generalizing from the knowledge of neurological structures to language structure".[17]		Neurolinguistics research is carried out in all the major areas of linguistics; the main linguistic subfields, and how neurolinguistics addresses them, are given in the table below.		Neurolinguistics research investigates several topics, including where language information is processed, how language processing unfolds over time, how brain structures are related to language acquisition and learning, and how neurophysiology can contribute to speech and language pathology.		Much work in neurolinguistics has, like Broca's and Wernicke's early studies, investigated the locations of specific language "modules" within the brain. Research questions include what course language information follows through the brain as it is processed,[18] whether or not particular areas specialize in processing particular sorts of information,[19] how different brain regions interact with one another in language processing,[20] and how the locations of brain activation differs when a subject is producing or perceiving a language other than his or her first language.[21][22][23]		Another area of neurolinguistics literature involves the use of electrophysiological techniques to analyze the rapid processing of language in time.[1] The temporal ordering of specific patterns of in brain activity may reflect discrete computational processes that the brain undergoes during language processing; for example, one neurolinguistic theory of sentence parsing proposes that three brain responses (the ELAN, N400, and P600) are products of three different steps in syntactic and semantic processing.[24]		Another topic is the relationship between brain structures and language acquisition.[25] Research in first language acquisition has already established that infants from all linguistic environments go through similar and predictable stages (such as babbling), and some neurolinguistics research attempts to find correlations between stages of language development and stages of brain development,[26] while other research investigates the physical changes (known as neuroplasticity) that the brain undergoes during second language acquisition, when adults learn a new language.[27] Neuroplasticity is observed when both Second Language acquisition and Language Learning experience are induced, the result of this language exposure concludes that an increase of gray and white matter could be found in children, young adults and the elderly.		Ping Li, Jennifer Legault, Kaitlyn A. Litcofsky, May 2014. Neuroplasticity as a function of second language learning: Anatomical changes in the human brain Cortex: A Journal Devoted to the Study of the Nervous System & Behavior, 410.1016/j.cortex.2014.05.00124996640		Neurolinguistic techniques are also used to study disorders and breakdowns in language, such as aphasia and dyslexia, and how they relate to physical characteristics of the brain.[22][26]		Since one of the focuses of this field is the testing of linguistic and psycholinguistic models, the technology used for experiments is highly relevant to the study of neurolinguistics. Modern brain imaging techniques have contributed greatly to a growing understanding of the anatomical organization of linguistic functions.[1][22] Brain imaging methods used in neurolinguistics may be classified into hemodynamic methods, electrophysiological methods, and methods that stimulate the cortex directly.		Hemodynamic techniques take advantage of the fact that when an area of the brain works at a task, blood is sent to supply that area with oxygen (in what is known as the Blood Oxygen Level-Dependent, or BOLD, response).[28] Such techniques include PET and fMRI. These techniques provide high spatial resolution, allowing researchers to pinpoint the location of activity within the brain;[1] temporal resolution (or information about the timing of brain activity), on the other hand, is poor, since the BOLD response happens much more slowly than language processing.[10][29] In addition to demonstrating which parts of the brain may subserve specific language tasks or computations,[19][24] hemodynamic methods have also been used to demonstrate how the structure of the brain's language architecture and the distribution of language-related activation may change over time, as a function of linguistic exposure.[21][27]		In addition to PET and fMRI, which show which areas of the brain are activated by certain tasks, researchers also use diffusion tensor imaging (DTI), which shows the neural pathways that connect different brain areas,[30] thus providing insight into how different areas interact. Functional near-infrared spectroscopy (fNIRS) is another hemodynamic method used in language tasks.[31]		Electrophysiological techniques take advantage of the fact that when a group of neurons in the brain fire together, they create an electric dipole or current. The technique of EEG measures this electric current using sensors on the scalp, while MEG measures the magnetic fields that are generated by these currents.[32] In addition to these non-invasive methods, electrocorticography has also been used to study language processing. These techniques are able to measure brain activity from one millisecond to the next, providing excellent temporal resolution, which is important in studying processes that take place as quickly as language comprehension and production.[32] On the other hand, the location of brain activity can be difficult to identify in EEG;[29][33] consequently, this technique is used primarily to how language processes are carried out, rather than where. Research using EEG and MEG generally focuses on event-related potentials (ERPs),[29] which are distinct brain responses (generally realized as negative or positive peaks on a graph of neural activity) elicited in response to a particular stimulus. Studies using ERP may focus on each ERP's latency (how long after the stimulus the ERP begins or peaks), amplitude (how high or low the peak is), or topography (where on the scalp the ERP response is picked up by sensors).[34] Some important and common ERP components include the N400 (a negativity occurring at a latency of about 400 milliseconds),[29] the mismatch negativity,[35] the early left anterior negativity (a negativity occurring at an early latency and a front-left topography),[36] the P600,[13][37] and the lateralized readiness potential.[38]		Neurolinguists employ a variety of experimental techniques in order to use brain imaging to draw conclusions about how language is represented and processed in the brain. These techniques include the subtraction paradigm, mismatch design, violation-based studies, various forms of priming, and direct stimulation of the brain.		Many language studies, particularly in fMRI, use the subtraction paradigm,[39] in which brain activation in a task thought to involve some aspect of language processing is compared against activation in a baseline task thought to involve similar non-linguistic processes but not to involve the linguistic process. For example, activations while participants read words may be compared to baseline activations while participants read strings of random letters (in attempt to isolate activation related to lexical processing—the processing of real words), or activations while participants read syntactically complex sentences may be compared to baseline activations while participants read simpler sentences.		The mismatch negativity (MMN) is a rigorously documented ERP component frequently used in neurolinguistic experiments.[35][40] It is an electrophysiological response that occurs in the brain when a subject hears a "deviant" stimulus in a set of perceptually identical "standards" (as in the sequence s s s s s s s d d s s s s s s d s s s s s d).[41][42] Since the MMN is elicited only in response to a rare "oddball" stimulus in a set of other stimuli that are perceived to be the same, it has been used to test how speakers perceive sounds and organize stimuli categorically.[43][44] For example, a landmark study by Colin Phillips and colleagues used the mismatch negativity as evidence that subjects, when presented with a series of speech sounds with acoustic parameters, perceived all the sounds as either /t/ or /d/ in spite of the acoustic variability, suggesting that the human brain has representations of abstract phonemes—in other words, the subjects were "hearing" not the specific acoustic features, but only the abstract phonemes.[41] In addition, the mismatch negativity has been used to study syntactic processing and the recognition of word category.[35][40][45]		Many studies in neurolinguistics take advantage of anomalies or violations of syntactic or semantic rules in experimental stimuli, and analyzing the brain responses elicited when a subject encounters these violations. For example, sentences beginning with phrases such as *the garden was on the worked,[46] which violates an English phrase structure rule, often elicit a brain response called the early left anterior negativity (ELAN).[36] Violation techniques have been in use since at least 1980,[36] when Kutas and Hillyard first reported ERP evidence that semantic violations elicited an N400 effect.[47] Using similar methods, in 1992, Lee Osterhout first reported the P600 response to syntactic anomalies.[48] Violation designs have also been used for hemodynamic studies (fMRI and PET): Embick and colleagues, for example, used grammatical and spelling violations to investigate the location of syntactic processing in the brain using fMRI.[19] Another common use of violation designs is to combine two kinds of violations in the same sentence and thus make predictions about how different language processes interact with one another; this type of crossing-violation study has been used extensively to investigate how syntactic and semantic processes interact while people read or hear sentences.[49][50]		In psycholinguistics and neurolinguistics, priming refers to the phenomenon whereby a subject can recognize a word more quickly if he or she has recently been presented with a word that is similar in meaning[51] or morphological makeup (i.e., composed of similar parts).[52] If a subject is presented with a "prime" word such as doctor and then a "target" word such as nurse, if the subject has a faster-than-usual response time to nurse then the experimenter may assume that word nurse in the brain had already been accessed when the word doctor was accessed.[53] Priming is used to investigate a wide variety of questions about how words are stored and retrieved in the brain[52][54] and how structurally complex sentences are processed.[55]		Transcranial magnetic stimulation (TMS), a new noninvasive[56] technique for studying brain activity, uses powerful magnetic fields that are applied to the brain from outside the head.[57] It is a method of exciting or interrupting brain activity in a specific and controlled location, and thus is able to imitate aphasic symptoms while giving the researcher more control over exactly which parts of the brain will be examined.[57] As such, it is a less invasive alternative to direct cortical stimulation, which can be used for similar types of research but requires that the subject's scalp be removed, and is thus only used on individuals who are already undergoing a major brain operation (such as individuals undergoing surgery for epilepsy).[58] The logic behind TMS and direct cortical stimulation is similar to the logic behind aphasiology: if a particular language function is impaired when a specific region of the brain is knocked out, then that region must be somehow implicated in that language function. Few neurolinguistic studies to date have used TMS;[1] direct cortical stimulation and cortical recording (recording brain activity using electrodes placed directly on the brain) have been used with macaque monkeys to make predictions about the behavior of human brains.[59]		In many neurolinguistics experiments, subjects do not simply sit and listen to or watch stimuli, but also are instructed to perform some sort of task in response to the stimuli.[60] Subjects perform these tasks while recordings (electrophysiological or hemodynamic) are being taken, usually in order to ensure that they are paying attention to the stimuli.[61] At least one study has suggested that the task the subject does has an effect on the brain responses and the results of the experiment.[62]		The lexical decision task involves subjects seeing or hearing an isolated word and answering whether or not it is a real word. It is frequently used in priming studies, since subjects are known to make a lexical decision more quickly if a word has been primed by a related word (as in "doctor" priming "nurse").[51][52][53]		Many studies, especially violation-based studies, have subjects make a decision about the "acceptability" (usually grammatical acceptability or semantic acceptability) of stimuli.[62][63][64][65][66] Such a task is often used to "ensure that subjects [are] reading the sentences attentively and that they [distinguish] acceptable from unacceptable sentences in the way the [experimenter] expect[s] them to do."[64]		Experimental evidence has shown that the instructions given to subjects in an acceptability judgment task can influence the subjects' brain responses to stimuli. One experiment showed that when subjects were instructed to judge the "acceptability" of sentences they did not show an N400 brain response (a response commonly associated with semantic processing), but that they did show that response when instructed to ignore grammatical acceptability and only judge whether or not the sentences "made sense".[62]		Some studies use a "probe verification" task rather than an overt acceptability judgment; in this paradigm, each experimental sentence is followed by a "probe word", and subjects must answer whether or not the probe word had appeared in the sentence.[53][64] This task, like the acceptability judgment task, ensures that subjects are reading or listening attentively, but may avoid some of the additional processing demands of acceptability judgments, and may be used no matter what type of violation is being presented in the study.[53]		Subjects may be instructed not to judge whether or not the sentence is grammatically acceptable or logical, but whether the proposition expressed by the sentence is true or false. This task is commonly used in psycholinguistic studies of child language.[67][68]		Some experiments give subjects a "distractor" task to ensure that subjects are not consciously paying attention to the experimental stimuli; this may be done to test whether a certain computation in the brain is carried out automatically, regardless of whether the subject devotes attentional resources to it. For example, one study had subjects listen to non-linguistic tones (long beeps and buzzes) in one ear and speech in the other ear, and instructed subjects to press a button when they perceived a change in the tone; this supposedly caused subjects not to pay explicit attention to grammatical violations in the speech stimuli. The subjects showed a mismatch response (MMN) anyway, suggesting that the processing of the grammatical errors was happening automatically, regardless of attention[35]—or at least that subjects were unable to consciously separate their attention from the speech stimuli.		Another related form of experiment is the double-task experiment, in which a subject must perform an extra task (such as sequential finger-tapping or articulating nonsense syllables) while responding to linguistic stimuli; this kind of experiment has been used to investigate the use of working memory in language processing.[69]		Some relevant journals include the Journal of Neurolinguistics and Brain and Language. Both are subscription access journals, though some abstracts may be generally available.		
Dead baby jokes are a joke cycle reflecting black comedy. The joke is presented in riddle form, beginning with a what question and concluded with a grotesque punch line answer.[1]						A Modest Proposal may be considered an early example of a dead baby joke. The satire by Jonathan Swift describes cannibalizing babies as a solution to starvation in Ireland.		According to the folklorist scholar Alan Dundes, the dead baby joke cycle likely began in the early 1960s.[1] Dundes theorizes that the origin of the dead baby joke lies in the rise of second-wave feminism in the U.S. during that decade and its rejection of the traditional societal role for women, which included support for legalized abortion and contraceptives. Consequently, "to fight the fear of pregnancy and ease the guilt of abortion, young people told dead-baby jokes. Babies, once dehumanized, could be laughingly destroyed."[2][3] It has also been suggested that the jokes emerged in response to images of graphic violence, often involving infants, from the Vietnam War.[4]		In the twenty-first century, the popularity of the joke cycle has led to the creation of a number of websites dedicated to dead baby jokes.[5]		What's the difference between a truckload of dead babies and a truckload of bowling balls? With bowling balls you can't use pitchforks.[6] (In an alternative version, the punchline ends with: You can't unload bowling balls with a pitchfork)		What's more fun than nailing a baby to a post? Ripping it off again.[5]		What's bright blue, pink, and sizzles? A baby breastfeeding on an electrical outlet.[5]		How do you get 100 dead babies into a box? With a blender![5] How do you get them out? With a bag of chips.		What's better than a thousand dead babies stapled to a tree? 1 dead baby stapled to a thousand trees.		How many babies does it take to paint a house? Depends on how hard you throw them[5]		
A joke is a display of humour in which words are used within a specific and well-defined narrative structure to make people laugh and is not meant to be taken seriously. It takes the form of a story, usually with dialogue, and ends in a punch line. It is in the punch line that the audience becomes aware that the story contains a second, conflicting meaning. This can be done using a pun or other word play such as irony, a logical incompatibility, nonsense, or other means. Linguist Robert Hetzron offers the definition:		A joke is a short humorous piece of oral literature in which the funniness culminates in the final sentence, called the punchline… In fact, the main condition is that the tension should reach its highest level at the very end. No continuation relieving the tension should be added. As for its being "oral," it is true that jokes may appear printed, but when further transferred, there is no obligation to reproduce the text verbatim, as in the case of poetry.[1]		It is generally held that jokes benefit from brevity, containing no more detail than is needed to set the scene for the punchline at the end. In the case of riddle jokes or one-liners the setting is implicitly understood, leaving only the dialogue and punchline to be verbalised. However, subverting these and other common guidelines can also be a source of humor -- the shaggy dog story is in a class of its own as an anti-joke; although presenting as a joke, it contains a long drawn-out narrative of time, place and character, rambles through many pointless inclusions and finally fails to deliver a punchline. Jokes are a form of humour, but not all humour is a joke. Some humorous forms which are not verbal jokes are: involuntary humour, situational humour, practical jokes, slapstick and anecdotes.		Identified as one of the simple forms of oral literature by the Dutch linguist André Jolles (de),[2] jokes are passed along anonymously. They are told in both private and public settings; a single person tells a joke to his friend in the natural flow of conversation, or a set of jokes is told to a group as part of scripted entertainment. Jokes are also passed along in written form or, more recently, through the internet.		Stand-up comics, comedians and slapstick work with comic timing, precision and rhythm in their performance, relying as much on actions as on the verbal punchline to evoke laughter. This distinction has been formulated in the popular saying "A comic says funny things; a comedian says things funny".[note 1]						There are many types of joke books in print today; a search on the internet provides a plethora of titles available for purchase. They can be read alone for solitary entertainment, or used to stock up on new jokes to entertain friends. Some people try to find a deeper meaning in jokes, for example "Plato and a Platypus Walk into a Bar... Understanding Philosophy Through Jokes".[3][note 2] However a deeper meaning is not necessary to appreciate their inherent entertainment value.[4] Magazines frequently use jokes and cartoons as filler for the printed page. Reader's Digest closes out many articles with an (unrelated) joke at the bottom of the article. The New Yorker was first published in 1925 with the stated goal of being a "sophisticated humour magazine" and is still known for its cartoons.		The practice of printers to use jokes and cartoons as page fillers was also widely used in the broadsides and chapbooks of the 19th century and earlier. With the increase in literacy in the general population and the growth of the printing industry, these publications were the most common forms of printed material between the 16th and 19th centuries throughout Europe and North America. Along with reports of events, executions, ballads and verse they also contained jokes. Only one of many broadsides archived in the Harvard library is described as "1706. Grinning made easy; or, Funny Dick's unrivalled collection of curious, comical, odd, droll, humorous, witty, whimsical, laughable, and eccentric jests, jokes, bulls, epigrams, &c. With many other descriptions of wit and humour."[5] These cheap publications, ephemera intended for mass distribution, were read alone, read aloud, posted and discarded.		Earlier during the 15th century,[6] the printing revolution spread across Europe following the development of the movable type printing press. This was coupled with the growth of literacy in all social classes. Printers turned out Jestbooks along with Bibles to meet both lowbrow and highbrow interests of the populace. One early anthology of jokes was the Facetiae by the Italian Poggio Bracciolini, first published in 1470. The popularity of this jest book can be measured on the twenty editions of the book documented alone for the 15th century. Another popular form was a collection of jests, jokes and funny situations attributed to a single character in a more connected, narrative form of the picaresque novel. Examples of this are the characters of Rabelais in France, Till Eulenspiegel in Germany, Lazarillo de Tormes in Spain and Master Skelton in England. There is also a jest book ascribed to William Shakespeare, the contents of which appear to both inform and borrow from his plays. All of these early jestbooks corroborate both the rise in the literacy of the European populations and the general quest for leisure activities during the Renaissance in Europe.[6]		The earliest extant joke book predates the printing press by a millennium; it is from the 4th century A.D. The Philogelos (The Laughter Lover) is (hand-)written in Greek and contains a collection of 265 jokes by Hierocles and Philagrius. The humour in this collection is surprisingly familiar, even though the typical protagonists are less recognisable to contemporary readers: the absent-minded professor, the eunuch, and people with hernias or bad breath. The Philogelos even contains a joke similar to Monty Python's "Dead Parrot Sketch".[7]		Various kinds of jokes have been identified in ancient pre-classical texts.[note 3] The oldest identified joke was found to be an ancient Sumerian proverb from 1900 BC containing toilet humour: "Something which has never occurred since time immemorial; a young woman did not fart in her husband’s lap." Its records were dated to the Old Babylonian period and the joke may go as far back as 2,300 BC. The second oldest joke found, discovered on the Westcar Papyrus and believed to be about Sneferu, was from Ancient Egypt circa 1600 BC: "How do you entertain a bored pharaoh? You sail a boatload of young women dressed only in fishing nets down the Nile and urge the pharaoh to go catch a fish." The tale of the three ox drivers from Adab completes the three known oldest jokes in the world. This is a comic triple dating back to 1200 BC Adab.[8]. These "oldest" jokes have two things in common: firstly, they were all written down, and secondly, their structure is remarkably similar to modern day jokes.[note 4]		Any joke documented from the past has been saved through happenstance rather than design. Jokes do not belong to refined culture, but rather to the entertainment and leisure of all classes. As such, any printed versions were considered ephemera, i.e., temporary documents created for a specific purpose and intended to be thrown away. Many of these early jokes deal with scatological and sexual topics, entertaining to all social classes but not to be valued and saved.		Telling a joke is a cooperative effort;[9][10] it requires that the teller and the audience mutually agree in one form or another to understand the narrative which follows as a joke. In a study of conversation analysis, the sociologist Harvey Sacks describes in detail the sequential organisation in the telling a single joke. "This telling is composed, as for stories, of three serially ordered and adjacently placed types of sequences … the preface [framing], the telling, and the response sequences."[11] Folklorists expand this to include the context of the joking. Who is telling what jokes to whom? And why is he telling them when?[12][13] The context of the joke telling in turn leads into a study of joking relationships, a term coined by anthropologists to refer to social groups within a culture who engage in institutionalised banter and joking.		Framing is done with a (frequently formulaic) expression which keys the audience in to expect a joke. "Have you heard the one…", "Reminds me of a joke I heard…", "So, a lawyer and a doctor…"; these conversational markers are just a few examples of linguistic frames used to start a joke. Regardless of the frame used, it creates a social space and clear boundaries around the narrative which follows.[14] Audience response to this initial frame can be acknowledgement and anticipation of the joke to follow. It can also be a dismissal, as in "this is no joking matter" or "this is no time for jokes".		Within its performance frame, joke-telling is labelled as a culturally marked form of communication. Both the performer and audience understand it to be set apart from the "real" world. "An elephant walks into a bar…"; a native English speaker automatically understands that this is the start of a joke, and the story that follows is not meant to be taken at face value (i.e. it is non-bona-fide communication).[15] The framing itself invokes a play mode; if the audience is unable or unwilling to move into play, then nothing will seem funny.[16]		Following its linguistic framing the joke, in the form of a story, can be told. It is not required to be verbatim text like other forms of oral literature such as riddles and proverbs. The teller can and does modify the text of the joke, depending both on memory and the present audience. The important characteristic is that the narrative is succinct, containing only those details which lead directly to an understanding and decoding of the punchline. This requires that it support the same (or similar) divergent scripts which are to be embodied in the punchline.[17]		The narrative always contains a protagonist who becomes the "butt" or target of the joke. This labelling serves to develop and solidify stereotypes within the culture. It also enables researchers to group and analyse the creation, persistence and interpretation of joke cycles around a certain character. Some people are naturally better performers than others, however anyone can tell a joke because the comic trigger is contained in the narrative text and punchline. A joke poorly told is still funny unless the punchline gets mangled.		The punchline is intended to make the audience laugh. A linguistic interpretation of this punchline / response is elucidated by Victor Raskin in his Script-based Semantic Theory of Humour. Humour is evoked when a trigger contained in the punchline causes the audience to abruptly shift its understanding of the story from the primary (or more obvious) interpretation to a secondary, opposing interpretation. "The punchline is the pivot on which the joke text turns as it signals the shift between the [semantic] scripts necessary to interpret [re-interpret] the joke text."[18] To produce the humour in the verbal joke, the two interpretations (i.e. scripts) need to be both compatible with the joke text AND opposite or incompatible with each other.[19] Thomas R. Shultz, a psychologist, independently expands Raskin's linguistic theory to include "two stages of incongruity: perception and resolution." He explains that "… incongruity alone is insufficient to account for the structure of humour. […] Within this framework, humour appreciation is conceptualized as a biphasic sequence involving first the discovery of incongruity followed by a resolution of the incongruity."[20] Resolution generates laughter.		This is the point at which the field of neurolinguistics offers some insight into the cognitive processing involved in this abrupt laughter at the punchline. Studies by the cognitive science researchers Coulson and Kutas directly address the theory of script switching articulated by Raskin in their work.[21] The article "Getting it: Human event-related brain response to jokes in good and poor comprehenders" measures brain activity in response to reading jokes.[22] Additional studies by others in the field support more generally the theory of two-stage processing of humour, as evidenced in the longer processing time they require.[23] In the related field of neuroscience, it has been shown that the expression of laughter is caused by two partially independent neuronal pathways: an "involuntary" or "emotionally driven" system and a "voluntary" system.[24] This study adds credence to the common experience when exposed to an off-colour joke; a laugh is followed in the next breath by a disclaimer: "Oh, that's bad…" Here the multiple steps in cognition are clearly evident in the stepped response, the perception being processed just a breath faster than the resolution of the moral / ethical content in the joke.		Expected response to a joke is laughter. The joke teller hopes the audience "gets it" and is entertained. This leads to the premise that a joke is actually an "understanding test" between individuals and groups.[25] If the listeners do not get the joke, they are not understanding the two scripts which are contained in the narrative as they were intended. Or they do "get it" and don't laugh; it might be too obscene, too gross or too dumb for the current audience. A woman might respond differently to a joke told by a male colleague around the water cooler than she would to the same joke overheard in a women's lavatory. A joke involving toilet humour may be funnier told on the playground at elementary school than on a college campus. The same joke will elicit different responses in different settings. The punchline in the joke remains the same, however it is more or less appropriate depending on the current context.		The context explores the specific social situation in which joking occurs.[26] The narrator automatically modifies the text of the joke to be acceptable to different audiences, while at the same time supporting the same divergent scripts in the punchline. The vocabulary used in telling the same joke at a university fraternity party and to one's grandmother might well vary. In each situation it is important to identify both the narrator and the audience as well as their relationship with each other. This varies to reflect the complexities of a matrix of different social factors: age, sex, race, ethnicity, kinship, political views, religion, power relationship, etc. When all the potential combinations of such factors between the narrator and the audience are considered, then a single joke can take on infinite shades of meaning for each unique social setting.		The context, however, should not be confused with the function of the joking. "Function is essentially an abstraction made on the basis of a number of contexts".[27] In one long-term observation of men coming off the late shift at a local café, joking with the waitresses was used to ascertain sexual availability for the evening. Different types of jokes, going from general to topical into explicitly sexual humour signalled openness on the part of the waitress for a connection.[28] This study describes how jokes and joking are used to communicate much more than just good humour. That is a single example of the function of joking in a social setting, but there are others. Sometimes jokes are used simply to get to know someone better. What makes them laugh, what do they find funny? Jokes concerning politics, religion or sexual topics can be used effectively to gage the attitude of the audience to any one of these topics. They can also be used as a marker of group identity, signalling either inclusion or exclusion for the group. Among pre-adolescents, "dirty" jokes allow them to share information about their changing bodies.[29] And sometimes joking is just simple entertainment for a group of friends.		The context of joking in turn leads into a study of joking relationships, a term coined by anthropologists to refer to social groups within a culture who take part in institutionalised banter and joking. These relationships can be either one-way or a mutual back and forth between partners. "The joking relationship is defined as a peculiar combination of friendliness and antagonism. The behaviour is such that in any other social context it would express and arouse hostility; but it is not meant seriously and must not be taken seriously. There is a pretence of hostility along with a real friendliness. To put it in another way, the relationship is one of permitted disrespect."[30] Joking relationships were first described by anthropologists within kinship groups in Africa. But they have since been identified in cultures around the world, where jokes and joking are used to mark and re-inforce appropriate boundaries of a relationship.[31]		The advent of electronic communications at the end of the 20th century introduced new traditions into jokes. A verbal joke or cartoon is emailed to a friend or posted on a bulletin board; reactions include a replied email with a :-) or LOL, or a forward on to further recipients. Interaction is limited to the computer screen and for the most part solitary. While preserving the text of a joke, both context and variants are lost in internet joking; for the most part emailed jokes are passed along verbatim.[32] The framing of the joke frequently occurs in the subject line: "RE: laugh for the day" or something similar. The forward of an email joke can increase the number of recipients exponentially.		Internet joking forces a re-evaluation of social spaces and social groups. They are no longer only defined by physical presence and locality, they also exist in the connectivity in cyberspace.[33] "The computer networks appear to make possible communities that, although physically dispersed, display attributes of the direct, unconstrained, unofficial exchanges folklorists typically concern themselves with".[34] This is particularly evident in the spread of topical jokes, "that genre of lore in which whole crops of jokes spring up seemingly overnight around some sensational event … flourish briefly and then disappear, as the mass media move on to fresh maimings and new collective tragedies".[35] This correlates with the new understanding of the internet as an "active folkloric space" with evolving social and cultural forces and clearly identifiable performers and audiences.[36]		A study by the folklorist Bill Ellis documented how an evolving cycle was circulated over the internet.[37] By accessing message boards that specialised in humour immediately following the 9/11 disaster, Ellis was able to observe in real time both the topical jokes being posted electronically and responses to the jokes. "Previous folklore research has been limited to collecting and documenting successful jokes, and only after they had emerged and come to folklorists' attention. Now, an Internet-enhanced collection creates a time machine, as it were, where we can observe what happens in the period before the risible moment, when attempts at humour are unsuccessful".[38] Access to archived message boards also enables us to track the development of a single joke thread in the context of a more complicated virtual conversation.[37]		A joke cycle is a collection of jokes about a single target or situation which displays consistent narrative structure and type of humour. Some well-known cycles are elephant jokes using nonsense humour, dead baby jokes incorporating black humour and light bulb jokes, which describe all kinds of operational stupidity. Joke cycles can centre on ethnic groups, professions (viola jokes), catastrophes, settings (…walks into a bar), absurd characters (wind-up dolls), or logical mechanisms which generate the humour (knock-knock jokes). A joke can be reused in different joke cycles; an example of this is the same Head & Shoulders joke refitted to the tragedies of Vic Morrow, Admiral Mountbatten and the crew of the Challenger space shuttle.[note 5][39] These cycles seem to appear spontaneously, spread rapidly across countries and borders only to dissipate after some time. Folklorists and others have studied individual joke cycles in an attempt to understand their function and significance within the culture.		Joke cycles circulated in the recent past include:		As with the 9/11 disaster discussed above, cycles attach themselves to celebrities or national catastrophes such as the death of Diana, Princess of Wales, the death of Michael Jackson, and the Space Shuttle Challenger disaster. These cycles arise regularly as a response to terrible unexpected events which command the national news. An in-depth analysis of the Challenger joke cycle documents a change in the type of humour circulated following the disaster, from February to March 1986. "It shows that the jokes appeared in distinct 'waves', the first responding to the disaster with clever wordplay and the second playing with grim and troubling images associated with the event…The primary social function of disaster jokes appears to be to provide closure to an event that provoked communal grieving, by signaling that it was time to move on and pay attention to more immediate concerns".[55]		The sociologist Christie Davies has written extensively on ethnic jokes told in countries around the world.[56] In ethnic jokes he finds that the "stupid" ethnic target in the joke is no stranger to the culture, but rather a peripheral social group (geographic, economic, cultural, linguistic) well known to the joke tellers.[57] So Americans tell jokes about Polacks and Italians, Germans tell jokes about Ostfriesens, and the English tell jokes about the Irish. In a review of Davies' theories it is said that "For Davies, [ethnic] jokes are more about how joke tellers imagine themselves than about how they imagine those others who serve as their putative targets…The jokes thus serve to center one in the world – to remind people of their place and to reassure them that they are in it."[58]		A third category of joke cycles identifies absurd characters as the butt: for example the grape, the dead baby or the elephant. Beginning in the 1960s, social and cultural interpretations of these joke cycles, spearheaded by the folklorist Alan Dundes, began to appear in academic journals. Dead baby jokes are posited to reflect societal changes and guilt caused by widespread use of contraception and abortion beginning in the 1960s.[note 6][59] Elephant jokes have been interpreted variously as stand-ins for American blacks during the Civil Rights Era[60] or as an "image of something large and wild abroad in the land captur[ing] the sense of counterculture" of the sixties.[61] These interpretations strive for a cultural understanding of the themes of these jokes which go beyond the simple collection and documentation undertaken previously by folklorists and ethnologists.		As folktales and other types of oral literature became collectibles throughout Europe in the 19th century (Brothers Grimm et al.), folklorists and anthropologists of the time needed a system to organise these items. The Aarne–Thompson classification system was first published in 1910 by Antti Aarne, and later expanded by Stith Thompson to become the most renowned classification system for European folktales and other types of oral literature. Its final section addresses anecdotes and jokes, listing traditional humorous tales ordered by their protagonist; "This section of the Index is essentially a classification of the older European jests, or merry tales – humorous stories characterized by short, fairly simple plots. …"[62] Due to its focus on older tale types and obsolete actors (e.g., numbskull), the Aarne–Thompson Index does not provide much help in identifying and classifying the modern joke.		A more granular classification system used widely by folklorists and cultural anthropologists is the Thompson Motif Index, which separates tales into their individual story elements. This system enables jokes to be classified according to individual motifs included in the narrative: actors, items and incidents. It does not provide a system to classify the text by more than one element at a time while at the same time making it theoretically possible to classify the same text under multiple motifs.[63]		The Thompson Motif Index has spawned further specialised motif indices, each of which focuses on a single aspect of one subset of jokes. A sampling of just a few of these specialised indices have been listed under other motif indices. Here one can select an index for medieval Spanish folk narratives,[64] another index for linguistic verbal jokes,[65] and a third one for sexual humour.[66] To assist the researcher with this increasingly confusing situation, there are also multiple bibliographies of indices[67] as well as a how-to guide on creating your own index.[68]		Several difficulties have been identified with these systems of identifying oral narratives according to either tale types or story elements.[69] A first major problem is their hierarchical organisation; one element of the narrative is selected as the major element, while all other parts are arrayed subordinate to this. A second problem with these systems is that the listed motifs are not qualitatively equal; actors, items and incidents are all considered side-by-side.[70] And because incidents will always have at least one actor and usually have an item, most narratives can be ordered under multiple headings. This leads to confusion about both where to order an item and where to find it. A third significant problem is that the "excessive prudery" common in the middle of the 20th century means that obscene, sexual and scatological elements were regularly ignored in many of the indices.[71]		The folklorist Robert Georges has summed up the concerns with these existing classification systems:		…Yet what the multiplicity and variety of sets and subsets reveal is that folklore [jokes] not only takes many forms, but that it is also multifaceted, with purpose, use, structure, content, style, and function all being relevant and important. Any one or combination of these multiple and varied aspects of a folklore example [such as jokes] might emerge as dominant in a specific situation or for a particular inquiry.[72]		It has proven difficult to organise all different elements of a joke into a multi-dimensional classification system which could be of real value in the study and evaluation of this (primarily oral) complex narrative form.		The General Theory of Verbal Humour or GTVH, developed by the linguists Victor Raskin and Salvatore Attardo, attempts to do exactly this. This classification system was developed specifically for jokes and later expanded to include longer types of humorous narratives.[73] Six different aspects of the narrative, labelled Knowledge Resources or KRs, can be evaluated largely independently of each other, and then combined into a concatenated classification label. These six KRs of the joke structure include:		As development of the GTVH progressed, a hierarchy of the KRs was established to partially restrict the options for lower level KRs depending on the KRs defined above them. For example, a lightbulb joke (SI) will always be in the form of a riddle (NS). Outside of these restrictions, the KRs can create a multitude of combinations, enabling a researcher to select jokes for analysis which contain only one or two defined KRs. It also allows for an evaluation of the similarity or dissimilarity of jokes depending on the similarity of their labels. "The GTVH presents itself as a mechanism … of generating [or describing] an infinite number of jokes by combining the various values that each parameter can take. … Descriptively, to analyze a joke in the GTVH consists of listing the values of the 6 KRs (with the caveat that TA and LM may be empty)."[75] This classification system provides a functional multi-dimensional label for any joke, and indeed any verbal humour.		Many academic disciplines lay claim to the study of jokes (and other forms of humour) as within their purview. Fortunately there are enough jokes, good, bad and worse, to go around. Unfortunately the studies of jokes from each of the interested disciplines brings to mind the tale of the blind men and an elephant where the observations, although accurate reflections of their own competent methodological inquiry, frequently fail to grasp the beast in its entirety. This attests to the joke as a traditional narrative form which is indeed complex, concise and complete in and of itself.[76] It requires a "multidisciplinary, interdisciplinary, and cross-disciplinary field of inquiry"[77] to truly appreciate these nuggets of cultural insight.[note 7][78]		Sigmund Freud was one of the first modern scholars to recognise jokes as an important object of investigation.[79] In his 1905 study Jokes and their Relation to the Unconscious[80] Freud describes the social nature of humour and illustrates his text with many examples of contemporary Viennese jokes.[81] His work is particularly noteworthy in this context because Freud distinguishes in his writings between jokes, humour and the comic.[82] These are distinctions which become easily blurred in many subsequent studies where everything funny tends to be gathered under the umbrella term of "humour", making for a much more diffuse discussion.		Since the publication of Freud's study, psychologists have continued to explore humour and jokes in their quest to explain, predict and control an individual's "sense of humour". Why do people laugh? Why do people find something funny? Can jokes predict character, or vice versa, can character predict the jokes an individual laughs at? What is a "sense of humour"? A current review of the popular magazine Psychology Today lists over 200 articles discussing various aspects of humour; in psychospeak[neologism?] the subject area has become both an emotion to measure and a tool to use in diagnostics and treatment. A new psychological assessment tool, the Values in Action Inventory developed by the American psychologists Christopher Peterson and Martin Seligman includes humour (and playfulness) as one of the core character strengths of an individual. As such, it could be a good predictor of life satisfaction.[83] For psychologists, it would be useful to measure both how much of this strength an individual has and how it can be measurably increased.		A 2007 survey of existing tools to measure humour identified more than 60 psychological measurement instruments.[84] These measurement tools use many different approaches to quantify humour along with its related states and traits. There are tools to measure an individual's physical response by their smile; the Facial Action Coding System (FACS) is one of several tools used to identify any one of multiple types of smiles.[85] Or the laugh can be measured to calculate the funniness response of an individual; multiple types of laughter have been identified. It must be stressed here that both smiles and laughter are not always a response to something funny. In trying to develop a measurement tool, most systems use "jokes and cartoons" as their test materials. However, because no two tools use the same jokes, and across languages this would not be feasible, how does one determine that the assessment objects are comparable? Moving on, whom does one ask to rate the sense of humour of an individual? Does one ask the person themselves, an impartial observer, or their family, friends and colleagues? Furthermore, has the current mood of the test subjects been considered; someone with a recent death in the family might not be much prone to laughter. Given the plethora of variants revealed by even a superficial glance at the problem,[86] it becomes evident that these paths of scientific inquiry are mined with problematic pitfalls and questionable solutions.		The psychologist Willibald Ruch (de) has been very active in the research of humour. He has collaborated with the linguists Raskin and Attardo on their General Theory of Verbal Humour (GTVH) classification system. Their goal is to empirically test both the six autonomous classification types (KRs) and the hierarchical ordering of these KRs. Advancement in this direction would be a win-win for both fields of study; linguistics would have empirical verification of this multi-dimensional classification system for jokes, and psychology would have a standardised joke classification with which they could develop verifiably comparable measurement tools.		"The linguistics of humor has made gigantic strides forward in the last decade and a half and replaced the psychology of humor as the most advanced theoretical approach to the study of this important and universal human faculty."[87] This recent statement by one noted linguist and humour researcher describes, from his perspective, contemporary linguistic humour research. Linguists study words, how words are strung together to build sentences, how sentences create meaning which can be communicated from one individual to another, how our interaction with each other using words creates discourse. Jokes have been defined above as oral narrative in which words and sentences are engineered to build toward a punchline. The linguist's question is: what exactly makes the punchline funny? This question focuses on how the words used in the punchline create humour, in contrast to the psychologist's concern (see above) with the audience response to the punchline. The assessment of humour by psychologists "is made from the individual's perspective; e.g. the phenomenon associated with responding to or creating humor and not a description of humor itself."[88] Linguistics, on the other hand, endeavours to provide a precise description of what makes a text funny.[89]		Two major new linguistic theories have been developed and tested within the last decades. The first was advanced by Victor Raskin in "Semantic Mechanisms of Humor", published 1985.[90] While being a variant on the more general concepts of the incongruity theory of humour, it is the first theory to identify its approach as exclusively linguistic. The Script-based Semantic Theory of Humour (SSTH) begins by identifying two linguistic conditions which make a text funny. It then goes on to identify the mechanisms involved in creating the punchline. This theory established the semantic/pragmatic foundation of humour as well as the humour competence of speakers.[note 8][91]		Several years later the SSTH was incorporated into a more expansive theory of jokes put forth by Raskin and his colleague Salvatore Attardo. In the General Theory of Verbal Humour, the SSTH was relabelled as a Logical Mechanism (LM) (referring to the mechanism which connects the different linguistic scripts in the joke) and added to five other independent Knowledge Resources (KR). Together these six KRs could now function as a multi-dimensional descriptive label for any piece of humorous text.		Linguistics has developed further methodological tools which can be applied to jokes: discourse analysis and conversation analysis of joking. Both of these subspecialties within the field focus on "naturally occurring" language use, i.e. the analysis of real (usually recorded) conversations. One of these studies has already been discussed above, where Harvey Sacks describes in detail the sequential organisation in the telling a single joke.[92] Discourse analysis emphasises the entire context of social joking, the social interaction which cradles the words.		Folklore and cultural anthropology have perhaps the strongest claims on jokes as belonging to their bailiwick. Jokes remain one of the few remaining forms of traditional folk literature transmitted orally in western cultures. Identified as one of the "simple forms" of oral literature by André Jolles (de) in 1930,[2] they have been collected and studied since there were folklorists and anthropologists abroad in the lands. As a genre they were important enough at the beginning of the 20th century to be included under their own heading in the Aarne–Thompson index first published in 1910: Anecdotes and jokes.		Beginning in the 1960s, cultural researchers began to expand their role from collectors and archivists of "folk ideas"[78] to a more active role of interpreters of cultural artefacts. One of the foremost scholars active during this transitional time was the folklorist Alan Dundes. He started asking questions of tradition and transmission with the key observation that "No piece of folklore continues to be transmitted unless it means something, even if neither the speaker nor the audience can articulate what that meaning might be."[93] In the context of jokes, this then becomes the basis for further research. Why is the joke told right now? Only in this expanded perspective is an understanding of its meaning to the participants possible.		This questioning resulted in a blossoming of monographs to explore the significance of many joke cycles. What is so funny about absurd nonsense elephant jokes? Why make light of dead babies? In an article on contemporary German jokes about Auschwitz and the Holocaust, Dundes justifies this research: "Whether one finds Auschwitz jokes funny or not is not an issue. This material exists and should be recorded. Jokes are always an important barometer of the attitudes of a group. The jokes exist and they obviously must fill some psychic need for those individuals who tell them and those who listen to them."[94] A stimulating generation of new humour theories flourishes like mushrooms in the undergrowth: Elliott Oring's theoretical discussions on "appropriate ambiguity" and Amy Carrell's hypothesis of an "audience-based theory of verbal humor (1993)" to name just a few.		In his book Humor and Laughter: An Anthropological Approach,[31] the anthropologist Mahadev Apte presents a solid case for his own academic perspective.[citation needed] "Two axioms underlie my discussion, namely, that humor is by and large culture based and that humor can be a major conceptual and methodological tool for gaining insights into cultural systems."[95] Apte goes on to call for legitimising the field of humour research as "humorology"; this would be a field of study incorporating an interdisciplinary character of humour studies.[96]		While the label "humorology" has yet to become a household word, great strides are being made in the international recognition of this interdisciplinary field of research. The International Society for Humor Studies was founded in 1989 with the stated purpose to "promote, stimulate and encourage the interdisciplinary study of humour; to support and cooperate with local, national, and international organizations having similar purposes; to organize and arrange meetings; and to issue and encourage publications concerning the purpose of the society." It also publishes Humor: International Journal of Humor Research and holds yearly conferences to promote and inform its speciality.		Computational humour is a new field of study which uses computers to model humour;[97] it bridges the disciplines of computational linguistics and artificial intelligence. A primary ambition of this field is to develop computer programs which can both generate a joke and recognise a text snippet as a joke. Early programming attempts have dealt almost exclusively with punning because this lends itself to simple straightforward rules. These primitive programs display no intelligence; instead they work off a template with a finite set of pre-defined punning options upon which to build.		More sophisticated computer joke programs have yet to be developed. Based on our understanding of the SSTH / GTVH humour theories, it is easy to see why. The linguistic scripts (a.k.a. frames) referenced in these theories include, for any given word, a "large chunk of semantic information surrounding the word and evoked by it [...] a cognitive structure internalized by the native speaker".[98] These scripts extend much further than the lexical definition of a word; they contain the speaker's complete knowledge of the concept as it exists in his world. As insentient machines, computers lack the encyclopaedic scripts which humans gain through life experience. They also lack the ability to gather the experiences needed to build wide-ranging semantic scripts and understand language in a broader context, a context that any child picks up in daily interaction with his environment.		Further development in this field must wait until computational linguists have succeeded in programming a computer with an ontological semantic natural language processing system. It is only "the most complex linguistic structures [which] can serve any formal and/or computational treatment of humor well".[99] Toy systems (i.e. dummy punning programs) are completely inadequate to the task. Despite the fact that the field of computational humour is small and underdeveloped, it is encouraging to note the many interdisciplinary efforts which are currently underway.[100] As this field grows in both understanding and methodology, it provides an ideal testbed for humour theories; the rules must firstly be cleanly defined in order to write a computer program around a theory.		In 1872, Charles Darwin published one of the first "comprehensive and in many ways remarkably accurate description of laughter in terms of respiration, vocalization, facial action and gesture and posture" (Laughter).[101] In this early study Darwin raises further questions about who laughs and why they laugh; the myriad responses since then illustrates the complexities of this behaviour. To understand laughter in humans and other primates, the science of gelotology (from the Greek gelos, meaning laughter) has been established; it is the study of laughter and its effects on the body from both a psychological and physiological perspective. While jokes can provoke laughter, laughter cannot be used as a one-to-one marker of jokes because there are multiple stimuli to laugher, humour being just one of them. The other six causes of laughter listed are: social context, ignorance, anxiety, derision, acting apology, and tickling.[102] As such, the study of laughter is a secondary albeit entertaining perspective in an understanding of jokes.		
A Polish joke is an ethnic joke intended to mock the Polish people in the English language based on the hostile stereotypes about them. A 'Polish joke' belongs to the category of conditional jokes (i.e., jokes whose understanding requires from the audience a prior knowledge of what a Polish joke is). Similar to all discriminatory jokes, they depend upon the audience's preconceived notions and affective dislikes for entertainment.[1] The relationship between these internalized negative stereotypes about the Polish people and the persistence of ethnic jokes about them is not easy to trace, although they can be understood by many.[2] Often an offensive term for the Poles themselves is used in the joke description as well, for example the Polack joke in English.		Example Joke- Q- Why did the Polak sell his water skis? A- He couldn’t find a lake with a hill in it.						Some of the early 20th century Polish jokes might have been told originally before World War II in disputed border-regions such as Silesia, suggesting that Polish jokes did not originate in Nazi Germany, but a lot earlier, as an outgrowth of regional jokes rooted in historical social class differences.[3] Nonetheless, these jokes were later fuelled by ethnic slurs disseminated by German warlords and National Socialist propaganda that attempted to justify the Nazi crimes against ethnic Poles by presenting them as dirty and relegating them as inferior on the basis of not being German.[4][5]		Polish Americans became the subject of derogatory jokes at the time when Polish immigrants moved to America in considerable numbers fleeing mass persecution at home perpetrated by Frederick the Great[6] and Tsar Nicholas I.[7][8] They took the only jobs available to them, usually requiring physical labor. The same job-related stereotypes persisted even as Polish Americans joined the middle class in the mid 20th century. "These degrading stereotypes were far from harmless. The constant derision, often publicly disseminated through the mass media, caused serious identity crises, feeling of inadequacy, and low self-esteem for many Polish Americans." During the Cold War era, despite the sympathy in the US for Poland being subjected to communism, negative stereotypes about Polish Americans endured, mainly because of the Hollywood/TV media involvement.[9][10]		Some Polish jokes were brought to America by German displaced persons fleeing war-torn Europe in the late 1940s.[4] During the political transformations of the Soviet controlled Eastern bloc in the 1980s, the much earlier German anti-Polish sentiment—dating at least to the policies of Otto von Bismarck and the persecution of Poles under the German Empire—was revived in East Germany against Solidarność (Solidarity). Polish jokes became common, reminding some of the spread of such jokes under the Nazis.[11]		According to Christie Davies, American versions of Polish jokes are an unrelated "purely American phenomenon" and do not express the "historical Old World hatreds".[12] This view is challenged by the Polish American Journal researchers who argue that Nazi and Soviet propaganda shaped the perception of Poles.[13]		Debate continues whether the early Polish jokes brought to states like Wisconsin by German immigrants were directly related to the wave of American jokes of the early 1960s.[3] Since the late 1960s, Polish American organizations made continuous effort to challenge the negative stereotyping of the Polish people once prevalent in U.S. media. In the 1960s and 70s TV shows like All in the Family, The Tonight Show, and Laugh-In often used jokes received by American Poles as demeaning.[10] The Polish jokes heard in the 1970s led the Polish Ministry of Foreign Affairs to approach the U.S. State Department to complain, a move that ultimately had no effect.[10] The 2010 documentary film Polack by James Kenney explores the source of the Polish joke in America, tracing it through history and into contemporary politics.[14][15] The depiction of Polish Americans in the play Polish Joke by David Ives has resulted in a number of complaints by the Polonia in the US.[16]		The book Hollywood's War with Poland shows how Hollywood's World War II (and onwards) negative portrayal of Polish people as being "backward", helped condition the American people to see Polish people as having inferior intelligence in the 20th century. The book supports the Polish American Journal's assertion that Hollywood historically was fertile ground for anti-Polish prejudice, based on its Left-wing/Soviet sympathies.[17]		The Polish American Congress Anti-Bigotry Committee was created in early 1980s to fight anti-Polish sentiment, including "Polish jokes". Notable public cases include protests against the use of Polish jokes by Drew Carey (early 2000s) and Jimmy Kimmel (2013), both joking at the ABC network.[18]		In the 1990s, popular culture in Germany experienced a surge of Polish jokes. In their televisions shows, entertainers like Harald Schmidt or Thomas Koschwitz used to make jokes about Polish economy or increased automobile theft in Germany attributed to Poles:		The Bild tabloid employed stereotypical headlines about Poland. This triggered public outrage among German and Polish intellectuals, but in the latter half of the decade, fears of theft had even led to a decrease in German tourists visiting Poland.[19][20] The greatest percentage of foreign tourists in Poland exceeding 1.3 million annually arrive from Germany.[21] Still in 2003, it was observed that the public image of Poland in Germany itself was largely shaped by stereotypical jokes.[22]		In Nordic countries, the phrase "Polish Parliament" is often used to describe bickering, chaotic legislative bodies. The phrase dates back to the 17th and 18th centuries, when Poland's actual parliament, the Sejm, allowed any member to nullify any legislation singlehandedly, which led to constant infighting and the country's collapse.		
Victor "Vic" Morrow (February 14, 1929 – July 23, 1982) was an American actor and director whose credits include a starring role in the 1960s television series Combat!, prominent roles in a handful of other television and film dramas, and numerous guest roles on television. Morrow and two child actors were killed in 1982 by a stunt helicopter crash during the filming of Twilight Zone: The Movie. Morrow also gained notice for his roles in movies like Blackboard Jungle (1955), God's Little Acre (1958), Dirty Mary, Crazy Larry (1974), and The Bad News Bears (1976).						Morrow was born Victor Morozoff in the New York City borough of the Bronx, to a middle-class Jewish family.[2] He was a son of Harry Morozoff, an electrical engineer, and his wife Jean (Kress) Morozoff.[3] Morrow dropped out of high school when he was 17 and enlisted in the United States Navy.		In 1958,[2] Morrow married actress and screenwriter Barbara Turner. They had two daughters, Carrie Ann Morrow (born 1958) and actress Jennifer Jason Leigh (born 1962). Morrow's marriage to Barbara ended in divorce after 7 years. He married Gale Lester in 1975, but they separated just prior to Morrow's death.		Morrow fell out with his daughter Jennifer Jason after his divorce from her mother. She changed her last name to Leigh to avoid being publicly associated with Morrow. They were still estranged at the time of his death.[4]		Rick Jason, co-star of Combat!, wrote in his memoirs,		Vic Morrow had an absolute dislike of firearms. He used a Thompson submachine gun in our series, but that was work. In any other respect he'd have nothing to do with them. On one of the few days we got off early while there were still several hours of daylight left, I said to him, "I've got a couple of shotguns in the back of my station wagon. You want to shoot some skeet?" Without so much as a pause he responded, "No, thanks. I can't stand to kill clay." He knew he could always break me up and during our five years together he did it quite a bit. His sense of humor happened to tickle my funny bone and he knew he had my number."[5]		Morrow's first movie role was in Blackboard Jungle (1955). In 1958, he starred alongside Elvis Presley and an all-star supporting cast in the movie King Creole, directed by Michael Curtiz.		Morrow's career then expanded after which he went into television. Later, he guest-starred on John Payne's NBC Western series, The Restless Gun. On April 16, 1959, he appeared in the premiere of NBC's 1920s crime drama The Lawless Years in the episode "The Nick Joseph Story". Morrow then appeared from 1960–1961 as Joe Cannon in three episodes of NBC's The Outlaws with Barton MacLane. On October 6, 1961, he appeared in an episode of the ABC drama series Target: The Corruptors! with Stephen McNally and Robert Harland.		He appeared in two episodes of The Untouchables, The Rifleman and Bonanza. He was cast in the early Bonanza episode "The Avenger" as a mysterious figure known only as "Lassiter" – named after his town of origin – who arrives in Virginia City, and helps save Ben and Adam Cartwright from an unjust hanging, while eventually gunning down one sought-after man, revealing himself as a hunter of a lynch mob who killed his father; having so far killed about half the mob, he rides off into the night,[6] in an episode that resembles the later Clint Eastwood film High Plains Drifter. Morrow later appeared in the third season Bonanza episode The Tin Badge.[7]		Morrow was cast in the lead role of Sergeant "Chip" Saunders in ABC's Combat!, a World War II drama, which aired from 1962–1967. Pop culture scholar Gene Santoro has written, "TV's longest-running World War II drama (1962-67) was really a collection of complex 50-minute movies. Salted with battle sequences, they follow a squad's travails from D-Day on--a gritty ground-eye view of men trying to salvage their humanity and survive. Melodrama, comedy, and satire come into play as top-billed Lieutenant Hanley (Rick Jason) and Sergeant Saunders (Vic Morrow) lead their men toward Paris... The relentlessness hollows antihero Saunders out: at times, you can see the tombstones in his eyes."[8]		His friend and fellow actor on Combat!, Rick Jason, described Morrow as "a master director" who directed "one of the greatest anti-war films I've ever seen." He was referring to the two-part episode of Combat! entitled Hills Are for Heroes, which was written by Gene L. Coon.[5]		Morrow also worked as a television director. Together with Leonard Nimoy, he produced a 1966 version of Deathwatch, an English-language film version of Jean Genet's play Haute Surveillance, adapted by Morrow and Barbara Turner, directed by Morrow, and starring Nimoy.		After Combat! ended, he worked in several films. Morrow appeared in two episodes of Australian-produced anthology series The Evil Touch (1973), one of which he also directed. He memorably played the wily local sheriff in director John Hough's road classic Dirty Mary Crazy Larry, as well as the homicidal sheriff, alongside Martin Sheen, in the television film The California Kid (1974), and had a key role, as aggressive, competitive baseball coach Roy Turner, in the comedy The Bad News Bears (1976). He also played Injun Joe in the television film Tom Sawyer (1973), which was filmed in Upper Canada Village. A musical version was released in theaters that same year.		Morrow wrote and directed a Spaghetti Western, produced by Dino DeLaurentiis, titled A Man Called Sledge (1970) and starring James Garner, Dennis Weaver and Claude Akins. After Deathwatch, it was Morrow's first and only big screen outing behind the camera. Sledge was filmed in Italy[9] with desert-like settings that were highly evocative of the Southwestern United States. Morrow also appeared in Hawaii Five-O, The Streets of San Francisco, McCloud and Sarge, among many other television guest roles.		In 1971, Vic Morrow starred in a television movie, produced by QM Productions for CBS entitled Travis Logan, D.A.. This movie was a pilot for a proposed weekly legal series in which he was to star. While many critics liked the series, the ratings were low; and the pilot was never sold as a series.		In 1982, Morrow was cast in a feature role in Twilight Zone: The Movie, directed by John Landis. Morrow was playing the role of Bill Connor, a racist who is taken back in time and placed in various situations where he would be a persecuted victim: as a Jewish Holocaust victim, a black man about to be lynched by the Ku Klux Klan, and a Vietnamese man about to be killed by U.S. soldiers.		In the early morning hours of July 23, 1982, Morrow and two children, 7-year-old Myca Dinh Le, and 6-year-old Renee Shin-Yi Chen, were filming on location in California in what had been known as Indian Dunes, near Santa Clarita. They were performing in a scene for the Vietnam sequence, in which their characters attempt to escape from a pursuing U.S. Army helicopter out of a deserted Vietnamese village. The helicopter was hovering at about 24 feet (7.3 m) above them when pyrotechnic explosions damaged it and caused it to crash on top of them, killing all three instantly. Morrow and Le were decapitated by the helicopter rotor, while Chen was crushed by a helicopter strut.[10][11]		Landis and four other defendants, including pilot Dorsey Wingo, were ultimately acquitted of involuntary manslaughter after a nearly nine-month trial. The parents of Le and Chen sued and settled out of court for an undisclosed amount. Morrow's children also sued and settled for an undisclosed amount.[11][12]		Morrow is interred in Hillside Memorial Park Cemetery in Culver City, California.[13]		
Victor Raskin (born April 17, 1944) is a distinguished professor of linguistics at Purdue University. He is the author of Semantic Mechanisms of Humor and Ontological Semantics and founding editor (now editor-at-large) of Humor, the journal for the International Society for Humor Studies.		He is an associate director and founding[citation needed] faculty member of CERIAS at Purdue University along with Gene Spafford and Mikhail Atallah.						Victor Raskin was born in Irbit, U.S.S.R. (now the Russian Federation). He has been married to Marina Bergelson since 1965; his daughter Sarah was born in 1982. He and his wife emigrated from the U.S.S.R. to Israel in 1973, and have been Israeli citizens since 1973. They moved to the U.S.A. in 1978, became permanent residents of the U.S.A. in 1979, and became U.S. citizens in 1984.		
Reader's Digest is an American general-interest family magazine, published ten times a year. Formerly based in Chappaqua, New York, it is now headquartered in Midtown Manhattan. The magazine was founded in 1920, by DeWitt Wallace and Lila Bell Wallace. For many years, Reader's Digest was the best-selling consumer magazine in the United States; it lost the distinction in 2009 to Better Homes and Gardens. According to Mediamark Research (2006), Reader's Digest reaches more readers with household incomes of $100,000+ than Fortune, The Wall Street Journal, Business Week, and Inc. combined.[2]		Global editions of Reader's Digest reach an additional 40 million people in more than 70 countries, via 49 editions in 21 languages. The periodical has a global circulation of 10.5 million, making it the largest paid circulation magazine in the world.		It is also published in Braille, digital, audio, and in a large type called Reader's Digest Large Print. The magazine is compact, with its pages roughly half the size of most American magazines'. Hence, in the summer of 2005, the U.S. edition adopted the slogan: "America in your pocket." In January 2008, it was changed to: "Life well shared."						The magazine was started in 1922[3] by DeWitt Wallace while he was recovering from shrapnel wounds received in World War I. Wallace had the idea to gather a sampling of favorite articles on many subjects from various monthly magazines, sometimes condensing and rewriting them, and to combine them into one magazine.[4]		Since its inception, Reader's Digest has maintained a conservative[5] and anti-Communist perspective on political and social issues.[6] The Wallaces initially hoped the journal could provide $5,000 of net income. Mr. Wallace’s assessment of what the potential mass-market audience wanted to read led to rapid growth. By 1929, the magazine had 290,000 subscribers and had a gross income of $900,000 a year. The first international edition was published in the United Kingdom in 1938 and was sold at 2 shillings. By the 40th anniversary of Reader’s Digest, there were 40 international editions, in 13 languages and Braille, and it was the largest-circulating journal in Canada, Mexico, Spain, Sweden, Peru and other countries, with a total international circulation of 23 million.[4]		The magazine's format for several decades consisted of 30 articles per issue (one per day), along with a vocabulary page, a page of "Amusing Anecdotes" and "Personal Glimpses", two features of funny stories entitled "Humor in Uniform" and "Life in these United States", and a lengthier article at the end, usually condensed from a published book.[7] These were all listed in the Table of Contents on the front cover. Each article was prefaced by a small, simple line drawing. In recent years, however, the format has greatly evolved into flashy, colorful eye-catching graphics throughout, and many short bits of data interspersed with full articles. The Table of Contents is now contained inside. From 2003 to 2007, the back cover featured "Our America," paintings of Rockwell-style whimsical situations by artist C. F. Payne.		The first "Word Power" column of the magazine was published in the January 1945 edition, written by Wilfred J. Funk.[8][9] In December 1952 the magazine published "Cancer by the Carton", a series of articles that linked smoking with lung cancer.[10] This first brought the dangers of smoking to public attention which, up to then, had ignored the health threats.[citation needed]		From 2002 through 2006, Reader's Digest conducted a vocabulary competition in schools throughout the United States called Reader's Digest National Word Power Challenge (NWPC). In 2007, the magazine said it had decided not to have the competition for the 2007–08 school year, "but rather to use the time to evaluate the program in every respect, including scope, mission, and model for implementation."[11]		In 2006, the magazine published three more local-language editions in Slovenia, Croatia and Romania. In October 2007, the Digest expanded in Serbia. The magazine's licensee in Italy stopped publishing in December 2007. The magazine launched in The People's Republic of China in 2008.		For 2010, the U.S. edition of the magazine planned to decrease its circulation to 5.5 million, from 8 million, to publish 10 times a year rather than 12, and to increase digital offerings. It also planned to reduce its number of celebrity profiles and how-to features, and increase the number of inspiring spiritual stories and stories about the military.[12] It has been increased back to 12 times a year from 2013.[13] The regular features include the cartoon series Reynolds Unwrapped by Dan Reynolds.		The magazine's parent company, The Reader's Digest Association, Inc. (RDA), became a publicly traded corporation in 1990. As of 2010[update] RDA has reported a net loss each year since 2005.[14] In March 2007, Ripplewood Holdings LLC led a consortium of private equity investors who bought the company through a leveraged buy-out for US$2.8 billion, financed primarily by the issuance of US$2.2 billion of debt.[4][4][12] Ripplewood invested $275 million of its own money, and had partners including Rothschild Bank of Zürich and GoldenTree Asset Management of New York. The private equity deal tripled the association's interest payments, to $148 million a year.[4]		On August 24, 2009 RDA announced it had filed with the U.S. Bankruptcy court a pre-arranged Chapter 11 bankruptcy, in order to continue operations, and to restructure the $2.2 billion debt undertaken by the leveraged buy-out transaction.[4][15][16] The company emerged from bankruptcy with the lenders exchanging debt for equity, and Ripplewood's entire equity investment was extinguished.[4]		In April 2010, the UK arm was sold to its management. It has a licensing deal with the U.S. company to continue publishing the UK edition.[17]		In 2010, the U.S. company cut the number of issues it published a year from 12 to 10. It also cut its circulation guarantee for advertisers to 5.5 million copies from 8 million. However it returned to monthly editions beginning in January 2013.		RDA Holding filed for bankruptcy for a second time on February 17, 2013.[18][19]		In 2001, 32 states attorneys general reached agreements with the company and other sweepstakes operators to settle allegations that they tricked the elderly into buying products because they were a "guaranteed winner" of a lottery. The settlement required the companies to expand the type size of notices in the packaging that no purchase is necessary to play the sweepstakes, and to:		The agreement appeared to adversely affect Reader's Digest circulation in the U.S.[clarification needed] Its 1970s peak circulation was 17 million U.S. subscribers.[4]		RDA offers many mail-order products included with "sweepstakes" or contests. U.S. Reader's Digest and the company's other U.S. magazines do not use sweepstakes in their direct mail promotions. A notable shift to electronic direct marketing has been undertaken by the company to adapt to shifting media landscape.[23]		Reader's Digest in the UK has been criticised by the Trading Standards Institute for preying on the elderly and vulnerable with misleading bulk mailings that claim the recipient is guaranteed a large cash prize and advising them not to discuss this with anyone else. Following their complaint, the Advertising Standards Authority said they would be launching an investigation.[24] The ASA investigation upheld the complaint in 2008, ruling that the Reader's Digest mailing was irresponsible, misleading (particularly for the elderly) and had breached three clauses of the Committee of Advertising Practice code.[25] Reader's Digest was told not to use this mailing again.		Although Reader's Digest was founded in the U.S., its international editions have made it the best-selling monthly journal in the world. Its worldwide circulation including all editions has reached 17 million copies and 70 million readers.		Reader's Digest is currently published in 49 editions and 21 languages and is available in over 70 countries, including Slovenia, Croatia, and Romania in 2008.		Its international editions account for about 50% of the magazine's trade volume. In each market, local editors commission or purchase articles for their own market and share content with U.S. and other editions. The selected articles are then translated by local translators and the translations edited by the local editors to make them match the "well-educated informal" style of the American edition.		Over the 90 years, the company has published editions in various languages in different countries, or for different regions.		Usually these editions started out as translations of the U.S. version of the magazine, but over time many non-U.S. editions became unique, providing local material more germane to local readers. Local editions that still publish the bulk of the American Reader's Digest are usually titled with a qualifier, such as the Portuguese edition, Seleções do Reader's Digest (Selections from Reader's Digest), or the Swedish edition, Reader's Digest Det Bästa (The Best of Reader's Digest).		The list is sorted by year of first publication.[26] Some countries had editions but no longer do; for example, the Danish version of Reader's Digest (Det Bedste) ceased publication in 2005 and was replaced by the Swedish version (Reader's Digest Det Bästa); as a result, the Swedish edition covers stories about both countries (but written solely in Swedish).		On February 17, 2014, The Guardian had this headline: "Reader's Digest sold for £1. Mike Luckwell buys struggling title from Jon Moulton's private equity company, Better Capital, with plan to target over-50s".[27]		The first Reader's Digest publication in the Arab World was printed in Egypt in September 1943.[29] The license was eventually terminated.		The second effort and the first Reader's Digest franchise agreement was negotiated through the efforts of Frederick Pittera, in 1976, an American entrepreneur, who sold the idea to Lebanon's former Foreign Minister, Lucien Dahdah, then son-in-law of Suleiman Frangieh, President of Lebanon. Dahdah partnered with Ghassan Tueni (former Lebanon Ambassador to the United Nations, and publisher of Al Nahar newspaper, Beirut) in publishing Reader's Digest in the Arabic language. It was printed in Cairo for distribution throughout the Arab world under title Al-Mukhtar. In format, Al-Mukhtar was the same as the U.S. edition with 75% of the editorial content. Philip Hitti, Chairman of Princeton University's Department of Oriental Languages and a team of Arabic advisers counseled on what would be of interest to Arabic readers. The publication of Al-Mukhtar was terminated by Reader's Digest in April 1993.		The Canadian edition first appeared in July 1947 in French and in February 1948 in English, and today the vast majority of it is Canadian content. All major articles in the August 2005 edition and most of the minor articles were selected from locally produced articles that matched the Digest style. There is usually at least one major American article in most issues.		"Life's Like That" is the Canadian name of "Life in These United States." All other titles are taken from the American publication. Recent "That's Outrageous" articles have been using editorials from the Calgary Sun.		Under new management—the new editor is Robert Goyette—the Canadian edition continues to publish.		The Indian edition was first published in 1954. Its circulation then was 40,000 copies. It was published for many years by the Tata Group of companies. Today, the magazine is published in India by Living Media India Ltd,[30] and sold over 600,000 copies monthly in 2008. It prints Indian and international articles.[30] According to the Indian Readership Survey Round II of 2009, the readership for Reader's Digest is 3.94 million, second only to India Today at 5.62 million.[30] The India edition Chief Executive Officer is Ashish Bagga. The India Editor is Sanghamitra Chakraborty.[31]		Reader's Digest Australia today has an any issue readership of 1.5 million (according to Nielsen) and a circulation of over 200,000. The magazine has a guaranteed audience with a 90% subscription rate. The editorial director is Lynn Lewis.		With a readership of 299,000 per month Reader's Digest remains a firm favourite magazine for New Zealanders. This magazine circulates approximately 50,000 copies per month.		Reader's Digest publishes bi-monthly a series of softcover anthologies called Reader's Digest Select Editions (previously known as Reader's Digest Condensed Books). During the 1970s, there was also a Reader's Digest Press, which published full-length, original works of non-fiction.		
A bar joke is a very common and basic type of joke. The basic syntax of this type of joke is "A man walks into a bar and <something happens here>". The initial perception of the joke is that a man is walking into a bar to have a drink, but this only lasts a few seconds as the punchline is quickly uttered. This joke has gained an incredible amount of variants over the years. It is often used by comedians, and people telling jokes to friends.		The bar joke has a large number of variations. The types of variations include puns or word plays (the man walks into a bar and pulls out a tiny piano and a 12-inch pianist, followed by any number of different punchlines; or man with dyslexia walked into a bra), or replace the man with woman, a famous person, people of various occupations, animals (a duck walks into a bar, orders a drink, and tells the bartender, put it on my bill) or inanimate objects (a sandwich walks into a bar, orders a beer, and is told by the bartender, we don't serve food here). Sometimes the unexpected happens: "A man walks into a bar. Ouch!"		Another major variant involves several men walking into the bar together, often with related professions, such as "a priest, a minister and a rabbi." In effect, this is a merger between the "bar joke" and jokes involving priests, ministers and rabbis (or Buddhist monks, etc.) in other settings. This form has become so well known that it is the subject of at least one joke about the popularity of the joke itself: "A priest, a minister, and a rabbi walk into a bar. The bartender looks at them and says, 'What is this, a joke?'"		According to Scott McNeely in the Ultimate Book of Jokes, the first bar joke was published in 1952.[1]		
Laughter is a physical reaction in humans and some other species of primate, consisting typically of rhythmical, often audible contractions of the diaphragm and other parts of the respiratory system. It is a response to certain external or internal stimuli. Laughter can arise from such activities as being tickled,[1] or from humorous stories or thoughts.[2] Most commonly, it is considered a visual expression of a number of positive emotional states, such as joy, mirth, happiness, relief, etc. On some occasions, however, it may be caused by contrary emotional states such as embarrassment, apology, or confusion such as nervous laughter or courtesy laugh. Age, gender, education, language, and culture are all factors[3] as to whether a person will experience laughter in a given situation.		Laughter is a part of human behavior regulated by the brain, helping humans clarify their intentions in social interaction and providing an emotional context to conversations. Laughter is used as a signal for being part of a group—it signals acceptance and positive interactions with others. Laughter is sometimes seen as contagious, and the laughter of one person can itself provoke laughter from others as a positive feedback.[4] This may account in part for the popularity of laugh tracks in situation comedy television shows.		The study of humor and laughter, and its psychological and physiological effects on the human body, is called gelotology.						Laughter might be thought of as an audible expression or appearance of excitement, an inward feeling of joy and happiness. It may ensue from jokes, tickling, and other stimuli completely unrelated to psychological state, such as nitrous oxide. One group of researchers speculated that noises from infants as early as 16 days old may be vocal laughing sounds or laughter,[5] however the weight of the evidence supports its appearance at 15 weeks to four months of age.		Laughter researcher Robert Provine (es) said: "Laughter is a mechanism everyone has; laughter is part of universal human vocabulary. There are thousands of languages, hundreds of thousands of dialects, but everyone speaks laughter in pretty much the same way." Babies have the ability to laugh before they ever speak. Children who are born blind and deaf still retain the ability to laugh.[6]		Provine argues that "Laughter is primitive, an unconscious vocalization." Provine argues that it probably is genetic. In a study of the "Giggle Twins", two happy twins who were separated at birth and only reunited 43 years later, Provine reports that "until they met each other, neither of these exceptionally happy ladies had known anyone who laughed as much as they did." They reported this even though they both had been brought together by their adoptive parents, who they indicated were "undemonstrative and dour." He indicates that the twins "inherited some aspects of their laugh sound and pattern, readiness to laugh, and maybe even taste in humor."[7]		Norman Cousins developed a recovery program incorporating megadoses of Vitamin C, along with a positive attitude, love, faith, hope, and laughter induced by Marx Brothers films. "I made the joyous discovery that ten minutes of genuine belly laughter had an anesthetic effect and would give me at least two hours of pain-free sleep," he reported. "When the pain-killing effect of the laughter wore off, we would switch on the motion picture projector again and not infrequently, it would lead to another pain-free interval."[8][9]		Scientists have noted the similarity in forms of laughter induced by tickling among various primates, which suggests that laughter derives from a common origin among primate species.[10][11]		A very rare neurological condition has been observed whereby the sufferer is unable to laugh out loud, a condition known as aphonogelia.[12]		Neurophysiology indicates that laughter is linked with the activation of the ventromedial prefrontal cortex, that produces endorphins.[13] Scientists have shown that parts of the limbic system are involved in laughter. This system is involved in emotions and helps us with functions necessary for humans' survival. The structures in the limbic system that are involved in laughter are the hippocampus and the amygdala.[14]		The December 7, 1984, Journal of the American Medical Association describes the neurological causes of laughter as follows:		Some drugs are well known for their laughter-facilitating properties (e. g. ethanol and cannabis), while the others, like salvinorin A (the active ingredient of Salvia divinorum), can even induce bursts of uncontrollable laughter.[15]		A link between laughter and healthy function of blood vessels was first reported in 2005 by researchers at the University of Maryland Medical Center with the fact that laughter causes the dilatation of the inner lining of blood vessels, the endothelium, and increases blood flow.[16] Drs. Michael Miller (University of Maryland) and William Fry (Stanford), theorize that beta-endorphin like compounds released by the hypothalamus activate receptors on the endothelial surface to release nitric oxide, thereby resulting in dilation of vessels. Other cardioprotective properties of nitric oxide include reduction of inflammation and decreased platelet aggregation.[17][18]		Laughter has proven beneficial effects on various other aspects of biochemistry. It has been shown to lead to reductions in stress hormones such as cortisol and epinephrine. When laughing the brain also releases endorphins that can relieve some physical pain.[19] Laughter also boosts the number of antibody-producing cells and enhances the effectiveness of T-cells, leading to a stronger immune system.[20] A 2000 study found that people with heart disease were 40 percent less likely to laugh and be able to recognize humor in a variety of situations, compared to people of the same age without heart disease.[21]		A number of studies using methods of conversation analysis and discourse analysis have documented the systematic workings of laughter in a variety of interactions, from casual conversations to interviews,meetings, and therapy sessions.[22] Working with recorded interactions, researchers have created detailed transcripts that indicate not only the presence of laughter but also features of its production and placement.		These studies challenge several widely held assumptions about the nature of laughter. Contrary to notions that it is spontaneous and involuntary, research documents that laughter is sequentially-organized and precisely placed relative to surrounding talk. Far more than merely a response to humor, laughter often works to manage delicate and serious moments. More than simply an external behavior “caused” by an inner state, laughter is highly communicative and helps accomplish actions and regulate relationships.		Common causes for laughter are sensations of joy and humor; however, other situations may cause laughter as well.		A general theory that explains laughter is called the relief theory. Sigmund Freud summarized it in his theory that laughter releases tension and "psychic energy". This theory is one of the justifications of the beliefs that laughter is beneficial for one's health.[23] This theory explains why laughter can be used as a coping mechanism when one is upset, angry or sad.		Philosopher John Morreall theorizes that human laughter may have its biological origins as a kind of shared expression of relief at the passing of danger. Friedrich Nietzsche, by contrast, suggested laughter to be a reaction to the sense of existential loneliness and mortality that only humans feel.		For example: a joke creates an inconsistency and the audience automatically try to understand what the inconsistency means; if they are successful in solving this 'cognitive riddle' and they realize that the surprise was not dangerous, they laugh with relief. Otherwise, if the inconsistency is not resolved, there is no laugh, as Mack Sennett pointed out: "when the audience is confused, it doesn't laugh." This is one of the basic laws of a comedian, referred to as "exactness". It is important to note that sometimes the inconsistency may be resolved and there may still be no laugh.[citation needed] Because laughter is a social mechanism, an audience may not feel as if they are in danger, and the laugh may not occur. In addition, the extent of the inconsistency (and aspects of it timing and rhythm) has to do with the amount of danger the audience feels, and how hard or long they laugh.		Laughter can also be brought on by tickling. Although most people find it unpleasant, being tickled often causes heavy laughter, thought to be an (often uncontrollable) reflex of the body.[24][25]		Laughter can be classified according to:		A normal laugh has the structure of "ha-ha-ha" or "ho-ho-ho." It is unnatural, and one is physically unable, to have a laugh structure of "ha-ho-ha-ho." The usual variations of a laugh most often occur in the first or final note in a sequence- therefore, "ho-ha-ha" or "ha-ha-ho" laughs are possible. Normal note durations with unusually long or short "inter-note intervals" do not happen due to the result of the limitations of our vocal cords. This basic structure allows one to recognize a laugh despite individual variants.[28]		It has also been determined that eyes moisten during laughter as a reflex from the tear glands.[20]		Laughter is not always a pleasant experience and is associated with several negative phenomena. Excessive laughter can lead to cataplexy, and unpleasant laughter spells, excessive elation, and fits of laughter can all be considered negative aspects of laughter. Unpleasant laughter spells, or "sham mirth," usually occur in people who have a neurological condition, including patients with pseudobulbar palsy, multiple sclerosis and Parkinson's disease. These patients appear to be laughing out of amusement but report that they are feeling undesirable sensations "at the time of the punch line."		Excessive elation is a common symptom associated with manic-depressive psychoses and mania/hypomania. Those who suffer from schizophrenic psychoses seem to suffer the opposite—they do not understand humor or get any joy out of it. A fit describes an abnormal time when one cannot control the laughter or one’s body, sometimes leading to seizures or a brief period of unconsciousness. Some believe that fits of laughter represent a form of epilepsy.[29]		Laughter has been used as a therapeutic tool for many years because it is a natural form of medicine. Laughter is available to everyone and it provides benefits to a person's physical, emotional, and social well being. Some of the benefits of using laughter therapy are that it can relieve stress and relax the whole body.[30] It can also boost the immune system and release endorphins to relieve pain.[31] Additionally, laughter can help prevent heart disease by increasing blood flow and improving the function of blood vessels. Some of the emotional benefits include diminishing anxiety or fear, improving overall mood, and adding joy to one's life. Laughter is also known to reduce allergic reactions in a preliminary study related to dust mite allergy sufferers.[32]		Laughter therapy also has some social benefits, such as strengthening relationships, improving teamwork and reducing conflicts, and making oneself more attractive to others. Therefore, whether a person is trying to cope with a terminal illness or just trying to manage their stress or anxiety levels, laughter therapy can be a significant enhancement to their life.[33][34]		Laughter in literature, although considered understudied by some,[35] is a subject that has received attention in the written word for millennia. The use of humor and laughter in literary works has been studied and analyzed by many thinkers and writers, from the Ancient Greek philosophers onward. Henri Bergson's Laughter: An Essay on the Meaning of the Comic (Le rire, 1901) is a notable 20th-century contribution.		For Herodotus, laughers can be distinguished into three types:[36]		According to Donald Lateiner, Herodotus reports about laughter for valid literary and historiological reasons. "Herodotus believes either that both nature (better, the gods' direction of it) and human nature coincide sufficiently, or that the latter is but an aspect or analogue of the former, so that to the recipient the outcome is suggested."[36] When reporting laughter, Herodotus does so in the conviction that it tells the reader something about the future and/or the character of the person laughing. It is also in this sense that it is not coincidental that in about 80% of the times when Herodotus speaks about laughter it is followed by a retribution. "Men whose laughter deserves report are marked, because laughter connotes scornful disdain, disdain feeling of superiority, and this feeling and the actions which stem from it attract the wrath of the gods."[36]		Thomas Hobbes understood the superiority of the laughter in a much wider sense than the aesthetic and quasi-moral sense of Aristotle, the seeds of the superiority theory are definitely Greek.[37] In Hobbes' own words: "The passion of laughter is nothing else but sudden glory arising from sudden conception of some eminency in ourselves, by comparison with the infirmity of others, or with our own formerly."		Philosopher Arthur Schopenhauer devotes the 13th chapter of the first part of his major work, The World as Will and Representation, to laughter.		Friedrich Nietzsche distinguishes two different purposes for the use of laughter. In a positive sense, "man uses the comical as a therapy against the restraining jacket of logic morality and reason. He needs from time to time a harmless demotion from reason and hardship and in this sense laughter has a positive character for Nietzsche."[38] Laughter can, however, also have a negative connotation when it is used for the expression of social conflict. This is expressed, for instance, in The Gay Science: "Laughter -- Laughter means to be schadenfroh, but with clear conscience."[39]		"Possibly Nietzsche's works would have had a totally different effect, if the playful, ironical and joking in his writings would have been factored in better"[40]		In Laughter: An Essay on the Meaning of the Comic, French philosopher Henri Bergson, renowned for his philosophical studies on materiality, memory, life and consciousness, tries to determine the laws of the comic and to understand the fundamental causes of comic situations.[41] His method consists in determining the causes of comic instead of analyzing its effects. He also deals with laughter in relation to human life, collective imagination and art, to have a better knowledge of society.[42] One of the theories of the essay is that laughter, as a collective activity, has a social and moral role, in forcing people to eliminate their vices. It is a factor of uniformity of behaviours, as it condemns ludicrous and eccentric behaviours.[43]		In this essay, Bergson also asserts that there is a central cause that all comic situations are derived from: that of mechanism applied to life. The fundamental source of comic is the presence of inflexibility and rigidness in life. For Bergson, the essence of life is movement, elasticity and flexibility, and every comic situation is due the presence of rigidity and inelasticity in life. Hence, for Bergson the source of the comic is not ugliness but rigidity.[44] All the examples taken by Bergson (such as a man falling in the street, one person's imitation of another, the automatic application of conventions and rules, absent-mindedness, repetitive gestures of a speaker, the resemblance between two faces) are comic situations because they give the impression that life is subject to rigidity, automatism and mechanism.		Bergson closes by noting that most comic situations are not laughable because they are part of collective habits.[45] He defines laughter as an intellectual activity that requires an immediate approach to a comic situation, detached from any form of emotion or sensibility.[46] A situation is laughable when the attention and the imagination are focused on the resistance and rigidity of the body. Thus somebody is laughable when he or she gives the impression of being a thing or a machine.		Anthony Ludovici developed the thoughts of Hobbes and Darwin even further in The Secret of Laughter. His conviction is that there's something sinister in laughter, and that the modern omnipresence of humour and the idolatry of it are signs of societal weakness, as instinctive resort to humour became a sort of escapism from responsibility and action. Ludovici considered laughter to be an evolutionary trait and he offered many examples of different triggers for laughter with their own distinct explanations. [47]		
The picaresque novel (Spanish: "picaresca", from "pícaro", for "rogue" or "rascal") is a genre of prose fiction that depicts the adventures of a roguish hero/heroine of low social class who lives by his or her wits in a corrupt society. Picaresque novels typically adopt a realistic style, with elements of comedy and satire. This style of novel originated in 16th-century Spain and flourished throughout Europe in the 17th and 18th centuries. It continues to influence modern literature.		According to the traditional view of Thrall and Hibbard (first published in 1936), seven qualities distinguish the picaresque novel or narrative form, all or some of which an author may employ for effect:[1]						The word pícaro first starts to appear in Spain with the current meaning in 1545, though at the time it had no association with literature.[2] The word pícaro does not appear in Lazarillo de Tormes (1554), the novella credited by modern scholars with founding the genre. The expression picaresque novel was coined in 1810.[3][4] Whether it has any validity at all as a generic label in the Spanish sixteenth and seventeenth centuries—and Cervantes certainly used "picaresque" with a different meaning than it has today—has been called into question. There is an unending campaign within Hispanic studies about what the term means, or meant, and which works were, or should be, so called. The only work clearly called "picaresque" by its contemporaries was Mateo Alemán's Guzmán de Alfarache (1599), which to them was the Libro del pícaro (The Book of the Pícaro).[5]		While elements of Chaucer and Boccaccio have a picaresque feel and may have contributed to the style,[6] the modern picaresque begins with Lazarillo de Tormes,[7] which was published anonymously in 1554 in Burgos, Medina del Campo, and Alcalá de Henares in Spain, and also in Antwerp, which at the time was under Spanish rule as a major city in the Spanish Netherlands. It is variously considered either the first picaresque novel or at least the antecedent of the genre.		The protagonist, Lázaro, lives by his wits in an effort to survive and succeed in an impoverished country full of hypocrisy. As a pícaro character, he is an alienated outsider, whose ability to expose and ridicule individuals compromised with society gives him a revolutionary stance.[8] Lázaro states that the motivation for his writing is to communicate his experiences of overcoming deception, hypocrisy, and falsehood (desengaño).[9]		The character type draws on elements of characterization already present in Roman literature, especially Petronius' Satyricon. Lázaro shares some of the traits of the central figure of Encolpius, a former gladiator,[10][11] though it is unlikely that the author had access to Petronius' work.[12] From the comedies of Plautus, Lazarillo borrows the figure of the parasite and the supple slave. Other traits are taken from Apuleius's The Golden Ass.[10] The Golden Ass and Satyricon are rare surviving samples of the "Milesian tale", a popular genre in the classical world, and were revived and widely read in renaissance Europe.		The principal episodes of Lazarillo are based on Arabic folktales that were well-known to the Moorish inhabitants of Spain. The Arabic influence may account for the negative portrayal of priests and other church officials in Lazarillo.[13] Arabic literature, which was read widely in Spain in the time of Al-Andalus and possessed a literary tradition with similar themes, is thus another possible influence on the picaresque style. Al-Hamadhani (d.1008) of Hamadhan (Iran) is credited with inventing the literary genre of maqamat in which a wandering vagabond makes his living on the gifts his listeners give him following his extemporaneous displays of rhetoric, erudition, or verse, often done with a trickster's touch.[14] Ibn al-Astarkuwi or al-Ashtarkuni (d.1134) also wrote in the genre maqamat, comparable to later European picaresque.[15]		The curious presence of Russian loan-words in the text of the Lazarillo also suggests the influence of medieval Slavic tales of tricksters, thieves, itinerant prostitutes, and brigands, who were common figures in the impoverished areas bordering on Germany to the west. When diplomatic ties to Germany and Spain were established under the emperor Charles V, these tales began to be read in Italian translations in the Iberian Peninsula.[16]		As narrator of his own adventures, Lázaro seeks to portray himself as the victim of both his ancestry and his circumstance. This means of appealing to the compassion of the reader would be directly challenged by later picaresque novels such as Guzmán de Alfarache (1599/1604) and the Buscón (composed in the first decade of the 17th century and first published in 1626) because the idea of determinism used to cast the pícaro as a victim clashed with the Counter-Reformation doctrine of free will.[17]		The autobiography of Benvenuto Cellini, written in Florence beginning in 1558, also has much in common with the picaresque. Another early example is Mateo Alemán's Guzmán de Alfarache (1599), characterized by religiosity. Guzmán de Alfrache is a fictional character who lived in San Juan de Aznalfarache, Seville, Spain.		Francisco de Quevedo's El buscón (1604 according to Francisco Rico; the exact date is uncertain, yet it was certainly a very early work) is considered the masterpiece of the subgenre by A. A. Parker, because of his baroque style and the study of the delinquent psychology. However, a more recent school of thought, led by Francisco Rico, rejects Parker's view, contending instead that the protagonist, Pablos, is a highly unrealistic character, simply a means for Quevedo to launch classist, racist and sexist attacks. Moreover, argues Rico, the structure of the novel is radically different from previous works of the picaresque genre: Quevedo uses the conventions of the picaresque as a mere vehicle to show off his abilities with conceit and rhetoric, rather than to construct a satirical critique of Spanish Golden Age society.		Indeed, in order to understand the historical context that led to the development of these paradigmatic picaresque novels in Spain during the 16th and 17th centuries, it is essential to take into consideration the circumstances surrounding the lives of conversos, whose ancestors had been Jewish, and whose New Christian faith was subjected to close scrutiny and mistrust.[18]		In other European countries, these Spanish novels were read and imitated. In Germany, Grimmelshausen wrote Simplicius Simplicissimus (1669), the most important of non-Spanish picaresque novels. It describes the devastation caused by the Thirty Years' War. In Le Sage's Gil Blas (1715) is a classic example of the genre,[19] which in France had declined into an aristocratic adventure.[citation needed] In Britain, the first example is Thomas Nashe's The Unfortunate Traveller (1594) in which a court page, Jack Wilson, exposes the underclass life in a string of European cities through lively, often brutal descriptions.[20] The body of Tobias Smollett's work, and Daniel Defoe's Moll Flanders (1722) are considered picaresque, but they lack the sense of religious redemption of delinquency that was very important in Spanish and German novels. The triumph of Moll Flanders is more economic than moral.		The classic Chinese novel Journey to the West is considered to have considerable picaresque elements. Having been written in 1590, it is contemporary with much of the above—but is unlikely to have been directly influenced by the European genre.		In the English-speaking world, the term "picaresque" has referred more to a literary technique or model than to the precise genre that the Spanish call picaresco.		The English-language term can simply refer to an episodic recounting of the adventures of an anti-hero on the road. Thomas Nashe's novel The Unfortunate Traveller is often cited as one of the earliest examples of an English picaresque novel.[citation needed] Henry Fielding proved his mastery of the form in Joseph Andrews (1742), The Life of Jonathan Wild the Great (1743) and The History of Tom Jones, a Foundling (1749), though Fielding attributed his style to an "imitation of the manner of Cervantes, author of Don Quixote", rather than of any particular picaresque novel;[21] Cervantes wrote a short picaresque novel, Rinconete y Cortadillo part of his Novelas Ejemplares (Exemplary Novels).[22]		Voltaire's French novel Candide (1759) contains elements of the picaresque. An interesting variation on the tradition of the picaresque is The Adventures of Hajji Baba of Ispahan (1824), a satirical view on early 19th-century Persia, written by a British diplomat, James Morier.		Gogol occasionally used the technique, as in Dead Souls (1842–52).[23] Mark Twain's Adventures of Huckleberry Finn (1884) was consciously written as a picaresque novel.[citation needed]		Rudyard Kipling's Kim (1901) combined the influence of the picaresque novel with the modern spy novel. Pío Baroja's novel Zalacain the Adventurer, published in 1909, used the picaresque format in the context of the Carlist Wars. The illustrated book The Magic Pudding (1918), by Australian author Norman Lindsay, is an example of the picaresque adapted for children's literature.		The Enormous Room is E. E. Cummings' 1922 autobiographical novel about his imprisonment in France during World War I on unfounded charges of "espionage", and it includes many picaresque depictions of his adventures as "an American in a French prison". Jaroslav Hašek's The Good Soldier Švejk (1923) is an example of the picaresque technique from Central Europe.		Kvachi Kvachantiradze is a novel written by Mikheil Javakhishvili in 1924.This is, in brief, the story of a swindler, a Georgian Felix Krull, or perhaps a cynical Don Quixote, named Kvachi Kvachantiradze: womanizer, cheat, perpetrator of insurance fraud, bank-robber, associate of Rasputin, filmmaker, revolutionary, and pimp.		The Twelve Chairs (1928) and its sequel, The Little Golden Calf (1931), by Ilya Ilf and Yevgeni Petrov became classics of the 20th century Russian satire and basis for numerous film adaptations. J.B. Priestley made use of the form in his The Good Companions (1929) which won the James Tait Black Memorial Prize for Fiction. Many other novels of vagabond life were consciously written as picaresque novels, such as Henry Miller's Tropic of Cancer (1934).[citation needed].		Camilo José Cela's La familia de Pascual Duarte (1942). John A. Lee's Shining with the Shiner (1944) tells amusing tales about New Zealand folk hero Ned Slattery (1840–1927) surviving by his wits and beating the Protestant work ethic. Saul Bellow's The Adventures of Augie March (1953) is a picaresque novel with bildungsroman traits. So too is Thomas Mann's Confessions of Felix Krull (1954), which like many novels emphasizes the theme of a charmingly roguish ascent in the social order. George MacDonald Fraser's novels about Harry Flashman (1969) combine the picaresque with historical fiction. Günter Grass's The Tin Drum (1959) is a German picaresque novel.		Sergio Leone identified his spaghetti westerns, more specifically his Dollars trilogy (1964), as being in the picaresque style.		One might characterize the novels and stories of Dashiell Hammett and other hardboiled and noir fiction as picaresque, with some qualifications (e.g., the novels of Hammett are very tightly plotted).		Hunter S. Thompson's "gonzo journalism" (1970) can be seen as a hybrid of fictional picaresque with memoir and traditional reporting. The picaresque elements are especially prominent in Thompson's less journalistic, more literary and psychotropically themed works, such as, Fear and Loathing in Las Vegas (1971) and The Great Shark Hunt (1979).		Recent examples include Under the Net (1954) by Iris Murdoch,[24] Thomas Berger's Little Big Man (1964), Jerzy Kosinski's The Painted Bird (1965), Vladimir Voinovich's The Life and Extraordinary Adventures of Private Ivan Chonkin (1969), Rita Mae Brown's Rubyfruit Jungle (1973), John Kennedy Toole's A Confederacy of Dunces (1980), Bernard Cornwell's Sharpe series (1981–), Angela Carter's Nights at the Circus (1984), Isabel Allende's Eva Luna (1987), Edward Abbey's The Fool's Progress: An Honest Novel (1988), Helen Zahavi's Dirty Weekend (1991), C. D. Payne's Youth in Revolt (1993), Christian Kracht's Faserland (1995), Umberto Eco's Baudolino (2000),[25] Neal Stephenson's Quicksilver (2003), and Aravind Adiga's The White Tiger (Booker Prize 2008).[26]		Some science fiction and fantasy books also show a clear picaresque influence, transported to a variety of invented worlds—for example, The Dying Earth series of Jack Vance, Fritz Leiber's Fafhrd and the Gray Mouser, Harry Harrison's The Stainless Steel Rat series, James H. Schmitz's The Witches of Karres, and L. Sprague de Camp's Novarian series.[citation needed] The genre-bending fiction of Gene Wolfe combines strong elements of the picaresque with a catalog of other forms of fiction—bildungsroman, memoir, mythic poem, classical drama, modernist fiction, and others; this is the case particularly in his Book of the New Sun, the tale of Severian the Torturer's rise to the monarchy in a remote future world that is probably Earth.[citation needed] More recently, Scott Lynch's The Gentleman Bastard Sequence fantasy novels have been described as fine examples of the subgenre.[27] The SF/F roleplaying game Shadowrun is based on picaresque player characters in a corrupt dystopia.[28]		Media related to Picaresque novel at Wikimedia Commons		
Charles Robert Darwin, FRS FRGS FLS FZS[2] (/ˈdɑːrwɪn/;[3] 12 February 1809 – 19 April 1882) was an English naturalist, geologist and biologist,[4] best known for his contributions to the science of evolution.[I] He established that all species of life have descended over time from common ancestors[5] and, in a joint publication with Alfred Russel Wallace, introduced his scientific theory that this branching pattern of evolution resulted from a process that he called natural selection, in which the struggle for existence has a similar effect to the artificial selection involved in selective breeding.[6]		Darwin published his theory of evolution with compelling evidence in his 1859 book On the Origin of Species, overcoming scientific rejection of earlier concepts of transmutation of species.[7][8] By the 1870s, the scientific community and much of the general public had accepted evolution as a fact. However, many favoured competing explanations and it was not until the emergence of the modern evolutionary synthesis from the 1930s to the 1950s that a broad consensus developed in which natural selection was the basic mechanism of evolution.[9][10] In modified form, Darwin's scientific discovery is the unifying theory of the life sciences, explaining the diversity of life.[11][12]		Darwin's early interest in nature led him to neglect his medical education at the University of Edinburgh; instead, he helped to investigate marine invertebrates. Studies at the University of Cambridge (Christ's College) encouraged his passion for natural science.[13] His five-year voyage on HMS Beagle established him as an eminent geologist whose observations and theories supported Charles Lyell's uniformitarian ideas, and publication of his journal of the voyage made him famous as a popular author.[14]		Puzzled by the geographical distribution of wildlife and fossils he collected on the voyage, Darwin began detailed investigations and in 1838 conceived his theory of natural selection.[15] Although he discussed his ideas with several naturalists, he needed time for extensive research and his geological work had priority.[16] He was writing up his theory in 1858 when Alfred Russel Wallace sent him an essay that described the same idea, prompting immediate joint publication of both of their theories.[17] Darwin's work established evolutionary descent with modification as the dominant scientific explanation of diversification in nature.[9] In 1871 he examined human evolution and sexual selection in The Descent of Man, and Selection in Relation to Sex, followed by The Expression of the Emotions in Man and Animals (1872). His research on plants was published in a series of books, and in his final book, The Formation of Vegetable Mould, through the Actions of Worms (1881), he examined earthworms and their effect on soil.[18][19]		Darwin has been described as one of the most influential figures in human history,[20] and he was honoured by burial in Westminster Abbey.[21]						Charles Robert Darwin was born in Shrewsbury, Shropshire, on 12 February 1809, at his family's home, The Mount.[22] He was the fifth of six children of wealthy society doctor and financier Robert Darwin and Susannah Darwin (née Wedgwood). He was the grandson of two prominent abolitionists: Erasmus Darwin on his father's side, and Josiah Wedgwood on his mother's side.		Both families were largely Unitarian, though the Wedgwoods were adopting Anglicanism. Robert Darwin, himself quietly a freethinker, had baby Charles baptised in November 1809 in the Anglican St Chad's Church, Shrewsbury, but Charles and his siblings attended the Unitarian chapel with their mother. The eight-year-old Charles already had a taste for natural history and collecting when he joined the day school run by its preacher in 1817. That July, his mother died. From September 1818, he joined his older brother Erasmus attending the nearby Anglican Shrewsbury School as a boarder.[23]		Darwin spent the summer of 1825 as an apprentice doctor, helping his father treat the poor of Shropshire, before going to the University of Edinburgh Medical School (at the time the best medical school in the UK) with his brother Erasmus in October 1825. Darwin found lectures dull and surgery distressing, so he neglected his studies. He learned taxidermy in around 40 daily hour-long sessions from John Edmonstone, a freed black slave who had accompanied Charles Waterton in the South American rainforest.[24]		In Darwin's second year at the university he joined the Plinian Society, a student natural-history group featuring lively debates in which radical democratic students with materialistic views challenged orthodox religious concepts of science.[25] He assisted Robert Edmond Grant's investigations of the anatomy and life cycle of marine invertebrates in the Firth of Forth, and on 27 March 1827 presented at the Plinian his own discovery that black spores found in oyster shells were the eggs of a skate leech. One day, Grant praised Lamarck's evolutionary ideas. Darwin was astonished by Grant's audacity, but had recently read similar ideas in his grandfather Erasmus' journals.[26] Darwin was rather bored by Robert Jameson's natural-history course, which covered geology – including the debate between Neptunism and Plutonism. He learned the classification of plants, and assisted with work on the collections of the University Museum, one of the largest museums in Europe at the time.[27]		Darwin's neglect of medical studies annoyed his father, who shrewdly sent him to Christ's College, Cambridge, to study for a Bachelor of Arts degree as the first step towards becoming an Anglican country parson. As Darwin was unqualified for the Tripos, he joined the ordinary degree course in January 1828.[28] He preferred riding and shooting to studying. His cousin William Darwin Fox introduced him to the popular craze for beetle collecting; Darwin pursued this zealously, getting some of his finds published in James Francis Stephens' Illustrations of British entomology. He became a close friend and follower of botany professor John Stevens Henslow and met other leading parson-naturalists who saw scientific work as religious natural theology, becoming known to these dons as "the man who walks with Henslow". When his own exams drew near, Darwin focused on his studies and was delighted by the language and logic of William Paley's Evidences of Christianity[29] (1794). In his final examination in January 1831 Darwin did well, coming tenth out of 178 candidates for the ordinary degree.[30]		Darwin had to stay at Cambridge until June 1831. He studied Paley's Natural Theology or Evidences of the Existence and Attributes of the Deity (first published in 1802), which made an argument for divine design in nature, explaining adaptation as God acting through laws of nature.[31] He read John Herschel's new book, Preliminary Discourse on the Study of Natural Philosophy (1831), which described the highest aim of natural philosophy as understanding such laws through inductive reasoning based on observation, and Alexander von Humboldt's Personal Narrative of scientific travels in 1799–1804. Inspired with "a burning zeal" to contribute, Darwin planned to visit Tenerife with some classmates after graduation to study natural history in the tropics. In preparation, he joined Adam Sedgwick's geology course, then travelled with him in the summer for a fortnight, in order to map strata in Wales.[32][33]		After a week with student friends at Barmouth, Darwin returned home on 29 August to find a letter from Henslow proposing him as a suitable (if unfinished) naturalist for a self-funded supernumerary place on HMS Beagle with captain Robert FitzRoy, emphasising that this was a position for a gentleman rather than "a mere collector". The ship was to leave in four weeks on an expedition to chart the coastline of South America.[34] Robert Darwin objected to his son's planned two-year voyage, regarding it as a waste of time, but was persuaded by his brother-in-law, Josiah Wedgwood II, to agree to (and fund) his son's participation.[35] Darwin took care to remain in a private capacity to retain control over his collection, intending it for a major scientific institution.[36]		After delays, the voyage began on 27 December 1831; it lasted almost five years. As FitzRoy had intended, Darwin spent most of that time on land investigating geology and making natural history collections, while the Beagle surveyed and charted coasts.[9][37] He kept careful notes of his observations and theoretical speculations, and at intervals during the voyage his specimens were sent to Cambridge together with letters including a copy of his journal for his family.[38] He had some expertise in geology, beetle collecting and dissecting marine invertebrates, but in all other areas was a novice and ably collected specimens for expert appraisal.[39] Despite suffering badly from seasickness, Darwin wrote copious notes while on board the ship. Most of his zoology notes are about marine invertebrates, starting with plankton collected in a calm spell.[37][40]		On their first stop ashore at St Jago in Cape Verde, Darwin found that a white band high in the volcanic rock cliffs included seashells. FitzRoy had given him the first volume of Charles Lyell's Principles of Geology, which set out uniformitarian concepts of land slowly rising or falling over immense periods,[II] and Darwin saw things Lyell's way, theorising and thinking of writing a book on geology.[41] When they reached Brazil, Darwin was delighted by the tropical forest,[42] but detested the sight of slavery, and disputed this issue with Fitzroy.[43]		The survey continued to the south in Patagonia. They stopped at Bahía Blanca, and in cliffs near Punta Alta Darwin made a major find of fossil bones of huge extinct mammals beside modern seashells, indicating recent extinction with no signs of change in climate or catastrophe. He identified the little-known Megatherium by a tooth and its association with bony armour, which had at first seemed to him to be like a giant version of the armour on local armadillos. The finds brought great interest when they reached England.[44][45]		On rides with gauchos into the interior to explore geology and collect more fossils, Darwin gained social, political and anthropological insights into both native and colonial people at a time of revolution, and learnt that two types of rhea had separate but overlapping territories.[46][47] Further south, he saw stepped plains of shingle and seashells as raised beaches showing a series of elevations. He read Lyell's second volume and accepted its view of "centres of creation" of species, but his discoveries and theorising challenged Lyell's ideas of smooth continuity and of extinction of species.[48][49]		Three Fuegians on board had been seized during the first Beagle voyage, then during a year in England were educated as missionaries. Darwin found them friendly and civilised, yet at Tierra del Fuego he met "miserable, degraded savages", as different as wild from domesticated animals.[50] He remained convinced that, despite this diversity, all humans were interrelated with a shared origin and potential for improvement towards civilisation. Unlike his scientist friends, he now thought there was no unbridgeable gap between humans and animals.[51] A year on, the mission had been abandoned. The Fuegian they had named Jemmy Button lived like the other natives, had a wife, and had no wish to return to England.[52]		Darwin experienced an earthquake in Chile and saw signs that the land had just been raised, including mussel-beds stranded above high tide. High in the Andes he saw seashells, and several fossil trees that had grown on a sand beach. He theorised that as the land rose, oceanic islands sank, and coral reefs round them grew to form atolls.[53][54]		On the geologically new Galápagos Islands, Darwin looked for evidence attaching wildlife to an older "centre of creation", and found mockingbirds allied to those in Chile but differing from island to island. He heard that slight variations in the shape of tortoise shells showed which island they came from, but failed to collect them, even after eating tortoises taken on board as food.[55][56] In Australia, the marsupial rat-kangaroo and the platypus seemed so unusual that Darwin thought it was almost as though two distinct Creators had been at work.[57] He found the Aborigines "good-humoured & pleasant", and noted their depletion by European settlement.[58]		The Beagle investigated how the atolls of the Cocos (Keeling) Islands had formed, and the survey supported Darwin's theorising.[54] FitzRoy began writing the official Narrative of the Beagle voyages, and after reading Darwin's diary he proposed incorporating it into the account.[59] Darwin's Journal was eventually rewritten as a separate third volume, on natural history.[60]		In Cape Town, Darwin and FitzRoy met John Herschel, who had recently written to Lyell praising his uniformitarianism as opening bold speculation on "that mystery of mysteries, the replacement of extinct species by others" as "a natural in contradistinction to a miraculous process".[61] When organising his notes as the ship sailed home, Darwin wrote that, if his growing suspicions about the mockingbirds, the tortoises and the Falkland Islands fox were correct, "such facts undermine the stability of Species", then cautiously added "would" before "undermine".[62] He later wrote that such facts "seemed to me to throw some light on the origin of species".[63]		When the Beagle reached Falmouth, Cornwall, on 2 October 1836, Darwin was already a celebrity in scientific circles as in December 1835 Henslow had fostered his former pupil's reputation by giving selected naturalists a pamphlet of Darwin's geological letters.[64] Darwin visited his home in Shrewsbury and saw relatives, then hurried to Cambridge to see Henslow, who advised him on finding naturalists available to catalogue the collections and agreed to take on the botanical specimens. Darwin's father organised investments, enabling his son to be a self-funded gentleman scientist, and an excited Darwin went round the London institutions being fêted and seeking experts to describe the collections. Zoologists had a huge backlog of work, and there was a danger of specimens just being left in storage.[65]		Charles Lyell eagerly met Darwin for the first time on 29 October and soon introduced him to the up-and-coming anatomist Richard Owen, who had the facilities of the Royal College of Surgeons to work on the fossil bones collected by Darwin. Owen's surprising results included other gigantic extinct ground sloths as well as the Megatherium, a near complete skeleton of the unknown Scelidotherium and a hippopotamus-sized rodent-like skull named Toxodon resembling a giant capybara. The armour fragments were actually from Glyptodon, a huge armadillo-like creature as Darwin had initially thought.[66][45] These extinct creatures were related to living species in South America.[67]		In mid-December, Darwin took lodgings in Cambridge to organise work on his collections and rewrite his Journal.[68] He wrote his first paper, showing that the South American landmass was slowly rising, and with Lyell's enthusiastic backing read it to the Geological Society of London on 4 January 1837. On the same day, he presented his mammal and bird specimens to the Zoological Society. The ornithologist John Gould soon announced that the Galapagos birds that Darwin had thought a mixture of blackbirds, "gros-beaks" and finches, were, in fact, twelve separate species of finches. On 17 February, Darwin was elected to the Council of the Geological Society, and Lyell's presidential address presented Owen's findings on Darwin's fossils, stressing geographical continuity of species as supporting his uniformitarian ideas.[69]		Early in March, Darwin moved to London to be near this work, joining Lyell's social circle of scientists and experts such as Charles Babbage,[70] who described God as a programmer of laws. Darwin stayed with his freethinking brother Erasmus, part of this Whig circle and a close friend of the writer Harriet Martineau, who promoted Malthusianism underlying the controversial Whig Poor Law reforms to stop welfare from causing overpopulation and more poverty. As a Unitarian, she welcomed the radical implications of transmutation of species, promoted by Grant and younger surgeons influenced by Geoffroy. Transmutation was anathema to Anglicans defending social order,[71] but reputable scientists openly discussed the subject and there was wide interest in John Herschel's letter praising Lyell's approach as a way to find a natural cause of the origin of new species.[61]		Gould met Darwin and told him that the Galápagos mockingbirds from different islands were separate species, not just varieties, and what Darwin had thought was a "wren" was also in the finch group. Darwin had not labelled the finches by island, but from the notes of others on the Beagle, including FitzRoy, he allocated species to islands.[72] The two rheas were also distinct species, and on 14 March Darwin announced how their distribution changed going southwards.[73]		By mid-March, Darwin was speculating in his Red Notebook on the possibility that "one species does change into another" to explain the geographical distribution of living species such as the rheas, and extinct ones such as the strange Macrauchenia, which resembled a giant guanaco. His thoughts on lifespan, asexual reproduction and sexual reproduction developed in his "B" notebook around mid-July on to variation in offspring "to adapt & alter the race to changing world" explaining the Galápagos tortoises, mockingbirds and rheas. He sketched branching descent, then a genealogical branching of a single evolutionary tree, in which "It is absurd to talk of one animal being higher than another", discarding Lamarck's independent lineages progressing to higher forms.[74]		While developing this intensive study of transmutation, Darwin became mired in more work. Still rewriting his Journal, he took on editing and publishing the expert reports on his collections, and with Henslow's help obtained a Treasury grant of £1,000 to sponsor this multi-volume Zoology of the Voyage of H.M.S. Beagle, a sum equivalent to about £82,000 in 2015.[75] He stretched the funding to include his planned books on geology, and agreed to unrealistic dates with the publisher.[76] As the Victorian era began, Darwin pressed on with writing his Journal, and in August 1837 began correcting printer's proofs.[77]		Darwin's health suffered under the pressure. On 20 September he had "an uncomfortable palpitation of the heart", so his doctors urged him to "knock off all work" and live in the country for a few weeks. After visiting Shrewsbury he joined his Wedgwood relatives at Maer Hall, Staffordshire, but found them too eager for tales of his travels to give him much rest. His charming, intelligent, and cultured cousin Emma Wedgwood, nine months older than Darwin, was nursing his invalid aunt. His uncle Josiah pointed out an area of ground where cinders had disappeared under loam and suggested that this might have been the work of earthworms, inspiring "a new & important theory" on their role in soil formation, which Darwin presented at the Geological Society on 1 November.[78]		William Whewell pushed Darwin to take on the duties of Secretary of the Geological Society. After initially declining the work, he accepted the post in March 1838.[79] Despite the grind of writing and editing the Beagle reports, Darwin made remarkable progress on transmutation, taking every opportunity to question expert naturalists and, unconventionally, people with practical experience such as farmers and pigeon fanciers.[9][80] Over time, his research drew on information from his relatives and children, the family butler, neighbours, colonists and former shipmates.[81] He included mankind in his speculations from the outset, and on seeing an orangutan in the zoo on 28 March 1838 noted its childlike behaviour.[82]		The strain took a toll, and by June he was being laid up for days on end with stomach problems, headaches and heart symptoms. For the rest of his life, he was repeatedly incapacitated with episodes of stomach pains, vomiting, severe boils, palpitations, trembling and other symptoms, particularly during times of stress, such as attending meetings or making social visits. The cause of Darwin's illness remained unknown, and attempts at treatment had little success.[83]		On 23 June, he took a break and went "geologising" in Scotland. He visited Glen Roy in glorious weather to see the parallel "roads" cut into the hillsides at three heights. He later published his view that these were marine raised beaches, but then had to accept that they were shorelines of a proglacial lake.[84]		Fully recuperated, he returned to Shrewsbury in July. Used to jotting down daily notes on animal breeding, he scrawled rambling thoughts about career and prospects on two scraps of paper, one with columns headed "Marry" and "Not Marry". Advantages included "constant companion and a friend in old age ... better than a dog anyhow", against points such as "less money for books" and "terrible loss of time."[85] Having decided in favour, he discussed it with his father, then went to visit Emma on 29 July. He did not get around to proposing, but against his father's advice he mentioned his ideas on transmutation.[86]		Continuing his research in London, Darwin's wide reading now included the sixth edition of Malthus's An Essay on the Principle of Population, and on 28 September 1838 he noted its assertion that human "population, when unchecked, goes on doubling itself every twenty five years, or increases in a geometrical ratio", a geometric progression so that population soon exceeds food supply in what is known as a Malthusian catastrophe. Darwin was well prepared to compare this to de Candolle's "warring of the species" of plants and the struggle for existence among wildlife, explaining how numbers of a species kept roughly stable. As species always breed beyond available resources, favourable variations would make organisms better at surviving and passing the variations on to their offspring, while unfavourable variations would be lost. He wrote that the "final cause of all this wedging, must be to sort out proper structure, & adapt it to changes", so that "One may say there is a force like a hundred thousand wedges trying force into every kind of adapted structure into the gaps of in the economy of nature, or rather forming gaps by thrusting out weaker ones."[9][87] This would result in the formation of new species.[9][88] As he later wrote in his Autobiography:		In October 1838, that is, fifteen months after I had begun my systematic enquiry, I happened to read for amusement Malthus on Population, and being well prepared to appreciate the struggle for existence which everywhere goes on from long-continued observation of the habits of animals and plants, it at once struck me that under these circumstances favourable variations would tend to be preserved, and unfavourable ones to be destroyed. The result of this would be the formation of new species. Here, then, I had at last got a theory by which to work..."[89]		By mid December, Darwin saw a similarity between farmers picking the best stock in selective breeding, and a Malthusian Nature selecting from chance variants so that "every part of newly acquired structure is fully practical and perfected",[90] thinking this comparison "a beautiful part of my theory".[91] He later called his theory natural selection, an analogy with what he termed the artificial selection of selective breeding.[9]		On 11 November, he returned to Maer and proposed to Emma, once more telling her his ideas. She accepted, then in exchanges of loving letters she showed how she valued his openness in sharing their differences, also expressing her strong Unitarian beliefs and concerns that his honest doubts might separate them in the afterlife.[92] While he was house-hunting in London, bouts of illness continued and Emma wrote urging him to get some rest, almost prophetically remarking "So don't be ill any more my dear Charley till I can be with you to nurse you." He found what they called "Macaw Cottage" (because of its gaudy interiors) in Gower Street, then moved his "museum" in over Christmas. On 24 January 1839, Darwin was elected a Fellow of the Royal Society (FRS).[2][93]		On 29 January, Darwin and Emma Wedgwood were married at Maer in an Anglican ceremony arranged to suit the Unitarians, then immediately caught the train to London and their new home.[94]		Darwin now had the framework of his theory of natural selection "by which to work",[89] as his "prime hobby".[95] His research included extensive experimental selective breeding of plants and animals, finding evidence that species were not fixed and investigating many detailed ideas to refine and substantiate his theory.[9] For fifteen years this work was in the background to his main occupation of writing on geology and publishing expert reports on the Beagle collections.[96]		When FitzRoy's Narrative was published in May 1839, Darwin's Journal and Remarks was such a success as the third volume that later that year it was published on its own.[97] Early in 1842, Darwin wrote about his ideas to Charles Lyell, who noted that his ally "denies seeing a beginning to each crop of species".[98]		Darwin's book The Structure and Distribution of Coral Reefs on his theory of atoll formation was published in May 1842 after more than three years of work, and he then wrote his first "pencil sketch" of his theory of natural selection.[99] To escape the pressures of London, the family moved to rural Down House in September.[100] On 11 January 1844, Darwin mentioned his theorising to the botanist Joseph Dalton Hooker, writing with melodramatic humour "it is like confessing a murder".[101][102] Hooker replied "There may in my opinion have been a series of productions on different spots, & also a gradual change of species. I shall be delighted to hear how you think that this change may have taken place, as no presently conceived opinions satisfy me on the subject."[103]		By July, Darwin had expanded his "sketch" into a 230-page "Essay", to be expanded with his research results if he died prematurely.[105] In November, the anonymously published sensational best-seller Vestiges of the Natural History of Creation brought wide interest in transmutation. Darwin scorned its amateurish geology and zoology, but carefully reviewed his own arguments. Controversy erupted, and it continued to sell well despite contemptuous dismissal by scientists.[106][107]		Darwin completed his third geological book in 1846. He now renewed a fascination and expertise in marine invertebrates, dating back to his student days with Grant, by dissecting and classifying the barnacles he had collected on the voyage, enjoying observing beautiful structures and thinking about comparisons with allied structures.[108] In 1847, Hooker read the "Essay" and sent notes that provided Darwin with the calm critical feedback that he needed, but would not commit himself and questioned Darwin's opposition to continuing acts of creation.[109]		In an attempt to improve his chronic ill health, Darwin went in 1849 to Dr. James Gully's Malvern spa and was surprised to find some benefit from hydrotherapy.[110] Then, in 1851, his treasured daughter Annie fell ill, reawakening his fears that his illness might be hereditary, and after a long series of crises she died.[111]		In eight years of work on barnacles (Cirripedia), Darwin's theory helped him to find "homologies" showing that slightly changed body parts served different functions to meet new conditions, and in some genera he found minute males parasitic on hermaphrodites, showing an intermediate stage in evolution of distinct sexes.[112] In 1853, it earned him the Royal Society's Royal Medal, and it made his reputation as a biologist.[113] In 1854 he became a Fellow of the Linnean Society of London, gaining postal access to its library.[114] He began a major reassessment of his theory of species, and in November realised that divergence in the character of descendants could be explained by them becoming adapted to "diversified places in the economy of nature".[115]		By the start of 1856, Darwin was investigating whether eggs and seeds could survive travel across seawater to spread species across oceans. Hooker increasingly doubted the traditional view that species were fixed, but their young friend Thomas Henry Huxley was firmly against the transmutation of species. Lyell was intrigued by Darwin's speculations without realising their extent. When he read a paper by Alfred Russel Wallace, "On the Law which has Regulated the Introduction of New Species", he saw similarities with Darwin's thoughts and urged him to publish to establish precedence. Though Darwin saw no threat, on 14 May 1856 he began writing a short paper. Finding answers to difficult questions held him up repeatedly, and he expanded his plans to a "big book on species" titled Natural Selection, which was to include his "note on Man". He continued his researches, obtaining information and specimens from naturalists worldwide including Wallace who was working in Borneo. In mid-1857 he added a chapter heading; "Theory applied to Races of Man", but then left out this topic. On 5 September 1857, Darwin sent the American botanist Asa Gray a detailed outline of his ideas, including an abstract of Natural Selection, which omitted human origins and sexual selection. In December, Darwin received a letter from Wallace asking if the book would examine human origins. He responded that he would avoid that subject, "so surrounded with prejudices", while encouraging Wallace's theorising and adding that "I go much further than you."[117]		Darwin's book was only partly written when, on 18 June 1858, he received a paper from Wallace describing natural selection. Shocked that he had been "forestalled", Darwin sent it on that day to Lyell, as requested by Wallace,[118][119] and although Wallace had not asked for publication, Darwin suggested he would send it to any journal that Wallace chose. His family was in crisis with children in the village dying of scarlet fever, and he put matters in the hands of his friends. After some discussion, Lyell and Hooker decided on a joint presentation at the Linnean Society on 1 July of On the Tendency of Species to form Varieties; and on the Perpetuation of Varieties and Species by Natural Means of Selection. On the evening of 28 June, Darwin's baby son died of scarlet fever after almost a week of severe illness, and he was too distraught to attend.[120]		There was little immediate attention to this announcement of the theory; the president of the Linnean Society remarked in May 1859 that the year had not been marked by any revolutionary discoveries.[121] Only one review rankled enough for Darwin to recall it later; Professor Samuel Haughton of Dublin claimed that "all that was new in them was false, and what was true was old".[122] Darwin struggled for thirteen months to produce an abstract of his "big book", suffering from ill health but getting constant encouragement from his scientific friends. Lyell arranged to have it published by John Murray.[123]		On the Origin of Species proved unexpectedly popular, with the entire stock of 1,250 copies oversubscribed when it went on sale to booksellers on 22 November 1859.[124] In the book, Darwin set out "one long argument" of detailed observations, inferences and consideration of anticipated objections.[125] In making the case for common descent, he included evidence of homologies between humans and other mammals.[126][III] Having outlined sexual selection, he hinted that it could explain differences between human races.[127][IV] He avoided explicit discussion of human origins, but implied the significance of his work with the sentence; "Light will be thrown on the origin of man and his history."[128][IV] His theory is simply stated in the introduction:		As many more individuals of each species are born than can possibly survive; and as, consequently, there is a frequently recurring struggle for existence, it follows that any being, if it vary however slightly in any manner profitable to itself, under the complex and sometimes varying conditions of life, will have a better chance of surviving, and thus be naturally selected. From the strong principle of inheritance, any selected variety will tend to propagate its new and modified form.[129]		At the end of the book he concluded that:		There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved.[130]		The last word was the only variant of "evolved" in the first five editions of the book. "Evolutionism" at that time was associated with other concepts, most commonly with embryological development, and Darwin first used the word evolution in The Descent of Man in 1871, before adding it in 1872 to the 6th edition of The Origin of Species.[131]		The book aroused international interest, with less controversy than had greeted the popular Vestiges of the Natural History of Creation.[133] Though Darwin's illness kept him away from the public debates, he eagerly scrutinised the scientific response, commenting on press cuttings, reviews, articles, satires and caricatures, and corresponded on it with colleagues worldwide.[134] The book did not explicitly discuss human origins,[128][IV] but included a number of hints about the animal ancestry of humans from which the inference could be made.[135] The first review asked, "If a monkey has become a man–what may not a man become?" and said it should be left to theologians as it was too dangerous for ordinary readers.[136] Amongst early favourable responses, Huxley's reviews swiped at Richard Owen, leader of the scientific establishment Huxley was trying to overthrow.[137] In April, Owen's review attacked Darwin's friends and condescendingly dismissed his ideas, angering Darwin,[138] but Owen and others began to promote ideas of supernaturally guided evolution. Patrick Matthew drew attention to his 1831 book which had a brief appendix suggesting a concept of natural selection leading to new species, but he had not developed the idea.[139]		The Church of England's response was mixed. Darwin's old Cambridge tutors Sedgwick and Henslow dismissed the ideas, but liberal clergymen interpreted natural selection as an instrument of God's design, with the cleric Charles Kingsley seeing it as "just as noble a conception of Deity".[140] In 1860, the publication of Essays and Reviews by seven liberal Anglican theologians diverted clerical attention from Darwin, with its ideas including higher criticism attacked by church authorities as heresy. In it, Baden Powell argued that miracles broke God's laws, so belief in them was atheistic, and praised "Mr Darwin's masterly volume [supporting] the grand principle of the self-evolving powers of nature".[141] Asa Gray discussed teleology with Darwin, who imported and distributed Gray's pamphlet on theistic evolution, Natural Selection is not inconsistent with natural theology.[140][142] The most famous confrontation was at the public 1860 Oxford evolution debate during a meeting of the British Association for the Advancement of Science, where the Bishop of Oxford Samuel Wilberforce, though not opposed to transmutation of species, argued against Darwin's explanation and human descent from apes. Joseph Hooker argued strongly for Darwin, and Thomas Huxley's legendary retort, that he would rather be descended from an ape than a man who misused his gifts, came to symbolise a triumph of science over religion.[140][143]		Even Darwin's close friends Gray, Hooker, Huxley and Lyell still expressed various reservations but gave strong support, as did many others, particularly younger naturalists. Gray and Lyell sought reconciliation with faith, while Huxley portrayed a polarisation between religion and science. He campaigned pugnaciously against the authority of the clergy in education,[140] aiming to overturn the dominance of clergymen and aristocratic amateurs under Owen in favour of a new generation of professional scientists. Owen's claim that brain anatomy proved humans to be a separate biological order from apes was shown to be false by Huxley in a long running dispute parodied by Kingsley as the "Great Hippocampus Question", and discredited Owen.[144]		Darwinism became a movement covering a wide range of evolutionary ideas. In 1863 Lyell's Geological Evidences of the Antiquity of Man popularised prehistory, though his caution on evolution disappointed Darwin. Weeks later Huxley's Evidence as to Man's Place in Nature showed that anatomically, humans are apes, then The Naturalist on the River Amazons by Henry Walter Bates provided empirical evidence of natural selection.[145] Lobbying brought Darwin Britain's highest scientific honour, the Royal Society's Copley Medal, awarded on 3 November 1864.[146] That day, Huxley held the first meeting of what became the influential "X Club" devoted to "science, pure and free, untrammelled by religious dogmas".[147] By the end of the decade most scientists agreed that evolution occurred, but only a minority supported Darwin's view that the chief mechanism was natural selection.[148]		The Origin of Species was translated into many languages, becoming a staple scientific text attracting thoughtful attention from all walks of life, including the "working men" who flocked to Huxley's lectures.[149] Darwin's theory also resonated with various movements at the time[V] and became a key fixture of popular culture.[VI] Cartoonists parodied animal ancestry in an old tradition of showing humans with animal traits, and in Britain these droll images served to popularise Darwin's theory in an unthreatening way. While ill in 1862 Darwin began growing a beard, and when he reappeared in public in 1866 caricatures of him as an ape helped to identify all forms of evolutionism with Darwinism.[132]		Despite repeated bouts of illness during the last twenty-two years of his life, Darwin's work continued. Having published On the Origin of Species as an abstract of his theory, he pressed on with experiments, research, and writing of his "big book". He covered human descent from earlier animals including evolution of society and of mental abilities, as well as explaining decorative beauty in wildlife and diversifying into innovative plant studies.		Enquiries about insect pollination led in 1861 to novel studies of wild orchids, showing adaptation of their flowers to attract specific moths to each species and ensure cross fertilisation. In 1862 Fertilisation of Orchids gave his first detailed demonstration of the power of natural selection to explain complex ecological relationships, making testable predictions. As his health declined, he lay on his sickbed in a room filled with inventive experiments to trace the movements of climbing plants.[150] Admiring visitors included Ernst Haeckel, a zealous proponent of Darwinismus incorporating Lamarckism and Goethe's idealism.[151] Wallace remained supportive, though he increasingly turned to Spiritualism.[152]		Darwin's book The Variation of Animals and Plants under Domestication (1868) was the first part of his planned "big book", and included his unsuccessful hypothesis of pangenesis attempting to explain heredity. It sold briskly at first, despite its size, and was translated into many languages. He wrote most of a second part, on natural selection, but it remained unpublished in his lifetime.[153]		Lyell had already popularised human prehistory, and Huxley had shown that anatomically humans are apes.[145] With The Descent of Man, and Selection in Relation to Sex published in 1871, Darwin set out evidence from numerous sources that humans are animals, showing continuity of physical and mental attributes, and presented sexual selection to explain impractical animal features such as the peacock's plumage as well as human evolution of culture, differences between sexes, and physical and cultural racial characteristics, while emphasising that humans are all one species.[154] His research using images was expanded in his 1872 book The Expression of the Emotions in Man and Animals, one of the first books to feature printed photographs, which discussed the evolution of human psychology and its continuity with the behaviour of animals. Both books proved very popular, and Darwin was impressed by the general assent with which his views had been received, remarking that "everybody is talking about it without being shocked."[155] His conclusion was "that man with all his noble qualities, with sympathy which feels for the most debased, with benevolence which extends not only to other men but to the humblest living creature, with his god-like intellect which has penetrated into the movements and constitution of the solar system–with all these exalted powers–Man still bears in his bodily frame the indelible stamp of his lowly origin."[156]		His evolution-related experiments and investigations led to books on Orchids, Insectivorous Plants, The Effects of Cross and Self Fertilisation in the Vegetable Kingdom, different forms of flowers on plants of the same species, and The Power of Movement in Plants. His botanical work was interpreted and popularised by various writers including Grant Allen and H. G. Wells, and helped transform plant science in the late C19 and early C20. In his last book he returned to The Formation of Vegetable Mould through the Action of Worms.		In 1882 he was diagnosed with what was called "angina pectoris" which then meant coronary thrombosis and disease of the heart. At the time of his death, the physicians diagnosed "anginal attacks", and "heart-failure".[157]		He died at Down House on 19 April 1882. His last words were to his family, telling Emma "I am not the least afraid of death – Remember what a good wife you have been to me – Tell all my children to remember how good they have been to me", then while she rested, he repeatedly told Henrietta and Francis "It's almost worth while to be sick to be nursed by you".[158] He had expected to be buried in St Mary's churchyard at Downe, but at the request of Darwin's colleagues, after public and parliamentary petitioning, William Spottiswoode (President of the Royal Society) arranged for Darwin to be honoured by burial in Westminster Abbey, close to John Herschel and Isaac Newton. The funeral was held on Wednesday 26 April and was attended by thousands of people, including family, friends, scientists, philosophers and dignitaries.[159][21]		By the time of his death, Darwin had convinced most scientists that evolution as descent with modification was correct, and he was regarded as a great scientist who had revolutionised ideas. In June 1909, though few at that time agreed with his view that "natural selection has been the main but not the exclusive means of modification", he was honoured by more than 400 officials and scientists from across the world who met in Cambridge to commemorate his centenary and the fiftieth anniversary of On the Origin of Species.[160] Around the beginning of the 20th century, a period that has been called "the eclipse of Darwinism", scientists proposed various alternative evolutionary mechanisms, which eventually proved untenable. Ronald Fisher, an English statistician, finally united Mendelian genetics with natural selection, in the period between 1918 and his 1930 book The Genetical Theory of Natural Selection.[161] He gave the theory a mathematical footing and brought broad scientific consensus that natural selection was the basic mechanism of evolution, thus founding the basis for population genetics and the modern evolutionary synthesis, with J.B.S. Haldane and Sewall Wright, which set the frame of reference for modern debates and refinements of the theory.[10]		During Darwin's lifetime, many geographical features were given his name. An expanse of water adjoining the Beagle Channel was named Darwin Sound by Robert FitzRoy after Darwin's prompt action, along with two or three of the men, saved them from being marooned on a nearby shore when a collapsing glacier caused a large wave that would have swept away their boats,[162] and the nearby Mount Darwin in the Andes was named in celebration of Darwin's 25th birthday.[163] When the Beagle was surveying Australia in 1839, Darwin's friend John Lort Stokes sighted a natural harbour which the ship's captain Wickham named Port Darwin: a nearby settlement was renamed Darwin in 1911, and it became the capital city of Australia's Northern Territory.[164]		More than 120 species and nine genera have been named after Darwin.[165] In one example, the group of tanagers related to those Darwin found in the Galápagos Islands became popularly known as "Darwin's finches" in 1947, fostering inaccurate legends about their significance to his work.[166]		Darwin's work has continued to be celebrated by numerous publications and events. The Linnean Society of London has commemorated Darwin's achievements by the award of the Darwin–Wallace Medal since 1908. Darwin Day has become an annual celebration, and in 2009 worldwide events were arranged for the bicentenary of Darwin's birth and the 150th anniversary of the publication of On the Origin of Species.[167]		Darwin has been commemorated in the UK, with his portrait printed on the reverse of £10 banknotes printed along with a hummingbird and HMS Beagle, issued by the Bank of England.[168]		A life-size seated statue of Darwin can be seen in the main hall of the Natural History Museum in London.[169]		A seated statue of Darwin, unveiled 1897, stands in front of Shrewsbury Library, the building that used to house Shrewsbury School, which Darwin attended as a boy. Another statue of Darwin as a young man is situated in the grounds of Christ's College, Cambridge.		Darwin College, a postgraduate college at Cambridge University, is named after the Darwin family.[170]		The Darwins had ten children: two died in infancy, and Annie's death at the age of ten had a devastating effect on her parents. Charles was a devoted father and uncommonly attentive to his children.[13] Whenever they fell ill, he feared that they might have inherited weaknesses from inbreeding due to the close family ties he shared with his wife and cousin, Emma Wedgwood. He examined this topic in his writings, contrasting it with the advantages of crossing amongst many organisms.[171] Despite his fears, most of the surviving children and many of their descendants went on to have distinguished careers (see Darwin-Wedgwood family).[172]		Of his surviving children, George, Francis and Horace became Fellows of the Royal Society,[173] distinguished as astronomer,[174] botanist and civil engineer, respectively. All three were knighted.[175] Another son, Leonard, went on to be a soldier, politician, economist, eugenicist and mentor of the statistician and evolutionary biologist Ronald Fisher.[176]		Darwin's family tradition was nonconformist Unitarianism, while his father and grandfather were freethinkers, and his baptism and boarding school were Church of England.[23] When going to Cambridge to become an Anglican clergyman, he did not doubt the literal truth of the Bible.[29] He learned John Herschel's science which, like William Paley's natural theology, sought explanations in laws of nature rather than miracles and saw adaptation of species as evidence of design.[31][32] On board the Beagle, Darwin was quite orthodox and would quote the Bible as an authority on morality.[178] He looked for "centres of creation" to explain distribution,[55] and related the antlion found near kangaroos to distinct "periods of Creation".[57]		By his return, he was critical of the Bible as history, and wondered why all religions should not be equally valid.[178] In the next few years, while intensively speculating on geology and the transmutation of species, he gave much thought to religion and openly discussed this with his wife Emma, whose beliefs also came from intensive study and questioning.[92] The theodicy of Paley and Thomas Malthus vindicated evils such as starvation as a result of a benevolent creator's laws, which had an overall good effect. To Darwin, natural selection produced the good of adaptation but removed the need for design,[179] and he could not see the work of an omnipotent deity in all the pain and suffering, such as the ichneumon wasp paralysing caterpillars as live food for its eggs.[142] He still viewed organisms as perfectly adapted, and On the Origin of Species reflects theological views. Though he thought of religion as a tribal survival strategy, Darwin was reluctant to give up the idea of God as an ultimate lawgiver. He was increasingly troubled by the problem of evil.[180][181]		Darwin remained close friends with the vicar of Downe, John Brodie Innes, and continued to play a leading part in the parish work of the church,[182] but from around 1849 would go for a walk on Sundays while his family attended church.[177] He considered it "absurd to doubt that a man might be an ardent theist and an evolutionist"[183][184] and, though reticent about his religious views, in 1879 he wrote that "I have never been an atheist in the sense of denying the existence of a God. – I think that generally ... an agnostic would be the most correct description of my state of mind".[92][183]		The "Lady Hope Story", published in 1915, claimed that Darwin had reverted to Christianity on his sickbed. The claims were repudiated by Darwin's children and have been dismissed as false by historians.[185]		Darwin's views on social and political issues reflected his time and social position. He grew up in a family of Whig reformers who, like his uncle Josiah Wedgwood, supported electoral reform and the emancipation of slaves. Darwin was passionately opposed to slavery, while seeing no problem with the working conditions of English factory workers or servants. His taxidermy lessons in 1826 from the freed slave John Edmonstone, who he long recalled as "a very pleasant and intelligent man", reinforced his belief that black people shared the same feelings, and could be as intelligent as people of other races. He took the same attitude to native people he met on the Beagle voyage.[186] These attitudes were not unusual in Britain in the 1820s, much as it shocked visiting Americans. British society became more racist in mid century,[24] but Darwin remained strongly against slavery, against "ranking the so-called races of man as distinct species", and against ill-treatment of native people.[187][VII] He valued European civilisation and saw colonisation as spreading its benefits, with the sad but inevitable effect that savage peoples who did not become civilised faced extinction. Darwin's theories presented this as natural, and were cited to promote policies that went against his humanitarian principles.[188]		He thought men's eminence over women was the outcome of sexual selection, a view disputed by Antoinette Brown Blackwell in The Sexes Throughout Nature.[189]		Darwin was intrigued by his half-cousin Francis Galton's argument, introduced in 1865, that statistical analysis of heredity showed that moral and mental human traits could be inherited, and principles of animal breeding could apply to humans. In The Descent of Man, Darwin noted that aiding the weak to survive and have families could lose the benefits of natural selection, but cautioned that withholding such aid would endanger the instinct of sympathy, "the noblest part of our nature", and factors such as education could be more important. When Galton suggested that publishing research could encourage intermarriage within a "caste" of "those who are naturally gifted", Darwin foresaw practical difficulties, and thought it "the sole feasible, yet I fear utopian, plan of procedure in improving the human race", preferring to simply publicise the importance of inheritance and leave decisions to individuals.[190] Francis Galton named this field of study "eugenics" in 1883.[VIII]		Darwin's fame and popularity led to his name being associated with ideas and movements that, at times, had only an indirect relation to his writings, and sometimes went directly against his express comments.		Thomas Malthus had argued that population growth beyond resources was ordained by God to get humans to work productively and show restraint in getting families, this was used in the 1830s to justify workhouses and laissez-faire economics.[191] Evolution was by then seen as having social implications, and Herbert Spencer's 1851 book Social Statics based ideas of human freedom and individual liberties on his Lamarckian evolutionary theory.[192]		Soon after the Origin was published in 1859, critics derided his description of a struggle for existence as a Malthusian justification for the English industrial capitalism of the time. The term Darwinism was used for the evolutionary ideas of others, including Spencer's "survival of the fittest" as free-market progress, and Ernst Haeckel's racist ideas of human development. Writers used natural selection to argue for various, often contradictory, ideologies such as laissez-faire dog-eat dog capitalism, racism, warfare, colonialism and imperialism. However, Darwin's holistic view of nature included "dependence of one being on another"; thus pacifists, socialists, liberal social reformers and anarchists such as Peter Kropotkin stressed the value of co-operation over struggle within a species.[193] Darwin himself insisted that social policy should not simply be guided by concepts of struggle and selection in nature.[194]		After the 1880s, a eugenics movement developed on ideas of biological inheritance, and for scientific justification of their ideas appealed to some concepts of Darwinism. In Britain, most shared Darwin's cautious views on voluntary improvement and sought to encourage those with good traits in "positive eugenics". During the "Eclipse of Darwinism", a scientific foundation for eugenics was provided by Mendelian genetics. Negative eugenics to remove the "feebleminded" were popular in America, Canada and Australia, and eugenics in the United States introduced compulsory sterilization laws, followed by several other countries. Subsequently, Nazi eugenics brought the field into disrepute.[VIII]		The term "Social Darwinism" was used infrequently from around the 1890s, but became popular as a derogatory term in the 1940s when used by Richard Hofstadter to attack the laissez-faire conservatism of those like William Graham Sumner who opposed reform and socialism. Since then, it has been used as a term of abuse by those opposed to what they think are the moral consequences of evolution.[195][191]		Darwin was a prolific writer. Even without publication of his works on evolution, he would have had a considerable reputation as the author of The Voyage of the Beagle, as a geologist who had published extensively on South America and had solved the puzzle of the formation of coral atolls, and as a biologist who had published the definitive work on barnacles. While On the Origin of Species dominates perceptions of his work, The Descent of Man and The Expression of the Emotions in Man and Animals had considerable impact, and his books on plants including The Power of Movement in Plants were innovative studies of great importance, as was his final work on The Formation of Vegetable Mould through the Action of Worms.[196][197]		I. ^ Darwin was eminent as a naturalist, geologist, biologist, and author; after working as a physician's assistant and two years as a medical student was educated as a clergyman; and was trained in taxidermy.[198]		II. ^ Robert FitzRoy was to become known after the voyage for biblical literalism, but at this time he had considerable interest in Lyell's ideas, and they met before the voyage when Lyell asked for observations to be made in South America. FitzRoy's diary during the ascent of the River Santa Cruz in Patagonia recorded his opinion that the plains were raised beaches, but on return, newly married to a very religious lady, he recanted these ideas.(Browne 1995, pp. 186, 414)		III. ^ In the section "Morphology" of Chapter XIII of On the Origin of Species, Darwin commented on homologous bone patterns between humans and other mammals, writing: "What can be more curious than that the hand of a man, formed for grasping, that of a mole for digging, the leg of the horse, the paddle of the porpoise, and the wing of the bat, should all be constructed on the same pattern, and should include the same bones, in the same relative positions?"[199] and in the concluding chapter: "The framework of bones being the same in the hand of a man, wing of a bat, fin of the porpoise, and leg of the horse … at once explain themselves on the theory of descent with slow and slight successive modifications."[200]		IV. 1 2 3 In On the Origin of Species Darwin mentioned human origins in his concluding remark that "In the distant future I see open fields for far more important researches. Psychology will be based on a new foundation, that of the necessary acquirement of each mental power and capacity by gradation. Light will be thrown on the origin of man and his history."[128]		In "Chapter VI: Difficulties on Theory" he referred to sexual selection: "I might have adduced for this same purpose the differences between the races of man, which are so strongly marked; I may add that some little light can apparently be thrown on the origin of these differences, chiefly through sexual selection of a particular kind, but without here entering on copious details my reasoning would appear frivolous."[127]		In The Descent of Man of 1871, Darwin discussed the first passage: "During many years I collected notes on the origin or descent of man, without any intention of publishing on the subject, but rather with the determination not to publish, as I thought that I should thus only add to the prejudices against my views. It seemed to me sufficient to indicate, in the first edition of my 'Origin of Species,' that by this work 'light would be thrown on the origin of man and his history;' and this implies that man must be included with other organic beings in any general conclusion respecting his manner of appearance on this earth."[201] In a preface to the 1874 second edition, he added a reference to the second point: "it has been said by several critics, that when I found that many details of structure in man could not be explained through natural selection, I invented sexual selection; I gave, however, a tolerably clear sketch of this principle in the first edition of the 'Origin of Species,' and I there stated that it was applicable to man."[202]		V. ^ See, for example, WILLA volume 4, Charlotte Perkins Gilman and the Feminization of Education by Deborah M. De Simone: "Gilman shared many basic educational ideas with the generation of thinkers who matured during the period of "intellectual chaos" caused by Darwin's Origin of the Species. Marked by the belief that individuals can direct human and social evolution, many progressives came to view education as the panacea for advancing social progress and for solving such problems as urbanisation, poverty, or immigration."		VI. ^ See, for example, the song "A lady fair of lineage high" from Gilbert and Sullivan's Princess Ida, which describes the descent of man (but not woman!) from apes.		VII. ^ Darwin's belief that black people had the same essential humanity as Europeans, and had many mental similarities, was reinforced by the lessons he had from John Edmonstone in 1826.[24] Early in the Beagle voyage, Darwin nearly lost his position on the ship when he criticised FitzRoy's defence and praise of slavery. (Darwin 1958, p. 74) He wrote home about "how steadily the general feeling, as shown at elections, has been rising against Slavery. What a proud thing for England if she is the first European nation which utterly abolishes it! I was told before leaving England that after living in slave countries all my opinions would be altered; the only alteration I am aware of is forming a much higher estimate of the negro character." (Darwin 1887, p. 246) Regarding Fuegians, he "could not have believed how wide was the difference between savage and civilized man: it is greater than between a wild and domesticated animal, inasmuch as in man there is a greater power of improvement", but he knew and liked civilised Fuegians like Jemmy Button: "It seems yet wonderful to me, when I think over all his many good qualities, that he should have been of the same race, and doubtless partaken of the same character, with the miserable, degraded savages whom we first met here."(Darwin 1845, pp. 205, 207–208)		In the Descent of Man, he mentioned the similarity of Fuegians' and Edmonstone's minds to Europeans' when arguing against "ranking the so-called races of man as distinct species".[203]		He rejected the ill-treatment of native people, and for example wrote of massacres of Patagonian men, women, and children, "Every one here is fully convinced that this is the most just war, because it is against barbarians. Who would believe in this age that such atrocities could be committed in a Christian civilized country?"(Darwin 1845, p. 102)		VIII. 1 2 Geneticists studied human heredity as Mendelian inheritance, while eugenics movements sought to manage society, with a focus on social class in the United Kingdom, and on disability and ethnicity in the United States, leading to geneticists seeing this as impractical pseudoscience. A shift from voluntary arrangements to "negative" eugenics included compulsory sterilisation laws in the United States, copied by Nazi Germany as the basis for Nazi eugenics based on virulent racism and "racial hygiene". (Thurtle, Phillip (17 December 1996). "the creation of genetic identity". SEHR. 5 (Supplement: Cultural and Technological Incubations of Fascism). Retrieved 11 November 2008. Edwards, A. W. F. (1 April 2000). "The Genetical Theory of Natural Selection". Genetics. 154 (April 2000). pp. 1419–1426. PMC 1461012 . PMID 10747041. Retrieved 11 November 2008.  Wilkins, John. "Evolving Thoughts: Darwin and the Holocaust 3: eugenics". Archived from the original on 5 December 2008. Retrieved 11 November 2008. )		
The early modern period of modern history follows the late Middle Ages of the post-classical era. Although the chronological limits of the period are open to debate, the timeframe spans the period after the late portion of the post-classical age (c. 1500), known as the Middle Ages, through the beginning of the Age of Revolutions (c. 1800) and is variously demarcated by historians as beginning with the Fall of Constantinople in 1453, with the Renaissance period, and with the Age of Discovery (especially with the voyages of Christopher Columbus beginning in 1492, but also with Vasco da Gama's discovery of the sea route to the East in 1498), and ending around the French Revolution in 1789.		Historians in recent decades have argued that from a worldwide standpoint, the most important feature of the early modern period was its globalizing character.[1] The period witnessed the exploration and colonization of the Americas and the rise of sustained contacts between previously isolated parts of the globe. The historical powers became involved in global trade, as the exchange of goods, plants, animals, and food crops extended to the Old World and the New World. The Columbian Exchange greatly affected the human environment.		New economies and institutions emerged, becoming more sophisticated and globally articulated over the course of the early modern period. This process began in the medieval North Italian city-states, particularly Genoa, Venice, and Milan. The early modern period also included the rise of the dominance of the economic theory of mercantilism. The European colonization of the Americas, Asia, and Africa occurred during the 15th to 19th centuries, and spread Christianity around the world.		The early modern trends in various regions of the world represented a shift away from medieval modes of organization, politically and economically. Feudalism declined in Europe, while the period also included the Protestant Reformation, the disastrous Thirty Years' War, the Commercial Revolution, the European colonization of the Americas, and the Golden Age of Piracy.		By the 16th century the economy under the Ming Dynasty was stimulated by trade with the Portuguese, the Spanish, and the Dutch, while Japan engaged in the Nanban trade after the arrival of the first European Portuguese during the Azuchi-Momoyama period.		Other notable trends of the early modern period include the development of experimental science, accelerated travel due to improvements in mapping and ship design, increasingly rapid technological progress, secularized civic politics, and the emergence of nation states. Historians typically date the end of the early modern period when the French Revolution of the 1790s began the "modern" period.[2]		In 16th-century China, the Ming Dynasty's economy was stimulated by maritime trade with the Portuguese, Spanish and Dutch Empires. China became involved in a new global trade of goods, plants, animals and crops. Trade with Early Modern Europe and Japan brought in massive amounts of silver, which then replaced copper and paper banknotes as the common medium of exchange in China.		During the last decades of the Ming dynasty, the flow of silver into China was greatly diminished, undermining state revenues and the entire Chinese economy. The damage to the economy was compounded by the effects on agriculture of the incipient Little Ice Age, natural calamities, crop failure and sudden epidemics. The ensuing breakdown of authority and people's livelihoods allowed rebel leaders, such as Li Zicheng, to challenge Ming authority.		The Ming Dynasty fell around 1644 to the Qing Dynasty, the last ruling dynasty of China, ruling from 1644 to 1912 (with a brief, abortive restoration in 1917). During its reign, the Qing Dynasty became highly integrated with Chinese culture.		Following contact with the Portuguese on Tanegashima Isle in 1543, the Japanese adopted several of the technologies and cultural practices of their visitors, whether in the military area (the arquebus, European-style cuirasses, European ships), religion (Christianity), decorative art, language (integration to Japanese of a Western vocabulary) and culinary: the Portuguese introduced tempura and valuable refined sugar.		The Azuchi-Momoyama period saw the political unification that preceded the establishment of the Tokugawa shogunate. Although a start date of 1573 is often given, in more broad terms, the period begins with Oda Nobunaga's entry into Kyoto in 1568, when he led his army to the imperial capital in order to install Ashikaga Yoshiaki as the 15th, and ultimately final, shogun of the Ashikaga shogunate, and it lasts until the coming to power of Tokugawa Ieyasu after his victory over supporters of the Toyotomi clan at the Battle of Sekigahara in 1600.[3]		The Edo period from 1600 to 1868 characterized early modern Japan. The Tokugawa shogunate was a feudal regime of Japan established by Tokugawa Ieyasu and ruled by the shoguns of the Tokugawa family. The period gets its name from the capital city, Edo, now called Tokyo. The Tokugawa shogunate ruled from Edo Castle from 1603 until 1868, when it was abolished during the Meiji Restoration in the late Edo period (often called the Late Tokugawa shogunate).		In 1392, General Yi Seong-gye established the Joseon Dynasty (1392–1910) with a largely bloodless coup. Joseon experienced advances in science and culture. King Sejong the Great (1418–1450) promulgated hangul, the Korean alphabet. The period saw various other cultural and technological advances as well as the dominance of neo-Confucianism over the entirety of Korea.		During the late 16th and early 17th centuries, invasions by the neighboring Japanese and northern Manchus nearly overran the Korean peninsula.		After invasions from Manchuria, Joseon experienced nearly 200 years of peace. However, whatever power the kingdom recovered during its isolation further waned as the 18th century came to a close, and Korea was faced with internal strife, power struggles, international pressure and rebellions at home. The Joseon Dynasty declined rapidly in the late 19th century.		On the Indian subcontinent, the Lodhi Dynasty ruled over the Delhi Sultanate during its last phase. The dynasty founded by Bhalul Lodhi ruled from 1451 to 1526. The dynasty's last ruler, Ibrahim Lodhi, was defeated and killed by Babur in the first Battle of Panipat.		The Vijayanagara Empire was based in the Deccan Plateau, but its power was diminished after a major military defeat in 1565 by the Deccan sultanates. The empire is named after its capital city of Vijayanagara.		The rise of the Great Mughal Empire is usually dated from 1526, around the end of the Middle Ages. It was an Islamic Persianate[4] imperial power that ruled most of the area as Hindustan by the late 17th and the early 18th centuries.[5] The empire dominated South and Southwestern Asia.		At the start of the modern era, the Spice Route between India and China crossed Majapahit,[6] an archipelagic empire based on the island of Java. It was the last of the major Hindu empires of Maritime Southeast Asia and is considered one of the greatest states in Indonesian history.[6] Its influence extended to states in Sumatra, the Malay Peninsula, Borneo and eastern Indonesia, but the effectiveness of the influence is the subject of debate.[7] Majapahit found itself unable to control the rising power of the Sultanate of Malacca, which grew to stretch from Muslim Malay settlements of Bukit (Phuket), Setol (Satun), Pantai ni (Pattani) bordering Ayutthaya Kingdom of Siam (Thailand) in the north to Sumatra in the southwest.[citation needed] The Portuguese invaded its capital in 1511 and in 1528 the Sultanate of Johor was established by a Malaccan prince to succeed Malacca.[citation needed]		During the early modern era, the Ottoman state enjoyed an expansion and consolidation of power, leading to a Pax Ottomana. This was perhaps the golden age of the Ottoman Empire. The Ottomans expanded southwest into North Africa while battling with the re-emergent Persian Shi'a Safavid Empire to the east.		In the Saracen sphere, the Ottomans seized Egypt in 1517 and established the regencies of Algeria, Tunisia, and Tripoli (between 1519 and 1551), Morocco remaining an independent Arabized Berber state under the Sharifan dynasty.		In the Ethiopian Highlands, the Solomonic dynasty established itself in the 13th century. Claiming direct descent from the old Axumite royal house, the Solomonic ruled the region well into modern history. In the 16th century, Shewa and the rest of Abyssinia were conquered by the forces of Ahmed Gurey of the Adal Sultanate to the northwest. The conquest of the area by the Oromo ended in the contraction of both Adal and Abyssinia, changing regional dynamics for centuries to come.		The Ajuran Empire, which was one of the largest and strongest empires in the Horn of Africa, began to decline in the 17th century, and several powerful successor states came to prominence. The Geledi Sultanate, established by Ibrahim Adeer, was a notable successor of the Ajuran Sultanate. The Sultanate reached its apex under the successive reigns of Sultan Yusuf Mahamud Ibrahim (reigned 1798 to 1848), who successfully consolidated Geledi power during the Bardera wars, and Sultan Ahmed Yusuf, who forced regional powers such as the Omani Empire to pay tribute. The Majeerteen Sultanate was a Somali Sultanate in the Horn of Africa. Ruled by King Osman Mahamuud during its golden age, it controlled much of northern and central Somalia in the 19th and early 20th centuries. The polity had all of the organs of an integrated modern state and maintained a robust trading network. Along with the Sultanate of Hobyo ruled by Sultan Yusuf Ali Kenadid, the Majeerteen Sultanate was eventually annexed into Italian Somaliland in the early 20th century, following the military Campaign of the Sultanates.		The Songhai Empire took control of the trans-Saharan trade at the beginning of the modern era. It seized Timbuktu in 1468 and Jenne in 1473, building the regime on trade revenues and the cooperation of Muslim merchants. The empire eventually made Islam the official religion, built mosques, and brought Muslim scholars to Gao.[8]		Around the beginning of the modern era, the Benin Empire was an independent trading power in West Africa, blocking the access of other inland nations to the coastal ports. Benin may have housed 100,000 inhabitants at its height, spreading over twenty-five square kilometres, enclosed by three concentric rings of earthworks. By the late 15th century Benin was in contact with Portugal. At its apogee in the 16th and 17th centuries, Benin encompassed parts of southeastern Yorubaland and the western Igbo.		The Safavid Empire was a great Shia Persianate empire after the Islamic conquest of Persia and established of Islam, marking an important point in the history of Islam in the east. The Safavid dynasty was founded about 1501. From their base in Ardabil, the Safavids established control over all of Persia and reasserted the Iranian identity of the region, thus becoming the first native dynasty since the Sassanids to establish a unified Iranian state. Problematic for the Safavids was the powerful Ottoman Empire. The Ottomans, a Sunni dynasty, fought several campaigns against the Safavids.		What fueled the growth of Safavid economy was its position between the burgeoning civilizations of Europe to its west and Islamic Central Asia to its east and north. The Silk Road, which led from Europe to East Asia, revived in the 16th century. Leaders also supported direct sea trade with Europe, particularly England and The Netherlands, which sought Persian carpet, silk, and textiles. Other exports were horses, goat hair, pearls, and an inedible bitter almond hadam-talka used as a spice in India. The main imports were spice, textiles (woolens from Europe, cotton from Gujarat), metals, coffee, and sugar. Despite their demise in 1722, the Safavids left their mark by establishing and spreading Shi'a Islam in major parts of the Caucasus and West Asia.		In the 16th to early 18th centuries, Central Asia was under the rule of Uzbeks, and the far eastern portions were ruled by the local Pashtuns. Between the 15th and 16th centuries, various nomadic tribes arrived from the steppes, including the Kipchaks, Naymans, Kanglis, Kungrats, and Manġits. These groups were led by Muhammad Shaybani, who was the Khan of the Uzbeks.		The lineage of the Afghan Pashtuns stretches back to the Hotaki dynasty.[9] Following Muslim Arab and Turkic conquests, Pashtun ghazis (warriors for the faith) invaded and conquered much of northern India during the Lodhi dynasty and Suri dynasty. Pashtun forces also invaded Persia, and the opposing forces were defeated in the Battle of Gulnabad. The Pashtuns later formed the Durrani Empire.		The beginning of the early modern period is not clear-cut, but is generally accepted as in the late 15th century or early 16th century. Significant dates in this transitional phase from medieval to early modern Europe can be noted:		This era in Western Europe is referred to as the early modern European period and includes the Protestant Reformation, the European wars of religion, the Age of Discovery and the beginning of European colonialism, the rise of strong centralized governments, the beginnings of recognizable nation-states that are the direct antecedents of today's states, the Age of Enlightenment, and from the associated scientific advances the first phase of the Industrial Revolution. The emergence of cultural and political dominance of the Western world during this period is known as the Great Divergence.		The early modern period is taken to end with the French Revolution, the Napoleonic Wars, and the dissolution of the Holy Roman Empire at the Congress of Vienna. At the end of the early modern period, the British and Russian empires had emerged as world powers from the multipolar contest of colonial empires, while the three great Asian empires of the early modern period, Ottoman Turkey, Mughal India and Qing China, all entered a period of stagnation or decline.		The expression "early modern" is at times incorrectly used as a substitute for the term Renaissance. However, "Renaissance" is properly used in relation to a diverse series of cultural developments that occurred over several hundred years in many different parts of Europe — especially central and northern Italy — and it spans the transition from late medieval civilization to the opening of the early modern period. In the visual arts and architecture, the term 'early modern' is not a common designation as the Renaissance period is clearly distinct from what came later. Only in the study of literature is the early modern period a standard designation. European music of the period is generally divided between Renaissance and Baroque. Similarly, philosophy is divided between Renaissance philosophy and the Enlightenment. In other fields, there is far more continuity through the period such as warfare and science.		In the early modern period, the Holy Roman Empire was a union of territories in Central Europe under a Holy Roman Emperor. The first emperor of the Holy Roman Empire was Otto I. The last was Francis II, who abdicated and dissolved the Empire in 1806 during the Napoleonic Wars. Despite its name, for much of its history the Empire did not include Rome within its borders.		The Renaissance[10] was a cultural movement that spanned roughly the 14th to the 17th century, beginning in Italy in the Late Middle Ages and later spreading to the rest of Europe. The term is also used more loosely to refer to the historic era, but since the changes of the Renaissance were not uniform across Europe, this is a general use of the term. As a cultural movement, it encompassed a rebellion of learning based on classical sources, the development of linear perspective in painting, and gradual but widespread educational reform.		Johannes Gutenberg is credited as the first European to use movable type printing, around 1439, and as the global inventor of the mechanical printing press. Nicolaus Copernicus formulated a comprehensive heliocentric cosmology (1543), which displaced the Earth from the center of the universe.[11] His book, De revolutionibus orbium coelestium (On the Revolutions of the Celestial Spheres) began modern astronomy and sparked the Scientific Revolution. Another notable individual was Machiavelli, an Italian political philosopher, considered a founder of modern political science. Machiavelli is most famous for a short political treatise, The Prince, a work of realist political theory.		Among the notable royalty of the time, Charles the Bold, known as Charles the Bold (or Rash) to his enemies,[12] he was the last Valois Duke of Burgundy, and his early death was a pivotal, if under-recognized, moment in European history. Charles has often been regarded as the last representative of the feudal spirit — a man who possessed no other quality than a blind bravery. Upon his death, Charles left an unmarried nineteen-year-old daughter, Mary of Burgundy, as his heir. Her marriage would have enormous implications for the political balance of Europe. The Habsburg Emperor secured the match for his son, the future Maximilian I, Holy Roman Emperor, with the aid of Mary's stepmother, Margaret. In 1477, the territory of the Duchy of Burgundy was annexed by France. In the same year, Mary married Maximilian, Archduke of Austria, giving the Habsburgs control of the remainder of the Burgundian Inheritance.		Claude de Lorraine was the first Duke of Guise, from 1528 to his death. Claude distinguished himself at the battle of Marignano (1515), and was long in recovering from the twenty-two wounds he received in the battle. In 1521, he fought at Fuenterrabia, and Louise of Savoy ascribed the capture of the place to his efforts. In 1523 he became governor of Champagne and Burgundy, after defeating at Neufchâteau the imperial troops who had invaded this province. In 1525 he destroyed the Anabaptist peasant army, which was overrunning Lorraine, at Lupstein, near Saverne (Zabern). On the return of Francis I from captivity in 1528, Claude was made Duke of Guise in the peerage of France, though up to this time only princes of the royal house had held the title of duke and peer of France. The Guises, as cadets of the sovereign house of Lorraine and descendants of the house of Anjou, claimed precedence of the Bourbon princes of Condé and Conti.		The 3rd Duke of Alba was a nobleman of importance in the early modern period, nicknamed the "Iron Duke" by the Protestants of the Low Countries because of his harsh rule and cruelty. Tales of atrocities committed during his military operations in Flanders became part of Dutch and English folklore, forming a central component of the Black Legend.		In England, Henry VIII was the King of England and a significant figure in the history of the English monarchy. Although in the greater part of his reign he brutally suppressed the influence of the Protestant Reformation in England,[13] a movement having some roots with John Wycliffe in the 14th century, he is more popularly known for his political struggles with Rome. These struggles ultimately led to the separation of the Church of England from papal authority, the Dissolution of the Monasteries, and establishing himself as the Supreme Head of the Church of England. Though Henry reportedly became a Protestant on his death-bed, he advocated Catholic ceremony and doctrine throughout his life. Royal support for the English Reformation began with his heirs, the devout Edward VI and the renowned Elizabeth I, whilst daughter Mary I temporarily reinstated papal authority over England. Henry also oversaw the legal union of England and Wales with the Laws in Wales Acts 1535–1542. He is also noted for his six wives, two of whom were beheaded.		Christianity was challenged at the beginning of the modern period with the fall of Constantinople in 1453 and later by various movements to reform the church (including Lutheran, Zwinglian, and Calvinist), followed by the Counter Reformation.		The Hussite Crusades involved the military actions against and amongst the followers of Jan Hus in Bohemia ending ultimately with the Battle of Grotniki. Also known as the Hussite Wars, they were arguably the first European war in which hand-held gunpowder weapons such as muskets made a decisive contribution. The Taborite faction of the Hussite warriors were basically infantry, and their many defeats of larger armies with heavily armored knights helped effect the infantry revolution. In totality, the Hussite Crusades were inconclusive.		The last crusade, the Crusade of 1456, was organized to counter the expanding Ottoman Empire and lift the Siege of Belgrade, and was led by John Hunyadi and Giovanni da Capistrano. The siege eventually escalated into a major battle, during which Hunyadi led a sudden counterattack that overran the Turkish camp, ultimately compelling the wounded Sultan Mehmet II to lift the siege and retreat. The siege of Belgrade has been characterized as having "decided the fate of Christendom".[14] The noon bell ordered by Pope Callixtus III commemorates the victory throughout the Christian world to this day.		Nearly a hundred years later, the Peace of Augsburg officially ended the idea that all Christians could be united under one church. The principle of cuius regio, eius religio ("whose the region is, [it shall have] his religion") established the religious, political and geographic divisions of Christianity, and this was established in international law with the Treaty of Westphalia in 1648, which legally ended the concept of a single Christian hegemony, i.e. the "One, Holy, Catholic, and Apostolic Church" of the Nicene Creed. Each government determined the religion of their own state. Christians living in states where their denomination was not the established church were guaranteed the right to practice their faith in public during allotted hours and in private at their will. With the Treaty of Westphalia, the Wars of Religion came to an end, and in the Treaty of Utrecht of 1713 the concept of the sovereign national state was born. The Corpus Christianum has since existed with the modern idea of a tolerant and diverse society consisting of many different communities.		The modern Inquisition refers to any one of several institutions charged with trying and convicting heretics (or other offenders against canon law) within the Catholic Church. In the modern era, the first manifestation was the Spanish Inquisition of 1478 to 1834.[15] The Inquisition prosecuted individuals accused of a wide array of crimes related to heresy, including sorcery,[16] blasphemy, Judaizing and witchcraft, as well for censorship of printed literature. Because of its objective — combating heresy — the Inquisition had jurisdiction only over baptized members of the Church (which, however, encompassed the vast majority of the population in Catholic countries). Secular courts could still try non-Christians for blasphemy (most of the witch trials went through secular courts).		The Protestant Reformation and rise of modernity in the early 16th century entailed the start of a series of changes in the Corpus Christianum. Martin Luther challenged the Catholic Church with his Ninety-Five Theses, generally accepted as the beginning of the Reformation, a Christian reform movement in Europe, though precursors such as Jan Hus predate him. The Protestant movement of the 16th century occurred under the protection of the Electorate of Saxony, an independent hereditary electorate of the Holy Roman Empire. The Elector Frederick III established a university at Wittenberg in 1502. The Augustinian monk Martin Luther became professor of philosophy there in 1508. At the same time, he became one of the preachers at the castle church of Wittenberg.		On 31 October 1517, Luther posted his Ninety-Five Theses on the door of the All Saints' Church, which served as a notice board for university-related announcements.[17] These were points for debate that criticized the Church and the Pope. The most controversial points centered on the practice of selling indulgences (especially by Johann Tetzel) and the Church's policy on purgatory. The reform movement soon split along certain doctrinal lines. Religious disagreements between various leading figures led to the emergence of rival Protestant churches. The most important denominations to emerge directly from the Reformation were the Lutherans, and the Reformed/Calvinists/Presbyterians. The process of reform had decidedly different causes and effects in other countries. In England, where it gave rise to Anglicanism, the period became known as the English Reformation. Subsequent Protestant denominations generally trace their roots back to the initial reforming movements.		The Diet of Worms in 1521, presided by Emperor Charles V, declared Martin Luther a heretic and an outlaw (although Charles V was more preoccupied with maintaining his vast empire than with arresting Luther). As a result of Charles V's distractions in East Europe and in Spain, he agreed through the Diet of Speyer in 1526 to allow German princes to effectively decide themselves whether to enforce the Edict of Worms or not, for the time being. After returning to the empire, Charles V attended the Diet of Augsburg in 1530 to order all Protestants in the empire to revert to Catholicism. In response, the Protestant territories in and around Germany formed the Schmalkaldic League to fight against the Catholic Holy Roman Empire. Charles V left again to handle the advance of the Ottoman Turks. He returned in 1547 to launch a military campaign against the Schmalkaldic League and to issue an imperial law requiring all Protestants to return to Catholic practices (with a few superficial concessions to Protestant practices). Warfare ended when Charles V relented in the Peace of Passau (1552) and in the Peace of Augsburg (1555), which formalized the law that the rulers of a land decide its religion.		Of the late Inquisitions in the modern era, there were two different manifestations:[15]		This Portuguese inquisition was a local analogue of the more famous Spanish Inquisition. The Roman Inquisition covered most of the Italian peninsula as well as Malta and also existed in isolated pockets of papal jurisdiction in other parts of Europe, including Avignon.		The Catholic Reformation began in 1545 when the Council of Trent was called in reaction to the Protestant Rebellion. The idea was to reform the state of worldliness and disarray that had befallen some of the clergy of the Church, while reaffirming the spiritual authority of the Catholic Church and its position as the sole true Church of Christ on Earth. The effort sought to prevent further damage to the Church and her faithful at the hands of the newly formed Protestant denominations.		In development of the Third Rome ideas, the Grand Duke Ivan IV (the "Awesome"[18] or "the Terrible") was officially crowned the first Tsar ("Caesar") of Russia in 1547. The Tsar promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor) and introduced local self-management into the rural regions.[19][20] During his long reign, Ivan IV nearly doubled the already large Russian territory by annexing the three Tatar khanates (parts of disintegrated Golden Horde): Kazan and Astrakhan along the Volga River, and Sibirean Khanate in South Western Siberia. Thus by the end of the 16th century Russia was transformed into a multiethnic, multiconfessional and transcontinental state.		The Age of Discovery was a period from the early 15th century and continuing into the early 17th century, during which European ships traveled around the world to search for new trading routes and partners to feed burgeoning capitalism in Europe. They also were in search of trading goods such as gold, silver and spices. In the process, Europeans encountered peoples and mapped lands previously unknown to them. This factor in the early European modern period was a globalizing character; the 'discovery' of the Americas and the rise of sustained contacts between previously isolated parts of the globe was an important historical event.		The search for new routes was based on the fact that the Silk Road was controlled by the Ottoman Empire, which was an impediment to European commercial interests, and other Eastern trade routes were not available to the Europeans due to Muslim control. The ability to outflank the Muslim states of North Africa was seen as crucial to European survival. At the same time, the Iberians learnt much from their Arab neighbors. The northwestern region of Eurasia has a very long coastline, and has arguably been more influenced by its maritime history than any other continent. Europe is uniquely situated between several navigable seas, and intersected by navigable rivers running into them in a way that greatly facilitated the influence of maritime traffic and commerce. In the maritime history of Europe, the carrack and caravel both incorporated the lateen sail that made ships far more maneuverable. By translating the Arab versions of lost ancient Greek geographical works into Latin, European navigators acquired a deeper knowledge of the shape of Africa and Asia.		Mercantilism was the dominant school of economic thought throughout the early modern period (from the 16th to the 18th century). This led to some of the first instances of significant government intervention and control over the economy, and it was during this period that much of the modern capitalist system was established. Internationally, mercantilism encouraged the many European wars of the period and fueled European imperialism. Belief in mercantilism began to fade in the late 18th century, as the arguments of Adam Smith and the other classical economists won out.		The Commercial Revolution was a period of economic expansion, colonialism, and mercantilism that lasted from approximately the 16th century until the early 18th century. Beginning with the Crusades, Europeans rediscovered spices, silks, and other commodities rare in Europe. This development created a new desire for trade, which expanded in the second half of the Middle Ages. European nations, through voyages of discovery, were looking for new trade routes in the fifteenth and sixteenth centuries, which allowed the European powers to build vast, new international trade networks. Nations also sought new sources of wealth. To deal with this new-found wealth, new economic theories and practices were created. Because of competing national interest, nations had the desire for increased world power through their colonial empires. The Commercial Revolution is marked by an increase in general commerce, and in the growth of non-manufacturing pursuits, such as banking, insurance, and investing.		In the Old World, the most desired trading goods were gold, silver, and spices. Western Europeans used the compass, new sailing ship technologies, new maps, and advances in astronomy to seek a viable trade route to Asia for valuable spices that Mediterranean powers could not contest.		Gold fueled European exploration of the Americas. Explorers reported Native Americans in Central America, Peru, Ecuador and Colombia were to have had large amounts.		Silver, valued as a precious metal, has been used to make expensive ornaments, fine jewelry, high-value tableware and utensils (silverware), and currency coins.		Spices were among the most luxurious products, the most common being black pepper, cinnamon (and the cheaper alternative cassia), cumin, nutmeg, ginger and cloves.		In terms of shipping advances, the most important developments were the creation of the carrack and caravel designs in Portugal. These vessels evolved from medieval European designs from the North Sea and both the Christian and Islamic Mediterranean. They were the first ships that could leave the relatively placid and calm Mediterranean, Baltic or North Sea and sail safely on the open Atlantic.		When the carrack and then the caravel were developed in Iberia, European thoughts returned to the fabled East. These explorations have a number of causes. Monetarists believe the main reason the Age of Exploration began was because of a severe shortage of bullion in Europe. The European economy was dependent on gold and silver currency, but low domestic supplies had plunged much of Europe into a recession. Another factor was the centuries-long conflict between the Iberians and the Muslims to the south.		The Golden Age of Piracy is a designation given to one or more outbursts of piracy in the early modern period, spanning from the mid-17th century to the mid-18th century. The buccaneering period covers approximately the late 17th century. The period is characterized by Anglo-French seamen based on Jamaica and Tortuga attacking Spanish colonies and shipping in the Caribbean and eastern Pacific. A sailing route known as the Pirate Round was followed by certain Anglo-American pirates at the turn of the 18th century, associated with long-distance voyages from Bermuda and the Americas to rob Muslim and East India Company targets in the Indian Ocean and Red Sea. The post-Spanish Succession period extending into the early 18th century, when Anglo-American sailors and privateers left unemployed by the end of the War of the Spanish Succession turned en masse to piracy in the Caribbean, the American eastern seaboard, the West African coast, and the Indian Ocean.		The 15th to 18th century period is marked by the first European colonies, the rise of strong centralized governments, and the beginnings of recognizable European nation states that are the direct antecedents of today's states. Although the Renaissance saw revolutions in many intellectual pursuits, as well as social and political upheaval, it is perhaps best known for European artistic developments and the contributions of such polymaths as Leonardo da Vinci and Michelangelo, who inspired the term "Renaissance man".[21][22]		The Baroque period brought the Thirty Years' War in Central Europe, which decimated the population by up to 20%. The treaties in 1648 ended several wars in Europe and established the beginning of sovereign states. The Peace of Westphalia refers to the two peace treaties of Osnabrück and Münster, signed on May 15 and October 24, 1648, respectively, and written in French, that ended both the Thirty Years' War in the Holy Roman Empire (today mostly Germany) and the Eighty Years' War between Spain and the Republic of the Seven United Netherlands. The treaties involved the Holy Roman Emperor, Ferdinand III (Habsburg), the Kingdoms of Spain, France and Sweden, the Netherlands and their respective allies among the princes and the Republican Imperial States of the Holy Roman Empire.		The Peace of Westphalia resulted from the first modern diplomatic congress. Until 1806, the regulations became part of the constitutional laws of the Holy Roman Empire. The Treaty of the Pyrenees, signed in 1659, ended the war between France and Spain and is often considered part of the overall accord.		The Age of Absolutism describes the monarchical power that was unrestrained by any other institutions, such as churches, legislatures, or social elites of the European monarchs during the transition from feudalism to capitalism. Monarchs described as absolute can especially be found in the 17th century through the 19th century. Nations that adopted Absolutism include France, Prussia, and Russia. Nobles tended to trade privileges for allegiance throughout the eighteenth century, so that the interests of the nobility aligned with that of the crown. Absolutism is characterized by the ending of feudal partitioning, consolidation of power with the monarch, rise of state power, unification of the state laws, drastic increase in tax revenue collected by the monarch, and a decrease in the influence of nobility.		For much of the reign of Louis XIV, who was known as the Sun King (French: le Roi Soleil), France stood as the leading power in Europe, engaging in three major wars—the Franco-Dutch War, the War of the League of Augsburg, and the War of the Spanish Succession—and two minor conflicts—the War of Devolution, and the War of the Reunions. Louis believed in the Divine Right of Kings, the theory that the King was crowned by God and accountable to him alone. Consequently, he has long been considered the archetypal absolute monarch. Louis XIV continued the work of his predecessor to create a centralized state, governed from the capital to sweep away the remnants of feudalism that persisted in parts of France. He succeeded in breaking the power of the provincial nobility, much of which had risen in revolt during his minority called the Fronde, and forced many leading nobles to live with him in his lavish Palace of Versailles.		Men who featured prominently in the political and military life of France during this period include Mazarin, Jean-Baptiste Colbert, Turenne, Vauban. French culture likewise flourished during this era, producing a number of figures of great renown, including Molière, Racine, Boileau, La Fontaine, Lully, Le Brun, Rigaud, Louis Le Vau, Jules Hardouin Mansart, Claude Perrault and Le Nôtre.		Before the Age of Revolution, the English Civil War was a series of armed conflicts and political machinations between Parliamentarians and Royalists. The first and second civil wars pitted the supporters of King Charles I against the supporters of the Long Parliament, while the third war saw fighting between supporters of King Charles II and supporters of the Rump Parliament. The Civil War ended with the Parliamentary victory at the Battle of Worcester. The monopoly of the Church of England on Christian worship in England ended with the victors consolidating the established Protestant Ascendancy in Ireland. Constitutionally, the wars established the precedent that an English monarch cannot govern without Parliament's consent. The English Restoration, or simply put as the Restoration, began in 1660 when the English, Scottish and Irish monarchies were all restored under Charles II after the Commonwealth of England that followed the English Civil War. The Glorious Revolution of 1688 establishes modern parliamentary democracy in England.		The War of the Spanish Succession was a war fought between 1701 and 1714, in which several European powers combined to stop a possible unification of the Kingdoms of Spain and France under a single Bourbon monarch, upsetting the European balance of power. It was fought mostly in Europe, but it included Queen Anne's War in North America. The war was marked by the military leadership of notable generals like the duc de Villars, the Jacobite Duke of Berwick, the Duke of Marlborough and Prince Eugene of Savoy.		The Peace of Utrecht established after a series of individual peace treaties signed in the Dutch city of Utrecht concluded between various European states helped end the War of the Spanish Succession. The representatives who met were Louis XIV of France and Philip V of Spain on the one hand, and representatives of Queen Anne of Great Britain, the Duke of Savoy, and the United Provinces on the other. The treaty enregistred the defeat of French ambitions expressed in the wars of Louis XIV and preserved the European system based on the balance of power.[23] The Treaty of Utrecht marked the change from Spanish to British naval supremacy.		The historical phenomenon of colonization in the modern era centres on the British Empire, but the term colonialism is normally used with reference to discontiguous overseas empires rather than contiguous land-based empires, European or otherwise. European colonisation during the 15th to 19th centuries resulted in the spread of Christianity to Sub-Saharan Africa, the Americas, Australia and the Philippines.		Christopher Columbus discovered the Americas in 1492. Subsequently, the major sea powers in Europe sent expeditions to the New World to build trade networks and colonies and to convert the native peoples to Christianity. Pope Alexander VI divided newly discovered lands outside Europe between Spain and Portugal along a north-south meridian 370 leagues west of the Cape Verde islands (off the west coast of Africa). The division was never accepted by the rulers of England or France. (See also the Treaty of Tordesillas, which followed the papal decree.)		What is now called Latin America, a designation first used in the late 19th century,[24] was claimed by Spain and Portugal. The Western Hemisphere, the New World,[25] was divided between the two Iberian powers by the Treaty of Tordesillas in what until the late 16th-century, was an area that could be called "Ibero-America." Spain called its overseas empire there "The Indies," with Portugal calling its territory in South America Brazil, after the dyewood found there. Spain concentrated building its empire where there were large indigenous populations, "Indians," who could be compelled to work and large deposits of precious metals, mainly silver. Both New Spain (colonial Mexico) and Peru fit those criteria and the Spanish crown established viceroyalties to rule those two large areas. As Spanish settlements and the economy grew in size and complexity, the Spanish established viceroyalties in the eighteenth century during administrative reforms Rio de la Plata (southeastern South America) and New Granada (northern South America).		Initially, Portuguese settlements (Brazil) in the coastal northeast were of lesser importance in the larger Portuguese overseas empire, where lucrative commerce and small settlements devoted to trade were established in coastal Africa, India and China. With sparse indigenous populations that could not be coerced to work and no known deposits of precious metals, Portugal sought a high-value, low-bulk export product and found it in sugarcane. Black African slave labour from Portugal's West African possessions was imported to do the grueling agricultural work. As the wealth of the Ibero-America increased, some Western European powers (Dutch, French, British, Danish) sought to duplicate the model in areas that the Iberians had not settled in numbers. They seized Caribbean islands from the Spanish and transferred the model of sugar production on plantations with slave labour and settled in northern areas of North America in what are now the Eastern Seaboard of the United States and Canada.[26]		North America outside the zone of Spanish settlement was a contested area in the 17th century. Spain had founded small settlements in Florida and Georgia but nowhere near the size of those in New Spain or the Caribbean islands. France, The Netherlands, and Great Britain held several colonies in North America and the West Indies from the 17th century, 100 years after the Spanish and Portuguese established permanent colonies. The British colonies in North America were founded between 1607 (Virginia) and 1733 (Georgia). The Dutch explored the east coast of North America and began founding settlements in what they called New Netherland (now New York State.). France colonized what is now Eastern Canada, founding Quebec City in 1608. France's loss in the Seven Years' War resulted in the transfer of New France to Great Britain. The Thirteen Colonies, in lower British North America, rebelled against British rule in 1775, largely by the taxation that Great Britain was imposing on the colonies. The British colonies in Canada remained loyal to the crown, and a provisional government formed by the Thirteen Colonies proclaimed their independence on July 4, 1776 and subsequently became the original 13 United States of America. With the 1783 Treaty of Paris ending the American Revolutionary War, Britain recognised the former Thirteen Colonies' independence.		A recent development in early modern history is the creation of Atlantic World as a category. The term generally encompasses western Europe, West Africa, North and South and America and the Caribbean islands. It seeks to show both local and regional development and the connections between the various geographical regions.		Concerning the development of Eastern philosophies, much of Eastern philosophy had been in an advanced state of development from study in the previous centuries. The various philosophies include Indian philosophy,[27] Chinese philosophy, Iranian philosophy, Japanese philosophy, and Korean philosophy.		The Islamic Golden Age reached its peak in the High Middle Ages, stopped short by the Mongol invasions of the 13th century. The re-establishment of three major Muslim empires by the 16th century (the aforementioned Ottoman Safavid and Mughal Empires) gave rise to a Muslim cultural revival.[clarification needed] The Safavids established Twelver Shi'a Islam as Iran's official religion, thus giving Iran a separate identity from its Sunni neighbors.		The early modern period was initiated by the Protestant Reformation and the collapse of the unity of the medieval Western Church. The theology of Calvinism in particular has been argued as instrumental to the rise of capitalism (The Protestant Ethic and the Spirit of Capitalism).		The Counter-Reformation was a period of Catholic revival in response to the Protestant Reformation during the mid-16th to mid-17th centuries. The Counter-Reformation was a comprehensive effort, involving ecclesiastical or structural reforms as well as a political dimension and spiritual movements.		Such reforms included the foundation of seminaries for the proper training of priests in the spiritual life and the theological traditions of the Church, the reform of religious life by returning orders to their spiritual foundations and new spiritual movements focusing on the devotional life and a personal relationship with Christ, including the Spanish mystics and the French school of spirituality. It also involved political activities that included the Roman Inquisition.		New religious orders were a fundamental part of this trend. Orders such as the Capuchins, Ursulines, Theatines, Discalced Carmelites, the Barnabites, and especially the Jesuits strengthened rural parishes, improved popular piety, helped to curb corruption within the church and set examples that would be a strong impetus for Catholic renewal.		With the adoption of large-scale printing after 1500, Italian Renaissance Humanism spread northward to France, Germany, Holland and England, where it became associated with the Protestant Reformation. In France, pre-eminent Humanist Guillaume Budé (1467–1540) applied the philological methods of Italian Humanism to the study of antique coinage and to legal history, composing a detailed commentary on Justinian's Code. Although a royal absolutist (and not a republican like the early Italian umanisti), Budé was active in civic life, serving as a diplomat for Francis I and helping to found the Collège des Lecteurs Royaux (later the Collège de France). Meanwhile, Marguerite de Navarre, the sister of Francis I, herself a poet, novelist and religious mystic,[28] gathered around her and protected a circle of vernacular poets and writers, including Clément Marot, Pierre de Ronsard and François Rabelais.		The philosophy of 17th-century Europe marks the departure from medieval scholasticism and the often occultist approach of Renaissance philosophy. The period was typified in Europe by the great system-builders, philosophers who presented unified systems of epistemology, metaphysics, logic, and ethics and often politics and the physical sciences as well.		Immanuel Kant classified his predecessors into two schools: the rationalists and the empiricists,[29] The three main rationalists are normally taken to have been René Descartes, Baruch Spinoza, and Gottfried Leibniz.		The mid-17th century saw the first great advances towards modern science, most notably the theory of gravity by Isaac Newton (1643–1727).		He and Baruch Spinoza (1632–1677), John Locke (1632–1704) and Pierre Bayle (1647–1706) were philosophers sparking the Age of Enlightenment in the following century.		The Great Divergence is epitomized by the Age of Enlightenment (or Age of Reason). The Enlightenment, starting in the 1750s, flourished until about 1790–1800 after which the emphasis on reason gave way to Romanticism's emphasis on emotion and a Counter-Enlightenment gained force.		The centre of the Enlightenment was France, where it was based in the salons and culminated in the great Encyclopédie (1751–1772), edited by Denis Diderot (1713–1784) with contributions by hundreds of leading philosophes (intellectuals) such as Voltaire (1694–1778) and Montesquieu (1689–1755). The French Enlightenment was received in Germany, notably fostered by Frederick the Great, the king of Prussia, and gave rise to a flowering of German philosophy, represented foremost by Immanuel Kant.		The French and German developments were further influential in Scottish, Russian, Spanish and Polish philosophy.		In modern history, the end of the early period falls in the late 18th century, as an Age of Revolutions dawns, beginning with those in North America and France. Subsequent important political changes occurred throughout Europe, including upheavals following the Napoleonic Wars, the redrawing of the map of Europe through the Second Treaty of Paris, the rise of new concepts of nationalism and the reorganization in military forces. The end of the early modern period is usually also associated with the Industrial Revolution, which began in Britain in the mid-18th century.		
The knock-knock joke is a "question-and-answer" joke, usually ending with a pun. The teller of the joke calls "Knock, knock!", the other person responds, "Who's there?" and the teller gives a name (such as "Boo"). The other person then responds by asking the caller's surname ("Boo who?"), to which the joke-teller delivers a pun involving the name ("Hey don't cry it's alright!!").[1]		The formula of the joke must be followed strictly. "Knock-knock." "Who's there?" "[Someone or something.]" "[Someone or something] who?" " Punchline." Children in preschool in the USA learn this formula.		Writing in the Oakland Tribune, Merely McEvoy recalled a style of joke from around 1900 where a person would ask a question such as "Do you know Arthur?", the unsuspecting listener responding with "Arthur who?" and the joke teller answering "Arthurmometer!"[1]		A variation of the format in the form of a children's game was described in 1929.[2] In the game of Buff, a child with a stick thumps it on the ground, and the dialogue ensues:		In 1936, the standard knock-knock joke format was used in a newspaper advertisement.[3] That joke was:		A 1936 Associated Press newspaper article said that "What's This?" had given way to "Knock Knock!" as a favorite parlor game.[4] The article also said that "knock knock" seemed to be an outgrowth of making up sentences with difficult words, an old parlor favorite. A popular joke of 1936 was "Knock knock. Who's there? Edward Rex. Edward Rex who? Edward wrecks the Coronation."[5] Fred Allen's December 30, 1936 radio broadcast included a humorous wrapup of the year's least important events, including a supposed interview with the man who "invented a negative craze" on April 1: "Ramrod Dank... the first man to coin a Knock Knock."[6]		"Knock knock" was the catchphrase of music hall performer Wee Georgie Wood, who was recorded in 1936 saying it in a radio play, but he simply used the words as a reference to his surname and did not use it as part of the well-known joke formula.[7] The format was well known in the UK and US in the 1950s and 1960s before falling out of favor. It then enjoyed a renaissance after the jokes became a regular part of the badinage on Rowan & Martin's Laugh-In.[7]		
Philogelos (Ancient Greek: Φιλόγελως, "Love of Laughter") is the oldest existing collection of jokes.		The collection is written in Greek, and the language used indicates that it may have been written in the 4th century AD, according to William Berg, an American classics professor.[1] It is attributed to Hierocles and Philagrius, about whom little is known.[2] Because the celebration of a thousand years of Rome is mentioned in joke 62, the collection perhaps dates from after that event in 248 AD.[3] Although it is the oldest existing collection of jokes, it is known that it was not the oldest collection, because Athenaeus wrote that Philip II of Macedon paid for a social club in Athens to write down its members' jokes, and at the beginning of the 2nd century BC, Plautus twice has a character mentioning books of jokes.[2]		The collection contains 265 jokes categorised into subjects such as teachers and scholars, and eggheads and fools.[4]						In 2008, British TV personality and comedian Jim Bowen tested the material on a modern audience.[5] One of the jokes in Philogelos has been described as "an ancestor of Monty Python's famous Dead Parrot comedy sketch."[1] Comedian Jimmy Carr has said that some of the jokes are "strikingly similar" to modern ones.[6]		
A lightbulb joke is a joke that asks how many people of a certain group are needed to change, replace, or screw in a light bulb. Generally, the punch line answer highlights a stereotype of the target group. There are numerous versions of the lightbulb joke satirizing a wide range of cultures, beliefs and occupations.[1][2]		Early versions of the joke, popular in the late 1960s[3][4] and the 1970s, were used to insult the intelligence of people, especially Poles ("Polish jokes").[5][6] For instance:		Q. How many Polacks does it take to change a light bulb? A. Three—one to hold the light bulb and two to turn the ladder.		Although lightbulb jokes tend to be derogatory in tone (e.g., "How many drummers..." / "Four: one to hold the light bulb and three to drink until the room spins"), the people targeted by them may take pride in the stereotypes expressed and are often themselves the jokes' originators,[7] as in "How many Germans does it take to change a lightbulb? One." where the joke itself becomes a statement of ethnic pride. Lightbulb jokes applied to subgroups can be used to ease tensions between them.[8]		Some versions of the joke are puns on the words "change"[9] or "screw":[10]		Q. How many psychiatrists does it take to change a light bulb? A. None–the light bulb will change when it's ready.[11][12]		Q. How many flies does it take to screw in a lightbulb? A. Two, but don't ask me how they got in there.[13]		Lightbulb jokes may be responses to current events, particularly those related to energy and political power.[14] For example, the lightbulb may not need to be changed at all due to ongoing power outages.[15] The Village Voice held a $200 lightbulb joke contest around the time of the Iran hostage crisis, with the winning joke being:[16]		Q. How many Iranians does it take to change a light bulb? A. You send us the prize money and we'll tell you the answer.		
Viola jokes are a category of jokes which are directed towards violas and viola players. The jokes are thought to have originated from the 18th century when the part of the viola was very uncomplicated and often just a filler part, thus attracting musicians who were not usually very talented musically.[1][2]		In Italy in the early 1700s, the following story occurred and it is thought that it was the origin of many viola jokes despite being a true story:[3]		The violinist Francesco Geminiani arrived in London in 1714, one of the many expatriate musicians who settled in England in the late seventeenth and early eighteenth centuries ... As a young man Geminiani was appointed head of the orchestra in Naples, where according to English music historian Charles Burney he was "so wild and unsteady a timist, that instead of regulating and conducting the band, he threw it into confusion", and was demoted to playing the viola.		The jokes come in many different forms. Some of them are only understandable to musicians and people acquainted with musical terms, while others are meant to be understood for everyone, regardless of their musical knowledge. Some jokes make fun of the viola itself while others make fun of violists, while some jokes are directed in the opposite direction: jokes about musicians who tell viola jokes.[4]		Making fun of the viola:		Making fun of violists:		Combining one or more of the above with other musical concepts:		Reverse viola jokes, i.e., jokes elevating the viola or violists and/or degrading other instruments or their players:		
Linguistics is the scientific[1] study of language,[2] and involves an analysis of language form, language meaning, and language in context.[3] The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini,[4][5] who wrote a formal description of the Sanskrit language in his Aṣṭādhyāyī.[6]		Linguists traditionally analyse human language by observing an interplay between sound and meaning.[7] Phonetics is the study of speech and non-speech sounds, and delves into their acoustic and articulatory properties. The study of language meaning, on the other hand, deals with how languages encode relations between entities, properties, and other aspects of the world to convey, process, and assign meaning, as well as manage and resolve ambiguity.[8] While the study of semantics typically concerns itself with truth conditions, pragmatics deals with how situational context influences the production of meaning.[9]		Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound[10] as well as meaning, and include componential sub-sets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences).[11] Modern theories that deal with the principles of grammar are largely based within Noam Chomsky's framework of generative linguistics.[12]		In the early 20th century, Ferdinand de Saussure distinguished between the notions of langue and parole in his formulation of structural linguistics. According to him, parole is the specific utterance of speech, whereas langue refers to an abstract phenomenon that theoretically defines the principles and system of rules that govern a language.[13] This distinction resembles the one made by Noam Chomsky between competence and performance in his theory of transformative or generative grammar. According to Chomsky, competence is an individual's innate capacity and potential for language (like in Saussure's langue), while performance is the specific way in which it is used by individuals, groups, and communities (i.e., parole, in Saussurean terms).[14]		The study of parole (which manifests through cultural discourses and dialects) is the domain of sociolinguistics, the sub-discipline that comprises the study of a complex system of linguistic facets within a certain speech community (governed by its own set of grammatical rules and laws). Discourse analysis further examines the structure of texts and conversations emerging out of a speech community's usage of language.[15] This is done through the collection of linguistic data, or through the formal discipline of corpus linguistics, which takes naturally occurring texts and studies the variation of grammatical and other features based on such corpora (or corpus data).		Stylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media.[16] In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself.[17] Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language.[18] The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages.		Linguistics also deals with the social, cultural, historical and political factors that influence language, through which linguistic and language-based context is often determined.[19] Research on language through the sub-branches of historical and evolutionary linguistics also focus on how languages change and grow, particularly over an extended period of time.		Language documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education – the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research.		Related areas of study also includes the disciplines of semiotics (the study of direct and indirect language through signs and symbols), literary criticism (the historical and ideological analysis of literature, cinema, art, or published material), translation (the conversion and documentation of meaning in written/spoken text from one language or dialect onto another), and speech-language pathology (a corrective method to cure phonetic disabilities and dis-functions at the cognitive level).						Before the 20th century, the term philology, first attested in 1716,[20] was commonly used to refer to the science of language, which was then predominantly historical in focus.[21][22] Since Ferdinand de Saussure's insistence on the importance of synchronic analysis, however, this focus has shifted[23] and the term "philology" is now generally used for the "study of a language's grammar, history, and literary tradition", especially in the United States[24] (where philology has never been very popularly considered as the "science of language").[25]		Although the term "linguist" in the sense of "a student of language" dates from 1641,[26] the term "linguistics" is first attested in 1847.[26] It is now the usual term in English for the scientific study of language, though "linguistic science" is sometimes used.		Today, the term linguist applies to someone who studies language or is a researcher within the field, or to someone who uses the tools of the discipline to describe and analyse specific languages.[27]		While some theories on linguistics focus on the different varieties that language produces, among different sections of society, others focus on the universal properties that are common to all human languages. The theory of variation therefore would elaborate on the different usages of popular languages like French and English across the globe, as well as its smaller dialects and regional permutations within their national boundaries. The theory of variation looks at the cultural stages that a particular language undergoes, and these include the following.		The pidgin stage in a language is a stage when communication occurs through a grammatically simplified means, developing between two or more groups that do not have a language in common. Typically, it is a mixture of languages at the stage when there occurs a mixing between a primary language with other language elements.		A creole stage in language occurs when there is a stable natural language developed from a mixture of different languages. It is a stage that occurs after a language undergoes its pidgin stage. At the creole stage, a language is a complete language, used in a community and acquired by children as their native language.		A dialect is a variety of language that is characteristic of a particular group among the language speakers.[28] The group of people who are the speakers of a dialect are usually bound to each other by social identity. This is what differentiates a dialect from a register or a discourse, where in the latter case, cultural identity does not always play a role. Dialects are speech varieties that have their own grammatical and phonological rules, linguistic features, and stylistic aspects, but have not been given an official status as a language. Dialects often move on to gain the status of a language due to political and social reasons. Differentiation amongst dialects (and subsequently, languages too) is based upon the use of grammatical rules, syntactic rules, and stylistic features, though not always on lexical use or vocabulary. The popular saying that "a language is a dialect with an army and navy" is attributed as a definition formulated by Max Weinreich.		Universal grammar takes into account general formal structures and features that are common to all dialects and languages, and the template of which pre-exists in the mind of an infant child. This idea is based on the theory of generative grammar and the formal school of linguistics, whose proponents include Noam Chomsky and those who follow his theory and work.		"We may as individuals be rather fond of our own dialect. This should not make us think, though, that it is actually any better than any other dialect. Dialects are not good or bad, nice or nasty, right or wrong – they are just different from one another, and it is the mark of a civilised society that it tolerates different dialects just as it tolerates different races, religions and sexes."[29]		A discourse is a way of speaking that emerges within a certain social setting and is based on a certain subject matter. A particular discourse becomes a language variety when it is used in this way for a particular purpose, and is referred to as a register.[30] There may be certain lexical additions (new words) that are brought into play because of the expertise of the community of people within a certain domain of specialization. Registers and discourses therefore differentiate themselves through the use of vocabulary, and at times through the use of style too. People in the medical fraternity, for example, may use some medical terminology in their communication that is specialized to the field of medicine. This is often referred to as being part of the "medical discourse", and so on.		When a dialect is documented sufficiently through the linguistic description of its grammar, which has emerged through the consensual laws from within its community, it gains political and national recognition through a country or region's policies. That is the stage when a language is considered a standard variety, one whose grammatical laws have now stabilised from within the consent of speech community participants, after sufficient evolution, improvisation, correction, and growth. The English language, besides perhaps the French language, may be examples of languages that have arrived at a stage where they are said to have become standard varieties.		The study of a language's universal properties, on the other hand, include some of the following concepts.		The lexicon is a catalogue of words and terms that are stored in a speaker's mind. The lexicon consists of words and bound morphemes, which are parts of words that can't stand alone, like affixes. In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included. Lexicography, closely linked with the domain of semantics, is the science of mapping the words into an encyclopedia or a dictionary. The creation and addition of new words (into the lexicon) is called coining or neologization,[31] and the new words are called neologisms.		It is often believed that a speaker's capacity for language lies in the quantity of words stored in the lexicon. However, this is often considered a myth by linguists. The capacity for the use of language is considered by many linguists to lie primarily in the domain of grammar, and to be linked with competence, rather than with the growth of vocabulary. Even a very small lexicon is theoretically capable of producing an infinite number of sentences.		As constructed popularly through the Sapir–Whorf hypothesis, relativists believe that the structure of a particular language is capable of influencing the cognitive patterns through which a person shapes his or her world view. Universalists believe that there are commonalities between human perception as there is in the human capacity for language, while relativists believe that this varies from language to language and person to person. While the Sapir–Whorf hypothesis is an elaboration of this idea expressed through the writings of American linguists Edward Sapir and Benjamin Lee Whorf, it was Sapir's student Harry Hoijer who termed it thus. The 20th century German linguist Leo Weisgerber also wrote extensively about the theory of relativity. Relativists argue for the case of differentiation at the level of cognition and in semantic domains. The emergence of cognitive linguistics in the 1980s also revived an interest in linguistic relativity. Thinkers like George Lakoff have argued that language reflects different cultural metaphors, while the French philosopher of language Jacques Derrida's writings have been seen to be closely associated with the relativist movement in linguistics, especially through deconstruction[32] and was even heavily criticized in the media at the time of his death for his theory of relativism.[33]		Linguistic structures are pairings of meaning and form. Any particular pairing of meaning and form is a Saussurean sign. For instance, the meaning "cat" is represented worldwide with a wide variety of different sound patterns (in oral languages), movements of the hands and face (in sign languages), and written symbols (in written languages).		Linguists focusing on structure attempt to understand the rules regarding language use that native speakers know (not always consciously). All linguistic structures can be broken down into component parts that are combined according to (sub)conscious rules, over multiple levels of analysis. For instance, consider the structure of the word "tenth" on two different levels of analysis. On the level of internal word structure (known as morphology), the word "tenth" is made up of one linguistic form indicating a number and another form indicating ordinality. The rule governing the combination of these forms ensures that the ordinality marker "th" follows the number "ten." On the level of sound structure (known as phonology), structural analysis shows that the "n" sound in "tenth" is made differently from the "n" sound in "ten" spoken alone. Although most speakers of English are consciously aware of the rules governing internal structure of the word pieces of "tenth", they are less often aware of the rule governing its sound structure. Linguists focused on structure find and analyse rules such as these, which govern how native speakers use language.		Linguistics has many sub-fields concerned with particular aspects of linguistic structure. The theory that elucidates on these, as propounded by Noam Chomsky, is known as generative theory or universal grammar. These sub-fields range from those focused primarily on form to those focused primarily on meaning. They also run the gamut of level of analysis of language, from individual sounds, to words, to phrases, up to cultural discourse.		Sub-fields that focus on a grammatical study of language include the following.		Stylistics is the study and interpretation of texts for aspects of their linguistic and tonal style. Stylistic analysis entails the analysis of description of particular dialects and registers used by speech communities. Stylistic features include rhetoric,[34] diction, stress, satire, irony, dialogue, and other forms of phonetic variations. Stylistic analysis can also include the study of language in canonical works of literature, popular fiction, news, advertisements, and other forms of communication in popular culture as well. It is usually seen as a variation in communication that changes from speaker to speaker and community to community. In short, Stylistics is the interpretation of text.		One major debate in linguistics concerns how language should be defined and understood. Some linguists use the term "language" primarily to refer to a hypothesized, innate module in the human brain that allows people to undertake linguistic behaviour, which is part of the formalist approach. This "universal grammar" is considered to guide children when they learn languages and to constrain what sentences are considered grammatical in any language. Proponents of this view, which is predominant in those schools of linguistics that are based on the generative theory of Noam Chomsky, do not necessarily consider that language evolved for communication in particular. They consider instead that it has more to do with the process of structuring human thought (see also formal grammar).		Another group of linguists, by contrast, use the term "language" to refer to a communication system that developed to support cooperative activity and extend cooperative networks. Such theories of grammar, called "functional", view language as a tool that emerged and is adapted to the communicative needs of its users, and the role of cultural evolutionary processes are often emphasized over that of biological evolution.[35]		Linguistics is primarily descriptive. Linguists describe and explain features of language without making subjective judgments on whether a particular feature or usage is "good" or "bad". This is analogous to practice in other sciences: a zoologist studies the animal kingdom without making subjective judgments on whether a particular species is "better" or "worse" than another.		Prescription, on the other hand, is an attempt to promote particular linguistic usages over others, often favouring a particular dialect or "acrolect". This may have the aim of establishing a linguistic standard, which can aid communication over large geographical areas. It may also, however, be an attempt by speakers of one language or dialect to exert influence over speakers of other languages or dialects (see Linguistic imperialism). An extreme version of prescriptivism can be found among censors, who attempt to eradicate words and structures that they consider to be destructive to society. Prescription, however, may be practised appropriately in the teaching of language, like in ELT, where certain fundamental grammatical rules and lexical terms need to be introduced to a second-language speaker who is attempting to acquire the language.		The objective of describing languages is often to uncover cultural knowledge about communities. The use of anthropological methods of investigation on linguistic sources leads to the discovery of certain cultural traits among a speech community through its linguistic features. It is also widely used as a tool in language documentation, with an endeavour to curate endangered languages. However, now, linguistic inquiry uses the anthropological method to understand cognitive, historical, sociolinguistic and historical processes that languages undergo as they change and evolve, as well as general anthropological inquiry uses the linguistic method to excavate into culture. In all aspects, anthropological inquiry usually uncovers the different variations and relativities that underlie the usage of language.		Most contemporary linguists work under the assumption that spoken data and signed data is more fundamental than written data. This is because:		Nonetheless, linguists agree that the study of written language can be worthwhile and valuable. For research that relies on corpus linguistics and computational linguistics, written language is often much more convenient for processing large amounts of linguistic data. Large corpora of spoken language are difficult to create and hard to find, and are typically transcribed and written. In addition, linguists have turned to text-based discourse occurring in various formats of computer-mediated communication as a viable site for linguistic inquiry.		The study of writing systems themselves, graphemics, is, in any case, considered a branch of linguistics.		Before the 20th century, linguists analysed language on a diachronic plane, which was historical in focus. This meant that they would compare linguistic features and try to analyse language from the point of view of how it had changed between then and later. However, with Saussurean linguistics in the 20th century, the focus shifted to a more synchronic approach, where the study was more geared towards analysis and comparison between different language variations, which existed at the same given point of time.		At another level, the syntagmatic plane of linguistic analysis entails the comparison between the way words are sequenced, within the syntax of a sentence. For example, the article "the" is followed by a noun, because of the syntagmatic relation between the words. The paradigmatic plane on the other hand, focuses on an analysis that is based on the paradigms or concepts that are embedded in a given text. In this case, words of the same type or class may be replaced in the text with each other to achieve the same conceptual understanding.		The formal study of language began in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. Pāṇini's systematic classification of the sounds of Sanskrit into consonants and vowels, and word classes, such as nouns and verbs, was the first known instance of its kind. In the Middle East, Sibawayh, a non-Arab, made a detailed description of Arabic in 760 AD in his monumental work, Al-kitab fi al-nahw (الكتاب في النحو, The Book on Grammar), the first known author to distinguish between sounds and phonemes (sounds as units of a linguistic system). Western interest in the study of languages began somewhat later than in the East,[36] but the grammarians of the classical languages did not use the same methods or reach the same conclusions as their contemporaries in the Indic world. Early interest in language in the West was a part of philosophy, not of grammatical description. The first insights into semantic theory were made by Plato in his Cratylus dialogue, where he argues that words denote concepts that are eternal and exist in the world of ideas. This work is the first to use the word etymology to describe the history of a word's meaning. Around 280 BC, one of Alexander the Great's successors founded a university (see Musaeum) in Alexandria, where a school of philologists studied the ancient texts in and taught Greek to speakers of other languages. While this school was the first to use the word "grammar" in its modern sense, Plato had used the word in its original meaning as "téchnē grammatikḗ" (Τέχνη Γραμματική), the "art of writing", which is also the title of one of the most important works of the Alexandrine school by Dionysius Thrax.[37] Throughout the Middle Ages, the study of language was subsumed under the topic of philology, the study of ancient languages and texts, practised by such educators as Roger Ascham, Wolfgang Ratke, and John Amos Comenius.[38]		In the 18th century, the first use of the comparative method by William Jones sparked the rise of comparative linguistics.[39] Bloomfield attributes "the first great scientific linguistic work of the world" to Jacob Grimm, who wrote Deutsche Grammatik.[40] It was soon followed by other authors writing similar comparative studies on other language groups of Europe. The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt, of whom Bloomfield asserts:[40]		This study received its foundation at the hands of the Prussian statesman and scholar Wilhelm von Humboldt (1767–1835), especially in the first volume of his work on Kavi, the literary language of Java, entitled Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwickelung des Menschengeschlechts (On the Variety of the Structure of Human Language and its Influence upon the Mental Development of the Human Race).		Early in the 20th century, Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them. By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still foundational in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the langue- parole distinction, distinguishing language as an abstract system (langue) from language as a concrete manifestation of this system (parole).[41] Substantial additional contributions following Saussure's definition of a structural approach to language came from The Prague school, Leonard Bloomfield, Charles F. Hockett, Louis Hjelmslev, Émile Benveniste and Roman Jakobson.[42][43]		During the last half of the 20th century, following the work of Noam Chomsky, linguistics was dominated by the generativist school. While formulated by Chomsky in part as a way to explain how human beings acquire language and the biological constraints on this acquisition, in practice it has largely been concerned with giving formal accounts of specific phenomena in natural languages. Generative theory is modularist and formalist in character. Chomsky built on earlier work of Zellig Harris to formulate the generative theory of language. According to this theory the most basic form of language is a set of syntactic rules universal for all humans and underlying the grammars of all human languages. This set of rules is called Universal Grammar, and for Chomsky describing it is the primary objective of the discipline of linguistics. For this reason the grammars of individual languages are of importance to linguistics only in so far as they allow us to discern the universal underlying rules from which the observable linguistic variability is generated.		In the classic formalization of generative grammars first proposed by Noam Chomsky in the 1950s,[44][45] a grammar G consists of the following components:		A formal description of language attempts to replicate a speaker's knowledge of the rules of their language, and the aim is to produce a set of rules that is minimally sufficient to successfully model valid linguistic forms.		Functional theories of language propose that since language is fundamentally a tool, it is reasonable to assume that its structures are best analysed and understood with reference to the functions they carry out. Functional theories of grammar differ from formal theories of grammar, in that the latter seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, whereas the former defines the functions performed by language and then relates these functions to the linguistic elements that carry them out. This means that functional theories of grammar tend to pay attention to the way language is actually used, and not just to the formal relations between linguistic elements.[46]		Functional theories describe language in term of the functions existing at all levels of language.		Cognitive linguistics emerged as a reaction to generativist theory in the 1970s and 1980s. Led by theorists like Ronald Langacker and George Lakoff, cognitive linguists propose that language is an emergent property of basic, general-purpose cognitive processes. In contrast to the generativist school of linguistics, cognitive linguistics is non-modularist and functionalist in character. Important developments in cognitive linguistics include cognitive grammar, frame semantics, and conceptual metaphor, all of which are based on the idea that form–function correspondences based on representations derived from embodied experience constitute the basic units of language.		Cognitive linguistics interprets language in terms of concepts (sometimes universal, sometimes specific to a particular tongue) that underlie its form. It is thus closely associated with semantics but is distinct from psycholinguistics, which draws upon empirical findings from cognitive psychology in order to explain the mental processes that underlie the acquisition, storage, production and understanding of speech and writing. Unlike generative theory, cognitive linguistics denies that there is an autonomous linguistic faculty in the mind; it understands grammar in terms of conceptualization; and claims that knowledge of language arises out of language use.[47] Because of its conviction that knowledge of language is learned through use, cognitive linguistics is sometimes considered to be a functional approach, but it differs from other functional approaches in that it is primarily concerned with how the mind creates meaning through language, and not with the use of language as a tool of communication.		Historical linguists study the history of specific languages as well as general characteristics of language change. The study of language change is also referred to as "diachronic linguistics" (the study of how one particular language has changed over time), which can be distinguished from "synchronic linguistics" (the comparative study of more than one language at a given moment in time without regard to previous stages). Historical linguistics was among the first sub-disciplines to emerge in linguistics, and was the most widely practised form of linguistics in the late 19th century. However, there was a shift to the synchronic approach in the early twentieth century with Saussure, and became more predominant in western linguistics with the work of Noam Chomsky.		Ecolinguistics explores the role of language in the life-sustaining interactions of humans, other species and the physical environment. The first aim is to develop linguistic theories which see humans not only as part of society, but also as part of the larger ecosystems that life depends on. The second aim is to show how linguistics can be used to address key ecological issues, from climate change and biodiversity loss to environmental justice. (Ecolinguistics Association)		Sociolinguistics is the study of how language is shaped by social factors. This sub-discipline focuses on the synchronic approach of linguistics, and looks at how a language in general, or a set of languages, display variation and varieties at a given point in time. The study of language variation and the different varieties of language through dialects, registers, and ideolects can be tackled through a study of style, as well as through analysis of discourse. Sociolinguists research on both style and discourse in language, and also study the theoretical factors that are at play between language and society.		Developmental linguistics is the study of the development of linguistic ability in individuals, particularly the acquisition of language in childhood. Some of the questions that developmental linguistics looks into is how children acquire language, how adults can acquire a second language, and what the process of language acquisition is.		Neurolinguistics is the study of the structures in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modelling		Linguists are largely concerned with finding and describing the generalities and varieties both within particular languages and among all languages. Applied linguistics takes the results of those findings and "applies" them to other areas. Linguistic research is commonly applied to areas such as language education, lexicography, translation, language planning, which involves governmental policy implementation related to language use, and natural language processing. "Applied linguistics" has been argued to be something of a misnomer.[48] Applied linguists actually focus on making sense of and engineering solutions for real-world linguistic problems, and not literally "applying" existing technical knowledge from linguistics. Moreover, they commonly apply technical knowledge from multiple sources, such as sociology (e.g., conversation analysis) and anthropology. (Constructed language fits under Applied linguistics.)		Today, computers are widely used in many areas of applied linguistics. Speech synthesis and speech recognition use phonetic and phonemic knowledge to provide voice interfaces to computers. Applications of computational linguistics in machine translation, computer-assisted translation, and natural language processing are areas of applied linguistics that have come to the forefront. Their influence has had an effect on theories of syntax and semantics, as modelling syntactic and semantic theories on computers constraints.		Linguistic analysis is a sub-discipline of applied linguistics used by many governments to verify the claimed nationality of people seeking asylum who do not hold the necessary documentation to prove their claim.[49] This often takes the form of an interview by personnel in an immigration department. Depending on the country, this interview is conducted either in the asylum seeker's native language through an interpreter or in an international lingua franca like English.[49] Australia uses the former method, while Germany employs the latter; the Netherlands uses either method depending on the languages involved.[49] Tape recordings of the interview then undergo language analysis, which can be done either by private contractors or within a department of the government. In this analysis, linguistic features of the asylum seeker are used by analysts to make a determination about the speaker's nationality. The reported findings of the linguistic analysis can play a critical role in the government's decision on the refugee status of the asylum seeker.[49]		Within the broad discipline of linguistics, various emerging sub-disciplines focus on a more detailed description and analysis of language, and are often organized on the basis of the school of thought and theoretical approach that they pre-suppose, or the external factors that influence them.		Semiotics is the study of sign processes (semiosis), or signification and communication, signs, and symbols, both individually and grouped into sign systems, including the study of how meaning is constructed and understood. Semioticians often do not restrict themselves to linguistic communication when studying the use of signs but extend the meaning of "sign" to cover all kinds of cultural symbols. Nonetheless, semiotic disciplines closely related to linguistics are literary studies, discourse analysis, text linguistics, and philosophy of language. Semiotics, within the linguistics paradigm, is the study of the relationship between language and culture. Historically, Edward Sapir and Ferdinand De Saussure's structuralist theories influenced the study of signs extensively until the late part of the 20th century, but later, post-modern and post-structural thought, through language philosophers including Jacques Derrida, Mikhail Bakhtin, Michel Foucault, and others, have also been a considerable influence on the discipline in the late part of the 20th century and early 21st century.[50] These theories emphasize the role of language variation, and the idea of subjective usage, depending on external elements like social and cultural factors, rather than merely on the interplay of formal elements.		Since the inception of the discipline of linguistics, linguists have been concerned with describing and analysing previously undocumented languages. Starting with Franz Boas in the early 1900s, this became the main focus of American linguistics until the rise of formal structural linguistics in the mid-20th century. This focus on language documentation was partly motivated by a concern to document the rapidly disappearing languages of indigenous peoples. The ethnographic dimension of the Boasian approach to language description played a role in the development of disciplines such as sociolinguistics, anthropological linguistics, and linguistic anthropology, which investigate the relations between language, culture, and society.		The emphasis on linguistic description and documentation has also gained prominence outside North America, with the documentation of rapidly dying indigenous languages becoming a primary focus in many university programmes in linguistics. Language description is a work-intensive endeavour, usually requiring years of field work in the language concerned, so as to equip the linguist to write a sufficiently accurate reference grammar. Further, the task of documentation requires the linguist to collect a substantial corpus in the language in question, consisting of texts and recordings, both sound and video, which can be stored in an accessible format within open repositories, and used for further research.[51]		The sub-field of translation includes the translation of written and spoken texts across mediums, from digital to print and spoken. To translate literally means to transmute the meaning from one language into another. Translators are often employed by organizations, such as travel agencies as well as governmental embassies to facilitate communication between two speakers who do not know each other's language. Translators are also employed to work within computational linguistics setups like Google Translate for example, which is an automated, programmed facility to translate words and phrases between any two or more given languages. Translation is also conducted by publishing houses, which convert works of writing from one language to another in order to reach varied audiences. Academic Translators, specialize and semi specialize on various other disciplines such as; Technology, Science, Law, Economics etc.		Biolinguistics is the study of the biology and evolution of language. It is a highly interdisciplinary field, including linguists, biologists, neuroscientists, psychologists, mathematicians, and others. By shifting the focus of investigation in linguistics to a comprehensive scheme that embraces natural sciences, it seeks to yield a framework by which we can understand the fundamentals of the faculty of language.		Clinical linguistics is the application of linguistic theory to the fields of Speech-Language Pathology. Speech language pathologists work on corrective measures to cure communication disorders and swallowing disorders		Chaika (1990) showed that schizophrenics with speech disorders, like rhyming inappropriately have attentional dysfunction, as when a patient, shown a colour chip and, then asked to identify it, responded "Looks like clay. Sounds like gray. Take you for a roll in the hay. Heyday, May Day." The color chip was actually clay-colored, so his first response was correct.'		However, normals suppress or ignore words which rhyme with what they've said unless they are deliberately producing a pun, poem or rap. Even then, the speaker shows connection between words chosen for rhyme and an overall meaning in discourse. schizophrenics with speech dysfunction show no such relation between rhyme and reason. Some even produce stretches of gibberish combined with recognizable words.		[52] copyright Elaine Ostrach Chaika>		Computational linguistics is the study of linguistic issues in a way that is "computationally responsible", i.e., taking careful note of computational consideration of algorithmic specification and computational complexity, so that the linguistic theories devised can be shown to exhibit certain desirable computational properties and their implementations. Computational linguists also work on computer language and software development.		Evolutionary linguistics is the interdisciplinary study of the emergence of the language faculty through human evolution, and also the application of evolutionary theory to the study of cultural evolution among different languages. It is also a study of the dispersal of various languages across the globe, through movements among ancient communities.[53]		Forensic linguistics is the application of linguistic analysis to forensics. Forensic analysis investigates on the style, language, lexical use, and other linguistic and grammatical features used in the legal context to provide evidence in courts of law. Forensic linguists have also contributed expertise in criminal cases.		
Christie Davies is a British sociologist, professor emeritus of sociology at the University of Reading, England, the author of many articles and books on criminology, the sociology of morality, censorship, and humour. He has also been visiting professor in India, Poland, United States, and Australia.[1][2]						In his 2002 book, The Mirth of Nations, he criticises the theories which derive humor from conflict and superiority, and argues instead that they are a form of play – a play with aggression, superiority, and taboo-breaking. He also argues against the Freudian theory about Jewish jokes being mostly self-deprecating, claiming that instead they are based on the cultural tradition of analytical thinking and self-awareness. American folklorist Alan Dundes called the book "the provocative critique of previous scholarship on the subject".[1]		Davies is past president of the International Society for Humor Studies.[3]		In 1983, Davies warned that when Britain handed Hong Kong back to China in 1997 there would be no future for its 5.5 million inhabitants.[4] He suggested a new "city state" could be created near Magilligan Point in between Coleraine and Derry for resettling Hong Kong inhabitants.[5]		
Physiology (/ˌfɪziˈɒlədʒi/; from Ancient Greek φύσις (physis), meaning 'nature, origin', and -λογία (-logia), meaning 'study of'[1]) is the scientific study of normal mechanisms, and their interactions, which works within a living system.[2] A sub-discipline of biology, its focus is in how organisms, organ systems, organs, cells, and biomolecules carry out the chemical or physical functions that exist in a living system.[3] Given the size of the field, it is divided into, among others, animal physiology (including that of humans), plant physiology, cellular physiology, microbial physiology (microbial metabolism), bacterial physiology, and viral physiology.[3]		Central to an understanding of physiological functioning is its integrated nature with other disciplines such as chemistry and physics, coordinated homeostatic control mechanisms, and continuous communication between cells.[4]		The Nobel Prize in Physiology or Medicine is awarded to those who make significant achievements in this discipline by the Royal Swedish Academy of Sciences. In medicine, a physiologic state is one occurring from normal body function, rather than pathologically, which is centered on the abnormalities that occur in animal diseases, including humans.[5]		Physiological studies date back to the ancient civilizations of India[6][7] and Egypt alongside anatomical studies, but did not utilize dissection or vivisection.[8]		The study of human physiology as a medical field dates back to at least 420 BC to the time of Hippocrates, also known as the "father of medicine."[9] Hippocrates incorporated his belief system called the theory of humours, which consisted of four basic substance: earth, water, air and fire. Each substance is known for having a corresponding humour: black bile, phlegm, blood and yellow bile, respectively. Hippocrates also noted some emotional connections to the four humours, which Claudis Galenus would later expand on. The critical thinking of Aristotle and his emphasis on the relationship between structure and function marked the beginning of physiology in Ancient Greece. Like Hippocrates, Aristotle took to the humoral theory of disease, which also consisted of four primary qualities in life: hot, cold, wet and dry.[10] Claudius Galenus (c. ~130–200 AD), known as Galen of Pergamum, was the first to use experiments to probe the functions of the body. Unlike Hippocrates though, Galen argued that humoral imbalances can be located in specific organs, including the entire body.[11] His modification of this theory better equipped doctors to make more precise diagnoses. Galen also played off of Hippocrates idea that emotions were also tied to the humours, and added the notion of temperaments: sanguine corresponds with blood; phlegmatic is tied to phlegm; yellow bile is connected to choleric; and black bile corresponds with melancholy. Galen also saw the human body consisting of three connected systems: the brain and nerves, which are responsible for thoughts and sensations; the heart and arteries, which give life; and the liver and veins, which can be attributed to nutrition and growth.[11] Galen was also the founder of experimental physiology.[12] And for the next 1,400 years, Galenic physiology was a powerful and influential tool in medicine.[11]		Jean Fernel (1497–1558), a French physician, introduced the term "physiology".[13]		In the 1820s, the French physiologist Henri Milne-Edwards introduced the notion of physiological division of labor, which allowed to "compare and study living things as if they were machines created by the industry of man." Inspired in the work of Adam Smith, Milne-Edwards wrote that the "body of all living beings, whether animal or plant, resembles a factory ... where the organs, comparable to workers, work incessantly to produce the phenomena that constitute the life of the individual." In more differentiated organisms, the functional labor could be apportioned between different instruments or systems (called by him as appareils).[14]		In 1858, Joseph Lister studied the cause of blood coagulation and inflammation that resulted after previous injuries and surgical wounds. He later discovered and implemented antiseptics in the operating room, and as a result decreases death rate from surgery by a substantial amount.[5][15]		The Physiological Society was founded in London in 1876 as a dining club.[16] The American Physiological Society (APS) is a nonprofit organization that was founded in 1887. The Society is, "devoted to fostering education, scientific research, and dissemination of information in the physiological sciences."[17]		In 1891, Ivan Pavlov performed research on "conditional reflexes" that involved dogs' saliva production in response to a plethora of sounds and visual stimuli.[15]		In the 19th century, physiological knowledge began to accumulate at a rapid rate, in particular with the 1838 appearance of the Cell theory of Matthias Schleiden and Theodor Schwann. It radically stated that organisms are made up of units called cells. Claude Bernard's (1813–1878) further discoveries ultimately led to his concept of milieu interieur (internal environment), which would later be taken up and championed as "homeostasis" by American physiologist Walter B. Cannon in 1929. By homeostasis, Cannon meant "the maintenance of steady states in the body and the physiological processes through which they are regulated."[18] In other words, the body's ability to regulate its internal environment. It should be noted that, William Beaumont was the first American to utilize the practical application of physiology.		Nineteenth century physiologists such as Michael Foster, Max Verworn, and Alfred Binet, based on Haeckel's ideas, elaborated what came to be called "general physiology", a unified science of life based on the cell actions,[14] later renamed in the twentieth century as cell biology.[19]		In the 20th century, biologists became interested in how organisms other than human beings function, eventually spawning the fields of comparative physiology and ecophysiology.[20] Major figures in these fields include Knut Schmidt-Nielsen and George Bartholomew. Most recently, evolutionary physiology has become a distinct subdiscipline.[21]		In 1920, August Krogh won the Nobel Prize for discovering how, in capillaries, blood flow is regulated.[15]		In 1954, Andrew Huxley and Hugh Huxley, alongside their research team, discovered the sliding filaments in skeletal muscle, known today as the sliding filament theory.[15]		Initially, women were largely excluded from official involvement in any physiological society. The American Physiological Society, for example, was founded in 1887 and included only men in its ranks.[22] In 1902, the American Physiological Society elected Ida Hyde as the first female member of the society.[23] Hyde, a representative of the American Association of University Women and a global advocate for gender equality in education,[24] attempted to promote gender equality in every aspect of science and medicine.		Soon thereafter, in 1913, J.S. Haldane proposed that women be allowed to formally join The Physiological Society, which had been founded in 1876.[citation needed] On 3 July 1915, six women were officially admitted: Florence Buchanan, Winifred Cullis, Ruth C. Skelton, Sarah C. M. Sowton, Constance Leetham Terry, and Enid M. Tribe.[25] The centenary of the election of women was celebrated in 2015 with the publication of a book "Women physiologists: centenary celebrations and beyond for The Physiological Society ISBN 978-0-9933410-0-7.		Prominent women physiologists include:		There are many ways to categorize the subdiscplines of physiology:[34]		Human physiology seeks to understand the mechanisms that work to keep the human body alive and functioning,[3] through scientific enquiry into the nature of mechanical, physical, and biochemical functions of humans, their organs, and the cells of which they are composed. The principal level of focus of physiology is at the level of organs and systems within systems. The endocrine and nervous systems play major roles in the reception and transmission of signals that integrate function in animals. Homeostasis is a major aspect with regard to such interactions within plants as well as animals. The biological basis of the study of physiology, integration refers to the overlap of many functions of the systems of the human body, as well as its accompanied form. It is achieved through communication that occurs in a variety of ways, both electrical and chemical.[citation needed]		Changes in physiology can impact the mental functions of individuals. Examples of this would be the effects of certain medications or toxic levels of substances.[35][36] Change in behavior as a result of these substances is often used to assess the health of individuals.[37][38]		Much of the foundation of knowledge in human physiology was provided by animal experimentation. Due to the frequent connection between form and function, physiology and anatomy are intrinsically linked and are studied in tandem as part of a medical curriculum.[39]		Human physiology		Animal physiology		Plant physiology		Fungal physiology		Protistan physiology		Algal physiology		Bacterial physiology		
Russian jokes (Russian: анекдо́ты, translit. anekdoty, lit. 'anecdotes'), the most popular form of Russian humor, are short fictional stories or dialogs with a punch line.		Russian joke culture includes a series of categories with fixed and highly familiar settings and characters. Surprising effects are achieved by an endless variety of plot twists. Russian jokes treat topics found everywhere in the world, including sex, politics, spousal relations, or mothers-in-law. This article discusses Russian joke subjects that are particular to Russian or Soviet culture. A major subcategory is Russian political jokes, which are discussed in a separate article.		Every category has numerous untranslatable jokes that rely on linguistic puns, wordplay, and the Russian language vocabulary of foul language. Below, (L) marks jokes whose humor value critically depends on intrinsic features of the Russian language.		Stierlitz is a fictional Soviet intelligence officer, portrayed by Vyacheslav Tikhonov in the popular Soviet TV series Seventeen Moments of Spring. In the jokes, Stierlitz interacts with various characters, most prominently his nemesis Müller. Usually two-liners spoofing the solemn style of the original TV voice-overs, the plot is resolved in grotesque plays on words or in parodies of the trains of thought and narrow escapes of the "original" Stierlitz.		Poruchik (First Lieutenant) Dmitry Rzhevsky is a cavalry (Hussar) officer, a straightforward, unsophisticated, and immensely rude military type whose rank and standing nevertheless gain him entrance into high society. In the aristocratic setting of high-society formal balls, and 19th-century social sophistication with widespread use of the French language, Rzhevsky, famous for brisk but usually unintelligent remarks, keeps puncturing the decorum with his vulgarities. In the jokes, he is often seen interacting with characters from the novel War and Peace by Leo Tolstoy. The name is borrowed from a character from a popular 1960s comedy, Hussar Ballad (Russian: «Гусарская баллада»), bearing little in common with the folklore hero. The 1967 film rendering of War and Peace contributed to the proliferation of the Rzhevsky jokes.[2] Some researchers point out that many jokes of this kind are versions of 19th-century Russian army jokes, retold as a new series of jokes about Rzhevsky.[3]		Rzhevsky (and supposedly all Hussars) has a casual, nonchalant attitude to love and sex:		He also gives his best advice to other Russian gentlemen on love matters. The Poruchik believes that the most straightforward approach is the most effective one:[3]		A series of jokes in which Rzhevsky wants to impress a high society gathering with a witticism, but messes up:		A series of jokes is based on a paradox of vulgarity within a high society setting:		While successful narration of quite a few Russian jokes heavily depends on using sexual vulgarities ("Russian mat"), Rzhevsky, with all his vulgarity, does not use heavy mat in traditional versions of his tales. One of his favorite words is "arse" (which is considered rather mild among Russian vulgarities), and there is a series of jokes where Rzhevsky answers "arse" to some innocent question (it is typical of Rzhevsky to blurt unromantic comments in the most romantic situations[3]):		The essence of Rzhevsky's peculiarity is captured in the following meta-joke:		This theme culminates in the following joke, sometimes called "the ultimate Hussar joke":		Rabinovich, is an archetypal Russian Jew. He is a crafty, cynical, sometimes bitter type, skeptical about the Soviet government, and often too smart for his own good. He is sometimes portrayed as an otkaznik ("refusenik"): someone who is refused permission to emigrate to Israel.		This following example explains Vladimir Putin's remark about "Comrade Wolf", describing the policies of the United States, that many non-Russians found cryptic.		Vovochka is the Russian equivalent of "Little Johnny". He interacts with his school teacher, Maria Ivanovna (shortened to "Marivanna", a stereotypical female teacher's name). "Vovochka" is a diminutive form of "Vladimir", creating the "little boy" effect. His fellow students bear similarly diminutive names. This "little boy" name is used in contrast with Vovochka's wisecracking, adult, often obscene statements.		Vasily Ivanovich Chapayev (Russian: Василий Иванович Чапаев), a Red Army hero of the Russian Civil War, in the rank of Division Commander, was featured in a hugely popular 1934 biopic. The most common topics are the war with the monarchist White Army, Chapayev's futile attempts to enroll into the Frunze Military Academy, and the circumstances of Chapayev's death (officially, he was gunned down by the Whites while attempting to flee across the Ural River after a lost battle).		Chapayev is usually accompanied by his aide-de-camp Petka (Петька, "Peter"), as well as Anka the Machine-Gunner (Анка-Пулемётчица), and political commissar Furmanov, all based on real people. (Being well known in Russian popular culture, Chapayev, Petka, and Anka were featured in a series of Russian adventure games released in the late 1990s and 2000s.)[5]		A number of jokes involve characters from the famous short stories by Sir Arthur Conan Doyle about the private detective Sherlock Holmes and his friend Doctor Watson. The jokes appeared and became popular soon after The Adventures of Sherlock Holmes and Dr. Watson film series was broadcast on Soviet TV in the late 1970s to mid-1980s. In all these movies, the characters were brilliantly played by the same actors – Vasily Livanov (as Sherlock Holmes) and Vitaly Solomin (as Watson). Quotes from these films are usually included in the jokes («Элементарно, Ватсон!» – "Elementary, my dear Watson!"). The narrator of such a joke usually tries to mimic the unique husky voice of Vasily Livanov. The standard plot of these jokes is a short dialog where Watson naïvely wonders about something, and Holmes finds a "logical" explanation to the phenomenon in question. Occasionally the jokes also include other characters – Mrs Hudson, the landlady of Holmes's residence on Baker Street; or Sir Henry and his butler Barrymore from The Hound of the Baskervilles; or the detective's nemesis Professor Moriarty.		The preceding joke won second place in the World's funniest joke contest.		Some older jokes involve Fantômas, a fictional criminal and master of disguise from the French detective series Fantômas, which were once widely popular in the USSR. His archenemy is Inspector Juve, charged with catching him. Fantômas' talent for disguise is usually the focus of the joke, allowing for jokes featuring all sorts of other characters:		New Russians (Russian: новые русские, Novye Russkie, the nouveau-riche), a class of arrogant, stupid, poorly-educated post-perestroika businessmen and gangsters, were a very common category of characters in Russian jokes of the 1990s. A common theme is the interaction of a New Russian in his archetypal shiny black Mercedes S600, arguing with a regular Russian in his modest Soviet-era Zaporozhets after their vehicles collide. The New Russian is often a violent criminal or at least speaks criminal argot, with a number of neologisms (or common words with skewed meaning) typical among New Russians. In a way, these anecdotes are a continuation of the Soviet-era series about Georgians, who were then depicted as extremely wealthy. The physical stereotype of the New Russians is often that of overweight men with short haircuts, dressed in thick gold chains and crimson jackets, with their fingers in the horns gesture, cruising around in the "600 Merc" and showing off their wealth. Jokes about expensive foreign sports cars can be compared to German Manta jokes.		Jokes set in the animal kingdom also feature characters, which draw their roots in the old Slavic fairy tales, where animals are portrayed as sapient beings with a stereotypical behavior, such as the violent Wolf; the sneaky (female) Fox; the cocky, cowardly Hare; the strong, simple-minded Bear; the multi-dimensional Hedgehog; and the Lion, king of the animal kingdom. In the Russian language all objects, animate and inanimate, have a (grammatical) gender – masculine, feminine, or neuter. The reader should assume that the Wolf, the Bear, the Hare, the Lion, and the Hedgehog are males, whereas the Fox (Vixen) is a female:		Animals in Russian jokes are and were very well aware of politics in the realm of humans:		Animal jokes are often fables, i.e. their punchline is (or eventually becomes) a kind of a maxim.		Aside from mammals, a rather common non-human is the "Golden Fish", who asks the catcher to release her in exchange for three wishes. The first Russian instance of this appeared in Alexander Pushkin's The Tale of the Fisherman and the Fish. In jokes, the Fisherman may be replaced by a representative of a nationality or ethnicity, and the third wish usually makes the punch line of the joke.		A similar type of joke involves a wish-granting Genie, the main difference being that in the case of the Golden Fish the Fisherman suffers from his own stupidity or greed, while the Genie is known for ingeniously twisting an interpretation of the wish to frustrate the grantee.		These often revolve around the supposition that the vast majority of Russian and Soviet militsioners (policemen, now called politzia) accept bribes. Also, they are not considered to be very bright.		Imperial Russia had been multi-ethnic for many centuries, and this situation continued throughout the Soviet period, and continues still. Throughout history, several ethnic stereotypes have developed, often in common with those views by other ethnicities (usually except for the ethnicity in question, but not always).		Chukchi, the native people of Chukotka, the most remote northeast corner of Russia, are a common minority targeted for generic ethnic jokes in Russia.[10] In jokes, they are depicted as generally primitive, uncivilized, and simple-minded, but clever in a naive kind of way.[11] A propensity for constantly saying odnako (equivalent to "however", depending on context) is a staple of Chukcha jokes. Often a straight man of the Chukcha in the jokes is a Russian geologist.		Chukchi do not miss their chance to retort:		Chukchi, due to their innocence, often see the inner truth of situations:		Ukrainians are depicted as rustic, stingy, and inordinately fond of salted salo (pork back fat); their accent, which is imitated in jokes, is perceived as funny.		Ukrainians are perceived to bear a grudge against Russians (derided as Moskali by Ukrainians):		Georgians are almost always depicted as stupid, greedy, hot-blooded, or sexually addicted, and in some cases, all four at the same time. A very loud and theatrical Georgian accent, including grammatical errors considered typical of Georgians, and occasional Georgian words are considered funny to imitate in Russian and often becomes a joke in itself.		In some jokes, Georgians are portrayed as rich, because in Soviet times they were also perceived as profiting immensely from black market businesses. There is a humorous expression deriving from the custom in police reports of referring to them as "persons of Caucasian nationality" (Russian: лицо кавказской национальности). Since the Russian word for "person" in the formal sense, (Russian: лицо), is the same as the word for "face", this allows a play on words about "faces of Caucasian nationality". In Russia itself, most people see "persons of Caucasian nationality" mostly at marketplaces selling fruits and flowers. In recent years, many old jokes about rich Georgians are being recast in terms of "New Russians".		Armenians are sometimes used interchangeably with Georgians, sharing some of the stereotypes. However their unique context is the fictitious Radio Yerevan, usually telling political jokes. Many other jokes are based on word play, often combined with the usage of a strong southern accent and consequent misunderstandings among the characters.		Estonians and Finns are depicted as having no sense of humor and being stubborn, taciturn, and especially slow. The Estonian accent, especially its sing-song tune and the lack of genders in grammar, forms part of the humor. Their common usage of long vowels and consonants both in speech and orthography (e.g. words such as Tallinn, Saaremaa) also led to the stereotype of being slow in speech, thinking, and action. In the everyday life, a person may be derisively called a "hot-headed Estonian fellow" (or in similar spirit, a "hot-tempered Finnish bloke", a phrase popularized by the 1995 Russian comedy Peculiarities of the National Hunt) to emphasize tardiness or lack of temperament. Indeed, Estonians play a similar role in Soviet humor to that of Finns in Scandinavian jokes.		Finnish political scientist Ilmari Susiluoto, also an author of three books on Russian humor, writes that Finns and Russians understand each other's humor. "Being included in a Russian anecdote is a privilege that Danes or Dutchmen have not attained. These nations are too boring and unvaried to rise into the consciousness of a large country. But the funny and slightly silly, stubborn Finns, the Chukhnas do."[12]		Finns share with Chukchi their ability to withstand cold:		Jewish humor is a highly developed subset of Russian humor, largely based on the Russian Jews' self-image. These Jewish anecdotes are not the same as anti-Semitic jokes. As some Jews say themselves, Jewish jokes are being made by either anti-Semites or the Jews themselves. Instead, whether told by Jews or non-Jewish Russians, these jokes show cynicism, self-irony, and wit that is characteristic of Jewish humor both in Russia and elsewhere in the world (see Jewish humor). The jokes are usually told with a characteristic Jewish accent (stretching out syllables, parodying the uvular trill of "R", etc.) and some peculiarities of sentence structure calqued into Russian from Yiddish. Many of these jokes are set in Odessa, and to some extent the phrase "Odessa humor" is synonymous with "Jewish jokes," even if the characters don't have Jewish names and even their religion/ethnicity is never mentioned. To Russians, it is sufficient to begin a joke with: "So, an Odessan woman gets on the bus...", and her Jewishness is implicitly understood by the listener.		During the 1967 Arab–Israeli War sympathies of the Soviet Jews were on the side of the Israel despite Egypt under Nasser being officially a Soviet ally, "on the Socialist path of development":		Common jokes center on the enormous size of the Chinese population, the Chinese language and the perceptions of the Chinese as cunning, industrious, and hard-working. Other popular jokes revolve around the belief that the Chinese are capable of amazing feats by primitive means, such as the Great Leap Forward.		A good many of the jokes are puns based on the fact that a widespread Chinese syllable (written as "hui" in pinyin) looks very similar to the obscene Russian word for penis. For this reason, since about 1956 the Russian-Chinese dictionaries render the Russian transcription of this syllable as "хуэй" (huey) (which actually is closer to the correct Chinese pronunciation). The most embarrassing case for the Chinese-Soviet friendship probably is the word "socialism" (社会主义; pinyin: shè huì zhǔ yì), rendered previously as шэ-хуй-чжу-и. The following humorous possibilities for the misunderstanding of the Chinese syllable "Hui" are derived from Aarons's (2012) text:[13]		Russians are a stereotype in Russian jokes themselves when set next to other stereotyped ethnicities. Thus, the Russian appearing in a triple joke with two Westerners, German, French, American or Englishman, will provide for a self-ironic punchline depicting himself as simple-minded and negligently careless but physically robust, which often ensures that he retains the upper hand over his less naive Western counterparts. Another common plot is a Russian holding a contest with technologically-superior opponents (usually, an American and a Japanese) and winning with sheer brute force or a clever trick.		Like elsewhere in the world, a good many of jokes in Russia are based on puns. Other jokes depend on grammatical and linguistic oddities and irregularities in the Russian language:		A similar story by Mikhail Zoshchenko involves yet another answer: after great care and multiple drafts to get the genitive case correct, including the substitution of "five штук (pieces)" for "five pokers", the response comes back: the warehouse has no kocherezhek (fully regular G pl of kocherezhka, "little poker").[14]		The Russian word for "testicle" is a diminutive of "egg", so the slang word is the non-diminutive form (yaitso, cf. Spanish huevo). A large variety of jokes capitalizes on this, ranging from predictably silly to surprisingly elegant:		Some religious jokes make fun of the clergy. They tend to be told in quasi-Church Slavonic, with its archaisms and the stereotypical okanye (a clear pronunciation of the unstressed /o/ as /o/; Modern Russian or "Muscovite" speech reduces unstressed /o/ to /a/). Clergymen in these jokes always bear obsolete names of distinctively Greek origin, and speak in basso profundo.		Other jokes touching on religion involve Heaven or Hell.		Probably any nation large enough to have an army has a good many of its own barracks jokes. Other than plays on words, these jokes are usually internationally understandable. In the Soviet Union, military service was universal (for males), so most people could relate to them. In these jokes a praporschik (warrant officer) is an archetypal bully, possessed of limited wit.		A. Dmitriev illustrates his sociological essay "Army Humor" with a large number of military jokes, mostly of Russian origin.[16]		There is an enormous number of one-liners, supposedly quoting a praporschik:		The punchline "from the fence to lunchtime" has become a well-known Russian cliché for an assignment with no defined ending (or for doing something forever).		Some of them are philosophical and apply not just to warrant officers:		A persistent theme in Russian military/police/law-enforcement-related jokes is the ongoing conflict between the representatives of the armed forces/law enforcement, and the "intelligentsia", i.e. well-educated members of society. Therefore, this theme is a satire of the image of military/law-enforcement officers and superiors as dumb and distrustful of "those educated smart-alecks":		Until shortly before perestroika, all fit male students of higher education had obligatory military ROTC courses from which they graduate as junior officers in the military reserve. A good many of military jokes originated there:		Sometimes, these silly statements can cross over, intentionally or unintentionally, into the realm of actual wit:		There are jokes about Russian nuclear missile forces and worldwide disasters because of lack of basic army discipline:		There is also eternal mutual disdain between servicemen and civilians:		Medical jokes are widespread. Often, they consist of a short dialogue of doctor or nurse with a patient:		The phrase "The doc said 'to the morgue' — to the morgue it is!" (Доктор сказал «в морг» — значит в морг!) became a well-known Russian cliché, meaning that something unpleasant must be done.		The life of most Russian university students is characterized by many people coming from small towns and crowded into grim dormitories. State universities (the only type of universities in existence during Soviet times) are notable for not caring about the students' comfort or the quality of their food. Most jokes make fun of these "interesting" conditions, inventive evasion by students of their academic duties or lecture attendance, constant shortage of money, and sometimes the alcoholic tendencies of engineering students.		Also, there are a number of funny student obsessions such as zachyotka (a book of grade transcripts, carried by every student), halyava (a chance of getting something (in this context, good or acceptable grades) without any effort), and getting a stipend for good grades.		A large number of jokes are about an exam: these are usually a dialogue between the professor and the student, based on a set of questions written on a bilet (a small sheet of paper, literally "ticket"), which the student draws at random in the exam room, and is given some time to prepare answers.		Other jokes use the fact that many (or even most) students really study only when the exam is in the imminent future (in one or two days), otherwise spending time with more interesting activities such as parties.		Cowboy jokes are a popular series about a Wild West full of trigger-happy simple-minded cowboys, and the perception that everything is big in Texas. It is often difficult to guess whether these are imported or genuinely Russian inventions:		A joke making fun of American films and their pirated English-to-Russian dubbing:		There is a series of jokes set in mental hospitals, some of which have a political subtext:		A large number of jokes are about distrofiks, people with severe muscular dystrophy. The main themes are the extreme weakness, slowness, gauntness, and emaciation of a dystrophic patient. Some of the jibes originated in jokes about Gulag camps[citation needed]. Alexander Solzhenitsyn, in his Gulag Archipelago, wrote that dystrophy was a typical phase in the life of a Gulag inmate, and quotes the following joke:		The very use of obscene Russian vocabulary, called mat, can enhance the humorous effect of a joke by its emotional impact. Due to the somewhat different cultural attitude to obscene slang, such an effect is difficult to render in English. The taboo status often makes mat itself the subject of a joke. One typical plot goes as follows.		(L) Another series of jokes exploits the richness of the mat vocabulary, which can give a substitute to a great many words of everyday conversation. Other languages often use profanity in a similar way (like the English fuck, for example), but the highly synthetic grammar of Russian provides for the unambiguity and the outstandingly great number of various derivations from a single mat root. Emil Draitser points out that linguists explain that the linguistic properties of the Russian language rich in affixes allows for expression of a wide variety of feelings and notions using only a few core mat words:[19]		As an ultimate joke in this series, the goal is to apply such substitution to as many words of a sentence as possible while keeping it meaningful. The following dialog at a construction site between a foreman and a worker retains a clear meaning even with all of its 14 words being derived from the single obscene word khuy. Russian language proficiency is needed to understand this fully: Word-by-word:		Possible, but incomplete translation:		After this example one may readily believe the following semi-apocryphal story. An inspection was expected at a Soviet plant to award it the Quality Mark, so the administration prohibited the usage of mat. On the next day the productivity dropped abruptly. People's Control figured out the reason: miscommunication. It turned out that workers knew all the tools and parts only by their mat-based names: khuyovina, pizdyulina, khuynyushka, khuyatina, etc. (all of these are loosely translated as "thing"); the same went for technological processes: otkhuyachit (to detach, cut, disconnect), zayebenit (to push through, force into), prikhuyachit (to attach, connect, bond, nail), khuynut (to move slightly, throw, pour), zakhuyarit (to throw far away, to put in deeply) etc.		Another story, possibly apocryphal, relates that during the time of the Space Race the CIA placed a bug in a Soviet rocket factory to gain intelligence about the manufacturing process. After six months of careful listening, the Americans had learned that Soviet rockets seemed to consist of khuyevina, pizd'ulina, and a poyeben' connecting them together, with all three parts being completely interchangeable.		
A broadside is a large sheet of paper printed on one side only.[1] Historically, broadsides were posters, announcing events or proclamations, or simply advertisements. Today, broadside printing is done by many smaller printers and publishers as a fine art variant, with poems often being available as broadsides, intended to be framed and hung on the wall.						The historical type of broadsides were ephemera, i.e., temporary documents created for a specific purpose and intended to be thrown away. They were one of the most common forms of printed material between the sixteenth and nineteenth centuries, particularly in Britain, Ireland and North America. They were often advertisements, but could also be used for news information or proclamations. It was also a very common format for printing the text of ballads (see Broadside (music)).		One classic example of a broadside used for proclamations is the Dunlap broadside, which was the first publication of the United States Declaration of Independence, printed on the night of July 4, 1776 by John Dunlap of Philadelphia in an estimated 200 copies.[2] An example of a broadside used for news information is the first published account of George Washington crossing the Delaware, printed on December 30, 1776 by John Dunlap.[3]		Broadsides were commonly sold at public executions in the United Kingdom in the 18th and 19th centuries. These were often produced by printers who specialised in them. They were typically illustrated by a crude picture of the crime, a portrait of the criminal, or a generic woodcut of a hanging taking place. There would be a written account of the crime and of the trial and often the criminal's confession of guilt. A doggerel verse warning others to not follow the executed person's example, to avoid their fate, was another common feature.[4]		
A smile is a facial expression formed primarily by flexing the muscles at the sides of the mouth.[1] Some smiles include a contraction of the muscles at the corner of the eyes, an action known as a "Duchenne smile". Smiles performed without the eye contraction can be perceived as "weird".		Among humans, smiling is an expression denoting pleasure, sociability, happiness, joy or amusement. It is distinct from a similar but usually involuntary expression of anxiety known as a grimace. Although cross-cultural studies have shown that smiling is a means of communication throughout the world,[2] there are large differences between different cultures, with some using smiles to convey confusion or embarrassment.						Primatologist Signe Preuschoft traces the smile back over 30 million years of evolution to a "fear grin" stemming from monkeys and apes who often used barely clenched teeth to portray to predators that they were harmless, or to signal submission to more dominant group members. The smile may have evolved differently among species and especially among humans.[3] Apart from Biology as an academic discipline that interprets the smile, those who study kinesics and psychology such as Freitas-Magalhaes view the smile as an affect display that can communicate feelings such as love, happiness, glee, pride, contempt, and embarrassment. Also, other types of monkeys can express this gesture as a symbol of happiness and fun.		A smile seems to have a favorable influence upon others and makes one likable and more approachable.[4] In the social context, smiling and laughter have different functions in the order of sequence in social situations:		Smiling is a signaling system that evolved from a need to communicate information of many different forms. One of these is advertisement of sexual interest. Female smiles are appealing to heterosexual males, increasing physical attractiveness and enhancing sex appeal. However, recent research indicates a man's smile may or may not be most effective in attracting heterosexual women, and that facial expressions such as pride or even shame might be more effective. The researchers ignored the role of smiles in other sexual preferences.[6]		The influence of smiling on others is not necessarily benign. It may take the form of positive reinforcement, possibly for an underhand manipulative and abusive purpose.[7] See also superficial smile.		While smiling is perceived as a positive emotion most of the time, there are many cultures that perceive smiling as a negative expression and consider it unwelcoming. Too much smiling can be viewed as a sign of shallowness or dishonesty.[8] In other parts of Asia, people may smile when they are embarrassed or in emotional pain. Some people may smile at others to indicate a friendly greeting. A smile may be reserved for close friends and family members. Many people in the former Soviet Union area consider smiling at strangers in public to be unusual and even suspicious behavior. [9]		Cheek dimples are visible indentations of the epidermis, caused by underlying flesh, which form on some people's cheeks, especially when they smile. Dimples are genetically inherited and are a dominant trait. A rarer form is the single dimple, which occurs on one side of the face only. Anatomically, dimples may be caused by variations in the structure of the facial muscle known as zygomaticus major. Specifically, the presence of a double or bifid zygomaticus major muscle may explain the formation of cheek dimples.[10]		This bifid variation of the muscle originates as a single structure from the zygomatic bone. As it travels anteriorly, it then divides with a superior bundle that inserts in the typical position above the corner of the mouth. An inferior bundle inserts below the corner of the mouth.		While conducting research on the physiology of facial expressions in the mid-19th century, French neurologist Guillaume Duchenne identified two distinct types of smiles. A Duchenne smile involves contraction of both the zygomatic major muscle (which raises the corners of the mouth) and the orbicularis oculi muscle (which raises the cheeks and forms crow's feet around the eyes).[11] The Duchenne smile has been described as "smizing", as in "smiling with the eyes".[12]		A non-Duchenne smile involves only the zygomatic major muscle.[13] "Research with adults initially indicated that joy was indexed by generic smiling, any smiling involving the raising of the lip corners by the zygomatic major [...]. More recent research suggests that smiling in which the muscle around the eye contracts, raising the cheeks high (Duchenne smiling), is uniquely associated with positive emotion."[14]		The Pan Am smile, also known as the "Botox smile", is the name given to a fake smile, in which only the zygomatic major muscle is voluntarily contracted to show politeness. It is named after the now defunct airline Pan American World Airways, whose flight attendants would always flash every passenger the same perfunctory smile.[15] Botox was introduced for cosmetic use in 2002.[16] Chronic use of Botox injections to deal with eye wrinkle can result in the paralysis of the small muscles around the eyes, preventing the appearance of a Duchenne smile.		In animals, the baring of teeth is often used as a threat or warning display—known as a snarl—or a sign of submission. For chimpanzees, it can also be a sign of fear. However, not all animal displays of teeth convey negative acts or emotions. For example, Barbary macaques demonstrate an open mouth display as a sign of playfulness which likely has similar roots and purposes as the human smile.[17]		
Computational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996.[1]		The first "computer model of a sense of humor" was suggested by Suslov as early as 1992.[2] Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory. However, an essentially new ingredient, the role of timing, is added to the well-known role of ambiguity. In biological systems, a sense of humor inevitably develops in the course of evolution, because its biological function consists of quickening the transmission of the processed information into consciousness and in a more effective use of brain resources. A realization of this algorithm in neural networks[3] justifies naturally Spencer's hypothesis on the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions.		A practical realization of this algorithm needs extensive databases, whose creation in the automatic regime was suggested recently.[4] As a result, this magistral direction was not developed properly and subsequent investigations accepted somewhat specialized colouring.						An approach to analysis of humor is classification of jokes. A further step is an attempt to generate jokes basing on the rules that underlie classification.		Simple prototypes for computer pun generation were reported in the early 1990s,[5] based on a natural language generator program, VINCI. Graeme Ritchie and Kim Binsted in their 1994 research paper described a computer program, JAPE, designed to generate question-answer-type puns from a general, i.e., non-humorous, lexicon.[6] (The program name is an acronym for "Joke Analysis and Production Engine".) Some examples produced by JAPE are:		Since then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language.[7][8] The STANDUP generator was tested on children within the framework of analyzing its usability for language skills development for children with communication disabilities, e.g., because of cerebral palsy. (The project name is an acronym for "System To Augment Non-speakers' Dialog Using Puns" and an allusion to standup comedy.) Children responded to this "language playground" with enthusiasm, and showed marked improvement on certain types of language tests.[7][9][10]		The two young people, who used the system over a ten-week period, regaled their peers, staff, family and neighbors with jokes such as: "What do you call a spicy missile? A hot shot!" Their joy and enthusiasm at entertaining others was inspirational.		Stock and Strapparava described a program to generate funny acronyms.[11]		"AskTheBrain" (2002) [1] used clustering and bayesian analysis to associate concepts in a comical way.		A statistical machine learning algorithm to detect whether a sentence contained a "That's what she said" double entendre was developed by Kiddon and Brun (2011).[12] There is an open-source Python implementation of Kiddon & Brun's TWSS system.[13]		A program to recognize knock-knock jokes was reported by Taylor and Mazlack.[14] This kind of research is important in analysis of human-computer interaction.[15]		An application of machine learning techniques for the distinguishing of joke texts from non-jokes was described by Mihalcea and Strapparava (2006).[16]		Takizawa et al. (1996) reported on a heuristic program for detecting puns in the Japanese language.[17]		A possible application for the assistance in language acquisition is described in the section "Pun generation". Another envisioned use of joke generators is in cases of steady supply of jokes where quantity is more important than quality. Another obvious, yet remote, direction is automated joke appreciation.		It is known[citation needed] that humans interact with computers in ways similar to interacting with other humans that may be described in terms of personality, politeness, flattery, and in-group favoritism. Therefore, the role of humor in human-computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested.[18][19][20]		Craig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences. Basing on the incongruity theory of humor, it is suggested that the resulting meaningless but funny sentences are easier to remember. For example, the password AjQA3Jtv is converted into "Arafat joined Quayle's Ant, while TARAR Jeopardized thurmond's vase".[21]		John Allen Paulos is known for his interest in mathematical foundations of humor.[22] His book Mathematics and Humor: A Study of the Logic of Humor demonstrates structures common to humor and formal sciences (mathematics, linguistics) and develops a mathematical model of jokes based on catastrophe theory.		
François Rabelais (/ˌræbəˈleɪ/;[1] French: [fʁɑ̃.swa ʁa.blɛ]; between 1483 and 1494 – 9 April 1553) was a French Renaissance writer, physician, Renaissance humanist, monk and Greek scholar. He has historically been regarded as a writer of fantasy, satire, the grotesque, bawdy jokes and songs. His best known work is Gargantua and Pantagruel. Because of his literary power and historical importance, Western literary critics consider him one of the great writers of world literature and among the creators of modern European writing.[2] His literary legacy is such that today, the word Rabelaisian has been coined as a descriptive inspired by his work and life. Merriam-Webster defines the word as describing someone or something that is "marked by gross robust humor, extravagance of caricature, or bold naturalism."[3]						No reliable documentation of the place or date of the birth of François Rabelais has survived. While some scholars put the date as early as 1483, he was probably born in November 1494 near Chinon in the province of Touraine, where his father worked as a lawyer.[4][5] The estate of La Devinière in Seuilly in the modern-day Indre-et-Loire, allegedly the writer's birthplace, houses a Rabelais museum.		Rabelais became a novice of the Franciscan order, and later a friar at Fontenay-le-Comte in Poitou, where he studied Greek and Latin as well as science, philology, and law, already becoming known and respected by the humanists of his era, including Guillaume Budé (1467-1540). Harassed due to the directions of his studies and frustrated with the Franciscan order's ban on the study of Greek (because of Erasmus' commentary on the Greek version of the Gospel of Saint Luke),[6] Rabelais petitioned Pope Clement VII (in office 1523-1534) and gained permission to leave the Franciscans and to enter the Benedictine order at Maillezais in Poitou, where he was more warmly received.[7]		Later he left the monastery to study medicine at the University of Poitiers and at the University of Montpellier. In 1532 he moved to Lyon, one of the intellectual centres of France, and in 1534 began working as a doctor at L'Hôtel Dieu de Lyon (hospital), for which he earned 40 livres a year. During his time in Lyon, he edited Latin works for the printer Sebastian Gryphius, became friends with Etienne Dolet, and worked up the nerve to write to Erasmus. Gryphius published his translations of Hippocrates, Galen and Giovanni Mainardi. As a physician, he used his spare time to write and publish humorous pamphlets critical of established authority and preoccupied with the educational and monastic mores of the time.[8]		In 1532, under the pseudonym Alcofribas Nasier (an anagram of François Rabelais), he published his first book, Pantagruel King of the Dipsodes, the first of his Gargantua series. The idea of basing an allegory on the lives of giants came to Rabelais from the folklore legend of les Grandes chroniques du grand et énorme géant Gargantua, which were sold as popular literature at the time in the form of inexpensive pamphlets by colporters and at the fairs of Lyon.[9] In this book, Rabelais sings the praises of Chinon wines through vivid descriptions of the "eat, drink and be merry" lifestyle of the main character, Pantagruel (a giant), and of his friends. This book, critical of the existing monastic and educational system, contained the first known occurrence in French of the words encyclopédie, caballe, progrès and utopie among others.[10] Despite the popularity of his book, both it and his prequel book (1534) on the life of Pantagruel's father Gargantua were condemned by the academics at the Sorbonne for their unorthodox ideas and by the Roman Catholic Church for their derision of certain religious practices.		In 1537, Rabelais gave an anatomy lesson at Hôtel Dieu on the corpse of a hanged man.[11] Rabelais's third book (Le Tiers Livre (fr)), published under his own name in 1546, was also banned. Its subject—Panurge's constant self-questioning as to whether he should marry or not— allowed Rabelais to revisit discussions he'd had while working as a secretary to Geoffrey d'Estissac (fr) earlier in Poitiers: la querelle des femmes being a lively subject in intellectual circles at the time. [12]		With support from members of the prominent du Bellay family, Rabelais received approval from King François I to continue to publish his collection. However, after the king's death in 1547, the academic élite frowned upon Rabelais, and the French Parlement suspended the sale of his fourth book (Le Quart Livre (fr)) published in 1552.[13][14]		Rabelais traveled frequently to Rome with his friend Cardinal Jean du Bellay, and lived for a short time in Turin (1540- ) as part of the household of du Bellay's brother, Guillaume, while king François I was his patron. Rabelais spent some time in hiding, under periodic threat of being condemned of heresy depending upon the health of his various protectors. Only the protection of du Bellay saved Rabelais after the condemnation of his novel by the Sorbonne.[15] Du Bellay would again help Rabelais in 1540 by seeking a papal authorization to legitimize two of his children (Auguste François, father of Jacques Rabelais, and Junie).[citation needed] Rabelais taught medicine at Montpellier in 1534 and in 1539.[16]		In June 1543 Rabelais became Master of Requests.[17]		Between 1545 and 1547 François Rabelais lived in Metz, then a free imperial city and a republic, to escape the condemnation by the University of Paris. In 1547, he became curate of Saint-Christophe-du-Jambet in Maine and of Meudon near Paris, from which he resigned in January 1553 before his death in Paris in April 1553.[18]		Different accounts survive of Rabelais' death and of his last words. According to some, he wrote a famous one sentence will: "I have nothing, I owe a great deal, and the rest I leave to the poor", and his last words were "I go to seek a Great Perhaps." One "last words" reference work provides at least four distinct versions of his last words (and additional variations of these). While many accounts feature the phrase "un grand peut-être" ("a Great Perhaps") – all are listed as "doubtful" due to lack of documentation. Additionally, some sources examined for Rabelais' last words cite Cardinal du Bellay; others cite Cardinal de Chatillon, creating further confusion.[19]		Gargantua and Pantagruel tells the story of two giants—a father, Gargantua, and his son, Pantagruel—and their adventures, written in an amusing, extravagant, and satirical vein.		While the first two books focus on the lives of the two giants, the rest of the series is mostly devoted to the adventures of Pantagruel's friends – such as Panurge, a roguish, erudite maverick, and Brother Jean, a bold, voracious and boozing ex-monk – and others on a collective naval journey in search of the Divine Bottle.		Even though most chapters are humorous, wildly fantastic and sometimes absurd, a few relatively serious passages have become famous for descriptions of humanistic ideals of the time. In particular, the letter of Gargantua to Pantagruel and the chapters on Gargantua's boyhood present a rather detailed vision of education.		It is in the first book that Rabelais writes of the Abbey of Thélème, built by the giant Gargantua. It differs remarkably from the monastic norm, as the abbey has a swimming pool, maid service, and no clocks in sight.		One of the verses of the inscription on the gate to the Abbey says:		Grace, honour, praise, delight, Here sojourn day and night. Sound bodies lined With a good mind, Do here pursue with might Grace, honour, praise, delight.		Rabelais gives us a description of the way of life of the Thélèmites of the abbey and their rule:		All their life was spent not in laws, statutes, or rules, but according to their own free will and pleasure. They rose out of their beds when they thought good; they did eat, drink, labour, sleep, when they had a mind to it and were disposed for it. None did awake them, none did offer to constrain them to eat, drink, nor to do any other thing; for so had Gargantua established it. In all their rule and strictest tie of their order there was but this one clause to be observed,		The French Renaissance was a time of linguistic controversies. Among the issues debated by scholars was the question of the origin of language. What was the first language? Is language something that all humans are born with or something that they learn? Is there some sort of connection between words and the objects they refer to, or are words purely arbitrary? Rabelais deals with these matters, among many others, in his books.		The early 16th century was also a time of innovations and change for the French language, especially in its written form. The first book of French grammar was published in 1530, followed nine years later by the language's first dictionary. Since spelling was far less codified than it is now, each author used his own orthography. Rabelais himself developed a personal set of rather complex rules. He was a supporter of etymological spelling, i.e., one that reflects the origin of words, and was thus opposed to those who favoured a simplified spelling, one that reflects the pronunciation of words.		Rabelais' use of his native tongue was original, lively, and creative. He introduced dozens of Greek, Latin, and Italian loan-words and direct translations of Greek and Latin compound words and idioms into French. He also used many dialectal forms and invented new words and metaphors, some of which have become part of the standard language and are still used today.		His works are also known for being filled with sexual double-entendres, dirty jokes, and bawdy songs.		Most scholars today agree that the French author wrote from a perspective of Christian humanism.[21] This has not always been the case. Abel Lefranc, in his 1922 introduction to Pantagruel, depicted Rabelais as a militant anti-Christian atheist.[22] M. A. Screech opposed this view and interpreted Rabelais as an Erasmian Christian humanist, the view that commands majority support today.[23]		Rabelais was Roman Catholic. Timothy Hampton writes that "to a degree unequaled by the case of any other writer from the European Renaissance, the reception of Rabelais's work has involved dispute, critical disagreement, and ... scholarly wrangling ..."[24] But at present, "whatever controversy still surrounds Rabelais studies can be found above all in the application of feminist theories to Rabelais criticism".[25]		In his novel Tristram Shandy, Laurence Sterne quotes extensively from Rabelais.[26]		Alfred Jarry performed, from memory, hymns of Rabelais at Symbolist Rachilde's Tuesday salons. Jarry worked for years on an unfinished libretto for an opera by Claude Terrasse based on Pantagruel.[27]		Anatole France lectured on him in Argentina. John Cowper Powys, D. B. Wyndham-Lewis, and Lucien Febvre (one of the founders of the French historical school Annales) wrote books about him. Mikhail Bakhtin, a Russian philosopher and critic, derived his celebrated concept of the carnivalesque and grotesque body from the world of Rabelais.[2]		Hilaire Belloc was a great admirer of Rabelais. He praised him as "at the summit" of authors of fantastic books.[28] He also wrote a short story entitled "On the Return of the Dead" in which Rabelais descended from heaven to earth in 1902 to give a lecture in praise of wine at the London School of Economics, but was instead arrested.[29]		Mikhail Bakhtin wrote Rabelais and His World, praising the author for his unbridled embrace of the carnival grotesque. In the book he analyzes Rabelais's use of the carnival grotesque throughout his writings and laments the death in modern culture of the purely communal spirit and regenerating laughter of the carnival.[2]		George Orwell was not an admirer of Rabelais. Writing in 1940, he called him "an exceptionally perverse, morbid writer, a case for psychoanalysis".[30]		Milan Kundera, in a 2007 article in The New Yorker, wrote: "(Rabelais) is, along with Cervantes, the founder of an entire art, the art of the novel." (page 31). He speaks in the highest terms of Rabelais, calling him "the best", along with Flaubert.[citation needed]		Rabelais was a major reference point for a few main characters (Boozing wayward monks, University Professors, and Assistants) in Robertson Davies's novel The Rebel Angels, part of The Cornish Trilogy. One of the main characters in the novel, Maria Theotoky, attempts to write her PhD on the works of Rabelais, while a murder plot unfolds around a scholarly unscathed manuscript. Rabelais was also mentioned in Davies's books The Lyre of Orpheus, (pp 178-181), and Tempest-Tost.[citation needed]		Rabelais is highlighted as a pivotal figure in Kenzaburō Ōe's acceptance speech for the Nobel Prize in Literature in 1994.[31]		Henry Miller, in his first novel, Tropic of Cancer, speaks admiringly of Rabelais in several passages.		Notes		Bibliography		
A conditional joke is a joke meant for a qualified audience only, possessing prior knowledge and understanding of the topic, which in turn, enables them to (quote-unquote) get that joke. Such ability is also called the prerequisite condition for laughter. The conditional joke is one of two main categories of jokes, according to Ted Cohen; the main one being a universal joke, which does not require familiarity with the hermetic language of a conditional joke.[1] The conditional, or hermetic jokes, often depend on the internalized negative stereotypes held by the audience toward a targeted group of people. Such affective disposition can also explain the persistence of ethnic jokes in multicultural societies. Although they can be understood by many, the conditional jokes usually don't make ridiculed individuals laugh at the punch line.[1][2]		The most common type of conditional jokes, which target the jargon and all topics specific to professions and occupations, include the doctor jokes (surgeons, internists, psychiatrists, etc.), the lawyers and politicians, musicians, and the rabbi jokes among many. Other hermetic jokes which target ethnicity include Polish jokes made in the US, Irish jokes made in England, Ukrainian jokes made in Russia, Newfie jokes made in Canada, Sikh jokes made in India, Russian jokes, Texas jokes, Jewish jokes made by non-Jews, the Black people jokes, and numerous others.[3]		
A riddle joke, joke riddle, pseudo-joke or conundrum is a riddle which does not expect the asked person to know the answer, but rather constitutes a set-up to the humorous punch line of the joke.[1]		It is one of the four major types of riddles, according to Nigel F. Barley.[2] There are many cycles of jokes in the form of a conundrum, such as Elephant jokes,[1] "Why did the chicken cross the road?" and lightbulb jokes.		Joke cycles implying inferiority or other stereotypes of certain categories of people, such as blonde jokes, or ethnic jokes (such as Pollack joke) have a considerable amount of joke riddles.[1]		In areas which have historical ties with Asia Minor, such as Greece, Turkey, Armenia, of popularity are "abstract riddles" that follow templates: "What is this: A inside and B outside?" or "What is this: A is around and B in the middle?". For example:		Q: What is wool outside and cotton inside? A: A poodle in front of a drugstore with cotton swabs on sale.[3]		Q:What is water around and the law in the middle? A: Judge Karapetyan in his pool.		Q:What is meat outside and iron inside? A:Tailor Hovhannes swallowed a needle.		
On 31 August 1997, Diana, Princess of Wales, died as a result of injuries sustained in a car crash in the Pont de l'Alma road tunnel in Paris, France. Dodi Fayed and Henri Paul, the driver of the Mercedes-Benz S280, were pronounced dead at the scene; the bodyguard of Diana and Fayed, Trevor Rees-Jones, was the only survivor. Although the media blamed the paparazzi following the car, an 18-month French judicial investigation found that the crash was caused by Paul, who lost control of the car at high speed while drunk. Paul was the deputy head of security at the Hôtel Ritz and had earlier goaded the paparazzi waiting outside the hotel.[1] His inebriation may have been exacerbated by anti-depressants and traces of a tranquilising anti-psychotic in his body.[2][3] The investigation concluded that the photographers were not near the Mercedes when it crashed.[4]		Since February 1998, Fayed's father, Mohamed Al-Fayed (the owner of the Hôtel Ritz, where Paul worked) has claimed that the crash was a result of a conspiracy,[5] and later contended that the crash was orchestrated by MI6 on the instructions of the Royal Family.[6] His claims were dismissed by a French judicial investigation[2] and by Operation Paget, a Metropolitan Police Service inquiry that concluded in 2006.[7] An inquest headed by Lord Justice Scott Baker into the deaths of Diana and Fayed began at the Royal Courts of Justice, London, on 2 October 2007, a continuation of the inquest that began in 2004.[8] On 7 April 2008, the jury concluded that Diana and Fayed were the victims of an "unlawful killing" by the "grossly negligent" chauffeur Paul and the drivers of the following vehicles.[9] Additional factors were "the impairment of the judgment of the driver of the Mercedes through alcohol" and "the death of the deceased was caused or contributed to by the fact that the deceased was not wearing a seat belt, the fact that the Mercedes struck the pillar in the Alma Tunnel rather than colliding with something else".[10]						On Saturday, 30 August 1997, Diana left Sardinia on a private jet and arrived in Paris with Dodi Fayed, the son of Mohamed Al-Fayed.[11] They had stopped there en route to London, having spent the preceding nine days together on board Mohamed Al-Fayed's yacht Jonikal on the French and Italian Riviera. They had intended to stay there for the night. Mohamed Al-Fayed was and is the owner of the Hôtel Ritz Paris. He also owned an apartment in Rue Arsène Houssaye, a short distance from the hotel, just off the Avenue des Champs Elysées.		Henri Paul, the deputy head of security at the Ritz Hotel, had been instructed to drive the hired black 1994 Mercedes-Benz S280 in order to elude the paparazzi;[12] a decoy vehicle left the Ritz first from the main entrance on Place Vendôme, attracting a throng of photographers. Diana and Fayed then departed from the hotel's rear entrance rue Cambon at around 00:20 on 31 August CEST (22:20 UTC), heading for the apartment in Rue Arsène Houssaye. They were the rear passengers; Trevor Rees-Jones, a member of the Fayed family's personal protection team, was in the (right) front passenger seat.		After leaving the rue Cambon and crossing the Place de la Concorde, they drove along Cours la Reine and Cours Albert 1er – the embankment road along the right bank of the River Seine – into the Place de l'Alma underpass.[13] At around 12:23 a.m., at the entrance to the tunnel, Paul lost control; the car swerved to the left of the two-lane carriageway before colliding head-on with the 13th pillar supporting the roof at an estimated speed of 105 km/h (65 mph).[14] It then spun and hit the stone wall of the tunnel backwards, finally coming to a stop. The impact caused substantial damage, particularly to the front half of the vehicle, as there was no guard rail between the pillars to prevent this. The Place de l'Alma underpass is the only one on that embankment road that has roof-supporting pillars.		As the victims lay in the wrecked car, the photographers, who had been driving slower and were accordingly some distance behind the Mercedes, reached the scene. Some rushed to help, tried to open the doors and help the victims, while some of them took pictures. Critically injured, Diana was reported to murmur repeatedly, "Oh my God," and after the photographers and other helpers were pushed away by police, "Leave me alone".[15]		Fayed had been sitting in the left rear passenger seat and appeared to be dead. Fire officers were still trying to resuscitate him when he was pronounced dead by a doctor at 1:32 am; Paul was declared dead on removal from the wreckage. Both were taken to the Institut Médico-Légal (IML), the Paris mortuary, not to a hospital. Autopsy examination concluded that Paul and Fayed had both suffered a rupture in the isthmus of the aorta and a fractured spine, with, in the case of Paul, a medullar section in the dorsal region and in the case of Fayed a medullar section in the cervical region.		Still conscious, Rees-Jones had suffered multiple serious facial injuries. The front occupants' airbags had functioned normally. The occupants were not wearing seat belts.[16] Diana, who had been sitting in the right rear passenger seat, was still conscious. It was first reported that she was crouched on the floor of the vehicle with her back to the road. It was also reported that a photographer described her as bleeding from the nose and ears with her head rested on the back of the front passenger seat; he tried to remove her from the car but her feet were stuck. Then he told her that help was on the way and to stay awake; there was no answer, just blinking.		In June 2007 the Channel 4 documentary Diana: The Witnesses in the Tunnel claimed that the first person to touch Diana was Dr. Maillez,[17] who chanced upon the scene. He reported that Diana had no visible injuries but was in shock and he supplied her with oxygen.		The first police patrol officers arrived at 12:30. Shortly afterwards, the seven paparazzi on the scene were arrested. Diana was removed from the car at 1:00 am. She then went into cardiac arrest. Following external cardiopulmonary resuscitation, her heart started beating again. She was moved to the SAMU ambulance at 1:18 am, left the scene at 1:41 am and arrived at the Pitié-Salpêtrière Hospital at 2:06 am.[18] Despite attempts to save her, her internal injuries were too extensive: her heart had been displaced to the right side of the chest, which tore the pulmonary vein and the pericardium. Despite lengthy resuscitation attempts, including internal cardiac massage, she died at 4:00 a.m.[19]		Later that morning, Jean-Pierre Chevènement (French Minister of the Interior), French Prime Minister Lionel Jospin, Bernadette Chirac (the wife of the French President, Jacques Chirac), and Bernard Kouchner (French Health Minister), visited the hospital room where Diana's body lay and paid their last respects. After their visits, the Anglican Archdeacon of France, Father Martin Draper, said commendatory prayers from the Book of Common Prayer.		At around 2:00 pm, Diana's former husband, Charles, Prince of Wales, and her two older sisters, Lady Sarah McCorquodale and Lady Jane Fellowes, arrived in Paris; they left with her body 90 minutes later.		Initial media reports stated Diana's car had collided with the pillar at 190 km/h (120 mph), and that the speedometer's needle had jammed at that position. It was later announced the car's speed on collision was about 95–110 km/h (60–70 mph). The car was certainly travelling much faster than the speed limit of 50 km/h (31 mph). In 1999, a French investigation concluded the Mercedes had come into contact with another vehicle (a white Fiat Uno) in the tunnel.[20] The driver of that vehicle has never been traced, and the specific vehicle has not been identified.[20]		An 18-month French judicial investigation concluded in 1999 that the crash was caused by Paul, who lost control at high speed while intoxicated.[21]		Diana's death was met with extraordinary public expressions of grief, and her funeral at Westminster Abbey on 6 September drew an estimated 3 million mourners and onlookers in London,[22] and worldwide television coverage watched by 2.5 billion people.[23] It was aired to 200 countries in 44 languages.[24] Singer Elton John performed a new version of his song "Candle in the Wind" at the service.		Members of the public were invited to sign a book of condolence at St James Palace. Throughout the night, members of the Women's Royal Voluntary Service and the Salvation Army provided support for people queuing along the Mall.[25] More than one million bouquets were left at her London home, Kensington Palace, while at her family's estate of Althorp the public was asked to stop bringing flowers as the volume of people and flowers in the surrounding roads was said to be causing a threat to public safety.		By 10 September, the pile of flowers outside Kensington Gardens was 5 feet (1.5 m) deep in places and the bottom layer had started to compost.[26] The people were quiet, queuing patiently to sign the book and leave their gifts. There were a few minor incidents. Fabio Piras, a Sardinian tourist, was given a one-week prison sentence on 10 September for having taken a teddy bear from the pile. When the sentence was later reduced to a £100 fine, Piras was punched in the face by a member of the public when he left the court.[27] The next day two women, a 54-year-old secondary school teacher and a 50-year-old communications technician, were each given a 28-day prison sentence for having taken 11 teddy bears and a number of flowers from the pile outside the palace.[28] This was reduced to a fine of £200 each after they had spent two nights in prison.		Some criticised the reaction to Diana's death at the time as being "hysterical" and "irrational". As early as 1998 philosopher Anthony O'Hear identified the mourning as a defining point in the "sentimentalisation of Britain", a media-fuelled phenomenon where image and reality become blurred.[29] These criticisms were repeated on the 10th anniversary, when journalist Jonathan Freedland expressed the opinion that "It has become an embarrassing memory, like a mawkish, self-pitying teenage entry in a diary ... we cringe to think about it." In 2010, Theodore Dalrymple suggested "sentimentality, both spontaneous and generated by the exaggerated attention of the media, that was necessary to turn the death of the princess into an event of such magnitude thus served a political purpose, one that was inherently dishonest in a way that parallels the dishonesty that lies behind much sentimentality itself".[30] Some cultural analysts disagreed. Sociologist Deborah Steinberg pointed out that many Britons associated Diana not with the Royal Family but with social change and a more liberal society: "I don't think it was hysteria, the loss of a public figure can be a touchstone for other issues."[31]		The reaction of the Royal Family to Diana's death caused resentment and outcry. They were at their summer residence at Balmoral Castle, and their initial decision not to return to London or to mourn more publicly was much criticised at the time. Their rigid adherence to protocol, and their concern to care for Diana's grieving sons, was interpreted as a lack of compassion.[32][33]		In particular, the refusal of Buckingham Palace to fly the Royal Standard at half-mast provoked angry headlines in newspapers.[34] "Where is our Queen? Where is her Flag?" asked The Sun.[35] The Palace's stance was one of royal protocol: no flag could fly over Buckingham Palace, as the Royal Standard is only flown when the Queen is in residence, and the Queen was then in Scotland. The Royal Standard never flies at half-mast as it is the Sovereign's flag and there is never an interregnum or vacancy in the monarchy, as the new monarch immediately succeeds his or her predecessor.		Finally, as a compromise, the Union Flag was flown at half-mast as the Queen left for Westminster Abbey on the day of the funeral.[35] This set a precedent, and Buckingham Palace has subsequently flown the Union Flag when the Queen is not in residence.		The Queen, who returned to London from Balmoral, agreed to a television broadcast to the nation.[35]		Over a million people lined the four-mile (6 km) route from Kensington Palace to Westminster Abbey.[36] Outside the Abbey and in Hyde Park crowds watched and listened to proceedings on large outdoor screens and speakers as guests filed in, including representatives of the many charities of which Diana was patron. Notable attendants included Hillary Clinton; Bernadette Chirac, wife of the French President, Jacques Chirac; and other celebrities, including Italian tenor Luciano Pavarotti and Diana's good friends singers George Michael and Elton John – the latter performed a rewritten version of his song "Candle in the Wind" that was dedicated to her.[37] The service was televised live around the world.		Protocol was disregarded when the guests applauded the speech by Diana's younger brother Earl Spencer, who strongly criticised the press and indirectly criticised the Royal Family for their treatment of her.[38] The funeral is estimated to have been watched by 31.5 million viewers in Britain. Precise calculation of the worldwide audience is not possible, but estimated at around 2.5 billion.[39]		After the end of the ceremony, the coffin was driven to Althorp in a Daimler hearse.[40] Mourners cast flowers at the funeral procession for almost the entire length of its journey and vehicles even stopped on the opposite carriageway of the M1 motorway as the cars passed.[41] In a private ceremony, Diana was buried on an island in the middle of a lake. In her coffin, she wore a black Catherine Walker dress and is clutching a rosary in her hands. The rosary had been a gift from Mother Teresa of Calcutta, a confidante of Diana, who had died the day before her funeral. A visitors' centre is open during summer months, with an exhibition about her and a walk around the lake. All profits are donated to the Diana, Princess of Wales Memorial Fund.		During the four weeks following her funeral, the suicide rate in England and Wales rose by 17% and cases of deliberate self-harm by 44.3% compared with the average for that period in the four previous years. Researchers suggest that this was caused by the "identification" effect, as the greatest increase in suicides was by people most similar to Diana: women aged 25 to 44, whose suicide rate increased by over 45%.[42]		In the years after her death, interest in the life of Diana has remained high. As a temporary memorial, the public co-opted the Flamme de la Liberté (Flame of Liberty), a monument near the Alma tunnel related to the French donation of the Statue of Liberty to the United States. The messages of condolence have since been removed and its use as a Diana memorial has discontinued, though visitors still leave messages in her memory. A permanent memorial, the Diana, Princess of Wales Memorial Fountain, was opened by the Queen in Hyde Park in London on 6 July 2004.[43]		After her death, the actor Kevin Costner, who had been introduced to Diana by her former sister-in-law, Sarah, Duchess of York, claimed he had been in negotiations with her to co-star in a sequel to the thriller The Bodyguard, which starred Costner and Whitney Houston. Buckingham Palace dismissed Costner's claims as unfounded.[44]		Actor George Clooney publicly lambasted several tabloids and paparazzi agencies following Diana's death. A few of the tabloids boycotted Clooney following the outburst, stating that he "owed a fair portion of his celebrity" to the tabloids and photo agencies in question.[45]		Diana was ranked third in the 2002 Great Britons poll sponsored by the BBC and voted for by the British public, after Sir Winston Churchill (1st) (a distant cousin), and Isambard Kingdom Brunel (2nd), just above Charles Darwin (4th), William Shakespeare (5th), and Isaac Newton (6th). That same year, another British poll named Diana's death as the most important event in the country's last 100 years.[46] Historian Nick Barrett criticised this outcome as being "a pretty shocking result".[47]		On 6 January 2004, six years after her death, an inquest into the deaths of Diana and Fayed opened in London held by Michael Burgess, the coroner of the Queen's household. The coroner asked the Metropolitan Police Commissioner, Sir John Stevens, to make inquiries, in response to speculation (see below) that the deaths were not an accident. The police investigation reported its findings in Operation Paget in December 2006.		Later in 2004, US TV network CBS showed pictures of the crash scene showing an intact rear side and centre section of the Mercedes, including one of an unbloodied Diana with no outward injuries crouched on the rear floor with her back to the right passenger seat—the right rear door is fully open. The release of these pictures caused uproar in the UK, where it was widely felt that the privacy of Diana was being infringed, and spurred another lawsuit by Mohammed al-Fayed.		In January 2006, Lord Stevens explained in an interview on GMTV that the case is substantially more complex than once thought. The Sunday Times wrote on 29 January 2006 that agents of the British secret service were cross-examined because they were in Paris at the time of the crash. It was suggested in many journals that these agents might have exchanged the blood test of the driver with another blood sample (although no evidence for this has been forthcoming).[48][49]		On 13 July 2006, the Italian magazine Chi published a photograph showing Diana in her "last moments" despite an unofficial blackout on such photographs being published. The photograph was taken shortly after the crash, and shows her slumped in the back seat while a paramedic attempts to fit an oxygen mask over her face. This photograph was also published in other Italian and Spanish magazines and newspapers. The editor of Chi defended his decision by saying he published the photographs taken from the French investigation dossier for the "simple reason that they haven't been seen before" and that he felt the images do not disrespect the memory of the Princess.[50]		On 17 August 2013, Scotland Yard revealed that they were examining the credibility of information from a source that alleged that Diana was murdered by a member of the British military.[51][52]		Although the initial French investigation found that Diana had died as a result of an accident, the conspiracy theories persistently raised by Mohammed al-Fayed and the Daily Express suggested that she was assassinated. In 2004 a special Metropolitan Police inquest team, Operation Paget, headed by Commissioner John Stevens, was established to investigate the conspiracy theories.		In October 2003, the Daily Mirror published a letter from Diana in which, ten months before her death, she wrote about a possible plot to kill her by tampering with the brakes of her car. "This particular phase in my life is the most dangerous." She said "my husband is planning 'an accident' in my car, brake failure and serious head injury in order to make the path clear for Charles to marry".[53][54]		A report with the findings of the criminal investigation was published on 14 December 2006. The inquest was closed following the conclusion of the British inquest into the deaths in April 2008.		Under English law, an inquest is required in cases of sudden or unexplained death.[55] The inquests into the deaths of Diana and Fayed opened on 8 January 2007, with Dame Elizabeth Butler-Sloss acting as Deputy Coroner of the Queen's Household for the Diana inquest and Assistant Deputy Coroner for Surrey in relation to the Fayed inquest. Butler-Sloss originally intended to sit without a jury;[56] this decision was later overturned by the High Court,[57] as well as the jurisdiction of the Coroner of the Queen's Household. On 24 April 2007, Butler-Sloss stepped down, saying she lacked the experience required to deal with an inquest with a jury.[8] The role of Coroner for the inquests was transferred to Lord Justice Scott Baker, who formally took up the role on 11 June as Coroner for Inner West London.		On 27 July 2007, Baker, following representations for the lawyers of the interested parties, issued a list of issues likely to be raised at the inquest, many of which had been dealt with in great detail by Operation Paget:		The inquests officially began on 2 October 2007 with the swearing of a jury of six women and five men. Scott Baker delivered a lengthy opening statement giving general instructions to the jury and introducing the evidence.[59] The BBC reported that Mohammed Al-Fayed, having earlier reiterated his claim that his son and Diana were murdered by the Royal Family, immediately criticised the opening statement as biased.[60]		The inquest heard evidence from people connected with Diana and the events leading to her death, including Paul Burrell, Mohamed Al-Fayed, her stepmother, the survivor of the crash, and the former head of MI5.[61]		Scott Baker began his summing up to the jury on 31 March 2008.[62] He opened by telling the jury "no-one except you and I and, I think, the gentleman in the public gallery with Diana and Fayed painted on his forehead sat through every word of evidence" and concluded that there was "not a shred of evidence" that Diana's death had been ordered by the Duke of Edinburgh or organised by the security services.[63] Lord Justice Scott Baker concluded his summing up on Wednesday, 2 April 2008.[64] After summing up, the jury retired to consider five verdicts, namely unlawful killing by the negligence of either or both the following vehicles or Paul; accidental death or an open verdict.[62] The jury decided on 7 April 2008 that Diana had been unlawfully killed by the following vehicles.[10][65]		The cost of the inquiry exceeded £12.5 million, with the coroner's inquest at £4.5 million, and a further £8 million spent on the Metropolitan Police investigation. It lasted 6 months and heard 250 witnesses, with the cost heavily criticised in the media.[66]		Diana's death occurred at a time when Internet use in the developed world was booming, and several national newspapers and at least one British regional newspaper had already launched online news services. BBC News had set up online coverage of the general election earlier in 1997[67] and as a result of the widespread public and media attention that Diana's death resulted in, BBC News swiftly created a website featuring news coverage of Diana's death and the events that followed it. Diana's death helped BBC News officials realise how important online news services were becoming, and a full online news service was launched on 4 November that year, alongside the launch of the BBC's rolling news channel BBC News 24 on 9 November.[68]		Coordinates: 48°51′51.7″N 2°18′06.8″E﻿ / ﻿48.864361°N 2.301889°E﻿ / 48.864361; 2.301889		
Admiral of the Fleet Louis Francis Albert Victor Nicholas Mountbatten, 1st Earl Mountbatten of Burma, KG GCB OM GCSI GCIE GCVO DSO PC FRS[1] (born Prince Louis of Battenberg; 25 June 1900 – 27 August 1979) was a British naval officer and statesman, an uncle of Prince Philip, Duke of Edinburgh, and second cousin once removed of Elizabeth II. During the Second World War, he was Supreme Allied Commander, South East Asia Command (1943–46). He was the last Viceroy of India (1947) and the first Governor-General of independent India (1947–48).		From 1954 until 1959 he was First Sea Lord, a position that had been held by his father, Prince Louis of Battenberg, some forty years earlier. Thereafter he served as Chief of the Defence Staff until 1965, making him the longest serving professional head of the British Armed Forces to date. During this period Mountbatten also served as Chairman of the NATO Military Committee for a year.		In 1979, Mountbatten, his grandson Nicholas, and two others were killed by the Provisional Irish Republican Army (IRA), which had placed a bomb in his fishing boat, Shadow V, in Mullaghmore, County Sligo, Ireland.						From the time of his birth at Frogmore House in the Home Park, Windsor, Berkshire until 1917, when he and several other relations of King George V dropped their German styles and titles, Mountbatten was known as His Serene Highness Prince Louis of Battenberg. He was the youngest child and the second son of Prince Louis of Battenberg and his wife Princess Victoria of Hesse and by Rhine. His maternal grandparents were Louis IV, Grand Duke of Hesse, and Princess Alice of the United Kingdom, who was a daughter of Queen Victoria and Prince Albert of Saxe-Coburg and Gotha. His paternal grandparents were Prince Alexander of Hesse and by Rhine and Julia, Princess of Battenberg.[2]		His paternal grandparents' marriage was morganatic because his grandmother was not of royal lineage; as a result, he and his father were styled "Serene Highness" rather than "Grand Ducal Highness", were not eligible to be titled Princes of Hesse and were given the less exalted Battenberg title. His siblings were Princess Andrew of Greece and Denmark (mother of Prince Philip, Duke of Edinburgh), Queen Louise of Sweden, and George Mountbatten, 2nd Marquess of Milford Haven.[2]		Young Mountbatten's nickname among family and friends was "Dickie", although "Richard" was not among his given names. This was because his great-grandmother, Queen Victoria, had suggested the nickname of "Nicky", but to avoid confusion with the many Nickys of the Russian Imperial Family ("Nicky" was particularly used to refer to Nicholas II, the last Tsar), "Nicky" was changed to "Dickie".[3]		Mountbatten was educated at home for the first 10 years of his life: he was then sent to Lockers Park School in Hertfordshire[4] and on to the Royal Naval College, Osborne in May 1913.[5] In childhood he visited the Imperial Court of Russia at St Petersburg and became intimate with the doomed Russian Imperial Family, harbouring romantic feelings towards his maternal first cousin Grand Duchess Maria Nikolaevna, whose photograph he kept at his bedside for the rest of his life.[6]		Mountbatten was posted as midshipman to the battlecruiser HMS Lion in July 1916 and, after seeing action in August 1916, transferred to the battleship HMS Queen Elizabeth during the closing phases of the First World War.[5] In June 1917, when the Royal Family stopped using their German names and titles and adopted the more British-sounding "Windsor", Prince Louis of Battenberg became Louis Mountbatten, and was created Marquess of Milford Haven. His second son acquired the courtesy title Lord Louis Mountbatten and was known as Lord Louis until he was created a peer in 1946.[7] He paid a visit of ten days to the Western Front, in July 1918.[8]		He was appointed executive officer (second-in-command) of the small warship HMS P.31 on 13 October 1918 and was promoted sub-lieutenant on 15 January 1919. HMS P.31 took part in the Peace River Pageant on 4 April 1919. Mountbatten attended Christ's College, Cambridge for two terms, starting in October 1919, where he studied English literature (including John Milton and Lord Byron) in a programme that was specially designed for ex-servicemen.[9][10][11] He was elected for a term to the Standing Committee of the Cambridge Union Society, and was suspected of sympathy for the Labour Party, then emerging as a potential party of government for the first time.[12]		He was posted to the battlecruiser HMS Renown in March 1920 and accompanied Edward, Prince of Wales, on a royal tour of Australia in her.[7] He was promoted lieutenant on 15 April 1920.[13] HMS Renown returned to Portsmouth on 11 October 1920.[14] Early in 1921 Royal Navy personnel were used for civil defence duties as serious industrial unrest seemed imminent. Mountbatten had to command a platoon of stokers, many of whom had never handled a rifle before, in northern England.[14] He transferred to the battlecruiser HMS Repulse in March 1921 and accompanied the Prince of Wales on a Royal tour of India and Japan.[7][15] Edward and Mountbatten formed a close friendship during the trip.[7] Mountbatten survived the deep defence cuts known as the Geddes Axe. Fifty-two percent of the officers of his year had had to leave the Royal Navy by the end of 1923; although he was highly regarded by his superiors, it was rumoured that wealthy and well-connected officers were more likely to be retained.[16] He was posted to the battleship HMS Revenge in the Mediterranean Fleet in January 1923.[7]		Pursuing his interests in technological development and gadgetry, Mountbatten joined the Portsmouth Signals School in August 1924 and then went on briefly to study electronics at the Royal Naval College, Greenwich.[7] Mountbatten became a Member of the Institution of Electrical Engineers (IEE), now the Institution of Engineering and Technology (IET), which annually awards the Mountbatten Medal for an outstanding contribution, or contributions over a period, to the promotion of electronics or information technology and their application.[17] He was posted to the battleship HMS Centurion in the Reserve Fleet in 1926 and became Assistant Fleet Wireless and Signals Officer of the Mediterranean Fleet under the command of Admiral Sir Roger Keyes in January 1927.[7] Promoted lieutenant-commander on 15 April 1928,[18] he returned to the Signals School in July 1929 as Senior Wireless Instructor.[7] He was appointed Fleet Wireless Officer to the Mediterranean Fleet in August 1931, and having been promoted commander on 31 December 1932,[19] was posted to the battleship HMS Resolution.[7]		In 1934, Mountbatten was appointed to his first command – the destroyer HMS Daring.[7] His ship was a new destroyer which he was to sail to Singapore and exchange for an older ship, HMS Wishart.[7] He successfully brought Wishart back to port in Malta and then attended the funeral of King George V in January 1936.[20] Mountbatten was appointed a Personal Naval Aide-de-Camp to King Edward VIII on 23 June 1936,[21] and, having joined the Naval Air Division of the Admiralty in July 1936,[1] he attended the coronation of King George VI in May 1937.[22] He was promoted Captain on 30 June 1937[23] and was then given command of the destroyer HMS Kelly in June 1939.[24]		In July 1939, Mountbatten was granted a patent (UK Number 508,956) for a system for maintaining a warship in a fixed position relative to another ship.[25]		When war broke out in September 1939, Mountbatten became commander of the 5th Destroyer Flotilla aboard HMS Kelly, which became famous for its exploits.[1] In late 1939 he brought the Duke of Windsor back from exile in France and in early May 1940, Mountbatten led a British convoy in through the fog to evacuate the Allied forces participating in the Namsos Campaign during the Norwegian Campaign.[24]		On the night of 9/10 May 1940, Kelly was torpedoed amidships by a German E-boat S 31 off the Dutch coast, and Mountbatten thereafter commanded the 5th Destroyer Flotilla from the destroyer HMS Javelin.[24] He rejoined Kelly in December 1940, by which time the torpedo damage had been repaired.[24]		Kelly was sunk by German dive bombers on 23 May 1941 during the Battle of Crete;[26] the incident serving as the basis for Noël Coward's film In Which We Serve.[27] Coward was a personal friend of Mountbatten and copied some of his speeches into the film.[26] Mountbatten was mentioned in despatches on 9 August 1940[28] and 21 March 1941[29] and awarded the Distinguished Service Order in January 1941.[30]		In August 1941, Mountbatten was appointed captain of the aircraft carrier HMS Illustrious which lay in Norfolk, Virginia, for repairs following action at Malta in the Mediterranean in January.[26] During this period of relative inactivity, he paid a flying visit to Pearl Harbor, three months before the Japanese attack on the US naval base there. Mountbatten, appalled at the lack of preparedness at the naval base, drawing on the Japanese history of launching wars with surprise attacks as well as the successful British surprise attack at the Battle of Taranto which had effectively knocked the Italian fleet out of the war, and the sheer effectiveness of aircraft against warships, accurately predicted that the US entry into the war would begin with a Japanese surprise attack on Pearl Harbor.[26][31]		Mountbatten was a favourite of Winston Churchill.[32] On 27 October 1941 Mountbatten replaced Roger Keyes as Chief of Combined Operations and promoted commodore.[26]		His duties in this role included inventing new technical aids to assist with opposed landings.[1] Noteworthy technical achievements of Mountbatten and his staff include the construction of "PLUTO", an underwater oil pipeline from the English coast to Normandy, an artificial harbour constructed of concrete caissons and sunken ships, and the development of amphibious tank-landing ships.[1] Another project that Mountbatten proposed to Churchill was Project Habakkuk. It was to be a massive and impregnable 600-metre aircraft carrier made from reinforced ice ("Pykrete"): Habakkuk was never carried out due to its enormous cost.[1]		In his role with Combined Operations, Mountbatten also planned commando raids across the English Channel. As acting vice-admiral in March 1942 (and honorary ranks of Lieutenant General, and an Air Marshal),[33] he was in large part responsible for the planning and organisation of The Raid at St. Nazaire in mid-1942, an operation which put out of action one of the most heavily defended docks in Nazi-occupied France until well after war's end, the ramifications of which contributed to allied supremacy in the Battle of the Atlantic. He personally pushed through the disastrous Dieppe Raid of 19 August 1942, (which some among the Allied forces, notably Field Marshal Montgomery, later claimed was ill-conceived from the start). The raid on Dieppe was a disaster, with casualties (including those wounded or taken prisoner) numbering in the thousands, the great majority of them Canadians.[26] Historian Brian Loring Villa concluded that Mountbatten conducted the raid without authority, but that his intention to do so was known to several of his superiors, who took no action to stop him.[34] As a result of the Dieppe raid, Mountbatten became a controversial figure in Canada,[34] with the Royal Canadian Legion distancing itself from him during his visits there during his later career; his relations with Canadian veterans "remained frosty".[35]		Mountbatten claimed that the lessons learned from the Dieppe Raid were necessary for planning the Normandy invasion on D-Day nearly two years later. However, military historians such as former Royal Marine Julian Thompson have written that these lessons should not have needed a debacle such as Dieppe to be recognised.[36] Nevertheless, as a direct result of the failings of the Dieppe raid, the British made several innovations, most notably Hobart's Funnies – specialized armoured vehicles which, in the course of the Normandy Landings, undoubtedly saved many lives on those three beachheads upon which Commonwealth soldiers were landing (Gold Beach, Juno Beach, and Sword Beach).[37]		In August 1943, Churchill appointed Mountbatten the Supreme Allied Commander South East Asia Command (SEAC) with promotion to acting full admiral.[26] His less practical ideas were sidelined by an experienced planning staff led by Lieutenant-Colonel James Allason, though some, such as a proposal to launch an amphibious assault near Rangoon, got as far as Churchill before being quashed.[38]		British interpreter Hugh Lunghi recounted an embarrassing episode which occurred during the Potsdam Conference, when Mountbatten, desiring to receive an invitation to visit the Soviet Union, repeatedly attempted to impress Josef Stalin with his former connections to the Russian imperial family. The attempt fell predictably flat, with Stalin dryly inquiring whether "it was some time ago that he had been there." Says Lunghi, "The meeting was embarrassing because Stalin was so unimpressed. He offered no invitation. Mountbatten left with his tail between his legs."[39]		During his time as Supreme Allied Commander of the Southeast Asia Theatre, his command oversaw the recapture of Burma from the Japanese by General William Slim.[40] A personal high point was the receipt of the Japanese surrender in Singapore when British troops returned to the island to receive the formal surrender of Japanese forces in the region led by General Itagaki Seishiro on 12 September 1945, codenamed Operation Tiderace.[41] South East Asia Command was disbanded in May 1946 and Mountbatten returned home with the substantive rank of rear-admiral.[42]		Following the war, Mountbatten was known to have largely shunned the Japanese for the rest of his life out of respect for his men killed during the war, and as per his will, Japan was not invited to send diplomatic representatives to his funeral in 1979, though he did meet Emperor Hirohito during a state visit to Britain in 1971, reportedly at the urging of the Queen.[43]		His experience in the region and in particular his perceived Labour sympathies at that time led to Clement Attlee appointing him Viceroy of India on 20 February 1947[44][45] charged with overseeing the transition of British India to independence no later than 30 June 1948. Mountbatten's instructions emphasised a united India as a result of the transference of power but authorised him to adapt to a changing situation in order to get Britain out promptly with minimal reputational damage.[46] Soon after he arrived, Mountbatten concluded that the situation was too volatile for even that short a wait. Although his advisers favoured a gradual transfer of independence, Mountbatten decided the only way forward was a quick and orderly transfer of independence before 1947 was out. In his view, any longer would mean civil war.[47] The Viceroy also hurried so he could return to his senior technical Navy courses.[48][49]		Mountbatten was fond of Congress leader Jawaharlal Nehru and his liberal outlook for the country. He felt differently about the Muslim leader Muhammed Ali Jinnah, but was aware of his power, stating "If it could be said that any single man held the future of India in the palm of his hand in 1947, that man was Mohammad Ali Jinnah."[49] During his meeting with Jinnah on 5 April 1947,[50] Mountbatten tried to persuade Jinnah of a united India, citing the difficult task of dividing the mixed states of Punjab and Bengal, but the Muslim leader was unyielding in his goal of establishing a separate Muslim state called Pakistan.[51]		Given the British government's recommendations to grant independence quickly, Mountbatten concluded that a united India was an unachievable goal and resigned himself to a plan for partition, creating the independent nations of India and Pakistan.[1] Mountbatten set a date for the transfer of power from the British to the Indians, arguing that a fixed timeline would convince Indians of his and the British government's sincerity in working towards a swift and efficient independence, excluding all possibilities of stalling the process.[52]		Among the Indian leaders, Mahatma Gandhi emphatically insisted on maintaining a united India and for a while successfully rallied people to this goal. During his meeting with Mountbatten, Gandhi asked Mountbatten to invite Jinnah to form a new Central government, but Mountbatten never uttered a word of Gandhi's ideas to Jinnah.[53] And when Mountbatten's timeline offered the prospect of attaining independence soon, sentiments took a different turn. Given Mountbatten's determination, Nehru and Patel's inability to deal with the Muslim League and lastly Jinnah's obstinacy, all Indian party leaders (except Gandhi) acquiesced to Jinnah's plan to divide India,[54] which in turn eased Mountbatten's task. Mountbatten also developed a strong relationship with the Indian princes, who ruled those portions of India not directly under British rule. His intervention was decisive in persuading the vast majority of them to see advantages in opting to join the Indian Union. On one hand, the integration of the princely states can be viewed as one of the positive aspects of his legacy.[55] But on the other, the refusal of Hyderabad, Jammu and Kashmir, and Junagadh to join one of the dominions led to future tension between Pakistan and India.[56]		Mountbatten brought forward the date of the partition from June 1948 to 15 August 1947.[57] The uncertainty of the borders caused Muslims and Hindus to move into the direction where they felt they would get the majority. Hindus and Muslims were thoroughly terrified, and the Muslim movement from the East was balanced by the similar movement of Hindus from the West.[58] A boundary committee chaired by Sir Cyril Radcliffe was charged with drawing boundaries for the new nations. With a mandate to leave as many Hindus and Sikhs in India and as many Muslims in Pakistan as possible, Radcliffe came up with a map that split the two countries along the Punjab and Bengal borders. This left 14 million people on the "wrong" side of the border, and very many of them fled to "safety" on the other side when the new lines were announced.[47]		When India and Pakistan attained independence at midnight on the night of 14–15 August 1947, Mountbatten remained in New Delhi for 10 months, serving as India's first governor general until June 1948.[59] On Mountbatten's advice, India took the issue of Kashmir to the newly formed United Nations in January 1948. The issue of Kashmir would become a lasting thorn in his legacy, one that is not resolved to this day.[60] Accounts differ on the future Mountbatten desired for Kashmir. Pakistani accounts suggest that Mountbatten favored the accession of Kashmir to India citing his close relationship to Nehru. Mountbatten's own account says that he simply wanted the maharaja Hari Singh to make up his mind. The viceroy made several attempts to mediate between the Congress leaders, Muhammad Ali Jinnah and Hari Singh on issues relating to the accession of Kashmir though he was largely unsuccessful in resolving the conflict.[61] After the tribal invasion of Kashmir, it was on his suggestion that India moved to secure the accession of Kashmir from Hari Singh before sending in military forces for his defence.[62]		Notwithstanding the self-promotion of his own part in Indian independence – notably in the television series The Life and Times of Admiral of the Fleet Lord Mountbatten of Burma, produced by his son-in-law Lord Brabourne, and Freedom at Midnight by Dominique Lapierre and Larry Collins (of which he was the main quoted source) – his record is seen as very mixed; one common view is that he hastened the independence process unduly and recklessly, foreseeing vast disruption and loss of life and not wanting this to occur on the British watch, but thereby actually helping it to occur, especially in Punjab and Bengal.[63] John Kenneth Galbraith, the Canadian-American Harvard University economist, who advised governments of India during the 1950s, an intimate of Nehru who served as the American ambassador from 1961 to 1963, was a particularly harsh critic of Mountbatten in this regard.[64]		The creation of Pakistan was never emotionally accepted by many British leaders, among them being Mountbatten.[65] Mountbatten clearly expressed his lack of support and faith in the Muslim League's idea of Pakistan.[66] Jinnah refused Mountbatten's offer to serve as Governor-General of Pakistan.[67] When Mountbatten was asked by Collins and Lapierre if he would have sabotaged Pakistan had he known that Jinnah was dying of tuberculosis, he replied, "Most probably."[68]		After India, Mountbatten served as commander of the 1st Cruiser Squadron in the Mediterranean Fleet and, having been granted the substantive rank of vice admiral on 22 June 1949,[69] he became Second-in-Command of the Mediterranean Fleet in April 1950.[59] He became Fourth Sea Lord at the Admiralty in June 1950. He then returned to the Mediterranean to serve as Commander-in-Chief, Mediterranean Fleet and NATO Commander Allied Forces Mediterranean from June 1952.[59] He was promoted to the substantive rank of full admiral on 27 February 1953.[70] In March 1953, he was appointed Personal Aide-de-Camp to the Queen.[33]		Mountbatten served his final posting at the Admiralty as First Sea Lord and Chief of the Naval Staff from April 1955 to July 1959, the position which his father had held some forty years prior. This was the first time in Royal Naval history that a father and son had both attained such high rank.[71] He was promoted to Admiral of the Fleet on 22 October 1956.[72]		While serving as First Sea Lord, his primary concerns dealt with devising plans on how the Royal Navy would keep shipping lanes open if Britain fell victim to a nuclear attack. Today, this seems of minor importance but at the time few people comprehended the potentially limitless destruction nuclear weapons possess and the ongoing dangers posed by the fallout. Military commanders did not understand the physics involved in a nuclear explosion. This became evident when Mountbatten had to be reassured that the fission reactions from the Bikini Atoll tests would not spread through the oceans and blow up the planet.[73] As Mountbatten became more familiar with this new form of weaponry, he increasingly grew opposed to its use in combat yet at the same time he realised the potential nuclear energy had, especially with regards to submarines. Mountbatten expressed his feelings towards the use of nuclear weapons in combat in his article "A Military Commander Surveys The Nuclear Arms Race", which was published shortly after his death in International Security in the winter of 1979–80.[74] After leaving the Admiralty, Lord Mountbatten took the position of Chief of the Defence Staff.[59] He served in this post for six years during which he was able to consolidate the three service departments of the military branch into a single Ministry of Defence.[75]		Mountbatten was appointed Colonel of the Life Guards, Gold Stick in Waiting and Life Colonel Commandant of the Royal Marines in 1965.[33] He was Governor of the Isle of Wight from 20 July 1965[76] and then the first Lord Lieutenant of the Isle of Wight from 1 April 1974.[77]		Mountbatten was elected a Fellow of the Royal Society[1] and had received an Honorary Doctorate from Heriot-Watt University in 1968.[78]		In 1969, Mountbatten tried unsuccessfully to persuade his cousin, the Spanish pretender Infante Juan, Count of Barcelona, to ease the eventual accession of his son, Juan Carlos, to the Spanish throne by signing a declaration of abdication while in exile.[79] The next year Mountbatten attended an official White House dinner during which he took the opportunity to have a 20-minute conversation with Richard Nixon and Secretary of State William P. Rogers, about which he later wrote, "I was able to talk to the President a bit about both Tino (Constantine II of Greece) and Juanito (Juan Carlos of Spain) to try and put over their respective points of view about Greece and Spain, and how I felt the US could help them."[79] In January 1971, Nixon hosted Juan Carlos and his wife Sofia (sister of the exiled King Constantine) during a visit to Washington and later that year the Washington Post published an article alleging that Nixon's administration was seeking to get Franco to retire in favour of the young Bourbon prince.[79]		From 1967 until 1978, Mountbatten was president of the United World Colleges Organisation, then represented by a single college: that of Atlantic College in South Wales. Mountbatten supported the United World Colleges and encouraged heads of state, politicians and personalities throughout the world to share his interest. Under Mountbatten's presidency and personal involvement, the United World College of South East Asia was established in Singapore in 1971, followed by the United World College of the Pacific (now known as the Lester B Pearson United World College of the Pacific) in Victoria, British Columbia, in 1974. In 1978, Mountbatten passed the presidency of the college to his great-nephew, the Prince of Wales.[80]		Peter Wright, in his book Spycatcher, claimed that in May 1968 Mountbatten attended a private meeting with press baron Cecil King, and the Government's Chief Scientific Adviser, Solly Zuckerman. Wright alleged that "up to thirty" MI5 officers had joined a secret campaign to undermine the crisis-stricken Labour government of Harold Wilson and that King was an MI5 agent. In the meeting, King allegedly urged Mountbatten to become the leader of a government of national salvation. Solly Zuckerman pointed out that it was "rank treachery", and the idea came to nothing because of Mountbatten's reluctance to act.[81]		In 2006, the BBC documentary The Plot Against Harold Wilson alleged that there had been another plot involving Mountbatten to oust Wilson during his second term in office (1974–76). The period was characterised by high inflation, increasing unemployment and widespread industrial unrest. The alleged plot revolved around right-wing former military figures who were supposedly building private armies to counter the perceived threat from trade unions and the Soviet Union. They believed that the Labour Party, which was (and still is[update]) partly funded by affiliated trade unions, was unable and unwilling to counter these developments and that Wilson was either a Soviet agent or at the very least a Communist sympathiser – claims Wilson strongly denied. The documentary alleged that a coup was planned to overthrow Wilson and replace him with Mountbatten using the private armies and sympathisers in the military and MI5.[82]		The first official history of MI5, The Defence of the Realm (2009), tacitly confirmed that there was a plot against Wilson and that MI5 did have a file on him. Yet it also made clear that the plot was in no way official and that any activity centred on a small group of discontented officers. This much had already been confirmed by former cabinet secretary Lord Hunt, who concluded in a secret inquiry conducted in 1996 that "there is absolutely no doubt at all that a few, a very few, malcontents in MI5...a lot of them like Peter Wright who were right-wing, malicious and had serious personal grudges – gave vent to these and spread damaging malicious stories about that Labour government."[83]		Mountbatten was married on 18 July 1922 to Edwina Cynthia Annette Ashley, daughter of Wilfred William Ashley, later 1st Baron Mount Temple, himself a grandson of the 7th Earl of Shaftesbury. She was the favourite granddaughter of the Edwardian magnate Sir Ernest Cassel and the principal heir to his fortune.[7] There followed a honeymoon tour of European royal courts and America which included a visit to Niagara Falls (because "all honeymooners went there").[3]		Mountbatten admitted "Edwina and I spent all our married lives getting into other people's beds."[84] He maintained an affair for several years with Frenchwoman Yola Letellier,[85] Edwina and Jawaharlal Nehru became intimate friends after Indian Independence. During the summers, she would frequent the prime minister's house so she could lounge about on his veranda during the hot Delhi days. Personal correspondence between the two reveals a satisfying yet frustrating relationship. Edwina states in one of her letters. "Nothing that we did or felt would ever be allowed to come between you and your work or me and mine – because that would spoil everything."[86]		Lord and Lady Mountbatten had two daughters: Lady Patricia Mountbatten, Countess Mountbatten of Burma (born 14 February 1924, died 13 June 2017),[87] sometime lady-in-waiting to the Queen, and Lady Pamela Carmen Louise (Hicks) (born 19 April 1929), who accompanied them to India in 1947–48 and was also sometime lady-in-waiting to the Queen.[2]		Since Mountbatten had no sons, when he was created Viscount Mountbatten of Burma, of Romsey in the County of Southampton on 27 August 1946[88] and then Earl Mountbatten of Burma and Baron Romsey, in the County of Southampton on 28 October 1947,[89] the Letters Patent were drafted such that in the event he left no sons or issue in the male line, the titles could pass to his daughters, in order of seniority of birth, and to their heirs respectively.[90]		Like many members of the royal family, Mountbatten was an aficionado of polo. He received U.S. patent 1,993,334 in 1931 for a polo stick.[91] Mountbatten introduced the sport to the Royal Navy in the 1920s, and wrote a book on the subject.[3] He also served as Commodore of Emsworth Sailing Club in Hampshire from 1931.[92]		Mountbatten was a strong influence in the upbringing of his grand-nephew, Charles, Prince of Wales, and later as a mentor – "Honorary Grandfather" and "Honorary Grandson", they fondly called each other according to the Jonathan Dimbleby biography of the Prince – though according to both the Ziegler biography of Mountbatten and the Dimbleby biography of the Prince, the results may have been mixed. He from time to time strongly upbraided the Prince for showing tendencies towards the idle pleasure-seeking dilettantism of his predecessor as Prince of Wales, King Edward VIII, whom Mountbatten had known well in their youth. Yet he also encouraged the Prince to enjoy the bachelor life while he could and then to marry a young and inexperienced girl so as to ensure a stable married life.[93]		Mountbatten's qualification for offering advice to this particular heir to the throne was unique; it was he who had arranged the visit of King George VI and Queen Elizabeth to Dartmouth Royal Naval College on 22 July 1939, taking care to include the young Princesses Elizabeth and Margaret in the invitation, but assigning his nephew, Cadet Prince Philip of Greece, to keep them amused while their parents toured the facility. This was the first recorded meeting of Charles's future parents.[94] But a few months later, Mountbatten's efforts nearly came to naught when he received a letter from his sister Alice in Athens informing him that Philip was visiting her and had agreed to permanently repatriate to Greece. Within days, Philip received a command from his cousin and sovereign, King Geórgios II of the Hellenes, to resume his naval career in Britain which, though given without explanation, the young prince obeyed.[95]		In 1974, Mountbatten began corresponding with Charles about a potential marriage to his granddaughter, Hon. Amanda Knatchbull.[96] It was about this time he also recommended that the 25-year-old prince get on with "sowing some wild oats".[96] Charles dutifully wrote to Amanda's mother (who was also his godmother), Lady Brabourne, about his interest. Her answer was supportive, but advised him that she thought her daughter still rather young to be courted.[97]		In February 1975, Charles visited New Delhi to play polo and was shown around Rashtrapati Bhavan, the former Viceroy's House, by Mountbatten.[98]		Four years later Mountbatten secured an invitation for himself and Amanda to accompany Charles on his planned 1980 tour of India.[97] Their fathers promptly objected. Prince Philip thought that the Indian public's reception would more likely reflect response to the uncle than to the nephew. Lord Brabourne counselled that the intense scrutiny of the press would be more likely to drive Mountbatten's godson and granddaughter apart than together.[97]		Charles was rescheduled to tour India alone, but Mountbatten did not live to the planned date of departure. When Charles finally did propose marriage to Amanda later in 1979, the circumstances were changed, and she refused him.[97]		In 1969 Mountbatten participated in a 12-part autobiographical television series Lord Mountbatten: A Man for the Century, also known as The Life and Times of Lord Mountbatten, produced by Associated-Rediffusion and scripted by historian John Terraine.[99][100] The episodes were:[101]		1. The King's Ships Were at Sea (1900–1917) 2. The Kings Depart (1917–1922) 3. Azure Main (1922–1936) 4. The Stormy Winds (1936–1941)		5. United We Conquer (1941–1943) 6. The Imperial Enemy 7. The March to Victory 8. The Meaning of Victory (1945–1947)		9. The Last Viceroy 10. Fresh Fields (1947–1955) 11. Full Circle (1955–1965) 12. A Man of This Century (1900–1968)		On 27 April 1977, shortly before his 77th birthday, Mountbatten became the first member of the Royal Family to appear on the TV guest show This Is Your Life.[102]		Mountbatten usually holidayed at his summer home, Classiebawn Castle, in Mullaghmore, a small seaside village in County Sligo, Ireland. The village was only 12 miles (19 km) from the border with Northern Ireland and near an area known to be used as a cross-border refuge by IRA members.[103][104] In 1978, the IRA had allegedly attempted to shoot Mountbatten as he was aboard his boat, but "choppy seas had prevented the sniper lining up his target".[105]		On 27 August 1979, Mountbatten went lobster-potting and tuna fishing in his 30-foot (9.1 m) wooden boat, Shadow V, which had been moored in the harbour at Mullaghmore.[104] IRA member Thomas McMahon had slipped onto the unguarded boat that night and attached a radio-controlled bomb weighing 50 pounds (23 kg). When Mountbatten was aboard, just a few hundred yards from the shore, the bomb was detonated. The boat was destroyed by the force of the blast, and Mountbatten's legs were almost blown off. Mountbatten, then aged 79, was pulled alive from the water by nearby fishermen, but died from his injuries before being brought to shore.[104][106][107] Also aboard the boat were his elder daughter Patricia (Lady Brabourne), her husband John (Lord Brabourne), their twin sons Nicholas and Timothy Knatchbull, John's mother Doreen, (dowager) Lady Brabourne, and Paul Maxwell, a young crew member from County Fermanagh.[108] Nicholas (aged 14) and Paul (aged 15) were killed by the blast and the others were seriously injured.[109] Doreen, Lady Brabourne (aged 83) died from her injuries the following day.[110]		The IRA issued a statement afterward, saying:		The IRA claim responsibility for the execution of Lord Louis Mountbatten. This operation is one of the discriminate ways we can bring to the attention of the English people the continuing occupation of our country. [...] The death of Mountbatten and the tributes paid to him will be seen in sharp contrast to the apathy of the British government and the English people to the deaths of over three hundred British soldiers, and the deaths of Irish men, women and children at the hands of their forces.[103][111]		Six weeks later,[112] Sinn Féin vice-president Gerry Adams said of Mountbatten's death:		The IRA gave clear reasons for the execution. I think it is unfortunate that anyone has to be killed, but the furor created by Mountbatten's death showed up the hypocritical attitude of the media establishment. As a member of the House of Lords, Mountbatten was an emotional figure in both British and Irish politics. What the IRA did to him is what Mountbatten had been doing all his life to other people; and with his war record I don't think he could have objected to dying in what was clearly a war situation. He knew the danger involved in coming to this country. In my opinion, the IRA achieved its objective: people started paying attention to what was happening in Ireland.[112]		On the day of the bombing, the IRA also ambushed and killed eighteen British soldiers in Northern Ireland, sixteen of them from the Parachute Regiment, in what became known as the Warrenpoint ambush. It was the deadliest attack on the British Army during the Troubles.[104]		On 5 September 1979 Mountbatten received a ceremonial funeral at Westminster Abbey, which was attended by the Queen, the Royal Family and members of the European royal houses. Watched by thousands of people, the funeral procession, which started at Wellington Barracks, included representatives of all three British Armed Services, and military contingents from Burma, India, the United States, France and Canada. His coffin was drawn on a gun carriage by 118 Royal Navy ratings. During the televised service, the Prince of Wales read the lesson from Psalm 107.[113] In an address, the Archbishop of Canterbury, Donald Coggan, highlighted his various achievements and his "lifelong devotion to the Royal Navy".[114] After the public ceremonies, which he had planned himself, Mountbatten was buried in Romsey Abbey.[115][116] As part of the funeral arrangements, his body had been embalmed by Desmond Henley.[117]		Thomas McMahon, who had been arrested two hours before the bomb detonated at a Garda checkpoint between Longford and Granard on suspicion of driving a stolen vehicle, was tried for the assassinations in Ireland, and convicted on 23 November 1979 by forensic evidence supplied by James O'Donovan that showed flecks of paint from the boat and traces of nitroglycerine on his clothes.[118] He was released in 1998 under the terms of the Good Friday Agreement.[104][119]		On hearing of Mountbatten's death the then Master of the Queen's Music, Malcolm Williamson, was moved to write the Lament in Memory of Lord Mountbatten of Burma for violin and string orchestra. The 11-minute work was given its first performance on 5 May 1980 by the Scottish Baroque Ensemble, conducted by Leonard Friedman.[120]		Mountbatten took pride in enhancing intercultural understanding and in 1984, with his elder daughter as the patron, the Mountbatten Institute was developed to allow young adults the opportunity to enhance their intercultural appreciation and experience by spending time abroad.[121]		The city of Ottawa, Ontario, erected Mountbatten Avenue in his memory. The avenue runs from Blossom Drive to Fairbanks Avenue.[122]		
Jest (1910–1921) was a British Thoroughbred racehorse and broodmare, best known for winning two Classics in 1913. The filly won four times from eight races in a track career which lasted from July 1912 until July 1913. As a two-year-old in 1912 she won twice from four starts. On her three-year-old debut she won the 1000 Guineas over one mile at Newmarket and then won the Oaks over one and a half miles at Epsom a month later. She was retired from racing after being beaten in the Coronation Stakes at Royal Ascot and the Nassau Stakes at Goodwood. As a broodmare she produced the 1921 Epsom Derby winner Humorist before dying at the age of eleven.						Jest was a small[1] chestnut mare with a white star bred by her owner Jack Barnato Joel, the South African mining magnate and three-time British flat racing Champion Owner.[2] Joel sent his filly to his private trainer Charles Morton at Letcombe Bassett in Berkshire.[3] Jest proved to be an extremely temperamental and difficult filly, giving Morton a great deal of trouble.[4]		Jest was sired by the sprinter Sundridge who won three editions of the July Cup before embarking on a successful stud career. Apart from Jest he sired the Epsom Derby winner Sunstar and the leading American runner Sun Briar, and was the British Champion sire in 1911.[5] Jest's dam was Absurdity, one of the foundation mare's of Joel's Childwick Bury stud: in addition to Jest she produced seven other winners including Black Jester (St Leger Stakes) and Absurd (Middle Park Stakes, champion sire in New Zealand).[6] Her later descendants included the classic winners Royal Palace, Fairy Footsteps (1000 Guineas) and Light Cavalry (St Leger).[7]		Jest began her racing career at Hurst Park in July, when she finished third to Prue and Light Brigade (a colt) in the Hurst Park Foal Plate. After a break of two months she returned in September for the Autumn Breeders' Foal Plate at Manchester. She finished second by a neck to Waiontha, who was regarded as the year's best two-year-old filly. At Newmarket in October she recorded her first success when winning the Bretby Stakes by three lengths from Streamlet. Later that month she defeated a field which included the subsequent Derby winner Aboyeur when winning the Free Handicap at Newmarket.[8]		In attempt to curb the filly's temperament, Morton sent Jest to Joel's stud farm to be covered by a stallion, but the experiment was not a success.[4] Before racing in public, she finished third to her stable companions Radiant and Sun Yat in a private trial race.[9] On her official three-year-old debut, she started at odds of 9/1 in a field of twenty-two for the 1000 Guineas over Newmarket's Rowley Mile. Ridden by Fred Rickaby, she won the race by a head and half a length from Taslett and Prue. The result was only confirmed after the racecourse stewards overruled an objection lodged by the rider of the runner up, who claimed that he had been bumped by the winner.[4] Danny Maher, riding Prue, raced on the opposite side of the wide course from the first two fillies and believed he had won.[10]		Jest was then moved up in distance for the Oaks a month later, when she ran in front of a crowd which included King George V. With Rickaby again in the saddle, she started at 8/1 against eleven opponents. She won comfortably by two lengths from Depeche, with Arda half a length away in third.[11] The winning time of 2:37.6 was a record for the race[12] and was identical to that recorded by Craganour and Aboyeur in the controversial "Suffragette Derby" two days earlier.[13]		Jest's two classic wins meant that she had to carry weight penalties her two remaining races. At Royal Ascot she was brought back in distance for the one mile Coronation Stakes and finished third to Prue and Sands of Time, fillies to whom she was conceding fourteen pounds. A month later she was sent to Goodwood for the ten furlong Nassau Stakes. She finished second, beaten less than a length by Arda, carrying twelve pounds more than the winner, in what was described as an unsatisfactory race.[9] Jest was then retired from racing: she had never been entered for the St Leger and was therefore unable to attempt the Fillies' Triple Crown.[14]		In their book, A Century of Champions, based on the Timeform rating system, John Randall and Tony Morris rated Jest an "average" winner of the 1000 Guineas and an "inferior" winner of the Oaks.[13]		Jack Joel regarded Jest as the best filly he ever bred.[15]		Jest was retired to become a broodmare at her owner's Childwick Bury stud. She failed to produce a foal until 1918 when she gave birth to a chestnut colt by Polymelus. Named Humorist, he won the 1921 Epsom Derby before dying of tuberculosis a month later. Jest produced two other foals, the more notable being Chief Ruler, an unraced colt who was twice champion sire in New Zealand.[6] Jest died in foaling in 1921.		
Blonde jokes are a class of jokes based on a stereotype of a dumb blonde woman.[1]		These jokes about people, generally women, who have blonde hair serve as a form of blonde versus brunette rivalry. They are often considered to be derogatory as many are mere variants on traditional ethnic jokes or jests about other identifiable groups that would be considered more offensive (such as Italian jokes involving Carabinieri).		In some cases, jokes about stereotypically stupid people have circulated since the seventeenth century with only the wording and targeted groups changed.[2]		Some blonde jokes rely on sexual humour to portray or stereotype their subjects as promiscuous.[3] Many of these are rephrased sorority girl or Essex girl jokes,[2] much as other jokes about dumb blondes are based on long-running ethnic jokes.						Blonde jokes nearly always take the format of the blonde placing herself in a situation or making a comment that serves to highlight her supposed promiscuity and/or lack of intelligence, cluelessness and clumsiness. The blonde of the joke is often placed in an unusual situation with a brunette or redhead.		The emergence of a class of meta-jokes about blondes ("meta-blonde jokes", i.e., jokes about blonde jokes) is noted. In a typical plot of this type a blonde complains about the unfairness of the stereotype propagated by blonde jokes, with a punch line actually reinforcing the stereotype.[4] An example is about a blonde objecting to a ventriloquist act packed with sexist jokes about blondes:		Blonde: "I’ve heard enough of your stupid blonde jokes. What makes you think you can stereotype women that way? What does the color of a person’s hair have to do with her worth as a human being? It’s men like you that keep women like me from being respected at work and in the community and from reaching our full potential as a person. Because you and men like you continue to perpetuate discrimination against not only blondes, but women in general, and all in the name of humor!"		Ventriloquist: "I'm sorry ma'am but..."		Blonde: "You stay out of this, mister! I’m talking to that little idiot on your knee."		The British Essex girl joke, very similar in content, became popular in the late 1980s; it satirises working-class girls from the county of Essex.		Like all humour based on stereotypes, blonde jokes are considered offensive to many people,[5] particularly women.[6] However, blonde jokes, unlike other jokes that target minorities, do appear in mainstream media such as newspapers and television.		Blonde jokes have been criticized as sexist by several authors, largely because the target is invariably dim-witted, female and sexually promiscuous.[7]		
Newfie (also Newf or sometimes Newfy) is a colloquial term used in Canada for someone who is from Newfoundland, most often to refer to someone proud from that place. Some Newfoundlanders consider "Newfie" a slur from its past use by American and Canadian military personnel present on the island and its use in Newfie jokes that depicted "Newfies" as stupid and foolish.[1] [2] The word was first recorded in a 1942 dictionary of slang. However, at this time Newfie only referred to Newfoundland itself; a Newfoundlander was a Newfier.		During the Second World War, sailors on convoy duty nicknamed St. John's (the capital of Newfoundland) Newfiejohn.[3]						The first edition of the Gage Canadian Dictionary, published in 1983, and the second edition of the Random House Unabridged Dictionary, published in 1987, both include usage notes describing the term 'Newfie' as offensive. However, neither the second edition of the Canadian Oxford Dictionary, published in 2004, or the current edition of the Dictionary of Newfoundland English, published in 1998, make such a mention.[4]		In March 2006, an Edmonton police officer was disciplined for using the word Newphie [sic] to describe the apprehension of an individual under the Mental Health Act.[5]		In the 1970s, the Government of Alberta added the term 'Newfie' to a list of words not allowed to be used on personalised licence plates, reasoning that it was an ethnic slur and hateful. In 2006, a man from Newfoundland argued that it was also a source of pride, and fought to have the word removed from the list, and he eventually won. As of 2008, the ban has been lifted.[6]		
A smiley (sometimes simply called a happy face or smiling face) is a stylized representation of a smiling humanoid face that is a part of popular culture worldwide. The classic form designed by Harry Ball in 1963 comprises a yellow circle with two black dots representing eyes and a black arc representing the mouth (☺). On the Internet and in other plain text communication channels, the emoticon form (sometimes also called the smiley-face emoticon) has traditionally been most popular, typically employing a colon and a right parenthesis to form sequences like :^), :), or (: that resemble a smiling face when viewed after rotation through 90 degrees. "Smiley" is also sometimes used as a generic term for any emoticon. The smiley has been referenced in nearly all areas of Western culture including music, movies, and art. The smiley has also been associated with late 1980s and early 1990s rave culture.[1][2][3]		The plural form "smilies" is commonly used,[4] but the variant spelling "smilie" is not as common as the "y" spelling.[5]						In or about 2017, a team of archaeologists led by Nicolò Marchetti of the University of Bologna pieced together the fragments of a Hittite pot from approximately 1700 BCE that had been found in Karkamış, Turkey. After it was pieced together, the team saw that it had what appeared to be a large smiley face painted on it.[6]		The poet and author Johannes V. Jensen was amongst other things famous for experimenting with the form of his writing. In a letter sent to publisher Ernst Bojesen in December 1900 he includes both a happy face and a sad face, resembling the modern smiley.		A commercial version of a smiley face with the word "THANKS" above it was available in 1919 and applied as a sticker on receipts issued by the Buffalo Steam Roller Company in Buffalo New York. The round face was much more detailed than the one depicted above, having eyebrows, nose, teeth, chin, facial creases and shading, and is reminiscent of "man-in-the-moon" style characterizations.		Ingmar Bergman's 1948 film Port of Call includes a scene where the unhappy Berit draws a sad face – closely resembling the modern "frowny", but including a dot for the nose – in lipstick on her mirror, before being interrupted.[7] In 1953 and 1958, similar happy faces were used in promotional campaigns for the films Lili and Gigi.		The smiley was first introduced to popular culture as part of a promotion by New York radio station WMCA beginning in 1962. Listeners who answered their phone "WMCA Good Guys!" were rewarded with a "WMCA good guys" sweatshirt that incorporated a happy face into its design. Thousands of these sweatshirts were given away.[8][9][10] The WMCA smiley was yellow with black dots as eyes, but it had a slightly crooked smile instead of a full smile, and no creases in the mouth.[10]		According to the Smithsonian Institution, the smiley face as we know it today was created by Harvey Ross Ball, an American graphic artist.[11] In 1963, Harvey Ball was employed by State Mutual Life Assurance Company of Worcester, Massachusetts (now known as Hanover Insurance) to create a happy face to raise the morale of the employees. Ball created the design in ten minutes and was paid $45 (equivalent to US$330 in 2012 currency). His rendition, with bright yellow background, dark oval eyes, full smile and creases at the sides of the mouth,[10] was imprinted on more than fifty million buttons and became familiar around the world. The design is so simple that it is certain that similar versions were produced before 1963, including those cited above. However, Ball’s rendition, as described here, has become the most iconic version.[9][12] In 1967, Seattle graphic artist George Tenagi drew his own version at the request of advertising agent, David Stern. Tenagi's design was used in an advertising campaign for Seattle-based University Federal Savings & Loan. The ad campaign was inspired by Charles Strouse' lyrics in "Put on a Happy Face" from the musical Bye Bye Birdie. Stern, the man behind this campaign, incorporated the Happy Face in his run for Seattle mayor in 1993.[12]		The graphic was further popularized in the early 1970s by Philadelphia brothers Bernard and Murray Spain, who seized upon it in September 1970 in a campaign to sell novelty items. The two produced buttons as well as coffee mugs, t-shirts, bumper stickers and many other items emblazoned with the symbol and the phrase "Have a happy day" (devised by Gyula Bogar),[13] which mutated into "Have a nice day". Working with New York button manufacturer NG Slater, some 50 million happy face badges were produced by 1972.[14]		In 1972, Frenchman Franklin Loufrani became the first person to legally trademark the smiley face. He used it to highlight the good news parts of the newspaper France Soir. He simply called the design "Smiley" and launched the Smiley Company. In 1996 Loufrani's son Nicolas Loufrani took over the family business and transformed it into a huge multinational corporation. Nicolas was outwardly skeptical of Harvey Ball's claim to creating the first smiley face. After all, the design that his father came up with and Ball's design were nearly identical. Loufrani argued that the design is so simple that no one person can lay claim to having created it. As evidence for this, Loufrani's website points to early cave paintings found in France (2500 BC) that he claims are the first depictions of a smiley face. Loufrani also points to a 1960 radio ad campaign that reportedly made use of a similar design.[15]		In the UK, the happy face has been associated with psychedelic culture since Ubi Dwyer and the Windsor Free Festival in the 1970s and the electronic dance music culture, particularly with acid house, that emerged during the Second Summer of Love in the late 1980s. The association was cemented when the band Bomb the Bass used an extracted smiley from Watchmen on the centre of its Beat Dis hit single.		The earliest known smiley-like image in a written document was drawn by a Slovak notary to indicate his satisfaction with the state of his town's municipal financial records in 1635.[16] A disputed early use of the smiley in a printed text may have been in Robert Herrick's poem To Fortune (1648),[17] which contains the line "Upon my ruines (smiling yet :)". Journalist Levi Stahl has suggested that this may have been an intentional "orthographic joke", while this occurrence is likely merely the colon placed inside parentheses rather than outside of them as is standard typographic practice today -- (smiling yet): . There are citations of similar punctuation in a non-humorous context, even within Herrick's own work.[18] It is likely that the parenthesis was added later by modern editors.[19]		On the Internet, the smiley has become a visual means of conveyance that uses images. The first known mention on the Internet was on September 19, 1982, when Scott Fahlman from Carnegie Mellon University wrote:		In Software, yellow graphical smileys have been used for many different purposes, including games.[22] One of the earliest uses of smileys in chat systems was in Yahoo! Messenger from 1998, where it can be seen in the user list next to each user, and it was also used as an icon for the application. In 2001, SmileyWorld launched the website "The official Smiley dictionary",[23] with smileys proposed to replace ASCII emoticons (i.e. emojis). In November 2001, and later, smiley emojis inside the actual chat text was adopted by several chat systems, including Yahoo Messenger.		The smiley is the printable version of characters 1 and 2 of (black-and-white versions of) codepage 437 (1981) of the first IBM PC and all subsequent PC compatible computers. For modern computers, all versions of Microsoft Windows after Windows 95[24] can use the smiley as part of Windows Glyph List 4, although some computer fonts miss some characters, and some characters cannot be reproduced by programs not compatible with Unicode.[25] It also appears in Unicode's Basic Multilingual Plane.[26]		The rights to the Smiley trademark in one hundred countries are owned by the Smiley Company.[27] Its subsidiary SmileyWorld Ltd, in London, headed by Nicolas Loufrani, creates or approves all the Smiley products sold in countries where it holds the trademark.[citation needed] The Smiley brand and logo have significant exposure through licensees in sectors such as clothing, home decoration, perfumery, plush, stationery, publishing, and through promotional campaigns.[28] The Smiley Company is one of the 100 biggest licensing companies in the world, with a turnover of US$167 million in 2012.[29] The first Smiley shop opened in London in the Boxpark shopping centre in December 2011.[30]		In 1997, Franklin Loufrani and Smiley World attempted to acquire trademark rights to the symbol (and even to the word "smiley" itself) in the United States. This brought Loufrani into conflict with Wal-Mart, which had begun prominently featuring a happy face in its "Rolling Back Prices" campaign over a year earlier. Wal-Mart responded first by trying to block Loufrani's application, then later by trying to register the smiley face itself; Loufrani in turn sued to stop Wal-Mart's application, and in 2002 after the issue went to court,[31] where it would languish for seven years before a decision.		Wal-Mart began phasing out the smiley face on its vests[32] and its website[33] in 2006. Despite that, Wal-Mart sued an online parodist for alleged "trademark infringement" after he used the symbol (as well as various portmanteaus of "Wal-", such as "Walocaust"). The District Court found in favor of the parodist when in March 2008, the judge concluded that Wal-Mart's smiley face logo was not shown to be "inherently distinctive" and that it "has failed to establish that the smiley face has acquired secondary meaning or that it is otherwise a protectible trademark" under U.S. law.[34]		In June 2010, Wal-Mart and the Smiley Company founded by Loufrani settled their 10-year-old dispute in front of the Chicago federal court. The terms remain confidential.[35] In 2016, Wal-Mart brought back the smiley face on its website, social media profiles, and in selected stores.[36]		
Psychology Today is a magazine published every two months in the United States.						Founded in 1967[2] by Nicolas Charney, Ph.D, its intent is to make psychology literature more accessible to the general public. The magazine focuses on behavior and covers a range of topics including psychology, neuroscience, relationships, sexuality, parenting, health (including from the perspectives of alternative medicine), work,[3] and the psychological aspects of current affairs.[1]		The magazine's website features therapy and health professionals directories[1] and hundreds of blogs written by a wide variety of psychologists, psychiatrists, social workers, medical doctors, anthropologists, sociologists, and science journalists. Its current editor-in-chief is Kaja Perina.[4]		In 1976 Psychology Today sold 1,026,872 copies.[2] The circulation of the magazine was 1,171,362 copies in 1981 and 862,193 copies in 1986.[2]		It has a circulation of 275,000 copies per issue as of 2013 and claims 14.1 people read each copy for a total audience of 3,877,500.[1] From June 2010 to June 2011, it was the one of the top ten consumer magazines by newsstand sales.[5] In recent years, while many magazines have suffered in readership declines, Adweek, in 2013, noted Psychology Today's 36 percent increase in number of readers.[6]		Owned and managed by the American Psychological Association from 1983 to 1987,[7] the publication is currently endorsed by the National Board for Certified Counselors, which promotes subscriptions and offers professional credit for a small fee and assigned assessment for each article read.[8]		
In German humour, East Frisian jokes (German: Ostfriesenwitz) belong to the group of jokes about certain nationalities, in this case the East Frisians of northern Germany.		The basic structure of these jokes takes the form of a simple question and answer; the question often asking something about the nature of the East Frisian and the humorous reply usually being at the expense of the supposedly stupid and/or primitive East Frisian. Often the East Frisian are portrayed as farmers, rural folk or coastal dwellers. Many punch lines describe the foolishness of East Frisians by using figure of speech or a word used in a different sense (a pun or play on words).		Sometimes the reverse situation also occurs in which the East Frisians are the wiser; and are contrasted usually with a group of people from the southern German-speaking world.		In addition, comedians such as Otto Waalkes and Karl Dall also tell jokes with or about East Frisians, usually in a completely open format.		In East Frisia itself these jokes are usually accepted. The positive effect of a greater awareness of the relatively small region of East Frisia resulting from this humour is recognized and welcomed. A modern legend even suggests that these jokes were invented by the East Frisians.						The East Frisian form of joke arose in the late 1960s and triggered one of the first large, nationwide waves of jokes in Germany. [1] Unlike other jokes about specific people groups, the history of East Frisian jokes is fairly well known. The grammar school in Westerstede in Ammerland, a neighbouring region of East Frisia, was and is attended by East Frisian pupils.[2] As with many other nearby regions, there is frequent taunting and teasing between the peoples of East Frisia and the Ammerland. The school said that culminated in 1968 and 1969 in one of the students, Borwin Bandelow, publishing a series in the school newspaper, Der Trompeter, called "From research and teaching." This series was about the so-called "Homo ostfrisiensis", the supposedly clumsy and stupid people of East Frisia. Wiard Raveling, himself an East Frisian and teacher at this school, published the "History of East Frisian Jokes" in book form in 1993.[1]		What followed from the series in the student newspaper, was a joke wave, which spread, first in the region, but was soon publicized on radio, newspapers and magazines in Germany. Media such as Stern or Spiegel reported on the curious neighbourhood disputes between East Frisians and Ammerlanders - and spread it by passing on the jokes. These were soon overtaken by the adaptations of the Polish jokes that had recently arisen in the 1960s in the U.S. with numerous variations, as well of jokes about other people groups.		In 1971 the East Frisian comedian and singer, Hannes Flesner, brought out several LPs with the then new East Frisian jokes ("East Frisia, as it laughs and sings"). Later, the two comedians from East Frisia, Otto Waalkes and Karl Dall, amongst others, built their careers on East Frisian jokes or the stereotype of the East Frisians and their country. Later joke waves, such as that in the 1980s about Federal Chancellor, Helmut Kohl, or those about Opel Manta drivers, or shortly thereafter about blondes in the 1990s partly took over the structure and content of the East Frisian jokes.		
A lightbulb joke is a joke that asks how many people of a certain group are needed to change, replace, or screw in a light bulb. Generally, the punch line answer highlights a stereotype of the target group. There are numerous versions of the lightbulb joke satirizing a wide range of cultures, beliefs and occupations.[1][2]		Early versions of the joke, popular in the late 1960s[3][4] and the 1970s, were used to insult the intelligence of people, especially Poles ("Polish jokes").[5][6] For instance:		Q. How many Polacks does it take to change a light bulb? A. Three—one to hold the light bulb and two to turn the ladder.		Although lightbulb jokes tend to be derogatory in tone (e.g., "How many drummers..." / "Four: one to hold the light bulb and three to drink until the room spins"), the people targeted by them may take pride in the stereotypes expressed and are often themselves the jokes' originators,[7] as in "How many Germans does it take to change a lightbulb? One." where the joke itself becomes a statement of ethnic pride. Lightbulb jokes applied to subgroups can be used to ease tensions between them.[8]		Some versions of the joke are puns on the words "change"[9] or "screw":[10]		Q. How many psychiatrists does it take to change a light bulb? A. None–the light bulb will change when it's ready.[11][12]		Q. How many flies does it take to screw in a lightbulb? A. Two, but don't ask me how they got in there.[13]		Lightbulb jokes may be responses to current events, particularly those related to energy and political power.[14] For example, the lightbulb may not need to be changed at all due to ongoing power outages.[15] The Village Voice held a $200 lightbulb joke contest around the time of the Iran hostage crisis, with the winning joke being:[16]		Q. How many Iranians does it take to change a light bulb? A. You send us the prize money and we'll tell you the answer.		
Desert island jokes are jokes about a person or group of people stranded on a desert island. This setting is typically used to play on stereotypes of the people present. This may refer to their profession, religion or nationality, or the people involved may be famous figures. The island setting highlights the absurdity of the stereotypical behaviour and prejudices of the protagonists, suggesting that they will find a way to express their own particular foibles even in a hostile setting.		This setting is also popular in cartoons.[1] Bob Mankoff, cartoon editor of The New Yorker attributes the strips, which began appearing in the publication in the 1930s, to the popularity of Robinson Crusoe. He describes earlier cartoons as having a large island with a ship sinking in the distance, and later cartoons merely showing one or two people on a tiny island with a single palm tree.[1]		A Scottish Presbyterian is rescued after many years on a desert island. As he stands on the deck of the rescuing vessel, the captain says to him, "I thought you were stranded alone. How come I can see three huts on the beach?"		
Electronic mail (email) is a method of exchanging messages between people using electronics. Email first entered substantial use in the 1960s and by the mid-1970s had taken the form now recognized as email. Email operates across computer networks, which today is primarily the Internet. Some early email systems required the author and the recipient to both be online at the same time, in common with instant messaging. Today's email systems are based on a store-and-forward model. Email servers accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need to connect only briefly, typically to a mail server or a webmail interface, for as long as it takes to send or receive messages.		Originally an ASCII text-only communications medium, Internet email was extended by Multipurpose Internet Mail Extensions (MIME) to carry text in other character sets and multimedia content attachments. International email, with internationalized email addresses using UTF-8, has been standardized, but as of 2017 it has not been widely adopted.[2]		The history of modern Internet email services reaches back to the early ARPANET, with standards for encoding email messages published as early as 1973 (RFC 561). An email message sent in the early 1970s looks very similar to a basic email sent today. Email had an important role in creating the Internet,[3] and the conversion from ARPANET to the Internet in the early 1980s produced the core of the current services.						Historically, the term electronic mail was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe fax document transmission.[4][5] As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.		Electronic mail has been most commonly called email or e-mail since around 1993,[6] but variations of the spelling have been used:		An Internet e-mail consists[24] of an envelope and content; the content in turn consists[25] of a header and a body.		Computer-based mail and messaging became possible with the advent of time-sharing computers in the early 1960s, and informal methods of using shared files to pass messages were soon expanded into the first mail systems. Most developers of early mainframes and minicomputers developed similar, but generally incompatible, mail applications. Over time, a complex web of gateways and routing systems linked many of them. Many US universities were part of the ARPANET (created in the late-1960s), which aimed at software portability between its systems. That portability helped make the Simple Mail Transfer Protocol (SMTP) increasingly influential.		For a time in the late 1980s and early 1990s, it seemed likely that either a proprietary commercial system or the X.400 email system, part of the Government Open Systems Interconnection Profile (GOSIP), would predominate. However, once the final restrictions on carrying commercial traffic over the Internet ended in 1995,[26][27] a combination of factors made the current Internet suite of SMTP, POP3 and IMAP email protocols the standard.		The diagram to the right shows a typical sequence of events[28] that takes place when sender Alice transmits a message using a mail user agent (MUA) addressed to the email address of the recipient.		In addition to this example, alternatives and complications exist in the email system:		Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called open mail relays. This was very important in the early days of the Internet when network connections were unreliable.[citation needed] However, this mechanism proved to be exploitable by originators of unsolicited bulk email and as a consequence open mail relays have become rare,[30] and many MTAs do not accept messages from open mail relays.		The Internet email message format is now defined by RFC 5322, with encoding of non-ASCII data and multimedia content attachments being defined in RFC 2045 through RFC 2049, collectively called Multipurpose Internet Mail Extensions or MIME. RFC 5322 replaced the earlier RFC 2822 in 2008, and in turn RFC 2822 in 2001 replaced RFC 822 – which had been the standard for Internet email for nearly 20 years. Published in 1982, RFC 822 was based on the earlier RFC 733 for the ARPANET.[31]		Internet email messages consist of two major sections, the message header and the message body, collectively known as content. The header is structured into fields such as From, To, CC, Subject, Date, and other information about the email. In the process of transporting email messages between systems, SMTP communicates delivery parameters and information using message header fields. The body contains the message, as unstructured text, sometimes containing a signature block at the end. The header is separated from the body by a blank line.		Each message has exactly one header, which is structured into fields. Each field has a name and a value. RFC 5322 specifies the precise syntax.		Informally, each line of text in the header that begins with a printable character begins a separate field. The field name starts in the first character of the line and ends before the separator character ":". The separator is then followed by the field value (the "body" of the field). The value is continued onto subsequent lines if those lines have a space or tab as their first character. Field names and values are restricted to 7-bit ASCII characters. Some non-ASCII values may be represented using MIME encoded words.		Email header fields can be multi-line, and each line should be at most 78 characters long and in no event more than 998 characters long.[32] Header fields defined by RFC 5322 can only contain US-ASCII characters; for encoding characters in other sets, a syntax specified in RFC 2047 can be used.[33] Recently the IETF EAI working group has defined some standards track extensions,[34][35] replacing previous experimental extensions, to allow UTF-8 encoded Unicode characters to be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some governments.[36]		The message header must include at least the following fields:[37][38]		RFC 3864 describes registration procedures for message header fields at the IANA; it provides for permanent and provisional field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:[39]		Note that the To: field is not necessarily related to the addresses to which the message is delivered. The actual delivery list is supplied separately to the transport protocol, SMTP, which may or may not originally have been extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter which is delivered according to the address on the outer envelope. In the same way, the "From:" field does not have to be the real sender of the email message. Some mail servers apply email authentication systems to messages being relayed. Data pertaining to server's activity is also part of the header, as defined below.		SMTP defines the trace information of a message, which is also saved in the header using the following two fields:[41]		Other fields that are added on top of the header by the receiving server may be called trace fields, in a broader sense.[42]		Internet email was originally designed for 7-bit ASCII.[47] Most email software is 8-bit clean but must assume it will communicate with 7-bit servers and mail readers. The MIME standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: quoted printable for mostly 7-bit content with a few characters outside that range and base64 for arbitrary binary data. The 8BITMIME and BINARY extensions were introduced to allow transmission of mail without the need for these encodings, but many mail transport agents still do not support them fully. In some countries, several encoding schemes coexist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is coincidence, when the sender and receiver use the same encoding scheme). Therefore, for international character sets, Unicode is growing in popularity.[citation needed]		Most modern graphic email clients allow the use of either plain text or HTML for the message body at the option of the user. HTML email messages often include an automatically generated plain text copy as well, for compatibility reasons. Advantages of HTML include the ability to include in-line links and images, set apart previous messages in block quotes, wrap naturally on any display, use emphasis such as underlines and italics, and change font styles. Disadvantages include the increased size of the email, privacy concerns about web bugs, abuse of HTML email as a vector for phishing attacks and the spread of malicious software.[48]		Some web-based mailing lists recommend that all posts be made in plain-text, with 72 or 80 characters per line[49][50] for all the above reasons, but also because they have a significant number of readers using text-based email clients such as Mutt. Some Microsoft email clients allow rich formatting using their proprietary Rich Text Format (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible email client.[51]		Messages are exchanged between hosts using the Simple Mail Transfer Protocol with software programs called mail transfer agents (MTAs); and delivered to a mail store by programs called mail delivery agents (MDAs, also sometimes called local delivery agents, LDAs). Accepting a message obliges an MTA to deliver it,[52] and when a message cannot be delivered, that MTA must send a bounce message back to the sender, indicating the problem.		Users can retrieve their messages from servers using standard protocols such as POP or IMAP, or, as is more likely in a large corporate environment, with a proprietary protocol specific to Novell Groupwise, Lotus Notes or Microsoft Exchange Servers. Programs used by users for retrieving, reading, and managing email are called mail user agents (MUAs).		Mail can be stored on the client, on the server side, or in both places. Standard formats for mailboxes include Maildir and mbox. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as IMAP, moving email from one server to another can be done with any MUA supporting the protocol.		Many current email users do not run MTA, MDA or MUA programs themselves, but use a web-based email platform, such as Gmail, Hotmail, or Yahoo! Mail, that performs the same tasks.[53] Such webmail interfaces allow users to access their mail with any standard web browser, from any computer, rather than relying on an email client.		Upon reception of email messages, email client applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the mbox format. The specific format used is often indicated by special filename extensions:		Some applications (like Apple Mail) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.		The URI scheme, as registered with the IANA, defines the mailto: scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the To: field.[54]		Many email providers have a web-based email client (e.g. AOL Mail, Gmail, Outlook.com, Hotmail and Yahoo! Mail). This allows users to log into the email account by using any compatible web browser to send and receive their email. Mail is typically not downloaded to the client, so can't be read without a current Internet connection.		The Post Office Protocol 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the server. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's).[55]		The Internet Message Access Protocol (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like smartphones are increasingly used to check email while travelling, and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually mail is left in folders in the mail server.		Messaging Application Programming Interface (MAPI) is used by Microsoft Outlook to communicate to Microsoft Exchange Server - and to a range of other email server products such as Axigen Mail Server, Kerio Connect, Scalix, Zimbra, HP OpenMail, IBM Lotus Notes, Zarafa, and Bynari where vendors have added MAPI support to allow their products to be accessed directly via Outlook.		Email has been widely accepted by business, governments and non-governmental organizations in the developed world, and it is one of the key parts of an 'e-revolution' in workplace communication (with the other key plank being widespread adoption of highspeed Internet). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.[56]		It has some key benefits to business and other organizations, including:		Email marketing via "opt-in" is often successfully used to send special sales offerings and new product information.[57] Depending on the recipient's culture,[58] email sent without permission—such as an "opt-in"—is likely to be viewed as unwelcome "email spam".		Many users access their personal email from friends and family members using a personal computer in their house or apartment.		Email has become used on smartphones and on all types of computers. Mobile "apps" for email increase accessibility to the medium for users who are out of their home. While in the earliest years of email, users could only access email on desktop computers, in the 2010s, it is possible for users to check their email when they are away from home, whether they are across town or across the world. Alerts can also be sent to the smartphone or other device to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. Today, there are an estimated 1.4 billion email users worldwide and 50 billion non-spam emails that are sent daily.		Individuals often check email on smartphones for both personal and work-related messages. It was found that US adults check their email more than they browse the web or check their Facebook accounts, making email the most popular activity for users to do on their smartphones. 78% of the respondents in the study revealed that they check their email on their phone.[59] It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.[60]		Email messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include Microsoft Word documents, pdf documents and scanned images of paper documents. In principle there is no technical restriction on the size or number of attachments, but in practice email clients, servers and Internet service providers implement various limitations on the size of files, or complete email - typically to 25MB or less.[61][62][63] Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ to what the user sees,[64] which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, file hosting services of various sorts are available; and generally suggested.[65][66] Some large files, such as digital photos, color presentations and video or music files are too large for some email systems.[67]		The ubiquity of email for knowledge workers and "white collar" employees has led to concerns that recipients face an "information overload" in dealing with increasing volumes of email.[68][69] With the growth in mobile devices, by default employees may also receive work-related emails outsde of their working day. This can lead to increased stress, decreased satisfaction with work, and some observers even argue it could have a significant negative economic effect,[70] as efforts to read the many emails could reduce productivity.		Email "spam" is the term used to describe unsolicited bulk email. The low cost of sending such email meant that by 2003 up to 30% of total email traffic was already spam.[71][72][73] and was threatening the usefulness of email as a practical tool. The US CAN-SPAM Act of 2003 and similar laws elsewhere[74] had some impact, and a number of effective anti-spam techniques now largely mitigate the impact of spam by filtering or rejecting it for most users,[75] but the volume sent is still very high—and increasingly consists not of advertisements for products, but malicious content or links.[76]		A range of malicious email types exist. These range from various types of email scams, including "social engineering" scams such as advance-fee scam "Nigerian letters", to phishing, email bombardment and email worms.		Email spoofing occurs when the email message header is designed to make the message appear to come from a known or trusted source. Email spam and phishing methods typically use spoofing to mislead the recipient about the true message origin. Email spoofing may be done as a prank, or as part of a criminal effort to defraud an individual or organization. An example of a potentially fraudulent email spoofing is if an individual creates an email which appears to be an invoice from a major company, and then sends it to one or more recipients. In some cases, these fraudulent emails incorporate the logo of the purported organization and even the email address may appear legitimate.		Email bombing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.		Today it can be important to distinguish between Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although information technology personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.		Email privacy, without some security precautions, can be compromised because:		There are cryptography applications that can serve as a remedy to one or more of the above. For example, Virtual Private Networks or the Tor anonymity network can be used to encrypt traffic from the user machine to a safer network while GPG, PGP, SMEmail,[77] or S/MIME can be used for end-to-end message encryption, and SMTP STARTTLS or SMTP over Transport Layer Security/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.		Additionally, many mail user agents do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as SASL prevent this. Finally, attached files share many of the same hazards as those found in peer-to-peer filesharing. Attached files may contain trojans or viruses.		Flaming occurs when a person sends a message (or many messages) with angry or antagonistic content. The term is derived from the use of the word "incendiary" to describe particularly heated email discussions. The ease and impersonality of email communications mean that the social norms that encourage civility in person or via telephone do not exist and civility may be forgotten.[78]		Also known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a "boilerplate" message explaining that their email inbox is full, and that they are in the process of clearing out all the messages. Harvard University law professor Lawrence Lessig is credited with coining this term, but he may only have popularized it.[79]		Originally Internet email was completely ASCII text-based. MIME now allows body content text and some header content text in international character sets, but other headers and email addresses using UTF-8, while standardized[80] have yet to be widely adopted.[2][81]		The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the IETF introduced Delivery Status Notifications (delivery receipts) and Message Disposition Notifications (return receipts); however, these are not universally deployed in production. (A complete Message Tracking mechanism was also defined, but it never gained traction; see RFCs 3885[82] through 3888.[83])		Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:		In the absence of standard methods, a range of system based around the use of web bugs have been developed. However, these are often seen as underhand or raising privacy concerns,[84][85][86] and only work with email clients that support rendering of HTML. Many mail clients now default to not showing "web content".[87] Webmail providers can also disrupt web bugs by pre-caching images.[88]		The U.S. state and federal governments have been involved in electronic messaging and the development of email in several different ways. Starting in 1977, the U.S. Postal Service (USPS) recognized that electronic messaging and electronic transactions posed a significant threat to First Class mail volumes and revenue. The USPS explored an electronic messaging initiative in 1977 and later disbanded it. Twenty years later, in 1997, when email volume overtook postal mail volume, the USPS was again urged to embrace email, and the USPS declined to provide email as a service.[89][90][91] The USPS initiated an experimental email service known as E-COM. E-COM provided a method for the simple exchange of text messages. In 2011, shortly after the USPS reported its state of financial bankruptcy, the USPS Office of Inspector General (OIG) began exploring the possibilities of generating revenue through email servicing.[92][93][94] Electronic messages were transmitted to a post office, printed out, and delivered as hard copy. To take advantage of the service, an individual had to transmit at least 200 messages. The delivery time of the messages was the same as First Class mail and cost 26 cents. Both the Postal Regulatory Commission and the Federal Communications Commission opposed E-COM. The FCC concluded that E-COM constituted common carriage under its jurisdiction and the USPS would have to file a tariff.[95] Three years after initiating the service, USPS canceled E-COM and attempted to sell it off.[96][97][98][99][100]		The early ARPANET dealt with multiple email clients that had various, and at times incompatible, formats. For example, in the Multics, the "@" sign meant "kill line" and anything before the "@" sign was ignored, so Multics users had to use a command-line option to specify the destination system.[101] The Department of Defense DARPA desired to have uniformity and interoperability for email and therefore funded efforts to drive towards unified inter-operable standards. This led to David Crocker, John Vittal, Kenneth Pogran, and Austin Henderson publishing RFC 733, "Standard for the Format of ARPA Network Text Message" (November 21, 1977), a subset of which provided a stable base for common use on the ARPANET, but which was not fully effective, and in 1979, a meeting was held at BBN to resolve incompatibility issues. Jon Postel recounted the meeting in RFC 808, "Summary of Computer Mail Services Meeting Held at BBN on 10 January 1979" (March 1, 1982), which includes an appendix listing the varying email systems at the time. This, in turn, led to the release of David Crocker's RFC 822, "Standard for the Format of ARPA Internet Text Messages" (August 13, 1982).[102] RFC 822 is a small adaptation of RFC 733's details, notably enhancing the host portion, to use Domain Names, that were being developed at the same time.		The National Science Foundation took over operations of the ARPANET and Internet from the Department of Defense, and initiated NSFNet, a new backbone for the network. A part of the NSFNet AUP forbade commercial traffic.[103] In 1988, Vint Cerf arranged for an interconnection of MCI Mail with NSFNET on an experimental basis. The following year Compuserve email interconnected with NSFNET. Within a few years the commercial traffic restriction was removed from NSFNETs AUP, and NSFNET was privatised. In the late 1990s, the Federal Trade Commission grew concerned with fraud transpiring in email, and initiated a series of procedures on spam, fraud, and phishing.[104] In 2004, FTC jurisdiction over spam was codified into law in the form of the CAN SPAM Act.[105] Several other U.S. federal agencies have also exercised jurisdiction including the Department of Justice and the Secret Service. NASA has provided email capabilities to astronauts aboard the Space Shuttle and International Space Station since 1991 when a Macintosh Portable was used aboard Space Shuttle mission STS-43 to send the first email via AppleLink.[106][107][108] Today astronauts aboard the International Space Station have email capabilities via the wireless networking throughout the station and are connected to the ground at 10 Mbit/s Earth to station and 3 Mbit/s station to Earth, comparable to home DSL connection speeds.[109]		
An elephant joke is a joke, almost always an absurd riddle or conundrum and often a sequence of such, that involves an elephant. Elephant jokes were a fad in the 1960s, with many people constructing large numbers of them according to a set formula. Sometimes they involve parodies or puns.[1][2][3]		Nine examples of elephant jokes are:[1][3]						In 1960, L.M. Becker Co of Appleton, Wisconsin, released a set of 50 trading cards titled "Elephant Jokes". Elephant jokes first appeared in the United States in 1962. They were first recorded in mid-1962 in Texas, and gradually spread across the US, reaching California in early 1963. By July 1963, elephant jokes were ubiquitous and could be found in newspaper columns, and in Time and Seventeen magazines, with millions of people working to construct more jokes according to the same formula.[1][2]		Both elephant jokes and Tom Swifties were in vogue in 1963, and were reported in the US national press. While the appeal of Tom Swifties was to literate adults, and gradually faded over subsequent decades, the appeal of elephant jokes was mainly to children, and has lasted. Elephant jokes began circulation primarily among schoolchildren, and have been discovered afresh by subsequent generations of children, remaining, in Isaac Asimov's words "favorites of youngsters and of unsophisticated adults".[1][2][4]		Asimov discusses one particular elephant joke that he states is notable for the exceptional sophistication of its humor. The joke was told in the aftermath of the murder of Lee Harvey Oswald by Jack Ruby, who had walked into Dallas police headquarters carrying a gun, and, in Asimov's words, while still maintaining the absurdity necessary for elephant jokes "carried a quick overtone of chill rationality":[2]		Elephant jokes rely upon absurdity and incongruity for their humor, and a contrast with the normal presumptions of knowledge about elephants. They rely upon absurdist reasoning such as that it would be the relatively incidental evidence regarding the smell of an elephant's breath or the presence of footprints in the butter that would allow for the detection of an elephant in one's bathtub or refrigerator. One key to the construction of an elephant joke is that the joke answers are somewhat appropriate if one merely overlooks the obvious absurdities inherent to the questions. If elephants were capable of climbing trees and if painting an elephant's toenails was an effective camouflage mechanism, then red would be the appropriate color for a cherry tree. If the common connotation that questions requesting the time are expected to be answered in terms of hours and minutes is ignored, then by the implied destruction of one's fence from being sat on by an elephant, it would be time to build a new fence. The appropriateness of the answer, when accounting for the absurd incongruences existing between the implied premise of the question and the normal assumptions said question invokes, distinguishes elephant jokes as jokes rather than nonsensical riddles.[5][6]		Elephant jokes are often parodies of conventional children's riddles. Consider the following commonly recited child's riddle:		Traditionally the challenge of solving this riddle relies on recognizing the ambiguity stemming from the riddle being generally shared aloud as opposed to in writing. Thus the appropriate homophone, "red" or "read", must be inferred. If "red" is assumed, then the problem arises regarding whether or not any object satisfying the condition of being "red all over" would necessarily preclude said object from also satisfying the requirement of being "black and white". However, if instead "read" is assumed, then there is no implied mutual exclusivity preventing a solution, conventionally a newspaper, from satisfying both required conditions. Compare the traditional riddle, which is solved by a well-known item that can be reasonably determined from the riddle, with the elephant joke parody:		The absurdity of an elephant wearing a nun costume makes it nearly impossible for anyone not familiar with the punchline to independently think of the parody answer. Ignoring how unlikely one is to ever encounter an elephant dressed as a nun, then the answer is somewhat appropriate. A nun costume would likely be both "black and white" and a sunburn would cause an elephant to be, somewhat, "red all over". Humor arises from the irony of ignoring the expected answer for the outlandish, yet appropriate, elephant answer .[7]		A series of elephant jokes can be constructed. Linking the appropriateness of each subsequent answer to the logically absurd structure of the preceding joke, the overall absurdity of a series can continuously compound. For example:[3][8]		The absurdity of the first riddle's answer subverts the audience's initial expectations. The second and third riddles reinforce the expectation for this logically absurd structure. The final riddle concludes by again absurdly subverting the audience's expected framework. The humor for independent elephant jokes relies on absurd answers that ignore expectations, yet have a certain appropriateness. Here the absurdity is compounded when the appropriateness of the final riddle's answer is dependent upon undermining the logically absurd structure built from the preceding riddles. Another example:		Elephant jokes thus not only deliberately undermine the conventions of riddles, they even act to undermine themselves. This even extends to undermining the implied premise, expected by those that are familiar with elephant jokes, that an elephant joke is automatically illogical, or even involves elephants at all. For example:[3]		There can even be an off-color tinge:		One time Gong Show act Mike Elephant is remembered for the following joke:		Elephant jokes can also use their inherent absurdity to point up the inherent absurdity in some current events. One such joke from the early 1960s refers to an incident in President Kennedy's on-again-off-again support for Cuban exiles' attempts to overthrow Fidel Castro:		Elephant jokes are seen by many commentators as symbolic of the culture of the United States and the United Kingdom in the 1960s. Elliot Oring notes that elephant jokes dismiss conventional questions and answers, repudiate established wisdom, and reject the authority of traditional knowledge. He draws a parallel between this and the counterculture of the 1960s, stating that "disestablishment was the purpose of both," pointing to the sexual revolution and noting that "[p]erhaps it was no accident that many of the elephant jokes emphasized the intrusion of sex into the most innocuous areas."[3]		In their paper, On elephantasy and elephanticide, Abrahams and Dundes consider elephant jokes to be convenient disguises for racism, and symbolised the nervousness of white people about the civil rights movement. Whilst blatantly racialist jokes became less acceptable, elephant jokes were a useful proxy. One example Abrahams and Dundes provide is the joke:		They state that the "big and grey and comes in quarts" is in fact a reference "to the supposed mammoth nature of black sexuality." Similarly, the joke about an elephant in the bathtub is argued to be a reference to the increased intrusion of black people into "the most intimate areas of white life."[9]		Oring strongly disagrees with this view, writing: "The Civil Rights movement, of course, was an integral part of the countercultural revolution. But there is no reason to view it as the single force conditioning the joke cycle. Much more than the relations between the races was being turned on its ear. Reducing elephant jokes to a mere front for racial aggression, it seems to me, not only misses the larger sense of what the jokes are about, but the larger sense of what was going on in the society at the time." and continuing: "Elephant joking is more than a description of the episodic career of an animal with a phallic nose. What engenders the humor in such jokes is the violation of categories of expectation, and not images of subjugation, degradation, or feminization of the elephant."[3]		Charles Gruner agrees with Oring that Abrahams' and Dundes' explanation (that "the elephant is an ambivalent father figure" that is, in reality, "the black man (perceived as a sexual threat) that stands hidden behind the image of the elephant") is an "explanation from Freudian Monsterland [that] holds no water."[10]		Gruner however disagrees with Oring about the chronological topicality of the elephant joke and its relation to social upheavals, arguing from personal experience of "one of the best motion picture sight gags in history", where Jimmy Durante in the 1962 movie Billy Rose's Jumbo is attempting to sneak an elephant unseen through a circus. Upon coming around a tent and being faced with a crowd of people and a policeman who demands "Where do you think you are you going with that elephant?" Durante backs against the elephant, arms wide, and asks, innocently, "What elephant?" Gunder proposes that the success of this sight gag spawned in comic writers the idea of "hiding the elephant by all sorts of ridiculous means," and thus, by extension to "other silly, stupid comparisons", the whole genre of elephant jokes.[10]		A turnabout to the Blind men and an elephant cited below is the joke about the 4 blind elephants who felt a human. The first reports that humans are flat, the other three agree.		
The National Aeronautics and Space Administration (NASA /ˈnæsə/) is an independent agency of the executive branch of the United States federal government responsible for the civilian space program, as well as aeronautics and aerospace research.[note 1]		President Dwight D. Eisenhower established NASA in 1958[10] with a distinctly civilian (rather than military) orientation encouraging peaceful applications in space science. The National Aeronautics and Space Act was passed on July 29, 1958, disestablishing NASA's predecessor, the National Advisory Committee for Aeronautics (NACA). The new agency became operational on October 1, 1958.[11][12]		Since that time, most US space exploration efforts have been led by NASA, including the Apollo Moon landing missions, the Skylab space station, and later the Space Shuttle. Currently, NASA is supporting the International Space Station and is overseeing the development of the Orion Multi-Purpose Crew Vehicle, the Space Launch System and Commercial Crew vehicles. The agency is also responsible for the Launch Services Program (LSP) which provides oversight of launch operations and countdown management for unmanned NASA launches.		NASA science is focused on better understanding Earth through the Earth Observing System,[13] advancing heliophysics through the efforts of the Science Mission Directorate's Heliophysics Research Program,[14] exploring bodies throughout the Solar System with advanced robotic spacecraft missions such as New Horizons,[15] and researching astrophysics topics, such as the Big Bang, through the Great Observatories and associated programs.[16] NASA shares data with various national and international organizations such as from the Greenhouse Gases Observing Satellite. Since 2011, NASA has been criticized for low cost efficiency, achieving little results in return for high development costs.[17][18][19]						From 1946, the National Advisory Committee for Aeronautics (NACA) had been experimenting with rocket planes such as the supersonic Bell X-1.[20] In the early 1950s, there was challenge to launch an artificial satellite for the International Geophysical Year (1957–58). An effort for this was the American Project Vanguard. After the Soviet launch of the world's first artificial satellite (Sputnik 1) on October 4, 1957, the attention of the United States turned toward its own fledgling space efforts. The US Congress, alarmed by the perceived threat to national security and technological leadership (known as the "Sputnik crisis"), urged immediate and swift action; President Dwight D. Eisenhower and his advisers counseled more deliberate measures. On January 12, 1958, NACA organized a "Special Committee on Space Technology", headed by Guyford Stever.[12] On January 14, 1958, NACA Director Hugh Dryden published "A National Research Program for Space Technology" stating:[21]		It is of great urgency and importance to our country both from consideration of our prestige as a nation as well as military necessity that this challenge [Sputnik] be met by an energetic program of research and development for the conquest of space... It is accordingly proposed that the scientific research be the responsibility of a national civilian agency... NACA is capable, by rapid extension and expansion of its effort, of providing leadership in space technology.[21]		While this new federal agency would conduct all non-military space activity, the Advanced Research Projects Agency (ARPA) was created in February 1958 to develop space technology for military application.[22]		On July 29, 1958, Eisenhower signed the National Aeronautics and Space Act, establishing NASA. When it began operations on October 1, 1958, NASA absorbed the 43-year-old NACA intact; its 8,000 employees, an annual budget of US$100 million, three major research laboratories (Langley Aeronautical Laboratory, Ames Aeronautical Laboratory, and Lewis Flight Propulsion Laboratory) and two small test facilities.[23] A NASA seal was approved by President Eisenhower in 1959.[24] Elements of the Army Ballistic Missile Agency and the United States Naval Research Laboratory were incorporated into NASA. A significant contributor to NASA's entry into the Space Race with the Soviet Union was the technology from the German rocket program led by Wernher von Braun, who was now working for the Army Ballistic Missile Agency (ABMA), which in turn incorporated the technology of American scientist Robert Goddard's earlier works.[25] Earlier research efforts within the US Air Force[23] and many of ARPA's early space programs were also transferred to NASA.[26] In December 1958, NASA gained control of the Jet Propulsion Laboratory, a contractor facility operated by the California Institute of Technology.[23]		The agency's leader, NASA's administrator, is nominated by the President of the United States subject to approval of the US Senate, and reports to him or her and serves as senior space science advisor. Though space exploration is ostensibly non-partisan, the appointee usually is associated with the President's political party (Democratic or Republican), and a new administrator is usually chosen when the Presidency changes parties. The only exceptions to this have been: James C. Fletcher, appointed by Republican Richard Nixon but stayed through May 1977 into the term of Democrat Jimmy Carter; Daniel Goldin, appointed by Republican George H. W. Bush and stayed through the administration of Democrat Bill Clinton; and Robert M. Lightfoot, Jr., associate administrator under Democrat Barack Obama kept on as acting administrator by Republican Donald Trump.[6] Though the agency is independent, the survival or discontinuation of projects can depend directly on the will of the President.[27]		The first administrator was Dr. T. Keith Glennan appointed by Republican President Dwight D. Eisenhower. During his term he brought together the disparate projects in American space development research.[28]		The third administrator, James E. Webb (1961–1968), appointed by President John F. Kennedy, was a Democrat who first publicly served under President Harry S. Truman. In order to implement the Apollo program to achieve Kennedy's Moon landing goal by the end of the 1960s, Webb directed major management restructuring and facility expansion, establishing the Houston Manned Spacecraft (Johnson) Center and the Florida Launch Operations (Kennedy) Center. Capitalizing on Kennedy's legacy, President Lyndon Johnson kept continuity with the Apollo program by keeping Webb on when he succeeded Kennedy in November 1963. But Webb resigned in October 1968 before Apollo achieved its goal, and Republican President Richard M. Nixon replaced Webb with Republican Thomas O. Paine.		James Fletcher was responsible for early planning of the Space Shuttle program during his first term as administrator under President Nixon. He was appointed for a second term as administrator from May 1986 through April 1989 by President Ronald Reagan to help the agency recover from the Space Shuttle Challenger disaster.		Former astronaut Charles Bolden served as NASA's twelfth administrator from July 2009 to January 20, 2017.[29] Administrator Bolden is one of three former astronauts who became NASA administrators, along with Richard H. Truly (served 1989–1992) and Frederick D. Gregory (acting, 2005).		The agency's administration is located at NASA Headquarters in Washington, DC and provides overall guidance and direction.[30] Except under exceptional circumstances, NASA civil service employees are required to be citizens of the United States.[31]		In response to the Apollo 1 accident which killed three astronauts in 1967, Congress directed NASA to form an Aerospace Safety Advisory Panel (ASAP) to advise the NASA Administrator on safety issues and hazards in NASA's aerospace programs. In the aftermath of the Shuttle Columbia accident, Congress required that the ASAP submit an annual report to the NASA Administrator and to Congress.[32] By 1971, NASA had also established the Space Program Advisory Council and the Research and Technology Advisory Council to provide the administrator with advisory committee support. In 1977, the latter two were combined to form the NASA Advisory Council (NAC).[33]		NASA has conducted many manned and unmanned spaceflight programs throughout its history. Unmanned programs launched the first American artificial satellites into Earth orbit for scientific and communications purposes, and sent scientific probes to explore the planets of the solar system, starting with Venus and Mars, and including "grand tours" of the outer planets. Manned programs sent the first Americans into low Earth orbit (LEO), won the Space Race with the Soviet Union by landing twelve men on the Moon from 1969 to 1972 in the Apollo program, developed a semi-reusable LEO Space Shuttle, and developed LEO space station capability by itself and with the cooperation of several other nations including post-Soviet Russia. Some missions include both manned and unmanned aspects, such as the Galileo probe, which was deployed by astronauts in Earth orbit before being sent unmanned to Jupiter.		The experimental rocket-powered aircraft programs started by NACA were extended by NASA as support for manned spaceflight. This was followed by a one-man space capsule program, and in turn by a two-man capsule program. Reacting to loss of national prestige and security fears caused by early leads in space exploration by the Soviet Union, in 1961 President John F. Kennedy proposed the ambitious goal "of landing a man on the Moon by the end of [the 1960s], and returning him safely to the Earth." This goal was met in 1969 by the Apollo program, and NASA planned even more ambitious activities leading to a manned mission to Mars. However, reduction of the perceived threat and changing political priorities almost immediately caused the termination of most of these plans. NASA turned its attention to an Apollo-derived temporary space laboratory, and a semi-reusable Earth orbital shuttle. In the 1990s, funding was approved for NASA to develop a permanent Earth orbital space station in cooperation with the international community, which now included the former rival, post-Soviet Russia. To date, NASA has launched a total of 166 manned space missions on rockets, and thirteen X-15 rocket flights above the USAF definition of spaceflight altitude, 260,000 feet (80 km).[34]		The X-15 was an NACA experimental rocket-powered hypersonic research aircraft, developed in conjunction with the US Air Force and Navy. The design featured a slender fuselage with fairings along the side containing fuel and early computerized control systems.[35] Requests for proposal were issued on December 30, 1954, for the airframe, and February 4, 1955, for the rocket engine. The airframe contract was awarded to North American Aviation in November 1955, and the XLR30 engine contract was awarded to Reaction Motors in 1956, and three planes were built. The X-15 was drop-launched from the wing of one of two NASA Boeing B-52 Stratofortresses, NB52A tail number 52-003, and NB52B, tail number 52-008 (known as the Balls 8). Release took place at an altitude of about 45,000 feet (14 km) and a speed of about 500 miles per hour (805 km/h).		Twelve pilots were selected for the program from the Air Force, Navy, and NACA (later NASA). A total of 199 flights were made between 1959 and 1968, resulting in the official world record for the highest speed ever reached by a manned powered aircraft (current as of 2014[update]), and a maximum speed of Mach 6.72, 4,519 miles per hour (7,273 km/h).[36] The altitude record for X-15 was 354,200 feet (107.96 km).[37] Eight of the pilots were awarded Air Force astronaut wings for flying above 260,000 feet (80 km), and two flights by Joseph A. Walker exceeded 100 kilometers (330,000 ft), qualifying as spaceflight according to the International Aeronautical Federation. The X-15 program employed mechanical techniques used in the later manned spaceflight programs, including reaction control system jets for controlling the orientation of a spacecraft, space suits, and horizon definition for navigation.[37] The reentry and landing data collected were valuable to NASA for designing the Space Shuttle.[35]		Shortly after the Space Race began, an early objective was to get a person into Earth orbit as soon as possible, therefore the simplest spacecraft that could be launched by existing rockets was favored. The US Air Force's Man in Space Soonest program considered many manned spacecraft designs, ranging from rocket planes like the X-15, to small ballistic space capsules.[38] By 1958, the space plane concepts were eliminated in favor of the ballistic capsule.[39]		When NASA was created that same year, the Air Force program was transferred to it and renamed Project Mercury. The first seven astronauts were selected among candidates from the Navy, Air Force and Marine test pilot programs. On May 5, 1961, astronaut Alan Shepard became the first American in space aboard Freedom 7, launched by a Redstone booster on a 15-minute ballistic (suborbital) flight.[40] John Glenn became the first American to be launched into orbit, by an Atlas launch vehicle on February 20, 1962, aboard Friendship 7.[41] Glenn completed three orbits, after which three more orbital flights were made, culminating in L. Gordon Cooper's 22-orbit flight Faith 7, May 15–16, 1963.[42]		The Soviet Union (USSR) competed with its own single-pilot spacecraft, Vostok. They sent the first man in space, by launching cosmonaut Yuri Gagarin into a single Earth orbit aboard Vostok 1 in April 1961, one month before Shepard's flight.[43] In August 1962, they achieved an almost four-day record flight with Andriyan Nikolayev aboard Vostok 3, and also conducted a concurrent Vostok 4 mission carrying Pavel Popovich.		Based on studies to grow the Mercury spacecraft capabilities to long-duration flights, developing space rendezvous techniques, and precision Earth landing, Project Gemini was started as a two-man program in 1962 to overcome the Soviets' lead and to support the Apollo manned lunar landing program, adding extravehicular activity (EVA) and rendezvous and docking to its objectives. The first manned Gemini flight, Gemini 3, was flown by Gus Grissom and John Young on March 23, 1965.[44] Nine missions followed in 1965 and 1966, demonstrating an endurance mission of nearly fourteen days, rendezvous, docking, and practical EVA, and gathering medical data on the effects of weightlessness on humans.[45][46]		Under the direction of Soviet Premier Nikita Khrushchev, the USSR competed with Gemini by converting their Vostok spacecraft into a two- or three-man Voskhod. They succeeded in launching two manned flights before Gemini's first flight, achieving a three-cosmonaut flight in 1963 and the first EVA in 1964. After this, the program was canceled, and Gemini caught up while spacecraft designer Sergei Korolev developed the Soyuz spacecraft, their answer to Apollo.		The U.S public's perception of the Soviet lead in the space race (by putting the first man into space) motivated President John F. Kennedy to ask the Congress on May 25, 1961, to commit the federal government to a program to land a man on the Moon by the end of the 1960s, which effectively launched the Apollo program.[47]		Apollo was one of the most expensive American scientific programs ever. It cost more than $20 billion in 1960s dollars[48] or an estimated $208 billion in present-day US dollars.[49] (In comparison, the Manhattan Project cost roughly $26.6 billion, accounting for inflation.)[49][50] It used the Saturn rockets as launch vehicles, which were far bigger than the rockets built for previous projects.[51] The spacecraft was also bigger; it had two main parts, the combined command and service module (CSM) and the lunar landing module (LM). The LM was to be left on the Moon and only the command module (CM) containing the three astronauts would eventually return to Earth.		The second manned mission, Apollo 8, brought astronauts for the first time in a flight around the Moon in December 1968.[52] Shortly before, the Soviets had sent an unmanned spacecraft around the Moon.[53] On the next two missions docking maneuvers that were needed for the Moon landing were practiced[54][55] and then finally the Moon landing was made on the Apollo 11 mission in July 1969.[56]		The first person to stand on the Moon was Neil Armstrong, who was followed by Buzz Aldrin, while Michael Collins orbited above. Five subsequent Apollo missions also landed astronauts on the Moon, the last in December 1972. Throughout these six Apollo spaceflights, twelve men walked on the Moon. These missions returned a wealth of scientific data and 381.7 kilograms (842 lb) of lunar samples. Topics covered by experiments performed included soil mechanics, meteoroids, seismology, heat flow, lunar ranging, magnetic fields, and solar wind.[57] The Moon landing marked the end of the space race; and as a gesture, Armstrong mentioned mankind when he stepped down on the Moon.[58]		Apollo set major milestones in human spaceflight. It stands alone in sending manned missions beyond low Earth orbit, and landing humans on another celestial body.[59] Apollo 8 was the first manned spacecraft to orbit another celestial body, while Apollo 17 marked the last moonwalk and the last manned mission beyond low Earth orbit to date. The program spurred advances in many areas of technology peripheral to rocketry and manned spaceflight, including avionics, telecommunications, and computers. Apollo sparked interest in many fields of engineering and left many physical facilities and machines developed for the program as landmarks. Many objects and artifacts from the program are on display at various locations throughout the world, notably at the Smithsonian's Air and Space Museums.		Skylab was the United States' first and only independently built space station.[60] Conceived in 1965 as a workshop to be constructed in space from a spent Saturn IB upper stage, the 169,950 lb (77,088 kg) station was constructed on Earth and launched on May 14, 1973, atop the first two stages of a Saturn V, into a 235-nautical-mile (435 km) orbit inclined at 50° to the equator. Damaged during launch by the loss of its thermal protection and one electricity-generating solar panel, it was repaired to functionality by its first crew. It was occupied for a total of 171 days by 3 successive crews in 1973 and 1974.[60] It included a laboratory for studying the effects of microgravity, and a solar observatory.[60] NASA planned to have a Space Shuttle dock with it, and elevate Skylab to a higher safe altitude, but the Shuttle was not ready for flight before Skylab's re-entry on July 11, 1979.[61]		To save cost, NASA used one of the Saturn V rockets originally earmarked for a canceled Apollo mission to launch the Skylab. Apollo spacecraft were used for transporting astronauts to and from the station. Three three-man crews stayed aboard the station for periods of 28, 59, and 84 days. Skylab's habitable volume was 11,290 cubic feet (320 m3), which was 30.7 times bigger than that of the Apollo Command Module.[61]		On May 24, 1972, US President Richard M. Nixon and Soviet Premier Alexei Kosygin signed an agreement calling for a joint manned space mission, and declaring intent for all future international manned spacecraft to be capable of docking with each other.[62] This authorized the Apollo-Soyuz Test Project (ASTP), involving the rendezvous and docking in Earth orbit of a surplus Apollo Command/Service Module with a Soyuz spacecraft. The mission took place in July 1975. This was the last US manned space flight until the first orbital flight of the Space Shuttle in April 1981.[63]		The mission included both joint and separate scientific experiments, and provided useful engineering experience for future joint US–Russian space flights, such as the Shuttle–Mir Program[64] and the International Space Station.		The Space Shuttle became the major focus of NASA in the late 1970s and the 1980s. Planned as a frequently launchable and mostly reusable vehicle, four space shuttle orbiters were built by 1985. The first to launch, Columbia, did so on April 12, 1981,[65] the 20th anniversary of the first known human space flight.[66]		Its major components were a spaceplane orbiter with an external fuel tank and two solid-fuel launch rockets at its side. The external tank, which was bigger than the spacecraft itself, was the only major component that was not reused. The shuttle could orbit in altitudes of 185–643 km (115–400 miles)[67] and carry a maximum payload (to low orbit) of 24,400 kg (54,000 lb).[68] Missions could last from 5 to 17 days and crews could be from 2 to 8 astronauts.[67]		On 20 missions (1983–98) the Space Shuttle carried Spacelab, designed in cooperation with the European Space Agency (ESA). Spacelab was not designed for independent orbital flight, but remained in the Shuttle's cargo bay as the astronauts entered and left it through an airlock.[69] Another famous series of missions were the launch and later successful repair of the Hubble Space Telescope in 1990 and 1993, respectively.[70]		In 1995, Russian-American interaction resumed with the Shuttle–Mir missions (1995–1998). Once more an American vehicle docked with a Russian craft, this time a full-fledged space station. This cooperation has continued with Russia and the United States as two of the biggest partners in the largest space station built: the International Space Station (ISS). The strength of their cooperation on this project was even more evident when NASA began relying on Russian launch vehicles to service the ISS during the two-year grounding of the shuttle fleet following the 2003 Space Shuttle Columbia disaster.		The Shuttle fleet lost two orbiters and 14 astronauts in two disasters: Challenger in 1986, and Columbia in 2003.[71] While the 1986 loss was mitigated by building the Space Shuttle Endeavour from replacement parts, NASA did not build another orbiter to replace the second loss.[71] NASA's Space Shuttle program had 135 missions when the program ended with the successful landing of the Space Shuttle Atlantis at the Kennedy Space Center on July 21, 2011. The program spanned 30 years with over 300 astronauts sent into space.[72]		The International Space Station (ISS) combines NASA's Space Station Freedom project with the Soviet/Russian Mir-2 station, the European Columbus station, and the Japanese Kibō laboratory module.[73] NASA originally planned in the 1980s to develop Freedom alone, but US budget constraints led to the merger of these projects into a single multi-national program in 1993, managed by NASA, the Russian Federal Space Agency (RKA), the Japan Aerospace Exploration Agency (JAXA), the European Space Agency (ESA), and the Canadian Space Agency (CSA).[74][75] The station consists of pressurized modules, external trusses, solar arrays and other components, which have been launched by Russian Proton and Soyuz rockets, and the US Space Shuttles.[73] It is currently being assembled in Low Earth Orbit. The on-orbit assembly began in 1998, the completion of the US Orbital Segment occurred in 2011 and the completion of the Russian Orbital Segment is expected by 2016.[76][77][needs update] The ownership and use of the space station is established in intergovernmental treaties and agreements[78] which divide the station into two areas and allow Russia to retain full ownership of the Russian Orbital Segment (with the exception of Zarya),[79][80] with the US Orbital Segment allocated between the other international partners.[78]		Long duration missions to the ISS are referred to as ISS Expeditions. Expedition crew members typically spend approximately six months on the ISS.[81] The initial expedition crew size was three, temporarily decreased to two following the Columbia disaster. Since May 2009, expedition crew size has been six crew members.[82] Crew size is expected to be increased to seven, the number the ISS was designed for, once the Commercial Crew Program becomes operational.[83] The ISS has been continuously occupied for the past 7003612200000000000♠16 years and 278 days, having exceeded the previous record held by Mir; and has been visited by astronauts and cosmonauts from 15 different nations.[84][85]		The station can be seen from the Earth with the naked eye and, as of 2017, is the largest artificial satellite in Earth orbit with a mass and volume greater than that of any previous space station.[86] The Soyuz spacecraft delivers crew members, stays docked for their half-year-long missions and then returns them home. Several uncrewed cargo spacecraft service the ISS, they are the Russian Progress spacecraft which has done so since 2000, the European Automated Transfer Vehicle (ATV) since 2008, the Japanese H-II Transfer Vehicle (HTV) since 2009, the American Dragon spacecraft since 2012, and the American Cygnus spacecraft since 2013. The Space Shuttle, before its retirement, was also used for cargo transfer and would often switch out expedition crew members, although it did not have the capability to remain docked for the duration of their stay. Until another US manned spacecraft is ready, crew members will travel to and from the International Space Station exclusively aboard the Soyuz.[87] The highest number of people occupying the ISS has been thirteen; this occurred three times during the late Shuttle ISS assembly missions.[88]		The ISS program is expected to continue until at least 2020, and may be extended beyond 2028.[89]		The development of the Commercial Resupply Services (CRS) vehicles began in 2006 with the purpose of creating American commercially operated uncrewed cargo vehicles to service the ISS.[90] The development of these vehicles was under a fixed price milestone-based program, meaning that each company that received a funded award had a list of milestones with a dollar value attached to them that they didn't receive until after they had successful completed the milestone.[91] Private companies were also required to have some "skin in the game" which refers raising an unspecified amount of private investment for their proposal.[92]		On December 23, 2008, NASA awarded Commercial Resupply Services contracts to SpaceX and Orbital Sciences Corporation.[93] SpaceX uses its Falcon 9 rocket and Dragon spacecraft.[94] Orbital Sciences uses its Antares rocket and Cygnus spacecraft. The first Dragon resupply mission occurred in May 2012.[95] The first Cygnus resupply mission occurred in September 2013.[96] The CRS program now provides for all America's ISS cargo needs; with the exception of a few vehicle-specific payloads that are delivered on the European ATV and the Japanese HTV.[97]		The Commercial Crew Development (CCDev) program was initiated in 2010 with the purpose of creating American commercially operated crewed spacecraft capable of delivering at least four crew members to the ISS, staying docked for 180 days and then returning them back to Earth.[98] It is hoped that these vehicles could also transport non-NASA customers to private space stations such those planned by Bigelow Aerospace.[99] Like COTS, CCDev is also a fixed price milestone-based developmental program that requires some private investment.[91]		In 2010, NASA announced the winners of the first phase of the program, a total of $50 million was divided among five American companies to foster research and development into human spaceflight concepts and technologies in the private sector. In 2011, the winners of the second phase of the program were announced, $270 million was divided among four companies.[100] In 2012, the winners of the third phase of the program were announced, NASA provided $1.1 billion divided among three companies to further develop their crew transportation systems.[101] In 2014, the winners of the final round were announced.[102] SpaceX's Dragon V2 (planned to be launched on a Falcon 9 v1.1) received a contract valued up to $2.6 billion and Boeing's CST-100 (to be launched on an Atlas V) received a contract valued up to $4.2 billion.[103] NASA expects these vehicles to begin transporting humans to the ISS in 2017.[103]		Dragon V2		Computer rendering of CST-100 in orbit		For missions beyond low Earth orbit (BLEO), NASA has been directed to develop the Space Launch System (SLS), a Saturn-V class rocket, and the two to six person, beyond low Earth orbit spacecraft, Orion. In February 2010, President Barack Obama's administration proposed eliminating public funds for the Constellation program and shifting greater responsibility of servicing the ISS to private companies.[104] During a speech at the Kennedy Space Center on April 15, 2010, Obama proposed a new heavy-lift vehicle (HLV) to replace the formerly planned Ares V.[105] In his speech, Obama called for a manned mission to an asteroid as soon as 2025, and a manned mission to Mars orbit by the mid-2030s.[105] The NASA Authorization Act of 2010 was passed by Congress and signed into law on October 11, 2010.[106] The act officially canceled the Constellation program.[106]		The Authorization Act required a newly designed HLV be chosen within 90 days of its passing; the launch vehicle was given the name "Space Launch System". The new law also required the construction of a beyond low earth orbit spacecraft.[107] The Orion spacecraft, which was being developed as part of the Constellation program, was chosen to fulfill this role.[108] The Space Launch System is planned to launch both Orion and other necessary hardware for missions beyond low Earth orbit.[109] The SLS is to be upgraded over time with more powerful versions. The initial capability of SLS is required to be able to lift 70 mt into LEO. It is then planned to be upgraded to 105 mt and then eventually to 130 mt.[108][110]		Exploration Flight Test 1 (EFT-1), an unmanned test flight of Orion's crew module, was launched on December 5, 2014, atop a Delta IV Heavy rocket.[110] Exploration Mission-1 (EM-1) is the unmanned initial launch of SLS that would also send Orion on a circumlunar trajectory, which is planned for 2019.[110] The first manned flight of Orion and SLS, Exploration Mission 2 (EM-2) is to launch in 2022; it is a 10- to 14-day mission planned to place a crew of four into Lunar orbit.[110] EM-3 is planned to deliver a crew of 4 to Lunar orbit along with the first module of Deep Space Gateway.		On June 5, 2016, NASA and DARPA announced plans to build a series of new X-planes over the next 10 years.[111] One of the planes will reportedly be a supersonic vehicle that burns low-carbon biofuels and generates quiet sonic booms.[111]		NASA plans to build full scale deep space habitats such at the Nautilus-X and Deep Space Gateway as part of its Next Space Technologies for Exploration Partnerships (NextSTEP) program.[112]		More than 1,000 unmanned missions have been designed to explore the Earth and the solar system.[113] Besides exploration, communication satellites have also been launched by NASA.[114] The missions have been launched directly from Earth or from orbiting space shuttles, which could either deploy the satellite itself, or with a rocket stage to take it farther.		The first US unmanned satellite was Explorer 1, which started as an ABMA/JPL project during the early part of the Space Race. It was launched in January 1958, two months after Sputnik. At the creation of NASA the Explorer project was transferred to this agency and still continues to this day. Its missions have been focusing on the Earth and the Sun, measuring magnetic fields and the solar wind, among other aspects.[115] A more recent Earth mission, not related to the Explorer program, was the Hubble Space Telescope, which as mentioned above was brought into orbit in 1990.[116]		The inner Solar System has been made the goal of at least four unmanned programs. The first was Mariner in the 1960s and '70s, which made multiple visits to Venus and Mars and one to Mercury. Probes launched under the Mariner program were also the first to make a planetary flyby (Mariner 2), to take the first pictures from another planet (Mariner 4), the first planetary orbiter (Mariner 9), and the first to make a gravity assist maneuver (Mariner 10). This is a technique where the satellite takes advantage of the gravity and velocity of planets to reach its destination.[117]		The first successful landing on Mars was made by Viking 1 in 1976. Twenty years later a rover was landed on Mars by Mars Pathfinder.[118]		Outside Mars, Jupiter was first visited by Pioneer 10 in 1973. More than 20 years later Galileo sent a probe into the planet's atmosphere, and became the first spacecraft to orbit the planet.[119] Pioneer 11 became the first spacecraft to visit Saturn in 1979, with Voyager 2 making the first (and so far only) visits to Uranus and Neptune in 1986 and 1989, respectively. The first spacecraft to leave the solar system was Pioneer 10 in 1983. For a time it was the most distant spacecraft, but it has since been surpassed by both Voyager 1 and Voyager 2.[120]		Pioneers 10 and 11 and both Voyager probes carry messages from the Earth to extraterrestrial life.[121][122] Communication can be difficult with deep space travel. For instance, it took about three hours for a radio signal to reach the New Horizons spacecraft when it was more than halfway to Pluto.[123] Contact with Pioneer 10 was lost in 2003. Both Voyager probes continue to operate as they explore the outer boundary between the Solar System and interstellar space.[124]		On November 26, 2011, NASA's Mars Science Laboratory mission was successfully launched for Mars. Curiosity successfully landed on Mars on August 6, 2012, and subsequently began its search for evidence of past or present life on Mars.[125][126][127]		NASA's ongoing investigations include in-depth surveys of Mars (Mars 2020 and InSight) and Saturn and studies of the Earth and the Sun. Other active spacecraft missions are Juno for Jupiter, Cassini for Saturn, New Horizons (for Jupiter, Pluto, and beyond), and Dawn for the asteroid belt. NASA continued to support in situ exploration beyond the asteroid belt, including Pioneer and Voyager traverses into the unexplored trans-Pluto region, and Gas Giant orbiters Galileo (1989–2003), Cassini (1997–), and Juno (2011–).		The New Horizons mission to Pluto was launched in 2006 and successfully performed a flyby of Pluto on July 14, 2015. The probe received a gravity assist from Jupiter in February 2007, examining some of Jupiter's inner moons and testing on-board instruments during the flyby. On the horizon of NASA's plans is the MAVEN spacecraft as part of the Mars Scout Program to study the atmosphere of Mars.[128]		On December 4, 2006, NASA announced it was planning a permanent moon base.[129] The goal was to start building the moon base by 2020, and by 2024, have a fully functional base that would allow for crew rotations and in-situ resource utilization. However, in 2009, the Augustine Committee found the program to be on an "unsustainable trajectory."[130] In 2010, President Barack Obama halted existing plans, including the Moon base, and directed a generic focus on manned missions to asteroids and Mars, as well as extending support for the International Space Station.[131]		Since 2011, NASA's strategic goals have been[132]		In August 2011, NASA accepted the donation of two space telescopes from the National Reconnaissance Office. Despite being stored unused, the instruments are superior to the Hubble Space Telescope.[133]		In September 2011, NASA announced the start of the Space Launch System program to develop a human-rated heavy lift vehicle. The Space Launch System is intended to launch the Orion Multi-Purpose Crew Vehicle and other elements towards the Moon, near-Earth asteroids, and one day Mars.[134] The Orion MPCV conducted an unmanned test launch on a Delta IV Heavy rocket in December 2014.[135]		The James Webb Space Telescope (JWST) is currently scheduled to launch in late 2018.[citation needed]		On August 6, 2012, NASA landed the rover Curiosity on Mars. On August 27, 2012, Curiosity transmitted the first pre-recorded message from the surface of Mars back to Earth, made by Administrator Charlie Bolden:		Hello. This is Charlie Bolden, NASA Administrator, speaking to you via the broadcast capabilities of the Curiosity Rover, which is now on the surface of Mars.		Since the beginning of time, humankind’s curiosity has led us to constantly seek new life...new possibilities just beyond the horizon. I want to congratulate the men and women of our NASA family as well as our commercial and government partners around the world, for taking us a step beyond to Mars. This is an extraordinary achievement. Landing a rover on Mars is not easy – others have tried – only America has fully succeeded. The investment we are making...the knowledge we hope to gain from our observation and analysis of Gale Crater, will tell us much about the possibility of life on Mars as well as the past and future possibilities for our own planet. Curiosity will bring benefits to Earth and inspire a new generation of scientists and explorers, as it prepares the way for a human mission in the not too distant future. Thank you.[136]		NASA's Aeronautics Research Mission Directorate conducts aeronautics research.		NASA has made use of technologies such as the Multi-Mission Radioisotope Thermoelectric Generator (MMRTG), which is a type of Radioisotope thermoelectric generator used on space missions.[137] Shortages of this material have curtailed deep space missions since the turn of the millennia.[138] An example of a spacecraft that was not developed because of a shortage of this material was New Horizons 2.[138]		The earth science research program was created and first funded in the 1980s under the administrations of Ronald Reagan and George H.W. Bush.[139][140]		NASA started an annual competition in 2014 named Cubes in Space.[141] It is jointly organized by NASA and the global education company I Doodle Learning, with the objective of teaching school students aged 11—18 to design and build scientific experiments to be launched into space on a NASA rocket or balloon. On June 21, 2017 the world's smallest satellite, Kalam SAT, built by an Indian team, was launched.[citation needed]		NASA also researches and publishes on climate issues.[142] Its statements concur with the interpretation that the global climate is heating.[143] Bob Walker, who has advised the 45th President of the United States Donald Trump on space issues, has advocated that NASA should focus on space exploration and that its climate study operations should be transferred to other agencies such as NOAA.[144]		NASA's facilities are research, construction and communication centers to help its missions. Some facilities serve more than one application for historic or administrative reasons. NASA also operates a short-line railroad at the Kennedy Space Center and own special aircraft, for instance two Boeing 747 that transport Space Shuttle orbiter.		John F. Kennedy Space Center (KSC), is one of the best-known NASA facilities. It has been the launch site for every United States human space flight since 1968. Although such flights are currently on pause, KSC continues to manage and operate unmanned rocket launch facilities for America's civilian space program from three pads at the adjoining Cape Canaveral Air Force Station.		Lyndon B. Johnson Space Center (JSC) in Houston is home to the Christopher C. Kraft Jr. Mission Control Center, where all flight control is managed for manned space missions. JSC is the lead NASA center for activities regarding the International Space Station and also houses the NASA Astronaut Corps that selects, trains, and provides astronauts as crew members for US and international space missions.		Another major facility is Marshall Space Flight Center in Huntsville, Alabama at which the Saturn 5 rocket and Skylab were developed.[145] The JPL worked together with ABMA, one of the agencies behind Explorer 1, the first American space mission.		The ten NASA field centers are:		Numerous other facilities are operated by NASA, including the Wallops Flight Facility in Wallops Island, Virginia; the Michoud Assembly Facility in New Orleans, Louisiana; the White Sands Test Facility in Las Cruces, New Mexico; and Deep Space Network stations in Barstow, California; Madrid, Spain; and Canberra, Australia.		NASA's budget has generally been approximately 1% of the federal budget from the early 1970s on, after briefly peaking at approximately 4.41% in 1966 during the Apollo program.[27][146] Public perception of NASA's budget has differed significantly from reality; a 1997 poll indicated that most Americans responded that 20% of the federal budget went to NASA.[147]		The percentage of federal budget that NASA has been allocated has been steadily dropping since the Apollo program and in 2012 it was estimated at 0.48% of the federal budget.[148] In a March 2012 meeting of the United States Senate Science Committee, Neil deGrasse Tyson testified that "Right now, NASA’s annual budget is half a penny on your tax dollar. For twice that—a penny on a dollar—we can transform the country from a sullen, dispirited nation, weary of economic struggle, to one where it has reclaimed its 20th century birthright to dream of tomorrow."[149][150]		For Fiscal Year 2015, NASA received an appropriation of US$18.01 billion from Congress—$549 million more than requested and approximately $350 million more than the 2014 NASA budget passed by Congress.[151]		The exhaust gases produced by rocket propulsion systems, both in Earth's atmosphere and in space, can adversely effect the Earth's environment. Some hypergolic rocket propellants, such as hydrazine, are highly toxic prior to combustion, but decompose into less toxic compounds after burning. Rockets using hydrocarbon fuels, such as kerosene, release carbon dioxide and soot in their exhaust.[152] However, carbon dioxide emissions are insignificant compared to those from other sources; on average, the United States consumed 802,620,000 US gallons (3.0382×109 L) gallons of liquid fuels per day in 2014, while a single Falcon 9 rocket first stage burns around 25,000 US gallons (95,000 L) of kerosene fuel per launch.[153][154] Even if a Falcon 9 were launched every single day, it would only represent 0.006% of liquid fuel consumption (and carbon dioxide emissions) for that day. Additionally, the exhaust from LOx- and LH2- fueled engines, like the SSME, is almost entirely water vapor.[155] NASA addressed environmental concerns with its canceled Constellation program in accordance with the National Environmental Policy Act in 2011.[156] In contrast, ion engines use harmless noble gases like xenon for propulsion.[157][158]		On May 8, 2003, Environmental Protection Agency recognized NASA as the first federal agency to directly use landfill gas to produce energy at one of its facilities—the Goddard Space Flight Center, Greenbelt, Maryland.[159]		An example of NASA's environmental efforts is the NASA Sustainability Base. Additionally, the Exploration Sciences Building was awarded the LEED Gold rating in 2010.[160]		Plot of orbits of known Potentially Hazardous Asteroids (size over 460 feet (140 m) and passing within 4.7 million miles (7.6×10^6 km) of Earth's orbit)		Various nebulae observed from a NASA space telescope		1 Ceres		Pluto		Hardware comparison of Apollo, Gemini and Mercury[note 2]		Cassini-Huygens, planned for disposal in 2017		Hubble Space Telescope, astronomy observatory in Earth orbit since 1990. Also visited by the Space Shuttle		Curiosity rover, roving Mars since 2012		James Webb Space Telescope		Orion spacecraft design as of January 2013		Space Launch System concept art						
Gian Francesco Poggio Bracciolini (11 February 1380[2] – 30 October 1459), best known simply as Poggio Bracciolini, was an Italian scholar and an early humanist. He was responsible for rediscovering and recovering a great number of classical Latin manuscripts, mostly decaying and forgotten in German, Swiss, and French monastic libraries. His most celebrated find was De rerum natura, the only surviving work by Lucretius.						Poggio di Guccio (the surname Bracciolini was added during his career)[3] was born at the village of Terranuova, since 1862 renamed in his honour Terranuova Bracciolini, near Arezzo in Tuscany.		Taken by his father to Florence to pursue the studies for which he appeared so apt, he studied Latin under Giovanni Malpaghino of Ravenna, the friend and protégé of Petrarch. His distinguished abilities and his dexterity as a copyist of manuscripts brought him into early notice with the chief scholars of Florence: both Coluccio Salutati and Niccolò de' Niccoli befriended him. He studied notarial law, and, at the age of twenty-one he was received into the Florentine notaries' guild, the Arte dei giudici e notai.		In October 1403, on high recommendations from Salutati and Leonardo Bruni ("Leonardo Aretino") he entered the service of Cardinal Landolfo Maramaldo, Bishop of Bari, as his secretary, and a few months later he was invited to join the Chancery of Apostolic Briefs in the Roman Curia of Pope Boniface IX, thus embarking on eleven turbulent years during which he served under four successive popes (1404–1415); first as scriptor (writer of official documents), soon moving up to abbreviator, then scriptor penitentiarius, and scriptor apostolicus. Under Martin V he reached the top rank of his office, as Apostolicus Secretarius, papal secretary. As such he functioned as a personal attendant (amanuensis) of the Pope, writing letters at his behest and dictation, with no formal registration of the briefs, but merely preserving copies. He was esteemed for his excellent Latin, his extraordinarily beautiful book hand, and as occasional liaison with Florence, which involved him in legal and diplomatic work.		Throughout his long office of 50 years, Poggio served a total of seven popes: Boniface IX (1389–1404), Innocent VII (1404–1406), Gregory XII (1406–1415), Antipope John XXIII (1410–1415), Martin V (1417–1431), Eugenius IV (1431–1447), and Nicholas V (1447−1455). While he held his office in the Curia through that momentous period, which saw the Councils of Konstanz (1414–1418), in the train of Pope John XXIII, and of Basel (1431–1449), and the final restoration of the papacy under Nicholas V (1447), he was never attracted to the ecclesiastical life (and the lure of its potential riches). In spite of his meager salary in the Curia, he remained a layman to the end of his life.		The greater part of Poggio's long life was spent in attendance to his duties in the Roman Curia at Rome and the other cities the pope was constrained to move his court. Although he spent most of his adult life in his papal service, he considered himself a Florentine working for the papacy. He actively kept his links to Florence and remained in constant communication with his learned and influential Florentine friends: Coluccio Salutati (1331–1406), Niccolò de' Niccoli (1364–1437), Lorenzo de' Medici (1395–1440), Leonardo Bruni (Chancellor, 1369–1444), Carlo Marsuppini ("Carlo Aretino", Chancellor, 1399–1453), and Cosimo de' Medici (1389–1464).		After Martin V was elected as the new pope in November 1417, Poggio, although not holding any office, accompanied his court to Mantua in late 1418, but, once there, decided to accept the invitation of Henry, Cardinal Beaufort, bishop of Winchester, to go to England. His five years spent in England, until returning to Rome in 1423, were the least productive and satisfactory of his life.[4]		Poggio resided in Florence during 1434−36 with Eugene IV. On the proceeds of a sale of a manuscript of Livy in 1434, he built himself a villa in the Valdarno, which he adorned with a collection of antique sculpture (notably a series of busts meant to represent thinkers and writers of Antiquity), coins and inscriptions, works that were familiar to his friend Donatello.		In December 1435, at age 56, tired of the unstable character of his single life, Poggio left his long-term mistress and delegitimized the fourteen children he had had with the mistress, scoured Florence for a wife, and married a girl not yet eighteen, Selvaggia dei Buondelmonti, of a noble Florentine family. In spite of the remonstrances and dire predictions of all his friends about the age discrepancy, the marriage was a happy one, producing five sons and a daughter. Poggio wrote a spate of long letters to justify his move, and composed one of his famous dialogues, An Seni Sit uxor ducenda (On Marriage in Old Age, 1436)[5]		From 1439 to 1442 during the Council of Florence, Poggio also lived in Florence.		In his quarrel against Lorenzo Valla — an expert at philological analysis of ancient texts, a redoubtable opponent endowed with a superior intellect, and a hot temperament fitted to protracted disputation − Poggio found his match.[6] Poggio started in February 1452 with a full-dress critique of the Elegantiae, Valla's major work on Latin language and style, where he supported a critical use of Latin eruditio going beyond pure admiration and respectful imitatio of the classics.		At stake was the new approach of the humanae litterae (profane classical Greek and Latin literature) in relation to the divinae litterae (biblical exegesis of the Judeo-Christian "sacred scriptures"). Valla claimed that biblical texts could be subjected to the same philological criticism as the great classics of antiquity. Poggio held that humanism and theology were separate fields of inquiry, and labeled Valla's mordacitas (radical criticism) as dementia.[7]		Poggio's series of five Orationes in Laurentium Vallam (re-labeled Invectivae by Valla) were countered, line by line, by Valla's Antidota in Pogium (1452−53). It is remarkable that eventually the belligerents acknowledged their talents, gained their mutual respect, and prompted by Filelfo, reconciled, and became good friends.		Shepherd finely comments on Valla's advantage in the literary dispute: the power of irony and satire (making a sharp imprint on memory) versus the ploddingly heavy dissertation (that is quickly forgotten).		These sportive polemics among the early Italian humanists were famous, and spawned a literary fashion in Europe which reverberated later, for instance, in Scaliger's contentions with Scioppius and Milton's with Salmasius.		Erasmus, in 1505, discovered Lorenzo Valla's Adnotationes in Novum Testamentum (New Testament Notes), which encouraged him to pursue the textual criticism of the Holy Scriptures, free of all academic entanglements that might cramp or hinder his scholarly independence — contributing to Erasmus's stature of leading Dutch Renaissance humanist.[8] In his introduction, Erasmus declared his support of Valla's thesis against the invidia of jealous scholars such as Poggio, whom he unfairly described as "a petty clerk so uneducated that even if he were not indecent he would still not be worth reading, and so indecent that he would deserve to be rejected by good men however learned he was." (Quoted in Salvatore I. Camporeale in his essay on the Poggio-Lorenzo dispute).		After the death, in April 1453, of his intimate friend Carlo Aretino, who had been the Chancellor of the Florentine Republic, the choice of his replacement, mostly dictated by Cosimo de' Medici, fell upon Poggio. He resolved to retire from his service of 50 years in the Chancery of Rome, and returned to Florence to assume this new function. This coincided with the news of the fall of Constantinople to the Ottomans.		Poggio's declining days were spent in the discharge of his prestigious Florentine office − glamorous at first, but soon turned irksome − conducting his intense quarrel with Lorenzo Valla, editing his correspondence for publication, and in the composition of his history of Florence. He died in 1459 before he could put the final polish to his work, and was buried in the church of Santa Croce. A statue by Donatello and a portrait by Antonio del Pollaiuolo remain to commemorate a citizen who chiefly for his services to humanistic literature deserved the notice of posterity. During his life, Poggio kept acquiring properties around Florence and invested in the city enterprises with the Medici bank. At his death, his gross assets amounted to 8,500 florins, with only 137 families in Florence owning a larger capital. His wife, five sons and daughter all survived him.		After July 1415 — Antipope John XXIII had been deposed by the Council of Constance and the Roman Pope Gregory XII had abdicated — the papal office remained vacant for two years, which gave Poggio some leisure time in 1416/17 for his pursuit of manuscript hunting. In the spring of 1416 (sometime between March and May), Poggio visited the baths at the German spa of Baden. In a long letter to Niccoli (p. 59−68) he reported his discovery of an "Epicurean" lifestyle — one year before finding Lucretius — where men and women bathe together, barely separated, in minimum clothing: "I have related enough to give you an idea what a numerous school of Epicureans is established in Baden. I think this must be the place where the first man was created, which the Hebrews call the garden of pleasure. If pleasure can make a man happy, this place is certainly possessed of every requisite for the promotion of felicity." (p. 66)		Poggio was marked by the passion of his teachers for books and writing, inspired by the first generation of Italian humanists centered around Francesco Petrarch (1304–1374), who had revived interest in the forgotten masterpieces of Livy and Cicero, Giovanni Boccaccio (1313–1375) and Coluccio Salutati (1331–1406). Poggio joined the second generation of civic humanists forming around Salutati. Resolute in glorifying studia humanitatis (the study of "humanities", a phrase popularized by Leonardo Bruni), learning (studium), literacy (eloquentia), and erudition (eruditio) as the chief concern of man, Poggio ridiculed the folly of popes and princes, who spent their time in wars and ecclesiastical disputes instead of reviving the lost learning of antiquity.		The literary passions of the learned Italians in the new Humanist Movement, which were to influence the future course of both Renaissance and Reformation, were epitomized in the activities and pursuits of this self-made man, who rose from the lowly position of scribe in the Roman Curia to the privileged role of apostolic secretary.		He became devoted to the revival of classical studies amid conflicts of popes and antipopes, cardinals and councils, in all of which he played an official part as first-row witness, chronicler and (often unsolicited) critic and adviser.		Thus, when his duties called him to the Council of Constance in 1414, he employed his forced leisure in exploring the libraries of Swiss and Swabian abbeys. His great manuscript finds date to this period, 1415−1417. The treasures he brought to light at Reichenau, Weingarten, and above all St. Gall, retrieved from the dust and abandon many lost masterpieces of Latin literature, and supplied scholars and students with the texts of authors whose works had hitherto been accessible only in fragmented or mutilated copies.		In his epistles he described how he recovered Cicero's Pro Sexto Roscio, Quintilian, Statius' Silvae, part of Gaius Valerius Flaccus, and the commentaries of Asconius Pedianus at St. Gallen. Manuscripts of Columella, Silius Italicus, Manilius and Vitruvius were unearthed, copied, and communicated to the learned. He carried on the same untiring research in many Western European countries. In 1415 at Cluny he found Cicero's complete great forensic orations, previously only partially available.[9] At Langres in the summer of 1417 he discovered Cicero's Oration for Caecina and nine other hitherto unknown orations of Cicero's.[10] At Monte Cassino, in 1425, a manuscript of Frontinus' late first century De aquaeductu on the ancient aqueducts of Rome. He was also credited with having recovered Ammianus Marcellinus, Nonius Marcellus, Probus, Flavius Caper and Eutyches.		If a codex could not be obtained by fair means, he was not above using subterfuge, as when he bribed a monk to abstract a Livy and an Ammianus from the library of Hersfeld Abbey.		Poggio's most famous was his January 1417 discovery of the only surviving manuscript of Lucretius's De rerum natura ("On the Nature of Things") known at the time, in a German monastery (never named by Poggio, but probably Fulda). Poggio spotted the name, which he remembered as quoted by Cicero. This was a Latin poem of 7,400 lines, divided into six books, giving a full description of the world as viewed by the ancient Greek philosopher Epicurus (see Epicureanism). It has been translated as On the Nature of the Universe (Oxford World's Classics). The manuscript found by Poggio was not preserved, but he sent the copy he had ordered to his friend Niccolò de' Niccoli, who made a transcription in his renowned book hand (as Niccoli was the creator of italic script), which became the model for the more than fifty other copies circulating at the time. Poggio would later complain that Niccoli had not returned his original copy for 14 years. Later, two 9th-century manuscripts were discovered, the O ("Oblongus", ca. 825) and Q ("Quadratus") codices, now kept at Leiden University.[11] The book was first printed in 1473.		The Pulitzer Prize-winning 2011 book The Swerve: How the World Became Modern by Stephen Greenblatt is a narrative of the discovery of the old Lucretius manuscript by Poggio.[12] Greenblatt analyzes the poem's subsequent impact on the development of the Renaissance, the Reformation, and modern science.[13][14][15]		Poggio cultivated and maintained throughout his life close friendships with some of the most important learned men of the age: Niccolò de' Niccoli (the inventor of the italic script), Leonardo Bruni ("Leonardo Aretino"), Lorenzo and Cosimo de' Medici, Carlo Marsuppini ("Carlo Aretino"), Guarino Veronese, Ambrogio Traversari, Francesco Barbaro, Francesco Accolti, Feltrino Boiardo, Lionello d'Este (who became Marquis of Ferrara, 1441–1450), and many others, who all shared his passion for retrieving the manuscripts and art of the ancient Greco-Roman world. His early friendship with Tommaso da Sarzana stood Poggio in good stead when his learned friend was elected pope, under the name of Nicolas V (1447−1455), a proven protector of scholars and an active sponsor of learning, who founded the Vatican library in 1448 with 350 codices.		These learned men were adept at maintaining an extended network of personal relations among a circle of talented and energetic scholars in which constant communication was secured by an immense traffic of epistolary exchanges.		They were bent on creating a rebirth of intellectual life for Italy by means of a vital reconnection with the texts of antiquity. Their worldview was eminently characteristic of Italian humanism in the earlier Italian Renaissance, which eventually spread all over Western Europe and led to the full Renaissance and the Reformation, announcing the modern age.		Poggio, like Aeneas Sylvius Piccolomini (who became Pius II), was a great traveller, and wherever he went he brought enlightened powers of observation trained in liberal studies to bear upon the manners of the countries he visited. We owe to his pen curious remarks on English and Swiss customs, valuable notes on the remains of ancient monuments in Rome, and a singularly striking portrait of Jerome of Prague as he appeared before the judges who condemned him to the stake.		In literature he embraced the whole sphere of contemporary studies, and distinguished himself as an orator, a writer of rhetorical treatises, a panegyrist of the dead, a passionate impugner of the living, a sarcastic polemist, a translator from the Greek, an epistolographer and grave historian and a facetious compiler of fabliaux in Latin.		His cultural/social/moral essays covered a wide range of subjects concerning the interests and values of his time:		These compositions, all written in Latin − and reviving the classical form of dialogues, between himself and learned friends[18] − belonged to a genre of socratic reflections which, since Petrarch set the fashion, was highly praised by Italian men of letters and made Poggio famous throughout Italy. They exemplify his conception of studia humanitatis as an epitome of human knowledge and wisdom reserved only to the most learned, and the key to what the ancient philosophers called "virtue" and "the good". And thus, they are invaluable windows into the knowledge and Weltanschauung of his age − geography, history, politics, morals, social aspects — and the emergence of the new values of the "Humanist Movement". They are loaded with rich nuggets of fact embedded in subtle disquisitions, with insightful comments, brilliant illustrations, and a wide display of historical and contemporary references. Poggio was always inclined to make objective observations and clinical comparisons between various cultural mores, for instance ancient Roman practices versus modern ones, or Italians versus the English. He compared the eloquence of Jerome of Prague and his fortitude before death with ancient philosophers. The abstruse points of theology presented no interest to him, only the social impact of the Church did, mostly as an object of critique and ridicule. On the Vicissitudes of Fortune became famous for including in book IV an account of the 25-year voyage of the Venetian adventurer Niccolo de' Conti in Persia and India, which was translated into Portuguese on express command of the Portuguese King Emmanuel I. An Italian translation was made from the Portuguese.		Poggio's Historia Florentina (History of Florence), is a history of the city from 1350 to 1455, written in avowed imitation of Livy and Sallust, and possibly Thucydides (available in Greek, but translated into Latin by Valla only in 1450–52) in its use of speeches to explain decisions. Poggio continued Leonardo Bruni's History of the Florentine People, which closed in 1402, and is considered the first modern history book. Poggio limited his focus to external events, mostly wars, in which Florence was the defensor Tusciae and of Italian liberty. But Poggio also pragmatically defended Florence's expansionist policies to insure the "safety of the Florentine Republic", which became the key motive of its history, as a premonition of Machiavelli's doctrine. Conceding to superior forces becomes an expression of reason and advising it a mark of wisdom. His intimate and vast experience of Italian affairs inculcated in him a strong sense of realism, echoing his views on laws expressed in his second Historia disceptativa convivialis (1450). Poggio's beautiful rhetorical prose turns his Historia Florentina into a vivid narrative, with a sweeping sense of movement, and a sharp portrayal of the main characters, but it also exemplifies the limitations of the newly emerging historical style, which, in the work of Leonardo Bruni, Carlo Marsuppini and Pietro Bembo, retained "romantic" aspects and did not reach yet the weight of objectivity later expected by the school of modern historians (especially since 1950).[19]		His Liber Facetiarum (1438−1452), or Facetiae, a collection of humorous and indecent tales expressed in the purest Latin Poggio could command, are the works most enjoyed today: they are available in several English translations. This book is chiefly remarkable for its unsparing satires on the monastic orders and the secular clergy. "The worst men in the world live in Rome, and worse than the others are the priests, and the worst of the priests they make cardinals, and the worst of all the cardinals is made pope." Poggio's book became an internationally popular work in all countries of Western Europe, and has gone through multiple editions until modern times.		In addition Poggio's works included his Epistolae, a collection of his letters, a most insightful witness of his remarkable age, in which he gave full play to his talent as chronicler of events, to his wide range or interests, and to his most acerbic critical sense.		In the way of many humanists of his time, Poggio rejected the vernacular Italian and always wrote only in Latin, and translated works from Greek into that language. His letters are full of learning, charm, detail, and amusing personal attack on his enemies and colleagues. It is also noticeable as illustrating the Latinizing tendency of an age which gave classic form to the lightest essays of the fancy.		Poggio was a fluent and copious writer in Latin, admired for his classical style inspired from Cicero, if not fully reaching the elegance of his model, but outstanding by the standards of his age. Italy was barely emerging from what Petrarch had termed the Dark Ages, while Poggio was facing the unique challenge of making "those frequent allusions to the customs and transactions of his own times, which render his writings so interesting... at a period when the Latin language was just rescued from the grossest barbarism... the writings of Poggio are truly astonishing. Rising to a degree of elegance, to be sought for in vain in the rugged Latinity of Petrarca and Coluccio Salutati..."[20] His knowledge of the ancient authors was wide, his taste encompassed all genres, and his erudition was as good as the limited libraries of the time allowed, when books were extremely rare and extraordinarily expensive.[21]		Good instruction in Greek was uncommon and hard to obtain in Italy. Proficient teachers, such as Ambrogio Traversari, were few and highly valued. Manuel Chrysoloras used to be occasionally credited as having instructed Poggio in Greek during his youth, but Shepherd cites a letter by Poggio to Niccolò Niccoli stating that he began the study of Greek in 1424, in Rome at age 44 (Shepherd, p. 6). Poggio's preface to his dialogue On Avarice notes that his task was made the harder "because I can neither translate from the Greek language for our benefit, nor are my abilities such that I should wish to discuss in public anything drawn from these writings"[22] Consequently, his knowledge of Greek never attained the quality of his Latin. His best-efforts translation of Xenophon's Cyropaedia into Latin cannot be praised for accuracy by modern standards. But he was the first critic to label it a "political romance", instead of history. He also translated Lucian's Ass, considered an influence of Apuleius's Latin masterpiece, The Golden Ass.		Among contemporaries he passed for one of the most formidable polemical or gladiatorial rhetoricians; and a considerable section of his extant works is occupied by a brilliant display of his sarcastic wit and his unlimited inventiveness in "invectives". One of these, published on the strength of Poggio's old friendship with the new pontiff, Nicolas V, the dialogue Against Hypocrites, was actuated by a vindictive hatred at the follies and vices of ecclesiastics. This was but another instance of his lifelong obstinate denouncing of the corruption of clerical life in the 15th century. Nicholas V then asked Poggio to deliver a philippic against Amadeus VIII, Duke of Savoy, who claimed to be the Antipope Felix V — a ferocious attack with no compunction in pouring on the Duke fantastic accusations, unrestrained abuse and the most extreme anathemas.		Invectivae ("Invectives") were a specialized literary genre used during the Italian Renaissance, tirades of exaggerated obloquy aimed at insulting and degrading an opponent beyond the bounds of any common decency. Poggio's most famous "Invectives" were those he composed in his literary quarrels, such as with George of Trebizond, Bartolomeo Facio, and Antonio Beccadelli, the author of a scandalous Hermaphroditus, inspired by the unfettered eroticism of Catullus and Martial. All the resources of Poggio's rich vocabulary of the most scurrilous Latin were employed to stain the character of his target; every imaginable crime was imputed to him, and the most outrageous accusations proffered, without any regard to plausibility. Poggio's quarrels against Francesco Filelfo and also Niccolo Perotti pitted him against well-known scholars.		Poggio was famous for his beautiful and legible book hand. The formal humanist script he invented developed into Roman type, which remains popular as a printing font today (his friend Niccolò de' Niccoli's script in turn developed into the Italic type, first used by Aldus Manutius in 1501).[23]		
A bulletin board (pinboard, pin board, noticeboard, or notice board in British English) is a surface intended for the posting of public messages, for example, to advertise items wanted or for sale, announce events, or provide information. Bulletin boards are often made of a material such as cork to facilitate addition and removal of messages, as well as a writing surface such as blackboard or whiteboard. A bulletin board which combines a pinboard (corkboard) and writing surface is known as a combination bulletin board. Bulletin boards can also be entirely in the digital domain and placed on computer networks so people can leave and erase messages for other people to read and see, as in a bulletin board system.		Bulletin boards are particularly prevalent at universities. They are used by many sports groups and extracurricular groups and anything from local shops to official notices. Dormitory corridors, well-trafficked hallways, lobbies, and freestanding kiosks often have cork boards attached to facilitate the posting of notices. At some universities, lampposts, bollards, trees, and walls often become impromptu posting sites in areas where official boards are sparse in number.		Internet forums are a replacement for traditional bulletin boards. Online bulletin boards are sometimes referred to as message boards. The terms bulletin board, message board and even Internet forum are interchangeable, although often one bulletin board or message board can contain a number of Internet forums or discussion groups. An online board can serve the same purpose as a physical bulletin board.		Magnet boards, or magnetic bulletin boards, are a popular substitute for cork boards because they lack the problem of board deterioration from the insertion and removal of pins over time.		
Alan Dundes (September 8, 1934 – March 30, 2005)[1] was a folklorist at the University of California, Berkeley. His work was said to have been central to establishing the study of folklore as an academic discipline.[citation needed] He wrote 12 books, both academic and popular, and edited or co-wrote two dozen more.[2] One of his most notable articles was called "Seeing is Believing" in which he indicated that Americans value the sense of sight more than the other senses.		He introduced the concept "allomotif" (coined in an analogy with "allomorph", to complement the concept of "motifeme" (cf. "morpheme") introduced by Kenneth L. Pike) as concept to be used in the analysis of the structures of folktales in terms of motifs identified in them.[3][4]						Dundes attended Yale University, where he studied English[1] and met his wife Carolyn. Sure that he would be drafted upon completion of his studies, Dundes joined the ROTC and trained to become a naval communications officer. When it turned out that the ship he was to be posted to, stationed in the Bay of Naples, already had a communications officer, Dundes asked what else that ship might need, not wanting to give up such a choice assignment. He then spent two years maintaining artillery guns on a ship in the Mediterranean. Upon completion of his service, Dundes attended Indiana University to pursue a Ph.D in folklore. At Indiana, he studied under the father of American Folklore, Richard Dorson.[1] He quickly established himself as a force to be reckoned with in the field of folkloristics. He completed his degree very quickly and went on to a teaching position at the University of Kansas where he stayed for only a year before being offered a position in the University of California, Berkeley anthropology department teaching folklore. Dundes held this position for 42 years, until his death in 2005.		Alan Dundes was an engaging lecturer, his Introduction to Folklore course attracting many students. In this course, students were introduced to the many various forms of folklore, from myth, legend, and folktale to proverbs and riddles to jokes, games, and folkspeech (slang), to folk belief and foodways. The final project for this course required that each student collect, identify, and analyze 40 items of folklore. All of this material (about 500,000 items) is housed and cataloged in the Berkeley Folklore Archives. Dundes also taught undergraduate courses in American folklore, and psychoanalytic approaches to folklore (his favorite approach) in addition to graduate seminars on the history of folkloristics, from an international perspective, and the history and progression of folklore theory.		Dundes was also a great supporter of the New Student Orientation Program at UC Berkeley (CalSO). He frequently gave the opening address during summer orientation programs, whetting students' appetites about the type of instruction they might receive at the University. These addresses were littered with jokes and stories which were a trademark of Dundes' lectures in his popular anthropology class and were a favorite of both in-coming students and the orientation staff alike.[citation needed]		Strongly opinionated, Dundes was not at all averse to the controversy that his theories often generated. He dealt frequently with folklore as an expression of unconscious desires and anxieties and was of the opinion that if people reacted strongly to what he had to say, he had probably hit a nerve and was probably on to something. Some of his more controversial work involved examining the New Testament and the Qur'an as folklore.[5]		However, of all his articles, the one that earned him death threats was "Into the Endzone for a Touchdown", an exploration via psychoanalysis of what he contended was the homoerotic subtext inherent in the terminology and rituals surrounding American football.[6] In 1980, Dundes was invited to give the presidential address at the American Folklore Society annual meeting.[7] His presentation, later published as a monograph titled "Life is Like a Chicken Coop Ladder", uses folkspeech, customs, material culture, and so forth seeking to demonstrate an anal-erotic fixation of German national character.[6] Reaction to this paper was incredibly strong[6] and because of it, Dundes declined to attend the AFS annual meeting for the next 20 years.[citation needed] When he finally did attend again, in 2004, he again gave a plenary address, this time taking his fellow folklorists to task for being weak on theory. In his opinion, the presentation of data, no matter how thorough, is useless without the development and application of theory to that data. It is not enough to simply collect, one must do something with what one has collected.[8] In 2012, linguist Anatol Stefanowitsch credited Dundes with having given rise to a still prevalent "stereotype about Germany as a culture enamored with excretion", but called his monograph "unstructured, poorly argued and flimsily sourced" and "methodologically flawed because he only looked for evidence supporting his theory, and not – as even a folklorist should – for evidence against his theory".[9]		Dundes fiercely defended the importance of the discipline of folkloristics throughout his career. Towards the end of his life, he received an envelope containing a check from a former student, which he asked his wife to open. She read the figure out as $1,000. In fact, the check was for $1,000,000. This money allowed Dundes to endow the university with a Distinguished Professorship in Folkloristics, thereby ensuring that upon his retirement folklore would not be abandoned in the department.[10]		The former student and benefactor wished to remain anonymous. Apparently he or she called the university prior to the donation to find out if Dundes was still teaching, or as Dundes told it, "to see if I was still alive." The student mentioned that he or she intended to send a check, but Dundes said he was not sure the student would follow through.		The check was made out to the university, Dundes said, but with instructions that he could use it in any manner he saw fit.		"I could just take all my students to Fiji and have one hell of a party," he said.		The professor instead decided to invest it in the study of folklore. The money funds a Distinguished Professorship of Folkloristics and helps fund the university's folklore archives and provides grants for folklore students.[11]		Shortly before his death, Dundes was interviewed by filmmaker Brian Flemming for his documentary, The God Who Wasn't There. He prominently recounted Lord Raglan's 22-point scale from his 1936 book The Hero, in which he ranks figures possessing similar divine attributions.[12] An extended interview[13] is on the DVD version of the documentary.		Dundes collapsed and died while giving a graduate seminar.[2]		Before the term folkloristics can be fully understood, it is necessary to understand that the terms folk and lore are defined in many different ways. While some use the word folk to mean only peasants or remote cultures, the folklorist Alan Dundes (1934–2005) of the University of California at Berkeley calls this definition a “misguided and narrow concept of the folk as the illiterate in a literate society” (Devolutionary Premise, 13).		Dundes is often credited with the promotion of folkloristics as a term denoting a specific field of academic study and applies instead what he calls a “modern” flexible social definition for folk: two or more persons who have any trait in common and express their shared identity through traditions. Dundes explains this point best in his essay, The Devolutionary Premise in Folklore Theory (1969):		With this expanded social definition of folk, a wider view of the material considered to be folklore also emerged that includes, as William Wilson points out, “things people make with words (verbal lore), things they make with their hands (material lore), and things they make with their actions (customary lore)” (2006, 85).		Another implication of this broader defining of the term folk, according to Dundes, is that folkloristic work is interpretative and scientific rather than descriptive or devoted solely to folklore preservation. In the 1978 collection of his academic work, Essays in Folkloristics, Dundes declares in his preface, “Folkloristics is the scientific study of folklore just as linguistics is the scientific study of language. [. . .] It implies a rigorous intellectual discipline with some attempt to apply theory and method to the materials of folklore” (vii). In other words, Dundes advocates the use of folkloristics as the preferred term for the academic discipline devoted to the study of folklore.		According to Dundes, folkloristic work will probably continue to be important in the future. Dundes writes, “folklore is a universal: there has always been folklore and in all likelihood there will always be folklore. As long as humans interact and in the course of so doing employ traditional forms of communication, folklorists will continue to have golden opportunities to study folklore” (Devolutionary Premise, 19). According to folklorist William A. Wilson, “the study of folklore, therefore, is not just a pleasant pastime useful primarily for whiling away idle moments. Rather, it is centrally and crucially important in our attempts to understand our own behavior and that of our fellow human beings" (2006, 203).		
The New Yorker is an American magazine of reportage, commentary, criticism, essays, fiction, satire, cartoons, and poetry. It is published by Condé Nast. Started as a weekly in 1925, the magazine is now published 47 times annually, with five of these issues covering two-week spans.		Although its reviews and events listings often focus on the cultural life of New York City, The New Yorker has a wide audience outside of New York and is read internationally. It is well known for its illustrated and often topical covers, its commentaries on popular culture and eccentric Americana, its attention to modern fiction by the inclusion of short stories and literary reviews, its rigorous fact checking and copyediting, its journalism on politics and social issues, and its single-panel cartoons sprinkled throughout each issue.						The New Yorker debuted on February 21, 1925. It was founded by Harold Ross and his wife, Jane Grant, a New York Times reporter. Ross wanted to create a sophisticated humor magazine that would be different from perceivably "corny" humor publications such as Judge, where he had worked, or Life. Ross partnered with entrepreneur Raoul H. Fleischmann (who founded the General Baking Company[4]) to establish the F-R Publishing Company. The magazine's first offices were at 25 West 45th Street in Manhattan. Ross edited the magazine until his death in 1951. During the early, occasionally precarious years of its existence, the magazine prided itself on its cosmopolitan sophistication. Ross famously declared in a 1925 prospectus for the magazine: "It has announced that it is not edited for the old lady in Dubuque."[5]		Although the magazine never lost its touches of humor, it soon established itself as a pre-eminent forum for serious fiction literature and journalism. Shortly after the end of World War II, John Hersey's essay Hiroshima filled an entire issue. In subsequent decades the magazine published short stories by many of the most respected writers of the 20th and 21st centuries, including Ann Beattie, Truman Capote, John Cheever, Roald Dahl, Mavis Gallant, Geoffrey Hellman, John McNulty, Joseph Mitchell, Alice Munro, Haruki Murakami, Vladimir Nabokov, John O'Hara, Dorothy Parker, Philip Roth, J. D. Salinger, Irwin Shaw, James Thurber, John Updike, Eudora Welty, Stephen King, and E. B. White. Publication of Shirley Jackson's "The Lottery" drew more mail than any other story in the magazine's history.		In its early decades, the magazine sometimes published two or even three short stories a week, but in recent years the pace has remained steady at one story per issue. While some styles and themes recur more often than others in its fiction, the stories are marked less by uniformity than by variety, and they have ranged from Updike's introspective domestic narratives to the surrealism of Donald Barthelme, and from parochial accounts of the lives of neurotic New Yorkers to stories set in a wide range of locations and eras and translated from many languages.[citation needed] Kurt Vonnegut said that The New Yorker has been an effective instrument for getting a large audience to appreciate modern literature. Vonnegut's 1974 interview with Joe David Bellamy and John Casey contained a discussion of The New Yorker's influence:		[T]he limiting factor [in literature] is the reader. No other art requires the audience to be a performer. You have to count on the reader's being a good performer, and you may write music which he absolutely can't perform – in which case it's a bust. Those writers you mentioned and myself are teaching an audience how to play this kind of music in their heads. It's a learning process, and The New Yorker has been a very good institution of the sort needed. They have a captive audience, and they come out every week, and people finally catch on to Barthelme, for instance, and are able to perform that sort of thing in their heads and enjoy it.[6]		The non-fiction feature articles (which usually make up the bulk of the magazine's content) cover an eclectic array of topics. Recent subjects have included eccentric evangelist Creflo Dollar, the different ways in which humans perceive the passage of time, and Münchausen syndrome by proxy.		The magazine is notable for its editorial traditions. Under the rubric Profiles, it publishes articles about notable people such as Ernest Hemingway, Henry R. Luce and Marlon Brando, Hollywood restaurateur Michael Romanoff, magician Ricky Jay and mathematicians David and Gregory Chudnovsky. Other enduring features have been "Goings on About Town", a listing of cultural and entertainment events in New York, and "The Talk of the Town", a miscellany of brief pieces—frequently humorous, whimsical or eccentric vignettes of life in New York—written in a breezily light style, or feuilleton, although in recent years the section often begins with a serious commentary. For many years, newspaper snippets containing amusing errors, unintended meanings or badly mixed metaphors ("Block That Metaphor") have been used as filler items, accompanied by a witty retort. There is no masthead listing the editors and staff. And despite some changes, the magazine has kept much of its traditional appearance over the decades in typography, layout, covers and artwork. The magazine was acquired by Advance Publications, the media company owned by Samuel Irving Newhouse Jr, in 1985,[7] for $200 million when it was earning less than $6 million a year.[8]		Ross was succeeded as editor by William Shawn (1951–87), followed by Robert Gottlieb (1987–92) and Tina Brown (1992–98). Among the important nonfiction authors who began writing for the magazine during Shawn's editorship were Dwight Macdonald, Kenneth Tynan, and Hannah Arendt; to a certain extent all three authors were controversial, Arendt the most obviously so (her Eichmann in Jerusalem reportage appeared in the magazine before it was published as a book), but in each case Shawn proved an active champion.		Brown's nearly six-year tenure attracted more controversy than Gottlieb's or even Shawn's, thanks to her high profile (Shawn, by contrast, had been an extremely shy, introverted figure) and the changes which she made to a magazine that had retained a similar look and feel for the previous half-century. She introduced color to the editorial pages (several years before The New York Times) and photography, with less type on each page and a generally more modern layout. More substantively, she increased the coverage of current events and hot topics such as celebrities and business tycoons, and placed short pieces throughout "Goings on About Town", including a racy column about nightlife in Manhattan. A new letters-to-the-editor page and the addition of authors' bylines to their "Talk of the Town" pieces had the effect of making the magazine more personal. The current editor of The New Yorker is David Remnick, who succeeded Brown in July 1998.[9]		Tom Wolfe wrote about the magazine: "The New Yorker style was one of leisurely meandering understatement, droll when in the humorous mode, tautological and litotical when in the serious mode, constantly amplified, qualified, adumbrated upon, nuanced and renuanced, until the magazine's pale-gray pages became High Baroque triumphs of the relative clause and appository modifier".[10]		Joseph Rosenblum, reviewing Ben Yagoda's About Town, a history of the magazine from 1925 to 1985, wrote, "... The New Yorker did create its own universe. As one longtime reader wrote to Yagoda, this was a place 'where Peter DeVries ... [sic] was forever lifting a glass of Piesporter, where Niccolò Tucci (in a plum velvet dinner jacket) flirted in Italian with Muriel Spark, where Nabokov sipped tawny port from a prismatic goblet (while a Red Admirable perched on his pinky), and where John Updike tripped over the master's Swiss shoes, excusing himself charmingly".[11]		As far back as the 1940s the magazine's commitment to fact-checking was already well known.[12] Yet the magazine played a role in a literary scandal and defamation lawsuit over two 1990s articles by Janet Malcolm, who wrote about Sigmund Freud's legacy. Questions were raised about the magazine's fact-checking process.[13] As of 2010, The New Yorker employs 16 fact checkers.[14] In July 2011, the magazine was sued for defamation in United States district court for a July 12, 2010 article written by David Grann,[15][16] but the case was summarily dismissed.[17][18]		Since the late 1990s, The New Yorker has used the Internet to publish current and archived material. It maintains a website with some content from the current issue (plus exclusive web-only content). Subscribers have access to the full current issue online, as well as a complete archive of back issues viewable as they were originally printed. In addition, The New Yorker's cartoons are available for purchase online. A digital archive of back issues from 1925 to April 2008 (representing more than 4,000 issues and half a million pages) has also been issued on DVD-ROMs and on a small portable hard drive. More recently, an iPad version of the current issue of the magazine has been released.		In its November 1, 2004 issue, the magazine for the first time endorsed a presidential candidate, choosing to endorse Democrat John Kerry over incumbent Republican George W. Bush.[19] This was continued in 2008 when the magazine endorsed Barack Obama over John McCain,[20] in 2012 when it endorsed Obama over Mitt Romney,[21] and in 2016 when it endorsed Hillary Clinton over Donald Trump.[22]		The New Yorker has featured cartoons (usually gag cartoons) since it began publication in 1925. The cartoon editor of The New Yorker for years was Lee Lorenz, who first began cartooning in 1956 and became a New Yorker contract contributor in 1958.[23] After serving as the magazine's art editor from 1973 to 1993 (when he was replaced by Françoise Mouly), he continued in the position of cartoon editor until 1998. His book The Art of the New Yorker: 1925–1995 (Knopf, 1995) was the first comprehensive survey of all aspects of the magazine's graphics. In 1998, Robert Mankoff took over as cartoon editor and edited at least 14 collections of New Yorker cartoons. In addition, Mankoff usually contributed a short article to each book, describing some aspect of the cartooning process or the methods used to select cartoons for the magazine. Mankoff retired from the magazine in 2017.		The New Yorker's stable of cartoonists has included many important talents in American humor, including Charles Addams, Peter Arno, Charles Barsotti, George Booth, Roz Chast, Tom Cheney, Sam Cobean, Leo Cullum, Richard Decker, J. B. Handelsman, Helen E. Hokinson, Ed Koren, Reginald Marsh, Mary Petty, George Price, Charles Saxon, David Snell, Otto Soglow, Saul Steinberg, William Steig, James Stevenson, Richard Taylor, James Thurber, Pete Holmes, Barney Tobey, and Gahan Wilson.		Many early New Yorker cartoonists did not caption their own cartoons. In his book The Years with Ross, Thurber describes the newspaper's weekly art meeting, where cartoons submitted over the previous week would be brought up from the mail room to be gone over by Ross, the editorial department, and a number of staff writers. Cartoons often would be rejected or sent back to artists with requested amendments, while others would be accepted and captions written for them. Some artists hired their own writers; Helen Hokinson hired James Reid Parker in 1931. (Brendan Gill relates in his book Here at The New Yorker that at one point in the early 1940s, the quality of the artwork submitted to the magazine seemed to improve. It later was found out that the office boy (a teen-aged Truman Capote) had been acting as a volunteer art editor, dropping pieces he didn't like down the far edge of his desk.)[24]		Several of the magazine's cartoons have climbed to a higher plateau of fame. One 1928 cartoon drawn by Carl Rose and captioned by E. B. White shows a mother telling her daughter, "It's broccoli, dear." The daughter responds, "I say it's spinach and I say the hell with it." The phrase "I say it's spinach" entered the vernacular (and three years later, the Broadway musical Face the Music included Irving Berlin's musical number entitled "I Say It's Spinach (And The Hell With It)").[25] The catchphrase "back to the drawing board" originated with the 1941 Peter Arno cartoon showing an engineer walking away from a crashed plane, saying, "Well, back to the old drawing board."[26][27]		The most reprinted is Peter Steiner's 1993 drawing of two dogs at a computer, with one saying, "On the Internet, nobody knows you're a dog". According to Mankoff, Steiner and the magazine have split more than $100,000 in fees paid for the licensing and reprinting of this single cartoon, with more than half going to Steiner.[28][29]		Over seven decades, many hardcover compilations of cartoons from The New Yorker have been published, and in 2004, Mankoff edited The Complete Cartoons of The New Yorker, a 656-page collection with 2004 of the magazine's best cartoons published during 80 years, plus a double CD set with all 68,647 cartoons ever published in the magazine. This features a search function allowing readers to search for cartoons by a cartoonist's name or by year of publication. The newer group of cartoonists in recent years includes Pat Byrnes, Frank Cotham, Michael Crawford, Joe Dator, Drew Dernavich, J. C. Duffy, Carolita Johnson, Zachary Kanin, Farley Katz, Robert Leighton, Glen Le Lievre, Michael Maslin, Ariel Molvig, Paul Noth, Barbara Smaller, David Sipress, Mick Stevens, Julia Suits, Christopher Weyant, P. C. Vey, and Jack Ziegler. The notion that some New Yorker cartoons have punchlines so non sequitur that they are impossible to understand became a subplot in the Seinfeld episode "The Cartoon", as well as a playful jab in an episode of The Simpsons, "The Sweetest Apu".		In April 2005, the magazine began using the last page of each issue for "The New Yorker Cartoon Caption Contest". Captionless cartoons by The New Yorker's regular cartoonists are printed each week. Captions are submitted by readers, and three are chosen as finalists. Readers then vote on the winner, and any resident of the US, UK, Australia, Ireland, or Canada (except Quebec) age 18 or older may vote.[citation needed] Each contest winner receives a print of the cartoon (with the winning caption), signed by the artist who drew the cartoon.		The New Yorker has been the source of a number of movies. Both fiction and non-fiction pieces have been adapted for the big screen, including: Flash of Genius (2008), based on a true account of the invention of the intermittent windshield wiper by John Seabrook; Away From Her, adapted from Alice Munro's short story "The Bear Came Over The Mountain", which debuted at the 2007 Sundance Film Festival; The Namesake (2007), similarly based on Jhumpa Lahiri's novel which originated as a short story in the magazine; The Bridge (2006), based on Tad Friend's 2003 non-fiction piece "Jumpers"; Brokeback Mountain (2005), an adaptation of the short story by Annie Proulx which first appeared in the October 13, 1997, issue of The New Yorker; Jonathan Safran Foer's 2001 debut in The New Yorker, which later came to theaters in Liev Schreiber's debut as both screenwriter and director, Everything is Illuminated (2005); Michael Cunningham's The Hours, which appeared in the pages of The New Yorker before becoming the film that garnered the 2002 Best Actress Academy Award for Nicole Kidman; Adaptation (2002), which Charlie Kaufman based on Susan Orlean's The Orchid Thief, written for The New Yorker; Frank McCourt's Angela's Ashes, which also appeared, in part, in The New Yorker in 1996 before its film adaptation was released in 1999; The Addams Family (1991) and its sequel, Addams Family Values (1993), both inspired by the work of famed New Yorker cartoonist Charles Addams; Brian De Palma's Casualties of War (1989), which began as a New Yorker article by Daniel Lang; Boys Don't Cry (1999), starring Hilary Swank, began as an article in the magazine, and Iris (2001), about the life of Iris Murdoch and John Bayley, the article written by John Bayley for the New Yorker, before he completed his full memoir, the film starring Judi Dench and Jim Broadbent; The Swimmer (1968), starring Burt Lancaster, based on a John Cheever short story from The New Yorker; In Cold Blood (1967), the widely nominated adaptation of the 1965 non-fiction serial written for The New Yorker by Truman Capote; Pal Joey (1957), based on a series of stories by John O'Hara; Mister 880 (1950), starring Edmund Gwenn, based on a story by longtime editor St. Clair McKelway; The Secret Life of Walter Mitty (1947) which began as a story by longtime New Yorker contributor James Thurber; and Meet Me in St. Louis (1944), adapted from Sally Benson's short stories.		The history of The New Yorker has also been portrayed in film: In Mrs. Parker and the Vicious Circle, a film about the celebrated Algonquin Round Table starring Jennifer Jason Leigh as Dorothy Parker, Sam Robards portrays founding editor Harold Ross trying to drum up support for his fledgling publication. The magazine's former editor, William Shawn, is portrayed in Capote (2005), Infamous (2006) and Hannah Arendt (2012).		The 2015 documentary, Very Semi-Serious, produced by Redora Films, presents a behind-the-scenes look at the cartoons of The New Yorker.		The New Yorker's signature display typeface, used for its nameplate and headlines and the masthead above The Talk of the Town section, is Irvin, named after its creator, the designer-illustrator Rea Irvin.[30] The body text of all articles in The New Yorker is set in Adobe Caslon.[31]		One uncommonly formal feature of the magazine's in-house style is the placement of diaeresis marks in words with repeating vowels—such as reëlected, preëminent, and coöperate—in which the two vowel letters indicate separate vowel sounds.[32] The magazine also continues to use a few spellings that are otherwise little used, such as focussed, venders, teen-ager,[33] traveller, marvellous, carrousel,[34] and cannister.[35]		The magazine also spells out the names of numerical amounts, such as "two million three hundred thousand dollars" instead of "$2.3 million", even for very large figures.[36]		Notwithstanding its title, The New Yorker is read nationwide, with 53 percent of its circulation in the top ten U.S. metropolitan areas. According to Mediamark Research Inc., the average age of The New Yorker reader in 2009 was 47 (compared to 43 in 1980 and 46 in 1990). The average household income of The New Yorker readers in 2009 was $109,877 (the average income in 1980 was $62,788 and the average income in 1990 was $70,233).[37]		The magazine's first cover illustration, a dandy peering at a butterfly through a monocle, was drawn by Rea Irvin, the magazine's first art editor, based on an 1834 caricature of the then Count d'Orsay which appeared as an illustration[38] in the 11th edition of the Encyclopædia Britannica. The gentleman on the original cover, now referred to as "Eustace Tilley", is a character created by Corey Ford for The New Yorker. The hero of a series entitled "The Making of a Magazine", which began on the inside front cover of the August 8 issue that first summer, Tilley was a younger man than the figure on the original cover. His top hat was of a newer style, without the curved brim. He wore a morning coat and striped trousers. Ford borrowed Eustace Tilley's last name from an aunt—he had always found it vaguely humorous. "Eustace" was selected by Ford for euphony.[39]		The character has become a kind of mascot for The New Yorker, frequently appearing in its pages and on promotional materials. Traditionally, Rea Irvin's original Tilley cover illustration is used every year on the issue closest to the anniversary date of February 21, though on several occasions a newly drawn variation has been substituted.[40]		The magazine is well known for its illustrated and often topical covers.		Saul Steinberg created 85 covers and 642 internal drawings and illustrations for the magazine. His most famous work is probably its March 29, 1976 cover,[41] an illustration most often referred to as "View of the World from 9th Avenue", sometimes referred to as "A Parochial New Yorker's View of the World" or "A New Yorker's View of the World", which depicts a map of the world as seen by self-absorbed New Yorkers.		The illustration is split in two, with the bottom half of the image showing Manhattan's 9th Avenue, 10th Avenue, and the Hudson River (appropriately labeled), and the top half depicting the rest of the world. The rest of the United States is the size of the three New York City blocks and is drawn as a square, with a thin brown strip along the Hudson representing "Jersey", the names of five cities (Los Angeles; Washington, D.C.; Las Vegas; Kansas City; and Chicago) and three states (Texas, Utah, and Nebraska) scattered among a few rocks for the United States beyond New Jersey. The Pacific Ocean, perhaps half again as wide as the Hudson, separates the United States from three flattened land masses labeled China, Japan and Russia.		The illustration—humorously depicting New Yorkers' self-image of their place in the world, or perhaps outsiders' view of New Yorkers' self-image—inspired many similar works, including the poster for the 1984 film Moscow on the Hudson; that movie poster led to a lawsuit, Steinberg v. Columbia Pictures Industries, Inc., 663 F. Supp. 706 (S.D.N.Y. 1987), which held that Columbia Pictures violated the copyright that Steinberg held on his work.		The cover was later satirized by Barry Blitt for the cover of The New Yorker on October 6, 2008. The cover featured Sarah Palin looking out of her window seeing only Alaska, with Russia in the far background.[42]		The March 21, 2009 cover of The Economist, "How China sees the World", is also an homage to the original image, but depicting the viewpoint from Beijing's Chang'an Avenue instead of Manhattan.[43]		Hired by Tina Brown in 1992, Art Spiegelman worked for The New Yorker for ten years but resigned a few months after the September 11 terrorist attacks. The cover created by Françoise Mouly and Spiegelman for the September 24, 2001 issue of The New Yorker received wide acclaim and was voted in the top ten of magazine covers of the past 40 years by the American Society of Magazine Editors, which commented:		New Yorker Covers Editor Françoise Mouly repositioned Art Spiegelman's silhouettes, inspired by Ad Reinhardt's black-on-black paintings, so that the North Tower's antenna breaks the "W" of the magazine's logo. Spiegelman wanted to see the emptiness, and find the awful/awe-filled image of all that disappeared on 9/11. The silhouetted Twin Towers were printed in a fifth, black ink, on a field of black made up of the standard four color printing inks. An overprinted clear varnish helps create the ghost images that linger, insisting on their presence through the blackness.		At first glance, the cover appears to be totally black, but upon close examination it reveals the silhouettes of the World Trade Center towers in a slightly darker shade of black. In some situations, the ghost images only become visible when the magazine is tilted toward a light source.[44] In September 2004, Spiegelman reprised the image on the cover of his book In the Shadow of No Towers, in which he relates his experience of the Twin Towers attack and the psychological after-effects.		In the December 2001 issue the magazine printed a cover by Maira Kalman and Rick Meyerowitz showing a map of New York in which various neighborhoods were labeled with humorous names reminiscent of Middle Eastern and Central Asian place names and referencing the neighborhood's real name or characteristics (e.g., "Fuhgeddabouditstan", "Botoxia"). The cover had some cultural resonance in the wake of September 11, and became a popular print and poster.[45][46]		For the 1993 Valentine's Day issue, the magazine cover by Art Spiegelman depicted a black woman and a Hasidic Jewish man kissing, referencing the Crown Heights riot of 1991.[47][48] The cover was criticized by both black and Jewish observers.[49] Jack Salzman and Cornel West describe the reaction to the cover as the magazine's "first national controversy".[50]		"The Politics of Fear", a cartoon by Barry Blitt featured on the cover of the July 21, 2008 issue, depicts then presumptive Democratic presidential nominee Barack Obama in the turban and salwar kameez typical of many Muslims, fist bumping with his wife, Michelle, portrayed with an Afro and wearing camouflage trousers with an assault rifle slung over her back. They are standing in the Oval Office, with a portrait of Osama Bin Laden hanging on the wall and an American flag burning in the fireplace in the background.[51]		Many New Yorker readers saw the image as a lampoon of "The Politics of Fear", as was its title. Some of Obama's supporters as well as his presumptive Republican opponent, Sen. John McCain, accused the magazine of publishing an incendiary cartoon whose irony could be lost on some readers. However, editor David Remnick felt the image's obvious excesses rebuffed the concern that it could be misunderstood, even by those unfamiliar with the magazine.[52][53] "The intent of the cover," he said, "is to satirize the vicious and racist attacks and rumors and misconceptions about the Obamas that have been floating around in the blogosphere and are reflected in public opinion polls. What we set out to do was to throw all these images together, which are all over the top and to shine a kind of harsh light on them, to satirize them."[54]		In an interview on Larry King Live shortly after the magazine issue began circulating, Obama said, "Well, I know it was The New Yorker's attempt at satire... I don't think they were entirely successful with it". But Obama also pointed to his own efforts to debunk the allegations portrayed in The New Yorker cover through a web site his campaign set up: "[They are] actually an insult against Muslim-Americans, something that we don't spend a lot of time talking about."[55][56]		Later that week, The Daily Show's Jon Stewart continued The New Yorker cover's argument about Obama stereotypes with a piece showcasing a montage of clips containing such stereotypes culled from various legitimate news sources.[57] The New Yorker Obama cover was later parodied by Stewart and Stephen Colbert on the October 3, 2008, cover of Entertainment Weekly magazine, with Stewart as Obama and Colbert as Michelle, photographed for the magazine in New York City on September 18.[58]		New Yorker covers are not always related to the contents of the magazine or are only tangentially so. In this case, the article in the July 21, 2008, issue about Obama did not discuss the attacks and rumors but rather Obama's political career. The magazine later endorsed Obama for president.		This parody was most likely inspired by Fox News host E. D. Hill's paraphrasing of an anonymous internet comment in asking whether a gesture made by Obama and his wife Michelle was a "terrorist fist jab".[59][60] Later, Hill's contract was not renewed.[61]		The New Yorker chose an image of Bert and Ernie by artist Jack Hunter, entitled 'Moment of Joy', as the cover of their July 8, 2013 publication, which covers the Supreme Court decisions on the Defense of Marriage Act and California Proposition 8.[62] The Sesame Street characters have long been rumored in popular culture and urban legend to be homosexual partners, though Sesame Workshop has repeatedly denied this, saying they are merely "puppets" and have no sexual orientation.[63] Reaction was mixed. Online magazine Slate criticized the cover, which shows Ernie leaning on Bert's shoulder as they watch a television with the Supreme Court justices on the screen, saying "it's a terrible way to commemorate a major civil-rights victory for gay and lesbian couples." The Huffington Post, meanwhile, said it was "one of [the magazine's] most awesome covers of all time."[64]		
Jest books (or Jestbooks) are collections of jokes and humorous anecdotes in book form - a literary genre which reached its greatest importance in the early modern period.[1]						The oldest surviving collection of jokes is the Byzantine Philogelos from the first millennium.[2] In Western Europe, the medieval fabliau[3] and the Arab/Italian novella[4] built up a large body of humorous tales; but it was only with the Facetiae of Poggio (1451) that the anecdote first appears rendered down into joke form (with prominent punchline) in an early modern collection.[5]		Like his immediate successors Heinrich Bebel and Girolamo Morlini, Poggio translated his folk material from their original language into Latin, the universal European language of the time.[6] From such universal collections, developed the particular vernacular jestbooks of the various European countries in the sixteenth century.[7]		Tudor and Stuart jest books were typically anonymous collections of individual jests in English,[8] a mix of verse and prose perhaps more comparable to the latter-day magazine than to a normal book.[9] Some, however (following a German model), did attempt to link their jokes into a picaresque sort of narrative around one, often roguish hero, as with Richard Tarlton.[10]		Jest books took a generally mocking tone,[11] with civility, and social superiors like the 'stupid scholar' as favourite targets.[12]		The low-life, realistic tone of the jest book, akin to coney-catching pamphlets, fed into the early English novels (or at least prose fiction) of writers like Thomas Nashe and Thomas Deloney.[13]		Jestbooks also contributed to popular stage entertainment, through such dramatists as Marlowe and Shakespeare.[14] Playbooks and jestbooks were treated as forms of light entertainment, with jokes from the one being recycled in the other, and vice versa.[15]		Advances in printing meant that quantitatively jestbooks reached their greatest circulation in the 17th and 18th centuries; but qualitatively their contents was increasingly either a repetition of earlier publications or an artificial imitation of what had in the Elizabethan jest book been a genuine folk content.[16]		Bowdlerisation in the 19th century completed the fall of the English-language jest book from Elizabethan vitality to subsequent triviality.[17]		Salcia Landmann		
Head & Shoulders (H&S) is an American brand of anti dandruff and non dandruff shampoo produced by parent company Procter & Gamble that was introduced in 1961.[2]		By 1982, it was the "number one brand" of shampoo, and it was noted that "[n]o one hair care brand gets so many ad dollars as Head & Shoulders, a twenty year old brand, and no other brand matches its sales", despite it being a "medicated" shampoo.[3]		Since the 1980s, the brand has long been marketed under the tagline, "You Never Get a Second Chance to Make a First Impression", which has been identified as an example of "anxiety marketing" commonly used by Procter & Gamble to drive sales by inducing fears of social consequences associated with the condition that the product claims to address.[4]		In the 2000s, however, sales of the product dropped off, a phenomenon blamed on overextension of the brand into too many varieties, with over thirty kinds of Head & Shoulders being sold.[5] A large number of celebrities have appeared in advertising campaigns for the brand, including Mexican celebrity Thalía,[6] British Formula One driver Jenson Button,[7] and Indian model Nauheed Cyrusi.[8]		The active ingredient is pyrithione zinc.[9]		
On June 25, 2009, Michael Jackson died of acute propofol and benzodiazepine intoxication at his home on North Carolwood Drive in the Holmby Hills neighborhood of Los Angeles. His personal physician, Conrad Murray, said he had found Jackson in his room, not breathing and with a barely detectable pulse, and that he administered CPR on Jackson to no avail. After a call was placed to 9-1-1 at 12:21 p.m. local time, Jackson was treated by paramedics at the scene and was later pronounced dead at the Ronald Reagan UCLA Medical Center.[1] On August 28, 2009, the Los Angeles County Coroner concluded that his death was a homicide.[2] Shortly before his death, Jackson had reportedly been administered propofol and two anti-anxiety benzodiazepines, lorazepam and midazolam, in his home.[3] His personal physician was convicted of involuntary manslaughter in 2011 and served a two-year prison sentence.[4][5]		Jackson's death triggered an outpouring of reactions around the world, creating unprecedented surges of Internet traffic and causing sales of his music and that of the Jackson 5 to increase dramatically.[6] Jackson had also intended to perform a series of comeback concerts to over one million people at London's O2 Arena from July 2009 to March 2010.[7] A public memorial service for Jackson was held on July 7, 2009, at the Staples Center in Downtown Los Angeles, where he had rehearsed for the London concerts the night before his death. The service was broadcast live around the world, attracting a global audience of more than one billion people.[8] In 2010, Sony Music Entertainment signed a US $250 million deal with Jackson's estate to retain distribution rights to his recordings up until 2017, and to release seven posthumous albums over the decade following his death. Jackson's death is ranked No. 1 on VH1/VH1 Classic's list of 100 Most Shocking Moments in Music.[9]		Jackson arrived for rehearsal at Staples Center around 6:30 p.m. on Wednesday, June 24, according to Ed Alonzo, a magician who was there. The singer jokingly complained of laryngitis and did not rehearse until 9 p.m. "He looked great and had great energy,"[10] Alonzo added. The rehearsal went past midnight.[10] The next morning, Jackson did not come out of his bedroom.[11] According to the attorney of Conrad Murray, Jackson's personal physician, Murray entered the room that afternoon and found Jackson in bed and not breathing. Jackson had a weak pulse, and his body was still warm.[12] Murray tried to revive Jackson for five to 10 minutes, at which point he realized he needed to call for help. Murray stated that he was hindered because there was no landline in the house. Murray also stated that he could not use his cell phone to call 911 because he did not know the exact address. Murray stated that he also phoned security, but did not get any answer. Finally, Murray ran downstairs, yelled for help, and told a chef to bring security up to the room.[13] By the time security called 911, Murray's lawyer stated that at least 30 minutes had passed.[13]		Statements described Murray as someone using a non-standard CPR technique on Jackson. During the tape of the emergency call, released on June 26 one day after Jackson's death, the doctor was described as administering CPR on a bed, not on a hard surface such as a floor, which would be standard practice.[14][15] The doctor's attorney said that Murray placed one hand underneath Jackson and used the other hand for chest compression, where the standard practice is to use both hands for compression.[11][16] A Los Angeles Fire Department (LAFD) spokesperson said the 911 call came in at 12:21:04 p.m. PDT (19:21:04 UTC). Paramedics reached Jackson at 12:26 p.m. and found that he still was not breathing.[17][18]		Paramedics performed CPR for 42 minutes at the house.[19] Murray's attorney stated that Jackson had a pulse when he was taken out of the house and put in the ambulance.[11] An LAFD official gave a different account, stating that paramedics found Jackson in "full cardiac arrest", and that they did not observe a change in Jackson's status on the route to the hospital.[20] LAFD transported Jackson to Ronald Reagan UCLA Medical Center.[17] After the ambulance arrived at the hospital at approximately 1:14 p.m., a team of medical personnel attempted to resuscitate Jackson for more than an hour. They were unsuccessful, and Jackson was pronounced dead at 2:26 p.m. at the age of 50.[21][22][23]		Jackson's body was flown by helicopter to the Los Angeles Coroner's offices in Lincoln Heights, where a three-hour autopsy was performed the next day on behalf of the Los Angeles County Coroner by the chief medical examiner Lakshmanan Sathyavagiswaran.[24] Jackson's family arranged for a second autopsy, a practice that could yield expedited, albeit limited, results.[25] After the preliminary autopsy was completed, Craig Harvey, chief investigator for the coroner's office, said there was no evidence of trauma or foul play.[26] On August 28, the LA County Coroner made an official statement classifying that Jackson's death was a homicide. The county coroner stated that he died from the combination of drugs in his body, with the most significant drugs being the anesthetic propofol and the anxiolytic lorazepam. Less significant drugs found in Jackson's body were midazolam, diazepam, lidocaine, and ephedrine. The coroner is keeping the complete toxicology report private, as requested by the police and district attorney.[2] The autopsy report revealed that Jackson was otherwise healthy[27] for his age and that his heart was strong. The document stated that Jackson's most significant health problem was his lungs were chronically inflamed, which however did not contribute to his death. His other major organs were normal and he had no atherosclerosis except for some slight plaque accumulation in the arteries in his leg.[28][29] The autopsy stated that he weighed 136 pounds (62 kg) and was 5'9" (175 cm) tall, which equates to a body mass index of 20.1.[30] Fox News said that this confirmed rumors that Jackson was emaciated,[31] while the Associated Press stated that his weight was in the acceptable range.[29]		Although they did not immediately announce that they suspected foul play, the Los Angeles Police Department (LAPD) began to investigate the unusual and high-profile case by the day after Jackson's death.[32] By August 28, the LAPD had announced that the case would be referred to prosecutors who might file criminal charges.[2] Because the LAPD did not secure Jackson's home and allowed the Jackson family access to it too before returning to remove certain items, the department raised concerns by some observers that the chain of custody had been broken.[33][34] The police maintained that they had followed protocol.[34] On July 1, the Drug Enforcement Administration (DEA) joined the LAPD in the investigation. Having the authority to investigate issues otherwise protected by doctor-patient confidentiality, the DEA could legally follow the entirety of what appeared to be the complex trail of prescription drugs supplied to Jackson.[33] California Attorney General Jerry Brown announced that his office was helping the LAPD and DEA to create a statewide database of all medical doctors and prescriptions filled.[35]		The LAPD subpoenaed medical records from doctors who had treated Jackson. On July 9, William J. Bratton, then the Los Angeles Chief of Police, indicated that investigators were focusing on the possibility of homicide or accidental overdose, but had to wait for the full toxicology reports from the coroner.[36] The Los Angeles Times quoted a senior law enforcement source as saying authorities may not pursue charges even if the coroner declares the case a homicide, because Jackson's well-documented drug abuse would make any prosecution difficult. Nonetheless, the source said prosecutors had not ruled out more serious charges "all the way up to involuntary manslaughter" if it were determined that Jackson's death was indeed caused by the drug propofol.[37]		Jackson was said to have used propofol, as well as alprazolam (an antianxiety agent), and sertraline (an antidepressant).[38] Other drugs named in connection with him included omeprazole, hydrocodone, paroxetine, carisoprodol, and hydromorphone.[39] After his death, police found several drugs in his home, which included propofol. Some of these drugs had labels made out to Jackson under pseudonyms, while others were unlabeled.[40][41] A 2004 police document prepared for the 2005 People v. Jackson child abuse trial alleged that Jackson was taking up to 40 alprazolam pills a night.[36] Alprazolam was not found in his bloodstream at his time of death.[3] Dr. A.J. Farshchian, Michael Jackson's friend and confidante, has claimed that Jackson was scared of drugs.[42]		Deepak Chopra, an internist, endocrinologist, and speaker about mind–body intervention who was a friend of Jackson's for 20 years, expressed concern that, despite presumably having access to a large arsenal of drugs, Jackson appears to have been given no naloxone, a drug used to counteract the effects of an opioid overdose.[43] Chopra also criticized what he saw as "enabling" by some Hollywood doctors: "This cult of drug-pushing doctors, with their co-dependent relationships with addicted celebrities, must be stopped. Let's hope that Michael's unnecessary death is the call for action."[43]		Eugene Aksenoff, a Tokyo-based physician who had treated Jackson and his children on a few occasions, expressed concern about Jackson's use of and interest in various drugs. Aksenoff said that Jackson asked for stimulants so that he could get through some demanding performances. He said he refused to prescribe them. He recalled that the singer suffered chronic fatigue, fever, insomnia and other symptoms and took a large amount of drugs. He suspected one of the major factors causing Jackson these symptoms was excessive use of steroids or other skin-whitening medications.[44] According to the toxicological tests effectuated on Jackson's body, no addiction had been reported, and none of the experts called to testify at Murray's trial have identified the singer as a drug addict.[3] Janet Jackson confirmed that the Jackson family tried to stage an intervention in early 2007, when Jackson was living in Las Vegas.[45] Janet Jackson and some of her brothers allegedly traveled to his home, but were turned away by security guards who were ordered not to let them in. He was also rumored to have refused calls from his mother. "If you tried to deal with him," one family source said, "he would shut you out. You just wouldn't hear from him for long periods." The family denied that they had tried to intervene.[46]		Of all the drugs found in Jackson's home, the one that most concerned investigators was propofol (Diprivan), a powerful anesthetic administered intravenously in hospitals to induce and maintain anesthesia during surgery.[47] Nicknamed "milk of amnesia" because of its opaque, milk-like appearance (and a play on the words "milk of magnesia"), the drug has been associated with cardiac arrest,[47] but it still may be increasingly used off-label for anxiolytic and other medically unsubstantiated purposes.[41] Several propofol bottles, some empty, some full, were found in Jackson's home.[41]		On June 30, Cherilyn Lee, a nurse who had worked as Jackson's nutritionist, said that he had asked her in May to provide propofol to help him sleep, but she refused. He told her he had been given the drug before for persistent insomnia, and that a doctor had said it was safe. Lee said she received a telephone call from an aide to Jackson on June 21 to say that Jackson was ill, although she no longer worked for him. She reported overhearing Jackson complain that one side of his body was hot, the other side cold. She advised the aide to send Jackson to a hospital.[48]		Arnold Klein said that Jackson used an anesthesiologist to administer propofol to help him sleep while he was on tour in Germany. The anesthesiologist would "take him down" at night and "bring him back up" in the morning during the HIStory tour of 1996 and 1997.[46]		The DEA was focusing on at least five doctors who prescribed drugs to Jackson, trying to determine whether they had had a "face to face" relationship with him, and whether they had made legally required diagnoses.[41] At least nine doctors were under investigation.[49] The police wanted to question 30 doctors, nurses, and pharmacists, including Arnold Klein.[50] Klein said that he occasionally had given Jackson pethidine to sedate him, but had administered nothing stronger, and that he had turned his records over to the medical examiner.[51]		Cardiologist Conrad Murray joined Jackson's camp in May 2009 as part of Jackson's agreement with AEG Live, the promoter of his London concerts. Murray first met Jackson in Las Vegas when the doctor treated one of the singer's children. AEG Live said the singer insisted the company hire Murray to accompany him to England.[52] During Murray's trial it emerged that AEG employed the doctor and that Jackson did not sign the contract for the above-cited employment either.[53]		Murray said through his attorney that he did not prescribe or administer pethidine or oxycodone to Jackson, but did not say what, if anything, he did prescribe or administer.[47] Los Angeles police said the doctor spoke to officers immediately after Jackson's death, and during an extensive interview two days later. They stressed that they found "no red flag" and did not suspect foul play.[14] On June 26, police towed away a car used by Murray, stating that it might contain medication or other evidence. The police released the car five days later.[33]		Politician and minister Jesse Jackson, a friend of Michael Jackson's family, said that the family was concerned about Murray's role. "They have good reason to be [...] he left the scene."[54] Over the next few weeks, law enforcement grew increasingly concerned about the doctor, and on July 22 detectives searched Murray's medical office and storage unit in Houston, removing items such as a computer and two hard drives, contact lists and a hospital suspension notice.[55] On the 27th, an anonymous source reported that Murray had administered propofol within 24 hours of Jackson's death.[56] Murray's lawyers refused to comment on what they called "rumors, innuendo or unnamed sources."[57] The following day it was reported that investigators had searched Murray's home and office in Las Vegas, and that Murray had become the primary focus of the investigation.[58] On August 11, a Las Vegas pharmacy was searched by investigators looking for evidence regarding Murray, according to an anonymous police source.[59] Murray's lawyer advised patience until the toxicology results arrived, noting that "things tend to shake out when all the facts are made known".[58] On February 8, 2010, Murray was charged with involuntary manslaughter by prosecutors in Los Angeles. Murray pleaded not guilty and was released after posting $75,000 (USD) bail.[60] Shortly after, the California Medical Board issued an order preventing Murray from administering heavy sedatives.[61]		On January 11, 2011, the judge from Murray's preliminary hearing determined that Murray should stand trial for involuntary manslaughter in the Jackson case. The judge also suspended Murray's license to practice medicine in California.[62] The trial was originally to begin on March 24, but a delayed opening rescheduled it for May 9. Finally, the trial was rescheduled to September 8, with no further delays.[63] The jury selection of Murray's trial began on September 8, 2011, in Los Angeles. The trial began on September 27, 2011.[64] On November 7, 2011, Murray was found guilty of involuntary manslaughter[65] and he was held without bail to await sentencing.[66] On November 29, 2011, Murray received the maximum sentence of 4 years in prison.[67] Murray was released on October 28, 2013,[5] due to California prison overcrowding and good behavior.[68]		Stacy Brown, a biographer, said Jackson had become "very frail, totally, totally underweight," and that his family had been worried about him. Another biographer, J. Randy Taraborrelli, who became friends with Jackson in the 1970s, said Jackson had suffered from an addiction to painkillers which went on and off for decades.[26] Arnold Klein, Jackson's dermatologist, confirmed that Jackson misused prescription drugs, and that Klein had diagnosed Jackson with vitiligo and lupus. Yet, Klein said, when he saw Jackson at his office three days before his death, the singer "was in very good physical condition. He was dancing for my patients. He was very mentally aware when we saw him and he was in a very good mood."[51]		Jackson is survived by his three children, Prince Michael Joseph Jackson (b. 1997); Paris-Michael Katherine Jackson (b. 1998), born during his marriage to his second wife, Debbie Rowe; and Prince Michael Jackson II, known as "Blanket", born in 2002 to a surrogate mother. He is also survived by his brothers, Jackie, Tito, Jermaine, Marlon and Randy; sisters Rebbie, La Toya and Janet; and parents Joseph and Katherine. Katherine was granted temporary guardianship of Michael's three children on June 29, 2009.[69]		The Jackson family released a collective statement following the death:		Our beloved son, brother, uncle and father of three children has gone so unexpectedly, in such a tragic way and much too soon. It leaves us, his family, speechless and devastated to a point, where communication with the outside world seems almost impossible at times.[70]		La Toya indicated that the family would file a lawsuit against anyone they believed responsible for her brother's death, as well as push for criminal charges.[71] In 2009, she stated that Jackson might have been administered an ultimately lethal dose of drugs by "a shadowy entourage" of handlers[71] and, in 2010, said that she believed her brother "was murdered for his music catalogue."[72] Shortly after Jackson's death, the family raised questions about the role of AEG Live, the This Is It concert promoter, in the last few weeks of his life.[54] Joseph has since filed a complaint with the California Medical Board alleging that AEG Live was illegally practicing medicine by demanding that Murray get Jackson off various medications. The complaint also alleges that AEG Live failed to provide the resuscitation equipment and nurse which Murray had requested. AEG spokesman Michael Roth declined to comment on the complaint.[73]		After Murray pleaded not guilty to the manslaughter charge, several members of the Jackson family said they felt he deserved a more severe charge.[74] On June 25, 2010, Joseph filed a wrongful death lawsuit against Murray. The lawsuit alleges that Murray repeatedly lied to cover up his use of propofol, did not keep sufficient medical records and was negligent in his use of medications on Jackson. Murray's civil attorney, Charles Peckham, denied that Murray gave Jackson anything life-threatening.[75] On August 15, 2012, Joseph dropped his wrongful death lawsuit against Murray.[76][77]		On September 15, 2010, Panish Shea & Boyle LLP also filed a wrongful death lawsuit on behalf of Michael Jackson's three children and his mother against the Anschutz Entertainment Group, Inc. (AEG) and its subsidiaries and principals (including Randy Phillips, Kenny Ortega, Paul Gongaware and Thimothy Leiweke).[78] The suit alleges that AEG put their desire for profits from the This Is It concerts over the health and safety of Michael Jackson, ultimately causing his death". Roth declined to comment on the lawsuit, saying that AEG had not seen it.[79]		On November 7, 2011, Michael Jackson's family arrived at the courthouse in Los Angeles shortly after the jury announced they reached their verdict; they had found Murray guilty. Michael's father Joe Jackson simply told reporters "Justice." LaToya Jackson tweeted that she was shaking uncontrollably when she heard the verdict, and continued to tweet her emotions throughout the day.[80] In 2017, Paris Jackson stated that she was "absolutely" convinced that her father had been murdered.[81]		Jackson's last will was filed by attorney John Branca at the Los Angeles County courthouse on July 1, 2009. Signed July 7, 2002, it names Branca and accountant John McClain as executors; they were confirmed as such by a Los Angeles judge on July 6, 2009.[82] All assets are given to the (pre-existing) Michael Jackson Family Trust (amended March 22, 2002),[83] the details of which have not been made public. The Associated Press reports that, in 2007, Jackson had a net worth of $236.6 million: $567.6 million in assets, which included Neverland Ranch and his 50% share of Sony/ATV Music Publishing' catalogue, and debts of $331 million.[84] The guardianship of his three children is given to his mother, Katherine, or if she is unable or unwilling, to singer Diana Ross.[85] The will states that Jackson's former wife Debbie Rowe was omitted intentionally.[86] Jackson's will allocates 20% of his fortune as well as 20% of money made after death to unspecified charities.[87]		Media reports suggested that settlement of Jackson's estate could last many years.[88] The value of Sony/ATV Music Publishing is estimated by Ryan Schinman, chief of Platinum Rye, to be US$1.5 billion. Shinman's estimate makes Jackson's share of Sony/ATV worth US$750 million, from which Jackson would have had an annual income of US$80 million. Sony Corporation has not commented on whether it intends to buy Jackson's share of Sony/ATV from the Jackson estate. Jackson's creditors could force a distressed sale, which would act in Sony's favor since it would lower the sale price, but only if the trust set up by Jackson for his stake in Sony/ATV is revocable. A distressed sale would lower the value of Jackson's estate, and thus might not raise enough to cover the debts owed by the estate.[89]		The estate administrators and the IRS have estimated portions of the estate differently.[90][91] The estate estimated that the value of Jackson's likeness is only $2,105; whereas the IRS estimated that the likeness to be worth $434.26 million.[91] The estate estimated "no worth in Jackson's interest in a trust that owns some songs of his and the Beatles, but the IRS valued it at $469 million."[91] Also in dispute is the value of "Jackson's share of the Jackson 5 master recordings rights, stocks and bonds, and various cars Jackson owned."[91][92] The IRS proposed "imposing $505 million in taxes plus an additional $197 million in penalties, including a gross valuation misstatement penalty."[90][91] On July 26, 2013, the estate filed a U.S. Tax Court petition claiming "the IRS overestimated the value of its assets, including Jackson's likeness, real estate, a Bentley, a Lloyds of London insurance policy, Jackson's share of MJJ Ventures Inc., and two trusts.".[92][93] Jackson estate attorney Paul Hoffman of Hoffman, Sabban & Watenmaker told Bloomberg News, "The IRS is wrong."[93] The case title is Estate of Michael J. Jackson v. IRS, 17152-13, U.S. Tax Court in Washington, DC.[92]		The first reports that Jackson had suffered a cardiac arrest then that he had died came from the Los Angeles-based celebrity news website TMZ. Doctors at Ronald Reagan UCLA Medical Center pronounced Jackson dead at 2:26 p.m. and 18 minutes later, TMZ published the following statement: "Michael Jackson passed away today at the age of 50."[18] The Los Angeles Times confirmed the report at 2:51 p.m. PDT (5:51 p.m. EDT).[94] The news spread quickly online, causing websites to slow down and crash from user overload. TMZ and the Los Angeles Times suffered outages.[95] Google initially believed that the millions of search requests meant their search engine was under DDoS attack, and blocked searches related to Michael Jackson for 30 minutes. Twitter reported a crash, as did Wikipedia at 3:15 pm PDT (22:15 UTC).[96] The Wikimedia Foundation reported nearly a million visitors to Jackson's biography within one hour, probably the most visitors in a one-hour period to any article in Wikipedia's history.[97] AOL Instant Messenger collapsed for 40 minutes. AOL called it a "seminal moment in internet history ... We've never seen anything like it in terms of scope or depth."[98] Around 15% of Twitter posts (5,000 tweets per minute) mentioned Jackson after the news broke,[99][100] compared to the 5% recalled as having mentioned the Iranian elections or the flu pandemic that had made headlines earlier in the year.[100] Overall, web traffic ranged from 11% to at least 20% higher than normal.[99][101] MTV and BET aired marathons of Jackson's music videos.[102]		Specials about Jackson aired on multiple television stations around the world. The British soap opera EastEnders added a last-minute scene, in which the character Denise Johnson discussed the news with Patrick Trueman, to the June 26 episode.[103] While most British newspapers printed pictures of Jackson in his youth or in his prime, The Sun (for the day after his death) was the only paper to show Jackson from 2009 at his frailest, and keeping to their regular promotion of 'Wacko Jacko.' The next day, The Sun fell into course with the rest of the newspapers and Jackson was the topic of every front-page headline in The Sun for about two weeks following his death.[104] Magazines including Time published commemorative editions.[105] A scene that had featured Jackson's sister La Toya was cut from the film Brüno out of respect toward Jackson's family.[106]		According to an analysis released by the Global Language Monitor, "72 hours after his death, Jackson jumped to the No. 9 spot for the global print and electronic media. For Internet, blogs and social media, Jackson jumped to the No. 2, only trailing the election of Barack Obama to the presidency of the United States. The results showed the growing disparity between the mainstream global media, and what is playing out for news on the Internet, and beyond".[107] Paul JJ Payack, president and chief word analyst of GLM, said the death of Michael Jackson had "resulted in a global media event of the first order" and "the fact that he has broken into the top media of the 21st century" was a "testament to the global impact of the man and his music." Commentators around the globe made connections between Jackson's death and the problems they perceived with everything from the racial dichotomy that Jackson sang about, to the "profoundly tragic figure" of Michael Jackson[108]—from American capitalism[109] and globalization, to the fall of the music industry in the 1980s. "Commentators around the world have absolutely flipped", wrote Patrik Etschmayer of Switzerland's Nachrichten newspaper.[110] Le Figaro columnist Yann Moix said that although Jackson, like his iconic Moonwalk, lived life in reverse, the world at his death shed "identical and universal tears".[111]		Statistics published by the Pew Research Center suggested that two out of three Americans believed the coverage of Jackson's death was excessive, while 3% felt it was insufficient.[112] In the UK, the BBC received over 700 complaints from viewers who thought his death dominated the news.[113] On June 29, American conservative commentator Rush Limbaugh said the coverage was a "horrible disgrace" and lent his support to activist-ministers Jesse Jackson and Al Sharpton, who were fighting to stem the press's speculation about what caused the death.[114] Other conservatives, including commentator Bill O'Reilly[115] and Congressman Peter T. King,[116] also disapproved the receiving of the media attention about Jackson's death. Meanwhile, Hugo Chávez, the President of Venezuela, called the pop star's death some "lamentable news",[117] but criticized CNN for giving this news more coverage than they gave a coup d'état taking place in Honduras.[109][117]		In August 2009, there were reports that Jackson's family paid social media marketing company uSocial.net to increase the numbers of followers on Jackson's Twitter profile.[118] According to the New York Daily News, uSocial was contracted to deliver 25,000 followers to the account.[119] It was not specified whether the service was rendered before or after his death.		News of Jackson's death triggered an outpouring of grief around the world. The circumstances and timing of his death were compared to those of Elvis Presley and John Lennon. Fans gathered outside the UCLA Medical Center, Neverland Ranch, his Holmby Hills home, the Hayvenhurst Jackson family home in Encino, the Apollo Theater in New York, and at Hitsville U.S.A., the old Motown headquarters in Detroit where Jackson's career began, now the Motown Museum. Streets around the hospital were blocked off, and across America people left offices and factories to watch the breaking news on television.[120] A small crowd, including the city's mayor, gathered outside his childhood home in Gary, where the flag on city hall was flown at half staff in his honor.[121] Fans in Hollywood initially gathered around the Walk of Fame star of another Michael Jackson—unable to access the singer's star, which had been temporarily covered by equipment in place for the Brüno film premiere.[122] Grieving fans and memorial tributes relocated from the talk radio host's star the next day.[123]		From Odessa[124] to Brussels,[125] and beyond, fans held their own memorial gatherings. U.S. President Barack Obama sent a letter of condolence to the Jackson family,[14] and the House of Representatives observed a moment of silence.[126] Obama later stated that Jackson "will go down in history as one of our greatest entertainers".[127] Former South African President Nelson Mandela issued a message through his foundation saying Jackson's loss would be felt worldwide.[128]		In Japan, where Jackson had somewhat of an idol status, the top government spokesman and other ministers expressed their condolences. Internal Affairs and Communications Minister Tsutomu Sato told reporters, "I feel sad as I had watched him since he was a member of Jackson Five." "Defense Minister Yasukazu Hamada has credited him with building a generation with his music."[129] "'He was a superstar. It is an extremely tragic loss. But it is fantastic he was able to give so many dreams and so much hope to the people of the world,' said Health Minister Yoichi Masuzoe."[130]		In the United Kingdom, Prime Minister Gordon Brown's spokesperson said:		Conservative opposition leader David Cameron said:		Russian fans gathered outside the U.S. embassy in Moscow to mourn. One told Russia's Novosti newspaper, "This is so difficult! I'm hurt, very hurt! … For us, this is a very great loss. To us, he became a symbol of the spiritual world. It's hard to convey how great a loss this is."[131] France's Minister Culture, Frédéric Mitterrand, said, "We all have a Michael Jackson within."[132] Elizabeth Taylor, a long-time friend, said she can't imagine life without him.[133] Liza Minnelli told CBS, "When the autopsy comes, all hell's going to break loose, so thank God we're celebrating him now."[134] His sister La Toya claimed that his daughter said he was being overworked. La Toya is quoted as saying: "She said, 'No, you don't understand. They kept working him and Daddy didn't want that, but they worked him constantly'. I felt so bad."[135]		On June 30, 2009, U2 while performing their first show of the U2 360 tour in Barcelona dedicated the song "Angel of Harlem" to Jackson. Bono sang verses from "Man In The Mirror" and "Don't Stop Till You Get Enough" at the end of the song. On July 10, 2009, six thousand fans attended a musical tribute in Jackson's hometown of Gary, Indiana. Local performers staged a medley of his songs, and mayor Rudy Clay unveiled a seven-foot memorial to him. Jesse Jackson addressed the crowd, stating, "This is where Michael learned to dance, where he learned to sing, where he learned to sacrifice."[136] The Game, was among the first performers to release a tribute song: his single "Better on the Other Side" came out the day after Jackson's death. Produced by DJ Khalil, this song featured vocals by Diddy, Chris Brown, Polow da Don, Mario Winans, Usher, and Boyz II Men.[137] A wide variety of other artists recorded musical tributes, such as 50 Cent,[138] LL Cool J, Robbie Williams, Akon and guitarist Buckethead (whose song entitled "The Homing Beacon" was inspired by Jackson's 3-D film, Captain EO.)[139]		On June 26, multiple artists, such as Pharrell Williams and Lily Allen, paid tribute to Jackson at the Glastonbury Festival.[140] Performances included Allen wearing a single white glove (which was a signature look for Jackson) for her set on the Pyramid Stage, while The Streets performed a cover of "Billie Jean".[140] Tributes to Jackson at the musical festival continued over the weekend from June 26 to June 28.[141] On July 5, 2009. Madonna performed a tribute to Jackson during the second leg of the Sticky & Sweet Tour.[142] While performing a medley of Jackson's songs, as a Jackson impersonator performed Jackson's signature moves, photos of Jackson were shown on screen behind them.[142][143] After the performance, Madonna told the crowd, "Let's give it up for one of the greatest artists the world has ever known", leading to applause from the crowd.[143] Beyoncé dedicated her song "Halo" to Jackson during multiple concerts during her I Am... World Tour. Knowles, who cites Jackson as her biggest influence, has been referred to as Jackson's heir apparent, along with drawing continuous comparisons to him.		Artists from the metal and hard rock community also paid homage to Jackson. Metallica paid tribute to Jackson during its encore at the Sonisphere Festival. Honoring Michael Jackson during its July 4 headlining appearance at the event, the band played a portion of 'Beat It' before easing into a riotous cover of Queen's 'Stone Cold Crazy'.[144] Boston hard rockers Extreme performed a cover version of Jacksons's "Wanna Be Startin' Somethin" at the Midnight Rodeo in Amarillo, Texas.[145] Judas Priest bassist Ian Hill, spoke about Jackson in an interview for an internet blog: "He was an immense star, wasn't he? Let's face it. He's a worldwide superstar really and the grief that everybody's is showing, it doesn't surprise me at all. He was a very, very talented man." [146] Members of the legendary metal band Black Sabbath released official statements regarding Jackson's death. Drummer Bill Ward: "For those in heartache today, I wish you all wellness in healing. A great entertainer has died. I think those who experienced and heard his heart are more enlightened people for it than before. I believe you're all most fortunate to have connected to Michael Jackson." [147] Bassist Geezer Butler: "Saddened and shocked to hear of the passing of Michael Jackson. He truly was, and always will be, a true icon. 'Thriller' was one of the greatest pop masterpieces of all time. A sad day for our world. R.I.P." [148] Legendary shock rocker, Alice Cooper released the following statement: "Michael Jackson was easily as influential as James Brown, and that's saying a lot. We had Vincent Price in common. I used him first on 'Welcome To My Nightmare' in 1975, and he later used him on 'Thriller'. Nobody moved like Michael, he was truly the King of Pop." [149] Van Halen guitarist Eddie Van Halen, who worked with Jackson during the recording of Thriller, stated: "I am really shocked; as I'm sure the world is, to hear the news. I had the pleasure of working with Michael on 'Beat It' back in '83 — one of my fondest memories in my career. Michael will be missed and may he rest in peace." [150] Queen guitarist Brian May stated in his official website: "Hard to know what to say — what to feel. I find myself wondering what might have happened on his tour... The number of dates in the U.K. that he had committed to was insane. I did have a feeling it was impossible, but I was so shocked to hear that he went so suddenly. Very sad. Of course, I still think of him as a boy — he used to come and see us (Queen) play when we were on tour in the States, and he and Freddie Mercury became close friends, close enough to record a couple of tracks together at Michael's house. Tracks which have never seen the light of day. Michael was the boy star of the Jackson Five, and always the most screamed at. I remember in their show, they tried very hard to make all the brothers equal in the presentation, but it was abundantly obvious that all most of the girl fans really wanted to see was little Michael. It was Michael who heard our track 'Another One Bites the Dust' when he came to see us on 'The Game' tour ... and told us we were mad if we didn't release it as a single." [150] Former Guns n' Roses guitarist Slash, who played guitar on Jackson's single "Give In to Me" stated: "Really sad news about Michael. He was a talent from on high." [150] Chris Cornell of Soundgarden and Audioslave performed a cover version of "Billie Jean" on June 27, 2009, at the Peace & Love festival in Borlänge, Sweden.[151] Steve Vai and Andy Timmons of Danger Danger performed an instrumental version of "Beat It" at the Meinl Guitar Festival 2009 on June 27 in Gutenstetten, Germany.[152] Former Skid Row frontman, Sebastian Bach commented: "Another angel down... I am very sorry for my friend Jermaine's loss. I lived with Jermaine for three weeks last year (during the filming of the second season of CMT's hit series, 'Gone Country'), and we talked about his brother frequently. He said to me, 'When you cut up my brother, you're cutting up me'. I feel for the Jackson family, because I know all too well how they feel. Unfortunately." [153] Geoff Tate of Queensrÿche stated: "I grew up listening to Michael Jackson, watching him and his brothers perform on television. He made performing seem easy and inspired my generation with his music and his grace. He was one of a kind and will be missed but his music will live forever." [153] Legendary Swedish guitarist Yngwie Malmsteen recorded "Beat It" with ex-Judas Priest/Iced Earth frontman Tim "Ripper" Owens on vocals.[154] Guitarist Buckethead wrote a song entitled "The Homing Beacon", inspired by Jackson's 3-D film, Captain EO.[139] Alternative metal band CKY performed "Beat It" for the duration of their "Carver City" Tour. In some instances, guitarist Chad I Ginsburg wore T-shirts depicting Jackson during the performances.[155] Poison drummer Rikki Rockett commented: "Michael Jackson — huge loss!!! The words genius and musical are used in the same sentence too often. Not in the case of Michael Jackson. His musical expression will never be topped and his inspiration will live forever. R.I.P." In October 2013, an all-star tribute album was released featuring current and former members of Iron Maiden, Kiss, Motörhead, Testament, Guns N' Roses, Fozzy, Quiet Riot, Dio, Whitesnake, Mr. Big, among others.[156]		Jackson's sister La Toya released her song, "Home", on July 28 as a charity single in her brother's honor. All proceeds are being donated to one of Michael's favorite charities.[157] BET's annual 2009 Awards Ceremony aired three days after Jackson's death, on June 28, 2009. It featured a tribute to the singer. Host Jamie Foxx said, "We want to celebrate this black man. He belongs to us and we shared him with everybody else." The ceremony featured performances of several of Jackson's songs, including pieces from his time with The Jackson Five and those from his solo career.[158] Joe Jackson and Al Sharpton were in the audience, and Janet Jackson spoke briefly on behalf of the family. The show was the most watched BET annual awards show in the awards shows history.[159] A few days after Jackson's death, there were news reports to the effect that AEG Live, the promoter for Jackson's This Is It concerts, was preparing a tribute concert for September 2009. The show would reportedly follow the style arranged for the This Is It concerts.[160][161] However, no details of any such concert have been announced.		The day after Jackson's death, the mayor of Rio de Janeiro announced that the city would erect a statue of the singer in the favela of Dona Marta. Jackson visited the community in 1996 and filmed a music video for "They Don't Care About Us" there. The mayor said that Jackson had helped make the community into "a model for social development."[162] Memorials were held all over the world, in places as diverse as Tokyo,[162] Bucharest[163] and Baku, Azerbaijan.[164] In Midyat, Turkey, even a Salat al-Janazah (Islamic funeral prayer) was performed, and traditional funeral helva was cooked and distributed.[165] The music video for "Do the Bartman", a Simpsons song co-written by Jackson, was broadcast ahead of an episode rerun of The Simpsons on June 28. It featured a title card paying tribute to Jackson.[166] The 1991 Simpsons episode that Jackson guest starred in under the name of John Jay Smith, "Stark Raving Dad", was broadcast on Fox on July 5.[167] The episode had been broadcast on the Dutch Comedy Central the day after his death, and was dedicated to Jackson's memory.[168] His 1978 film The Wiz (in which he co-starred alongside Diana Ross and Richard Pryor) was briefly re-released in a rare 35mm format and was shown at the Hollywood Theater in his honor. It was also re-released a week prior to the release of Michael Jackson's This Is It in select cities. Madonna opened the 2009 MTV Video Music Awards with a speech about Michael Jackson. Janet Jackson made an appearance at the VMAs to pay musical tribute to her late brother and honor his career.[169] He was honored with a posthumous lifetime achievement award during the 52nd Grammy Awards on January 31, 2010.[170] Jackson, who had an acting role in the 1978 film The Wiz, was featured in the 82nd annual Academy Awards ceremony's "In Memoriam" tribute.[171]		Jackson's record sales increased dramatically, eightyfold by June 29, according to HMV.[172] Bill Carr of Amazon said the website sold out of all Jackson and Jackson 5 CDs within minutes of the news breaking, and that demand surpassed that for Elvis Presley and John Lennon after their sudden deaths.[173] In Japan, six of his albums made SoundScan Japan's Top 200 Albums chart,[162] and in Poland, Thriller 25 topped the national album chart and was replaced by King of Pop the following week.[174]		In Australia, 15 of his albums occupied the ARIA top 100 as of July 5, four of them in the top ten, with three occupying the top three spots. He had 34 singles in the top 100 singles chart, including four in the top ten. Album sales were 62,015 for the previous week; singles tallied 107,821 units.[175] In the second week, album sales rose from the previous week and tallied 88,650 copies. On July 12, four albums were in the top 10 with three occupying the top three spots. In New Zealand, Thriller 25 topped the chart.[176] In Germany, King of Pop topped the album chart,[177] and from June 28 to July 4, nine of his albums occupied the Top 20 of CAPIF in Argentina.[178] In Billboard's European Top 100 Albums, he made history with eight of his albums in the top ten positions.[179] As of August 3, King of Pop has spent four weeks atop Billboard's European Top 100 Albums chart.[180] The Collection also spent two weeks atop the same chart.[181]		In the UK, on the Sunday following his death, his albums occupied 14 of the top 20 places on the Amazon.co.uk sales chart, with Off the Wall at the top. Number Ones reached the top of the UK Album Chart, and his studio albums occupied number two to number eight on the iTunes Music Store top albums. Six of his songs charted in the top 40: "Man in the Mirror" (11), "Thriller" (23), "Billie Jean" (25), "Smooth Criminal" (28)", "Beat It" (30), and "Earth Song" (38).[182] The following Sunday, 13 of Jackson's songs charted in the top 40, including "Man in the Mirror", which landed the number two spot.[183] He broke Ruby Murray's 1955 record of five songs in the top 30.[184] The Essential Michael Jackson topped the album chart, giving Jackson a second number one album in as many weeks. He had five of the top ten albums in the album chart.[185] In third week sales, The Essential Michael Jackson retained the number one position and Jackson held three other positions within the top five.[186] By August 3, Jackson had sold 2 million records and spent six consecutive weeks atop the album chart.[180][187] He retained the top spot on the album chart for a seventh consecutive week.[188]		In the U.S., Jackson broke three chart records on the first Billboard issue date that followed his death. The entire top nine positions on Billboard's Top Pop Catalog Albums featured titles related to him. By the third week it would be the entire top 12 positions.[189] Number Ones was the best-selling album of the week and topped the catalog chart with sales of 108,000, an increase of 2,340 percent. The Essential Michael Jackson (2) and Thriller (3) also sold over 100,000 units. The other titles on the chart are Off the Wall (4), Jackson Five's Ultimate Collection (5), Bad (6), Dangerous (7), HIStory: Past, Present and Future – Volume 1 (8) and Jackson's Ultimate Collection (9). Collectively, his solo albums sold 422,000 copies in the week following his death, 800,000 copies in the first full week, and 1.1 million copies in the following week of his memorial service.[189] He also broke a record on the Top Digital Albums chart, with six of the top 10 slots, including the entire top four. On the Hot Digital Songs chart he placed a record of 25 songs on the 75-position list. In the U.S., Jackson became the first artist to sell over one million downloads in a week, with 2.6 million sales.[190][191]		By August 5, Jackson had sold nearly 3.8 million albums and 7.6 million tracks in the U.S.. Number Ones was the best-selling album for six out of seven weeks that followed his death.[192][193] By year's end in 2009, Jackson had become the best selling artist of the year selling 8.2 million albums in the U.S.[194] He also became the first artist in history to have four of the top 20 best-selling albums in a single year in the U.S., nearly doubling the sales of his nearest competitor.[195][196] Jackson was also the third best selling digital artist of 2009 in the U.S., selling approximately 12.35 million units.[197] In the 12 months that followed his death Jackson sold nine million albums in the U.S., and 35 million albums worldwide.[198] His estate also generated revenues of one billion dollars.[199]		A private family service was held at Forest Lawn Memorial Park in Los Angeles, after a public memorial at the Staples Center in Los Angeles, California on July 7, where Jackson had rehearsed on June 24, the day before he died. The memorial service was organized by Jackson's concert promoter, AEG Live,[200] who gave away 17,500 free tickets (even if AEG was initially out to sell them, but due to complaints had to desist)[201] to fans worldwide through an online lottery that attracted over 1.2 million applicants in 24 hours,[202] and over a half-billion hits to the webpage.[203] The service was broadcast live around the world, and was believed to have been watched by more than 2.5 billion people.[8][204][205]		Jackson's solid-bronze casket (which reportedly cost USD $25,000)[50] was placed in front of the stage. Numerous celebrity guests attended the services.[206] His brothers each wore a single, white, sparkling glove, while Stevie Wonder, Mariah Carey, Lionel Richie, Jermaine Jackson and others sang his songs. Jackson's then 11-year-old daughter, Paris, broke down as she told the crowd, "I just want to say, ever since I was born, Daddy has been the best father you could ever imagine ... and I just want to say I love him... so much."[207] Marlon Jackson said, "Maybe now, Michael, they will leave you alone."[208]		According to reports, Jackson's burial was originally scheduled for August 29, 2009 (which would have been his 51st birthday).[209] His service and burial was held at Glendale's Forest Lawn Memorial Park on September 3, 2009.[210] The burial was attended by his family members, first wife Lisa Marie Presley as well as his old friends Macaulay Culkin, Chris Tucker, Quincy Jones, Eddie Murphy and Elizabeth Taylor, amongst others.[211] The service began with Jackson's three children placing a golden crown on his casket.[211]		Jackson's funeral cost one million dollars.[212][213] Cost for the funeral included; $590,000 for Jackson's crypt in Forest Lawn's Great Mausoleum, a vast granite and marble filled palazzo, guest invitations for $11,716.[212][213] The bill for security, including the fleet of luxury cars that delivered Jackson's children, parents and siblings to the ceremony, came to $30,000; the florist's bill was $16,000; and the funeral planner was paid $15,000.[212] Howard Weitzman, a lawyer for the estate executors noted that Jackson's family decided on the details of the ceremony, but said a lavish funeral fit the life Jackson lived, commenting, "It was Michael Jackson. He was bigger than life when he was alive."[212][213]		Jackson's remains are interred in the Holly Terrace section in the Great Mausoleum. The mausoleum is a secure facility that is not accessible to the general public or to the media, except on an extremely limited basis. The unmarked crypt, which is partially visible at the tinted entrance of the Holly Terrace mausoleum, is covered in flowers fans leave, which are placed by security guards outside the crypt.[210] The family had considered burying Jackson at Neverland Ranch. However, some family members objected to the site, saying that the ranch had been tainted by the sexual abuse allegations.[210] Also, the owners of the ranch would have had to go through a permitting process with county and state government before establishing a cemetery at the site. In July 2010, security was increased at the mausoleum due to vandalism by fans leaving messages such as "Keep the dream alive" and "Miss you sweet angel" in permanent ink.[214]		
Conversation analysis (commonly abbreviated as CA) is an approach to the study of social interaction, embracing both verbal and non-verbal conduct, in situations of everyday life. As its name implies, CA began with a focus on casual conversation, but its methods were subsequently adapted to embrace more task- and institution-centered interactions, such as those occurring in doctors' offices, courts, law enforcement, helplines, educational settings, and the mass media. As a consequence, the term 'conversation analysis' has become something of a misnomer, but it has continued as a term for a distinctive and successful approach to the analysis of social interactions.						Inspired by Harold Garfinkel's ethnomethodology[1] and Erving Goffman's conception of the interaction order,[2] CA was developed in the late 1960s and early 1970s principally by the sociologist Harvey Sacks and his close associates Emanuel Schegloff and Gail Jefferson.[3] Today CA is an established method used in sociology, anthropology, linguistics, speech-communication and psychology. It is particularly influential in interactional sociolinguistics, discourse analysis and discursive psychology.		As in all research, conversational analysis begins by setting up a research problem. The data collected for CA is in the form of video or audio recorded conversations. The data is collected with or without researchers' involvement, often simply by adding a video camera to the room where the conversation takes place (e.g. medical doctors consultation with a patient). From the audio or video recording the researchers construct a detailed transcription (ideally with no details left out). After transcription, the researchers perform inductive data-driven analysis aiming to find recurring patterns of interaction. Based on the analysis, the researchers develop a rule or model to explain the occurrence of the patterns.		In the absence of formal agendas, the set of practices through which turns are allocated in conversation has been the subject of its own study. The Turn-taking model for conversation was founded through empirical investigation of field recordings of conversations and fitted to the concept that in conversation, participants are expected to issue their utterances in allocated turns. The most basic forms take place in two-party conversations where sentence completion, or pause, might be enough to allocate the next turn to the co-present party in the manner that has been discussed under the rubric of 'adjacency pairs'.		In multi-party conversations the mechanisms were found to be more complicated where 'current speaker selects next' is a possibility, and how frequently individual utterances are tailored for the sequence of speakers within the conversation. The possibility of obtaining not only the next turn, but a series of turns (required for example in telling a joke or story) is documented in analyses of announcements and story prefaces. A certain balance in conversation could be located in the process whereby turns are allocated. That balance was directed at the 'turn commodity', but also in countless other instances, for example with person identifiers and locators where minimal forms are utilized to evenly distribute whom speaks during the conversation.		Other collections of turn allocation mechanisms include use of 'repeats', the elision of lexical forms (words), the use of temporal regulators in turns including chuckles, 'uhm', ‘yuh know’, and ‘right’, the use of speech particles like ‘uh’, and ‘oh’, and other specifically short-syllabic devices that are consonant-prefaced like ‘tih’.		According to CA, the turn-taking system consists of two distinct components: the allocation mechanism which is responsible for distributing a turn (in any case), and the lexical components that parties utilize in filling that turn while remaining sequentially implicit in order to deal with the contingency of conversations that forces turn taking to happen.[4]		The turn constructional component describes basic units out of which turns are fashioned. These basic units are known as Turn construction unit or TCUs. Unit types include: lexical, clausal, phrasal, and sentential.		The turn allocation component describes how participants organize their interaction by distributing turns to speakers which coincide with sequence organization which focuses on how actions are ordered in conversation.		At a transition relevance place (TRP) which is a place in the conversation in where who is speaking shifts, a set of rules apply in quick succession so that turns are allocated instantly: 1. Current speaker selects next speaker: this can be done by the use of addressing terms (e.g. names), initiating action with eye contact, initiating action that limits the potential eligible respondents and the availability of environmental cues (e.g. requesting the passing of salt in a situation where only a particular person is sitting close to the salt). 2. Next speaker self-selects: when there is no apparent addressee and potential respondents, one might self-select to continue the conversation. This can be done by overlapping, using turn-entry devices such as "well" or "you know"; and recycled turn beginning, which is a practice that involves repeating the part of a turn beginning that gets absorbed in an overlap. 3. Current speaker continues: If no one takes up the conversation, the original speaker may again speak to provide further information to aid the continuation of the conversation. This can be done by adding an increment, which is a grammatically fitted continuation of an already completed turn construction unit (TCU). Alternatively, the speaker can choose to start a new TCU, usually to offer clarification or to start a new topic.		Talk tends to occur in responsive pairs; however, the pairs may be split over a sequence of turns. Adjacency pairs divide utterance types into 'first pair parts' and 'second pair parts' to form a 'pair type'. There are lots of examples of adjacency pairs including Questions-Answers, Offer-Acceptance/Refusal and Compliment-Response. (Schegloff & Sacks:1973) [5]		Sequence expansion allows talk which is made up of more than a single adjacency pair to be constructed and understood as performing the same basic action and the various additional elements are as doing interactional work related to the basic action underway. Sequence expansion is constructed in relation to a base sequence of a first pair part (FPP) and a second pair part (SPP) in which the core action underway is achieved. It can occur prior to the base FPP, between the base FPP and SPP, and following the base SPP. 1. Pre-expansion: an adjacency pair that may be understood as preliminary to the main course of action. A generic pre-expansion is a summon-answer adjacency pair, as in "Mary?"/ "Yes?".It is generic in the sense that it does not contribute to any particular types of base adjacency pair, such as request or suggestion. There are other types of pre-sequence that work to prepare the interlocutors for the subsequent speech action. For example, "Guess what!"/"What?" as preliminary to an announcement of some sort, or "What are you doing?"/"Nothing" as preliminary to an invitation or a request. 2. Insert expansion: an adjacency pair that comes between the FPP and SPP of the base adjacency pair. Insert expansions interrupt the activity under way, but are still relevant to that action.[6] Insert expansion allows a possibility for a second speaker, the speaker who must produce the SPP, to do interactional work relevant to the projected SPP. An example of this would be a typical conversation between a customer and a shopkeeper:		3. Post-expansion: a turn or an adjacency pair that comes after, but is still tied to, the base adjacency pair. There are two types: minimal and non-minimal. Minimal expansion is also termed sequence closing thirds, because it is a single turn after the base SPP (hence third) that does not project any further talk beyond their turn (hence closing). Examples of SCT include "oh", "I see", "okay", etc.		4. Silence: Silence can occur throughout the entire speech act but in what context it is happening depends what the silence means. Three different assets can be imp;lied through silence:		CA may reveal structural (i.e. practice-underwritten) preferences in conversation for some types of actions (within sequences of action) over others. For example, responsive actions which agree with, or accept, positions taken by a first action tend to be performed more straightforwardly and faster than actions that disagree with, or decline, those positions (Pomerantz 1984; Davidson 1984). The former is termed an unmarked turn shape, meaning the turn is not preceded by silence nor is it produced with delays, mitigation and accounts. The latter is termed marked turn shape, which describes a turn with opposite characteristics. One consequence of this is that agreement and acceptance are promoted over their alternatives, and are more likely to be the outcome of the sequence. Pre-sequences are also a component of preference organization and contribute to this outcome (Schegloff 2007).		Repair organization describes how parties in conversation deal with problems in speaking, hearing, or understanding. Repair segments are classified by who initiates repair (self or other), by who resolves the problem (self or other), and by how it unfolds within a turn or a sequence of turns. The organization of repair is also a self-righting mechanism in social interaction (Schegloff, Jefferson, and Sacks 1977). Participants in conversation seek to correct the trouble source by initiating and preferring self repair, the speaker of the trouble source, over other repair (Schegloff, Jefferson, and Sacks 1977). Self repair initiations can be placed in three locations in relation to the trouble source, in a first turn, a transition space or in a third turn (Schegloff, Jefferson, and Sacks).		This focuses on the description of the practices by which turns at talk are composed and positioned so as to realize one or another actions.		In contrast to the research inspired by Noam Chomsky, which is based on a distinction between competence and performance and dismisses the particulars of actual speech, Conversation Analysis studies naturally-occurring talk and shows that spoken interaction is systematically orderly in all its facets (cf. Sacks in Atkinson and Heritage 1984: 21-27). In contrast to the theory developed by John Gumperz, CA maintains it is possible to analyze talk-in-interaction by examining its recordings alone (audio for telephone, video for copresent interaction). CA researchers do not believe that the researcher needs to consult with the talk participants or members of their speech community.		It is distinct from discourse analysis in focus and method. (i) Its focus is on processes involved in social interaction and does not include written texts or larger sociocultural phenomena (for example, 'discourses' in the Foucauldian sense). (ii) Its method, following Garfinkel and Goffman's initiatives, is aimed at determining the methods and resources that the interacting participants use and rely on to produce interactional contributions and make sense of the contributions of others. Thus CA is neither designed for, nor aimed at, examining the production of interaction from a perspective that is external to the participants' own reasoning and understanding about their circumstances and communication. Rather the aim is to model the resources and methods by which those understandings are produced.		In recent years, CA has been employed by researchers in other fields, such as feminism and feminist linguistics, or used in complement with other theories, such as Membership Categorization Analysis (MCA). MCA was influenced by the work on Harvey Sacks and his work on Membership Categorization Device (MCD). Sacks argues that 'members’ categories comprise part of the central machinery of organization and developed the notion of MCD to explain how categories can be hearably linked together by native speakers of a culture. His example that is taken from a children's storybook (The baby cried. The mommy picked it up.) shows how "mommy" is interpreted as the mother of the baby by speakers of the same culture. In light of this, categories are inference rich[7] – a great deal of knowledge members of a society have about the society is stored in terms of these categories.[8] Stokoe further contends that members’ practical categorizations form part of ethnomethodology's description of the ongoing production and realization of ‘facts’ about social life and including members’ gendered reality analysis, thus making CA compatible with feminist studies.[9]		The following is a list of important phenomena identified in the conversation analysis literature, followed by a brief definition and citations to articles that examine the named phenomenon either empirically or theoretically. Articles in which the term for the phenomenon is coined or which present the canonical treatment of the phenomenon are in bold, those that are otherwise centrally concerned with the phenomenon are in italics, and the rest are articles that otherwise aim to make a significant contribution to an understanding of the phenomenon.				
Lawyer jokes, which predate Shakespeare's era, are commonly told by those outside the profession as an expression of contempt, scorn and derision.[1] They serve as a form of social commentary or satire reflecting the cultural perception of lawyers.		The first thing we do, let's kill all the lawyers						In 1728, John Gay wrote this verse as part of The Beggar's Opera:		A Fox may steal your hens, sir A Whore your health and pence, sir Your daughter rob your chest, sir Your wife may steal your rest, sir A thief your goods and plate But this is all but picking With rest, pence, chest and chicken It ever was decreed, sir If Lawyer's Hand is fee'd, sir He steals your whole estate[3]		At the end of the 1800s, Ambrose Bierce satirically defined litigation as "a machine which you go into as a pig and come out as a sausage".[4]		The line "Doesn't it strike the company as a little unusual that a lawyer should have his hands in his own pockets?" is cited by Samuel Clemens (Mark Twain) but likely originated earlier.[5]		In the modern era, many complaints about lawyers fall into five general categories:		A recurring theme, historically and today, is that of exorbitant legal fees consuming the entire value of property at stake in an estate or a dispute:		How many lawyers does it take to change a light bulb? How many can you afford?		Or:		It takes only one lawyer to change your lightbulb to his lightbulb.[7]		The tale of the freshly-acquitted horse thief pleading that the judge issue an arrest warrant for "that dirty lawyer of mine" because "Your honour, you see, I didn't have the money to pay his fee, so he went and took the horse I stole"[8] is often modernised to "he went and took the car I stole"[9] with little or nothing else changed.		While telling an ethnic joke risks the label of racism, lawyers are perceived as a highly privileged class, seemingly accountable only to other lawyers; the Bar Association, the judges, even many of the politicians and legislators are their fellow lawyers who inevitably give them free rein. After all, one does not choose one's ethnicity but may choose whether to pursue a career in law.[10]		Of those of all the professions, lawyer jokes are often the most blunt and to the point:		What is the difference between a catfish and a lawyer? One is a scum-sucking, bottom-feeding scavenger. The other is a fish.		Or:		Why don't sharks eat lawyers? Professional courtesy.[11]		Much like the foul-mouthed parrot or the dumb blonde, the heartless, cynical attorney is a stock character in many joke collections.		Often told is the anecdote where a wealthy lawyer, solicited for a charitable donation, replies "Do you realise my mother is dying of a long-term illness and has medical bills several times her income? Did you know that my brother, a disabled veteran, is blind and in a wheelchair? Do you understand my sister is widowed and penniless with three dependent children? Well, since I don't give any money to them, why should I give any to you?"[12]		Similarly:		Lawyer: "I have some good news for you" Client: "What good news? You lost my case, I was convicted of a murder I did not commit and was sentenced to die in the electric chair." Lawyer: "That's all true, but I got the voltage lowered."[12]		Other anecdotes are based on logical fallacy, such as a lawyer defending a client on trial for killing his parents: "Ladies and gentlemen of the jury, I appeal to your basic decency to take mercy on this poor, defenceless orphan!"[13]		Occasionally, lawyers themselves use self-deprecating humour about lawyers or the legal profession in an attempt to add levity to otherwise bland topics. Lawyers giving a talk, especially to the profession, often employ jokes as icebreakers.		
Email forwarding generically refers to the operation of re-sending an email message delivered to one email address on to a possibly different email address. The term forwarding has no specific technical meaning.[1] Users and administrators of email systems use the same term when speaking of both server-based and client-based forwarding.		Email forwarding can also redirect mail going to one address and send it to one or several other addresses. Vice versa, email items going to several different addresses can converge via forwarding to end up in a single address in-box.						The domain name (the part appearing to the right of @ in an email address) defines the target server(s)[2] for the corresponding class of addresses. A domain may also define backup servers; they have no mailboxes and forward messages without changing any part of their envelopes.[3] By contrast, primary servers can deliver a message to a user's mailbox and/or forward it by changing some envelope addresses. ~/.forward files (see below) provide a typical example of server-based forwarding to different recipients.		Email administrators sometimes use the term redirection as a synonym for server-based email-forwarding to different recipients. Protocol engineers sometimes use the term Mediator to refer to a forwarding server.[4]		Because of spam, it is becoming increasingly difficult to reliably forward mail across different domains, and some recommend avoiding it if at all possible.[5]		Plain message-forwarding changes the envelope recipient(s) and leaves the envelope sender field untouched. The "envelope sender" field does not equate to the From header which Email client software usually displays: it represents a field used in the early stages of the SMTP protocol, and subsequently saved as the Return-Path header. This field holds the address to which mail-systems must send bounce messages — reporting delivery-failure (or success) — if any.		By contrast, the terms remailing or redistribution can sometimes mean re-sending the message and also rewriting the "envelope sender" field. Electronic mailing lists furnish a typical example. Authors submit messages to a reflector that performs remailing to each list address. That way, bounce messages (which report a failure delivering a message to any list- subscriber) will not reach the author of a message. However, annoying misconfigured vacation autoreplies do reach authors.		Typically, plain message-forwarding does alias-expansion, while proper message-forwarding, also named forwarding tout-court[1] serves for mailing-lists. When additional modifications to the message are carried out, so as to rather resemble the action of a Mail User Agent submitting a new message, the term forwarding becomes deceptive and remailing seems more appropriate.		In the Sender Policy Framework (SPF), the domain-name in the envelope sender remains subject to policy restrictions. Therefore, SPF generally disallows plain message-forwarding. Intra domain redirection complies with SPF as long as the relevant servers share a consistent configuration. Mail servers that practise inter-domain message-forwarding may break SPF even if they don't implement SPF themselves, i.e. they neither apply SPF checks nor publish SPF records.[7] Sender Rewriting Scheme provides for a generic forwarding mechanism compatible with SPF.		Client forwarding can take place automatically using a non-interactive client such as a mail retrieval agent. Although the retrieval agent uses a client protocol, this forwarding resembles server forwarding in that it keeps the same message-identity. Concerns about the envelope-sender apply.[7]		An end-user can manually forward a message using an email client. Forwarding inline quotes the message below the main text of the new message, and usually preserves original attachments as well as a choice of selected headers (e.g. the original From and Reply-To.) The recipient of a message forwarded this way may still be able to reply to the original message; the ability to do so depends on the presence of original headers and may imply manually copying and pasting the relevant destination addresses.		Forwarding as attachment prepares a MIME attachment (of type message/rfc822) that contains the full original message, including all headers and any attachment. Note that including all the headers discloses much information about the message, such as the servers that transmitted it and any client-tag added on the mailbox. The recipient of a message forwarded this way may be able to open the attached message and reply to it seamlessly.		This kind of forwarding actually constitutes a remailing from the points of view of the envelope-sender and of the recipient(s). The message identity also changes.		RFC 821, Simple Mail Transfer Protocol, by Jonathan B. Postel in 1982, provided for a forward-path for each recipient, in the form of, for example, @USC-ISIE.ARPA, @USC-ISIF.ARPA: Q-Smith@ISI-VAXA.ARPA — an optional list of hosts and a required destination-mailbox. When the list of hosts existed, it served as a source-route, indicating that each host had to relay the mail to the next host on the list. Otherwise, in the case of insufficient destination-information but where the server knew the correct destination, it could take the responsibility to deliver the message by responding as follows:		The concept at that time envisaged the elements of the forward-path (source route) moving to the return-path (envelope sender) as a message got relayed from one SMTP server to another. Even if the system discouraged the use of source-routing,[8] dynamically building the return-path implied that the "envelope sender" information could not remain in its original form during forwarding. Thus RFC 821 did not originally allow plain message-forwarding.		The introduction of the MX record[9] made source-routing unnecessary. In 1989, RFC 1123 recommended accepting source-routing only for backward-compatibility. At that point, plain message forwarding[7] became the recommended action for alias-expansion. In 2008, RFC 5321 still mentions that "systems may remove the return path and rebuild [it] as needed", taking into consideration that not doing so might inadvertently disclose sensitive information.[10] Actually, plain message-forwarding can be conveniently used for alias expansion within the same server or a set of coordinated servers.		sendmail, the reference SMTP implementation in the early 1980s, provided for ~/.forward files, which can store the target email-addresses for given users. This kind of server-based forwarding is sometimes called dot-forwarding.[11] One can configure some email-program filters to automatically perform forwarding or replying actions immediately after receiving. Forward files can also contain shell scripts, which have become a source of many security problems. Formerly only trusted users could utilize the command-line switch for setting the envelope sender, -f arg; some systems disabled this feature for security reasons.[12]		Email predates the formalization of client–server architectures in the 1990s.[13] Therefore, the distinction between client and server seems necessarily forced. The original distinction contrasted daemons and user-controlled programs which run on the same machine. The sendmail daemon used to run with root privileges so it could impersonate any user whose mail it had to manage. On the other hand, users can access their own individual mail-files and configuration files, including ~/.forward. Client programs may assist in editing the server configuration-files of a given user, thereby causing some confusion as to what role each program plays.		The term "virtual users" refers to email users who never log on a mail-server system and only access their mailboxes using remote clients. A mail-server program may work for both virtual and regular users, or it may require minor modifications to take advantage of the fact that virtual users frequently share the same system id. The latter circumstance allows the server program to implement some features more easily, as it doesn't have to obey system-access restrictions. The same principles of operations apply. However, virtual users have more difficulty in accessing their configuration files, for good or ill.		
In linguistics and social sciences, markedness is the state of standing out as unusual or difficult in comparison to a more common or regular form. In a marked–unmarked relation, one term of an opposition is the broader, dominant one. The dominant default or minimum-effort form is known as unmarked; the other, secondary one is marked. In other words, markedness involves the characterization of a "normal" linguistic unit against one or more of its possible "irregular" forms.		In linguistics, markedness can apply to, among others, phonological, grammatical, and semantic oppositions, defining them in terms of marked and unmarked oppositions, such as honest (unmarked) vs. dishonest (marked). Marking may be purely semantic, or may be realized as extra morphology. The term derives from the marking of a grammatical role with a suffix or another element, and has been extended to situations where there is no morphological distinction.		In social sciences more broadly, markedness is, among other things, used to distinguish two meanings of the same term, where one is common usage (unmarked sense) and the other is specialized to a certain cultural context (marked sense).		In statistics and psychology, the social science concept of markedness is quantified as a measure of how much one variable is marked as a predictor or possible cause of another, and is also known as Δp (deltaP) in simple two-choice cases.						In terms of lexical opposites, a marked form is a non-basic one, often one with inflectional or derivational endings. Thus, a morphologically negative word form is marked as opposed to a positive one: happy/unhappy, honest/dishonest, fair/unfair, clean/unclean and so forth. Similarly, unaffixed masculine or singular forms are taken to be unmarked in contrast to affixed feminine or plural forms: lion/lioness, host/hostess, automobile/automobiles, child/children. An unmarked form is also a default form. For example, the unmarked lion can refer to a male or female, while lioness is marked because it can refer only to females.		The default nature allows unmarked lexical forms to be identified even when the opposites are not morphologically related. In the pairs old/young, big/little, happy/sad, clean/dirty, the first term of each pair is taken as unmarked because it occurs generally in questions. For example, English speakers typically ask how (old, big, happy, or clean) something or someone is; use of the marked term (how young are you?) would presuppose youth, smallness, unhappiness, or dirtiness, respectively.		While the idea of linguistic asymmetry predated the actual coining of the terms marked and unmarked, the modern concept of markedness originated in the Prague School structuralism of Roman Jakobson and Nikolai Trubetzkoy as a means of characterizing binary oppositions.[1] Both sound and meaning were analyzed into systems of binary distinctive features. Edwin Battistella said "Binarism suggests symmetry and equivalence in linguistic analysis; markedness adds the idea of hierarchy."[2] Trubetzkoy and Jakobson analyzed phonological oppositions such as nasal versus non-nasal as defined as the presence versus the absence of nasality; the presence of the feature, nasality, was marked; its absence, non-nasality, was unmarked. For Jakobson and Trubetzkoy, binary phonological features formed part of a universal feature alphabet applicable to all languages. In his 1932 article "Structure of the Russian Verb", Jakobson extended the concept to grammatical meanings in which the marked element "announces the existence of [some meaning] A" while the unmarked element "does not announce the existence of A, i.e., does not state whether A is present or not'.[3] Forty years later, Jakobson described language by saying that "every single constituent of a linguistic system is built on an opposition of two logical contradictories: the presence of an attribute ('markedness') in contraposition to its absence ('unmarkedness')."[4]		In his 1941 Child Language, Aphasia, and Universals of Language, Jakobson suggested that phonological markedness played a role in language acquisition and loss. Drawing on existing studies of acquisition and aphasia, Jakobson suggested a mirror-image relationship determined by a universal feature hierarchy of marked and unmarked oppositions. Today many still see Jakobson's theory of phonological acquisition as identifying useful tendencies.[5]		The work of Cornelius van Schooneveld, Edna Andrews, Rodney Sangster, Yishai Tobin and others on 'semantic invariance' (different general meanings reflected in the contextual specific meanings of features) has further developed the semantic analysis of grammatical items in terms of marked and unmarked features. Other semiotically-oriented work has investigated the isomorphism of form and meaning with less emphasis on invariance, including the efforts of Henning Andersen, Michael Shapiro, and Edwin Battistella. Shapiro and Andrews have especially made connections between the semiotic of C. S. Peirce and markedness, treating it "as species of interpretant" in Peirce's sign-object-interpretant triad.		Functional linguists such as Talmy Givón have suggested that markedness is related to cognitive complexity—"in terms of attention, mental effort or processing time".[6] Linguistic 'naturalists' view markedness relations in terms of the ways in which extralinguistic principles of perceptibility and psychological efficiency determine what is natural in language. Willi Mayerthaler, another linguist, for example, defines unmarked categories as those "in agreement with the typical attributes of the speaker".[7]		Since a main component of markedness is the information content and information value of an element,[8] some studies have taken markedness as an encoding of that which is unusual or informative, and this is reflected in formal probabilistic definitions of Markedness and Informedness as chance-correct unidirectional components of the Matthews Correlation Coefficient corresponding to Δp and Δp'.[9] Conceptual familiarity with cultural norms provided by familiar categories creates a ground against which marked categories provide a figure, opening the way for markedness to be applied to cultural and social categorization.		As early as the 1930s Jakobson had already suggested applying markedness to all oppositions, explicitly mentioning such pairs as life/death, liberty/bondage, sin/virtue, and holiday/working day. Linda Waugh extended this to oppositions like male/female, white/black, sighted/blind, hearing/deaf, heterosexual/homosexual, right/left, fertility/barrenness, clothed/nude, and spoken language/written language.[10] Battistella expanded this with the demonstration of how cultures align markedness values to create cohesive symbol systems, illustrating with examples based on Rodney Needham's work.[11] Other work has applied markedness to stylistics, music, and myth.[12][13][14]		Markedness depends on context. What is more marked in some general contexts may be less marked in other local contexts. Thus, "ant" is less marked than "ants" on the morphological level, but on the semantic (and frequency) levels it may be more marked since ants are more often encountered many at once than one at a time. Often a more general markedness relation may be reversed in a particular context. Thus, voicelessness of consonants is typically unmarked. But between vowels or in the neighborhood of voiced consonants, voicing may be the expected or unmarked value.		Reversal is reflected in certain Frisian words' plural and singular forms:[15] In Frisian, nouns with irregular singular-plural stem variations are undergoing regularization.[citation needed] Usually this means that the plural is reformed to be a regular form of the singular:		However, a number of words instead reform the singular by extending the form of the plural:		The common feature of the nouns that regularize the singular to match the plural is that they occur more often in pairs or groups than singly; they are said to be semantically (but not morphologically) locally unmarked in the plural.		Joseph Greenberg's 1966 book Language Universals was an influential application of markedness to typological linguistics and a break from the tradition of Jakobson and Trubetzkoy. Greenberg took frequency to be the primary determining factor of markedness in grammar and suggested that unmarked categories could be determined by "the frequency of association of things in the real world".		Greenberg also applied frequency cross-linguistically, suggesting that unmarked categories would be those that are unmarked in a wide number of languages. However, critics have argued that frequency is problematic because categories that are cross-linguistically infrequent may have a high distribution in a particular language.[16]		More recently the insights related to frequency have been formalized as chance-corrected conditional probabilities, with Informedness (Δp') and Markedness (Δp) corresponding to the different directions of prediction in human association research (binary associations or distinctions)[17] and more generally (including features with more than two distinctions).[9]		Universals have also been connected to implicational laws. This entails that a category is taken as marked if every language that has the marked category also has the unmarked one but not vice versa.		Markedness has been extended and reshaped over the past century and reflects a range of loosely connected theoretical approaches. From emerging in the analysis of binary oppositions, it has become a global semiotic principle, a means of encoding naturalness and language universals, and a terminology for studying defaults and preferences in language acquisition. What connects various approaches is a concern for the evaluation of linguistic structure, though the details of how markedness is determined and what its implications and diagnostics are varies widely. Other approaches to universal markedness relations focus on functional economic and iconic motivations, tying recurring symmetries to properties of communication channels and communication events. Croft (1990), for example, notes that asymmetries among linguistic elements may be explainable in terms economy of form, in terms of iconism between the structure of language and conceptualization of the world.		Markedness entered generative linguistic theory through Chomsky and Halle's The Sound Pattern of English. For Chomsky and Halle, phonological features went beyond a universal phonetic vocabulary to encompass an 'evaluation metric', a means of selecting the most highly-valued adequate grammar. In The Sound Pattern of English, the value of a grammar was the inverse of the number of features required in that grammar. However, Chomsky and Halle realized that their initial approach to phonological features made implausible rules and segment inventories as highly valued as natural ones. The unmarked value of a feature was cost-free with respect to the evaluation metric, while the marked feature values were counted by the metric. Segment inventories could also be evaluated according to the number of marked features. However, the use of phonological markedness as part of the evaluation metric was never able to fully account for the fact that some features are more likely than others or for the fact that phonological systems must have a certain minimal complexity and symmetry[18]		In generative syntax, markedness as feature-evaluation did not receive the same attention that it did in phonology. Chomsky came to view unmarked properties as an innate preference structure based first in constraints and later in parameters of universal grammar. In their 1977 article 'Filters and Control', Chomsky and Howard Lasnik extended this to view markedness as part of a theory of 'core grammar':		A few years later Chomsky describes it thus:		Some generative researchers have applied markedness to second-language acquisition theory, treating it as an inherent learning hierarchy which reflects the sequence in which constructions are acquired, the difficulty of acquiring certain constructions, and the transferability of rules across languages.[21] More recently, Optimality theory approaches emerging in the 1990s have incorporated markedness in the ranking of constraints.[22]		
