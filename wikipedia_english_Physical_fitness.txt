Sprinting is running over a short distance in a limited period of time. It is used in many sports that incorporate running, typically as a way of quickly reaching a target or goal, or avoiding or catching an opponent. Human physiology dictates that a runner's near-top speed cannot be maintained for more than 30–35 seconds due to the depletion of phosphocreatine stores in muscles, and perhaps secondarily to excessive metabolic acidosis as a result of anaerobic glycolysis.[1]		In athletics and track and field, sprints (or dashes) are races over short distances. They are among the oldest running competitions. The first 13 editions of the Ancient Olympic Games featured only one event—the stadion race, which was a race from one end of the stadium to the other.[2] There are three sprinting events which are currently held at the Summer Olympics and outdoor World Championships: the 100 metres, 200 metres, and 400 metres. These events have their roots in races of imperial measurements which were later altered to metric: the 100 m evolved from the 100-yard dash,[3] the 200 m distance came from the furlong (or  1⁄8 mile),[4] and the 400 m was the successor to the 440-yard dash or quarter-mile race.[1]		At the professional level, sprinters begin the race by assuming a crouching position in the starting blocks before leaning forward and gradually moving into an upright position as the race progresses and momentum is gained. The set position differs depending on the start. Body alignment is of key importance in producing the optimal amount of force. Ideally the athlete should begin in a 4-point stance and push off using both legs for maximum force production.[5] Athletes remain in the same lane on the running track throughout all sprinting events,[1] with the sole exception of the 400 m indoors. Races up to 100 m are largely focused upon acceleration to an athlete's maximum speed.[5] All sprints beyond this distance increasingly incorporate an element of endurance.[6]		The 60 metres is a common indoor event and it is an indoor world championship event. Less common events include the 50 metres, 55 metres, 300 metres, and 500 metres which are used in some high school and collegiate competitions in the United States.		Biological factors that determine a sprinter's potential include:		Note: Indoor distances are less standardized as many facilities run shorter or occasionally longer distances depending on available space. 60m is the championship distance.		* A rarely run sprinting event that was once more commonplace. The world record time of 5.99 is held by Lee McRae, and was set in 1987. The time is often used for American Football speed training.		Starting blocks are used for all competition sprint (up to and including 400 m) and relay events (first leg only, up to 4x400 m).[15] The starting blocks consist of two adjustable footplates attached to a rigid frame. Races commence with the firing of the starter's gun.[15] The starting commands are "On your marks" and "Set".[15] Once all athletes are in the set position, the starter's gun is fired, officially starting the race. For the 100 m, all competitors are lined up side-by-side. For the 200 m, 300 m and 400 m, which involve curves, runners are staggered for the start.		In the rare event that there are technical issues with a start, a green card is shown to all the athletes. The green card carries no penalty. If an athlete is unhappy with track conditions after the "on your marks" command is given, the athlete must raise a hand before the "set" command and provide the Start referee with a reason. It is then up to the Start referee to decide if the reason is valid. In the event that the Start referee deems the reason invalid, a yellow card (warning) is issued to that particular athlete. In the event that the athlete is already on a warning the athlete is disqualified.		According to the IAAF rules, "An athlete, after assuming a full and final set position, shall not commence his starting motion until after receiving the report of the gun, or approved starting apparatus. If, in the judgement of the Starter or Recallers, he does so any earlier, it shall be deemed a false start."[15]		The 100 m Olympic Gold and Silver medallist Linford Christie of Great Britain famously had frequent false starts that were marginally below the legal reaction time of 0.1 seconds. Christie and his coach, Ron Roddan, both claimed that the false starts were due to Christie's exceptional reaction times being under the legal time. His frequent false starting eventually led to his disqualification from the 1996 Summer Olympics 100 m final in Atlanta, Georgia, US due to a second false start by Christie. Since January 2010, under IAAF rules, a single false start by an athlete results in disqualification. In 2012, a new development to the false start rule was added. Because certain athletes could be disqualified for twitching in the starting blocks but some athletes could make a twitch without the starter noticing and disqualifying the athlete, it was decided that twitching in the starting block while being in the 'set' position would only carry a maximum penalty of a yellow card or a warning. In order to instantly be disqualified for a false start, an athlete's hands must leave the track or their feet must leave the starting blocks, while the athlete is in their final 'set' position.		For all Olympic sprint events, runners must remain within their pre-assigned lanes, which measure 1.22 metres (4 feet) wide, from start to finish.[16] The lanes can be numbered 1 through 8, 9, or rarely 10, starting with the inside lane. Any athlete who runs outside the assigned lane to gain an advantage is subject to disqualification. If the athlete is forced to run outside of his or her lane by another person, and no material advantage is gained, there will be no disqualification. Also, a runner who strays from his or her lane in the straightaway, or crosses the outer line of his or her lane on the bend, and gains no advantage by it, will not be disqualified as long as no other runner is obstructed.		The first athlete whose torso reaches the vertical plane of the closest edge of the finish line is the winner. To ensure that the sprinter's torso triggers the timing impulse at the finish line rather than an arm, foot, or other body part, a double Photocell is commonly used. Times are only recorded by an electronic timing system when both of these Photocells are simultaneously blocked. Photo finish systems are also used at some track and field events.		While genetics play a large role in one's ability to sprint,[17] athletes must be dedicated to their training to ensure that they can optimize their performances. Sprint training includes various running workouts, targeting acceleration, speed development, speed endurance, special endurance, and tempo endurance. Additionally, athletes perform intense strength training workouts, as well as plyometric or jumping workouts. Collectively, these training methods produce qualities which allow athletes to be stronger, more powerful, in hopes of ultimately running faster.		|group2 = See also |list2 =		|below = }}		
Anaerobic exercise is a physical exercise intense enough to cause lactate to form. It is used by athletes in non-endurance sports to promote strength, speed and power and by body builders to build muscle mass. Muscle energy systems trained using anaerobic exercise develop differently compared to aerobic exercise, leading to greater performance in short duration, high intensity activities, which last from mere seconds to up to about 2 minutes.[1] Any activity lasting longer than about two minutes has a large aerobic metabolic component.[citation needed]		Anaerobic metabolism, or anaerobic energy expenditure, is a natural part of whole-body metabolic energy expenditure.[2] Fast twitch muscle (as compared to slow twitch muscle) operates using anaerobic metabolic systems, such that any recruitment of fast twitch muscle fibers leads to increased anaerobic energy expenditure. Intense exercise lasting upwards of about four minutes (e.g., a mile race) may still have a considerable anaerobic energy expenditure component. High-intensity interval training, although based on aerobic exercises like running, cycling and rowing, effectively becomes anaerobic when performed in excess of 90% maximum heart rate. Anaerobic energy expenditure is difficult to accurately quantify, although several reasonable methods to estimate the anaerobic component to exercise are available.[1][3][4]		In contrast, aerobic exercise includes lower intensity activities performed for longer periods of time. Activities such as walking, long slow runs, rowing, and cycling require a great deal of oxygen to generate the energy needed for prolonged exercise (i.e., aerobic energy expenditure). In sports which require repeated short bursts of exercise however, the anaerobic system enables muscles to recover for the next burst. Therefore, training for many sports demands that both energy producing systems be developed.		The two types of anaerobic energy systems are: 1) high energy phosphates, adenosine triphosphate and creatine phosphate; and 2) anaerobic glycolysis. The former is called alactic anaerobic and the latter lactic anaerobic system.[5] High energy phosphates are stored in limited quantities within muscle cells. Anaerobic glycolysis exclusively uses glucose (and glycogen) as a fuel in the absence of oxygen, or more specifically when ATP is needed at rates that exceed those provided by aerobic metabolism. The consequence of such rapid glucose breakdown is the formation of lactic acid (or more appropriately, its conjugate base lactate at biological pH levels). Physical activities that last up to about thirty seconds rely primarily on the former, ATP-CP phosphagen system. Beyond this time both aerobic and anaerobic glycolysis-based metabolic systems begin to predominate.		The by-product of anaerobic glycolysis, lactate, has traditionally been thought to be detrimental to muscle function. However, this appears likely only when lactate levels are very high. Elevated lactate levels are only one of many changes that occur within and around muscle cells during intense exercise that can lead to fatigue. Fatigue, that is muscle failure, is a complex subject. Elevated muscle and blood lactate concentrations are a natural consequence of any physical exertion. The effectiveness of anaerobic activity can be improved through training.[6]		|group2 = See also |list2 =		|below = }}		
Bodyweight exercises are strength training exercises that do not require free weights or machines as the individual's own weight provides resistance against gravity.[1] It is recognised that bodyweight exercises can enhance a range of biomotor abilities including strength, power, endurance, speed, flexibility, coordination and balance.[2] This type of strength training has grown in popularity for both recreational and professional athletes, with a range of sports disciplines using bodyweight resistance training as part of their fitness programs.[2] Bodyweight training utilises simple abilities such as pushing, pulling, squatting, bending, twisting and balancing.[2] Movements such as the push-up, the pull-up, and the sit-up are some of the most common bodyweight exercises.[3]						Bodyweight exercises are the ideal choice for individuals who are interested in fitness but do not have access to equipment, because they do not require weights or specialised machinery.[1] While some exercises may require some type of equipment, the majority of bodyweight exercises require none. For those exercises that do require equipment, common items found in the household are usually sufficient (such as a bath towel for towel curls), or substitutes can usually be improvised (for example, using a horizontal tree branch to perform pull ups). Therefore, bodyweight exercises are a good choice when travelling or on holiday, when access to a gym or specialised equipment may not be possible.[4] Another advantage of bodyweight training is that there are no costs involved,[1] such as gym membership fees.		Most bodyweight exercises can be progressed or regressed to match the individual's abilities. This progression/regression strategy allows people of nearly all levels of fitness to participate. Some basic methods to increase or decrease the difficulty of a bodyweight exercise, without adding extra weight, are: changing the amount of leverage in an exercise (such as elevating the feet for a standard push-up, or performing the push-up with knees on the ground), performing the exercise on an unstable platform (such as performing push-ups on a basketball), modifying the range of motion in an exercise (such as squatting to a 45 degree angle rather than a 90 degree angle), incorporating unilateral movements as opposed to bilateral movements (such as performing a one-armed push-up), and adding isometric pauses during the exercise (such as holding for a few seconds at the bottom of a push-up).		Gymnasts make extensive use of isometrics by doing much of their training with straight arms (such as iron crosses, levers, and planches).[5] When compared to weight lifting, bodyweight exercises often require much more flexibility and balance.		Bodyweight exercises have a far lower risk of injury compared to using free weights and machines due to the absence of an external load that is placing strain on the muscles that they may or may not be able to deal with. However, the lower risk of injury is only provided that the athlete/trainee is progressing through the correct progressions and not immediately skipping to strenuous movements that can place undue and possibly harmful stress on ligaments, tendons, and other tissues. Although falling on the head, chest, buttocks, and falling backwards can occur, these are far less harmful injuries than dropping a weight on a body part, or having a joint extended beyond its natural range of motion due to a weight being used incorrectly.		Bodyweight exercises also give the advantage of having minimal bulking and cutting requirements that are normally utilised in free weight and machines training. This is due to bulking bringing extra fat that decreases the performance of bodyweight exercises, thus bodyweight exercises not only remove the need for a bulking or cutting phase, but it can help a person retain a low body fat percentage all year round.		Bodyweight exercises also work several muscle groups at once, due to the lack of isolation and the need of a large majority of muscles to perform a movement properly. For example, in a pushup, the body must form a rigid straight line, and the elbow joint must move from a straight angle to the smallest angle possible, and thus the core muscles, chest muscles, triceps, and legs are all involved in ensuring proper, strict form.		As bodyweight exercises use the individual's own weight to provide the resistance for the movement, the weight being lifted is never greater than the weight of one's own body. This can make it difficult to achieve a level of intensity that is near the individual's one rep maximum, which is desirable for strength training. Another disadvantage is that bodyweight training may be daunting to novices and seen to be too easy for experienced athletes.[1] Women, in general, also find it more difficult to do bodyweight exercises involving upper body strength and may be discouraged from undertaking these exercises in their fitness regimes.[1]		Bodyweight exercises can be increased in intensity by including additional weights (such as wearing a weighted vest or holding a barbell, kettlebell, sandbell or plate during a sit up), but this deviates from the general premise that bodyweight exercises rely solely on the weight of the individual to provide resistance.		However, difficulty can be added by changing the leverage, which places more emphasis on specific limbs and muscles, e.g. a one legged squat works a leg far stronger than a two legged squat, which not only requires strength but progressing to a one legged squat builds strength along the way. The same can be seen with one arm pushups, pull ups, and many other exercises.		Difficulty can also be added by increasing volume, adding explosiveness to the movements, or slowing down the movement to increase time under tension.		Some bodyweight exercises have been shown to benefit not just the young, but the elderly as well.[6] Older people undertaking bodyweight exercises benefit through increased muscle mass, increased mobility, increased bone density, decreased depression and improved sleep habits.[7][8] It is also believed that bodyweight training may assist in decreasing or even preventing cognitive decline as people age.[4] In addition, the increased risk of falls seen in elderly people can be mitigated by bodyweight training. Exercises focusing on the legs and abdomen such as squats, lunges and step ups are recommended to increase leg and core strength and, in doing so, reduce fall risk.[9] These bodyweight exercises are preferable to using specialised gym equipment as they provide multi-directional movement that mimics daily activities.[9]		Bodyweight exercises are generally grouped into four rough classes: Push, which requires the individual to use pushing movements to direct the body against gravity; Pull, which requires the practitioner to use pulling to direct the body; Core, which involves contracting movements of the abdominal and back muscles; and Legs/Glutes, which involve movements of the legs and glutes to direct the individual's body against gravity.		Push bodyweight exercises use a resistive or static pushing motion to work various muscle groups. Most push exercises focus on the pectoral, shoulder, and triceps muscles, but other muscle groups such as the abdominal and back muscles are leveraged to maintain good form during the push exercise.		The individual begins in a sit-up position with the hands positioned by the ears, palms down, fingers facing the legs. The individual pushes up with the arms and the back muscles until the body resembles a lowercase 'n'. The spine must be convex and the limbs straight. The difficulty can be increased by entering the bridge from a standing position and bending backwards in a controlled manner into the bridge.		From a standing position, the individual drops to a squat with hands on floor (count 1), thrusts the legs back to a pushup position (count 2), returns the legs to the squat position (count 3) and then returns to standing position (count 4). The military 8-Count Bodybuilder adds a full pushup after count 2 (count 3 and 4), and opens and closes the legs while in push-up position (count 5 and 6). The Burpee variation replaces count 4 with a plyometric squat jump before returning to the standing starting position.		The individual begins with the hands placed on two solid surfaces at or around waist height. The knees are then bent to raise the feet from the ground, and the body is lowered as far as possible using the arms, then raised again.		The individual begins with their feet on the floor, legs out straight, and hands placed on a supporting level surface between knee and waist height. Starting with straight arms with the shoulders above the hands, the body is lowered until the arms are bent at a 90 degrees angle. The body is then raised to the starting position.		The difficulty may be decreased by moving the feet closer to the body. The difficulty may be increased by raising the feet onto a stable surface. The Hanging Dip or Parallel Dip variation requires an apparatus such as a dip bar or two parallel bars (or substitutes such as tree branches or two tables) and the legs are fully raised off the ground, with the individual's bodyweight supported by the arms alone.		The individual sits with the body in an L-position, the upper body perpendicular to the ground and the legs out straight and parallel to the ground. The hands are placed beside the glutes. The hands and arms then push the entire body, including the legs, upwards off the ground with the legs remaining parallel to the ground. This exercise taxes the muscles through isometric tension.		The V-Sit variation increaess the difficulty by holding the legs higher, angled away from the ground, so the individual's body forms a 'V' shape.		The individual stands on flat surface, steps forward with one leg and bends down until the front knee is bent at a 90-degree angle. The back knee bends to almost touch the ground. The front knee should not extend past the front toes in order to maintain good form. The individual then returns to the starting position by pushing back with the front leg and stepping back so both feet are together.		The Back Lunges variation is performed from the same position, but instead the individual steps back with the leg until the front knee is bent at a 90-degree angle and the back knee is almost touching the ground. The Iron Mikes variation starts out in the bottom position of the lunge, whereby the individual performs a plyometric jump and switches leg positions so the landing position is opposite to the starting position. The Walking Lunges variation does not return the front leg to the starting position, but instead the individual steps forward with the back leg to place the feet together.		The individual starts with the feet positioned slightly apart and takes a wide step to the side with the left foot, toes pointing slightly outward. As the left foot contacts the ground, the individual shifts their weight to the left so the majority of the individual's bodyweight is supported by the left leg. The individual lowers the hips and slides the hips back until the left thigh is parallel with the ground. The back and the head are kept straight throughout the movement. The individual holds the position for a moment, then raises the body by pushing up with the left leg and moves the feet together again. The exercise is then repeated on the right side.		The difficulty may be increased by performing the Wide Side Lunge variant; the individual starts with the feet in a wide stance instead of together. The individual keeps the feet in the wide stance throughout the exercise and omits the intermediate step of moving the feet together between repetitions.		The individual places the hands and the feet on the ground, with the head facing the ground. The individual then proceeds to crawl around by striding with the arms and legs.		The individual begins in a fully extended plank or push-up position. The body is then pushed slowly forward about six to ten inches, while the arms are kept straight. The body is then returned to the starting position.		The difficulty of this exercise may be increased by bending the arms and lowering the body until it is close to the floor. The body is then slowly pushed forward and returned to the starting position. The difficulty may be further increased by extending the arms between sets to perform a push-up.		The individual begins by standing in front of an elevated surface with a ledge that will bear the weight of the individual. The body is tilted forward with the hands and arms extended and the back and legs held straight. The body is allowed to continue to fall forward and the individual catches their weight on the elevated surface with their hands in a palm-down position and arms bent. The arms are then forcefully extended to push the body back to the upright position. The waist is not bent at any time during the exercise.		The difficulty of this exercise may be increased by selecting a lower surface which decreases the leverage of the arms and moves the center of gravity forwards towards the hands.		The individual begins in a push-up position, with the body in a straight line and elbows locked. The left knee is brought to the chest and the left foot placed on the ground, with the right leg remaining outstretched. The individual then performs a small hop and switches the position of the feet so that the right knee is brought to the chest, the right foot placed on the ground and the left leg is extended behind the body. The exercise is then repeated, most commonly at a fast pace for a defined length of time.		The individual begins in a push-up position on a smooth surface. The body is propelled forward using only the arms which are never bent beyond 90 degrees. The feet are dragged behind the individual, the body held in a straight line. This exercise is best performed on a smooth floor while wearing socks or with a folder towel placed under the feet. If performed on a carpeted surface, sneakers should be worn and the toes pointed backwards while the exercise is performed.		The feet are placed on the ground just a few inches apart, with the legs held straight. The individual bends over at the waist and places their hands on the ground a few feet in front of the toes, forming an inverted 'V' with the body, the hips forming the vertex of the 'V'. The individual swings their chest and shoulders down in an arc, between the hands, so the chest nearly touches the ground. The head and shoulders are curved up in an arc as high as possible, until the back is fully arched, the head is facing forward, and the pelvis is only a few inches off the ground. The motion is then reversed, the chest and shoulders moving through the hands, close to the ground, with the arms pushing the body back to the starting point. The arms should end up straight and in line with the back.		The Half Dive Bomber variant simply stops the movement at the point the chest is between the hands and then reverses the movement to return to the starting position. The Hindu dand variant returns directly to the starting position without bending the arms or arcing the chest and shoulders back through the hands.		The difficulty of the exercise can be decreased by moving the feet further apart, or by elevating the hands on a stable surface. The difficulty can be increased by placing only a single leg on the ground at a time.		The individual starts by lying facedown on a smooth, hard floor. The legs are placed out straight with the toes on the floor, and the arms out to the sides. Two small towels are placed under the palms. With the arms and body kept straight, the palms are slid together in a controlled manner until the hands are under the shoulders. The hands are then slowly slid apart until the chest is barely touching the floor.		The individual starts by lying down on their right side with the body in a straight line. The right hand is placed on the left shoulder, and the left hand is placed palm down on the ground, under the right shoulder, fingers pointing towards the head. The left arm pushes the upper body off the ground until the arm is straight, bending at the waist to keep the lower body on the ground. The body is then lowered to the starting position. The exercise is repeated on the left side to work the right triceps.		The individual starts by sitting on the ground with the knees bent. Both feet and both palms are placed on the floor. The body is lifted off the floor and the individual walks like a crab, both forward and backward.		The individual sits on the ground in an L-position with the back perpendicular to the ground and legs out straight. The palms are placed on the ground beside the hips. The soles of the feet are placed on the ground and the pelvis is lifted off the floor until the knees are bent at a 90-degree angle and the body is straight from the head to the knees, with the face pointed straight up. The position is held for a moment and then the body is returned to the starting position.		The individual starts by lying down on the ground flat on the back, with the arms placed palm-down on the ground. The legs are lifted until they are straight in the air, perpendicular to the ground. The arms are used to push the hips off the ground as high as possible, keeping the legs perpendicular to the ground. The hips are then lowered slowly to the starting position. Lie flat on the back, arms to the side, palms on the ground.		The difficulty of the exercise can be increased by holding the hips in the top position for a few seconds before they are lowered to the ground.		The individual starts by grasping a stable, waist-level surface such as a couch, railing, table or a horizontal bar. The surface is grasped with an overhand grip, hands shoulder-width apart. The feet are placed back slightly further than a standard push up position. The body is kept straight, while the arms are bent and the body lowered until the head is below the hands. The body is then raised by pushing up with the arms until the arms are locked out straight. The elbows should be kept pointed straight down throughout the movement.		The difficulty of the exercise may be decreased by grasping a higher surface to move the center of gravity closer to the body.		The individual starts by standing and placing the arms straight out and perpendicular with the body. The hands and arms are moved in circles, first forward, then backward, for a selected number of rotations.		The targeted muscle groups of this exercise can be modified by repositioning the arm and body: making circles with the arms pointed out straight in front of the individual moves the focus to the front deltoids, while bending over and moving the arms up and down instead of in circles emphasizes the rear deltoids.		The individual begins in a push up position and performs a single push up. Then the individual will kneel and raise their hands in the air four times as if they are performing an unweighted overhead press. The individual then performs two push ups, then kneels and performs eight unweighted overhead presses. The individual will continue to ladder up in this manner, with the count of unweighted overhead presses equalling four times the number of pushups. When muscle failure is reached, the individual then ladders down with a decreasing number of push ups and a corresponding number of unweighted overhead presses.		The bodyweight Push Up is a common marker of an individual's general fitness level; for this reason it is included as one of the "big three" bodyweight exercises in the Navy SEALs' BUD/S Physical Screening Test[citation needed]. The bodyweight push-up has many distinct variations, many of which are listed below.		The individual starts by lying on the ground in the prone position. The feet are placed together and the palms are placed on the ground under the shoulders. The arms then push the body off the ground with the body is kept in a straight line. Once the arms are straight, the body is then lowered until the chest touches the ground.		The difficulty of this exercise may be decreased by elevating the hands onto a stable horizontal surface to move the center of gravity away from the arms. The arms may even be placed on a solid wall or other sturdy vertical surface to make the exercise as easy as possible.		The difficulty of this exercise may be increased by elevating the feet on to a stable horizontal surface to move the center of gravity towards the arms. As well, the exercise may be performed with the hands on an unstable surface such as a medicine ball. The exercise can be further modified by performing the push up on one leg with the other leg held in the air to put more focus on the lower lumbar region. To move the focus to the pectoral muscles, the hands may be moved further apart.		The individual starts with the hands about three feet from a wall or other solid vertical surface. The legs are placed on the wall one at a time, then the hands are 'walked' toward the wall, sliding the feet and legs up the wall until the hands are approximately a foot from the wall. The body is lowered in a controlled fashion by bending the arms, until the head nearly touches the ground between the hands.		The individual starts with their feet on the ground, heels together. The palms are then placed on the ground five hand lengths away from the toes, forming a diamond with the thumbs and the fingers. The body is bent at the hips to form a 90-degree angle between the torso and legs. The arms are bent at the elbow until the top of the head almost touches the ground between the hands. The arms are then straighted to return to the starting position. The back and legs should be kept as straight as possible throughout the exercise.		The difficulty of the exercise may be decreased by placing the hands on an elevated surface, while placing the feet on the elevated surface will cause the exercise to become more difficult.		The individual starts in a push up position, but places one hand directly under the forehead while the other hand is placed under the sternum. The arms are bent and the body lowered to the floor as in a normal pu`sh up, the elbows kept as close to the body as possible. The hands may be alternated with every repetition or with every set.		This exercise is performed just as a classic push up, but the hands are moved closer together to approximately one or two hand widths apart. As with the classic push up, the hands may be elevated to decrease the difficulty, or the feet raised to increase the difficulty.		The Military Press is performed in a similar manner to the Chinese Push Up, but the hands are placed shoulder-width apart.		The Shoulder Drop is performed in a similar manner to the Classic Push Up, but one shoulder is lowered to the ground as the opposite shoulder is raised high in the air.		Deep Push Ups are performed as a Classic Push-up, with each hand placed on a raised surface so the body can be lowered between the hands at the bottom of the movement. This modification places more emphasis on the pectorals and deltoids.		Performed like a Classic Push Up, except one hand is placed forward of the normal starting position and one hand is placed slightly behind.		Performed as a Classic Push Up, but the body is propelled upwards with a plyometric movement so the hands leave the floor for a moment. The individual then lands gently on the fingers and palms of the hand and lowers the body again to the floor.		The individual begins in a prone position, with the hands palm-down on the ground with the fingers pointed toward the feet. The arms are then extended to raise the entire body off the ground so that only the palms of the hands and the toes are touching the ground. The body is then returned to the starting position.		Performed as a Semi-Planche Push Up, but the toes are also raised off the ground and the entire body is balanced on the hands which remain stationary on the ground.		Performed in the form of a Classic Push Up, but one arm is placed behind the back, with the elbow of the other arm held tightly against the ribs. The feet are spread apart to provide balance, and the body is lowered and raised using only a single arm.		The individual begins in a prone position on the ground, the balls of the feet on the ground and the hands placed on the ground above the head, fingers splayed. The body is then raised in the air, keeping the midsection as straight as possible, until only the fingers and balls of the feet touch the ground. The body is then lowered to the starting position.		Pull bodyweight exercises use a resistive or static pulling motion to work various muscle groups.		The individual starts by grabbing a vertical object such as a pole or tree trunk, with both hands palms pronated. The body is then lifted into a horizontal position using the abdominal muscles, with the arms remaining as straight as possible.		The individual starts with an aggressive standard Pull Up with an overhand grip to chest level, at which point the wrists are rotated forward to permit the elbows and arms to swing above the bar. The arms then push the body up until the arms are straight and the waist is at the level of the bar. The motion is then reversed so the body can be lowered back to the starting position. The transition between the high pull up and the low dip is the most difficult part and emphasizes the trapezius.		The bodyweight Pull Up is another common indicator of an individual's general fitness level and is also included as one of the "big three" bodyweight exercises in the Navy Seal BUD/S Physical Screening Test[citation needed].		The individual starts by hanging from a bar with the arms extended and the palms facing away from the exerciser. The body is then pulled up using the arms until the elbows are bent and the head is higher than the hands. If the hands are moved closer, more emphasis is placed on the biceps and elbow flexors.		The individual starts by facing the outer edge of an open door that has a standard doorknob set. The feet are placed on either side of the door and the door pressed between the feet, the heels directly below the doorknob. The individual then leans back until the arms are straight and bends the knees so a 90-degree angle is formed between the thighs and back. The body is then pulled toward the door until the chest touches the edge of the door. The thighs and back should remain locked into a 90-degree angle throughout the exercise. The body is then lowered to the starting point.		The exercise can be performed with either a side grip or over-handed grip, which places emphasis on the extensors on the outside of the forearm, or an under-handed grip, which shifts the focus to the flexors on the inside of the forearms.		The difficulty can be modified by moving the feet; moving them forward increases the difficulty while moving the feet back decreases the difficulty. The exercise can also be performed with unilateral movements (one-handed) to increase the difficulty.		The Towel Grip variation works to increase grip strength. A small towel or rope is hooked around the doorknob and the individual grasps one end of the towel in each hand to perform the exercise. In lieu of a door, the same exercise can be performed with a tree trunk, railing, or any vertical stable pole.		The individual starts by lying on the ground in the supine position, and grasps a bar mounted at arm's length above the chest. The arms are bent to pull the body up to the bar, while the body remains as straight as possible from the ankles to the shoulders. The body is then lowered until the arms are straight.		The exercise may be made less difficult by moving the feet closer to the bar and bending the knees. The exercise may be increased in difficulty by raising the feet onto a raised surface. Performing the exercise with an overhand grip focuses on the extensors on the outside of the forearm, while an underhand grip changes the focus to the flexors on the inside of the forearm.		The individual starts in a standing position with the back against a wall. The ends of a bath-sized towel are grasped in each hand, and the towel is looped under the foot of one leg. The towel is pulled upwards with the arms, the elbows locked against the side of the body, while pushing down with the foot to provide resistance. The arms are then lowered slowly as the foot continues to provide resistance until the arms are at the starting position.		The difficulty of the exercise may be modified by providing more or less resistance with the foot; the exercise may be made even more difficult by performing it with one hand.		The Ledge Curl variant uses a fixed ledge between waist and chest height to provide resistance. The hands are balled into fists and placed under the ledge. The individual then bends over slowly while pressing up against the bottom of the ledge, then returns slowly to the starting position, maintaining the same level of resistance along the way.		The Isometric Curl variant uses one hand placed on the wrist of the other hand to provide resistance to the curling motion; the curling arm does not move in this case but instead benefits from the isometric tension of the exercise.		The individual places the arms in front of the body, and opens and closes the hands and fingers as tightly and as quickly as possible. This exercise is usually performed for a large number of repetitions.		Core exercises primarily involve dynamic and static contraction of the back and abdominal muscles. Core exercises can aid with improved balance and overall stability.[citation needed]		The Curl-Up, or Crunch, is another measure of a person's fitness level and is the third of the "big three" bodyweight exercises in the Navy Seal BUD/S Physical Screening Test[citation needed].		The individual starts in a supine position on the ground. The shoulders are curled towards the pelvis while the lower back remains flat against the floor. The focus is placed on contracting the abdominal muscles.		The Crunch It Up variant places the feet under a stationary object such as a low bed or couch. The arms are crossed over the stomach and the knees bent. Using the abdominal muscles, the torse is brought up just until the arms touch the thighs. The torso is then lowered to the starting position.		The V-Ups variant starts with the individual in a supine position with arms straight out on the ground and parallel to the body. The body is bent at the hips, the torso is raised off the ground and the legs brought to the chest with knees bent. The legs and torso are then lowered until they are just a few inches off the ground, but not touching it.		The Side-V variant starts with the individual on the ground, lying on one side of the body, with the arm closest to the ground stretched out perpendicular to the body. The other arm is bent and the hand placed behind the head. The torso is raised and the legs, kept straight, are raised until the legs form a 90-degree angle with the torso. The legs and torso are then lowered until they are just a few inches off the ground, but not touching it.		The Jack-Knife variant starts with the individual on the ground, legs stretched out straight and the arms on the ground extended straight up over the head. The chest and legs are simultaneously brought up until the hands touch the feet. The legs and torso are then lowered until they are just a few inches off the ground, but not touching it.		The Bicycle variant starts with the individual on the ground, the hands behind the head. The knee is pulled in toward the chest while the upper body curls up to touch the opposite elbow to the knee. The leg is then straightened and the exercise performed on the other side. The legs should be suspended off the ground during the exercise.		The individual starts in a prone position on the ground with the arms straight out in front of the body. The arms, legs and upper chest are lifted off the ground, and then slowly lowered back to the ground. This exercise is also known as "Supermans".		The Thumbs-Up variant starts in the same position, but the individual forms two fists with the thumbs pointed straight up, then lifts the head, shoulders and chest off the ground as high as possible.		The Swimmers variation raises and lowers the opposite leg and arm and alternates sides.		The Pillow Humpers variant places a towel under the hips and the feet under a stationary object like a low bed or couch. The hands are placed behind the head and the torso is raised off the ground as far as possible.		The individual starts on the ground in a prone position, with the hands at the side of the body by the hips, palm down. The body is held straight while the arms push the body off the floor until the arms are straight. The entire weight of the individual is balanced on the arms. The body is then lowered to the ground.		The individual places the toes and the forearms on the ground, with the elbows underneath the shoulders and the arm bent at a 90-degree angle. This position is maintained for as long as possible.		The Static Push Up variant simply holds the starting position of a Classic Push Up for as long as possible.		The S&M Push Up variant builds on the Static Push Up variant, but opposite legs and arms are lifted from the ground. The position is held as long as possible before switching sides.		The individual starts by sitting upright on the ground, with arms crossed and knees bent. The feet are lifted off the ground while the torso is twisted so the left elbow can touch the right knee, then twisted in the opposite direction so the right elbow can touch the left knee. The movement is repeated as long as possible.		The individual starts by standing upright, with arms raised out in front of the body. The left knee is brought up as high as possible, held up for a few moments, then lowered to the ground. The right knee is then raised as high as possible, held, then lowered to the ground.		The individual starts in a supine position on the floor, palms on the floor under the lower back or buttocks. The legs are slowly raised to a 45-degree angle with the ground, then slowly lowered to the ground.		The exercise can be increased in difficulty by raising the legs to a 90-degree angle, and not allowing the legs to return fully to the floor between repetitions.		The Flutter Kicks variation raises both legs off the ground by several inches, then alternates lifting each leg to the 45-degree position and returning it to its starting position.		The Hello Darlings variant raises both legs off the ground by several inches, then opens and closes the legs with a horizontal movement.		The Hanging Leg Lift variant starts with the individual hanging from a horizontal bar by their hands. The knees are brought slowly up to the chest and then returned to the starting position. The difficulty can be increased by keeping the legs straight as they are raised as high as possible.		The individual begins by lying on the side, one hand propping up the head, both legs kept straight. The upper leg is raised as high as possible, held in the air for a moment, then lowered to the starting position. The difficulty may be increased by propping up the body on one elbow.		The individual begins by lying on the ground, propped up on one elbow, hip and feet touching the ground. The hips are then raised until the body is in a straight line. The hips are then lowered to the starting position.		The individual begins by lying on the ground in a supine position, legs raised in the air at 90 degrees, arms stretched out the sides. The legs are then lowered to the right side by rotating the hips, then brought back to the starting position. The legs are then lowered to the left side, then returned to the starting position.		The individual begins in a supine position on a raised surface, with the head and neck extending off the edge. The head is then moved up and down in a "yes" fashion. The head is then turned from side to side in a "no" fashion. Finally, the head is moved from side to side, bringing each ear to the nearest shoulder in a "maybe" fashion. The exercise may also be performed in a prone position, with the hands placed on the back of the head to provide extra resistance.		Bodyweight exercises that work the thigh, calf and glute muscles are generally performed in the upright, seated, and all-fours positions. Increasing the difficulty of exercises in this class is usually accomplished through unilateral modifications (performed on one leg) or providing additional weight over and above the individual's own bodyweight.		The individual starts with both feet on the edge of a raised surface, with the toes on the surface and the heels lower than the toes. The heels are raised as high as possible, then returned to the starting position.		The difficulty may be increased by performing the exercise on one leg.		The Cliffhanger variant requires one foot only to be placed on the surface and the position held as long as possible in isometric tension.		The Donkey Calf Raises variant requires that the individual bend at the waist to about 90 degrees and rest the arms on a chair or other stable surface.		The Little Piggies variant is performed by placing the heels on the surface, and moves the toes instead.		The individual starts in a standing position with feet shoulder width apart. The legs are bent at the nees and hips, and the torso is lowered between the legs. The knees should remain behind the toes at all times. The body is then raised to the starting position.		The Invisible Chair variant is performed with the back against the wall, knees bent at 90 degrees, and the body is held in this position for as long as possible.		The Wall Squat variant is performed with the back against the wall and the feet one step forward from the wall. The back slides down the wall as the knees are bent to a 90-degree angle.		The Sumo Squat variant is performed with a wide stance, and the body is lowered until the thighs are parallel to the ground.		The One-Legged Squat is performed with one leg held out straight in front of the body while the other leg bears the full weight of the individual during the squat.		The Pistol Squat variant builds on the One-Legged Squat and brings the buttocks all the way down to the heel of the foot on the ground. This variety of squats is made to challenge your balance.		The Bulgarian Split Squat. Put the rear leg on a bench, drop straight down, and make sure that the front heel always stays in contact with the ground to avoid any excess stress on the knees. Retain a tall posture throughout the whole exercise. These can work the abs, quads and glutes, as well as the ability to stabilize. Moreover, 3 sets of 6-10 reps do the job to satisfaction.		The Sissy Squat variant uses a pole or other support to hold with one hand, while the body leans backward through the squat until the buttocks are resting on the heels.		The individual starts in a standing position, hands behind the head. The body is bent at the waist and the back is kept straight until the legs and torso form a 90-degree angle. The torso is returned slowly to the starting position.		The individual starts in an all-fours position, then lifts one knee off the ground and swings the knee out to the side as far as possible, maintaining the bent knee at a 90-degree angle. The leg is then returned to the starting position and the exercise is then performed with the other leg.		The Mule Kick variant is performed by straightening the leg as it is lifted away from the body as high as possible.		The individual stands with their feet hip-width apart. The leg is lifted to the side in a slow, controlled manner until it forms a 45-degree angle with the stationary leg. The leg is then returned to the starting position and the exercise performed on the other side. One hand may be rested on a chair or other stable surface for support.		The individual starts with the feet shoulder-width apart. The leg is lifted from the ground, with the knee bent, and the foot curled in toward the buttocks. The leg is returned to the starting position and the exercise performed on the other side. One or two hands may be rested on a chair or other stable surface for support.		The individual starts in a standing position with the feet together. Bending at the waist, one leg is raised in the air while the hand reaches for the floor. The leg is lowered to the starting position and the body returned to the upright position. The leg and back should stay straight at all times during the exercise.		The individual starts with the back resting on the ground, and the legs bent at 90 degrees with the feet resting on an elevated surface such as a chair. Using only the legs, the hips are pushed up as high as possible, held in contraction for a moment, then lowered to the starting position.		The individual stands on one leg, body held vertically, closes the eyes, then holds the position for as long as possible. The difficulty may be increased by performing the exercise on a soft or unstable surface.		The individual lies in a prone position on a raised, horizontal surface so the legs may project freely beyond the edge of the surface and the toes rest on the ground. The legs are then spread as wide as possible, then raised slowly and brought together until the heels touch. The feet are then returned to the ground. The legs are held as straight as possible throughout the exericse.		The individual kneels on the ground, with the feet anchored under a solid surface, or held to the ground by another person. The body is then lowered until the chest is touching the ground. The individual then uses a plyometric movement with the arms to return to the starting position.		The feet are placed together on the ground and the individual bends at the waist to grab the ankles, with the legs kept straight. The knees are then bent until the buttocks touch the ankles. The body is then returned to the starting position.		The arabesque is a technique that is borrowed from the ballet moves. It works excellently for the butt muscles, and does not even make the use of free weights. However, if you want to add cuffs or ankle weights, you need to follow the following procedure. Place your hands on the back of the chair or on a railing, and lift one leg behind you as high as possible, while holding your glutes and squeezing them for a count of about 4 or 5. Make sure to maintain an upright position so that you do not stress your lower back instead of the glutes.		Duck walks are really good exercises to help shape your butt. The procedure to do this exercise is to assume and hold a squatting position while walking forward for the repetitions and then walk backwards in the same positions for the repetitions. This position might not be very “diva” looking, but is highly effective all the same.		[10]		|group2 = See also |list2 =		|below = }}		
Exercise physiology is the physiology of physical exercise. It is the study of the acute responses and chronic adaptations to a wide range of exercise conditions.		Exercise physiologists study the effect of exercise on pathology, and the mechanisms by which exercise can reduce or reverse disease progression.						Humans have a high capacity to expend energy for many hours during sustained exertion. For example, one individual cycling at a speed of 26.4 km/h (16.4 mph) through 8,204 km (5,098 mi) over 50 consecutive days expended a total of 1,145 MJ (273,850 kcal; 273,850 dieter calories) with an average power output of 182.5 W.[1]		Skeletal muscle burns 90 mg (0.5 mmol) of glucose each minute during continuous activity (such as when repetitively extending the human knee),[2] generating ≈24 W of mechanical energy, and since muscle energy conversion is only 22–26% efficient,[3] ≈76 W of heat energy. Resting skeletal muscle has a basal metabolic rate (resting energy consumption) of 0.63 W/kg[4] making a 160 fold difference between the energy consumption of inactive and active muscles. For short duration muscular exertion, energy expenditure can be far greater: an adult human male when jumping up from a squat can mechanically generate 314 W/kg. Such rapid movement can generate twice this amount in nonhuman animals such as bonobos,[5] and in some small lizards.[6]		This energy expenditure is very large compared to the basal resting metabolic rate of the adult human body. This rate varies somewhat with size, gender and age but is typically between 45 W and 85 W.[7] [8] Total energy expenditure (TEE) due to muscular expended energy is much higher and depends upon the average level of physical work and exercise done during a day.[9] Thus exercise, particularly if sustained for very long periods, dominates the energy metabolism of the body. Physical activity energy expenditure correlates strongly with the gender, age, weight, heart rate, and VO2 max of an individual, during physical activity.[10]		Energy needed to perform short lasting, high intensity bursts of activity is derived from anaerobic metabolism within the cytosol of muscle cells, as opposed to aerobic respiration which utilizes oxygen, is sustainable, and occurs in the mitochondria. The quick energy sources consist of the phosphocreatine (PCr) system, fast glycolysis, and adenylate kinase. All of these systems re-synthesize adenosine triphosphate (ATP), which is the universal energy source in all cells. The most rapid source, but the most readily depleted of the above sources is the PCr system which utilizes the enzyme creatine kinase. This enzyme catalyzes a reaction that combines phosphocreatine and adenosine diphosphate (ADP) into ATP and creatine. This resource is short lasting because oxygen is required for the resynthesis of phosphocreatine via mitochondrial creatine kinase. Therefore, under anaerobic conditions, this substrate is finite and only lasts between approximately 10 to 30 seconds of high intensity work. Fast glycolysis, however, can function for approximately 2 minutes prior to fatigue, and predominately uses intracellular glycogen as a substrate. Glycogen is broken down rapidly via glycogen phosphorylase into individual glucose units during intense exercise. Glucose is then oxidized to pyruvate and under anaerobic condition is reduced to lactic acid. This reaction oxidizes NADH to NAD, thereby releasing a hydrogen ion, promoting acidosis. For this reason, fast glycolysis can not be sustained for long periods of time. Lastly, adenylate kinase catalyzes a reaction by which 2 ADP are combined to form ATP and adenosine monophosphate (AMP). This reaction takes place during low energy situations such as extreme exercise or conditions of hypoxia, but is not a significant source of energy. The creation of AMP resulting from this reaction stimulates AMP-activated protein kinase (AMP kinase) which is the energy sensor of the cell. After sensing low energy conditions, AMP kinase stimulates various other intracellular enzymes geared towards increasing energy supply and decreasing all anabolic, or energy requiring, cell functions.[citation needed]		Plasma glucose is said to be maintained when there is an equal rate of glucose appearance (entry into the blood) and glucose disposal (removal from the blood). In the healthy individual, the rates of appearance and disposal are essentially equal during exercise of moderate intensity and duration; however, prolonged exercise or sufficiently intense exercise can result in an imbalance leaning towards a higher rate of disposal than appearance, at which point glucose levels fall producing the onset of fatigue. Rate of glucose appearance is dictated by the amount of glucose being absorbed at the gut as well as liver (hepatic) glucose output. Although glucose absorption from the gut is not typically a source of glucose appearance during exercise, the liver is capable of catabolizing stored glycogen (glycogenolysis) as well as synthesizing new glucose from specific reduced carbon molecules (glycerol, pyruvate, and lactate) in a process called gluconeogenesis. The ability of the liver to release glucose into the blood from glycogenolysis is unique, since skeletal muscle, the other major glycogen reservoir, is incapable of doing so. Unlike skeletal muscle, liver cells contain the enzyme glycogen phosphatase, which removes a phosphate group from glucose-6-P to release free glucose. In order for glucose to exit a cell membrane, the removal of this phosphate group is essential. Although gluconeogenesis is an important component of hepatic glucose output, it alone can not sustain exercise. For this reason, when glycogen stores are depleted during exercise, glucose levels fall and fatigue sets in. Glucose disposal, the other side of the equation, is controlled by uptake of glucose at the working skeletal muscles. During exercise, despite decreased insulin concentrations, muscle increases GLUT4 translocation of and glucose uptake. The mechanism for increased GLUT4 translocation is an area of ongoing research.		glucose control: As mentioned above, insulin secretion is reduced during exercise, and does not play a major role in maintaining normal blood glucose concentration during exercise, but its counter-regulatory hormones appear in increasing concentrations. Principle among these are glucagon, epinephrine, and growth hormone. All of these hormones stimulate liver (hepatic) glucose output, among other functions. For instance, both epinephrine and growth hormone also stimulate adipocyte lipase, which increases non-esterified fatty acid (NEFA) release. By oxidizing fatty acids, this spares glucose utilization and helps to maintain blood sugar level during exercise.		Exercise for diabetes: Exercise is a particularly potent tool for glucose control in those who have diabetes mellitus. In a situation of elevated blood glucose (hyperglycemia), moderate exercise can induce greater glucose disposal than appearance, thereby decreasing total plasma glucose concentrations. As stated above, the mechanism for this glucose disposal is independent of insulin, which makes it particularly well-suited for people with diabetes. In addition, there appears to be an increase in sensitivity to insulin for approximately 12–24 hours post-exercise. This is particularly useful for those who have type II diabetes and are producing sufficient insulin but demonstrate peripheral resistance to insulin signaling. However, during extreme hyperglycemic episodes, people with diabetes should avoid exercise due to potential complications associated with ketoacidosis. Exercise could exacerbate ketoacidosis by increasing ketone synthesis in response to increased circulating NEFA's.		Type II diabetes is also intricately linked to obesity, and there may be a connection between type II diabetes and how fat is stored within pancreatic, muscle, and liver cells. Likely due to this connection, weight loss from both exercise and diet tends to increase insulin sensitivity in the majority of people. In some people, this effect can be particularly potent and can result in normal glucose control. Although nobody is technically cured of diabetes, individuals can live normal lives without the fear of diabetic complications; however, regain of weight would assuredly result in diabetes signs and symptoms.		Vigorous physical activity (such as exercise or hard labor) increases the body's demand for oxygen. The first-line physiologic response to this demand is an increase in heart rate, breathing rate, and depth of breathing.		Oxygen consumption (VO2) during exercise is best described by the Fick Equation: VO2=Q x (a-vO2diff), which states that the amount of oxygen consumed is equal to cardiac output (Q) multiplied by the difference between arterial and venous oxygen concentrations. More simply put, oxygen consumption is dictated by the quantity of blood distributed by the heart as well as the working muscle's ability to take up the oxygen within that blood; however, this is a bit of an oversimplification. Although cardiac output is thought to be the limiting factor of this relationship in healthy individuals, it is not the only determinant of VO2 max. That is, factors such as the ability of the lung to oxygenate the blood must also be considered. Various pathologies and anomalies cause conditions such as diffusion limitation, ventilation/perfusion mismatch, and pulmonary shunts that can limit oxygenation of the blood and therefore oxygen distribution. In addition, the oxygen carrying capacity of the blood is also an important determinant of the equation. Oxygen carrying capacity is often the target of exercise (ergogenic aids) aids used in endurance sports to increase the volume percentage of red blood cells (hematocrit), such as through blood doping or the use of erythropoietin (EPO). Furthermore, peripheral oxygen uptake is reliant on a rerouting of blood flow from relatively inactive viscera to the working skeletal muscles, and within the skeletal muscle, capillary to muscle fiber ratio influences oxygen extraction.		Dehydration refers both to hypohydration (dehydration induced prior to exercise) and to exercise-induced dehydration (dehydration that develops during exercise). The latter reduces aerobic endurance performance and results in increased body temperature, heart rate, perceived exertion, and possibly increased reliance on carbohydrate as a fuel source. Although the negative effects of exercise-induced dehydration on exercise performance were clearly demonstrated in the 1940s, athletes continued to believe for years thereafter that fluid intake was not beneficial. More recently, negative effects on performance have been demonstrated with modest (<2%) dehydration, and these effects are exacerbated when the exercise is performed in a hot environment. The effects of hypohydration may vary, depending on whether it is induced through diuretics or sauna exposure, which substantially reduce plasma volume, or prior exercise, which has much less impact on plasma volume. Hypohydration reduces aerobic endurance, but its effects on muscle strength and endurance are not consistent and require further study.[11] Intense prolonged exercise produces metabolic waste heat, and this is removed by sweat-based thermoregulation. A male marathon runner loses each hour around 0.83 L in cool weather and 1.2 L in warm (losses in females are about 68 to 73% lower).[12] People doing heavy exercise may lose two and half times as much fluid in sweat as urine.[13] This can have profound physiological effects. Cycling for 2 hours in the heat (35 °C) with minimal fluid intake causes body mass decline by 3 to 5%, blood volume likewise by 3 to 6%, body temperature to rise constantly, and in comparison with proper fluid intake, higher heart rates, lower stroke volumes and cardiac outputs, reduced skin blood flow, and higher systemic vascular resistance. These effects are largely eliminated by replacing 50 to 80% of the fluid lost in sweat.[12][14]		At rest, the human brain receives 15% of total cardiac output, and uses 20% of the body's energy consumption.[21] The brain is normally dependent for its high energy expenditure upon aerobic metabolism. The brain as a result is highly sensitive to failure of its oxygen supply with loss of consciousness occurring within six to seven seconds,[22] with its EEG going flat in 23 seconds.[23] If it affected the oxygen and glucose supply to the brain, the metabolic demands of exercise could therefore quickly disrupt its functioning.		Protecting the brain from even minor disruption is important since exercise depends upon motor control, and particularly, because humans are bipeds, the motor control needed for keeping balance. Indeed, for this reason, brain energy consumption is increased during intense physical exercise due to the demands in the motor cognition needed to control the body.[24]		Cerebral autoregulation usually ensures the brain has priority to cardiac output, though this is impaired slightly by exhaustive exercise.[25] During submaximal exercise, cardiac output increases and cerebral blood flow increases beyond the brain’s oxygen needs.[26] However, this is not the case for continuous maximal exertion: "Maximal exercise is, despite the increase in capillary oxygenation [in the brain], associated with a reduced mitochondrial O2 content during whole body exercise"[27] The autoregulation of the brain’s blood supply is impaired particularly in warm environments[28]		In adults, exercise depletes the plasma glucose available to the brain: short intense exercise (35 min ergometer cycling) can reduce brain glucose uptake by 32%.[29]		At rest, energy for the adult brain is normally provided by glucose but the brain has a compensatory capacity to replace some of this with lactate. Research suggests that this can be raised, when a person rests in a brain scanner, to about 17%,[30] with a higher percentage of 25% occurring during hypoglycemia.[31] During intense exercise, lactate has been estimated to provide a third of the brain’s energy needs.[29][32] There is evidence that the brain might, however, in spite of these alternative sources of energy, still suffer an energy crisis since IL-6 (a sign of metabolic stress) is released during exercise from the brain.[16][24]		Humans use sweat thermoregulation for body heat removal, particularly to remove the heat produced during exercise. Moderate dehydration as a consequence of exercise and heat is reported to impair cognition.[33][34] These impairments can start after body mass lost that is greater than 1%.[35] Cognitive impairment, particularly due to heat and exercise is likely to be due to loss of integrity to the blood brain barrier.[36] Hyperthermia also can lower cerebral blood flow,[37][38] and raise brain temperature.[24]		Researchers once attributed fatigue to a build-up of lactic acid in muscles.[39] However, this is no longer believed.[40][41] Rather, lactate may stop muscle fatigue by keeping muscles fully responding to nerve signals.[42] The available oxygen and energy supply, and disturbances of muscle ion homeostasis are the main factor determining exercise performance, at least during brief very intense exercise.		Each muscle contraction involves an action potential that activates voltage sensors, and so releases Ca2+ ions from the muscle fibre’s sarcoplasmic reticulum. The action potentials that cause this require also ion changes: Na influxes during the depolarization phase and K effluxes for the repolarization phase. Cl− ions also diffuse into the sarcoplasm to aid the repolarization phase. During intense muscle contraction, the ion pumps that maintain homeostasis of these ions are inactivated and this (with other ion related disruption) causes ionic disturbances. This causes cellular membrane depolarization, inexcitability, and so muscle weakness.[43] Ca2+ leakage from type 1 ryanodine receptor) channels has also been identified with fatigue.[44]		After intense prolonged exercise, there can be a collapse in body homeostasis. Some famous examples include:		Tim Noakes, based on an earlier idea by the 1922 Nobel Prize in Physiology or Medicine winner Archibald Hill[46] has proposed the existence of a central governor. In this, the brain continuously adjusts the power output by muscles during exercise in regard to a safe level of exertion. These neural calculations factor in prior length of strenuous exercise, the planned duration of further exertion, and the present metabolic state of the body. This adjusts the number of activated skeletal muscle motor units, and is subjectively experienced as fatigue and exhaustion. The idea of a central governor rejects the earlier idea that fatigue is only caused by mechanical failure of the exercising muscles ("peripheral fatigue"). Instead, the brain models[47] the metabolic limits of the body to ensure that whole body homeostasis is protected, in particular that the heart is guarded from hypoxia, and an emergency reserve is always maintained.[48][49][50][51] The idea of the central governor has been questioned since ‘physiological catastrophes’ can and do occur suggesting athletes (such as Dorando Pietri, Jim Peters and Gabriela Andersen-Schiess) can over-ride the ‘‘central governor’.[52]		The exercise fatigue has also been suggested to be effected by:		Prolonged exercise such as marathons can increase cardiac biomarkers such as troponin, B-type natriuretic peptide (BNP), and ischemia-modified (aka MI) albumin. This can be misinterpreted by medical personnel as signs of myocardial infarction, or cardiac dysfunction. In these clinical conditions, such cardiac biomarkers are produced by irreversible injury of muscles. In contrast, the processes that create them after strenuous exertion in endurance sports are reversible, with their levels returning to normal within 24-hours (further research, however, is still needed).[60][61][62]		Humans are specifically adapted to engage in prolonged strenuous muscular activity (such as efficient long distance bipedal running).[63] This capacity for endurance running evolved to allow the running down of game animals by persistent slow but constant chase over many hours.[64]		Central to the success of this is the ability of the human body, unlike that of the animals they hunt, to effectively remove muscle heat waste. In most animals, this is stored by allowing a temporary increase in body temperature. This allows them to escape from animals that quickly speed after them for a short duration (the way nearly all predators catch their prey). Humans, unlike other animals that catch prey, remove heat with a specialized thermoregulation based on sweat evaporation. One gram of sweat can remove 2,598 J of heat energy.[65] Another mechanism is increased skin blood flow during exercise that allows for greater convective heat loss that is aided by our upright posture. This skin based cooling has resulted in humans acquiring an increased number of sweat glands, combined with a lack of body fur that would otherwise stop air circulation and efficient evaporation.[66] Because humans can remove exercise heat, they can avoid the fatigue from heat exhaustion that affects animals chased in a persistent manner, and so eventually catch them.[67]		Rodents have been specifically bred for exercise behavior or performance in several different studies.[68] For example, laboratory rats have been bred for high or low performance on a motorized treadmill with electrical stimulation as motivation.[69] The high-performance line of rats also exhibits increased voluntary wheel-running behavior as compared with the low-capacity line.[70] In an experimental evolution approach, four replicate lines of laboratory mice have been bred for high levels of voluntary exercise on wheels, while four additional control lines are maintained by breeding without regard to the amount of wheel running.[71] These selected lines of mice also show increased endurance capacity in tests of forced endurance capacity on a motorized treadmill.[72] However, in neither selection experiment have the precise causes of fatigue during either forced or voluntary exercise been determined.		Physical exercise may cause pain both as an immediate effect that may result from stimulation of free nerve endings by low pH, as well as a delayed onset muscle soreness. The delayed soreness is fundamentally the result of ruptures within the muscle, although apparently not involving the rupture of whole muscle fibers.[73]		Muscle pain can range from a mild soreness to a debilitating injury depending on intensity of exercise, level of training, and other factors.[74]		Accreditation programs exist with professional bodies in most developed countries, ensuring the quality and consistency of education. In Canada, one may obtain the professional certification title – Certified Exercise Physiologist for those working with clients (both clinical and non clinical) in the health and fitness industry. An exercise physiologist's area of study may include but is not limited to biochemistry, bioenergetics, cardiopulmonary function, hematology, biomechanics, skeletal muscle physiology, neuroendocrine function, and central and peripheral nervous system function. Furthermore, exercise physiologists range from basic scientists, to clinical researchers, to clinicians, to sports trainers.		Colleges and universities offer exercise physiology as a program of study on various different levels, including undergraduate, graduate, and doctoral programs. The basis of Exercise Physiology as a major is to prepare students for a career in field of health sciences. A program that focuses on the scientific study of the physiological processes involved in physical or motor activity, including sensorimotor interactions, response mechanisms, and the effects of injury, disease, and disability. Includes instruction in muscular and skeletal anatomy; molecular and cellular basis of muscle contraction; fuel utilization; neurophysiology of motor mechanics; systemic physiological responses (respiration, blood flow, endocrine secretions, and others); fatigue and exhaustion; muscle and body training; physiology of specific exercises and activities; physiology of injury; and the effects of disabilities and disease. Careers available with a degree in Exercise Physiology can include: non-clinical, client-based work; strength and conditioning specialists; cardiopulmonary treatment; and clinical-based research.[75]		In order to gauge the multiple areas of study, students are taught processes in which to follow on a client-based level. Practical and lecture teachings are instructed in the classroom and in a laboratory setting. These include:		The curriculum for exercise physiology includes biology, chemistry, and applied sciences. The purpose of the classes selected for this major is to have a proficient understanding of human anatomy, human physiology, and exercise physiology. Includes instruction in muscular and skeletal anatomy; molecular and cellular basis of muscle contraction; fuel utilization; neurophysiology of motor mechanics; systemic physiological responses (respiration, blood flow, endocrine secretions, and others); fatigue and exhaustion; muscle and body training; physiology of specific exercises and activities; physiology of injury; and the effects of disabilities and disease. Not only is a full class schedule needed to complete a degree in Exercise Physiology, but a minimum amount of practicum experience is required and internships are recommended.[77]		|group2 = See also |list2 =		|below = }}		
A myokine is one of several hundred cytokines or other small proteins (~5–20 kDa) and proteoglycan peptides that are produced and released by muscle cells (myocytes) in response to muscular contractions.[1] They have autocrine, paracrine and/or endocrine effects; their systemic effects occur at picomolar concentrations.[2][3]		Receptors for myokines are found on muscle, fat, liver, pancreas, bone, heart, immune, and brain cells. The location of these receptors explain the fact that myokines have multiple functions. Foremost, they are involved in exercise-associated metabolic changes, as well as in the metabolic changes following training adaptation.[1] They also participate in tissue regeneration and repair, maintenance of healthy bodily functioning, immunomodulation; and cell signaling, expression and differentiation.[1]						The present definition of the term myokine is attributed to Dr. Bente Klarlund Pedersen et al., who suggested its use in 2003.[4] In 2008, the first myokine, myostatin, was identified.[3][5] The gp130 receptor cytokine IL-6 (Interleukin 6) was the first myokine found to be secreted into the blood stream in response to muscle contractions.[6]		There is an emerging understanding of skeletal muscle as a secretory organ, and of myokines as mediators of physical fitness through the practice of regular physical exercise (aerobic exercise and strength training), as well as new awareness of the anti-inflammatory and thus disease prevention aspects of exercise. Different muscle fiber types -slow twitch muscle fibers, oxidative muscle fibers, intermediate twitch muscle fibers, and fast twitch muscle fibers - release different clusters of myokines during contraction. This implies that variation of exercise types, particularly aerobic training/endurance training and muscle contraction against resistance (strength training) may offer differing myokine-induced benefits. This topic has been discussed by fitness training specialists.[7]		"Some myokines exert their effects within the muscle itself. Thus, myostatin, LIF, IL-6 and IL-7 are involved in muscle hypertrophy and myogenesis, whereas BDNF and IL-6 are involved in AMPK-mediated fat oxidation. IL-6 also appears to have systemic effects on the liver, adipose tissue and the immune system, and mediates crosstalk between intestinal L cells and pancreatic islets. Other myokines include the osteogenic factors IGF-1 and FGF-2; FSTL-1, which improves the endothelial function of the vascular system; and the PGC-1alpha-dependent myokine irisin, which drives brown fat-like development. Studies in the past few years suggest the existence of yet unidentified factors, secreted from muscle cells, which may influence cancer cell growth and pancreas function. Many proteins produced by skeletal muscle are dependent upon contraction; therefore, physical inactivity probably leads to an altered myokine response, which could provide a potential mechanism for the association between sedentary behaviour and many chronic diseases."[2]		"In summary, physical inactivity and muscle disuse lead to loss of muscle mass and accumulation of visceral adipose tissue and consequently to the activation of a network of inflammatory pathways, which promote development of insulin resistance, atherosclerosis, neurodegeneration and tumour growth and, thereby, promote the development of a cluster of chronic diseases. By contrast, the finding that muscles produce and release myokines provides a molecular basis for understanding how physical activity could protect against premature mortality.... Given that muscle is the largest organ in the body, the identification of the muscle secretome could set a new agenda for the scientific community. To view skeletal muscle as a secretory organ provides a conceptual basis for understanding how muscles communicate with other organs such as adipose tissue, liver, pancreas, bone and brain. Physical inactivity or muscle disuse potentially leads to an altered or impaired myokine response and/or resistance to the effects of myokines, which explains why lack of physical activity increases the risk of a whole network of diseases, including cardiovascular diseases, T2DM (Type 2 Diabetes Mellitus), cancer and osteoporosis."[2]		Heart muscle is subject to two kinds of stress: physiologic stress, i.e. exercise; and pathologic stress, i.e. disease related. Likewise, the heart has two potential responses to either stress: cardiac hypertrophy, which is a normal, physiologic, adaptive growth; or cardiac remodeling, which is an abnormal, pathologic, maladaptive growth. Upon being subjected to either stress, the heart "chooses" to turn on one of the responses and turn off the other. If it has chosen the abnormal path,i.e. remodeling, exercise can reverse this choice by turning off remodeling and turning on hypertrophy. The mechanism for reversing this choice is the microRNA miR-222 in cardiac muscle cells, which exercise up-regulates via unknown myokines. miR-222 represses genes involved in fibrosis and cell-cycle control.[8]		Immunomodulation and immunoregulation were a particular focus of early myokine research, as, according to Dr. Bente Klarlund Pedersen and her colleagues, "the interactions between exercise and the immune system provided a unique opportunity to evaluate the role of underlying endocrine and cytokine mechanisms."[1]		Both aerobic exercise and strength training (resistance exercise) attenuate myostatin expression, and myostatin inactivation potentiates the beneficial effects of endurance exercise on metabolism.[9]		Aerobic exercise provokes a systemic cytokine response, including, for example, IL-6, IL-1 receptor antagonist (IL-1ra), and IL-10 (Interleukin 10). IL-6 was serendipitously discovered as a myokine because of the observation that it increased in an exponential fashion proportional to the length of exercise and the amount of muscle mass engaged in the exercise. This increase is followed by the appearance of IL-1ra and the anti-inflammatory cytokine IL-10. In general, the cytokine response to exercise and sepsis differs with regard to TNF-α. Thus, the cytokine response to exercise is not preceded by an increase in plasma-TNF-α. Following exercise, the basal plasma IL-6 concentration may increase up to 100-fold, but less dramatic increases are more frequent. The exercise-induced increase of plasma IL-6 occurs in an exponential manner and the peak IL-6 level is reached at the end of the exercise or shortly thereafter. It is the combination of mode, intensity, and duration of the exercise that determines the magnitude of the exercise-induced increase of plasma IL-6.[6]		IL-6 had previously been classified as a proinflammatory cytokine. Therefore, it was first thought that the exercise-induced IL-6 response was related to muscle damage.[10] However, it has become evident that eccentric exercise is not associated with a larger increase in plasma IL-6 than exercise involving concentric “nondamaging” muscle contractions. This finding clearly demonstrates that muscle damage is not required to provoke an increase in plasma IL-6 during exercise. As a matter of fact, eccentric exercise may result in a delayed peak and a much slower decrease of plasma IL-6 during recovery.[3]		IL-6, among an increasing number of other recently identified myokines, thus remains an important topic of myokine research. It appears in muscle tissue and in the circulation during exercise at levels up to one hundred times basal rates, as noted, and is seen as having a beneficial impact on health and bodily functioning in most circumstances. P. Munoz-Canoves et al. write: "It appears consistently in the literature that IL-6, produced locally by different cell types, has a positive impact on the proliferative capacity of muscle stem cells. This physiological mechanism functions to provide enough muscle progenitors in situations that require a high number of these cells, such as during the processes of muscle regeneration and hypertrophic growth after an acute stimulus. IL-6 is also the founding member of the myokine family of muscle-produced cytokines. Indeed, muscle-produced IL-6 after repeated contractions also has important autocrine and paracrine benefits, acting as a myokine, in regulating energy metabolism, controlling, for example, metabolic functions and stimulating glucose production. It is important to note that these positive effects of IL-6 and other myokines are normally associated with its transient production and short-term action."[11]		Interleukin-15 may play a significant role in the reduction of the volume of visceral (intra-abdominal) fat. IL-15 may accumulate within the muscle as a consequence of regular training. There is a negative association between plasma IL-15 concentration and trunk fat mass, but not limb fat mass.[12]		Brain-derived neurotrophic factor (BDNF) is also a myokine, though BDNF produced by contracting muscle is not released into circulation. Rather, BDNF produced in skeletal muscle appears to enhance the oxidation of fat. Skeletal muscle activation through exercise also contributes to an increase in BDNF secretion in the brain. A beneficial effect of BDNF on neuronal function has been noted in multiple studies.[12][13] Dr. Pedersen writes, "Neurotrophins are a family of structurally related growth factors, including brain-derived neurotrophic factor (BDNF), which exert many of their effects on neurons primarily through Trk receptor tyrosine kinases. Of these, BDNF and its receptor TrkB are most widely and abundantly expressed in the brain. However, recent studies show that BDNF is also expressed in non-neurogenic tissues, including skeletal muscle. BDNF has been shown to regulate neuronal development and to modulate synaptic plasticity. BDNF plays a key role in regulating survival, growth and maintenance of neurons, and BDNF has a bearing on learning and memory. However, BDNF has also been identified as a key component of the hypothalamic pathway that controls body mass and energy homeostasis.		"Most recently, we have shown that BDNF appears to be a major player not only in central metabolic pathways but also as a regulator of metabolism in skeletal muscle. Hippocampal samples from Alzheimer’s disease donors show decreased BDNF expression and individuals with Alzheimer’s disease have low plasma levels of BDNF. Also, patients with major depression have lower levels of serum BDNF than normal control subjects. Other studies suggest that plasma BDNF is a biomarker of impaired memory and general cognitive function in ageing women and a low circulating BDNF level was recently shown to be an independent and robust biomarker of mortality risk in old women. Interestingly, low levels of circulating BDNF are also found in obese individuals and those with type 2 diabetes. In addition, we have demonstrated that there is a cerebral output of BDNF and that this is inhibited during hyperglycaemic clamp conditions in humans. This last finding may explain the concomitant finding of low circulating levels of BDNF in individuals with type 2 diabetes, and the association between low plasma BDNF and the severity of insulin resistance.		"BDNF appears to play a role in both neurobiology and metabolism. Studies have demonstrated that physical exercise may increase circulating BDNF levels in humans. To identify whether the brain is a source of BDNF during exercise, eight volunteers rowed for 4 h while simultaneous blood samples were obtained from the radial artery and the internal jugular vein. To further identify the putative cerebral region(s) responsible for BDNF release, mouse brains were dissected and analysed for BDNF mRNA expression following treadmill exercise. In humans, a BDNF release from the brain was observed at rest and increased 2- to 3-fold during exercise. Both at rest and during exercise, the brain contributed 70–80% of the circulating BDNF, while this contribution decreased following 1 h of recovery. In mice, exercise induced a 3- to 5-fold increase in BDNF mRNA expression in the hippocampus and cortex, peaking 2 h after the termination of exercise. These results suggest that the brain is a major but not the sole contributor to circulating BDNF. Moreover, the importance of the cortex and hippocampus as sources of plasma BDNF becomes even more prominent in the response to exercise.”[12]		With respect to studies of exercise and brain function, a 2010 report is of particular interest. Erickson et al. have shown that the volume of the anterior hippocampus increased by 2% in response to aerobic training in a randomized controlled trial with 120 older adults. The authors also summarize several previously-established research findings relating to exercise and brain function: (1) Aerobic exercise training increases grey and white matter volume in the prefrontal cortex of older adults and increases the functioning of key nodes in the executive control network. (2) Greater amounts of physical activity have been associated with sparing of prefrontal and temporal brain regions over a 9-y period, which reduces the risk for cognitive impairment. (3) Hippocampal and medial temporal lobe volumes are larger in higher-fit older adults (larger hippocampal volumes have been demonstrated to mediate improvements in spatial memory). (4) Exercise training increases cerebral blood volume and perfusion of the hippocampus.[13]		Regarding the 2010 study, the authors conclude: "We also demonstrate that increased hippocampal volume is associated with greater serum levels of BDNF, a mediator of neurogenesis in the dentate gyrus. Hippocampal volume declined in the control group, but higher preintervention fitness partially attenuated the decline, suggesting that fitness protects against volume loss. Caudate nucleus and thalamus volumes were unaffected by the intervention. These theoretically important findings indicate that aerobic exercise training is effective at reversing hippocampal volume loss in late adulthood, which is accompanied by improved memory function."[13]		Seldin, Peterson, Byerly, Wei and Wong clarify that most myokines are also secreted by non-muscle cells. In 2012, they reported: "The expression of all myokines described to date is not restricted to skeletal muscle; they are generally expressed by a variety of cell types, and most are, in fact, expressed at much higher levels by nonmuscle tissues. Prior to (our) study, no myokine has been discovered to be preferentially expressed by skeletal muscle. While characterizing the metabolic function of the C1q/TNF-related protein (CTRP) family of proteins we recently uncovered, we identified myonectin (CTRP15) as a novel member of the family on the basis of sequence homology in the shared C1q domain, the signature that defines this protein family.... (Myonectin) is a novel nutrient-responsive myokine secreted by skeletal muscle to regulate whole-body fatty acid metabolism.... Circulating levels of myonectin were tightly regulated by the metabolic state; fasting suppressed, but refeeding dramatically increased, its mRNA and serum levels. Although mRNA and circulating levels of myonectin were reduced in a diet-induced obese state, voluntary exercise increased its expression and circulating levels. Accordingly, myonectin transcript was up-regulated by compounds (forskolin, epinephrine, ionomycin) that raise cellular cAMP or calcium levels.... In vitro results of myonectin expression in myotubes suggest that exercise-induced rises in intracellular calcium levels may also up-regulate myonectin expression in intact skeletal muscle.... Consistent with enhanced mRNA expression in skeletal muscle of mice subjected to voluntary exercise, circulating levels of myonectin also increased, suggesting a potential role of myonectin in exercise-induced physiology.		"Given that exercise induces myonectin expression in skeletal muscle, we next addressed whether short- and long-term changes in nutritional/metabolic state also regulate myonectin expression and circulating levels. Surprisingly, an overnight fast greatly suppressed myonectin expression, but a 2-h refeeding period (following an overnight fast) dramatically up-regulated its mRNA expression in skeletal muscle. Intriguingly, refeeding induced myonectin mRNA expression to a much greater extent in soleus than in plantaris muscle fiber of both male and female mice (data not shown), suggesting that myonectin expression may be regulated differentially depending on muscle fiber type. Consistent with the mRNA data, fasting reduced, but refeeding substantially increased, circulating levels of myonectin.... As compared with mice fed an isocaloric matched low-fat diet, mice fed a high-fat diet had lower myonectin mRNA and serum levels, suggesting that obesity-induced alteration in energy balance may be linked to dysregulation of myonectin-mediated processes in the obese state.... A relatively modest rise in serum myonectin levels was sufficient to lower (by >30%) nonesterified free fatty acid (NEFA) levels over time relative to vehicle-injected controls. However, no significant difference was observed in serum triacylglycerol levels between the two groups of mice. These data suggest a potential role of myonectin in regulating systemic fatty acid metabolism.... Treatment of adipocytes with recombinant myonectin (5 micrograms/ml) also enhanced fatty acid uptake to the same extent as insulin.... To determine whether myonectin-mediated enhancement of lipid uptake is specific to adipocytes, we also tested the effect of myonectin on lipid uptake in rat H4IIE hepatocytes. We observed a modest (>25%) but consistent increase in fatty acid uptake into hepatocytes stimulated with myonectin (5 micrograms/ml), an effect similar to cells treated with a saturating dose of insulin (50 nM).... Together, these results indicate that myonectin promotes lipid uptake into adipocytes and hepatocytes via transcriptional up-regulation of genes involved in fatty acid uptake....		"We provide the first characterization of myonectin, with in vitro and in vivo evidence that it is a novel myokine with important metabolic function. Unlike the other CTRPs characterized to date, myonectin (CTRP15) is expressed and secreted predominantly by skeletal muscle.... (Our) results suggest that myonectin is a nutrient-responsive metabolic regulator secreted by skeletal muscle in response to changes in cellular energy state resulting from glucose or fatty acid fluxes. Many metabolically relevant secreted proteins (e.g. adiponectin, leptin, resistin, and RBP) and the signaling pathways they regulate in tissues are known to be dysregulated in the condition of obesity. The reduction in expression and circulating levels of myonectin in the obese state may represent yet another component of the complex metabolic circuitry dysregulated by excess caloric intake. Although exercise has long been known to have profound positive impacts on systemic insulin sensitivity and energy balance, the underlying mechanisms remain incompletely understood. That voluntary exercise dramatically increases the expression and circulating levels of myonectin to promote fatty acid uptake into cells may underlie one of the beneficial effects of physical exercise.... A modest rise in the circulating levels of myonectin resulting from recombinant protein administration is sufficient to lower serum NEFA without altering serum triglyceride levels. Unlike CTRP1, CTRP3 and CTRP12, injection of recombinant myonectin into mice appears to have no glucose-lowering effect. Reduction in circulating NEFA is not due to suppression of adipose tissue lipolysis; rather, it results from increased fatty acid uptake by adipocytes and hepatocytes. Although the myonectin-mediated enhancement of lipid uptake in vitro appears modest (25–50%), in fact, the magnitude of this effect is comparable with cells stimulated with 50 nM insulin, a saturating dose that leads to maximum increase in fatty acid uptake.... In accordance with myonectin mediating its metabolic effect through a transcriptional mechanism, a reduction in circulating NEFA in mice occurred only 2 h after recombinant protein injection, a lag period presumably required for mRNA and protein synthesis."[14]		Decorin is an example of a proteoglycan which functions as a myokine. Kanzleiter et al have established that this myokine is secreted during muscular contraction against resistance, and plays a role in muscle growth. They reported on July 1, 2014: "The small leucine-rich proteoglycan decorin has been described as a myokine for some time. However, its regulation and impact on skeletal muscle (had) not been investigated in detail. In (our recent) study, we report decorin to be differentially expressed and released in response to muscle contraction using different approaches. Decorin is released from contracting human myotubes, and circulating decorin levels are increased in response to acute resistance exercise in humans. Moreover, decorin expression in skeletal muscle is increased in humans and mice after chronic training. Because decorin directly binds myostatin, a potent inhibitor of muscle growth, we investigated a potential function of decorin in the regulation of skeletal muscle growth. In vivo overexpression of decorin in murine skeletal muscle promoted expression of the pro-myogenic factor Mighty, which is negatively regulated by myostatin. We also found Myod1 and follistatin to be increased in response to decorin overexpression. Moreover, muscle-specific ubiquitin ligases atrogin1 and MuRF1, which are involved in atrophic pathways, were reduced by decorin overexpression. In summary, our findings suggest that decorin secreted from myotubes in response to exercise is involved in the regulation of muscle hypertrophy and hence could play a role in exercise-related restructuring processes of skeletal muscle."[15]		Irisin is a cleaved version of FNDC5. Boström and coworkers named the cleaved product irisin, after the Greek messenger goddess Iris.[16] FNDC5 was initially discovered in 2002 by two independent groups of researchers.[17][18][19]		Rana et al. reported in January 2014 that irisin (fibronectin type III domain-containing protein 5 or FNDC5), a recently described myokine hormone produced and secreted by acutely exercising skeletal muscles, is thought to bind white adipose tissue cells via undetermined receptors. Irisin has been reported to promote a brown adipose tissue-like phenotype upon white adipose tissue by increasing cellular mitochondrial density and expression of uncoupling protein-1, thereby increasing adipose tissue energy expenditure via thermogenesis. This is considered important, because excess visceral adipose tissue in particular distorts the whole body energy homeostasis, increases the risk of cardiovascular disease and raises exposure to a milieu of adipose tissue-secreted hormones (adipokines) that promote inflammation and cellular aging. The authors enquired whether the favorable impact of irisin on white adipose tissue might be associated with maintenance of telomere length, a well-established genetic marker in the aging process. They conclude that these data support the view that irisin may have a role in the modulation not only of energy balance but also the aging process.[20]		However, exogenous irisin may aid in heightening energy expenditure, and thus in reducing obesity. Boström et al. reported on December 14, 2012: "Since the conservation of calories would likely provide an overall survival advantage for mammals, it appears paradoxical that exercise would stimulate the secretion of a polypeptide hormone that increases thermogenesis and energy expenditure. One explanation for the increased irisin expression with exercise in mouse and man may have evolved as a consequence of muscle contraction during shivering. Muscle secretion of a hormone that activates adipose thermogenesis during this process might provide a broader, more robust defense against hypothermia. The therapeutic potential of irisin is obvious. Exogenously administered irisin induces the browning of subcutaneous fat and thermogenesis, and it presumably could be prepared and delivered as an injectable polypeptide. Increased formation of brown or beige/brite fat has been shown to have anti-obesity, anti-diabetic effects in multiple murine models, and adult humans have significant deposits of UCP1-positive brown fat. (Our data show) that even relatively short treatments of obese mice with irisin improves glucose homeostasis and causes a small weight loss. Whether longer treatments with irisin and/or higher doses would cause more weight loss remains to be determined. The worldwide, explosive increase in obesity and diabetes strongly suggests exploring the clinical utility of irisin in these and related disorders. Another potentially important aspect of this work relates to other beneficial effects of exercise, especially in some diseases for which no effective treatments exist. The clinical data linking exercise with health benefits in many other diseases suggests that irisin could also have significant effects in these disorders."[21]		While the murine findings reported by Boström et al. appear encouraging, other researchers have questioned whether irisin operates in a similar manner in humans. For example, Timmons et al. noted that over 1,000 genes are upregulated by exercise and examined how expression of FNDC5 was affected by exercise in ~200 humans. They found that it was upregulated only in highly active elderly humans, casting doubt on the conclusions of Boström et al.[22] Further discussion of this issue can be found in the Wikipedia entry for irisin under the "function" heading.		A novel myokine Osteonectin, or SPARC, plays a vital role in bone mineralization, cell-matrix interactions, and collagen binding. Osteonectin inhibits tumorigenesis in mice. Osteonectin can be classed as a myokine, as it was found that even a single bout of exercise increased its expression and secretion in skeletal muscle in both mice and humans.[23]				
Sodium is a chemical element with symbol Na (from Latin natrium) and atomic number 11. It is a soft, silvery-white, highly reactive metal. Sodium is an alkali metal, being in group 1 of the periodic table, because it has a single electron in its outer shell that it readily donates, creating a positively charged atom—the Na+ cation. Its only stable isotope is 23Na. The free metal does not occur in nature, but must be prepared from compounds. Sodium is the sixth most abundant element in the Earth's crust, and exists in numerous minerals such as feldspars, sodalite and rock salt (NaCl). Many salts of sodium are highly water-soluble: sodium ions have been leached by the action of water from the Earth's minerals over eons, and thus sodium and chlorine are the most common dissolved elements by weight in the oceans.		Sodium was first isolated by Humphry Davy in 1807 by the electrolysis of sodium hydroxide. Among many other useful sodium compounds, sodium hydroxide (lye) is used in soap manufacture, and sodium chloride (edible salt) is a de-icing agent and a nutrient for animals including humans.		Sodium is an essential element for all animals and some plants. Sodium ions are the major cation in the extracellular fluid (ECF) and as such are the major contributor to the ECF osmotic pressure and ECF compartment volume. Loss of water from the ECF compartment increases the sodium concentration, a condition called hypernatremia. Isotonic loss of water and sodium from the ECF compartment decreases the size of that compartment in a condition called ECF hypovolemia.		By means of the sodium-potassium pump, living human cells pump three sodium ions out of the cell in exchange for two potassium ions pumped in; comparing ion concentrations across the cell membrane, inside to outside, potassium measures about 40:1, and sodium, about 1:10. In nerve cells, the electrical charge across the cell membrane enables transmission of the nerve impulse—an action potential—when the charge is dissipated; sodium plays a key role in that activity.						Sodium at standard temperature and pressure is a soft silvery metal that combines with oxygen in air and forms grayish white sodium oxide unless immersed in oil or inert gas, which are the conditions it is usually stored in. Sodium metal can be easily cut with a knife and is a good conductor of electricity and heat because it has only one electron in its valence shell, resulting in weak metallic bonding and free electrons, which carry energy. Due to having low atomic mass and large atomic radius, sodium is third-least dense of all elemental metals and is one of only three metals that can float on water, the other two being lithium and potassium.[4] The melting (98 °C) and boiling (883 °C) points of sodium are lower than those of lithium but higher than those of the heavier alkali metals potassium, rubidium, and caesium, following periodic trends down the group.[5] These properties change dramatically at elevated pressures: at 1.5 Mbar, the color changes from silvery metallic to black; at 1.9 Mbar the material becomes transparent with a red color; and at 3 Mbar, sodium is a clear and transparent solid. All of these high-pressure allotropes are insulators and electrides.[6]		In a flame test, sodium and its compounds glow yellow[7] because the excited 3s electrons of sodium emit a photon when they fall from 3p to 3s; the wavelength of this photon corresponds to the D line at about 589.3 nm. Spin-orbit interactions involving the electron in the 3p orbital split the D line into two, at 589.0 and 589.6 nm; hyperfine structures involving both orbitals cause many more lines.[8]		Twenty isotopes of sodium are known, but only 23Na is stable. 23Na is created in the carbon-burning process in stars by fusing two carbon atoms together; this requires temperatures above 600 megakelvins and a star of at least three solar masses.[9] Two radioactive, cosmogenic isotopes are the byproduct of cosmic ray spallation: 22Na has a half-life of 2.6 years and 24Na, a half-life of 15 hours; all other isotopes have a half-life of less than one minute.[10] Two nuclear isomers have been discovered, the longer-lived one being 24mNa with a half-life of around 20.2 milliseconds. Acute neutron radiation, as from a nuclear criticality accident, converts some of the stable 23Na in human blood to 24Na; the neutron radiation dosage of a victim can be calculated by measuring the concentration of 24Na relative to 23Na.[11]		Sodium atoms have 11 electrons, one more than the extremely stable configuration of the noble gas neon. Because of this and its low first ionization energy of 495.8 kJ/mol, the sodium atom is much more likely to lose the last electron and acquire a positive charge than to gain one and acquire a negative charge.[12] This process requires so little energy that sodium is readily oxidized by giving up its 11th electron. In contrast, the second ionization energy is very high (4562 kJ/mol), because the 10th electron is closer to the nucleus than the 11th electron. As a result, sodium usually forms ionic compounds involving the Na+ cation.[13]		The most common oxidation state for sodium is +1. It is generally less reactive than potassium and more reactive than lithium.[14] Sodium metal is highly reducing, with the standard reduction potential for the Na+/Na couple being −2.71 volts,[15] though potassium and lithium have even more negative potentials.[16]		Sodium compounds are of immense commercial importance, being particularly central to industries producing glass, paper, soap, and textiles.[17] The most important sodium compounds are table salt (NaCl), soda ash (Na2CO3), baking soda (NaHCO3), caustic soda (NaOH), sodium nitrate (NaNO3), di- and tri-sodium phosphates, sodium thiosulfate (Na2S2O3·5H2O), and borax (Na2B4O7·10H2O).[18] In compounds, sodium is usually ionically bonded to water and anions, and is viewed as a hard Lewis acid.[19]		Most soaps are sodium salts of fatty acids. Sodium soaps have a higher melting temperature (and seem "harder") than potassium soaps.[18]		Like all the alkali metals, sodium reacts exothermically with water, and sufficiently large pieces melt to a sphere and may explode. The reaction produces caustic soda (sodium hydroxide) and flammable hydrogen gas. When burned in air, it forms primarily sodium peroxide with some sodium oxide.[20]		Sodium tends to form water-soluble compounds, such as halides, sulfates, nitrates, carboxylates and carbonates. The main aqueous species are the aquo complexes [Na(H2O)n]+, where n = 4–8; with n = 6 indicated from X-ray diffraction data and computer simulations.[21]		Direct precipitation of sodium salts from aqueous solutions is rare because sodium salts typically have a high affinity for water; an exception is sodium bismuthate (NaBiO3).[22] Because of this, sodium salts are usually isolated as solids by evaporation or by precipitation with an organic solvent, such as ethanol; for example, only 0.35 g/L of sodium chloride will dissolve in ethanol.[23] Crown ethers, like 15-crown-5, may be used as a phase-transfer catalyst.[24]		Sodium content in bulk may be determined by treating with a large excess of uranyl zinc acetate; the hexahydrate (UO2)2ZnNa(CH3CO2)·6H2O precipitates and can be weighed. Caesium and rubidium do not interfere with this reaction, but potassium and lithium do.[25] Lower concentrations of sodium may be determined by atomic absorption spectrophotometry[26] or by potentiometry using ion-selective electrodes.[27]		Like the other alkali metals, sodium dissolves in ammonia and some amines to give deeply colored solutions; evaporation of these solutions leaves a shiny film of metallic sodium. The solutions contain the coordination complex (Na(NH3)6)+, with the positive charge counterbalanced by electrons as anions; cryptands permit the isolation of these complexes as crystalline solids. Sodium forms complexes with crown ethers, cryptands and other ligands.[28] For example, 15-crown-5 has high affinity for sodium because the cavity size of 15-crown-5 is 1.7–2.2 Å, which is enough to fit sodium ion (1.9 Å).[29][30] Cryptands, like crown ethers and other ionophores, also have a high affinity for the sodium ion; derivatives of the alkalide Na− are obtainable[31] by the addition of cryptands to solutions of sodium in ammonia via disproportionation.[32]		Many organosodium compounds have been prepared. Because of the high polarity of the C-Na bonds, they behave like sources of carbanions (salts with organic anions). Some well known derivatives include sodium cyclopentadienide (NaC5H5) and trityl sodium ((C6H5)3CNa).[33] Because of the large size and very low polarising power of the Na+ cation, it can stabilize large, aromatic, polarisable radical anions, such as in sodium naphthalenide, Na+[C10H8•]−, a strong reducing agent.[34]		Sodium forms alloys with many metals, such as potassium, calcium, lead, and the group 11 and 12 elements. Sodium and potassium form KNa2 and NaK. NaK is 40–90% potassium and it is liquid at ambient temperature. It is excellent thermal and electrical conductor. Sodium-calcium alloys are by-products of electrolytic production of sodium from binary salt mixture of NaCl-CaCl2 and ternary mixture NaCl-CaCl2-BaCl2. Calcium is only partially miscible with sodium. In liquid state, sodium is completely miscible with lead. There are several methods to make sodium-lead alloys. One is to melt them together and another is to deposit sodium electrolycally on molten lead cathodes. NaPb3, NaPb, Na9Pb4, Na5Pb2, and Na15Pb4 are some of the known sodium-lead alloys. Sodium also forms alloys with gold (NaAu2) and silver (NaAg2). Group 12 metals (zinc, cadmium and mercury) are known to make alloys with sodium. NaZn13 and NaCd2 are alloys of zinc and cadmium. Sodium and mercury form NaHg, NaHg4, NaHg2, Na3Hg2, and Na3Hg.[35]		Because of its importance in human metabolism, salt has long been an important commodity, as shown by the English word salary, which derives from salarium, the wafers of salt sometimes given to Roman soldiers along with their other wages. In medieval Europe, a compound of sodium with the Latin name of sodanum was used as a headache remedy. The name sodium is thought to originate from the Arabic suda, meaning headache, as the headache-alleviating properties of sodium carbonate or soda were well known in early times.[36] Although sodium, sometimes called soda, had long been recognized in compounds, the metal itself was not isolated until 1807 by Sir Humphry Davy through the electrolysis of sodium hydroxide.[37][38] In 1809, the German physicist and chemist Ludwig Wilhelm Gilbert proposed the names Natronium for Humphry Davy's "sodium" and Kalium for Davy's "potassium".[39] The chemical abbreviation for sodium was first published in 1814 by Jöns Jakob Berzelius in his system of atomic symbols,[40][41] and is an abbreviation of the element's New Latin name natrium, which refers to the Egyptian natron,[36] a natural mineral salt mainly consisting of hydrated sodium carbonate. Natron historically had several important industrial and household uses, later eclipsed by other sodium compounds.[42]		Sodium imparts an intense yellow color to flames. As early as 1860, Kirchhoff and Bunsen noted the high sensitivity of a sodium flame test, and stated in Annalen der Physik und Chemie:[43]		In a corner of our 60 m3 room farthest away from the apparatus, we exploded 3 mg. of sodium chlorate with milk sugar while observing the nonluminous flame before the slit. After a while, it glowed a bright yellow and showed a strong sodium line that disappeared only after 10 minutes. From the weight of the sodium salt and the volume of air in the room, we easily calculate that one part by weight of air could not contain more than 1/20 millionth weight of sodium.		The Earth's crust contains 2.27% sodium, making it the seventh most abundant element on Earth and the fifth most abundant metal, behind aluminium, iron, calcium, and magnesium and ahead of potassium.[44] Sodium's estimated oceanic abundance is 1.08×104 milligrams per liter.[45] Because of its high reactivity, it is never found as a pure element. It is found in many different minerals, some very soluble, such as halite and natron, others much less soluble, such as amphibole and zeolite. The insolubility of certain sodium minerals such as cryolite and feldspar arises from their polymeric anions, which in the case of feldspar is a polysilicate.		In the interstellar medium, sodium is identified by the D spectral line; though it has a high vaporization temperature, its abundance in Mercury's atmosphere enabled its detection by Potter and Morgan using ground-based high resolution spectroscopy.[citation needed] Sodium has been detected in at least one comet; astronomers watching Comet Hale-Bopp in 1997 observed a sodium tail consisting of neutral atoms (not ions) and extending to some 50 million kilometres behind the head.[46]		Employed only in rather specialized applications, only about 100,000 tonnes of metallic sodium are produced annually.[17] Metallic sodium was first produced commercially in the late 19th century[47] by carbothermal reduction of sodium carbonate at 1100 °C, as the first step of the Deville process for the production of aluminium:[48][49][50]		The high demand of aluminium created the need for the production of sodium. After the introduction of the Hall–Héroult process for the production of aluminium by electrolysing a molten salt bath ended the need for large quantities of sodium. A related process based on the reduction of sodium hydroxide was developed in 1886.[48]		Sodium is now produced commercially through the electrolysis of molten sodium chloride, based on a process patented in 1924.[51][52] This is done in a Downs cell in which the NaCl is mixed with calcium chloride to lower the melting point below 700 °C. As calcium is less electropositive than sodium, no calcium will be deposited at the cathode.[53] This method is less expensive than the previous Castner process (the electrolysis of sodium hydroxide).[54]		The market for sodium is volatile due to the difficulty in its storage and shipping; it must be stored under a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of sodium oxide or sodium superoxide.[55]		Though metallic sodium has some important uses, the major applications for sodium use compounds; millions of tons of sodium chloride, hydroxide, and carbonate are produced annually. Sodium chloride is extensively used for anti-icing and de-icing and as a preservative; sodium bicarbonate is mainly used for cooking. Along with potassium, many important medicines have sodium added to improve their bioavailability; though potassium is the better ion in most cases, sodium is chosen for its lower price and atomic weight.[56] Sodium hydride is used as a base for various reactions (such as the aldol reaction) in organic chemistry, and as a reducing agent in inorganic chemistry.[57]		Metallic sodium is used mainly for the production of sodium borohydride, sodium azide, indigo, and triphenylphosphine. A once-common use was the making of tetraethyllead and titanium metal; because of the move away from TEL and new titanium production methods, the production of sodium declined after 1970.[17] Sodium is also used as an alloying metal, an anti-scaling agent,[58] and as a reducing agent for metals when other materials are ineffective. Note the free element is not used as a scaling agent, ions in the water are exchanged for sodium ions. Sodium plasma ("vapor") lamps are often used for street lighting in cities, shedding light that ranges from yellow-orange to peach as the pressure increases.[59] By itself or with potassium, sodium is a desiccant; it gives an intense blue coloration with benzophenone when the desiccate is dry.[60] In organic synthesis, sodium is used in various reactions such as the Birch reduction, and the sodium fusion test is conducted to qualitatively analyse compounds.[61] Sodium reacts with alcohol and gives alkoxides, and when sodium is dissolved in ammonia solution, it can be used to reduce alkynes to trans-alkenes.[62][63] Lasers emitting light at the sodium D line are used to create artificial laser guide stars that assist in the adaptive optics for land-based visible light telescopes.[64]		Liquid sodium is used as a heat transfer fluid in some fast reactors[66] because it has the high thermal conductivity and low neutron absorption cross section required to achieve a high neutron flux in the reactor.[67] The high boiling point of sodium allows the reactor to operate at ambient (normal) pressure,[67] but the drawbacks include its opacity, which hinders visual maintenance, and its explosive properties.[68] Radioactive sodium-24 may be produced by neutron bombardment during operation, posing a slight radiation hazard; the radioactivity stops within a few days after removal from the reactor.[69] If a reactor needs to be shut down frequently, NaK is used; because NaK is a liquid at room temperature, the coolant does not solidify in the pipes.[70] In this case, the pyrophoricity of potassium requires extra precautions to prevent and detect leaks.[71] Another heat transfer application is poppet valves in high-performance internal combustion engines; the valve stems are partially filled with sodium and work as a heat pipe to cool the valves.[72]		In humans, sodium is an essential mineral that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day.[73] Sodium chloride is the principal source of sodium in the diet, and is used as seasoning and preservative in such commodities as pickled preserves and jerky; for Americans, most sodium chloride comes from processed foods.[74] Other sources of sodium are its natural occurrence in food and such food additives as monosodium glutamate (MSG), sodium nitrite, sodium saccharin, baking soda (sodium bicarbonate), and sodium benzoate.[75] The US Institute of Medicine set its Tolerable Upper Intake Level for sodium at 2.3 grams per day,[76] but the average person in the United States consumes 3.4 grams per day.[77] Studies have found that lowering sodium intake by 2 g per day tends to lower systolic blood pressure by about two to four mm Hg.[78] It has been estimated that such a decrease in sodium intake would lead to between 9 and 17% fewer cases of hypertension.[78]		Hypertension causes 7.6 million premature deaths worldwide each year.[79] (Note that salt contains about 39.3% sodium[80]—the rest being chlorine and trace chemicals; thus, 2.3 g sodium is about 5.9 g, or 2.7 ml of salt—about half a US teaspoon.[81][82]) The American Heart Association recommends no more than 1.5 g of sodium per day.[83]		One study found that people with or without hypertension who excreted less than 3 grams of sodium per day in their urine (and therefore were taking in less than 3 g/d) had a higher risk of death, stroke, or heart attack than those excreting 4 to 5 grams per day. Levels of 7 g per day or more in people with hypertension were associated with higher mortality and cardiovascular events, but this was not found to be true for people without hypertension.[84] The US FDA states that adults with hypertension and prehypertension should reduce daily intake to 1.5 g.[82]		The renin-angiotensin system regulates the amount of fluid and sodium concentration in the body. Reduction of blood pressure and sodium concentration in the kidney result in the production of renin, which in turn produces aldosterone and angiotensin, retaining sodium in the urine. When the concentration of sodium increases, the production of renin decreases, and the sodium concentration returns to normal.[85] The sodium ion (Na+) is an important electrolyte in neuron function, and in osmoregulation between cells and the extracellular fluid. This is accomplished in all animals by Na+/K+-ATPase, an active transporter pumping ions against the gradient, and sodium/potassium channels.[86] Sodium is the most prevalent metallic ion in extracellular fluid.[87]		Unusually low or high sodium levels in humans are recognized in medicine as hyponatremia and hypernatremia. These conditions may be caused by genetic factors, ageing, or prolonged vomiting or diarrhea.[88]		In C4 plants, sodium is a micronutrient that aids metabolism, specifically in regeneration of phosphoenolpyruvate and synthesis of chlorophyll.[89] In others, it substitutes for potassium in several roles, such as maintaining turgor pressure and aiding in the opening and closing of stomata.[90] Excess sodium in the soil can limit the uptake of water by decreasing the water potential, which may result in plant wilting; excess concentrations in the cytoplasm can lead to enzyme inhibition, which in turn causes necrosis and chlorosis.[91] In response, some plants have developed mechanisms to limit sodium uptake in the roots, to store it in cell vacuoles, and restrict salt transport from roots to leaves;[92] excess sodium may also be stored in old plant tissue, limiting the damage to new growth. Halophytes have adapted to be able to flourish in sodium rich environments.[92]		Sodium forms flammable hydrogen and caustic sodium hydroxide on contact with water;[94] ingestion and contact with moisture on skin, eyes or mucous membranes can cause severe burns.[95][96] Sodium spontaneously explodes in the presence of an oxidizer such as water.[97] Fire extinguishers based on water accelerate sodium fires; those based on carbon dioxide and bromochlorodifluoromethane should not be used on sodium fire.[96] Metal fires are Class D, but not all Class D extinguishers are workable with sodium. An effective extinguishing agent for sodium fires is Met-L-X.[96] Other effective agents include Lith-X, which has graphite powder and an organophosphate flame retardant, and dry sand.[98] Sodium fires are prevented in nuclear reactors by isolating sodium from oxygen by surrounding sodium pipes with inert gas.[99] Pool-type sodium fires are prevented using different design measures called catch pan systems. They collect leaking sodium into a leak-recovery tank where it is isolated from oxygen.[99]		
The neurobiological effects of physical exercise are numerous and involve a wide range of interrelated effects on brain structure, brain function, and cognition.[1][2][3][4] A large body of research in humans has demonstrated that consistent aerobic exercise (e.g., 30 minutes every day) induces persistent improvements in certain cognitive functions, healthy alterations in gene expression in the brain, and beneficial forms of neuroplasticity and behavioral plasticity; some of these long-term effects include: increased neuron growth, increased neurological activity (e.g., c-Fos and BDNF signaling), improved stress coping, enhanced cognitive control of behavior, improved declarative, spatial, and working memory, and structural and functional improvements in brain structures and pathways associated with cognitive control and memory.[1][2][3][4][5][6][7][8][9][10] The effects of exercise on cognition have important implications for improving academic performance in children and college students, improving adult productivity, preserving cognitive function in old age, preventing or treating certain neurological disorders, and improving overall quality of life.[1][11][12]		In healthy adults, aerobic exercise has been shown to induce transient effects on cognition after a single exercise session and persistent effects on cognition following regular exercise over the course of several months.[1][10][13] People who regularly perform aerobic exercise (e.g., running, jogging, brisk walking, swimming, and cycling) have greater scores on neuropsychological function and performance tests that measure certain cognitive functions, such as attentional control, inhibitory control, cognitive flexibility, working memory updating and capacity, declarative memory, spatial memory, and information processing speed.[1][5][7][9][10][13] The transient effects of exercise on cognition include improvements in most executive functions (e.g., attention, working memory, cognitive flexibility, inhibitory control, problem solving, and decision making) and information processing speed for a period of up to 2 hours after exercising.[13]		Aerobic exercise induces short- and long-term effects on mood and emotional states by promoting positive affect, inhibiting negative affect, and decreasing the biological response to acute psychological stress.[13] Over the short-term, aerobic exercise functions as both an antidepressant and euphoriant,[14][15][16][17] whereas consistent exercise produces general improvements in mood and self-esteem.[18][19]		Regular aerobic exercise improves symptoms associated with a variety of central nervous system disorders and may be used as an adjunct therapy for these disorders. There is clear evidence of exercise treatment efficacy for major depressive disorder and attention deficit hyperactivity disorder.[11][16][20][21][22][23] A large body of preclinical evidence and emerging clinical evidence supports the use of exercise therapy for treating and preventing the development of drug addictions.[24][25][26][27][28] Reviews of clinical evidence also support the use of exercise as an adjunct therapy for certain neurodegenerative disorders, particularly Alzheimer’s disease and Parkinson's disease.[29][30][31][32][33][34] Regular exercise is also associated with a lower risk of developing neurodegenerative disorders.[32][35] Regular exercise has also been proposed as an adjunct therapy for brain cancers.[36]		Neuroplasticity is the process by which neurons adapt to a disturbance over time, and most often occurs in response to repeated exposure to stimuli.[37] Aerobic exercise increases the production of neurotrophic factors[note 1] (e.g., BDNF, IGF-1, VEGF) which mediate improvements in cognitive functions and various forms of memory by promoting blood vessel formation in the brain, adult neurogenesis,[note 2] and other forms of neuroplasticity.[2][5][18][39][40] Consistent aerobic exercise over a period of several months induces clinically significant improvements in executive functions and increased gray matter volume in multiple brain regions, particularly those which give rise to executive functions.[1][5][6][7][9] The brain structures that show the greatest improvements in gray matter volume in response to aerobic exercise are the prefrontal cortex, caudate nucleus, and hippocampus;[1][5][6][8] less significant increases in gray matter volume occur in the anterior cingulate cortex, parietal cortex, cerebellum, and nucleus accumbens.[5][6][8] The prefrontal cortex, caudate nucleus, and anterior cingulate cortex are among the most significant brain structures in the dopamine and norepinephrine systems that give rise to cognitive control.[6][41] Exercise-induced neurogenesis (i.e., the increases in gray matter volume) in the hippocampus is associated with measurable improvements in spatial memory.[6][8][19][42] Higher physical fitness scores, as measured by VO2 max, are associated with better executive function, faster information processing speed, and greater gray matter volume of the hippocampus, caudate nucleus, and nucleus accumbens.[1][6] Long-term aerobic exercise is also associated with persistent beneficial epigenetic changes that result in improved stress coping, improved cognitive function, and increased neuronal activity (c-Fos and BDNF signaling).[4][43]		One of the most significant effects of exercise on the brain is the increased synthesis and expression of BDNF, a neuropeptide hormone, in the brain and periphery, resulting in increased signaling through its tyrosine kinase receptor, tropomyosin receptor kinase B (TrkB).[4][44][45] Since BDNF is capable of crossing the blood–brain barrier, higher peripheral BDNF synthesis also increases BDNF signaling in the brain.[39] Exercise-induced increases in brain BDNF signaling are associated with beneficial epigenetic changes, improved cognitive function, improved mood, and improved memory.[4][8][18][44] Furthermore, research has provided a great deal of support for the role of BDNF in hippocampal neurogenesis, synaptic plasticity, and neural repair.[5][44] Engaging in moderate-high intensity aerobic exercise such as running, swimming, and cycling increases BDNF biosynthesis through myokine signaling, resulting in up to a threefold increase in blood plasma and brain BDNF levels;[4][44][45] exercise intensity is positively correlated with the magnitude of increased BDNF biosynthesis and expression.[4][44][45] A meta-analysis of studies involving the effect of exercise on BDNF levels found that consistent exercise modestly increases resting BDNF levels as well.[18]		IGF-1 is a peptide and neurotrophic factor that mediates some of the effects of growth hormone;[46] IGF-1 elicits its physiological effects by binding to a specific tyrosine kinase receptor, the IGF-1 receptor, to control tissue growth and remodeling.[46] In the brain, IGF-1 functions as a neurotrophic factor that, like BDNF, plays a significant role in cognition, neurogenesis, and neuronal survival.[44][47][48] Physical activity is associated with increased levels of IGF-1 in blood serum, which is known to contribute to neuroplasticity in the brain due to its capacity to cross the blood–brain barrier and blood–cerebrospinal fluid barrier;[5][44][46][47] consequently, one review noted that IGF-1 is a key mediator of exercise-induced adult neurogenesis, while a second review characterized it as a factor which links "body fitness" with "brain fitness".[46][47] The amount of IGF-1 released into blood plasma during exercise is positively correlated with exercise intensity and duration.[49]		VEGF is a neurotrophic and angiogenic (i.e., blood vessel growth promoting) signaling protein that binds to two receptor tyrosine kinases, VEGFR1 and VEGFR2, which are expressed in neurons and glial cells in the brain.[48] Hypoxia, or inadequate cellular oxygen supply, strongly upregulates VEGF expression and VEGF exerts a neuroprotective effect in hypoxic neurons.[48] Like BDNF and IGF-1, aerobic exercise has been shown to increase VEGF biosynthesis in peripheral tissue which subsequently crosses the blood–brain barrier and promotes neurogenesis and blood vessel formation in the central nervous system.[39][40][50] Exercise-induced increases in VEGF signaling have been shown to improve cerebral blood volume and contribute to exercise-induced neurogenesis in the hippocampus.[5][40][50]		Reviews of neuroimaging studies indicate that consistent aerobic exercise increases gray matter volume in brain regions associated with memory processing, cognitive control, motor function, and reward;[1][5][6][8] the most prominent gains in gray matter volume are seen in the prefrontal cortex, caudate nucleus, and hippocampus, which support cognitive control and memory processing, among other cognitive functions.[1][6][8][9] Moreover, the left and right halves of the prefrontal cortex, the hippocampus, and the cingulate cortex appear to become more functionally interconnected in response to consistent aerobic exercise.[1][7] Three reviews indicate that marked improvements in prefrontal and hippocampal gray matter volume occur in healthy adults that regularly engage in medium intensity exercise for several months.[1][6][51] Other regions of the brain that demonstrate moderate or less significant gains in gray matter volume during neuroimaging include the anterior cingulate cortex, parietal cortex, cerebellum, and nucleus accumbens.[5][6][8][52]		Regular exercise has been shown to counter the shrinking of the hippocampus and memory impairment that naturally occurs in late adulthood.[5][6][8] Sedentary adults over age 55 show a 1–2% decline in hippocampal volume annually.[8][53] A neuroimaging study with a sample of 120 adults revealed that participating in regular aerobic exercise increased the volume of the left hippocampus by 2.12% and the right hippocampus by 1.97% over a one-year period.[8][53] Subjects in the low intensity stretching group who had higher fitness levels at baseline showed less hippocampal volume loss, providing evidence for exercise being protective against age-related cognitive decline.[53] In general, individuals that exercise more over a given period have greater hippocampal volumes and better memory function.[5][8] Aerobic exercise has also been shown to induce growth in the white matter tracts in the anterior corpus callosum, which normally shrink with age.[5][51]		The various functions of the brain structures that show exercise-induced increases in gray matter volume include:		Concordant with the functional roles of the brain structures that exhibit increased gray matter volumes, regular exercise over a period of several months has been shown to persistently improve numerous executive functions and several forms of memory.[5][7][9][60][61] In particular, consistent aerobic exercise has been shown to improve attentional control,[note 3] information processing speed, cognitive flexibility (e.g., task switching), inhibitory control,[note 4] working memory updating and capacity,[note 5] declarative memory,[note 6] and spatial memory.[5][6][7][9][10][60][61] In healthy young and middle-aged adults, the effect sizes of improvements in cognitive function are largest for indices of executive functions and small to moderate for aspects of memory and information processing speed.[1][10] It may be that in older adults, individuals benefit cognitively by taking part in both aerobic and resistance type exercise of at least moderate intensity.[63] Individuals who have a sedentary lifestyle tend to have impaired executive functions relative to other more physically active non-exercisers.[9][60] A reciprocal relationship between exercise and executive functions has also been noted: improvements in executive control processes, such as attentional control and inhibitory control, increase an individual's tendency to exercise.[9]		In addition to the persistent effects of regular exercise over the course of several months on cognitive functions, acute exercise (i.e., a single bout of exercise) has been shown to transiently improve a number of cognitive functions.[13][64][65] Reviews and meta-analyses of research on the effects of acute exercise in healthy young and middle-aged adults on cognition have concluded that information processing speed and a number of executive functions – including attention, working memory, problem solving, cognitive flexibility, verbal fluency, decision making, and inhibitory control – all improve for a period of up to 2 hours post-exercise.[13][64][65] A systematic review of studies conducted on children also suggested that some of the exercise-induced improvements in executive function are apparent after single bouts of exercise, while other aspects (e.g., attentional control) only improve following consistent exercise on a regular basis.[61]		The "stress hormone", cortisol, is a glucocorticoid that binds to glucocorticoid receptors.[66][67][68] Psychological stress induces the release of cortisol from the adrenal gland by activating the hypothalamic–pituitary–adrenal axis (HPA axis).[66][67][68] Short-term increases in cortisol levels are associated with adaptive cognitive improvements, such as enhanced inhibitory control;[40][67][68] however, excessively high exposure or prolonged exposure to high levels of cortisol causes impairments in cognitive control and has neurotoxic effects in the human brain.[40][60][68] For example, chronic psychological stress decreases BDNF expression which has detrimental effects on hippocampal volume and can lead to depression.[40][66]		As a physical stressor, aerobic exercise stimulates cortisol secretion in an intensity-dependent manner;[67] however, it does not result in long-term increases in cortisol production since this exercise-induced effect on cortisol is a response to transient negative energy balance.[note 7][67] Individuals who have recently exercised exhibit improvements in stress coping behaviors.[4][40][43] Aerobic exercise increases physical fitness and lowers neuroendocrine (i.e., HPA axis) reactivity and therefore reduces the biological response to psychological stress in humans (e.g., reduced cortisol release and attenuated heart rate response).[13][40][69] Exercise also reverses stress-induced decreases in BDNF expression and signaling in the brain, thereby acting as a buffer against stress-related diseases like depression.[40][66][69]		Continuous exercise can produce short-term euphoria, an affective state associated with feelings of profound contentment, elation, and well-being, which is colloquially known as a "runner's high" in distance running or a "rower's high" in rowing.[14][15][70][71] Current medical reviews indicate that several endogenous euphoriants are responsible for producing exercise-related euphoria, specifically phenethylamine (a stimulant), β-endorphin (an opioid), and anandamide (an endocannabinoid).[72][73][74][75][76]		β-Phenylethylamine, commonly referred to as phenethylamine, is a potent human trace amine and neuromodulator which functions as endogenous amphetamine.[note 8][77][78] Thirty minutes of moderate to high intensity physical exercise has been shown to induce an enormous increase in urinary β-phenylacetic acid, the primary metabolite of phenethylamine.[72][73][74] Two reviews noted a study where the mean 24 hour urinary β-phenylacetic acid concentration following just 30 minutes of intense exercise rose 77% above its base level;[72][73][74] the reviews suggest that phenethylamine synthesis sharply increases during physical exercise during which it is rapidly metabolized due to its short half-life of roughly 30 seconds.[72][73][74][79] In a resting state, phenethylamine is synthesized in catecholamine neurons from L-phenylalanine by aromatic amino acid decarboxylase at approximately the same rate at which dopamine is produced.[79]		In light of this observation, the original paper and both reviews suggest that phenethylamine plays a prominent role in mediating the mood-enhancing euphoric effects of a runner's high, as both phenethylamine and amphetamine are potent euphoriants.[72][73][74]		β-Endorphins (contracted from "endogenous morphine") are endogenous opioid neuropeptides that bind to μ-opioid receptors, in turn producing euphoria and pain relief.[75] A meta-analytic review found that exercise significantly increases the secretion of β-endorphins and that this secretion is correlated with improved mood states.[75] β-endorphins have also been found to improve sleep.[80] Moderate intensity exercise produces the greatest increase in β-endorphin synthesis, while higher and lower intensity forms of exercise are associated with smaller increases in β-endorphin synthesis.[75]		A review on β-endorphins and exercise noted that an individual's mood improves for the remainder of the day following physical exercise and that one's mood is positively correlated with overall daily physical activity level.[75] Exercise-induced improvements in mood occur in sedentary individuals, recreational exercisers, and marathon runners, but recreational athletes and marathon runners experience more pronounced mood-lifting effects from exercising.[75]		Anandamide is an endogenous cannabinoid neurotransmitter that binds to cannabinoid receptors.[76] It has been shown that aerobic exercise causes an increase in plasma anandamide levels, where the magnitude of this increase is highest at moderate exercise intensity (i.e., exercising at ~70–80% maximum heart rate).[76] Increases in plasma anandamide levels are associated with psychoactive effects because anandamide is able to cross the blood–brain barrier and act within the central nervous system.[76] Thus, because anandamide is a euphoriant and aerobic exercise is associated with euphoric effects, it has been proposed that anandamide partly mediates the short-term mood-lifting effects of exercise (e.g., the euphoria of a runner's high) via exercise-induced increases in its synthesis.[70][76]		In mice it was demonstrated that certain features of a runner's high depend on cannabinoid receptors. Pharmacological or genetic disruption of cannabinoid signaling via cannabinoid receptors prevents the analgesic and anxiety-reducing effects of running.[81][non-primary source needed]		Glutamate, one of the most common neurochemicals in the brain, is an excitatory neurotransmitter involved in many aspects of brain function, including learning and memory.[82] Exercise normalizes the cotransmission of glutamate and dopamine in the nucleus accumbens.[25] A review of the effects of exercise on neurocardiac function in preclinical models noted that exercise-induced neuroplasticity of the rostral ventrolateral medulla (RVLM) has an inhibitory effect on glutamatergic neurotransmission, in turn reducing sympathetic activity;[83] the review hypothesized that this neuroplasticity in the RVLM is a mechanism by which regular exercise prevents inactivity-related cardiovascular disease.[83]		Sibley and Etnier (2003) performed a meta-analysis that looked at the relationship between physical activity and cognitive performance in children.[84] They reported a beneficial relationship in the categories of perceptual skills, intelligence quotient, achievement, verbal tests, mathematic tests, developmental level/academic readiness and other, with the exception of memory, that was found to be unrelated to physical activity.[84] The correlation was strongest for the age ranges of 4–7 and 11–13 years.[84] On the other hand, Chaddock and colleagues (2011) found results that contrasted Sibley and Etnier's meta-analysis. In their study, the hypothesis was that lower-fit children would perform poorly in executive control of memory and have smaller hippocampal volumes compared to higher-fit children.[85] Instead of physical activity being unrelated to memory in children between 4 and 18 years of age, it may be that preadolescents of higher fitness have larger hippocampal volumes, than preadolescents of lower fitness. According to a previous study done by Chaddock and colleagues (Chaddock et al. 2010), a larger hippocampal volume would result in better executive control of memory.[86] They concluded that hippocampal volume was positively associated with performance on relational memory tasks.[86] Their findings are the first to indicate that aerobic fitness may relate to the structure and function of the preadolescent human brain.[86] In Best’s (2010) meta-analysis of the effect of activity on children’s executive function, there are two distinct experimental designs used to assess aerobic exercise on cognition. The first is chronic exercise, in which children are randomly assigned to a schedule of aerobic exercise over several weeks and later assessed at the end.[87] The second is acute exercise, which examines the immediate changes in cognitive functioning after each session.[87] The results of both suggest that aerobic exercise may briefly aid children’s executive function and also influence more lasting improvements to executive function.[87] Other studies have suggested that exercise is unrelated to academic performance, perhaps due to the parameters used to determine exactly what academic achievement is.[88] This area of study has been a focus for education boards that make decisions on whether physical education should be implemented in the school curriculum, how much time should be dedicated to physical education, and its impact on other academic subjects.[84]		Animal studies have also shown that exercise can impact brain development early on in life. Mice that had access to running wheels and other such exercise equipment had better neuronal growth in the neural systems involved in learning and memory.[88] Neuroimaging of the human brain has yielded similar results, where exercise leads to changes in brain structure and function.[88] Some investigations have linked low levels of aerobic fitness in children with impaired executive function in older adults, but there is mounting evidence it may also be associated with a lack of selective attention, response inhibition, and interference control.[85]		Clinical and preclinical evidence indicate that consistent aerobic exercise, especially endurance exercise (e.g., marathon running), actually prevents the development of certain drug addictions and is an effective adjunct treatment for drug addiction, psychostimulant addiction in particular.[24][25][26][27][28] Consistent aerobic exercise magnitude-dependently (i.e., by duration and intensity) reduces drug addiction risk, which appears to occur through the reversal of drug-induced, addiction-related neuroplasticity.[25][26] One review noted that exercise may prevent the development of drug addiction by altering ΔFosB or c-Fos immunoreactivity in the striatum or other parts of the reward system.[28] Moreover, aerobic exercise decreases psychostimulant self-administration, reduces the reinstatement (i.e., relapse) of drug-seeking, and induces opposite effects on striatal dopamine receptor D2 (DRD2) signaling (increased DRD2 density) to those induced by pathological stimulant use (decreased DRD2 density).[25][26] Consequently, consistent aerobic exercise may lead to better treatment outcomes when used as an adjunct treatment for drug addiction.[25][27] As of 2016[update], more clinical research is still needed to understand the mechanisms and confirm the efficacy of exercise in drug addiction treatment and prevention.[24][28]		Regular physical exercise, particularly aerobic exercise, is an effective add-on treatment for ADHD in children and adults, particularly when combined with stimulant medication (i.e., amphetamine or methylphenidate), although the best intensity and type of aerobic exercise for improving symptoms are not currently known.[22][23][89] In particular, the long-term effects of regular aerobic exercise in ADHD individuals include better behavior and motor abilities, improved executive functions (including attention, inhibitory control, and planning, among other cognitive domains), faster information processing speed, and better memory.[22][23][89] Parent-teacher ratings of behavioral and socio-emotional outcomes in response to regular aerobic exercise include: better overall function, reduced ADHD symptoms, better self-esteem, reduced levels of anxiety and depression, fewer somatic complaints, better academic and classroom behavior, and improved social behavior.[22] Exercising while on stimulant medication augments the effect of stimulant medication on executive function.[22] It is believed that these short-term effects of exercise are mediated by an increased abundance of synaptic dopamine and norepinephrine in the brain.[22]		A number of medical reviews have indicated that exercise has a marked and persistent antidepressant effect in humans,[5][16][17][20][90][91] an effect believed to be mediated through enhanced BDNF signaling in the brain.[8][20] Several systematic reviews have analyzed the potential for physical exercise in the treatment of depressive disorders. The 2013 Cochrane Collaboration review on physical exercise for depression noted that, based upon limited evidence, it is more effective than a control intervention and comparable to psychological or antidepressant drug therapies.[90] Three subsequent 2014 systematic reviews that included the Cochrane review in their analysis concluded with similar findings: one indicated that physical exercise is effective as an adjunct treatment (i.e., treatments that are used together) with antidepressant medication;[20] the other two indicated that physical exercise has marked antidepressant effects and recommended the inclusion of physical activity as an adjunct treatment for mild–moderate depression and mental illness in general.[16][17] One systematic review noted that yoga may be effective in alleviating symptoms of prenatal depression.[92] Another review asserted that evidence from clinical trials supports the efficacy of physical exercise as a treatment for depression over a 2–4 month period.[5]		A 2015 review of clinical evidence and medical guideline for the treatment of depression with exercise noted that the available evidence on the effectiveness of exercise therapy for depression suffers from some limitations;[21] nonetheless, it stated that there is clear evidence of efficacy for reducing symptoms of depression.[21] The review also noted that patient characteristics, the type of depressive disorder, and the nature of the exercise program all affect the antidepressant properties of exercise therapy.[21] A July 2016 meta-analysis concluded that physical exercise improves overall quality of life in individuals with depression relative to controls.[11]		Alzheimer's Disease is a cortical neurodegenerative disorder and the most prevalent form of dementia, representing approximately 65% of all cases of dementia; it is characterized by impaired cognitive function, behavioral abnormalities, and a reduced capacity to perform basic activities of daily life.[29][30] Two meta-analytic systematic reviews of randomized controlled trials with durations of 3–12 months have examined the effects of physical exercise on the aforementioned characteristics of Alzheimer's disease.[29][30] The reviews found beneficial effects of physical exercise on cognitive function, the rate of cognitive decline, and the ability to perform activities of daily living in individuals with Alzheimer's disease.[29][30] One review suggested that, based upon transgenic mouse models, the cognitive effects of exercise on Alzheimer's disease may result from a reduction in the quantity of amyloid plaque.[29][93]		The Caerphilly Prospective study followed 2,375 male subjects over 30 years and examined the association between healthy lifestyles and dementia, among other factors.[94] Analyses of the Caerphilly study data have found that exercise is associated with a lower incidence of dementia and a reduction in cognitive impairment.[94][95] A subsequent systematic review of longitudinal studies also found higher levels of physical activity to be associated with a reduction in the risk of dementia and cognitive decline;[35] this review further asserted that increased physical activity appears to be causally related with these reduced risks.[35]		Parkinson's disease (PD) is a movement disorder that produces symptoms such as bradykinesia, rigidity, shaking, and impaired gait.[96]		A review by Kramer and colleagues (2006) found that some neurotransmitter systems are affected by exercise in a positive way.[97] A few studies reported seeing an improvement in brain health and cognitive function due to exercise.[97] One particular study by Kramer and colleagues (1999) found that aerobic training improved executive control processes supported by frontal and prefrontal regions of the brain.[98] These regions are responsible for the cognitive deficits in PD patients, however there was speculation that the difference in the neurochemical environment in the frontal lobes of PD patients may inhibit the benefit of aerobic exercise.[99] Nocera and colleagues (2010) performed a case study based on this literature where they gave participants with early-to mid-staged PD, and the control group cognitive/language assessments with exercise regimens. Individuals performed 20 minutes of aerobic exercise three times a week for 8 weeks on a stationary exercise cycle. It was found that aerobic exercise improved several measures of cognitive function,[99] providing evidence that such exercise regimens may be beneficial to patients with PD.		|group2 = See also |list2 =		|below = }}		
Aerobics is a form of physical exercise that combines rhythmic aerobic exercise with stretching and strength training routines with the goal of improving all elements of fitness (flexibility, muscular strength, and cardio-vascular fitness). It is usually performed to music and may be practiced in a group setting led by an instructor (fitness professional), although it can be done solo and without musical accompaniment. With the goal of preventing illness and promoting physical fitness, practitioners perform various routines comprising a number of different dance-like exercises. Formal aerobics classes are divided into different levels of intensity and complexity. A well-balanced aerobics class will have five components: warm-up (5–10 minutes), cardio vascular conditioning (25–30 minutes), muscular strength and conditioning (10–15 minutes), cool-down (5–8 minutes) and stretching and flexibility (5–8 minutes). Aerobics classes may allow participants to select their level of participation according to their fitness level. Many gyms offer a variety of aerobic classes. Each class is designed for a certain level of experience and taught by a certified instructor with a specialty area related to their particular class.						Both the term and the specific exercise method were developed by Dr. Kenneth H. Cooper, an exercise physiologist, and Col. Pauline Potts, a physical therapist, both of the United States Air Force. Dr. Cooper, an avowed exercise enthusiast, was personally and professionally puzzled about why some people with excellent muscular strength were still prone to poor performance at tasks such as long-distance running, swimming, and bicycling. He began measuring systematic human performance using a bicycle ergometer, and began measuring sustained performance in terms of a person's ability to use oxygen. In 1968, he published Aerobics, which included exercise programs using running, walking, swimming and bicycling. The book came at a time when increasing weakness and inactivity in the general population was causing a perceived need for increased exercise.		Aerobics gained worldwide popularity after the release of Jane Fonda's exercise videos in 1982.		Step aerobics is a form of aerobic exercise using of an elevated platform (the step). The height can be tailored to individual needs by inserting risers under the step. Step aerobics classes are offered at many gyms and fitness centers which have a group exercise program.		Step aerobics was developed by Gin Miller around 1989[citation needed]. After a knee injury, Gin consulted with an orthopedic doctor, who recommended she strengthen the muscles supporting the knee by stepping up and down on a milk crate and from this she developed the step regimen.		Step aerobics can also be involved in dancing games, such as Dance Dance Revolution or In the Groove.		Often moves are referred to as Reebok step moves in reference to one of the first makers of the plastic step commonly used in gyms.		The "basic" step involves stepping one foot first on the step then the other on top of the platform then stepping the first foot back on the floor with the second following. A "right basic" would involve stepping right foot up, then the left, then returning to the floor alternating right then left.		Many instructors of step will switch immediately between different moves, for example between a right basic and a left basic without any intervening moves, forcing people to "tap" their foot instead of shifting weight. However, one form of step is called tap-free or smooth step in which feet always alternate without the ambiguous "taps" that can make learning step difficult for beginners. This requires a bit of foresight and planning by the instructor in order to insert a transitional or switching move that maintains the natural alternating weight shift akin to walking. For example, from a series of right basics one may insert a "knee up" (which involves stepping up and lifting the knee and returning the lifted leg to the ground, thereby switching feet) and then continuing to a left basic. However, this requires planning and the extra beats required for the transitional move.		Common moves include:		Many instructors will prepare a set of moves that will be executed together to form the choreography of the class. Usually, the choreography will be timed to 32 beats in a set, ideally switching legs so that the set can be repeated in a mirrored fashion. A set may consist of many different moves and the different moves may have different durations. For example, a basic step as described above takes 4 beats (for the 4 steps the person takes). Similarly, the "knee up" move also takes 4 beats. Another common move, the repeater knee, is an 8-beat move.		Classes vary in the level of choreography. Basic level classes will tend to have a series of relatively basic moves strung together into a sequence. More advanced classes incorporate dance elements such as turns, mambos, and stomps. These elements are put together into 2–3 routines in each class. One learns the routines during the class and then all are performed at the end of the class. Regardless of the complexity of the choreography, most instructors offer various options for different levels of intensity/dance ability while teaching the routines.		Freestyle aerobics is an aerobics style in which a group instructor choreographs several short dance combinations and teaches them to the class. This is usually achieved by teaching the class 1-2 movements at a time and repeating the movements until the class is able to join the whole choreography together. Aerobic music is used throughout the class. This is sometimes followed by a strength section which uses body weight exercises to strengthen muscles and a stretch routine to cool down and improve flexibility. Classes are usually 30–60 minutes in length and may include the use of equipment such as a barbell, aerobic step, or small weights.		In freestyle aerobics, the instructor choreographs the routine and adjusts it to the needs and wants of her/his class. There is often no difference between base movements in freestyle and pre-choreographed programs.		It is practiced to improve aerobic fitness, flexibility and strength.		Aerobic gymnastics, also known as sport aerobics and competitive aerobics, may combine complicated choreography, rhythmic and acrobatic gymnastics with elements of aerobics.[1] Performance is divided into categories by age, sex and groups (individual, mixed pairs and trios) and are judged on the following elements: dynamic and static strength, jumps and leaps, kicks, balance and flexibility. Ten exercises are mandatory: four consecutive high leg kicks, patterns. A maximum of ten elements from following families are allowed: push-ups, supports and balances, kicks and splits, jumps and leaps. Elements of tumbling such as handsprings, handstands, back flips, and aerial somersaults are prohibited. Scoring is by judging of artistic quality, creativity, execution, and difficulty of routines. Sport aerobics has state, national, and international competitions, but is not an olympic sport.		Like other forms of exercise, step aerobics helps burn calories and fat. The number of calories burned depends on the speed of movements, step height, length of exercise, and the persons height and weight.		|group2 = See also |list2 =		|below = }}		
The American Council on Exercise (ACE) is a nonprofit fitness certification, education and training provider with more than 60,000 [1] certified professionals who hold more than 67,000 ACE certifications. Founded in 1985, American Council on Exercise (ACE) is a nonprofit organization committed to enriching quality of life through safe and effective exercise and physical activity. The organization’s mission is to protect all segments of society against ineffective fitness products, programs and trends through independent, third-party research, education and outreach. ACE national headquarters is located in San Diego.						In 2003, American Council on Exercise was granted accreditation by the National Commission for Certifying Agencies for all four of its primary certification programs.[2]		The American Council on Exercise executive team consists of:[3] Scott Goudeseune, President and Chief Executive Officer Alex Mirnezam, Chief Financial Officer Cedric X. Bryant, Ph.D., Chief Science Officer Graham Melstrand, Executive Vice President, Engagement, Corporate Affairs Kerri O'Brien, Executive Vice President of Product and Business Development Carolann Dekker, Executive Vice President, Marketing Amanda Cass, Vice President, Finance and Human Resources				
A triangle is a polygon with three edges and three vertices. It is one of the basic shapes in geometry. A triangle with vertices A, B, and C is denoted △ A B C {\displaystyle \triangle ABC} .		In Euclidean geometry any three points, when non-collinear, determine a unique triangle and a unique plane (i.e. a two-dimensional Euclidean space). This article is about triangles in Euclidean geometry except where otherwise noted.						Triangles can be classified according to the lengths of their sides:		Hatch marks, also called tick marks, are used in diagrams of triangles and other geometric figures to identify sides of equal lengths. A side can be marked with a pattern of "ticks", short line segments in the form of tally marks; two sides have equal lengths if they are both marked with the same pattern. In a triangle, the pattern is usually no more than 3 ticks. An equilateral triangle has the same pattern on all 3 sides, an isosceles triangle has the same pattern on just 2 sides, and a scalene triangle has different patterns on all sides since no sides are equal. Similarly, patterns of 1, 2, or 3 concentric arcs inside the angles are used to indicate equal angles. An equilateral triangle has the same pattern on all 3 angles, an isosceles triangle has the same pattern on just 2 angles, and a scalene triangle has different patterns on all angles since no angles are equal.		Triangles can also be classified according to their internal angles, measured here in degrees.		A triangle that has two angles with the same measure also has two sides with the same length, and therefore it is an isosceles triangle. It follows that in a triangle where all angles have the same measure, all three sides have the same length, and such a triangle is therefore equilateral.		Triangles are assumed to be two-dimensional plane figures, unless the context provides otherwise (see Non-planar triangles, below). In rigorous treatments, a triangle is therefore called a 2-simplex (see also Polytope). Elementary facts about triangles were presented by Euclid in books 1–4 of his Elements, around 300 BC.		The sum of the measures of the interior angles of a triangle in Euclidean space is always 180 degrees.[5] This fact is equivalent to Euclid's parallel postulate. This allows determination of the measure of the third angle of any triangle given the measure of two angles. An exterior angle of a triangle is an angle that is a linear pair (and hence supplementary) to an interior angle. The measure of an exterior angle of a triangle is equal to the sum of the measures of the two interior angles that are not adjacent to it; this is the exterior angle theorem. The sum of the measures of the three exterior angles (one for each vertex) of any triangle is 360 degrees.[note 2]		Two triangles are said to be similar if every angle of one triangle has the same measure as the corresponding angle in the other triangle. The corresponding sides of similar triangles have lengths that are in the same proportion, and this property is also sufficient to establish similarity.		Some basic theorems about similar triangles are:		Two triangles that are congruent have exactly the same size and shape:[note 4] all pairs of corresponding interior angles are equal in measure, and all pairs of corresponding sides have the same length. (This is a total of six equalities, but three are often sufficient to prove congruence.)		Some individually necessary and sufficient conditions for a pair of triangles to be congruent are:		Some individually sufficient conditions are:		An important condition is:		Using right triangles and the concept of similarity, the trigonometric functions sine and cosine can be defined. These are functions of an angle which are investigated in trigonometry.		A central theorem is the Pythagorean theorem, which states in any right triangle, the square of the length of the hypotenuse equals the sum of the squares of the lengths of the two other sides. If the hypotenuse has length c, and the legs have lengths a and b, then the theorem states that		The converse is true: if the lengths of the sides of a triangle satisfy the above equation, then the triangle has a right angle opposite side c.		Some other facts about right triangles:		For all triangles, angles and sides are related by the law of cosines and law of sines (also called the cosine rule and sine rule).		The triangle inequality states that the sum of the lengths of any two sides of a triangle must be greater than or equal to the length of the third side. That sum can equal the length of the third side only in the case of a degenerate triangle, one with collinear vertices. It is not possible for that sum to be less than the length of the third side. A triangle with three given positive side lengths exists if and only if those side lengths satisfy the triangle inequality.		Three given angles form a non-degenerate triangle (and indeed an infinitude of them) if and only if both of these conditions hold: (a) each of the angles is positive, and (b) the angles sum to 180°. If degenerate triangles are permitted, angles of 0° are permitted.		Three positive angles α, β, and γ, each of them less than 180°, are the angles of a triangle if and only if any one of the following conditions holds:		the last equality applying only if none of the angles is 90° (so the tangent function's value is always finite).		There are thousands of different constructions that find a special point associated with (and often inside) a triangle, satisfying some unique property: see the article Encyclopedia of Triangle Centers for a catalogue of them. Often they are constructed by finding three lines associated in a symmetrical way with the three sides (or vertices) and then proving that the three lines meet in a single point: an important tool for proving the existence of these is Ceva's theorem, which gives a criterion for determining when three such lines are concurrent. Similarly, lines associated with a triangle are often constructed by proving that three symmetrically constructed points are collinear: here Menelaus' theorem gives a useful general criterion. In this section just a few of the most commonly encountered constructions are explained.		A perpendicular bisector of a side of a triangle is a straight line passing through the midpoint of the side and being perpendicular to it, i.e. forming a right angle with it. The three perpendicular bisectors meet in a single point, the triangle's circumcenter, usually denoted by O; this point is the center of the circumcircle, the circle passing through all three vertices. The diameter of this circle, called the circumdiameter, can be found from the law of sines stated above. The circumcircle's radius is called the circumradius.		Thales' theorem implies that if the circumcenter is located on one side of the triangle, then the opposite angle is a right one. If the circumcenter is located inside the triangle, then the triangle is acute; if the circumcenter is located outside the triangle, then the triangle is obtuse.		An altitude of a triangle is a straight line through a vertex and perpendicular to (i.e. forming a right angle with) the opposite side. This opposite side is called the base of the altitude, and the point where the altitude intersects the base (or its extension) is called the foot of the altitude. The length of the altitude is the distance between the base and the vertex. The three altitudes intersect in a single point, called the orthocenter of the triangle, usually denoted by H. The orthocenter lies inside the triangle if and only if the triangle is acute.		An angle bisector of a triangle is a straight line through a vertex which cuts the corresponding angle in half. The three angle bisectors intersect in a single point, the incenter, usually denoted by I, the center of the triangle's incircle. The incircle is the circle which lies inside the triangle and touches all three sides. Its radius is called the inradius. There are three other important circles, the excircles; they lie outside the triangle and touch one side as well as the extensions of the other two. The centers of the in- and excircles form an orthocentric system.		A median of a triangle is a straight line through a vertex and the midpoint of the opposite side, and divides the triangle into two equal areas. The three medians intersect in a single point, the triangle's centroid or geometric barycenter, usually denoted by G. The centroid of a rigid triangular object (cut out of a thin sheet of uniform density) is also its center of mass: the object can be balanced on its centroid in a uniform gravitational field. The centroid cuts every median in the ratio 2:1, i.e. the distance between a vertex and the centroid is twice the distance between the centroid and the midpoint of the opposite side.		The midpoints of the three sides and the feet of the three altitudes all lie on a single circle, the triangle's nine-point circle. The remaining three points for which it is named are the midpoints of the portion of altitude between the vertices and the orthocenter. The radius of the nine-point circle is half that of the circumcircle. It touches the incircle (at the Feuerbach point) and the three excircles.		The centroid (yellow), orthocenter (blue), circumcenter (green) and center of the nine-point circle (red point) all lie on a single line, known as Euler's line (red line). The center of the nine-point circle lies at the midpoint between the orthocenter and the circumcenter, and the distance between the centroid and the circumcenter is half that between the centroid and the orthocenter.		The center of the incircle is not in general located on Euler's line.		If one reflects a median in the angle bisector that passes through the same vertex, one obtains a symmedian. The three symmedians intersect in a single point, the symmedian point of the triangle.		There are various standard methods for calculating the length of a side or the measure of an angle. Certain methods are suited to calculating values in a right-angled triangle; more complex methods may be required in other situations.		In right triangles, the trigonometric ratios of sine, cosine and tangent can be used to find unknown angles and the lengths of unknown sides. The sides of the triangle are known as follows:		The sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse. In our case		Note that this ratio does not depend on the particular right triangle chosen, as long as it contains the angle A, since all those triangles are similar.		The cosine of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse. In our case		The tangent of an angle is the ratio of the length of the opposite side to the length of the adjacent side. In our case		The acronym "SOH-CAH-TOA" is a useful mnemonic for these ratios.		The inverse trigonometric functions can be used to calculate the internal angles for a right angled triangle with the length of any two sides.		Arcsin can be used to calculate an angle from the length of the opposite side and the length of the hypotenuse.		Arccos can be used to calculate an angle from the length of the adjacent side and the length of the hypotenuse.		Arctan can be used to calculate an angle from the length of the opposite side and the length of the adjacent side.		In introductory geometry and trigonometry courses, the notation sin−1, cos−1, etc., are often used in place of arcsin, arccos, etc. However, the arcsin, arccos, etc., notation is standard in higher mathematics where trigonometric functions are commonly raised to powers, as this avoids confusion between multiplicative inverse and compositional inverse.		The law of sines, or sine rule,[8] states that the ratio of the length of a side to the sine of its corresponding opposite angle is constant, that is		This ratio is equal to the diameter of the circumscribed circle of the given triangle. Another interpretation of this theorem is that every triangle with angles α, β and γ is similar to a triangle with side lengths equal to sin α, sin β and sin γ. This triangle can be constructed by first constructing a circle of diameter 1, and inscribing in it two of the angles of the triangle. The length of the sides of that triangle will be sin α, sin β and sin γ. The side whose length is sin α is opposite to the angle whose measure is α, etc.		The law of cosines, or cosine rule, connects the length of an unknown side of a triangle to the length of the other sides and the angle opposite to the unknown side.[8] As per the law:		For a triangle with length of sides a, b, c and angles of α, β, γ respectively, given two known lengths of a triangle a and b, and the angle between the two known sides γ (or the angle opposite to the unknown side c), to calculate the third side c, the following formula can be used:		If the lengths of all three sides of any triangle are known the three angles can be calculated:		The law of tangents, or tangent rule, can be used to find a side or an angle when two sides and an angle or two angles and a side are known. It states that:[9]		"Solution of triangles" is the main trigonometric problem: to find missing characteristics of a triangle (three angles, the lengths of the three sides etc.) when at least three of these characteristics are given. The triangle can be located on a plane or on a sphere. This problem often occurs in various trigonometric applications, such as geodesy, astronomy, construction, navigation etc.		Calculating the area T of a triangle is an elementary problem encountered often in many different situations. The best known and simplest formula is:		where b is the length of the base of the triangle, and h is the height or altitude of the triangle. The term "base" denotes any side, and "height" denotes the length of a perpendicular from the vertex opposite the side onto the line containing the side itself. In 499 CE Aryabhata, a great mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, used this method in the Aryabhatiya (section 2.6).		Although simple, this formula is only useful if the height can be readily found, which is not always the case. For example, the surveyor of a triangular field might find it relatively easy to measure the length of each side, but relatively difficult to construct a 'height'. Various methods may be used in practice, depending on what is known about the triangle. The following is a selection of frequently used formulae for the area of a triangle.[10]		The height of a triangle can be found through the application of trigonometry.		Knowing SAS: Using the labels in the image on the right, the altitude is h = a sin γ {\displaystyle \gamma } . Substituting this in the formula T = 1 2 b h {\displaystyle T={\frac {1}{2}}bh} derived above, the area of the triangle can be expressed as:		(where α is the interior angle at A, β is the interior angle at B, γ {\displaystyle \gamma } is the interior angle at C and c is the line AB).		Furthermore, since sin α = sin (π − α) = sin (β + γ {\displaystyle \gamma } ), and similarly for the other two angles:		Knowing AAS:		and analogously if the known side is a or c.		Knowing ASA:[11]		and analogously if the known side is b or c.		The shape of the triangle is determined by the lengths of the sides. Therefore, the area can also be derived from the lengths of the sides. By Heron's formula:		where s = a + b + c 2 {\displaystyle s={\tfrac {a+b+c}{2}}} is the semiperimeter, or half of the triangle's perimeter.		Three other equivalent ways of writing Heron's formula are		The area of a parallelogram embedded in a three-dimensional Euclidean space can be calculated using vectors. Let vectors AB and AC point respectively from A to B and from A to C. The area of parallelogram ABDC is then		which is the magnitude of the cross product of vectors AB and AC. The area of triangle ABC is half of this,		The area of triangle ABC can also be expressed in terms of dot products as follows:		In two-dimensional Euclidean space, expressing vector AB as a free vector in Cartesian space equal to (x1,y1) and AC as (x2,y2), this can be rewritten as:		If vertex A is located at the origin (0, 0) of a Cartesian coordinate system and the coordinates of the other two vertices are given by B = (xB, yB) and C = (xC, yC), then the area can be computed as  1⁄2 times the absolute value of the determinant		For three general vertices, the equation is:		which can be written as		If the points are labeled sequentially in the counterclockwise direction, the above determinant expressions are positive and the absolute value signs can be omitted.[12] The above formula is known as the shoelace formula or the surveyor's formula.		If we locate the vertices in the complex plane and denote them in counterclockwise sequence as a = xA + yAi, b = xB + yBi, and c = xC + yCi, and denote their complex conjugates as a ¯ {\displaystyle {\bar {a}}} , b ¯ {\displaystyle {\bar {b}}} , and c ¯ {\displaystyle {\bar {c}}} , then the formula		is equivalent to the shoelace formula.		In three dimensions, the area of a general triangle A = (xA, yA, zA), B = (xB, yB, zB) and C = (xC, yC, zC) is the Pythagorean sum of the areas of the respective projections on the three principal planes (i.e. x = 0, y = 0 and z = 0):		The area within any closed curve, such as a triangle, is given by the line integral around the curve of the algebraic or signed distance of a point on the curve from an arbitrary oriented straight line L. Points to the right of L as oriented are taken to be at negative distance from L, while the weight for the integral is taken to be the component of arc length parallel to L rather than arc length itself.		This method is well suited to computation of the area of an arbitrary polygon. Taking L to be the x-axis, the line integral between consecutive vertices (xi,yi) and (xi+1,yi+1) is given by the base times the mean height, namely (xi+1 − xi)(yi + yi+1)/2. The sign of the area is an overall indicator of the direction of traversal, with negative area indicating counterclockwise traversal. The area of a triangle then falls out as the case of a polygon with three sides.		While the line integral method has in common with other coordinate-based methods the arbitrary choice of a coordinate system, unlike the others it makes no arbitrary choice of vertex of the triangle as origin or of side as base. Furthermore, the choice of coordinate system defined by L commits to only two degrees of freedom rather than the usual three, since the weight is a local distance (e.g. xi+1 − xi in the above) whence the method does not require choosing an axis normal to L.		When working in polar coordinates it is not necessary to convert to Cartesian coordinates to use line integration, since the line integral between consecutive vertices (ri,θi) and (ri+1,θi+1) of a polygon is given directly by riri+1sin(θi+1 − θi)/2. This is valid for all values of θ, with some decrease in numerical accuracy when |θ| is many orders of magnitude greater than π. With this formulation negative area indicates clockwise traversal, which should be kept in mind when mixing polar and cartesian coordinates. Just as the choice of y-axis (x = 0) is immaterial for line integration in cartesian coordinates, so is the choice of zero heading (θ = 0) immaterial here.		Three formulas have the same structure as Heron's formula but are expressed in terms of different variables. First, denoting the medians from sides a, b, and c respectively as ma, mb, and mc and their semi-sum (ma + mb + mc)/2 as σ, we have[13]		Next, denoting the altitudes from sides a, b, and c respectively as ha, hb, and hc, and denoting the semi-sum of the reciprocals of the altitudes as H = ( h a − 1 + h b − 1 + h c − 1 ) / 2 {\displaystyle H=(h_{a}^{-1}+h_{b}^{-1}+h_{c}^{-1})/2} we have[14]		And denoting the semi-sum of the angles' sines as S = [(sin α) + (sin β) + (sin γ)]/2, we have[15]		where D is the diameter of the circumcircle: D = a sin ⁡ α = b sin ⁡ β = c sin ⁡ γ . {\displaystyle D={\tfrac {a}{\sin \alpha }}={\tfrac {b}{\sin \beta }}={\tfrac {c}{\sin \gamma }}.}		See Pick's theorem for a technique for finding the area of any arbitrary lattice polygon (one drawn on a grid with vertically and horizontally adjacent lattice points at equal distances, and with vertices on lattice points).		The theorem states:		where I {\displaystyle I} is the number of internal lattice points and B is the number of lattice points lying on the border of the polygon.		Numerous other area formulas exist, such as		where r is the inradius, and s is the semiperimeter (in fact this formula holds for all tangential polygons), and[16]:Lemma 2		where r a , r b , r c {\displaystyle r_{a},\,r_{b},\,r_{c}} are the radii of the excircles tangent to sides a, b, c respectively.		We also have		and[17]		for circumdiameter D; and[18]		for angle α ≠ 90°.		The area can also be expressed as[19]		In 1885, Baker[20] gave a collection of over a hundred distinct area formulas for the triangle. These include:		for circumradius (radius of the circumcircle) R, and		The area T of any triangle with perimeter p satisfies		with equality holding if and only if the triangle is equilateral.[21][22]:657		Other upper bounds on the area T are given by[23]:p.290		and		both again holding if and only if the triangle is equilateral.		There are infinitely many lines that bisect the area of a triangle.[24] Three of them are the medians, which are the only area bisectors that go through the centroid. Three other area bisectors are parallel to the triangle's sides.		Any line through a triangle that splits both the triangle's area and its perimeter in half goes through the triangle's incenter. There can be one, two, or three of these for any given triangle.		The formulas in this section are true for all Euclidean triangles.		The medians and the sides are related by[25]:p.70		and		and equivalently for mb and mc.		For angle A opposite side a, the length of the internal angle bisector is given by[26]		for semiperimeter s, where the bisector length is measured from the vertex to where it meets the opposite side.		The interior perpendicular bisectors are given by		where the sides are a ≥ b ≥ c {\displaystyle a\geq b\geq c} and the area is T . {\displaystyle T.} [27]:Thm 2		The altitude from, for example, the side of length a is		The following formulas involve the circumradius R and the inradius r:		where ha etc. are the altitudes to the subscripted sides;[25]:p.79		and		The product of two sides of a triangle equals the altitude to the third side times the diameter D of the circumcircle:[25]:p.64		Suppose two adjacent but non-overlapping triangles share the same side of length f and share the same circumcircle, so that the side of length f is a chord of the circumcircle and the triangles have side lengths (a, b, f) and (c, d, f), with the two triangles together forming a cyclic quadrilateral with side lengths in sequence (a, b, c, d). Then[28]:84		Let G be the centroid of a triangle with vertices A, B, and C, and let P be any interior point. Then the distances between the points are related by[28]:174		The sum of the squares of the triangle's sides equals three times the sum of the squared distances of the centroid from the vertices:		Let qa, qb, and qc be the distances from the centroid to the sides of lengths a, b, and c. Then[28]:173		and		for area T.		Carnot's Theorem states that the sum of the distances from the circumcenter to the three sides equals the sum of the circumradius and the inradius.[25]:p.83 Here a segment's length is considered to be negative if and only if the segment lies entirely outside the triangle. This method is especially useful for deducing the properties of more abstract forms of triangles, such as the ones induced by Lie algebras, that otherwise have the same properties as usual triangles.		Euler's theorem states that the distance d between the circumcenter and the incenter is given by[25]:p.85		or equivalently		where R is the circumradius and r is the inradius. Thus for all triangles R ≥ 2r, with equality holding for equilateral triangles.		If we denote that the orthocenter divides one altitude into segments of lengths u and v, another altitude into segment lengths w and x, and the third altitude into segment lengths y and z, then uv = wx = yz.[25]:p.94		The distance from a side to the circumcenter equals half the distance from the opposite vertex to the orthocenter.[25]:p.99		The sum of the squares of the distances from the vertices to the orthocenter H plus the sum of the squares of the sides equals twelve times the square of the circumradius:[25]:p.102		In addition to the law of sines, the law of cosines, the law of tangents, and the trigonometric existence conditions given earlier, for any triangle		Morley's trisector theorem states that in any triangle, the three points of intersection of the adjacent angle trisectors form an equilateral triangle, called the Morley triangle.		As discussed above, every triangle has a unique inscribed circle (incircle) that is interior to the triangle and tangent to all three sides.		Every triangle has a unique Steiner inellipse which is interior to the triangle and tangent at the midpoints of the sides. Marden's theorem shows how to find the foci of this ellipse.[30] This ellipse has the greatest area of any ellipse tangent to all three sides of the triangle.		The Mandart inellipse of a triangle is the ellipse inscribed within the triangle tangent to its sides at the contact points of its excircles.		For any ellipse inscribed in a triangle ABC, let the foci be P and Q. Then[31]		Every convex polygon with area T can be inscribed in a triangle of area at most equal to 2T. Equality holds (exclusively) for a parallelogram.[32]		The Lemoine hexagon is a cyclic hexagon with vertices given by the six intersections of the sides of a triangle with the three lines that are parallel to the sides and that pass through its symmedian point. In either its simple form or its self-intersecting form, the Lemoine hexagon is interior to the triangle with two vertices on each side of the triangle.		Every acute triangle has three inscribed squares (squares in its interior such that all four of a square's vertices lie on a side of the triangle, so two of them lie on the same side and hence one side of the square coincides with part of a side of the triangle). In a right triangle two of the squares coincide and have a vertex at the triangle's right angle, so a right triangle has only two distinct inscribed squares. An obtuse triangle has only one inscribed square, with a side coinciding with part of the triangle's longest side. Within a given triangle, a longer common side is associated with a smaller inscribed square. If an inscribed square has side of length qa and the triangle has a side of length a, part of which side coincides with a side of the square, then qa, a, the altitude ha from the side a, and the triangle's area T are related according to[33][34]		The largest possible ratio of the area of the inscribed square to the area of the triangle is 1/2, which occurs when a2 = 2T, q = a/2, and the altitude of the triangle from the base of length a is equal to a. The smallest possible ratio of the side of one inscribed square to the side of another in the same non-obtuse triangle is 2 2 / 3 = 0.94.... {\displaystyle 2{\sqrt {2}}/3=0.94....} [34] Both of these extreme cases occur for the isosceles right triangle.		From an interior point in a reference triangle, the nearest points on the three sides serve as the vertices of the pedal triangle of that point. If the interior point is the circumcenter of the reference triangle, the vertices of the pedal triangle are the midpoints of the reference triangle's sides, and so the pedal triangle is called the midpoint triangle or medial triangle. The midpoint triangle subdivides the reference triangle into four congruent triangles which are similar to the reference triangle.		The Gergonne triangle or intouch triangle of a reference triangle has its vertices at the three points of tangency of the reference triangle's sides with its incircle. The extouch triangle of a reference triangle has its vertices at the points of tangency of the reference triangle's excircles with its sides (not extended).		The tangential triangle of a reference triangle (other than a right triangle) is the triangle whose sides are on the tangent lines to the reference triangle's circumcircle at its vertices.		As mentioned above, every triangle has a unique circumcircle, a circle passing through all three vertices, whose center is the intersection of the perpendicular bisectors of the triangle's sides.		Further, every triangle has a unique Steiner circumellipse, which passes through the triangle's vertices and has its center at the triangle's centroid. Of all ellipses going through the triangle's vertices, it has the smallest area.		The Kiepert hyperbola is the unique conic which passes through the triangle's three vertices, its centroid, and its circumcenter.		Of all triangles contained in a given convex polygon, there exists a triangle with maximal area whose vertices are all vertices of the given polygon.[35]		One way to identify locations of points in (or outside) a triangle is to place the triangle in an arbitrary location and orientation in the Cartesian plane, and to use Cartesian coordinates. While convenient for many purposes, this approach has the disadvantage of all points' coordinate values being dependent on the arbitrary placement in the plane.		Two systems avoid that feature, so that the coordinates of a point are not affected by moving the triangle, rotating it, or reflecting it as in a mirror, any of which give a congruent triangle, or even by rescaling it to give a similar triangle:		A non-planar triangle is a triangle which is not contained in a (flat) plane. Some examples of non-planar triangles in non-Euclidean geometries are spherical triangles in spherical geometry and hyperbolic triangles in hyperbolic geometry.		While the measures of the internal angles in planar triangles always sum to 180°, a hyperbolic triangle has measures of angles that sum to less than 180°, and a spherical triangle has measures of angles that sum to more than 180°. A hyperbolic triangle can be obtained by drawing on a negatively curved surface, such as a saddle surface, and a spherical triangle can be obtained by drawing on a positively curved surface such as a sphere. Thus, if one draws a giant triangle on the surface of the Earth, one will find that the sum of the measures of its angles is greater than 180°; in fact it will be between 180° and 540°.[36] In particular it is possible to draw a triangle on a sphere such that the measure of each of its internal angles is equal to 90°, adding up to a total of 270°.		Specifically, on a sphere the sum of the angles of a triangle is		where f is the fraction of the sphere's area which is enclosed by the triangle. For example, suppose that we draw a triangle on the Earth's surface with vertices at the North Pole, at a point on the equator at 0° longitude, and a point on the equator at 90° West longitude. The great circle line between the latter two points is the equator, and the great circle line between either of those points and the North Pole is a line of longitude; so there are right angles at the two points on the equator. Moreover, the angle at the North Pole is also 90° because the other two vertices differ by 90° of longitude. So the sum of the angles in this triangle is 90° + 90° + 90° = 270°. The triangle encloses 1/4 of the northern hemisphere (90°/360° as viewed from the North Pole) and therefore 1/8 of the Earth's surface, so in the formula f = 1/8; thus the formula correctly gives the sum of the triangle's angles as 270°.		From the above angle sum formula we can also see that the Earth's surface is locally flat: If we draw an arbitrarily small triangle in the neighborhood of one point on the Earth's surface, the fraction f of the Earth's surface which is enclosed by the triangle will be arbitrarily close to zero. In this case the angle sum formula simplifies to 180°, which we know is what Euclidean geometry tells us for triangles on a flat surface.		Rectangles have been the most popular and common geometric form for buildings since the shape is easy to stack and organize; as a standard, it is easy to design furniture and fixtures to fit inside rectangularly shaped buildings. But triangles, while more difficult to use conceptually, provide a great deal of strength. As computer technology helps architects design creative new buildings, triangular shapes are becoming increasingly prevalent as parts of buildings and as the primary shape for some types of skyscrapers as well as building materials. In Tokyo in 1989, architects had wondered whether it was possible to build a 500-story tower to provide affordable office space for this densely packed city, but with the danger to buildings from earthquakes, architects considered that a triangular shape would have been necessary if such a building was ever to have been built (it hasn't by 2011).[37]		In New York City, as Broadway crisscrosses major avenues, the resulting blocks are cut like triangles, and buildings have been built on these shapes; one such building is the triangularly shaped Flatiron Building which real estate people admit has a "warren of awkward spaces that do not easily accommodate modern office furniture" but that has not prevented the structure from becoming a landmark icon.[38] Designers have made houses in Norway using triangular themes.[39] Triangle shapes have appeared in churches[40] as well as public buildings including colleges[41] as well as supports for innovative home designs.[42]		Triangles are sturdy; while a rectangle can collapse into a parallelogram from pressure to one of its points, triangles have a natural strength which supports structures against lateral pressures. A triangle will not change shape unless its sides are bent or extended or broken or if its joints break; in essence, each of the three sides supports the other two. A rectangle, in contrast, is more dependent on the strength of its joints in a structural sense. Some innovative designers have proposed making bricks not out of rectangles, but with triangular shapes which can be combined in three dimensions.[43] It is likely that triangles will be used increasingly in new ways as architecture increases in complexity. It is important to remember that triangles are strong in terms of rigidity, but while packed in a tessellating arrangement triangles are not as strong as hexagons under compression (hence the prevalence of hexagonal forms in nature). Tessellated triangles still maintain superior strength for cantilevering however, and this is the basis for one of the strongest man made structures, the tetrahedral truss.		
Menopause, also known as the climacteric, is the time in most women's lives when menstrual periods stop permanently, and they are no longer able to bear children.[2][8] Menopause typically occurs between 49 and 52 years of age.[3] Medical professionals often define menopause as having occurred when a woman has not had any vaginal bleeding for a year.[4] It may also be defined by a decrease in hormone production by the ovaries.[9] In those who have had surgery to remove their uterus but still have ovaries, menopause may be viewed to have occurred at the time of the surgery or when their hormone levels fell.[9] Following the removal of the uterus, symptoms typically occur earlier, at an average of 45 years of age.[10]		Before menopause, a woman's periods typically become irregular, which means that periods may be longer or shorter in duration or be lighter or heavier in the amount of flow. During this time, women often experience hot flashes; these typically last from 30 seconds to ten minutes and may be associated with shivering, sweating, and reddening of the skin.[11] Hot flashes often stop occurring after a year or two.[8] Other symptoms may include vaginal dryness, trouble sleeping, and mood changes.[11] The severity of symptoms varies between women.[8] While menopause is often thought to be linked to an increase in heart disease, this primarily occurs due to increasing age and does not have a direct relationship with menopause. In some women, problems that were present like endometriosis or painful periods will improve after menopause.[8]		Menopause is usually a natural change.[5] It can occur earlier in those who smoke tobacco.[4][12] Other causes include surgery that removes both ovaries or some types of chemotherapy.[4] At the physiological level, menopause happens because of a decrease in the ovaries' production of the hormones estrogen and progesterone.[2] While typically not needed, a diagnosis of menopause can be confirmed by measuring hormone levels in the blood or urine.[13] Menopause is the opposite of menarche, the time when a girl's periods start.[14]		Specific treatment is not usually needed. Some symptoms, however, may be improved with treatment. With respect to hot flashes, avoiding smoking, caffeine, and alcohol is often recommended. Sleeping in a cool room and using a fan may help.[6] The following medications may help: menopausal hormone therapy (MHT), clonidine, gabapentin, or selective serotonin reuptake inhibitors.[6][7] Exercise may help with sleeping problems. While MHT was once routinely prescribed, it is now only recommended in those with significant symptoms, as there are concerns about side effects.[6] High-quality evidence for the effectiveness of alternative medicine has not been found.[8] There is tentative evidence for phytoestrogens.[15]		During early menopause transition, the menstrual cycles remain regular but the interval between cycles begins to lengthen. Hormone levels begin to fluctuate. Ovulation may not occur with each cycle.[16]		The date of the final menstrual period is usually taken as the point when menopause has occurred.[16] During the menopausal transition and after menopause, women can experience a wide range of symptoms.		During the transition to menopause, menstrual patterns can show shorter cycling (by 2–7 days);[16] longer cycles remain possible.[16] There may be irregular bleeding (lighter, heavier, spotting).[16] Dysfunctional uterine bleeding is often experienced by women approaching menopause due to the hormonal changes that accompany the menopause transition. Spotting or bleeding may simply be related to vaginal atrophy, a benign sore (polyp or lesion), or may be a functional endometrial response. The European Menopause and Andropause Society has released guidelines for assessment of the endometrium, which is usually the main source of spotting or bleeding.[17]		In post-menopausal women, however, any genital bleeding is an alarming symptom that requires an appropriate study to rule out the possibility of malignant diseases.		Symptoms that may appear during menopause and continue through postmenopause include:		Other physical symptoms of menopause include lack of energy, joint soreness, stiffness,[16] back pain,[16] breast enlargement,[16] breast pain,[16] heart palpitations,[16] headache,[16] dizziness,[16] dry, itchy skin,[16] thinning, tingling skin, weight gain,[16] urinary incontinence,[16][18] urinary urgency,[16] interrupted sleeping patterns,[16][19][20][21] heavy night sweats,[16] hot flashes.[16]		Psychological symptoms include anxiety,[22] poor memory,[16] inability to concentrate,[16] depressive mood,[16][22] irritability,[16] mood swings,[16] less interest in sexual activity.[16]		Menopause confers:		Women who experience menopause before 45 years of age have an increased risk of heart disease, death,[28] and impaired lung function.[26]		In the Western world, the typical age of menopause (last period from natural causes) is between 40 and 61[29] and the average age for last period is 51 years.[30] The average age of natural menopause in Australia is 51.7 years.[31] In India and the Philippines, the median age of natural menopause is considerably earlier, at 44 years.[32]		In rare cases, a woman's ovaries stop working at a very early age, ranging anywhere from the age of puberty to age 40. This is known as premature ovarian failure and affects 1 to 2% of women by age 40.[33]		Undiagnosed and untreated coeliac disease is a risk factor for early menopause. Coeliac disease can present with several non-gastrointestinal symptoms, in the absence of gastrointestinal symptoms, and most cases escape timely recognition and go undiagnosed, leading to a risk of long-term complications. A strict gluten-free diet reduces the risk. Women with early diagnosis and treatment of coeliac disease present a normal duration of fertile life span.[34][35]		Women who have undergone hysterectomy with ovary conservation go through menopause on average 3.7 years earlier than the expected age. Other factors that can promote an earlier onset of menopause (usually 1 to 3 years early) are smoking cigarettes or being extremely thin.[36]		Premature ovarian failure (POF) is the cessation of the ovarian function before the age of 40 years.[37][38] It is diagnosed or confirmed by high blood levels of follicle stimulating hormone (FSH) and luteinizing hormone (LH) on at least three occasions at least four weeks apart.[39]		Known causes of premature ovarian failure include autoimmune disorders, thyroid disease, diabetes mellitus, chemotherapy, being a carrier of the fragile X syndrome gene, and radiotherapy.[38] However, in about 50–80% of spontaneous cases of premature ovarian failure, the cause is unknown, i.e., it is generally idiopathic.[37][39]		Women who have a functional disorder affecting the reproductive system (e.g., endometriosis, polycystic ovary syndrome, cancer of the reproductive organs) can go into menopause at a younger age than the normal timeframe. The functional disorders often significantly speed up the menopausal process.		An early menopause can be related to cigarette smoking, higher body mass index, racial and ethnic factors, illnesses, and the surgical removal of the ovaries, with or without the removal of the uterus.[40]		Rates of premature menopause have been found to be significantly higher in fraternal and identical twins; approximately 5% of twins reach menopause before the age of 40. The reasons for this are not completely understood. Transplants of ovarian tissue between identical twins have been successful in restoring fertility.		Menopause can be surgically induced by bilateral oophorectomy (removal of ovaries), which is often, but not always, done in conjunction with removal of the Fallopian tubes (salpingo-oophorectomy) and uterus (hysterectomy).[41] Cessation of menses as a result of removal of the ovaries is called "surgical menopause". The sudden and complete drop in hormone levels usually produces extreme withdrawal symptoms such as hot flashes, etc.		Removal of the uterus without removal of the ovaries does not directly cause menopause, although pelvic surgery of this type can often precipitate a somewhat earlier menopause, perhaps because of a compromised blood supply to the ovaries.[citation needed]		The menopausal transition, and postmenopause itself, is a natural change, not usually a disease state or a disorder. The main cause of this transition is the natural depletion and aging of the finite amount of oocytes (ovarian reserve). This process is sometimes accelerated by other conditions and is known to occur earlier after a wide range of gynecologic procedures such as hysterectomy (with and without ovariectomy), endometrial ablation and uterine artery embolisation. The depletion of the ovarian reserve causes an increase in circulating follicle-stimulating hormone (FSH) and luteinizing hormone (LH) levels because there are fewer oocytes and follicles responding to these hormones and producing estrogen.		The transition has a variable degree of effects.[42]		The stages of the menopause transition have been classified according to a woman's reported bleeding pattern, supported by changes in the pituitary follicle-stimulating hormone (FSH) levels.[43]		In younger women, during a normal menstrual cycle the ovaries produce estradiol, testosterone and progesterone in a cyclical pattern under the control of FSH and luteinising hormone (LH) which are both produced by the pituitary gland. During perimenopause (approaching menopause), estradiol levels and patterns of production remain relatively unchanged or may increase compared to young women, but the cycles become frequently shorter or irregular.[44] The often observed increase in estrogen is presumed to be in response to elevated FSH levels that, in turn, is hypothesized to be caused by decreased feedback by inhibin.[45] Similarly, decreased inhibin feedback after hysterectomy is hypothesized to contribute to increased ovarian stimulation and earlier menopause.[46][47]		The menopausal transition is characterized by marked, and often dramatic, variations in FSH and estradiol levels. Because of this, measurements of these hormones are not considered to be reliable guides to a woman's exact menopausal status.[48]		Menopause occurs because of the sharp decrease of estradiol and progesterone production by the ovaries. After menopause, estrogen continues to be produced mostly by aromatase in fat tissues and is produced in small amounts in many other tissues such as ovaries, bone, blood vessels, and the brain where it acts locally.[49] The substantial fall in circulating estradiol levels at menopause impacts many tissues, from brain to skin.		In contrast to the sudden fall in estradiol during menopause, the levels of total and free testosterone, as well as dehydroepiandrosterone sulfate (DHEAS) and androstenedione appear to decline more or less steadily with age. An effect of natural menopause on circulating androgen levels has not been observed.[50] Thus specific tissue effects of natural menopause cannot be attributed to loss of androgenic hormone production.[51]		Hot flashes and other vasomotor symptoms accompany the menopausal transition. While many sources continue to claim that hot flashes during the menopausal transition are caused by low estrogen levels, this assertion was shown incorrect in 1935 and, in most cases, hot flashes are observed despite elevated estrogen levels. The exact cause of these symptoms is not yet understood, possible factors considered are higher and erratic variation of estradiol level during the cycle, elevated FSH levels which may indicate hypothalamic dysregulation perhaps caused by missing feedback by inhibin. It has been also observed that the vasomotor symptoms differ during early perimenopause and late menopausal transition and it is possible that they are caused by a different mechanism.[44]		Long-term effects of menopause may include osteoporosis, vaginal atrophy as well as changed metabolic profile resulting in cardiac risks.		Decreased inhibin feedback after hysterectomy is hypothesized to contribute to increased ovarian stimulation and earlier menopause. Hastened ovarian aging has been observed after endometrial ablation. While it is difficult to prove that these surgeries are causative, it has been hypothesized that the endometrium may be producing endocrine factors contributing to the endocrine feedback and regulation of the ovarian stimulation. Elimination of this factors contributes to faster depletion of the ovarian reserve. Reduced blood supply to the ovaries that may occur as a consequence of hysterectomy and uterine artery embolisation has been hypothesized to contribute to this effect.[46][47]		Impaired DNA repair mechanisms may contribute to earlier depletion of the ovarian reserve during aging.[52] As women age, double-strand breaks accumulate in the DNA of their primordial follicles. Primordial follicles are immature primary oocytes surrounded by a single layer of granulosa cells. An enzyme system is present in oocytes that ordinarily accurately repairs DNA double-strand breaks. This repair system is called "homologous recombinational repair", and it is especially effective during meiosis. Meiosis is the general process by which germ cells are formed in all sexual eukaryotes; it appears to be an adaptation for efficiently removing damages in germ line DNA.[53] (See Meiosis.)		Human primary oocytes are present at an intermediate stage of meiosis, termed prophase I (see Oogenesis). Expression of four key DNA repair genes that are necessary for homologous recombinational repair during meiosis (BRCA1, MRE11, Rad51, and ATM) decline with age in oocytes.[52] This age-related decline in ability to repair DNA double-strand damages can account for the accumulation of these damages, that then likely contributes to the depletion of the ovarian reserve.		One way of assessing the impact on women of some of these menopause effects are the Greene climacteric scale questionnaire,[54] the Cervantes scale[55] and the Menopause rating scale.[19]		Premenopause is a term used to mean the years leading up to the last period, when the levels of reproductive hormones are becoming more variable and lower, and the effects of hormone withdrawal are present.[41] Premenopause starts some time before the monthly cycles become noticeably irregular in timing.[56]		The term "perimenopause", which literally means "around the menopause", refers to the menopause transition years, a time before and after the date of the final episode of flow. According to the North American Menopause Society, this transition can last for four to eight years.[57] The Centre for Menstrual Cycle and Ovulation Research describes it as a six- to ten-year phase ending 12 months after the last menstrual period.[58]		During perimenopause, estrogen levels average about 20–30% higher than during premenopause, often with wide fluctuations.[58] These fluctuations cause many of the physical changes during perimenopause as well as menopause.[59] Some of these changes are hot flashes, night sweats, difficulty sleeping, vaginal dryness or atrophy, incontinence, osteoporosis, and heart disease.[58] During this period, fertility diminishes but is not considered to reach zero until the official date of menopause. The official date is determined retroactively, once 12 months have passed after the last appearance of menstrual blood.		The menopause transition typically begins between 40 and 50 years of age (average 47.5).[60][61] The duration of perimenopause may be for up to eight years.[61] Women will often, but not always, start these transitions (perimenopause and menopause) about the same time as their mother did.[62]		In some women, menopause may bring about a sense of loss related to the end of fertility. In addition, this change often occurs when other stressors may be present in a woman's life:		Some research appears to show that melatonin supplementation in perimenopausal women can improve thyroid function and gonadotropin levels, as well as restoring fertility and menstruation and preventing depression associated with menopause.[63]		The term "postmenopausal" describes women who have not experienced any menstrual flow for a minimum of 12 months, assuming that they have a uterus and are not pregnant or lactating.[41] In women without a uterus, menopause or postmenopause can be identified by a blood test showing a very high FSH level. Thus postmenopause is the time in a woman's life that takes place after her last period or, more accurately, after the point when her ovaries become inactive.		The reason for this delay in declaring postmenopause is because periods are usually erratic at this time of life. Therefore, a reasonably long stretch of time is necessary to be sure that the cycling has ceased. At this point a woman is considered infertile; however, the possibility of becoming pregnant has usually been very low (but not quite zero) for a number of years before this point is reached.		A woman's reproductive hormone levels continue to drop and fluctuate for some time into post-menopause, so hormone withdrawal effects such as hot flashes may take several years to disappear.		A period-like flow during postmenopause, even spotting, may be a sign of endometrial cancer.		Perimenopause is a natural stage of life. It is not a disease or a disorder. Therefore, it does not automatically require any kind of medical treatment. However, in those cases where the physical, mental, and emotional effects of perimenopause are strong enough that they significantly disrupt the life of the woman experiencing them, palliative medical therapy may sometimes be appropriate.		In the context of the menopause, hormone replacement therapy (HRT) is the use of estrogen in women without a uterus and estrogen plus progestin in women who have an intact uterus.[64]		HRT may be reasonable for the treatment of menopausal symptoms, such as hot flashes.[65] It is the most effective treatment option.[66] Its use, however, appears to increase the risk of strokes and blood clots.[67] When used for menopausal symptoms some recommend it be used for the shortest time possible and at the lowest dose possible.[67] Evidence to support long term use however is poor.[66]		It also appears effective for preventing bone loss and osteoporotic fracture,[68]  but it is generally recommended only for women at significant risk for whom other therapies are unsuitable.[69]		HRT may be unsuitable for some women, including those at increased risk of cardiovascular disease, increased risk of thromboembolic disease (such as those with obesity or a history of venous thrombosis) or increased risk of some types of cancer.[69] There is some concern that this treatment increases the risk of breast cancer.[70]		Adding testosterone to hormone therapy has a positive effect on sexual function in postmenopausal women, although it may be accompanied by hair growth, acne and a reduction in high-density lipoprotein (HDL) cholesterol.[71] These side effects diverge depending on the doses and methods of using testosterone.[71]		SERMs are a category of drugs, either synthetically produced or derived from a botanical source, that act selectively as agonists or antagonists on the estrogen receptors throughout the body. The most commonly prescribed SERMs are raloxifene and tamoxifen. Raloxifene exhibits oestrogen agonist activity on bone and lipids, and antagonist activity on breast and the endometrium.[72] Tamoxifen is in widespread use for treatment of hormone sensitive breast cancer. Raloxifene prevents vertebral fractures in postmenopausal, osteoporotic women and reduces the risk of invasive breast cancer.[73]		Some of the SSRIs and SNRIs appear to provide some relief.[7] Low dose paroxetine has been FDA-approved for hot moderate-to-severe vasomotor symptoms associated with menopause.[74] They may, however, be associated with sleeping problems.[7]		Gabapentin or clonidine may help but does not work as well as hormone therapy.[7] Clonidine may be associated with constipation and sleeping problems.[7]		There is no evidence of consistent benefit of alternative therapies for menopausal symptoms despite their popularity.[75] The effect of soy isoflavones on menopausal symptoms is promising for reduction of hot flashes and vaginal dryness.[15][76] Evidence does not support a benefit from phytoestrogens such as coumestrol,[77] femarelle,[78] or the non-phytoestrogen black cohosh.[15][79] There is no evidence to support the efficacy of acupuncture as a management for menopausal symptoms.[80] As of 2011 there is no support for herbal or dietary supplements in the prevention or treatment of the mental changes that occur around menopause.[81] A 2016 Cochrane review found not enough evidence to show a difference between Chinese herbal medicine and placebo for the vasomotor symptoms.[82]		The cultural context within which a woman lives can have a significant impact on the way she experiences the menopausal transition. Menopause has been described as a subjective experience, with social and cultural factors playing a prominent role in the way menopause is experienced and perceived.		Within the United States, social location affects the way women perceive menopause and its related biological effects. Research indicates that whether a woman views menopause as a medical issue or an expected life change is correlated with her socio-economic status.[84] The paradigm within which a woman considers menopause influences the way she views it: Women who understand menopause as a medical condition rate it significantly more negatively than those who view it as a life transition or a symbol of aging.[85]		Ethnicity and geography play roles in the experience of menopause. American women of different ethnicities report significantly different types of menopausal effects. One major study found Caucasian women most likely to report what are sometimes described as psychosomatic symptoms, while African-American women were more likely to report vasomotor symptoms.[86]		It seems that Japanese women experience menopause effects, or konenki, in a different way from American women.[87] Japanese women report lower rates of hot flashes and night sweats; this can be attributed to a variety of factors, both biological and social. Historically, konenki was associated with wealthy middle-class housewives in Japan, i.e., it was a "luxury disease" that women from traditional, inter-generational rural households did not report. Menopause in Japan was viewed as a symptom of the inevitable process of aging, rather than a "revolutionary transition", or a "deficiency disease" in need of management.[87]		In Japanese culture, reporting of vasomotor symptoms has been on the increase, with research conducted by Melissa Melby in 2005 finding that of 140 Japanese participants, hot flashes were prevalent in 22.1%.[88] This was almost double that of 20 years prior.[89] Whilst the exact cause for this is unknown, possible contributing factors include significant dietary changes, increased medicalisation of middle-aged women and increased media attention on the subject.[89] However, reporting of vasomotor symptoms is still significantly lower than North America.[90]		Additionally, while most women in the United States apparently have a negative view of menopause as a time of deterioration or decline, some studies seem to indicate that women from some Asian cultures have an understanding of menopause that focuses on a sense of liberation and celebrates the freedom from the risk of pregnancy.[91] Postmenopausal Indian women can enter Hindu temples and participate in rituals, marking it as a celebration for reaching an age of wisdom and experience.		Diverging from these conclusions, one study appeared to show that many American women "experience this time as one of liberation and self-actualization".[92]		Generally speaking, women raised in the Western world or developed countries in Asia live long enough so that a third of their life is spent in post-menopause. For some women, the menopausal transition represents a major life change, similar to menarche in the magnitude of its social and psychological significance. Although the significance of the changes that surround menarche is fairly well recognized, in countries such as the United States, the social and psychological ramifications of the menopause transition are frequently ignored or underestimated.[citation needed]		The medicalization of menopause within biomedical practice began in the early 19th century and has affected the way menopause is viewed within society. By the 1930s in North America and Europe, biomedicine practitioners began to think of menopause as a disease-like state. This idea coincided with the concept of the "standardization of the body". The bodies of young premenopausal women began to be considered the "normal", against which all female bodies were compared.[93]		Menopause literally means the "end of monthly cycles" (the end of monthly periods or menstruation), from the Greek word pausis ("pause") and mēn ("month"). This is a medical calque; the Greek word for menses is actually different. In Ancient Greek, the menses were described in the plural, ta emmēnia, ("the monthlies"), and its modern descendant has been clipped to ta emmēna. The Modern Greek medical term is emmenopausis in Katharevousa or emmenopausi in Demotic Greek.		The word "menopause" was coined specifically for human females, where the end of fertility is traditionally indicated by the permanent stopping of monthly menstruations. However, menopause exists in some other animals, many of which do not have monthly menstruation;[94] in this case, the term means a natural end to fertility that occurs before the end of the natural lifespan.		Various theories have been suggested that attempt to suggest evolutionary benefits to the human species stemming from the cessation of women's reproductive capability before the end of their natural lifespan. Explanations can be categorized as adaptive and non-adaptive:		The high cost of female investment in offspring may lead to physiological deteriorations that amplify susceptibility to becoming infertile. This hypothesis suggests the reproductive lifespan in humans has been optimized, but it has proven more difficult in females and thus their reproductive span is shorter. If this hypothesis were true, however, age at menopause should be negatively correlated with reproductive effort[95] and the available data do not support this.[96]		A recent increase in female longevity due to improvements in the standard of living and social care has also been suggested.[97] It is difficult for selection, however, to favour aid to offspring from parents and grandparents.[98] Irrespective of living standards, adaptive responses are limited by physiological mechanisms. In other words, senescence is programmed and regulated by specific genes.[99]		This hypothesis suggests that younger mothers and offspring under their care will fare better in a difficult and predatory environment because a younger mother will be stronger and more agile in providing protection and sustenance for herself and a nursing baby. The various biological factors associated with menopause had the effect of male members of the species investing their effort with the most viable of potential female mates.[100][page needed] One problem with this hypothesis is that we would expect to see menopause exhibited in the animal kingdom.[94]		The mother hypothesis suggests that menopause was selected for humans because of the extended development period of human offspring and high costs of reproduction so that mothers gain an advantage in reproductive fitness by redirecting their effort from new offspring with a low survival chance to existing children with a higher survival chance.[101]		The Grandmother hypothesis suggests that menopause was selected for humans because it promotes the survival of grandchildren. According to this hypothesis, post-reproductive women feed and care for children, adult nursing daughters, and grandchildren whose mothers have weaned them. Human babies require large and steady supplies of glucose to feed the growing brain. In infants in the first year of life, the brain consumes 60% of all calories, so both babies and their mothers require a dependable food supply. Some evidence suggests that hunters contribute less than half the total food budget of most hunter-gatherer societies, and often much less than half, so that foraging grandmothers can contribute substantially to the survival of grandchildren at times when mothers and fathers are unable to gather enough food for all of their children. In general, selection operates most powerfully during times of famine or other privation. So although grandmothers might not be necessary during good times, many grandchildren cannot survive without them during times of famine. Arguably, however, there is no firm consensus on the supposed evolutionary advantages (or simply neutrality) of menopause to the survival of the species in the evolutionary past.		Indeed, analysis of historical data found that the length of a female's post-reproductive lifespan was reflected in the reproductive success of her offspring and the survival of her grandchildren.[102] Interestingly, another study found comparative effects but only in the maternal grandmother—paternal grandmothers had a detrimental effect on infant mortality (probably due to paternity uncertainty).[103] Differing assistance strategies for maternal and paternal grandmothers have also been demonstrated. Maternal grandmothers concentrate on offspring survival, whereas paternal grandmothers increase birth rates.[104]		Some believe a problem concerning the grandmother hypothesis is that it requires a history of female philopatry while in the present day the majority of hunter-gatherer societies are patriarchal.[105] However, there is disagreement split along ideological lines about whether patrilineality would have existed before modern times.[106] Some believe variations on the mother, or grandmother effect fail to explain longevity with continued spermatogenesis in males (oldest verified paternity is 94 years, 35 years beyond the oldest documented birth attributed to females).[107] Notably, the survival time past menopause is roughly the same as the maturation time for a human child. That a mother's presence could aid in the survival of a developing child, while an unidentified father's absence might not have affected survival, could explain the paternal fertility near the end of the father's lifespan.[108] A man with no certainty of which children are his may merely attempt to father additional children, with support of existing children present but small. Note the existence of partible paternity supporting this.[109] Some argue that the mother and grandmother hypotheses fail to explain the detrimental effects of losing ovarian follicular activity, such as osteoporosis, osteoarthritis, Alzheimer's disease and coronary artery disease.[110]		The theories discussed above assume that evolution directly selected for menopause. Another theory states that menopause is the byproduct of the evolutionary selection for follicular atresia, a factor that causes menopause. Menopause results from having too few ovarian follicles to produce enough estrogen to maintain the ovarian-pituitary-hypothalamic loop, which results in the cessation of menses and the beginning of menopause. Human females are born with approximately a million oocytes, and approximately 400 oocytes are lost to ovulation throughout life.[111][112]		Menopause in the animal kingdom appears to be uncommon, but the presence of this phenomenon in different species has not been thoroughly researched. Life histories show a varying degree of senescence; rapid senescing organisms (e.g., Pacific salmon and annual plants) do not have a post-reproductive life-stage. Gradual senescence is exhibited by all placental mammalian life histories.		Menopause has been observed in several species of nonhuman primates,[94] including rhesus monkeys,[113] and chimpanzees.[114] Menopause also has been reported in a variety of other vertebrate species including elephants,[115] short-finned pilot whales,[116] killer whales,[117] and other cetaceans,[118][119] the guppy,[120] the platyfish, the budgerigar, the laboratory rat and mouse, and the opossum. However, with the exception of the short-finned pilot whale, such examples tend to be from captive individuals, and thus they are not necessarily representative of what happens in natural populations in the wild.		Dogs do not experience menopause; the canine estrus cycle simply becomes irregular and infrequent. Although older female dogs are not considered good candidates for breeding, offspring have been produced by older animals.[121] Similar observations have been made in cats.[122]						
PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintains the database as part of the Entrez system of information retrieval.		From 1971 to 1997, MEDLINE online access to the MEDLARS Online computerized database primarily had been through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching.[1] The PubMed system was offered free to the public in June 1997, when MEDLINE searches via the Web were demonstrated, in a ceremony, by Vice President Al Gore.[2]						In addition to MEDLINE, PubMed provides access to:		Many PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central[4] and local mirrors such as UK PubMed Central.[5]		Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.[6]		As of 11 July 2017[update], PubMed has more than 27.3 million records going back to 1966, selectively to the year 1865, and very selectively to 1809; about 500,000 new records are added each year. As of the same date[update], 13.1 million of PubMed's records are listed with their abstracts, and 14.2 million articles have links to full-text (of which 3.8 million articles are available, full-text for free for any user).[7] Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950's to 16% in 2016.[8] Other significant proportion of records correspond to “Chemistry” (8.69%), “Therapy” (8.39%) and "Infection" (5%).				In 2016, NLM changed the indexing system so that publishers will be able to directly correct typos and errors in PubMed indexed articles.[9]		Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window.		PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms.		The examples given in a PubMed tutorial[10] demonstrate how this automatic process works:		Likewise,		The new PubMed interface, launched in October 2009, encourages the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches.[11]		For comprehensive, optimal searches in PubMed, it is necessary to have a thorough understanding of its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features, and are best carried out by PubMed search specialists or librarians,[12] who are able to select the right type of search and carefully adjust it for precision and recall.[13]		When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., "Clinical Trial"), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).		Publication type parameter enables many special features. A special feature of PubMed is its "Clinical Queries" section, where "Clinical Categories", "Systematic Reviews", and "Medical Genetics" subjects can be searched, with study-type 'filters' automatically applied to identify substantial, robust studies.[14] As these 'clinical girish' can generate small sets of robust studies with considerable precision, it has been suggested that this PubMed section can be used as a 'point-of-care' resource.[15]		Since July 2005, the MEDLINE article indexing process extracts important identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).[16]		A reference which is judged particularly relevant can be marked and "related articles" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the 'Find related data' option. The related articles are then listed in order of "relatedness". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm.[17] The 'related articles' function has been judged to be so precise that some researchers suggest it can be used instead of a full search.[18]		A strong feature of PubMed is its ability to automatically link to MeSH terms and subheadings. Examples would be: "bad breath" links to (and includes in the search) "halitosis", "heart attack" to "myocardial infarction", "breast cancer" to "breast neoplasms". Where appropriate, these MeSH terms are automatically "expanded", that is, include more specific terms. Terms like "nursing" are automatically linked to "Nursing [MeSH]" or "Nursing [Subheading]". This important feature makes PubMed searches automatically more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.		The PubMed optional facility "My NCBI" (with free registration) provides tools for		and a wide range of other options.[19] The "My NCBI" area can be accessed from any computer with web-access. An earlier version of "My NCBI" was called "PubMed Cubby".[20]		LinkOut, a NLM facility to link (and make available full-text) local journal holdings.[21] Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March 2010[update]), from Aalborg University in Denmark to ZymoGenetics in Seattle.[22] Users at these institutions see their institutions logo within the PubMed search result (if the journal is held at that institution) and can access the full-text.		In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016.[23]		PubMed/MEDLINE can be accessed via handheld devices, using for instance the "PICO" option (for focused clinical questions) created by the NLM.[24] A "PubMed Mobile" option, providing access to a mobile friendly, simplified PubMed version, is also available.[25]		askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.[26]		A PMID (PubMed identifier or PubMed unique identifier)[27] is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID which is the identifier for all works published in the free-to-access PubMed Central.[28]		The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.		The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers.[29] As of October 2008[update], more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range[30] of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.		Lu[30] identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:		As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term "PubMed derivatives" has been suggested.[30] Without the need to store about 90 GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in "The E-utilities In-Depth: Parameters, Syntax and More", by Eric Sayers, PhD.[44]		Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment. Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has rise from 6% in the 1950's to 16% in 2016.[45]		
Blood pressure (BP) is the pressure of circulating blood on the walls of blood vessels. When used without further specification, "blood pressure" usually refers to the pressure in large arteries of the systemic circulation. Blood pressure is usually expressed in terms of the systolic pressure (maximum during one heart beat) over diastolic pressure (minimum in between two heart beats) and is measured in millimeters of mercury (mmHg), above the surrounding atmospheric pressure (considered to be zero for convenience).		It is one of the vital signs, along with respiratory rate, heart rate, oxygen saturation, and body temperature. Normal resting blood pressure in an adult is approximately 120 millimetres of mercury (16 kPa) systolic, and 80 millimetres of mercury (11 kPa) diastolic, abbreviated "120/80 mmHg".		Traditionally, blood pressure was measured non-invasively using a mercury manometer and this is still generally considered the gold standard.[1] More recently other semi-automated methods have become common, largely due to concerns about potential mercury toxicity,[2] although cost and ease of use have also influenced this trend.[1] Early alternatives to mercury sphygmomanometers were often inaccurate, but more modern validated devices have similar accuracy to mercury devices.[1]		Blood pressure is influenced by cardiac output, total peripheral resistance and arterial stiffness and varies depending on situation, emotional state, activity, and relative health/disease states. In the short term it is regulated by baroreceptors which act via the brain to influence nervous and endocrine systems.		Blood pressure that is low due to a disease state is called hypotension, and pressure that is consistently high is hypertension. Both have many causes and may be of sudden onset or of long duration. Long term hypertension is a risk factor for many diseases, including heart disease, stroke and kidney failure. Long term hypertension is more common than long term hypotension. Long term hypertension often goes undetected because of infrequent monitoring and the absence of symptoms.		The risk of cardiovascular disease increases progressively above 115/75 mmHg.[6] In practice blood pressure is considered too low only if noticeable symptoms are present.[4]		Observational studies demonstrate that people who maintain arterial pressures at the low end of these pressure ranges have much better long term cardiovascular health. There is an ongoing medical debate over what is the optimal level of blood pressure to target when using drugs to lower blood pressure with hypertension, particularly in older people.[7]		The table shows the classification of blood pressure adopted by the American Heart Association for adults who are 18 years and older.[3] It assumes the values are a result of averaging resting blood pressure readings measured at two or more visits to the doctor.[8][9]		In the UK, clinic blood pressures are usually categorized into three groups; low (90/60 or lower), normal (between 90/60 and 139/89), and high (140/90 or higher).[10][11]		Blood pressure fluctuates from minute to minute and normally shows a circadian rhythm over a 24-hour period, with highest readings in the early morning and evenings and lowest readings at night.[12][13] Loss of the normal fall in blood pressure at night is associated with a greater future risk of cardiovascular disease and there is evidence that night-time blood pressure is a stronger predictor of cardiovascular events than day-time blood pressure.[14] Also, an individual's blood pressure varies with exercise, emotional reactions, sleep, digestion and time of day (circadian rhythm).		Various other factors, such as age and sex, also influence a person's blood pressure. In children, the normal ranges are lower than for adults and depend on height.[15] Reference blood pressure values have been developed for children in different countries, based on the distribution of blood pressure in children of these countries.[16] As adults age, systolic pressure tends to rise and diastolic pressure tends to fall.[17] Consequently, in the elderly, systolic blood pressure often exceeds the normal adult range,[18] this is thought to be due to increased stiffness of the arteries.[19]		Differences between left and right arm blood pressure measurements tend to be small. However, occasionally there is a consistent difference greater than 10 mmHg which may need further investigation, e.g. for obstructive arterial disease.[20][21]		The mean arterial pressure (MAP) is the average over a cardiac cycle and is determined by the cardiac output (CO), systemic vascular resistance (SVR), and central venous pressure (CVP):[23]		In practice the contribution of CVP (which is small) is generally ignored and so		MAP can be estimated from measurements of the systolic pressure P sys {\displaystyle \!P_{\text{sys}}}   and the diastolic pressure P dias {\displaystyle \!P_{\text{dias}}}  [23]		The pulse pressure is the difference between the measured systolic and diastolic pressures,[24]		The up and down fluctuation of the arterial pressure results from the pulsatile nature of the cardiac output, i.e. the heartbeat. Pulse pressure is determined by the interaction of the stroke volume of the heart, the compliance (ability to expand) of the arterial system—largely attributable to the aorta and large elastic arteries—and the resistance to flow in the arterial tree. By expanding under pressure, the aorta absorbs some of the force of the blood surge from the heart during a heartbeat. In this way, the pulse pressure is reduced from what it would be if the aorta were not compliant.[24] The loss of arterial compliance that occurs with aging explains the elevated pulse pressures found in elderly patients.		Pulmonary capillary wedge pressure		Blood pressure generally refers to the arterial pressure in the systemic circulation. However, measurement of pressures in the venous system and the pulmonary vessels plays an important role in intensive care medicine but requires invasive measurement of pressure using a catheter.		Venous pressure is the vascular pressure in a vein or in the atria of the heart. It is much less than arterial pressure, with common values of 5 mmHg in the right atrium and 8 mmHg in the left atrium.		Variants of venous pressure include:		Normally, the pressure in the pulmonary artery is about 15 mmHg at rest.[29]		Increased blood pressure in the capillaries of the lung cause pulmonary hypertension, leading to interstitial edema if the pressure increases to above 20 mmHg, and to pulmonary edema at pressures above 25 mmHg.[30]		Disorders of blood pressure control include: high blood pressure, low blood pressure, and blood pressure that shows excessive or maladaptive fluctuation.		Arterial hypertension can be an indicator of other problems and may have long-term adverse effects. Sometimes it can be an acute problem, for example hypertensive emergency.		Levels of arterial pressure put mechanical stress on the arterial walls. Higher pressures increase heart workload and progression of unhealthy tissue growth (atheroma) that develops within the walls of arteries. The higher the pressure, the more stress that is present and the more atheroma tend to progress and the heart muscle tends to thicken, enlarge and become weaker over time.		Persistent hypertension is one of the risk factors for strokes, heart attacks, heart failure and arterial aneurysms, and is the leading cause of chronic kidney failure. Even moderate elevation of arterial pressure leads to shortened life expectancy. At severely high pressures, mean arterial pressures 50% or more above average, a person can expect to live no more than a few years unless appropriately treated.[31]		In the past, most attention was paid to diastolic pressure; but nowadays it is recognised that both high systolic pressure and high pulse pressure (the numerical difference between systolic and diastolic pressures) are also risk factors. In some cases, it appears that a decrease in excessive diastolic pressure can actually increase risk, due probably to the increased difference between systolic and diastolic pressures (see the article on pulse pressure). If systolic blood pressure is elevated (>140 mmHg) with a normal diastolic blood pressure (<90 mmHg), it is called "isolated systolic hypertension" and may present a health concern.[32][33]		For those with heart valve regurgitation, a change in its severity may be associated with a change in diastolic pressure. In a study of people with heart valve regurgitation that compared measurements 2 weeks apart for each person, there was an increased severity of aortic and mitral regurgitation when diastolic blood pressure increased, whereas when diastolic blood pressure decreased, there was a decreased severity.[34]		Blood pressure that is too low is known as hypotension. Hypotension is a medical concern if it causes signs or symptoms, such as dizziness, fainting, or in extreme cases, shock.[9]		When arterial pressure and blood flow decrease beyond a certain point, the perfusion of the brain becomes critically decreased (i.e., the blood supply is not sufficient), causing lightheadedness, dizziness, weakness or fainting.[35]		Sometimes the arterial pressure drops significantly when a patient stands up from sitting. This is known as orthostatic hypotension (postural hypotension); gravity reduces the rate of blood return from the body veins below the heart back to the heart, thus reducing stroke volume and cardiac output.[citation needed]		When people are healthy, the veins below their heart quickly constrict and the heart rate increases to minimize and compensate for the gravity effect. This is carried out involuntarily by the autonomic nervous system. The system usually requires a few seconds to fully adjust and if the compensations are too slow or inadequate, the individual will suffer reduced blood flow to the brain, dizziness and potential blackout. Increases in G-loading, such as routinely experienced by aerobatic or combat pilots 'pulling Gs', greatly increases this effect. Repositioning the body horizontally largely eliminates the problem.[citation needed]		Other causes of low arterial pressure include:[citation needed]		Shock is a complex condition which leads to critically decreased perfusion. The usual mechanisms are loss of blood volume, pooling of blood within the veins reducing adequate return to the heart and/or low effective heart pumping. Low arterial pressure, especially low pulse pressure, is a sign of shock and contributes to and reflects decreased perfusion.		If there is a significant difference in the pressure from one arm to the other, that may indicate a narrowing (for example, due to aortic coarctation, aortic dissection, thrombosis or embolism) of an artery.[citation needed]		Normal fluctuation in blood pressure is adaptive and necessary. Fluctuations in pressure that are significantly greater than the norm are associated with greater white matter hyperintensity, a finding consistent with reduced local cerebral blood flow[36] and a heightened risk of cerebrovascular disease.[37] Within both high and low blood pressure groups, a greater degree of fluctuation was found to correlate with an increase in cerebrovascular disease compared to those with less variability, suggesting the consideration of the clinical management of blood pressure fluctuations, even among normotensive older adults.[37] Older individuals and those who had received blood pressure medications were more likely to exhibit larger fluctuations in pressure.[37]		During each heartbeat, blood pressure varies between a maximum (systolic) and a minimum (diastolic) pressure.[38] The blood pressure in the circulation is principally due to the pumping action of the heart.[39] Differences in mean blood pressure are responsible for blood flow from one location to another in the circulation. The rate of mean blood flow depends on both blood pressure and the resistance to flow presented by the blood vessels. Mean blood pressure decreases as the circulating blood moves away from the heart through arteries and capillaries due to viscous losses of energy. Mean blood pressure drops over the whole circulation, although most of the fall occurs along the small arteries and arterioles.[40] Gravity affects blood pressure via hydrostatic forces (e.g., during standing), and valves in veins, breathing, and pumping from contraction of skeletal muscles also influence blood pressure in veins.[39]		Most influences on blood pressure can be understood in terms of their effect on cardiac output and resistance (the determinants of mean arterial pressure).[41]		Some factors are:		In practice, each individual's autonomic nervous system and other systems regulating blood pressure respond to and regulate all these factors so that, although the above issues are important, they rarely act in isolation and the actual arterial pressure response of a given individual can vary widely in the short and long-term.		The endogenous regulation of arterial pressure is not completely understood, but the following mechanisms of regulating arterial pressure have been well-characterized:		These different mechanisms are not necessarily independent of each other, as indicated by the link between the RAS and aldosterone release. When blood pressure falls many physiological cascades commence in order to return the blood pressure to a more appropriate level.		Currently, the RAS is targeted pharmacologically by ACE inhibitors and angiotensin II receptor antagonists. The aldosterone system is directly targeted by spironolactone, an aldosterone antagonist. The fluid retention may be targeted by diuretics; the antihypertensive effect of diuretics is due to its effect on blood volume. Generally, the baroreceptor reflex is not targeted in hypertension because if blocked, individuals may suffer from orthostatic hypotension and fainting.		Arterial pressure is most commonly measured via a sphygmomanometer, which historically used the height of a column of mercury to reflect the circulating pressure.[49] The most common automated blood pressure measurement technique is based on the so-called "oscillometric" method.[50] Blood pressure values are generally reported in millimetres of mercury (mmHg), though aneroid and electronic devices do not contain mercury.		For each heartbeat, blood pressure varies between systolic and diastolic pressures. Systolic pressure is peak pressure in the arteries, which occurs near the end of the cardiac cycle when the ventricles are contracting. Diastolic pressure is minimum pressure in the arteries, which occurs near the beginning of the cardiac cycle when the ventricles are filled with blood. An example of normal measured values for a resting, healthy adult human is 120 mmHg systolic and 80 mmHg diastolic (written as 120/80 mmHg, and spoken as "one-twenty over eighty").		Systolic and diastolic arterial blood pressures are not static but undergo natural variations from one heartbeat to another and throughout the day (in a circadian rhythm). They also change in response to stress, nutritional factors, drugs, disease, exercise, and momentarily from standing up. Sometimes the variations are large. Hypertension refers to arterial pressure being abnormally high, as opposed to hypotension, when it is abnormally low. Along with body temperature, respiratory rate, and pulse rate, blood pressure is one of the four main vital signs routinely monitored by medical professionals and healthcare providers.[51]		Measuring pressure invasively, by penetrating the arterial wall to take the measurement, is much less common and usually restricted to a hospital setting.		In pregnancy, it is the fetal heart and not the mother's heart that builds up the fetal blood pressure to drive blood through the fetal circulation. The blood pressure in the fetal aorta is approximately 30 mmHg at 20 weeks of gestation, and increases to approximately 45 mmHg at 40 weeks of gestation.[52]		The average blood pressure for full-term infants:[53]		
The following outline is provided as an overview of and topical guide to sports:		Sport – a physical activity that is governed by a set of rules or customs and often engaged in competitively, sports can be played on land, in water and in the air.		Sports can be described as all of the following:		List of sports		Africa		Antarctica		Asia		Caucasus (a region considered to be in both Asia and Europe, or between them)		Europe		North America		Greenland • Mexico • Saint Pierre and Miquelon		South America		South Atlantic		History of sports		Muscles training		Sports medicine		Sport venue		Sport management		Politics and sports		Sports governing body		World governing bodies of various notable sports:		Sociology of sport		Sport psychology		
Health is the level of functional and metabolic efficiency of a living organism. In humans it is the ability of individuals or communities to adapt and self-manage when facing physical, mental, psychological and social changes with environment.[1] The World Health Organization (WHO) defined health in its broader sense in its 1948 constitution as "a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity."[2][3] This definition has been subject to controversy, in particular as lacking operational value, the ambiguity in developing cohesive health strategies, and because of the problem created by use of the word "complete".[4][5][6] Other definitions have been proposed, among which a recent definition that correlates health and personal satisfaction.[7] [8] Classification systems such as the WHO Family of International Classifications, including the International Classification of Functioning, Disability and Health (ICF) and the International Classification of Diseases (ICD), are commonly used to define and measure the components of health.						The definition of health has evolved over time. In keeping with the biomedical perspective, early definitions of health focused on the theme of the body's ability to function; health was seen as a state of normal function that could be disrupted from time to time by disease. An example of such a definition of health is: "a state characterized by anatomic, physiologic, and psychological integrity; ability to perform personally valued family, work, and community roles; ability to deal with physical, biological, psychological, and social stress".[9] Then, in 1948, in a radical departure from previous definitions, the World Health Organization (WHO) proposed a definition that aimed higher: linking health to well-being, in terms of "physical, mental, and social well-being, and not merely the absence of disease and infirmity".[10] Although this definition was welcomed by some as being innovative, it was also criticized as being vague, excessively broad, and was not construed as measurable. For a long time it was set aside as an impractical ideal and most discussions of health returned to the practicality of the biomedical model.[11]		Just as there was a shift from viewing disease as a state to thinking of it as a process, the same shift happened in definitions of health. Again, the WHO played a leading role when it fostered the development of the health promotion movement in the 1980s. This brought in a new conception of health, not as a state, but in dynamic terms of resiliency, in other words, as "a resource for living". The 1984 WHO revised definition of health defined it as "the extent to which an individual or group is able to realize aspirations and satisfy needs, and to change or cope with the environment. Health is a resource for everyday life, not the objective of living; it is a positive concept, emphasizing social and personal resources, as well as physical capacities".[12] Thus, health referred to the ability to maintain homeostasis and recover from insults. Mental, intellectual, emotional, and social health referred to a person's ability to handle stress, to acquire skills, to maintain relationships, all of which form resources for resiliency and independent living.[11]		Since the late 1970s, the federal Healthy People Initiative has been a visible component of the United States’ approach to improving population health.[13] In each decade, a new version of Healthy People is issued,[14] featuring updated goals and identifying topic areas and quantifiable objectives for health improvement during the succeeding ten years, with assessment at that point of progress or lack thereof. Progress has been limited for many objectives, leading to concerns about the effectiveness of Healthy People in shaping outcomes in the context of a decentralized and uncoordinated US health system. Healthy People 2020 gives more prominence to health promotion and preventive approaches, and adds a substantive focus on the importance of addressing societal determinants of health. A new expanded digital interface facilitates use and dissemination rather than bulky printed books as produced in the past. The impact of these changes to Healthy People will be determined in the coming years.[15]		Systematic activities to prevent or cure health problems and promote good health in humans are undertaken by health care providers. Applications with regard to animal health are covered by the veterinary sciences. The term "healthy" is also widely used in the context of many types of non-living organizations and their impacts for the benefit of humans, such as in the sense of healthy communities, healthy cities or healthy environments. In addition to health care interventions and a person's surroundings, a number of other factors are known to influence the health status of individuals, including their background, lifestyle, and economic, social conditions, and spirituality; these are referred to as "determinants of health." Studies have shown that high levels of stress can affect human health.[16]		Generally, the context in which an individual lives is of great importance for both his health status and quality of their life. It is increasingly recognized that health is maintained and improved not only through the advancement and application of health science, but also through the efforts and intelligent lifestyle choices of the individual and society. According to the World Health Organization, the main determinants of health include the social and economic environment, the physical environment, and the person's individual characteristics and behaviors.[17]		More specifically, key factors that have been found to influence whether people are healthy or unhealthy include the following:[17][18][19]		An increasing number of studies and reports from different organizations and contexts examine the linkages between health and different factors, including lifestyles, environments, health care organization, and health policy – such as the 1974 Lalonde report from Canada;[19] the Alameda County Study in California;[20] and the series of World Health Reports of the World Health Organization, which focuses on global health issues including access to health care and improving public health outcomes, especially in developing countries.[21]		The concept of the "health field," as distinct from medical care, emerged from the Lalonde report from Canada. The report identified three interdependent fields as key determinants of an individual's health. These are:[19]		The maintenance and promotion of health is achieved through different combination of physical, mental, and social well-being, together sometimes referred to as the "health triangle."[22][23] The WHO's 1986 Ottawa Charter for Health Promotion further stated that health is not just a state, but also "a resource for everyday life, not the objective of living. Health is a positive concept emphasizing social and personal resources, as well as physical capacities."[24]		Focusing more on lifestyle issues and their relationships with functional health, data from the Alameda County Study suggested that people can improve their health via exercise, enough sleep, maintaining a healthy body weight, limiting alcohol use, and avoiding smoking.[25] Health and illness can co-exist, as even people with multiple chronic diseases or terminal illnesses can consider themselves healthy.[26]		The environment is often cited as an important factor influencing the health status of individuals. This includes characteristics of the natural environment, the built environment, and the social environment. Factors such as clean water and air, adequate housing, and safe communities and roads all have been found to contribute to good health, especially to the health of infants and children.[17][27] Some studies have shown that a lack of neighborhood recreational spaces including natural environment leads to lower levels of personal satisfaction and higher levels of obesity, linked to lower overall health and well being.[28] This suggests that the positive health benefits of natural space in urban neighborhoods should be taken into account in public policy and land use.		Genetics, or inherited traits from parents, also play a role in determining the health status of individuals and populations. This can encompass both the predisposition to certain diseases and health conditions, as well as the habits and behaviors individuals develop through the lifestyle of their families. For example, genetics may play a role in the manner in which people cope with stress, either mental, emotional or physical. For example, obesity is a very large problem in the United States[citation needed] that contributes to bad mental health and causes stress in a lot of people's lives. (One difficulty is the issue raised by the debate over the relative strengths of genetics and other factors; interactions between genetics and environment may be of particular importance.)		There are a lot of types of health issues common with many people across the globe. Disease is one of the most common. According to GlobalIssues.org, approximately 36 million people die each year from non-communicable (not contagious) disease including cardiovascular disease cancer, diabetes, and chronic lung disease (Shah, 2014).		As for communicable diseases, both viral and bacterial, AIDS/HIV, tuberculosis, and malaria are the most common also causing millions of deaths every year (2014).		Another health issue that causes death or contributes to other health problems is malnutrition majorly among children. One of the groups malnutrition affects most is young children. Approximately 7.5 million children under the age of 5 die from malnutrition, and it is usually brought on by not having the money to find or make food (2014).		Bodily injuries are also a common health issue worldwide. These injuries, including broken bones, fractures, and burns can reduce a person's quality of life or can cause fatalities including infections that resulted from the injury or the severity injury in general (Moffett, 2013).[29]		Some contributing factors to poor health are lifestyle choices. These include smoking cigarettes, and also can include a poor diet, whether it is overeating or an overly constrictive diet. Inactivity can also contribute to health issues and also a lack of sleep, excessive alcohol consumption, and neglect of oral hygiene (2013). There are also genetic disorders that are inherited by the person and can vary in how much they affect the person and when they surface (2013).		The one health issue that is the most unfortunate because the majority of these health issues are preventable is that approximately 1 billion people lack access to health care systems (Shah, 2014). It is easy to say that the most common and harmful health issue is that a lot of people do not have access to quality remedies.[30][31]		The World Health Organization describes mental health as "a state of well-being in which the individual realizes his or her own abilities, can cope with the normal stresses of life, can work productively and fruitfully, and is able to make a contribution to his or her community".[32] Mental Health is not just the absence of mental illness.[33]		Mental illness is described as 'the spectrum of cognitive, emotional, and behavioral conditions that interfere with social and emotional well-being and the lives and productivity of people. Having a mental illness can seriously impair, temporarily or permanently, the mental functioning of a person. Other terms include: 'mental health problem', 'illness', 'disorder', 'dysfunction'.[34]		Roughly a quarter of all adults 18 and over in the US suffer from a diagnosable mental illness. Mental illnesses are the leading cause of disability in the US and Canada. Examples include, schizophrenia, ADHD, major depressive disorder, bipolar disorder, anxiety disorder, post-traumatic stress disorder and autism.[35]		Many teens suffer from mental health issues in response to the pressures of society and social problems they encounter. Some of the key mental health issues seen in teens are: depression, eating disorders, and drug abuse. There are many ways to prevent these health issues from occurring such as communicating well with a teen suffering from mental health issues. Mental health can be treated and be attentive to teens' behavior.[36]		Achieving and maintaining health is an ongoing process, shaped by both the evolution of health care knowledge and practices as well as personal strategies and organized interventions for staying healthy.		An important way to maintain your personal health is to have a healthy diet. A healthy diet includes a variety of plant-based and animal-based foods that provide nutrients to your body. Such nutrients give you energy and keep your body running. Nutrients help build and strengthen bones, muscles, and tendons and also regulate body processes (i.e. blood pressure). The food guide pyramid is a pyramid-shaped guide of healthy foods divided into sections. Each section shows the recommended intake for each food group (i.e. Protein, Fat, Carbohydrates, and Sugars). Making healthy food choices is important because it can lower your risk of heart disease, developing some types of cancer, and it will contribute to maintaining a healthy weight.[39]		The Mediterranean diet is commonly associated with health-promoting effects due to the fact that it contains some bioactive compounds like phenolic compounds, isoprenoids and alkaloids.[40]		Physical exercise enhances or maintains physical fitness and overall health and wellness. It strengthens muscles and improves the cardiovascular system.		Sleep is an essential component to maintaining health. In children, sleep is also vital for growth and development. Ongoing sleep deprivation has been linked to an increased risk for some chronic health problems. In addition, sleep deprivation has been shown to correlate with both increased susceptibility to illness and slower recovery times from illness.[41] In one study, people with chronic insufficient sleep, set as six hours of sleep a night or less, were found to be four times more likely to catch a cold compared to those who reported sleeping for seven hours or more a night.[42] Due to the role of sleep in regulating metabolism, insufficient sleep may also play a role in weight gain or, conversely, in impeding weight loss.[43] Additionally, in 2007, the International Agency for Research on Cancer, which is the cancer research agency for the World Health Organization, declared that "shiftwork that involves circadian disruption is probably carcinogenic to humans," speaking to the dangers of long-term nighttime work due to its intrusion on sleep.[44] In 2015, the National Sleep Foundation released updated recommendations for sleep duration requirements based on age and concluded that "Individuals who habitually sleep outside the normal range may be exhibiting signs or symptoms of serious health problems or, if done volitionally, may be compromising their health and well-being."[45]		Health science is the branch of science focused on health. There are two main approaches to health science: the study and research of the body and health-related issues to understand how humans (and animals) function, and the application of that knowledge to improve health and to prevent and cure diseases and other physical and mental impairments. The science builds on many sub-fields, including biology, biochemistry, physics, epidemiology, pharmacology, medical sociology. Applied health sciences endeavor to better understand and improve human health through applications in areas such as health education, biomedical engineering, biotechnology and public health.		Organized interventions to improve health based on the principles and procedures developed through the health sciences are provided by practitioners trained in medicine, nursing, nutrition, pharmacy, social work, psychology, occupational therapy, physical therapy and other health care professions. Clinical practitioners focus mainly on the health of individuals, while public health practitioners consider the overall health of communities and populations. Workplace wellness programs are increasingly adopted by companies for their value in improving the health and well-being of their employees, as are school health services in order to improve the health and well-being of children.		Public health has been described as "the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals."[46] It is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health has many sub-fields, but typically includes the interdisciplinary categories of epidemiology, biostatistics and health services. Environmental health, community health, behavioral health, and occupational health are also important areas of public health.		The focus of public health interventions is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behavior, communities, and (in aspects relevant to human health) environments. Its aim is to prevent health problems from happening or re-occurring by implementing educational programs, developing policies, administering services and conducting research.[47] In many cases, treating a disease or controlling a pathogen can be vital to preventing it in others, such as during an outbreak. Vaccination programs and distribution of condoms to prevent the spread of communicable diseases are examples of common preventive public health measures, as are educational campaigns to promote vaccination and the use of condoms (including overcoming resistance to such).		Public health also takes various actions to limit the health disparities between different areas of the country and, in some cases, the continent or world. One issue is the access of individuals and communities to health care in terms of financial, geographical or socio-cultural constraints to accessing and using services.[48] Applications of the public health system include the areas of maternal and child health, health services administration, emergency response, and prevention and control of infectious and chronic diseases.		The great positive impact of public health programs is widely acknowledged. Due in part to the policies and actions developed through public health, the 20th century registered a decrease in the mortality rates for infants and children and a continual increase in life expectancy in most parts of the world. For example, it is estimated that life expectancy has increased for Americans by thirty years since 1900,[49] and worldwide by six years since 1990.[50]		Personal health depends partially on the active, passive, and assisted cues people observe and adopt about their own health. These include personal actions for preventing or minimizing the effects of a disease, usually a chronic condition, through integrative care. They also include personal hygiene practices to prevent infection and illness, such as bathing and washing hands with soap; brushing and flossing teeth; storing, preparing and handling food safely; and many others. The information gleaned from personal observations of daily living – such as about sleep patterns, exercise behavior, nutritional intake and environmental features – may be used to inform personal decisions and actions (e.g., "I feel tired in the morning so I am going to try sleeping on a different pillow"), as well as clinical decisions and treatment plans (e.g., a patient who notices his or her shoes are tighter than usual may be having exacerbation of left-sided heart failure, and may require diuretic medication to reduce fluid overload).[51]		Personal health also depends partially on the social structure of a person's life. The maintenance of strong social relationships, volunteering, and other social activities have been linked to positive mental health and also increased longevity. One American study among seniors over age 70, found that frequent volunteering was associated with reduced risk of dying compared with older persons who did not volunteer, regardless of physical health status.[52] Another study from Singapore reported that volunteering retirees had significantly better cognitive performance scores, fewer depressive symptoms, and better mental well-being and life satisfaction than non-volunteering retirees.[53]		Prolonged psychological stress may negatively impact health, and has been cited as a factor in cognitive impairment with aging, depressive illness, and expression of disease.[54] Stress management is the application of methods to either reduce stress or increase tolerance to stress. Relaxation techniques are physical methods used to relieve stress. Psychological methods include cognitive therapy, meditation, and positive thinking, which work by reducing response to stress. Improving relevant skills, such as problem solving and time management skills, reduces uncertainty and builds confidence, which also reduces the reaction to stress-causing situations where those skills are applicable.		In addition to safety risks, many jobs also present risks of disease, illness and other long-term health problems. Among the most common occupational diseases are various forms of pneumoconiosis, including silicosis and coal worker's pneumoconiosis (black lung disease). Asthma is another respiratory illness that many workers are vulnerable to. Workers may also be vulnerable to skin diseases, including eczema, dermatitis, urticaria, sunburn, and skin cancer.[55][56] Other occupational diseases of concern include carpal tunnel syndrome and lead poisoning.		As the number of service sector jobs has risen in developed countries, more and more jobs have become sedentary, presenting a different array of health problems than those associated with manufacturing and the primary sector. Contemporary problems, such as the growing rate of obesity and issues relating to stress and overwork in many countries, have further complicated the interaction between work and health.		Many governments view occupational health as a social challenge and have formed public organizations to ensure the health and safety of workers. Examples of these include the British Health and Safety Executive and in the United States, the National Institute for Occupational Safety and Health, which conducts research on occupational health and safety, and the Occupational Safety and Health Administration, which handles regulation and policy relating to worker safety and health.[57][58][59]		
The marathon is a long-distance running race with an official distance of 42.195 kilometres (26.219 miles, or 26 miles 385 yards),[1] usually run as a road race. The event was instituted in commemoration of the fabled run of the Greek soldier Pheidippides, a messenger from the Battle of Marathon to Athens, who reported the victory.		The marathon was one of the original modern Olympic events in 1896, though the distance did not become standardized until 1921. More than 800 marathons are held throughout the world each year, with the vast majority of competitors being recreational athletes as larger marathons can have tens of thousands of participants.[2]						The name Marathon[n 1] comes from the legend of Philippides or Pheidippides, the Greek messenger. The legend states that he was sent from the battlefield of Marathon to Athens to announce that the Persians had been defeated in the Battle of Marathon (in which he had just fought),[3] which took place in August or September, 490 BC.[4] It is said that he ran the entire distance without stopping and burst into the assembly, exclaiming νενικήκαμεν (nenikēkamen, "we have won!"), before collapsing and dying.[5] The account of the run from Marathon to Athens first appears in Plutarch's On the Glory of Athens in the 1st century AD, which quotes from Heraclides Ponticus's lost work, giving the runner's name as either Thersipus of Erchius or Eucles.[6] Lucian of Samosata (2nd century AD) also gives the story, but names the runner Philippides (not Pheidippides).[7]		There is debate about the historical accuracy of this legend.[8][9] The Greek historian Herodotus, the main source for the Greco-Persian Wars, mentions Philippides as the messenger who ran from Athens to Sparta asking for help, and then ran back, a distance of over 240 kilometres (150 mi) each way.[10] In some Herodotus manuscripts, the name of the runner between Athens and Sparta is given as Philippides. Herodotus makes no mention of a messenger sent from Marathon to Athens, and relates that the main part of the Athenian army, having fought and won the grueling battle, and fearing a naval raid by the Persian fleet against an undefended Athens, marched quickly back from the battle to Athens, arriving the same day.[11]		In 1879, Robert Browning wrote the poem Pheidippides. Browning's poem, his composite story, became part of late 19th century popular culture and was accepted as a historic legend.[12]		Mount Penteli stands between Marathon and Athens, which means that, if Philippides actually made his famous run after the battle, he had to run around the mountain, either to the north or to the south. The latter and more obvious route matches almost exactly the modern Marathon-Athens highway, which follows the lay of the land southwards from Marathon Bay and along the coast, then takes a gentle but protracted climb westwards towards the eastern approach to Athens, between the foothills of Mounts Hymettus and Penteli, and then gently downhill to Athens proper. This route, as it existed when the Olympics were revived in 1896, was approximately 40 kilometres (25 mi) long, and this was the approximate distance originally used for marathon races. However, there have been suggestions that Philippides might have followed another route: a westward climb along the eastern and northern slopes of Mount Penteli to the pass of Dionysos, and then a straight southward downhill path to Athens. This route is considerably shorter, some 35 kilometres (22 mi), but includes a very steep initial climb of more than 5 kilometres (3.1 mi).		When the modern Olympics began in 1896, the initiators and organizers were looking for a great popularizing event, recalling the glory of ancient Greece. The idea of a marathon race came from Michel Bréal, who wanted the event to feature in the first modern Olympic Games in 1896 in Athens. This idea was heavily supported by Pierre de Coubertin, the founder of the modern Olympics, as well as by the Greeks.[13] The Greeks staged a selection race for the Olympic marathon on 22 March 1896 (Gregorian)[14] that was won by Charilaos Vasilakos in 3 hours and 18 minutes (with the future winner of the introductory Olympic Games marathon, Spyridon "Spyros" Louis, coming in fifth at a second race two weeks later).[15] The winner of the first Olympic marathon, on 10 April 1896 (a male-only race), was Spyridon Louis, a Greek water-carrier, in 2 hours 58 minutes and 50 seconds.[16] The marathon of the 2004 Summer Olympics was run on the traditional route from Marathon to Athens, ending at Panathinaiko Stadium, the venue for the 1896 Summer Olympics. That men's marathon was won by Italian Stefano Baldini in 2 hours 10 minutes and 55 seconds, a record time for this route until the non-Olympics Athens Classic Marathon of 2014, when Felix Kandie lowered the course record to 2 hours 10 minutes and 37 seconds.		The women's marathon was introduced at the 1984 Summer Olympics (Los Angeles, USA) and was won by Joan Benoit of the United States with a time of 2 hours 24 minutes and 52 seconds.[19]		It has become a tradition for the men's Olympic marathon to be the last event of the athletics calendar, on the final day of the Olympics.[20] For many years the race finished inside the Olympic stadium; however, at the 2012 London Olympics, the start and finish were on The Mall,[21] and at the 2016 Rio games, the start and finish were in the Sambódromo, the parade area that serves as a spectator mall for Carnival.[22]		Often, the men's marathon medals are awarded during the closing ceremony (including the 2004 games, 2012 games and 2016 games).		The Olympic men's record is 2:06:32, set at the 2008 Summer Olympics by Samuel Kamau Wanjiru of Kenya[23] (average speed about 20.01 kilometres per hour or 12.43 miles per hour). The Olympic women's record is 2:23:07, set at the 2012 Summer Olympics by Tiki Gelana of Ethiopia.[24] The men's London 2012 Summer Olympic marathon winner was Stephen Kiprotich of Uganda (2:08:01). Per capita, the Kalenjin ethnic group of Rift Valley Province in Kenya has produced a highly disproportionate share of marathon and track-and-field winners.		Johnny Hayes' victory at the 1908 Summer Olympics contributed to the early growth of long-distance running and marathoning in the United States.[25][26] Later that year, races around the holiday season including the Empire City Marathon held on New Year's Day 1909 in Yonkers, New York, marked the early running craze referred to as "marathon mania".[27] Following the 1908 Olympics, the first five amateur marathons in New York City were held on days that held special meanings to ethnic communities: Thanksgiving Day, the day after Christmas, New Year's Day, Washington's Birthday, and Lincoln's Birthday.[28]		Frank Shorter's victory in the marathon at the 1972 Summer Olympics would spur national enthusiasm for the sport more intense than that which followed Hayes' win 64 years earlier.[26] In 2014, an estimated 550,600 runners completed a marathon within the United States.[29] This can be compared to 143,000 in 1980. Today marathons are held all around the world on a nearly weekly basis.[30]		For a long time after the Olympic marathon started, there were no long-distance races, such as the marathon, for women. Although a few women such as Stamata Revithi had run the marathon distance, they were not included in any official results.[31][32] Marie-Louise Ledru has been credited as the first woman to complete a marathon, in 1918.[33][34][35] Violet Piercy has been credited as the first woman to be officially timed in a marathon, in 1926.[31]		Arlene Pieper became the first woman to officially finish a marathon in the United States when she completed the Pikes Peak Marathon in Manitou Springs, Colorado, in 1959.[36][37] Kathrine Switzer was the first woman to run the Boston Marathon "officially" (with a number).[38] However, Switzer's entry, which was accepted through an "oversight" in the screening process, was in "flagrant violation of the rules", and she was treated as an interloper once the error was discovered.[39] Bobbi Gibb had completed the Boston race unofficially the previous year (1966),[40] and was later recognized by the race organizers as the women's winner for that year, as well as 1967 and 1968.[41]		In 2015 Afghanistan held its first marathon; among those who ran the entire marathon was one woman, Zainab, age 25, who thus became the first Afghan woman to run in a marathon within her own country.[42]		Olympic marathon distances		The length of an Olympic marathon was not precisely fixed at first, but the marathon races in the first few Olympic Games were about 40 kilometres (25 mi),[43] roughly the distance from Marathon to Athens by the longer, flatter route. The exact length depended on the route established for each venue.		The International Olympic Committee agreed in 1907 that the distance for the 1908 London Olympic marathon would be about 25 miles or 40 kilometres. The organisers decided on a course of 26 miles from the start at Windsor Castle to the royal entrance to the White City Stadium, followed by a lap (586 yards 2 feet; 536 m) of the track, finishing in front of the Royal Box.[44][45] The course was later altered to use a different entrance to the stadium, followed by a partial lap of 385 yards to the same finish.		The modern 42.195 km standard distance for the marathon was set by the International Amateur Athletic Federation (IAAF) in May 1921[46][47][48][49] directly from the length used at the 1908 Summer Olympics in London.		An official IAAF marathon course is 42.195 km (42 m tolerance only in excess).[50] Course officials add a short course prevention factor of up to one metre per kilometre to their measurements to reduce the risk of a measuring error producing a length below the minimum distance.		For events governed by IAAF rules, it is mandatory that the route be marked so that all competitors can see the distance covered in kilometres.[1] The rules make no mention of the use of miles. The IAAF will only recognise world records that are established at events that are run under IAAF rules. For major events, it is customary to publish competitors' timings at the midway mark and also at 5 km splits; marathon runners can be credited with world records for lesser distances recognised by the IAAF (such as 20 km, 30 km and so on) if such records are established while the runner is running a marathon, and completes the marathon course.[51]		Annually, more than 800 marathons are organized worldwide.[52] Some of these belong to the Association of International Marathons and Distance Races (AIMS) which has grown since its foundation in 1982 to embrace over 300 member events in 83 countries and territories.[53] The marathons of Berlin, Boston, Chicago, London, New York City and Tokyo form the biennial World Marathon Majors series, awarding $500,000 annually to the best overall male and female performers in the series.		In 2006, the editors of Runner's World selected a "World's Top 10 Marathons",[54] in which the Amsterdam, Honolulu, Paris, Rotterdam, and Stockholm marathons were featured along with the five original World Marathon Majors events (excluding Tokyo). Other notable large marathons include United States Marine Corps Marathon, Los Angeles, and Rome. The Boston Marathon is the world's oldest annual marathon, inspired by the success of the 1896 Olympic marathon and held every year since 1897 to celebrate Patriots Day, a holiday marking the beginning of the American Revolution, thereby purposely linking Athenian and American struggle for democracy.[55] The oldest annual marathon in Europe is the Košice Peace Marathon, held since 1924 in Košice, Slovakia. The historic Polytechnic Marathon was discontinued in 1996. The Athens Classic Marathon traces the route of the 1896 Olympic course, starting in Marathon on the eastern coast of Attica, site of the Battle of Marathon of 490 B.C.E., and ending at the Panathenaic Stadium in Athens.[56]		The Midnight Sun Marathon is held in Tromsø, Norway at 70 degrees north. Using unofficial and temporary courses, measured by GPS, races of marathon distance are now held at the North Pole, in Antarctica and over desert terrain. Other unusual marathons include the Great Wall Marathon on The Great Wall of China, the Big Five Marathon among the safari wildlife of South Africa, the Great Tibetan Marathon – a marathon in an atmosphere of Tibetan Buddhism at an altitude of 3,500 metres (11,500 ft), and the Polar Circle Marathon on the permanent ice cap of Greenland.		The Intercontinental Istanbul Eurasia Marathon is the only marathon where participants run over two continents (Europe and Asia) during the course of a single event. In the Detroit Free Press Marathon, participants cross the US/Canada border twice.[57] The Niagara Falls International Marathon includes one international border crossing, via the Peace Bridge from Buffalo, New York, United States to Fort Erie, Ontario, Canada.		Many marathons feature a wheelchair division. Typically, those in the wheelchair racing division start their races earlier than their running counterparts.		The first wheelchair marathon was in 1974 in Toledo, Ohio, won by Bob Hall in 2:54.[58][59] Hall competed in the 1975 Boston Marathon and finished in 2:58, inaugurating the introduction of wheelchair divisions into the Boston Marathon.[60][61] From 1977 the race was declared the US National Wheelchair championship.[62] The Boston Marathon awards $10,000 to the winning push-rim athlete.[63] Ernst van Dyk has won the Boston Marathon wheelchair division ten times and holds the world record at 1:18:27, set in Boston in 2004.[64] Jean Driscoll won eight times (seven consecutively) and holds the women's world record at 1:34:22.[65]		The New York City Marathon banned wheelchair entrants in 1977, citing safety concerns, but then voluntarily allowed Bob Hall to compete after the state Division of Human Rights ordered the marathon to show cause.[66][67] The Division ruled in 1979 that the New York City Marathon and New York Road Runners club had to allow wheelchair athletes to compete, and confirmed this at appeal in 1980,[68] but the State Supreme Court ruled in 1981 that a ban on wheelchair racers was not discriminatory as the marathon was historically a foot race.[69] However, by 1986 14 wheelchair athletes were competing,[70] and an official wheelchair division was added to the marathon in 2000.[63] The first person to complete a marathon in a robotic walking device was Claire Lomas (UK) who completed the 2012 Virgin London Marathon in sixteen days.[71]		Some of the quickest people to complete a wheel-chair marathon include Thomas Geierpichler (Austria) who won gold in men's T52-class marathon (no lower limb function) in 1 hr 49 min 7 sec in Beijing China, on September 17, 2008; and, Heinz Frei (Switzerland) who won the men's T54 marathon (for racers with spinal cord injuries) in a time of 1 hr 20 min and 14 sec in Oita, Japan, October 31, 1999.[72]		World records were not officially recognized by the IAAF until 1 January 2004; previously, the best times for the marathon were referred to as the 'world best'. Courses must conform to IAAF standards for a record to be recognized. However, marathon routes still vary greatly in elevation, course, and surface, making exact comparisons impossible. Typically, the fastest times are set over relatively flat courses near sea level, during good weather conditions and with the assistance of pacesetters.[citation needed]		The current world record time for men over the distance is 2 hours 2 minutes and 57 seconds, set in the Berlin Marathon by Dennis Kimetto of Kenya on 28 September 2014,[73] an improvement of 26 seconds over the previous record also set in the Berlin Marathon by Wilson Kipsang, also of Kenya on 29 September 2013.[74] The world record for women was set by Paula Radcliffe of Great Britain in the London Marathon on 13 April 2003, in 2 hours 15 minutes and 25 seconds.[75]		Below is a list of all other times equal or superior to 2:04:11:		Below is a list of all other times equal or superior to 2:20:00:		Fauja Singh, then 100, finished the Toronto Waterfront Marathon, becoming the first centenarian ever to officially complete that distance. Singh, a British citizen, finished the race on 16 October 2011 with a time of 8:11:05.9, making him the oldest marathoner.[81] Because Singh could not produce a birth certificate from rural 1911 Colonial India, the place of his birth, his age could not be verified and his record was not accepted by the official governing body World Masters Athletics.		Gladys Burrill, a 92-year-old Prospect, Oregon woman and part-time resident of Hawaii, previously held the Guinness World Records title of oldest person to complete a marathon with her 9 hours 53 minutes performance at the 2010 Honolulu Marathon.[82][83] The records of the Association of Road Racing Statisticians, at that time, however, suggested that Singh was overall the oldest marathoner, completing the 2004 London Marathon at the age of 93 years and 17 days, and that Burrill was the oldest female marathoner, completing the 2010 Honolulu Marathon at the age of 92 years and 19 days.[84] Singh's age was also reported to be 93 by other sources.[85][86]		In 2015, 92-yr-old Harriette Thompson of Charlotte, North Carolina, completed the Rock 'n' Roll San Diego Marathon in 7 hours 24 minutes 36 seconds, thus becoming the oldest woman to complete a marathon.[87] While Gladys Burrill was 92 years and 19 days old when she completed her record-setting marathon, Harriette Thompson was 92 years and 65 days old when she completed hers.[87]		Budhia Singh, a boy from Odisha, India, completed his first marathon at age three. He trained under the coach Biranchi Das, who saw potential in him. In May 2006, Budhia was temporarily banned from running by the ministers of child welfare, as his life could be at risk. His coach was also arrested for exploiting and cruelty to a child. Budhia is now at a state-run sports academy.[88]		In 2011, Running USA reported that there were approximately 518,000 marathon finishers in the United States,[89] while other sources reported around 550,000 finishers.[90]		As marathon running has become more popular, some athletes have undertaken challenges involving running a series of marathons.		The 100 Marathon Club is intended to provide a focal point for all runners, particularly from the United Kingdom or Ireland, who have completed 100 or more races of marathon distance or longer. At least 10 of these events must be United Kingdom or Ireland Road Marathons.[91] Club chairman Roger Biggs has run more than 700 marathons or ultras. Brian Mills completed his 800th marathon on 17 September 2011.		Steve Edwards, a member of the 100 Marathon Club, set the world record for running 500 marathons in the fastest average finish time of 3 hours 15 minutes, at the same time becoming the first man to run 500 marathons with an official time below 3 hours 30 minutes, on 11 November 2012 at Milton Keynes, England. The records took 24 years to achieve. Edwards was 49 at the time.[92]		Over 350 individuals have completed a marathon in each state of the United States plus Washington, D.C. and some have done it as many as eight times.[93] Beverly Paquin, a 22-year-old nurse from Iowa, was the youngest woman to run a marathon in all 50 states in 2010.[94] A few weeks later, still in 2010, Morgan Cummings (also 22) became the youngest woman to complete a marathon in all 50 states and DC.[95] In 2004, Chuck Bryant of Miami, Florida, who lost his right leg below the knee, became the first amputee to finish this circuit.[96] Bryant has completed a total of 59 marathons on his prosthesis. Twenty-seven people have run a marathon on each of the seven continents, and 31 people have run a marathon in each of the Canadian provinces. In 1980, in what was termed the Marathon of Hope, Terry Fox, who had lost a leg to cancer and so ran with one artificial leg, attained 5,373 kilometres (3,339 mi) of his proposed cross-Canada cancer fundraising run, maintaining an average of over 37 kilometres (23 mi), close to the planned marathon distance, for each of 143 consecutive days.[97]		On 25 September 2011, Patrick Finney of Grapevine, Texas became the first person with multiple sclerosis to finish a marathon in each state of the United States. In 2004, "the disease had left him unable to walk. But unwilling to endure a life of infirmity, Finney managed to regain his ability to balance on two feet, to walk – and eventually to run – through extensive rehabilitation therapy and new medications."[98]		In 2003 British adventurer Sir Ranulph Fiennes completed seven marathons on seven continents in seven days.[99] He completed this feat despite suffering from a heart attack and undergoing a double heart bypass operation just four months before.[100] This feat has since been eclipsed by Irish ultramarathon runner Richard Donovan who in 2009 completed seven marathons on seven continents in under 132 hours (five and a half days).[101] Starting 1 February 2012 he improved on this by completing the 7 on 7 in under 120 hours or in less than five days.[102][103]		On 30 November 2013, 69-year-old Larry Macon set a Guinness World Record for Most Marathons Run in a Year by Man by running 238 marathons. Larry Macon celebrated his 1,000th career marathon at the Cowtown Marathon in Ft. Worth on 24 February 2013.[104]		Other goals are to attempt to run marathons on a series of consecutive weekends (Richard Worley on 159 weekends),[105] or to run the most marathons during a particular year or the most in a lifetime. A pioneer in running multiple marathons was Sy Mah of Toledo, Ohio, who ran 524 before he died in 1988.[106] As of 30 June 2007, Horst Preisler of Germany had successfully completed 1214 marathons plus 347 ultramarathons, a total of 1561 events at marathon distance or longer.[107] Sigrid Eichner, Christian Hottas and Hans-Joachim Meyer have also all completed over 1000 marathons each.[108] Norm Frank of the United States is credited with 945 marathons.[109]		Christian Hottas is meanwhile the first runner who ever completed 2000 marathons. He ran his 2000th at TUI Marathon Hannover on 5 May 2013 together with a group of more than 80 friends from 11 countries, including 8 officers from the 100 Marathons Clubs U.K., North-America, Germany, Denmark, Austria and Italy.[110] Hottas completed his 2500th marathon on 4 December 2016.[111]		In 2010, Stefaan Engels, a Belgian, set out to run the marathon distance every day of the year. Because of a foot injury he had to resort to a handbike near the end of January 2010. However, on 5 February he was fully recovered and decided to reset the counter back to zero.[112] By 30 March he broke the existing record of Akinori Kusuda, from Japan, who completed 52 marathons in a row in 2009. On 5 February 2011, Engels had run 365 marathon distances in as many days.[113] Ricardo Abad Martínez, from Spain, later ran 150 marathons in 150 consecutive days in 2009,[114] and subsequently 500 marathons in a row, from October 2010 to February 2012.[115]		Some runners compete to run the same marathons for the most consecutive years. For example, Johnny Kelley completed 61 Boston Marathons.[116] Currently, the longest consecutive streak of Boston Marathon finishes—45 in a row—is held by Bennett Beach, of Bethesda, Maryland.[117]		Most participants do not run a marathon to win. More important for most runners is their personal finish time and their placement within their specific gender and age group, though some runners just want to finish. Strategies for completing a marathon include running the whole distance[121] and a run–walk strategy.[3] In 2005, the average marathon time in the U.S. was 4 hours 32 minutes 8 seconds for men, 5 hours 6 minutes 8 seconds for women.[122] In 2015, the men's and women's median marathon times were 4 hours 20 minutes 13 seconds and 4 hours 45 minutes 30 seconds respectively.[123]		A goal many runners aim for is to break certain time barriers. For example, recreational first-timers often try to run the marathon under four hours; more competitive runners may attempt to finish under three hours.[124] Other benchmarks are the qualifying times for major marathons. The Boston Marathon, the oldest marathon in the United States, requires a qualifying time for all non-professional runners.[125] The New York City Marathon also requires a qualifying time for guaranteed entry, at a significantly faster pace than Boston's.[126]		Typically, there is a maximum allowed time of about six hours after which the marathon route is closed, although some larger marathons keep the course open considerably longer (eight hours or more). Many marathons around the world have such time limits by which all runners must have crossed the finish line. Anyone slower than the limit will be picked up by a sweeper bus. In many cases the marathon organizers are required to reopen the roads to the public so that traffic can return to normal.		With the growth in popularity of marathoning, many marathons across the United States and the world have been filling to capacity faster than ever before. When the Boston Marathon opened up registration for its 2011 running, the field capacity was filled within eight hours.[127]		The long run is an important element in marathon training.[128] Recreational runners commonly try to reach a maximum of about 32 km (20 mi) in their longest weekly run and a total of about 64 km (40 mi) a week when training for the marathon, but wide variability exists in practice and in recommendations. More experienced marathoners may run a longer distance during the week. Greater weekly training mileages can offer greater results in terms of distance and endurance, but also carry a greater risk of training injury.[129] Most male elite marathon runners will have weekly mileages of over 160 km (100 mi).[129] It is recommended that those new to running should get a checkup from their doctor, as there are certain warning signs and risk factors that should be evaluated before undertaking any new workout program, especially marathon training.[130]		Many training programs last a minimum of five or six months, with a gradual increase in the distance run and finally, for recovery, a period of tapering in the weeks preceding the race. For beginners wishing to merely finish a marathon, a minimum of four months of running four days a week is recommended.[131][132] Many trainers recommend a weekly increase in mileage of no more than 10%. It is also often advised to maintain a consistent running program for six weeks or so before beginning a marathon training program, to allow the body to adapt to the new stresses.[133] The marathon training program itself would suppose variation between hard and easy training, with a periodization of the general plan.[134]		Training programs can be found at the websites of Runner's World,[135] Hal Higdon,[121] Jeff Galloway,[3] and the Boston Athletic Association,[136] and in numerous other published sources, including the websites of specific marathons.		The last long training run might be undertaken up to two weeks prior to the event. Many marathon runners also "carbo-load" (increase carbohydrate intake while holding total caloric intake constant) during the week before the marathon to allow their bodies to store more glycogen.		Carbohydrates that a person eats are converted by the liver and muscles into glycogen for storage. Glycogen burns rapidly to provide quick energy. Runners can store about 8 MJ or 2,000 kcal worth of glycogen in their bodies, enough for about 30 km/18–20 miles of running. Many runners report that running becomes noticeably more difficult at that point.[137] When glycogen runs low, the body must then obtain energy by burning stored fat, which does not burn as readily. When this happens, the runner will experience dramatic fatigue and is said to "hit the wall". The aim of training for the marathon, according to many coaches,[138] is to maximize the limited glycogen available so that the fatigue of the "wall" is not as dramatic. This is accomplished in part by utilizing a higher percentage of energy from burned fat even during the early phase of the race, thus conserving glycogen.[citation needed]		Carbohydrate-based "energy gels" are used by runners to avoid or reduce the effect of "hitting the wall", as they provide easy to digest energy during the run. Energy gels usually contain varying amounts of sodium and potassium and some also contain caffeine. They need to be consumed with a certain amount of water. Recommendations for how often to take an energy gel during the race range widely.[138]		Alternatives to gels include various forms of concentrated sugars, and foods high in simple carbohydrates that can be digested easily. Many runners experiment with consuming energy supplements during training runs to determine what works best for them. Consumption of food while running sometimes makes the runner sick. Runners are advised not to ingest a new food or medicine just prior to or during a race.[138] It is also important to refrain from taking any of the non-steroidal anti-inflammatory class of pain relievers (NSAIDs, e.g., aspirin, ibuprofen, naproxen), as these drugs may change the way the kidneys regulate their blood flow and may lead to serious kidney problems, especially in cases involving moderate to severe dehydration. NSAIDS block the COX-2 enzyme pathway to prevent the production of prostaglandins. These prostaglandins may act as inflammation factors throughout the body, but they also play a crucial role in maintenance of water retention. In less than 5% of the whole population that take NSAIDS, individuals may be more negatively sensitive to renal prostaglandin synthesis inhibition.[139]		Marathon participation may result in various medical, musculoskeletal, and dermatological complaints.[140] Delayed onset muscle soreness (DOMS) is a common condition affecting runners during the first week following a marathon.[141] Various types of mild exercise or massage have been recommended to alleviate pain secondary to DOMS.[141] Dermatological issues frequently include "jogger's nipple", "jogger's toe", and blisters.[142]		The immune system is reportedly suppressed for a short time. Changes to the blood chemistry may lead physicians to mistakenly diagnose heart malfunction.		After long training runs and the marathon itself, consuming carbohydrates to replace glycogen stores and protein to aid muscle recovery is commonly recommended. In addition, soaking the lower half of the body for approximately 20 minutes in cold or ice water may force blood through the leg muscles to speed recovery.[143]		Marathon running has various health risks.[144] Training and the races themselves put runners under stress. While rare, even death is a possibility during a race.		Common health risks include injury such as blisters, tendonitis, fatigue, knee or ankle sprain, dehydration (electrolyte imbalance), and other conditions. Many are categorised as overuse injuries.		In 2016, a systematic medical review found that the risk of sudden cardiac death during or immediately after a marathon was between 0.6 and 1.9 deaths per 100,000 participants, varying across the specific studies and the methods used, and not controlling for age or gender.[145] Since the risk is small, cardiac screening programs for marathons are uncommon. However, this review was not an attempt to assess the overall cardiac health impact of marathon running.		A 2006 study of non-elite Boston Marathon participants tested runners for certain proteins that indicate heart damage or dysfunction (see Troponin) and gave them echocardiogram scans, before and after the marathon. The study revealed that, in that sample of 60 people, runners who had done less than 56 km (35 mi) of weekly training before the race were most likely to show some heart damage or dysfunction, while runners who had done more than 72 km (45 mi) of weekly training showed few or no heart problems.[146]		According to a Canadian study presented in 2010, running a marathon can temporarily result in decreased function of more than half the muscle segments in the heart's main pumping chamber, but neighboring segments are generally able to compensate. Full recovery is reached within one to three months. The fitter the runner, the less the effect. According to one of the researchers: "Regular exercise reduces cardiovascular risk by a factor of two or three in the long run, but while we're doing vigorous exercise such as marathon running, our cardiac risk increases by seven."[147][148]		Overconsumption is the most significant concern associated with water consumption during marathons. Drinking excessive amounts of fluid during a race can lead to dilution of sodium in the blood, a condition called exercise-associated hyponatremia, which may result in vomiting, seizures, coma and even death.[149] Dr. Lewis G. Maharam, medical director for the New York City Marathon, stated in 2005: "There are no reported cases of dehydration causing death in the history of world running, but there are plenty of cases of people dying of hyponatremia."[150]		For example, Dr. Cynthia Lucero died at the age of 28 while participating in the 2002 Boston Marathon. It was Lucero's second marathon.[151] At mile 22, Lucero complained of feeling "dehydrated and rubber-legged."[152] She soon wobbled and collapsed to the ground, and was unconscious by the time the paramedics reached her. Lucero was admitted to Brigham and Women's Hospital and died two days later.[153]		Lucero's cause of death was determined to be hyponatremic encephalopathy, a condition that causes swelling of the brain due to an imbalance of sodium in the blood known as exercise-associated hyponatremia (EAH). While EAH is sometimes referred to as "water intoxication," Lucero drank large amounts of Gatorade during the race,[154][155] demonstrating that runners who consume sodium-containing sports drinks in excess of thirst can still develop EAH.[154][154][156] Because hyponatremia is caused by excessive water retention, and not just loss of sodium, consumption of sports drinks or salty foods may not prevent hyponatremia.[157]		Women are more prone to hyponatremia than men. A study in the New England Journal of Medicine found that 13% of runners completing the 2002 Boston Marathon had hyponatremia.[158]		Fluid intake should be adjusted individually as factors such as body weight, sex, climate, pace, fitness (VO2 max), and sweat rate are just a few variables that change fluid requirements between people and races. The International Marathon Medical Directors Association (IMMDA) advises that runners drink a sports drink that includes carbohydrates and electrolytes instead of plain water and that runners should "drink to thirst" instead of feeling compelled to drink at every fluid station.[159] Heat exposure leads to diminished thirst drive and thirst may not be a sufficient incentive to drink in many situations.[160] The IMMDA and HSL Harpur Hill give recommendations to drink fluid in small volumes frequently at an approximate rate falling between 100–250 ml (3.4–8.5 US fl oz) every 15 minutes.[160][159] A patient suffering hyponatremia can be given a small volume of a concentrated salt solution intravenously to raise sodium concentrations in the blood. Some runners weigh themselves before running and write the results on their bibs. If anything goes wrong, first aid workers can use the weight information to tell if the patient had consumed too much water.		Exertional heat stroke is an emergency condition in which thermoregulation fails and the body temperature rises dangerously above 104 °F (40 °C). It becomes a greater risk in warm and humid weather, even for young and fit individuals. Treatment requires rapid physical cooling of the body.[161]		Some charities seek to associate with various races. Some marathon organizers set aside a portion their limited entry slots for charity organizations to sell to members in exchange for donations. Runners are given the option to sign up to run particular races, especially when marathon entries are no longer available to the general public.[citation needed]		In some cases, charities organize their own marathon as a fund-raiser (gaining funds via entry fees or through sponsorships).		In 2015 the Mars rover Opportunity attained the distance of a marathon from its starting location on Mars, and the valley where it achieved this distance was called Marathon Valley, which was then explored.		
In computing, a Digital Object Identifier or DOI is a persistent identifier or handle used to uniquely identify objects, standardized by the ISO.[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they also have been used to identify other types of information resources, such as commercial videos.		A DOI aims to be "resolvable", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to uniquely identify their referents. The DOI system uses the indecs Content Model for representing metadata.		The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.[4][5][6]		The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.						A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the identifier, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is a series of at least 4 numbers greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]		For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The "10." part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).		DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.		The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.		The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".[15] Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL provides the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18] This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.		Major applications of the DOI system currently include:		In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[19]		A multilingual European DOI registration agency activity, mEDRA, and a Chinese registration agency, Wanfang Data, are active in non-English language markets.		The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.		The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[20] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[21] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[22]		The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.		The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.		A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[23]		A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[24]		The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).		A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.		DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.		To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.		Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as http://doi.org/ (preferred)[25] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.		Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.[26][27] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016.[28] While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[26][28]		An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[29] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.		The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[30] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.		The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.		Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[31]		Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.		The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[32] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[33] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[34] The final standard was published on 23 April 2012.[1]		DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[35]		The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[36]		The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:		URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.		
Quehanna Wild Area is a wildlife area in Cameron, Clearfield, and Elk counties in the U.S. state of Pennsylvania. At 48,186 acres (75 sq mi; 195 km2), it is the largest state forest wild area in Pennsylvania, and hosts herds of native elk. In the 19th and early 20th centuries, the logging industry cut the area's virgin forests. In 1955 the Curtiss-Wright Corporation bought 80 square miles (210 km2) of state forest for a facility developing nuclear-powered jet engines. A succession of tenants further contaminated the nuclear reactor facility and its hot cells with radioactive isotopes, including strontium-90 and cobalt-60. Pennsylvania reacquired the land in 1963 and 1967, and in 1965 established Quehanna as a wild area, but retained the nuclear facility and industrial complex. The facilities were used to treat hardwood flooring with radiation until 2002. The cleanup of the reactor and hot cells took over eight years and cost $30 million. Quehanna Wild Area has many sites with radioactive and toxic waste; some have been cleaned up, but others have been dug up by black bears and white-tailed deer. (Full article...)		Sophia Duleep Singh (b. 1876) · Ernest Lawrence (b. 1901) · Sheila Varian (b. 1937)		The Toyota FJ Cruiser is a mid-size SUV with styling and off road performance reminiscent of the original Toyota Land Cruiser (FJ40). Introduced as a concept car at the 2003 North American International Auto Show, a production version debuted two years later. Produced by the Toyota subsidiary Hino Motors and sharing many structural underpinnings with the Prado, the FJ Cruiser was discontinued in the United States in 2014 after selling more than 150,000 units. It was discontinued worldwide in 2016.		Photograph: Stefan Krause		Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects:		This Wikipedia is written in English. Started in 2001 (2001), it currently contains 5,455,999 articles. Many other Wikipedias are available; some of the largest are listed below.		
Bodybuilding is the use of progressive resistance exercise to control and develop one's musculature.[1] An individual who engages in this activity is referred to as a bodybuilder. In professional bodybuilding, bodybuilders appear in lineups and perform specified poses (and later individual posing routines) for a panel of judges who rank the competitors based on criteria such as symmetry, muscularity, and conditioning. Bodybuilders prepare for competitions through a combination of intentional dehydration, elimination of nonessential body fat, and carbohydrate loading to achieve maximum vascularity, as well as tanning to accentuate muscular definition.		The winner of the annual IFBB Mr. Olympia contest is generally recognized as the world's top male professional bodybuilder. The title is currently held by Phil Heath, who has won every year from 2011 to 2016. The winner of the Women's Physique portion of the competition is widely regarded as the world's top female professional bodybuilder. The title is currently held by Juliana Malacarne, who has won every year since 2014. Since 1950, the NABBA Universe Championships have been considered the top amateur bodybuilding contests, with notable winners such as Reg Park, Lee Priest, Steve Reeves, and Arnold Schwarzenegger.						Stone-lifting traditions were practised in ancient Egypt, Greece and Tamilakam.[2] Western weightlifting developed in Europe from 1880 to 1953, with strongmen displaying feats of strength for the public and challenging each other. The focus was not on their physique, and they often had large bellies and fatty limbs.[3]		Bodybuilding developed in the late 19th century, promoted in England by German Eugen Sandow, now called the "Father of Modern Bodybuilding". He allowed audiences to enjoy viewing his physique in "muscle display performances". Although audiences were thrilled to see a well-developed physique, the men simply displayed their bodies as part of strength demonstrations or wrestling matches. Sandow had a stage show built around these displays through his manager, Florenz Ziegfeld. The Oscar-winning 1936 musical film The Great Ziegfeld depicts this beginning of modern bodybuilding, when Sandow began to display his body for carnivals.		Sandow was so successful at flexing and posing his physique that he later created several businesses around his fame, and was among the first to market products branded with his name. He was credited with inventing and selling the first exercise equipment for the masses: machined dumbbells, spring pulleys, and tension bands. Even his image was sold by the thousands in "cabinet cards" and other prints. Sandow was a perfect "gracilian", a standard of ideal body proportions close to those of ancient Greek and Roman statues. Men were judged by how closely they matched these proportions.		Sandow organised the first bodybuilding contest on September 14, 1901, called the "Great Competition". It was held at the Royal Albert Hall in London. Judged by Sandow, Sir Charles Lawes, and Sir Arthur Conan Doyle, the contest was a great success and many bodybuilding enthusiasts were turned away due to the overwhelming amount of audience members.[4] The trophy presented to the winner was a gold statue of Sandow sculpted by Frederick Pomeroy. The winner was William L. Murray of Nottingham. The silver Sandow trophy was presented to second-place winner D. Cooper. The bronze Sandow trophy, now the most famous of all, was presented to third-place winner A.C. Smythe. In 1950, this same bronze trophy was presented to Steve Reeves for winning the inaugural NABBA Mr. Universe. It would not resurface again until 1977, when the winner of the IFBB Mr. Olympia contest, Frank Zane, was presented with the bronze trophy, or at least a replica of it. Since then, Mr. Olympia winners have been awarded a replica of the bronze trophy.		On January 16, 1904, the first large-scale bodybuilding competition in America took place at Madison Square Garden in New York City. The competition was promoted by Bernarr Macfadden, the father of physical culture and publisher of the original bodybuilding magazines such as Health & Strength. The winner was Al Treloar, who was declared "The Most Perfectly Developed Man in the World".[5] Treloar won a $1,000 cash prize, a substantial sum at that time. Two weeks later, Thomas Edison made a film of Treloar's posing routine. Edison had also made two films of Sandow a few years before. Those were the first three motion pictures featuring a bodybuilder. In the early 20th century, Macfadden and Charles Atlas continued to promote bodybuilding across the world. Alois P. Swoboda was an early pioneer in America.		Many other important bodybuilders in the early history of bodybuilding prior to 1930 include Earle Liederman (writer of some of the earliest bodybuilding instruction books), Zishe Breitbart, Georg Hackenschmidt, Emy Nkemena, George F. Jowett, Finn Hateral (a pioneer in the art of posing), Frank Saldo, Monte Saldo, William Bankier, Launceston Elliot, Sig Klein, Sgt. Alfred Moss, Joe Nordquist, Lionel Strongfort ("Strongfortism"),[6] Gustav Frištenský, Ralph Parcaut (a champion wrestler who also authored an early book on "physical culture"), and Alan P. Mead (who became an impressive muscle champion despite the fact that he lost a leg in World War I). Actor Francis X. Bushman, who was a disciple of Sandow, started his career as a bodybuilder and sculptor's model before beginning his famous silent movie career.		Bodybuilding became more popular in the 1950s and 1960s with the emergence of strength and gymnastics champions, and the simultaneous popularization of bodybuilding magazines, training principles, nutrition for bulking up and cutting down, the use of protein and other food supplements, and the opportunity to enter physique contests. The number of bodybuilding organizations grew, most notably the International Federation of Bodybuilders (IFBB), founded by Canadian brothers Joe and Ben Weider. Other bodybuilding organizations included the Amateur Athletic Union (AAU), National Amateur Bodybuilding Association (NABBA), and the World Bodybuilding Guild (WBBG). Consequently, the male-dominated contests grew both in number and in size. Besides the many "Mr. (insert town, city, state, or region)" championships, the most prestigious titles[according to whom?] were Mr. America, Mr. World, Mr. Universe, Mr. Galaxy, and ultimately Mr. Olympia (which was started in 1965 by the IFBB and is now considered the most important bodybuilding competition in the world).		During the 1950s, the most famous competing bodybuilders[according to whom?] were Bill Pearl, Reg Park, Leroy Colbert, and Clarence Ross. Certain bodybuilders rose to fame thanks to the relatively new medium of television, as well as movies. The most notable[according to whom?] were Jack LaLanne, Steve Reeves, Reg Park, and Mickey Hargitay. While there were well-known gyms throughout the country during the 1950s (such as Vince's Gym in North Hollywood, California and Vic Tanny's chain gyms), there were still segments of the United States that had no "hardcore" bodybuilding gyms until the advent of Gold's Gym in the mid-1960s. Finally, the famed Muscle Beach in Santa Monica, California continued its popularity as the place to be for witnessing acrobatic acts, feats of strength, and the like. The 1960s grew more in TV and movie exposure, as bodybuilders were typecast in popular shows and movies.[citation needed]		In the 1970s, bodybuilding had major publicity thanks to the appearance of Arnold Schwarzenegger, Franco Columbu, Lou Ferrigno, and others in the 1977 docudrama Pumping Iron. By this time, the IFBB dominated the competitive bodybuilding landscape and the Amateur Athletic Union (AAU) took a back seat. The National Physique Committee (NPC) was formed in 1981 by Jim Manion,[7] who had just stepped down as chairman of the AAU Physique Committee. The NPC has gone on to become the most successful bodybuilding organization in America, and is the amateur division of the IFBB. The late 1980s and early 1990s saw the decline of AAU-sponsored bodybuilding contests. In 1999, the AAU voted to discontinue its bodybuilding events.		This period also saw the rise of anabolic steroids in bodybuilding and many other sports. In bodybuilding lore, this is partly attributed to the rise of "mass monsters", beginning with Arnold Schwarzenegger, Sergio Oliva, and Lou Ferrigno in the late 1960s and early 1970s, and continuing to the present day with Lee Haney, Dorian Yates, Ronnie Coleman, and Markus Rühl. Bodybuilders such as Greg Kovacs and Paul Demayo attained mass and size that were not seen previously, but were not particularly successful at the pro level. At the time of shooting Pumping Iron, Schwarzenegger (while never admitting to steroid use until long after his retirement) said that "you have to do anything you can to get the advantage in competition".[citation needed] He would later say that he does not regret using anything.[8]		To combat steroid use and in the hopes of becoming a member of the IOC, the IFBB introduced doping tests for both steroids and other banned substances. Although doping tests occurred, the majority of professional bodybuilders still used anabolic steroids for competition. During the 1970s, the use of anabolic steroids was openly discussed, partly due to the fact they were legal.[9] In the Anabolic Steroid Control Act of 1990, U.S. Congress placed anabolic steroids into Schedule III of the Controlled Substances Act (CSA). In Canada, steroids were added to the Canadian Criminal Code as a Class IV controlled substance (a class created expressly for steroids).		In 1990, professional wrestling promoter Vince McMahon announced that he was forming a new bodybuilding organization named the World Bodybuilding Federation (WBF). McMahon wanted to bring WWF-style showmanship and bigger prize money to the sport of bodybuilding. A number of IFBB stars were recruited but the roster was never very large, and featured the same athletes competing; the most notable winner and first WBF champion was Gary Strydom. McMahon formally dissolved the WBF in July 1992. Reasons for this reportedly included lack of income from the pay-per-view broadcasts of the contests, slow sales of the WBF's magazine Bodybuilding Lifestyles (later WBF Magazine), and the expense of paying multiple six-figure contracts while producing two TV shows and a monthly magazine.		In 2003, Joe Weider sold Weider Publications to AMI, which owns the National Enquirer. The position of president of the IFBB was filled by Rafael Santonja following the death of Ben Weider in October 2008. In 2004, contest promoter Wayne DeMilia broke ranks with the IFBB and AMI took over the promotion of the Mr. Olympia contest. Other professional contests emerged in this period, such as the Arnold Classic, Night of Champions, and the European Grand Prix of Bodybuilding.		In the early 21st century, patterns of consumption and recreation similar to those of the United States became more widespread in Europe and especially in Eastern Europe following the collapse of the Soviet Union. This resulted in the emergence of whole new populations of bodybuilders emerged from former Eastern Bloc states.		In the early 2000s, the IFBB was attempting to make bodybuilding an Olympic sport. It obtained full IOC membership in 2000 and was attempting to get approved as a demonstration event at the Olympics, which would hopefully lead to it being added as a full contest. This did not happen and Olympic recognition for bodybuilding remains controversial, since many argue that bodybuilding is not a sport.[10]		In 2014, the FTM Fitness Conference hosted the FTM Fitness World Bodybuilding Competition, the first bodybuilding competition for transgender men.[11]		In the modern bodybuilding industry, the term "professional" generally means a bodybuilder who has won qualifying competitions as an amateur and has earned a "pro card" from their respective organization. Professionals earn the right to compete in competitions that include monetary prizes. A pro card also prohibits the athlete from competing in federations other than the one from which they have received the pro card.[12] Depending on the level of success, these bodybuilders may receive monetary compensation from sponsors, much like athletes in other sports.		Due to the growing concerns of the high cost, health consequences, and illegal nature of some steroids, many organizations have formed in response and have deemed themselves "natural" bodybuilding competitions. In addition to the concerns noted, many promoters of bodybuilding have sought to shed the "freakish" perception that the general public has of bodybuilding and have successfully introduced a more mainstream audience to the sport of bodybuilding by including competitors whose physiques appear much more attainable and realistic.		In natural contests, the testing protocol ranges among organizations from lie detectors to urinalysis. Penalties also range from organization to organization from suspensions to strict bans from competition. It is also important to note that natural organizations also have their own list of banned substances and it is important to refer to each organization's website for more information about which substances are banned from competition. There are many natural bodybuilding organizations. Some of the larger ones include MuscleMania, Ultimate Fitness Events (UFE), INBF/WNBF, and INBA/PNBA. These organizations either have American or worldwide presence and are not limited to the country in which they are headquartered.		Other notable natural bodybuilding organization include the National Physique Committee (NPC) and the North American Natural Bodybuilding Federation (NANBF). NPC competitions screen competitors using ineffective lie detector tests to ensure fair practices. Such tests are very error prone, and some competitors are not even tested.		This is how the NPC differs from the NANBF. The NANBF takes a more direct approach by taking urine samples from all competitors that are tested for steroids and any other substances on the banned list. The NANBF also differs from the NPC when it comes to judging. The criteria of certain poses differs from organization to organization. The NANBF even has an elevated calf pose which is unique for their competitions.		The first U.S. Women's National Physique Championship, promoted by Henry McGhee and held in Canton, Ohio in 1978, is generally regarded as the first true female bodybuilding contest—that is, the first contest where the entrants were judged solely on muscularity.[13] In 1980, the first Ms. Olympia (initially known as the "Miss" Olympia), the most prestigious contest for professionals, was held. The first winner was Rachel McLish, who had also won the NPC's USA Championship earlier in the year. The contest was a major turning point for female bodybuilding. McLish inspired many future competitors to start training and competing. In 1985, a movie called Pumping Iron II: The Women was released. It documented the preparation of several women for the 1983 Caesars Palace World Cup Championship. Competitors prominently featured in the film were Kris Alexander, Lori Bowen, Lydia Cheng, Carla Dunlap, Bev Francis, and McLish. At the time, Francis was actually a powerlifter, though she soon made a successful transition to bodybuilding, becoming one of the leading competitors of the late 1980s and early 1990s.		In recent years, the related areas of fitness and figure competition have increased in popularity, surpassing that of female bodybuilding, and have provided an alternative for women who choose not to develop the level of muscularity necessary for bodybuilding. McLish would closely resemble what is thought of today as a fitness and figure competitor, instead of what is now considered a female bodybuilder. Fitness competitions also have a gymnastic element to them. A study by the Clinical Journal of Sport Medicine found that female bodybuilders who are taking anabolic steroids are more likely to have qualified for substance dependence disorder and have been diagnosed with a psychiatric illness and have a history of sexual abuse.[14]		E Wilma Conner competed in the 2011 NPC Armbrust Pro Gym Warrior Classic Championships in Loveland, Colorado, at the age of 75 years and 349 days.[15]		In competitive bodybuilding, bodybuilders aspire to develop and maintain an aesthetically pleasing body and balanced physique.[16][17] In prejudging, competitors do a series of mandatory poses: the front lat spread, rear lat spread, front double biceps, back double biceps, side chest, side triceps, Most Muscular (men only), and the thigh abdominal. Each competitor also performs a routine to display their physique. A posedown is usually held at the end of a posing round, while judges are finishing their scoring. Bodybuilders spend a lot of time practising their posing in mirrors.		In contrast to strongman or powerlifting competitions, where physical strength is important, or to Olympic weightlifting, where the main point is equally split between strength and technique, bodybuilding competitions typically emphasize condition, size, and symmetry. Different organizations emphasize particular aspects of competition, and sometimes have different categories in which to compete.		The general strategy adopted by most present-day competitive bodybuilders is to make muscle gains for most of the year (known as the "off-season") and, approximately 12–14 weeks from competition, attempt to lose body fat (referred to as "cutting"). The bulking phase entails remaining in a net positive energy balance (calorie surplus). The amount of a surplus in which a person remains is based on the person's goals, as a bigger surplus and longer bulking phase will create more fat tissue. The surplus of calories relative to one's energy balance will ensure that muscles remain in a state of growth.		The cutting phase entails remaining in a net negative energy balance (calorie deficit). The main goal of cutting is to oxidize fat while preserving as much muscle as possible. The larger the calorie deficit, the faster one will lose weight. However, a large calorie deficit will also create the risk of losing muscle tissue.[18]		The precise effectiveness of the cutting and bulking strategy is unknown, with only limited observational case studies on the subject. No studies involving precise hypercaloric feeding combined with resistance exercise have been conducted.		Many non-competitive bodybuilders choose not to adopt the conventional strategy, as it often results in significant unwanted fat gain during the "bulking" phase. The attempt to increase muscle mass in one's body without any gain in fat is called clean bulking. Competitive bodybuilders focus their efforts to achieve a peak appearance during a brief "competition season".[citation needed]		"Dirty bulking" is the process of eating at a caloric surplus, without finding the exact number of macronutrients (carbs, fats, and proteins). Weight lifters who are attempting to gain mass quickly often choose to use the "dirty bulk" method.[19]		In the week leading up to a contest, bodybuilders may decrease their consumption of water, sodium, and carbohydrates, the former two to alter how water is retained by the body and the latter to reduce glycogen in the muscle. The day before the show, water is removed from the diet, and diuretics may be introduced, while carbohydrate loading is undertaken to increase the size of the muscles through replenishment of their glycogen. The goal is to maximize leanness and increase the visibility of veins, or "vascularity". The appearance of veins is further enhanced immediately before appearing on stage by darkening the skin through tanning products and applying oils to the skin to increase shine. Some competitors will eat sugar-rich foods to increase the visibility of their veins. A final step is the use of weights to fill the muscles with blood and further increase their size.		Short or long term health issues may arise from Competitive Bodybuilding preparations, caused by abuse and/or dangerous/unsupervised Anabolic Steroid use, extreme dieting and/or training, diuretics, Human Growth hormone, Insulin, and other related drugs. Numerous deaths, severe diseases and stage fainting have been related to these protocols and routines since the early 80's, including physically visible issues in bodies seen on competitors, such as "Palumboism". (a term coined on the former bodybuilder and former Muscular Development columnist Dave Palumbo ) a strange, not yet studied or medically understood condition alleged to be caused by an abnormal growth of abdominal muscles. stomach walls, and organs, related to believed growth hormone and insulin abuse, creating a "belly", and consequently causing a distortion in the competitor physique.		Bodybuilders use three main strategies to maximize muscle hypertrophy:		Bodybuilders often shorten these three steps into the well-known motto "eat clean, train hard, sleep well".		Weight training causes micro-tears to the muscles being trained; this is generally known as microtrauma. These micro-tears in the muscle contribute to the soreness felt after exercise, called delayed onset muscle soreness (DOMS). It is the repair to these micro-trauma that result in muscle growth. Normally, this soreness becomes most apparent a day or two after a workout. However, as muscles become adapted to the exercises, soreness tends to decrease.[20]		Weight training aims to build muscle by prompting two different types of hypertrophy: sarcoplasmic and myofibrillar. Sarcoplasmic hypertrophy leads to larger muscles and so is favored by bodybuilders more than myofibrillar hypertrophy, which builds athletic strength. Sarcoplasmic hypertrophy is triggered by increasing repetitions, whereas myofibrillar hypertrophy is triggered by lifting heavier weight.[21] In either case, there is an increase in size and strength of the muscles (compared to if that same individual does not lift weights at all). However, the emphasis is different.		Many trainees like to cycle between the two methods in order to prevent the body from adapting (maintaining a progressive overload), possibly emphasizing whichever method more suits their goals. i.e. a bodybuilder will use sarcoplasmic hypertrophy most of the time, but may change to myofibrillar hypertrophy temporarily in order to move past a plateau. However, no real evidence has been provided to show that trainees ever reach this plateau, and rather was more of a hype created from "muscular confusion".		The high levels of muscle growth and repair achieved by bodybuilders require a specialized diet. Generally speaking, bodybuilders require more calories than the average person of the same weight to provide the protein and energy requirements needed to support their training and increase muscle mass. A sub-maintenance level of food energy is combined with cardiovascular exercise to lose body fat in preparation for a contest. The ratios of calories from carbohydrates, proteins, and fats vary depending on the goals of the bodybuilder.[22]		Carbohydrates play an important role for bodybuilders. They give the body energy to deal with the rigors of training and recovery. Carbohydrates also promote secretion of insulin, a hormone enabling cells to get the glucose they need. Insulin also carries amino acids into cells and promotes protein synthesis.[23] Insulin has steroid-like effects in terms of muscle gains.[citation needed] It is impossible to promote protein synthesis without the existence of insulin, which means that without ingesting carbohydrates or protein—which also induces the release of insulin—it is impossible to add muscle mass.[24] Bodybuilders seek out low-glycemic polysaccharides and other slowly digesting carbohydrates, which release energy in a more stable fashion than high-glycemic sugars and starches. This is important as high-glycemic carbohydrates cause a sharp insulin response, which places the body in a state where it is likely to store additional food energy as fat. However, bodybuilders frequently do ingest some quickly digesting sugars (often in form of pure dextrose or maltodextrin) after a workout. This may help to replenish glycogen stores within the muscle, and to stimulate muscle protein synthesis.[25]		The motor proteins actin and myosin generate the forces exerted by contracting muscles. Current advice says that bodybuilders should consume 25–30% of protein per total calorie intake to further their goal of maintaining and improving their body composition.[26] This is a widely debated topic, with many arguing that 1 gram of protein per pound of body weight per day is ideal, some suggesting that less is sufficient, while others recommending 1.5, 2, or more.[27] It is believed that protein needs to be consumed frequently throughout the day, especially during/after a workout, and before sleep.[28] There is also some debate concerning the best type of protein to take. Chicken, turkey, beef, pork, fish, eggs and dairy foods are high in protein, as are some nuts, seeds, beans and lentils. Casein or whey are often used to supplement the diet with additional protein. Whey protein is the type of protein contained in many popular brands of protein supplements, and is preferred by many bodybuilders because of its high Biological Value (BV) and quick absorption rates. However, whey has a bigger effect than Casein on insulin levels. Whey triggers about double the amount of insulin release.[29] That effect is somewhat overcome by combining Casein and whey. Bodybuilders are usually thought to require protein with a higher BV than that of soy, which is additionally avoided due to its claimed estrogenic properties. Still, some nutrition experts believe that soy, flax seeds and many other plants that contain the weak estrogen-like compounds or phytoestrogens can be used beneficially, as phytoestrogens compete with estrogens for receptor sites in the male body and can block its actions. This can also include some inhibition of pituitary functions while stimulating the P450 system (the system that eliminates hormones, drugs and metabolic waste product from the body) in the liver to more actively process and excrete excess estrogen.[30][31] Cortisol decreases amino acid uptake by muscle, and inhibits protein synthesis.[32]		Contrary to certain rumors that animal-based protein is more suitable to trigger muscle growth than plant-based protein, a study by Mangano et al. (2017) could not provide any evidence for this.[33] In contrast, if combined properly plant-based protein even has a higher biological quality. A combination of one part wheat protein (e.g. seitan) and two parts soy protein (e.g. tofu) has thus been favored by many bodybuilders. Some bodybuilders, such as Patrik Baboumian and Robert Cheeke, follow a strict vegan diet.[citation needed]		Bodybuilders often split their food intake for the day into 5 to 7 meals of roughly equal nutritional content and attempt to eat at regular intervals (e.g. every 2 to 3 hours). This method can serve two purposes: to limit overindulging in the cutting phase, and to physically allow for the consumption of large volumes of food during the bulking phase. Contrary to popular belief, eating more frequently does not increase basal metabolic rate when compared to the traditional 3 meals a day. While food does have a metabolic cost to digest, absorb, and store, called the thermic effect of food, it depends on the quantity and type of food, not how the food is spread across the meals of the day. Well-controlled studies using whole-body calorimetry and doubly labeled water have demonstrated that there is no metabolic advantage to eating more frequently.[34][35][36]		The important role of nutrition in building muscle and losing fat means bodybuilders may consume a wide variety of dietary supplements.[37] Various products are used in an attempt to augment muscle size, increase the rate of fat loss, improve joint health, increase natural testosterone production, enhance training performance and prevent potential nutrient deficiencies. There are three major macronutrients that the human body needs in order for muscle building. The major nutrients – protein, carbohydrate, and fat – provide the body with energy.[38]		Some bodybuilders use drugs such as anabolic steroids and precursor substances such as prohormones to increase muscle hypertrophy. Anabolic steroids cause muscle hypertrophy of both types (I and II) of muscle fibers caused likely by an increased synthesis of muscle proteins and are accompanied with undesired side effects including hepatotoxicity, gynecomastia, acne, early onset male pattern baldness and a decline in the body's own testosterone production, which can cause testicular atrophy.[39][40][41] Other performance-enhancing substances used by competitive bodybuilders include human growth hormone (HGH), which can cause acromegaly.		Muscle growth is more difficult to achieve in older adults than younger adults because of biological aging, which leads to many metabolic changes detrimental to muscle growth; for instance, by diminishing growth hormone and testosterone. Some recent clinical studies have shown that low-dose HGH treatment for adults with HGH deficiency changes the body composition by increasing muscle mass, decreasing fat mass, increasing bone density and muscle strength, improves cardiovascular parameters, and affects the quality of life without significant side effects.[42][43][unreliable medical source?][44]		A recent trend in bodybuilding is to inject synthol[45] into muscles to create larger bulges, or injecting PMMA into muscles to shape them. Use of PMMA to shape muscles is prohibited in the United States.[46]		Although muscle stimulation occurs in the gym (or home gym) when lifting weights, muscle growth occurs afterward during rest. Without adequate rest and sleep (6 to 8 hours), muscles do not have an opportunity to recover and build. About eight hours of sleep a night is desirable for the bodybuilder to be refreshed, although this varies from person to person.[47] Additionally, many athletes find a daytime nap further increases their body's ability to build muscles. Some individual bodybuilders add a massage, sometimes by professional masseuse, massager or masseur at the end of each workout to their routine as a method of recovering.[48]		Overtraining occurs when a bodybuilder has trained to the point where his workload exceeds his recovery capacity. There are many reasons that overtraining occurs, including lack of adequate nutrition, lack of recovery time between workouts, insufficient sleep, and training at a high intensity for too long (a lack of splitting apart workouts). Training at a high intensity too frequently also stimulates the central nervous system (CNS) and can result in a hyper-adrenergic state that interferes with sleep patterns.[49] To avoid overtraining, intense frequent training must be met with at least an equal amount of purposeful recovery. Timely provision of carbohydrates, proteins, and various micronutrients such as vitamins, minerals, phytochemicals, even nutritional supplements are acutely critical.		It has been argued that overtraining can be beneficial. One article published by Muscle & Fitness magazine stated that you can "Overtrain for Big Gains". It suggested that if one is planning a restful holiday and they do not wish to inhibit their bodybuilding lifestyle too much, they should overtrain before taking the holiday, so the body can rest easily and recuperate and grow. Overtraining can be used advantageously, as when a bodybuilder is purposely overtrained for a brief period of time to super compensate during a regeneration phase. These are known as "shock micro-cycles" and were a key training technique used by Soviet athletes.[50]		Some bodybuilders, particularly at the professional level, inject substances such as "site enhancement oil", commonly known as synthol, to mimic the appearance of developed muscle where it may otherwise be disproportionate or lagging.[51] This is known as "fluffing".[52][53] Synthol is 85% oil, 7.5% lidocaine, and 7.5% alcohol.[52] It is not restricted, and many brands are available on the Internet.[54] The use of injected oil to enhance muscle appearance is common among bodybuilders,[55][56] despite the fact that synthol can cause pulmonary embolisms, nerve damage, infections, sclerosing lipogranuloma,[57] stroke,[52] and the formation of oil-filled granulomas, cysts or ulcers in the muscle.[56][58][59] Sesame oil is often used, which can cause allergic reactions such as vasculitis.[60] An aesthetic issue is drooping of muscle under gravity.[54]		
A personal trainer is an individual certified to have a varying degree of knowledge of general fitness involved in exercise prescription and instruction. They motivate clients by setting goals and providing feedback and accountability to clients. Trainers also measure their client's strengths and weaknesses with fitness assessments. These fitness assessments may also be performed before and after an exercise program to measure their client's improvements in physical fitness. They may also educate their clients in many other aspects of wellness besides exercise, including general health and nutrition guidelines.		Qualified personal trainers recognize their own areas of expertise. If a trainer suspects that one of his or her clients has a medical condition that could prevent the client from safe participation in an exercise program, they must refer the client to the proper health professional for prior clearance.[1]						The scope of practice for a personal trainer is to enhance the components of fitness for the general, healthy population.		Proper exercise prescription may result in improved body composition, physical performance, heart condition and health outcomes.[2] The decision to hire a trainer may be related to a perceived ability to facilitate these factors through proper prescription and instruction or factors related to motivation and adherence. A trainer pays close attention to their client's exercise form, workout routine, and nutrition plan.		Few studies have investigated training for men, however, training in women has been shown to exercise behavior patterns, improve perceptual benefit-to-concern ratio for exercise (decisional balance), and increase confidence to choose exercise in the face of other time demands (scheduling self-efficacy).[3] Personal training results in higher strength, higher workout intensities, and higher perceived exertion during exercise in women. Although women working with personal trainers do self-select heavier loads than women who did not, the loads used are still below recommended training load percentages.[4]		The profession is generally not restricted by venue, and personal trainers may work in fitness facilities, in their personal homes, in client homes, over live video (also called "virtual personal trainers"),[5] or outdoors. Almost all personal trainers and group exercise instructors work in physical fitness facilities, health clubs, and fitness centers located in the amusement and recreation industry or in civic and social organizations.[6] Personal training is not regulated in any jurisdiction in the United States except for Washington D.C. which adopted registration requirements for personal fitness trainers in February 2014.[7]		Personal trainers may specialize in a certain training type, training philosophy, performance type, exercise modality, or client population. In general, most personal trainers develop exercise prescription plans for aerobic exercise, resistance exercise, and/or flexibility training. With aerobic exercise prescription, personal trainers determine the type of exercise, duration of exercise, and frequency of exercise. For resistance exercise prescription, the type of exercise, total session volume, rest period, frequency, and intensity are determined.[8] Personal trainers may also be involved in prescription of stretching routines or other approaches. While some discuss nutrition, ergogenic supplementation, and spiritual practices with clients, there is debate within the industry as to whether it fits within their scope of practice and training qualifications.[9]		Personal trainer accreditation is a process that provides certification of competency as a personal trainer. Qualification standards for personal trainers vary between countries.		In Australia, personal trainers may work independently with suitable insurance or choose to be a member of a registering body (Fitness Australia or Physical Activity Australia). The qualifications levels include; Level 1 - Certificate III in Fitness, Level 2 - Certificate IV in Fitness and Level 3 - Diploma of Fitness. These can be obtained from nationally accredited colleges (TAFE, Australian College of Sport & Fitness, Fitness Industry Training, Global Fitness Institute, Australian Institute of Fitness, Australian Fitness Academy). Once working in the industry, trainers who are members of associations are also required to complete short courses to obtain continuing education credit (CEC) points they need to keep their registration. A minimum of 20 CEC points every two years is required. Many personal trainers also have additional qualifications in weight loss, strength training, kid's fitness, and nutrition, which is in part due to the CEC program. CEC courses can cover a wide variety of topics such as different training techniques, nutrition, exercise styles, health conditions, physiology, lifestyle and rehabilitation.[10]		In Brazil, personal trainers must have a bachelor's degree in "Physical Education" (a degree that combines knowledge in the fields of Exercise Science and Healthcare science) and be registered with the Conselho Federal de Educação Física (Federal Council of Physical Education), and risk criminal charges if they operate without these two requirements.		In Canada, the main certifying bodies are Canadian Fitness Education Services (CFES), Canadian Fitness Professionals, Certified Personal Trainers Network, and Canadian Society of Exercise Physiology. CSEP requires a diploma or degree in the exercise field, most require experience and/or workshops. Ontario does not have any personal training regulation. Many personal trainers receive a CFES, CanFit Pro certification or an NCCA accredited certification. The National Personal Training Institute is the only private trade school and is a registered College under the Private Career Colleges Act.[11][12]		In the UK, there are several ways to achieve a personal training qualification. Most personal training qualifications are accredited through awarding bodies like CYQ (Central YMCA Qualifications), Active IQ (Active International Qualifications) and City and Guilds. These qualifications are generally delivered by Further Education (FE) establishments like colleges, or by private training providers. Upon successful completion of an accredited awarding body qualification, candidates become eligible for Level 3 REPs(Register of Exercise Professionals)status. University graduates with an appropriate honours degree can also apply to become an approved by REPs through Accreditation of Prior Learning (APL) and Accreditation of Prior Achievement (APA).		REPs is the professional body for the UK health and fitness industry, and does not award qualifications directly. Most health and fitness qualifications endorsed by REPs vary in levels from 1 - 5, 1 being basic GCSE level and 5 being advanced specialized training professionals.[13] For a qualification to become eligible for endorsement by REPs, it must conform to the National Occupational Standards (NOS), which are set at governmental level by the Sector Skills Council (SSC) Skills Active.		There is no legal restriction on the title of Personal Trainer nor any formal body associated with regulating Personal Training.		A number of certifications are available in the U.S., although a number are not accredited. Most require a high school diploma, cardiopulmonary resuscitation (CPR) and automated external defibrillator (AED) certification, and some type of examination.[6]		A 2002 investigation evaluated a random sample of 115 personal trainers using the Fitness Instructors Knowledge Assessment (FIKA) (which measures knowledge in nutrition, health screening, testing protocols, exercise prescription, and special populations). The study described that:[14][15]		In partnership with the fitness industry, the International Health, Racquet & Sportsclub Association (IHRSA), which represents over 9,000 health and fitness facilities, started an initiative in 2002 to improve standards for both its own clubs and the industry as a whole. In January 2006, IHRSA implemented a recommendation that its facilities only accept personal trainers with certifications recognized by the National Commission for Certifying Agencies (NCCA) if recognized either by the Council for Higher Education Accreditation (CHEA) and/or the U.S. Department of Education (USDE). As a result, the Distance Education and Training Council (DETC) was recognized by IHRSA as a recognized accreditor of fitness professional certification organizations. Since then, the DETC has accredited several personal trainer certification organizations, including the Aerobics and Fitness Association of America (AFAA) and the International Sports Sciences Association (ISSA) among others. As of August 2012, NASM, ISSA, AFAA, ACSM and NSCA certifications are among the 15 accredited certifications recognized by IHRSA, three of which are accredited by the Distance Education Training Council (DETC).[16][17]		Various organizations within the profession have lobbied for the adoption of a more stringent criteria for certification developed by the NSF International.[18] There remains no national legal restriction on the industry to date except for the District of Columbia (D.C.) which as of February 2014, passed legislation requiring personal fitness trainers to register in that jurisdiction. The law is expected to go into effect in the first half of 2014.		
Weight training is a common type of strength training for developing the strength and size of skeletal muscles. It utilizes the force of gravity in the form of weighted bars, dumbbells or weight stacks in order to oppose the force generated by muscle through concentric or eccentric contraction. Weight training uses a variety of specialized equipment to target specific muscle groups and types of movement.		Sports where strength training is central are bodybuilding, weightlifting, powerlifting, and strongman, highland games, shot put, discus throw, and javelin throw. Many other sports use strength training as part of their training regimen, notably; mixed martial arts, American football, wrestling, rugby football, track and field, rowing, lacrosse, basketball, baseball, and hockey. Strength training for other sports and physical activities is becoming increasingly popular.						Strength training is an inclusive term that describes all exercises devoted toward increasing physical strength. Weight training is a type of strength training that uses weights, Eccentric Training or muscular resistance to increase strength. Endurance training is associated with aerobic exercise while flexibility training is associated with stretching exercise like yoga or pilates. Weight training is often used as a synonym for strength training, but is actually a specific type within the more inclusive category. Contrary to popular belief, weight training can be beneficial for both men and women.		The genealogy of lifting can be traced back to the beginning of history[1] where humanity's fascination with physical abilities can be found among numerous ancient writings. Progressive resistance training dates back at least to Ancient Greece, when legend has it that wrestler Milo of Croton trained by carrying a newborn calf on his back every day until it was fully grown. Another Greek, the physician Galen, described strength training exercises using the halteres (an early form of dumbbell) in the 2nd century.		Ancient Greek sculptures also depict lifting feats. The weights were generally stones, but later gave way to dumbbells. The dumbbell was joined by the barbell in the later half of the 19th century. Early barbells had hollow globes that could be filled with sand or lead shot, but by the end of the century these were replaced by the plate-loading barbell commonly used today.[2]		Another early device was the Indian club, which came from ancient Persia where it was called the "meels". It subsequently became popular during the 19th century, and has recently made a comeback in the form of the clubbell.		The 1960s saw the gradual introduction of exercise machines into the still-rare strength training gyms of the time. Weight training became increasingly popular in the 1970s, following the release of the bodybuilding movie Pumping Iron, and the subsequent popularity of Arnold Schwarzenegger. Since the late 1990s increasing numbers of women have taken up weight training, influenced by programs like Body for Life; currently nearly one in five U.S. women engage in weight training on a regular basis.[3]		The basic principles of weight training are essentially identical to those of strength training, and involve a manipulation of the number of repetitions (reps), sets, tempo, exercise types, and weight moved to cause desired increases in strength, endurance, and size. The specific combinations of reps, sets, exercises, and weights depends on the aims of the individual performing the exercise. Sets with fewer reps can be performed with heavier weights contributing to an increase in lean muscle mass and sets with higher reps can be performed with lighter weights contributing to increased muscular endurance.		In addition to the basic principles of strength training, a further consideration added by weight training is the equipment used. Types of equipment include barbells, dumbbells, pulleys and stacks in the form of weight machines, and the body's own weight in the case of chin-ups and push-ups. Different types of weights will give different types of resistance, and often the same absolute weight can have different relative weights depending on the type of equipment used. For example, lifting 10 kilograms using a dumbbell sometimes requires more force than moving 10 kilograms on a weight stack if certain pulley arrangements are used. In other cases, the weight stack may require more force than the equivalent dumbbell weight due to additional torque or resistance in the machine. Additionally, although they may display the same weight stack, different machines may be heavier or lighter depending on the number of pulleys and their arrangements.		Weight training also requires the use of 'good form', performing the movements with the appropriate muscle group, and not transferring the weight to different body parts in order to move greater weight (called 'cheating'). Failure to use good form during a training set can result in injury or a failure to meet training goals; since the desired muscle group is not challenged sufficiently, the threshold of overload is never reached and the muscle does not gain in strength. At a particularly advanced level; however, "cheating" can be used to break through strength plateaus and encourage neurological and muscular adaptation.		The benefits of weight training overall are comparable to most other types of strength training: increased muscle, tendon and ligament strength, bone density, flexibility, tone, metabolic rate, and postural support. This type of training will also help prevent injury for athletes. There are benefits and limitations to weight training as compared to other types of strength training.		Isometric exercise provides a maximum amount of resistance based on the force output of the muscle, or muscles pitted against one another. This maximum force maximally strengthens the muscles over all of the joint angles at which the isometric exercise occurs. By comparison, weight training also strengthens the muscle throughout the range of motion the joint is trained in, but only maximally at one angle, causing a lesser increase in physical strength at other angles from the initial through terminating joint angle as compared with isometric exercise. In addition, the risk of injury from weights used in weight training is greater than with isometric exercise (no weights), and the risk of asymmetric training is also greater than with isometric exercise of identical opposing muscles.		Although weight training is similar to bodybuilding, they have different objectives. Bodybuilders use weight training to develop their muscles for size, shape, and symmetry regardless of any increase in strength for competition in bodybuilding contests; they train to maximize their muscular size and develop extremely low levels of body fat. In contrast, many weight trainers train to improve their strength and anaerobic endurance while not giving special attention to reducing body fat far below normal.		The bodybuilding community has been the source of many of weight training's principles, techniques, vocabulary, and customs. Weight training does allow tremendous flexibility in exercises and weights which can allow bodybuilders to target specific muscles and muscle groups, as well as attain specific goals. Not all bodybuilding is undertaken to compete in bodybuilding contests and, in fact, the vast majority of bodybuilders never compete, but bodybuild for their own personal reasons.		Weight training is a safe form of exercise when the movements are controlled and carefully defined. However, as with any form of exercise, improper execution and the failure to take appropriate precautions can result in injury.		Maintaining proper form is one of the many steps in order to perfectly perform a certain technique. Correct form in weight training improves strength, muscle tone, and maintaining a healthy weight. Proper form will prevent any strains or fractures.[5] When the exercise becomes difficult towards the end of a set, there is a temptation to cheat, i.e., to use poor form to recruit other muscle groups to assist the effort. Avoid heavy weight and keep the number of repetitions to a minimum. This may shift the effort to weaker muscles that cannot handle the weight. For example, the squat and the deadlift are used to exercise the largest muscles in the body—the leg and buttock muscles—so they require substantial weight. Beginners are tempted to round their back while performing these exercises. The relaxation of the spinal erectors which allows the lower back to round can cause shearing in the vertebrae of the lumbar spine, potentially damaging the spinal discs.		Weight trainers commonly spend 5 to 20 minutes warming up their muscles before starting a workout. It is common to stretch the entire body to increase overall flexibility; however, many people stretch just the area being worked that day. The main reason for warming up is injury prevention. Warming up increases blood flow and flexibility, which lessens the chance of a muscle pull or joint pain.		Warm up sets are also important. For example, the same lifter working on his chest would also be advised to complete at least two warm up sets prior to hitting his "core tonnage." Core tonnage refers to the heavier lifts that actually strain your muscles. For example, if the lifter's main sets were at 205 lbs, 225 lbs and 235 lbs on the bench, then a warmup of 5 reps of 135 and 5 reps of 185 would be advisable. Some lifters will warm up with a 50/50 set for example 50% of the target weight for 50% of the target repetitions. When properly warmed up the lifter will then have more strength and stamina since the blood has begun to flow to the muscle groups.[6]		Breathing shallowly or holding one's breath while working out limits the oxygen supply to the muscles and the brain, decreasing performance and, under extreme stress, risking a black-out or a stroke by aneurysm.[7] Most trainers advise weight trainees to consciously "exhale on effort" and to inhale when lowering the weight. This technique ensures that the trainee breathes through the most difficult part of the exercise, where one would reflexively hold one's breath.[8]		However, biomechanics and kinesiology expert Stuart McGill indicates that spine stabilization is assured by "the ability to cocontract the abdominal wall (abdominal brace) independently of any lung ventilation patterns. Good stabilizers maintain the critical symmetrical muscle stiffness...Poor stabilizers allow abdominal contraction levels to cycle with breathing at critical moments when stability is required. Grooving muscular activation patterns so that a particular direction in lung air flow is entrained to a particular part of any exertion is not helpful. This would be of little carryover value to other activities; in fact it would be counterproductive." [9]		Other coaches advise trainees to perform the valsalva maneuver during exercises which place a load on the spine, since the risk of a stroke by aneurysm is astronomically lower than the risk of an orthopedic injury caused by inadequate rigidity of the torso.[10] Stuart McGill adds that the mechanism of building "high levels of intra-abdominal pressure (IAP)...produced by breath holding using the Valsava maneuver", to "ensure spine stiffness and stability during these extraordinary demands", "should be considered only for extreme weight-lifting challenges — not for rehabilitation exercise".[11]		As with other sports, weight trainers should avoid dehydration throughout the workout by drinking sufficient water. This is particularly true in hot environments, or for those older than 65.[12][13][14][15][16]		Some athletic trainers advise athletes to drink about 7 imperial fluid ounces (200 mL) every 15 minutes while exercising, and about 80 imperial fluid ounces (2.3 L) throughout the day.[17]		However, a much more accurate determination of how much fluid is necessary can be made by performing appropriate weight measurements before and after a typical exercise session, to determine how much fluid is lost during the workout. The greatest source of fluid loss during exercise is through perspiration, but as long as your fluid intake is roughly equivalent to your rate of perspiration, hydration levels will be maintained.[14]		Under most circumstances, sports drinks do not offer a physiological benefit over water during weight training.[18] However, high-intensity exercise for a continuous duration of at least one hour may require the replenishment of electrolytes which a sports drink may provide.[19] Some may maintain that energy drinks, such as Red Bull that contain caffeine, improve performance in weight training and other physical exercise, but in fact, these energy drinks can cause dehydration, tremors, heat stroke, and heart attack when consumed in excess.[20] 'Sports drinks' that contain simple carbohydrates & water do not cause ill effects, but are most likely unnecessary for the average trainee.		Insufficient hydration may cause lethargy, soreness or muscle cramps.[21] The urine of well-hydrated persons should be nearly colorless, while an intense yellow color is normally a sign of insufficient hydration.[21]		An exercise should be halted if marked or sudden pain is felt, to prevent further injury. However, not all discomfort indicates injury. Weight training exercises are brief but very intense, and many people are unaccustomed to this level of effort. The expression "no pain, no gain" refers to working through the discomfort expected from such vigorous effort, rather than to willfully ignore extreme pain, which may indicate serious soft tissue injuries.		Discomfort can arise from other factors. Individuals who perform large numbers of repetitions, sets, and exercises for each muscle group may experience a burning sensation in their muscles. These individuals may also experience a swelling sensation in their muscles from increased blood flow (the "pump"). True muscle fatigue is experienced as a marked and uncontrollable loss of strength in a muscle, arising from the nervous system (motor unit) rather than from the muscle fibers themselves. Extreme neural fatigue can be experienced as temporary muscle failure. Some weight training programs, such as Metabolic Resistance Training, actively seek temporary muscle failure; evidence to support this type of training is mixed at best.[22] Irrespective of their program, however, most athletes engaged in high-intensity weight training will experience muscle failure during their regimens.		Beginners are advised to build up slowly to a weight training program. Untrained individuals may have some muscles that are comparatively stronger than others. An injury can result if, in a particular exercise, the primary muscle is stronger than its stabilising muscles. Building up slowly allows muscles time to develop appropriate strengths relative to each other. This can also help to minimize delayed onset muscle soreness. A sudden start to an intense program can cause significant muscular soreness. Unexercised muscles contain cross-linkages that are torn during intense exercise. A regimen of flexibility exercises should be implemented before weight training begins, to help avoid soft tissue pain and injuries.		Anyone beginning an intensive physical training program is typically advised to consult a physician, because of possible undetected heart or other conditions for which such activity is contraindicated.		Exercises like the bench press or the squat in which a failed lift can potentially result in the lifter becoming trapped under the weight are normally performed inside a power rack or in the presence of one or more spotters, who can safely re-rack the barbell if the weight trainer is unable to do so.		Weight training usually requires different types of equipment, most commonly dumbbells, barbells, weight plates, and weight machines. Various combinations of specific exercises, machines, dumbbells, and barbells allow trainees to exercise body parts in numerous ways.		Other types of equipment include:		These terms combine the prefix "iso" (meaning "same") with "tonic" (strength) and "plio" (more) with "metric" (distance). In "isotonic" exercises the force applied to the muscle does not change (while the length of the muscle decreases or increases) while in "plyometric" exercises the length of the muscle stretches and contracts rapidly to increase the power output of a muscle.		Weight training is primarily an isotonic form of exercise, as the force produced by the muscle to push or pull weighted objects should not change (though in practice the force produced does decrease as muscles fatigue). Any object can be used for weight training, but dumbbells, barbells, and other specialised equipment are normally used because they can be adjusted to specific weights and are easily gripped. Many exercises are not strictly isotonic because the force on the muscle varies as the joint moves through its range of motion. Movements can become easier or harder depending on the angle of muscular force relative to gravity; for example, a standard biceps curl becomes easier as the hand approaches the shoulder as more of the load is taken by the structure of the elbow. Originating from Nautilus, Inc., some machines use a logarithmic-spiral cam to keep resistance constant irrespective of the joint angle.		Plyometrics exploit the stretch-shortening cycle of muscles to enhance the myotatic (stretch) reflex. This involves rapid alternation of lengthening and shortening of muscle fibers against resistance. The resistance involved is often a weighted object such as a medicine ball or sandbag, but can also be the body itself as in jumping exercises or the body with a weight vest that allows movement with resistance. Plyometrics is used to develop explosive speed, and focuses on maximal power instead of maximal strength by compressing the force of muscular contraction into as short a period as possible, and may be used to improve the effectiveness of a boxer's punch, or to increase the vertical jumping ability of a basketball player. Care must be taken when performing plyometric exercises because they inflict greater stress upon the involved joints and tendons than other forms of exercise.		An isolation exercise is one where the movement is restricted to one joint only. For example, the leg extension is an isolation exercise for the quadriceps. Specialized types of equipment are used to ensure that other muscle groups are only minimally involved—they just help the individual maintain a stable posture—and movement occurs only around the knee joint. Most isolation exercises involve machines rather than dumbbells and barbells (free weights), though free weights can be used when combined with special positions and joint bracing.		Compound exercises work several muscle groups at once, and include movement around two or more joints. For example, in the leg press, movement occurs around the hip, knee and ankle joints. This exercise is primarily used to develop the quadriceps, but it also involves the hamstrings, glutes and calves. Compound exercises are generally similar to the ways that people naturally push, pull and lift objects, whereas isolation exercises often feel a little unnatural.		Each type of exercise has its uses. Compound exercises build the basic strength that is needed to perform everyday pushing, pulling and lifting activities. Isolation exercises are useful for "rounding out" a routine, by directly exercising muscle groups that cannot be fully exercised in the compound exercises. Compound exercises are also very useful in promoting the production of testosterone.[medical citation needed]		The type of exercise performed also depends on the individual's goals. Those who seek to increase their performance in sports would focus mostly on compound exercises, with isolation exercises being used to strengthen just those muscles that are holding the athlete back. Similarly, a powerlifter would focus on the specific compound exercises that are performed at powerlifting competitions. However, those who seek to improve the look of their body without necessarily maximizing their strength gains (including bodybuilders) would put more of an emphasis on isolation exercises. Both types of athletes, however, generally make use of both compound and isolation exercises.		Free weights include dumbbells, barbells, medicine balls, sandbells, and kettlebells. Unlike weight machines, they do not constrain users to specific, fixed movements, and therefore require more effort from the individual's stabilizer muscles. It is often argued that free weight exercises are superior for precisely this reason. For example, they are recommended for golf players, since golf is a unilateral exercise that can break body balances, requiring exercises to keep the balance in muscles.[25]		Some free weight exercises can be performed while sitting or lying on an exercise ball.		There are a number of weight machines that are commonly found in neighborhood gyms. The Smith machine is a barbell that is constrained to vertical movement. The cable machine consists of two weight stacks separated by 2.5 metres, with cables running through adjustable pulleys (that can be fixed at any height) to various types of handles. There are also exercise-specific weight machines such as the leg press. A multigym includes a variety of exercise-specific mechanisms in one apparatus.		One limitation of many free weight exercises and exercise machines is that the muscle is working maximally against gravity during only a small portion of the lift. Some exercise-specific machines feature an oval cam (first introduced by Nautilus) which varies the resistance, so that the resistance, and the muscle force required, remains constant throughout the full range of motion of the exercise.		A push–pull workout is a method of arranging a weight training routine so that exercises alternate between push motions and pull motions.[26] A push–pull superset is two complementary segments (one pull/one push) done back-to-back. An example is bench press (push) / bent-over row (pull). Another push–pull technique is to arrange workout routines so that one day involves only push (usually chest, shoulders and triceps) exercises, and an alternate day only pull (usually back and biceps) exercises.		Benefits of weight training include increased strength, muscle mass, endurance, bone and bone mineral density, insulin sensitivity, GLUT 4 density, HDL cholesterol, improved cardiovascular health and appearance, and decreased body fat, blood pressure, LDL cholesterol and triglycerides.[27]		The body's basal metabolic rate increases with increases in muscle mass, which promotes long-term fat loss and helps dieters avoid yo-yo dieting.[28] Moreover, intense workouts elevate metabolism for several hours following the workout, which also promotes fat loss.[29]		Weight training also provides functional benefits. Stronger muscles improve posture, provide better support for joints, and reduce the risk of injury from everyday activities. Older people who take up weight training can prevent some of the loss of muscle tissue that normally accompanies aging—and even regain some functional strength—and by doing so become less frail.[citation needed] They may be able to avoid some types of physical disability. Weight-bearing exercise also helps to prevent osteoporosis.[30] The benefits of weight training for older people have been confirmed by studies of people who began engaging in it even in their 80s and 90s.		For many people in rehabilitation or with an acquired disability, such as following stroke or orthopaedic surgery, strength training for weak muscles is a key factor to optimise recovery.[31] For people with such a health condition, their strength training is likely to need to be designed by an appropriate health professional, such as a physiotherapist.		Stronger muscles improve performance in a variety of sports. Sport-specific training routines are used by many competitors. These often specify that the speed of muscle contraction during weight training should be the same as that of the particular sport. Sport-specific training routines also often include variations to both free weight and machine movements that may not be common for traditional weightlifting.		Though weight training can stimulate the cardiovascular system, many exercise physiologists, based on their observation of maximal oxygen uptake, argue that aerobics training is a better cardiovascular stimulus. Central catheter monitoring during resistance training reveals increased cardiac output, suggesting that strength training shows potential for cardiovascular exercise. However, a 2007 meta-analysis found that, though aerobic training is an effective therapy for heart failure patients, combined aerobic and strength training is ineffective; "the favorable antiremodeling role of aerobic exercise was not confirmed when this mode of exercise was combined with strength training".[32]		One side-effect of any intense exercise is increased levels of dopamine, serotonin and norepinephrine, which can help to improve mood and counter feelings of depression.[33]		Weight training has also been shown to benefit dieters as it inhibits lean body mass loss (as opposed to fat loss) when under a caloric deficit. Weight training also strengthens bones, helping to prevent bone loss and osteoporosis. By increasing muscular strength and improving balance, weight training can also reduce falls by elderly persons. Weight training is also attracting attention for the benefits it can have on the brain, and in older adults, a 2017 meta analysis found that it was effective in improving cognitive performance.[34]		|group2 = See also |list2 =		|below = }}		
Mental health is a level of psychological well-being, or an absence of mental illness. It is the "psychological state of someone who is functioning at a satisfactory level of emotional and behavioral adjustment".[1] From the perspective of positive psychology or holism, mental health may include an individual's ability to enjoy life, and create a balance between life activities and efforts to achieve psychological resilience.		According to the World Health Organization (WHO), mental health includes "subjective well-being, perceived self-efficacy, autonomy, competence, inter-generational dependence, and self-actualization of one's intellectual and emotional potential, among others."[2] The WHO further states that the well-being of an individual is encompassed in the realization of their abilities, coping with normal stresses of life, productive work and contribution to their community.[3] Cultural differences, subjective assessments, and competing professional theories all affect how "mental health" is defined.[2] A widely accepted definition of health by mental health specialists is psychoanalyst Sigmund Freud's definition: the capacity "to work and to love".[4]						According to the U.S. surgeon general (1999), mental health is the successful performance of mental function, resulting in productive activities, fulfilling relationships with other people, and providing the ability to adapt to change and cope with adversity. The term mental illness refers collectively to all diagnosable mental disorders—health conditions characterized by alterations in thinking, mood, or behavior associated with distress or impaired functioning.[5]		A person struggling with their mental health may experience stress, depression, anxiety, relationship problems, grief, addiction, ADHD or learning disabilities, mood disorders, or other mental illnesses of varying degrees.[6][7] Therapists, psychiatrists, psychologists, social workers, nurse practitioners or physicians can help manage mental illness with treatments such as therapy, counseling, or medication.		In the mid-19th century, William Sweetser was the first to coin the term "mental hygiene", which can be seen as the precursor to contemporary approaches to work on promoting positive mental health.[8][9] Isaac Ray, one of the thirteen[citation needed] founders of the American Psychiatric Association, further defined mental hygiene as "the art of preserving the mind against all incidents and influences calculated to deteriorate its qualities, impair its energies, or derange its movements."[9]		Dorothea Dix (1802–1887) was an important figure in the development of "mental hygiene" movement. Dix was a school teacher who endeavored throughout her life to help people with mental disorders, and to bring to light the deplorable conditions into which they were put.[10] This was known as the "mental hygiene movement".[10] Before this movement, it was not uncommon that people affected by mental illness in the 19th century would be considerably neglected, often left alone in deplorable conditions, barely even having sufficient clothing.[10] Dix's efforts were so great that there was a rise in the number of patients in mental health facilities, which sadly resulted in these patients receiving less attention and care, as these institutions were largely understaffed.[10]		Emil Kraepelin in 1896 developed the taxonomy mental disorders which has dominated the field for nearly 80 years. Later the proposed disease model of abnormality was subjected to analysis and considered normality to be relative to the physical, geographical and cultural aspects of the defining group.		At the beginning of the 20th century, Clifford Beers founded the Mental Health America – National Committee for Mental Hygiene after publication of his accounts from lived experience in lunatic asylums "A mind that found itself" in 1908[11] and opened the first outpatient mental health clinic in the United States.[12]		The mental hygiene movement, related to the social hygiene movement, had at times been associated with advocating eugenics and sterilisation of those considered too mentally deficient to be assisted into productive work and contented family life.[13][14] In the post-WWII years, references to mental hygiene were gradually replaced by the term 'mental health' due to its positive aspect that evolves from the treatment of illness to preventive and promotive areas of healthcare.[15]		Mental illnesses are more common than cancer, diabetes, or heart disease. Over 26 percent of all Americans over the age of 18 meet the criteria for having a mental illness. Serious mental disorders affect an estimated 6 percent of the adult population, or approximately 1 in 17 people. A little more than half receive treatment.[16] A WHO report estimates the global cost of mental illness at nearly $2.5 trillion (two-thirds in indirect costs) in 2010, with a projected increase to over $6 trillion by 2030.		Evidence from the World Health Organization suggests that nearly half of the world's population are affected by mental illness with an impact on their self-esteem, relationships and ability to function in everyday life.[17] An individual's emotional health can also impact physical health and poor mental health can lead to problems such as substance abuse.[18]		Maintaining good mental health is crucial to living a long and healthy life. Good mental health can enhance one's life, while poor mental health can prevent someone from living an enriching life. According to Richards, Campania, & Muse-Burke, "There is growing evidence that is showing emotional abilities are associated with prosocial behaviors such as stress management and physical health."[18] Their research also concluded that people who lack emotional expression are inclined to anti-social behaviors (e.g., drug and alcohol abuse, physical fights, vandalism), which are a direct reflection of their mental health and suppress emotions.[18]		Mental health can be seen as an unstable continuum, where an individual's mental health may have many different possible values.[19] Mental wellness is generally viewed as a positive attribute, even if the person does not have any diagnosed mental health condition. This definition of mental health highlights emotional well-being, the capacity to live a full and creative life, and the flexibility to deal with life's inevitable challenges. Some discussions are formulated in terms of contentment or happiness.[20] Many therapeutic systems and self-help books offer methods and philosophies espousing strategies and techniques vaunted as effective for further improving the mental wellness. Positive psychology is increasingly prominent in mental health.		A holistic model of mental health generally includes concepts based upon anthropological, educational, psychological, religious and sociological perspectives, as well as theoretical perspectives from personality, social, clinical, health and developmental psychology.[21][22]		An example of a wellness model includes one developed by Myers, Sweeney and Witmer. It includes five life tasks—essence or spirituality, work and leisure, friendship, love and self-direction—and twelve sub tasks—sense of worth, sense of control, realistic beliefs, emotional awareness and coping, problem solving and creativity, sense of humor, nutrition, exercise, self care, stress management, gender identity, and cultural identity—which are identified as characteristics of healthy functioning and a major component of wellness. The components provide a means of responding to the circumstances of life in a manner that promotes healthy functioning.		The tripartite model of mental well-being[19][23] views mental well-being as encompassing three components of emotional well-being, social well-being, and psychological well-being. Emotional well-being is defined as having high levels of positive emotions, whereas social and psychological well-being are defined as the presence of psychological and social skills and abilities that contribute to optimal functioning in daily life. The model has received empirical support across cultures.[23][24][25] The Mental Health Continuum-Short Form (MHC-SF) is the most widely used scale to measure the tripartite model of mental well-being.[26][27][28]		Mental health and stability is a very important factor in a person’s everyday life. Social skills, behavioural skills, and someone’s way of thinking are just some of the things that the human brain develops at an early age. Learning how to interact with others and how to focus on certain subjects are essential lessons to learn from the time we can talk all the way to when we are so old that we can barely walk. However, there are some people out there who have difficulty with these kind of skills and behaving like an average person. This is a most likely the cause of having a mental illness. A mental illness is a wide range of conditions that affect a person’s mood, thinking, and behavior. About 26% of people in the United States, ages 18 and older, have been diagnosed with some kind of mental disorder. However, not much is said about children with mental illnesses even though there are many that will develop one, even as early as age three.		The most common mental illnesses in children include, but are not limited to, ADHD, autism and anxiety disorder, as well as depression in older children and teens. Having a mental illness at a younger age is much different from having one in your thirties. Children's brains are still developing and will continue to develop until around the age of twenty-five.[29] When a mental illness is thrown into the mix, it becomes significantly harder for a child to acquire the necessary skills and habits that people use throughout the day. For example, behavioral skills don’t develop as fast as motor or sensory skills do.[29] So when a child has an anxiety disorder, they begin to lack proper social interaction and associate many ordinary things with intense fear.[30] This can be scary for the child because they don’t necessarily understand why they act and think the way that they do. Many researchers say that parents should keep an eye on their child if they have any reason to believe that something is slightly off.[29] If the children are evaluated earlier, they become more acquainted to their disorder and treating it becomes part of their daily routine.[29] This is opposed to adults who might not recover as quickly because it is more difficult for them to adapt.		Mental illness affects not only the person themselves, but the people around them. Friends and family also play an important role in the child’s mental health stability and treatment. If the child is young, parents are the ones who evaluate their child and decide whether or not they need some form of help.[31] Friends are a support system for the child and family as a whole. Living with a mental disorder is never easy, so it’s always important to have people around to make the days a little easier. However, there are negative factors that come with the social aspect of mental illness as well. Parents are sometimes held responsible for their child’s own illness.[31] People also say that the parents raised their children in a certain way or they acquired their behavior from them. Family and friends are sometimes so ashamed of the idea of being close to someone with a disorder that the child feels isolated and thinks that they have to hide their illness from others.[31] When in reality, hiding it from people prevents the child from getting the right amount of social interaction and treatment in order to thrive in today’s society.		Stigma is also a well-known factor in mental illness. Stigma is defined as “a mark of disgrace associated with a particular circumstance, quality, or person.” Stigma is used especially when it comes to the mentally disabled. People have this assumption that everyone with a mental problem, no matter how mild or severe, is automatically considered destructive or a criminal person. Thanks to the media, this idea has been planted in our brains from a young age.[32] Watching movies about teens with depression or children with Autism makes us think that all of the people that have a mental illness are like the ones on TV. In reality, the media displays an exaggerated version of most illnesses. Unfortunately, not many people know that, so they continue to belittle those with disorders. In a recent study, a majority of young people associate mental illness with extreme sadness or violence.[33] Now that children are becoming more and more open to technology and the media itself, future generations will then continue to pair mental illness with negative thoughts. The media should be explaining that many people with disorders like ADHD and anxiety, with the right treatment, can live ordinary lives and should not be punished for something they cannot help.		Sueki, (2013) carried out a study titled “The effect of suicide–related internet use on users’ mental health: A longitudinal Study”. This study investigated the effects of suicide-related internet use on user’s suicidal thoughts, predisposition to depression and anxiety and loneliness. The study consisted of 850 internet users; the data was obtained by carrying out a questionnaire amongst the participants. This study found that browsing websites related to suicide, and methods used to commit suicide, had a negative effect on suicidal thoughts and increased depression and anxiety tendencies. The study concluded that as suicide-related internet use adversely affected the mental health of certain age groups it may be prudent to reduce or control their exposure to these websites. These findings certainly suggest that the internet can indeed have a profoundly negative impact on our mental health.[34]		Psychiatrist Thomas Szasz compared that 50 years ago children were either categorized as good or bad, and today "all children are good, but some are mentally healthy and others are mentally ill". The social control and forced identity creation is the cause of many mental health problems among today's children.[35] A behaviour or misbehaviour can not be an illness but exercise of their free will and today's immediacy in drug administration for every problem along with the legal over-guarding and disregard of a child's status as a dependent shakes their personal self and invades their internal growth.		Mental health is conventionally defined as a hybrid of absence of a mental disorder and presence of well-being. Focus is increasing on preventing mental disorders. Prevention is beginning to appear in mental health strategies, including the 2004 WHO report "Prevention of Mental Disorders", the 2008 EU "Pact for Mental Health" and the 2011 US National Prevention Strategy.[36][page needed] Prevention of a disorder at a young age may significantly decrease the chances that a child will suffer from a disorder later in life, and shall be the most efficient and effective measure from a public health perspective.[37] Prevention may require the regular consultation of a physician for at least twice a year to detect any signs that reveal any mental health concerns.[38][unreliable medical source?]		Mental health is a socially constructed and socially defined concept; that is, different societies, groups, cultures, institutions and professions have very different ways of conceptualizing its nature and causes, determining what is mentally healthy, and deciding what interventions, if any, are appropriate.[39] Thus, different professionals will have different cultural, class, political and religious backgrounds, which will impact the methodology applied during treatment.		Research has shown that there is stigma attached to mental illness.[40] In the United Kingdom, the Royal College of Psychiatrists organized the campaign Changing Minds (1998–2003) to help reduce stigma.[41] Due to this stigma, responses to a positive diagnosis may be a display of denialism.[42]		Many mental health professionals are beginning to, or already understand, the importance of competency in religious diversity and spirituality. The American Psychological Association explicitly states that religion must be respected. Education in spiritual and religious matters is also required by the American Psychiatric Association.[43]		Unemployment has been shown to have a negative impact on an individual's emotional well-being, self-esteem and more broadly their mental health. Increasing unemployment has been show to have a significant impact on mental health, predominantly depressive disorders.[citation needed] This is an important consideration when reviewing the triggers for mental health disorders in any population survey.[44] In order to improve your emotional mental health, the root of the issue has to be resolved. "Prevention emphasizes the avoidance of risk factors; promotion aims to enhance an individual's ability to achieve a positive sense of self-esteem, mastery, well-being, and social inclusion."[45] It is very important to improve your emotional mental health by surrounding yourself with positive relationships. We as humans, feed off companionships and interaction with other people. Another way to improve your emotional mental health is participating in activities that can allow you to relax and take time for yourself. Yoga is a great example of an activity that calms your entire body and nerves. According to a study on well-being by Richards, Campania and Muse-Burke, "mindfulness is considered to be a purposeful state, it may be that those who practice it believe in its importance and value being mindful, so that valuing of self-care activities may influence the intentional component of mindfulness."[18]		Mental health care navigation helps to guide patients and families through the fragmented, often confusing mental health industries. Care navigators work closely with patients and families through discussion and collaboration to provide information on best therapies as well as referrals to practitioners and facilities specializing in particular forms of emotional improvement. The difference between therapy and care navigation is that the care navigation process provides information and directs patients to therapy rather than providing therapy. Still, care navigators may offer diagnosis and treatment planning. Though many care navigators are also trained therapists and doctors. Care navigation is the link between the patient and the below therapies. A clear recognition that mental health requires medical intervention was demonstrated in a study by Kessler et al. of the prevalence and treatment of mental disorders from 1990 to 2003 in the United States. Despite the prevalence of mental health disorders remaining unchanged during this period, the number of patients seeking treatment for mental disorders increased threefold.[46]		Emotional mental disorders are a leading cause of disabilities worldwide. Investigating the degree and severity of untreated emotional mental disorders throughout the world is a top priority of the World Mental Health (WMH) survey initiative,[47] which was created in 1998 by the World Health Organization (WHO).[48] "Neuropsychiatric disorders are the leading causes of disability worldwide, accounting for 37% of all healthy life years lost through disease.These disorders are most destructive to low and middle-income countries due to their inability to provide their citizens with proper aid. Despite modern treatment and rehabilitation for emotional mental health disorders, "even economically advantaged societies have competing priorities and budgetary constraints".		The World Mental Health survey initiative has suggested a plan for countries to redesign their mental health care systems to best allocate resources. "A first step is documentation of services being used and the extent and nature of unmet needs for treatment. A second step could be to do a cross-national comparison of service use and unmet needs in countries with different mental health care systems. Such comparisons can help to uncover optimum financing, national policies, and delivery systems for mental health care."		Knowledge of how to provide effective emotional mental health care has become imperative worldwide. Unfortunately, most countries have insufficient data to guide decisions, absent or competing visions for resources, and near constant pressures to cut insurance and entitlements. WMH surveys were done in Africa (Nigeria, South Africa), the Americas (Colombia, Mexico, United States), Asia and the Pacific (Japan, New Zealand, Beijing and Shanghai in the People's Republic of China), Europe (Belgium, France, Germany, Italy, Netherlands, Spain, Ukraine), and the middle east (Israel, Lebanon). Countries were classified with World Bank criteria as low-income (Nigeria), lower middle-income (China, Colombia, South Africa, Ukraine), higher middle-income (Lebanon, Mexico), and high-income.		The coordinated surveys on emotional mental health disorders, their severity, and treatments were implemented in the aforementioned countries. These surveys assessed the frequency, types, and adequacy of mental health service use in 17 countries in which WMH surveys are complete. The WMH also examined unmet needs for treatment in strata defined by the seriousness of mental disorders. Their research showed that "the number of respondents using any 12-month mental health service was generally lower in developing than in developed countries, and the proportion receiving services tended to correspond to countries' percentages of gross domestic product spent on health care". "High levels of unmet need worldwide are not surprising, since WHO Project ATLAS' findings of much lower mental health expenditures than was suggested by the magnitude of burdens from mental illnesses. Generally, unmet needs in low-income and middle-income countries might be attributable to these nations spending reduced amounts (usually <1%) of already diminished health budgets on mental health care, and they rely heavily on out-of-pocket spending by citizens who are ill equipped for it".		Activity therapies, also called recreation therapy and occupational therapy, promote healing through active engagement. Making crafts can be a part of occupational therapy. Walks can be a part of recreation therapy.		Biofeedback is a process of gaining control of physical processes and brainwaves. It can be used to decrease anxiety, increase well-being, increase relaxation, and other methods of mind-over-body control.[citation needed]		Expressive therapies are a form of psychotherapy that involves the arts or art-making. These therapies include music therapy, art therapy, dance therapy, drama therapy, and poetry therapy.		Group therapy involves any type of therapy that takes place in a setting involving multiple people. It can include psychodynamic groups, activity groups for expressive therapy, support groups (including the Twelve-step program), problem-solving and psychoeducation groups.		Psychotherapy is the general term for scientific based treatment of mental health issues based on modern medicine. It includes a number of schools, such as gestalt therapy, psychoanalysis, cognitive behavioral therapy and dialectical behavioral therapy.		The practice of mindfulness meditation has several mental health benefits, such as bringing about reductions in depression, anxiety and stress.[49][50][51][52] Mindfulness meditation may also be effective in treating substance use disorders.[53][54] Further, mindfulness meditation appears to bring about favorable structural changes in the brain.[55][56][57]		Spiritual counselors meet with people in need to offer comfort and support and to help them gain a better understanding of their issues and develop a problem-solving relation with spirituality. These types of counselors deliver care based on spiritual, psychological and theological principles.[58][unreliable source?]		Social work in mental health, also called psychiatric social work, is a process where an individual in a setting is helped to attain freedom from overlapping internal and external problems (social and economic situations, family and other relationships, the physical and organizational environment, psychiatric symptoms, etc.). It aims for harmony, quality of life, self-actualization and personal adaptation across all systems. Psychiatric social workers are mental health professionals that can assist patients and their family members in coping with both mental health issues and various economic or social problems caused by mental illness or psychiatric dysfunctions and to attain improved mental health and well-being. They are vital members of the treatment teams in Departments of Psychiatry and Behavioral Sciences in hospitals. They are employed in both outpatient and inpatient settings of a hospital, nursing homes, state and local governments, substance abuse clinics, correctional facilities, health care services...etc.[59]		In psychiatric social work there are three distinct groups. One made up of the social workers in psychiatric organizations and hospitals. The second group consists members interested with mental hygiene education and holding designations that involve functioning in various mental health services and the third group consist of individuals involved directly with treatment and recovery process.[60]		In the United States, social workers provide most of the mental health services. According to government sources, 60 percent of mental health professionals are clinically trained social workers, compared to 10 percent of psychiatrists, 23 percent of psychologists, and 5 percent of psychiatric nurses.[61]		Mental health social workers in Japan have professional knowledge of health and welfare and skills essential for person's well-being. Their social work training enables them as a professional to carry out Consultation assistance for mental disabilities and their social reintegration; Consultation regarding the rehabilitation of the victims; Advice and guidance for post-discharge residence and re-employment after hospitalized care, for major life events in regular life, money and self-management and in other relevant matters in order to equip them to adapt in daily life. Social workers provide individual home visits for mentally ill and do welfare services available, with specialized training a range of procedural services are coordinated for home, workplace and school. In an administrative relationship, Psychiatric social workers provides consultation, leadership, conflict management and work direction. Psychiatric social workers who provides assessment and psychosocial interventions function as a clinician, counselor and municipal staff of the health centers.[62]		Social workers play many roles in mental health settings, including those of case manager, advocate, administrator, and therapist. The major functions of a psychiatric social worker are promotion and prevention, treatment, and rehabilitation. Social workers may also practice:		Psychiatric social workers conduct psychosocial assessments of the patients and work to enhance patient and family communications with the medical team members and ensure the inter-professional cordiality in the team to secure patients with the best possible care and to be active partners in their care planning. Depending upon the requirement, social workers are often involved in illness education, counseling and psychotherapy. In all areas, they are pivotal to the aftercare process to facilitate a careful transition back to family and community. [63]		During the 1840s, Dorothea Lynde Dix, a retired Boston teacher who is considered the founder of the mental health movement, began a crusade that would change the way people with mental disorders were viewed and treated. Dix was not a social worker; the profession was not established until after her death in 1887. However, her life and work were embraced by early psychiatric social workers, and she is considered one of the pioneers of psychiatric social work along with Elizabeth Horton, who in 1907 was the first psychiatric social worker in the New York hospital system, and others.[64] The early twentieth century was a time of progressive change in attitudes towards mental illness. Community Mental Health Centers Act was passed in 1963. This policy encouraged the deinstitutionalisation of people with mental illness. Later mental health consumer movement came by 1980s. A consumer was defined as a person who has received or is currently receiving services for a psychiatric condition. People with mental disorders and their families became advocates for better care. Building public understanding and awareness through consumer advocacy helped bring mental illness and its treatment into mainstream medicine and social services.[65] In the 2000s focus was on Managed care movement which aimed at a health care delivery system to eliminate unnecessary and inappropriate care in order to reduce costs & Recovery movement in which by principle acknowledges that many people with serious mental illness spontaneously recover and others recover and improve with proper treatment.[66]		Role of social workers made an impact with 2003 invasion of Iraq and War in Afghanistan (2001–14) social workers worked out of the NATO hospital in Afghanistan and Iraq bases. They made visits to provide counseling services at forward operating bases. Twenty-two percent of the clients were diagnosed with post-traumatic stress disorder, 17 percent with depression, and 7 percent with alcohol abuse.[67] In 2009, a high level of suicides was reached among active-duty soldiers: 160 confirmed or suspected Army suicides. In 2008, the Marine Corps had a record 52 suicides.[68] The stress of long and repeated deployments to war zones, the dangerous and confusing nature of both wars, wavering public support for the wars, and reduced troop morale have all contributed to the escalating mental health issues.[69] Military and civilian social workers are primary service providers in the veterans’ health care system.		Mental health services, is a loose network of services ranging from highly structured inpatient psychiatric units to informal support groups, where psychiatric social workers indulges in the diverse approaches in multiple settings along with other paraprofessional workers.		A role for psychiatric social workers was established early in Canada’s history of service delivery in the field of population health. Native North Americans understood mental trouble as an indication of an individual who had lost their equilibrium with the sense of place and belonging in general, and with the rest of the group in particular. In native healing beliefs, health and mental health were inseparable, so similar combinations of natural and spiritual remedies were often employed to try to relieve both mental and physical illness. These communities and families greatly valued holistic approaches for preventative health care. Indigenous peoples in Canada have faced cultural oppression and social marginalization through the actions of European colonizers and their institutions since the earliest periods of contact. Culture contact brought with it many forms of depredation. Economic, political, and religious institutions of the European settlers all contributed to the displacement and oppression of indigenous people.[70][page needed] The officially recorded treatment practices started in 1714, when Quebec opened wards for the mentally ill. Asylums for the insane were opened in 1835 in Saint John, New Brunswick, and in 1841 in Toronto, when care for the mentally ill became institutionally based. Canada became a self-governing dominion in 1867, retaining its ties to the British crown. During this period age of industrial capitalism began, which lead to a social and economic dislocation in many forms. By 1887 asylums were converted to hospitals and nurses and attendants were employed for the care of the mentally ill. In 1918 Clarence Hincks & Clifford Beers founded the Canadian National Committee for Mental Hygiene, which later became the Canadian Mental Health Association. In 1930s Dr. Clarence Hincks promoted prevention and of treating sufferers of mental illness before they were incapacitated/early detection. World War II profoundly affected attitudes towards mental health. The medical examinations of recruits revealed that thousands of apparently healthy adults suffered mental difficulties. This knowledge changed public attitudes towards mental health, and stimulated research into preventive measures and methods of treatment.[71] In 1951 Mental Health Week was introduced across Canada. For the first half of the twentieth century, with a period of deinstitutionalisation beginning in the late 1960s psychiatric social work succeeded to the current emphasis on community-based care, Psychiatric Social Work focused beyond the medical model’s aspects on individual diagnosis to identify and address social inequities and structural issues. In the 1980s Mental Health Act was amended to give consumers the right to choose treatment alternatives. Later the focus shifted to workforce mental health issues and environment.[72]		The earliest citing of Mental disorders in India are from Vedic Era (2000 BC – AD 600).[73] Charaka Samhita, an ayurvedic textbook believed to be from 400–200 BC describes various factors of mental stability. It also has instructions regarding how to set up a care delivery system.[74] In the same era In south India Siddha was a medical system, the great sage Agastya, one of the 18 siddhas contributing to a system of medicine has included the Agastiyar Kirigai Nool, a compendium of psychiatric disorders and their recommended treatments.[75] In Atharva Veda too there are descriptions and resolutions about mental health afflictions. In the Mughal period Unani system of medicine was introduced by an Indian physician Unhammad in 1222.[76] Then existed form of psychotherapy was known then as ilaj-i-nafsani in Unani medicine.		The 18th century was a very unstable period in Indian history, which contributed to psychological and social chaos in the Indian subcontinent. In 1745 of lunatic asylums were developed in Bombay (Mumbai) followed by Calcutta (Kolkata) in 1784, and Madras (Chennai) in 1794. The need to establish hospitals became more acute, first to treat and manage Englishmen and Indian ‘sepoys’ (military men) employed by the British East India Company.[77] The First Lunacy Act (also called Act No. 36) that came into effect in 1858 was later modified by a committee appointed in Bengal in 1888. Later, the Indian Lunacy Act, 1912 was brought under this legislation. A rehabilitation programme was initiated between 1870s and 1890s for persons with mental illness at the Mysore Lunatic Asylum, and then an occupational therapy department was established during this period in almost each of the lunatic asylums. The programme in the asylum was called ‘work therapy’. In this programme, persons with mental illness were involved in the field of agriculture for all activities. This programme is considered as the seed of origin of psychosocial rehabilitation in India.		Berkeley-Hill, superintendent of the European Hospital (now known as the Central Institute of Psychiatry (CIP), established in 1918), was deeply concerned about the improvement of mental hospitals in those days. The sustained efforts of Berkeley-Hill helped to raise the standard of treatment and care and he also persuaded the government to change the term ‘asylum’ to ‘hospital’ in 1920.[78] Techniques similar to the current token-economy were first started in 1920 and called by the name ‘habit formation chart’ at the CIP, Ranchi. In 1937, the first post of psychiatric social worker was created in the child guidance clinic run by the Dhorabji Tata School of Social Work (established in 1936), It is considered as the first documented evidence of social work practice in Indian mental health field.		After Independence in 1947, general hospital psychiatry units (GHPUs) where established to improve conditions in existing hospitals, while at the same time encouraging outpatient care through these units. In Amritsar a Dr. Vidyasagar, instituted active involvement of families in the care of persons with mental illness. This was advanced practice ahead of its times regarding treatment and care. This methodology had a greater impact on social work practice in the mental health field especially in reducing the stigmatisation. In 1948 Gauri Rani Banerjee, trained in the United States, started a master’s course in medical and psychiatric social work at the Dhorabji Tata School of Social Work (Now TISS). Later the first trained psychiatric social worker was appointed in 1949 at the adult psychiatry unit of Yervada mental hospital, Pune.		In various parts of the country, in mental health service settings, social workers were employed—in 1956 at a mental hospital in Amritsar, in 1958 at a child guidance clinic of the college of nursing, and in Delhi in 1960 at the All India Institute of Medical Sciences and in 1962 at the Ram Manohar Lohia Hospital. In 1960, the Madras Mental Hospital (Now Institute of Mental Health), employed social workers to bridge the gap between doctors and patients. In 1961 the social work post was created at the NIMHANS. In these settings they took care of the psychosocial aspect of treatment. This had long-term greater impact of social work practice in mental health.[79]		In 1966 by the recommendation Mental Health Advisory Committee, Ministry of Health, Government of India, NIMHANS commenced Department of Psychiatric Social Work in and started a two-year Postgraduate Diploma in Psychiatric Social Work was introduced in 1968. In 1978, the nomenclature of the course was changed to MPhil in Psychiatric Social Work. Subsequently, a PhD Programme was introduced. By the recommendations Mudaliar committee in 1962, Diploma in Psychiatric Social Work was started in 1970 at the European Mental Hospital at Ranchi (now CIP), upgraded the program and added other higher training courses subsequently.		A new initiative to integrate mental health with general health services started in 1975 in India. The Ministry of Health, Government of India formulated the National Mental Health Programme (NMHP) and launched it in 1982. The same was reviewed in 1995 and based on that, the District Mental Health Program (DMHP) launched in 1996 and sought to integrate mental health care with public health care.[80] This model has been implemented in all the states and currently there are 125 DMHP sites in India.		National Human Rights Commission (NHRC) in 1998 and 2008 carried out systematic, intensive and critical examinations of mental hospitals in India. This resulted in recognition of the human rights of the persons with mental illness by the NHRC. From the NHRC's report as part of the NMHP, funds were provided for upgrading the facilities of mental hospitals. This is studied to result in positive changes over the past 10 years than in the preceding five decades by the 2008 report of the NHRC and NIMHANS.[81] In 2016 Mental Health Care Bill was passed which ensures and legally entitles access to treatments with coverage from insurance, safeguarding dignity of the afflicted person, improving legal and healthcare access and allows for free medications.[82][83][84] In December 2016, Disabilities Act 1995 was repealed with Rights of Persons with Disabilities Act (RPWD), 2016 from the 2014 Bill which ensures benefits to a wider population with disabilities. The Bill before becoming an Act was pushed for amendments by stakeholders mainly against alarming clauses in the "Equality and Non discrimination" section that diminishes the power of the act and allows establishments to overlook or discriminate against persons with disabilities and against the general lack of directives that requires to ensure the proper implementation of the Act.[85][86]		Lack of any universally accepted single licensing authority compared to foreign countries puts Social Workers at general in risk. But general bodies/councils accepts automatically a university qualified Social Worker as a professional licensed to practice or as a qualified clinician. Lack of a centralized council in tie-up with Schools of Social Work also makes a decline in promotion for the scope of social workers as mental health professionals. Though in this midst the service of Social Workers has given a facelift of the mental health sector in the country with other allied professionals.[87]		Evidence suggests that 450 million people worldwide are impacted by mental health, major depression ranks fourth among the top 10 leading causes of disease worldwide. Within 20 years, mental illness is predicted to become the leading cause of disease worldwide. Women are more likely to have a mental illness than men. One million people commit suicide every year and 10 to 20 million attempt it.[88]		A survey conducted by Australian Bureau of Statistics in 2008 regarding adults with manageable to severe neurosis reveals almost half of the population had a mental disorder at some point of their life and one in five people had sustaining disorder preceding 12 months. In Neurotic disorders, 14% of the population experienced anxiety disorders, comorbidity disorders were the next common mental disorder with vulnerability to substance abuse and relapses. There were distinct gender differences in disposition to mental health illness. Women were found to have high rate of mental health disorders and Men had higher propensity of risk for substance abuse. The SMHWB survey showed low socioeconomic status and high dysfunctional pattern in the family was proportional to greater risk for mental health disorders. A 2010 survey regarding adults with psychosis revealed 5 persons per 1000 in the population seeks professional mental health services for psychotic disorders and the most common psychotic disorder was schizophrenia.[89][90]		According to statistics released by the Centre of Addiction and Mental Health one in five people in Ontario experience a mental health or addiction problem. Young people ages 15 to 25 are particularly vulnerable. Major depression is found to affect 8% and anxiety disorder 12% of the population. Women are 1.5 times more likely to suffer from mood and anxiety disorders. WHO points out that there are distinct gender differences in patterns of mental health and illness. The lack of power and control over their socioeconomic status, gender based violence; low social position and responsibility for the care of others render women vulnerable to mental health risks. Since more women than men seek help regarding a mental health problem, this has led to not only gender stereotyping but also reinforcing social stigma. WHO has found that this stereotyping has led doctors to diagnose depression more often in women than in men even when they display identical symptoms. Often communication between health care providers and women is authoritarian leading to either the under-treatment or over-treatment of these women.[3]		Firstly, Women's College Hospital is specifically dedicated to women's health in Canada. This hospital is located at the heart of downtown, Toronto where there are several locations available for specific medical conditions. WCH is a great organization that helps educate women on mental illness due to its specialization with women and mental health. Women's College Hospital helps women who have symptoms of mental illnesses such as depression, anxiety, menstruation, pregnancy, childbirth, and menopause. They also focus on psychological issues, abuse, neglect and mental health issues from various medications.[91]		The countless aspect about this organization is that WCH is open to women of all ages, including pregnant women that experience poor mental health. WCH not only provides care for good mental health, but they also have a program called the "Women's Mental Health Program" where doctors and nurses help treat and educate women regarding mental health collaboratively, individually, and online by answering questions from the public.[91]		The second organization is the Centre for Addiction and Mental Health (CAMH). CAMH is one of Canada's largest and most well-known health and addiction facilities, and it has received international recognitions from the Pan American Health Organization and World Health Organization Collaborating Centre. They practice in doing research in areas of addiction and mental health in both men and women. In order to help both men and women, CAMH provides "clinical care, research, education, policy development and health promotion to help transform the lives of people affected by mental health and addiction issues."[92] CAMH is different from Women's College Hospital due to its widely known rehab centre for women who have minor addiction issues, to severe ones. This organization provides care for mental health issues by assessments, interventions, residential programs, treatments, and doctor and family support.[92]		According to the World Health Organization in 2004, depression is the leading cause of disability in the United States for individuals ages 15 to 44.[93] Absence from work in the U.S. due to depression is estimated to be in excess of $31 billion per year. Depression frequently co-occurs with a variety of medical illnesses such as heart disease, cancer, and chronic pain and is associated with poorer health status and prognosis.[94] Each year, roughly 30,000 Americans take their lives, while hundreds of thousands make suicide attempts (Centers for Disease Control and Prevention).[95] In 2004, suicide was the 11th leading cause of death in the United States (Centers for Disease Control and Prevention), third among individuals ages 15–24. Despite the increasingly availability of effectual depression treatment, the level of unmet need for treatment remains high.[citation needed] By way of comparison, a study conducted in Australia during 2006 to 2007 reported that one-third (34.9%) of patients diagnosed with a mental health disorder had presented to medical health services for treatment.[96]		There are many factors that influence mental health including:		Emotional mental illnesses should be a particular concern in the United States since the U.S. has the highest annual prevalence rates (26 percent) for mental illnesses among a comparison of 14 developing and developed countries.[97] While approximately 80 percent of all people in the United States with a mental disorder eventually receive some form of treatment, on the average persons do not access care until nearly a decade following the development of their illness, and less than one-third of people who seek help receive minimally adequate care.[98]		The mental health policies in the United States have experienced four major reforms: the American asylum movement led by Dorothea Dix in 1843; the "mental hygiene" movement inspired by Clifford Beers in 1908; the deinstitutionalization started by Action for Mental Health in 1961; and the community support movement called for by The CMCH Act Amendments of 1975.[99]		In 1843, Dorothea Dix submitted a Memorial to the Legislature of Massachusetts, describing the abusive treatment and horrible conditions received by the mentally ill patients in jails, cages, and almshouses. She revealed in her Memorial: "I proceed, gentlemen, briefly to call your attention to the present state of insane persons confined within this Commonwealth, in cages, closets, cellars, stalls, pens! Chained, naked, beaten with rods, and lashed into obedience…."[100] Many asylums were built in that period, with high fences or walls separating the patients from other community members and strict rules regarding the entrance and exit. In those asylums, traditional treatments were well implemented: drugs were not used as a cure for a disease, but a way to reset equilibrium in a person's body, along with other essential elements such as healthy diets, fresh air, middle class culture, and the visits by their neighboring residents.[citation needed] In 1866, a recommendation came to the New York State Legislature to establish a separate asylum for chronic mentally ill patients. Some hospitals placed the chronic patients into separate wings or wards, or different buildings.[101]		In A Mind That Found Itself (1908) Clifford Whittingham Beers described the humiliating treatment he received and the deplorable conditions in the mental hospital.[102] One year later, the National Committee for Mental Hygiene (NCMH) was founded by a small group of reform-minded scholars and scientists – including Beer himself – which marked the beginning of the "mental hygiene" movement. The movement emphasized the importance of childhood prevention. World War I catalyzed this idea with an additional emphasis on the impact of maladjustment, which convinced the hygienists that prevention was the only practical approach to handle mental health issues.[103] However, prevention was not successful, especially for chronic illness; the condemnable conditions in the hospitals were even more prevalent, especially under the pressure of the increasing number of chronically ill and the influence of the Depression.[99]		In 1961, the Joint Commission on Mental Health published a report called Action for Mental Health, whose goal was for community clinic care to take on the burden of prevention and early intervention of the mental illness, therefore to leave space in the hospitals for severe and chronic patients. The court started to rule in favor of the patients' will on whether they should be forced to treatment. By 1977, 650 community mental health centers were built to cover 43 percent of the population and serve 1.9 million individuals a year, and the lengths of treatment decreased from 6 months to only 23 days.[104] However, issues still existed. Due to inflation, especially in the 1970s, the community nursing homes received less money to support the care and treatment provided. Fewer than half of the planned centers were created, and new methods did not fully replace the old approaches to carry out its full capacity of treating power.[104] Besides, the community helping system was not fully established to support the patients' housing, vocational opportunities, income supports, and other benefits.[99] Many patients returned to welfare and criminal justice institutions, and more became homeless. The movement of deinstitutionalization was facing great challenges.[105]		After realizing that simply changing the location of mental health care from the state hospitals to nursing houses was insufficient to implement the idea of deinstitutionalization, the National Institute of Mental Health in 1975 created the Community Support Program (CSP) to provide funds for communities to set up a comprehensive mental health service and supports to help the mentally ill patients integrate successfully in the society. The program stressed the importance of other supports in addition to medical care, including housing, living expenses, employment, transportation, and education; and set up new national priority for people with serious mental disorders. In addition, the Congress enacted the Mental Health Systems Act of 1980 to prioritize the service to the mentally ill and emphasize the expansion of services beyond just clinical care alone.[106] Later in the 1980s, under the influence from the Congress and the Supreme Court, many programs started to help the patients regain their benefits. A new Medicaid service was also established to serve people who were suffering from a "chronic mental illness." People who were temporally hospitalized were also provided aid and care and a pre-release program was created to enable people to apply for reinstatement prior to discharge.[104] Not until 1990, around 35 years after the start of the deinstitutionalization, did the first state hospital begin to close. The number of hospitals dropped from around 300 by over 40 in the 1990s, and finally a Report on Mental Health showed the efficacy of mental health treatment, giving a range of treatments available for patients to choose.[106]		However, several critics maintain that deinstitutionalization has, from a mental health point of view, been a thoroughgoing failure. The seriously mentally ill are either homeless, or in prison; in either case (especially the latter), they are getting little or no mental health care. This failure is attributed to a number of reasons over which there is some degree of contention, although there is general agreement that community support programs have been ineffective at best, due to a lack of funding.[105]		The 2011 National Prevention Strategy included mental and emotional well-being, with recommendations including better parenting and early intervention programs, which increase the likelihood of prevention programs being included in future US mental health policies.[107][page needed] The NIMH is researching only suicide and HIV/AIDS prevention, but the National Prevention Strategy could lead to it focusing more broadly on longitudinal prevention studies.[108][not in citation given]		In 2013, United States Representative Tim Murphy introduced the Helping Families in Mental Health Crisis Act, HR2646. The bipartisan bill went through substantial revision and was reintroduced in 2015 by Murphy and Congresswoman Eddie Bernice Johnson. In November 2015, it passed the Health Subcommittee by an 18–12 vote.[citation needed]		
Physical exercise is any bodily activity that enhances or maintains physical fitness and overall health and wellness.[1] It is performed for various reasons, including increasing growth and development, preventing aging, strengthening muscles and the cardiovascular system, honing athletic skills, weight loss or maintenance, and also enjoyment. Frequent and regular physical exercise boosts the immune system and helps prevent "diseases of affluence" such as cardiovascular disease, type 2 diabetes, and obesity.[2][3] It may also help prevent stress and depression, increase quality of sleep and act as a non-pharmaceutical sleep aid to treat diseases such as insomnia, help promote or maintain positive self-esteem, improve mental health, maintain steady digestion and treat constipation and gas, regulate fertility health, and augment an individual's sex appeal or body image, which has been found to be linked with higher levels of self-esteem.[4][5] Childhood obesity is a growing global concern,[6] and physical exercise may help decrease some of the effects of childhood and adult obesity. Some care providers call exercise the "miracle" or "wonder" drug—alluding to the wide variety of benefits that it can provide for many individuals.[7][8] Aside from the health advantages, these benefits may include different social rewards for staying active while enjoying the environment of one’s culture. Many individuals choose to exercise publicly outdoors where they can congregate in groups, socialize, and appreciate life.[9]		In the United Kingdom two to four hours of light activity are recommended during working hours.[10] This includes walking and standing.[10] In the United States, the CDC/ACSM consensus statement and the Surgeon General's report states that every adult should participate in moderate exercise, such as walking, swimming, and household tasks, for a minimum of 30 minutes daily.[11]		Physical exercises are generally grouped into three types, depending on the overall effect they have on the human body:[12]		Physical exercise can also include training that focuses on accuracy, agility, power, and speed.[16]		Sometimes the terms 'dynamic' and 'static' are used.[citation needed] 'Dynamic' exercises such as steady running, tend to produce a lowering of the diastolic blood pressure during exercise, due to the improved blood flow. Conversely, static exercise (such as weight-lifting) can cause the systolic pressure to rise significantly (during the exercise).[17]		Physical exercise is important for maintaining physical fitness and can contribute to maintaining a healthy weight, regulating digestive health, building and maintaining healthy bone density, muscle strength, and joint mobility, promoting physiological well-being, reducing surgical risks, and strengthening the immune system. Some studies indicate that exercise may increase life expectancy and the overall quality of life.[19] People who participate in moderate to high levels of physical exercise have a lower mortality rate compared to individuals who by comparison are not physically active.[20] Moderate levels of exercise have been correlated with preventing aging by reducing inflammatory potential.[21] The majority of the benefits from exercise are achieved with around 3500 metabolic equivalent (MET) minutes per week.[22] For example, climbing stairs 10 minutes, vacuuming 15 minutes, gardening 20 minutes, running 20 minutes, and walking or bicycling for transportation 25 minutes on a daily basis would together achieve about 3000 MET minutes a week.[22] A lack of physical activity causes approximately 6% of the burden of disease from coronary heart disease, 7% of type 2 diabetes, 10% of breast cancer and 10% of colon cancer worldwide.[23] Overall, physical inactivity causes 9% of premature mortality worldwide.[23]		Individuals can increase fitness following increases in physical activity levels.[24] Increases in muscle size from resistance training is primarily determined by diet and testosterone.[25] This genetic variation in improvement from training is one of the key physiological differences between elite athletes and the larger population.[26][27] Studies have shown that exercising in middle age leads to better physical ability later in life.[28]		The beneficial effect of exercise on the cardiovascular system is well documented. There is a direct correlation between physical inactivity and cardiovascular mortality, and physical inactivity is an independent risk factor for the development of coronary artery disease. Low levels of physical exercise increase the risk of cardiovascular diseases mortality.[29]		Children who participate in physical exercise experience greater loss of body fat and increased cardiovascular fitness.[30] Studies have shown that academic stress in youth increases the risk of cardiovascular disease in later years; however, these risks can be greatly decreased with regular physical exercise.[31] There is a dose-response relation between the amount of exercise performed from approximately 700–2000 kcal of energy expenditure per week and all-cause mortality and cardiovascular disease mortality in middle-aged and elderly populations. The greatest potential for reduced mortality is in the sedentary who become moderately active. Studies have shown that since heart disease is the leading cause of death in women, regular exercise in aging women leads to healthier cardiovascular profiles. Most beneficial effects of physical activity on cardiovascular disease mortality can be attained through moderate-intensity activity (40–60% of maximal oxygen uptake, depending on age). Persons who modify their behavior after myocardial infarction to include regular exercise have improved rates of survival. Persons who remain sedentary have the highest risk for all-cause and cardiovascular disease mortality.[32] According to the American Heart Association, exercise reduces blood pressure, LDL and total cholesterol, and body weight. It increases HDL cholesterol, insulin sensitivity, and exercise tolerance.[11]		Although there have been hundreds of studies on exercise and the immune system, there is little direct evidence on its connection to illness. Epidemiological evidence suggests that moderate exercise has a beneficial effect on the human immune system; an effect which is modeled in a J curve. Moderate exercise has been associated with a 29% decreased incidence of upper respiratory tract infections (URTI), but studies of marathon runners found that their prolonged high-intensity exercise was associated with an increased risk of infection occurrence. However, another study did not find the effect. Immune cell functions are impaired following acute sessions of prolonged, high-intensity exercise, and some studies have found that athletes are at a higher risk for infections. Studies have shown that strenuous stress for long durations, such as training for a marathon, can suppress the immune system by decreasing the concentration of lymphocytes.[33] The immune systems of athletes and nonathletes are generally similar. Athletes may have slightly elevated natural killer cell count and cytolytic action, but these are unlikely to be clinically significant.[34]		Vitamin C supplementation has been associated with lower incidence of URTIs in marathon runners.[34]		Biomarkers of inflammation such as C-reactive protein, which are associated with chronic diseases, are reduced in active individuals relative to sedentary individuals, and the positive effects of exercise may be due to its anti-inflammatory effects. In individuals with heart disease, exercise interventions lower blood levels of fibrinogen and C-reactive protein, an important cardiovascular risk marker.[35] The depression in the immune system following acute bouts of exercise may be one of the mechanisms for this anti-inflammatory effect.[34]		A systematic review evaluated 45 studies that examined the relationship between physical activity and cancer survivorship. According to the study results "There was consistent evidence from 27 observational studies that physical activity is associated with reduced all-cause, breast cancer–specific, and colon cancer–specific mortality".[36]		Physical exercise was correlated with a lower methylation frequency of two tumor suppressor genes, CACNA2D3 and L3MBTL.[37][38] Hypermethylation of CACNA2D3 is associated with gastric cancer, while hypermethylation of L3MBTL is associated with breast cancer, brain tumors and hematological malignancies.[37][38][39][40] A recent study indicates that exercise results in reduced DNA methylation at CpG sites on genes associated with breast cancer.[41]		Physical exercise is becoming a widely accepted non-pharmacological intervention for the prevention and attenuation of cancer cachexia.[42] "Cachexia is a multiorganic syndrome associated with cancer, characterized by inflammation, body weight loss (at least 5%) and muscle and adipose tissue wasting".[43] Exercise triggers the activation of the transcriptional coactivator peroxisome proliferator-activated receptor gamma coactivator-1α (PGC-1α), which suppresses FoxO- and NF-κB-dependent gene transcription during muscle atrophy that is induced by fasting or denervation; thus, PGC-1α may be a key intermediate responsible for the beneficial antiatrophic effects of physical exercise on cancer cachexia.[44][45] The exercise-induced isoform PGC-1α4, which can repress myostatin and induce IGF1 and hypertrophy, is a potential drug target for treatment of cancer cachexia.[46] Other factors, such as JUNB and SIRT1, that maintain skeletal muscle mass and promote hypertrophy are also induced with regular physical exercise.[47][48]		The neurobiological effects of physical exercise are numerous and involve a wide range of interrelated effects on brain structure, brain function, and cognition.[49][50][51][52] A large body of research in humans has demonstrated that consistent aerobic exercise (e.g., 30 minutes every day) induces persistent improvements in certain cognitive functions, healthy alterations in gene expression in the brain, and beneficial forms of neuroplasticity and behavioral plasticity; some of these long-term effects include: increased neuron growth, increased neurological activity (e.g., c-Fos and BDNF signaling), improved stress coping, enhanced cognitive control of behavior, improved declarative, spatial, and working memory, and structural and functional improvements in brain structures and pathways associated with cognitive control and memory.[49][50][51][52][53][54][55][56][57][58] The effects of exercise on cognition have important implications for improving academic performance in children and college students, improving adult productivity, preserving cognitive function in old age, preventing or treating certain neurological disorders, and improving overall quality of life.[49][59][60]		In healthy adults, aerobic exercise has been shown to induce transient effects on cognition after a single exercise session and persistent effects on cognition following regular exercise over the course of several months.[49][58][61] People who regularly perform aerobic exercise (e.g., running, jogging, brisk walking, swimming, and cycling) have greater scores on neuropsychological function and performance tests that measure certain cognitive functions, such as attentional control, inhibitory control, cognitive flexibility, working memory updating and capacity, declarative memory, spatial memory, and information processing speed.[49][53][55][57][58][61] The transient effects of exercise on cognition include improvements in most executive functions (e.g., attention, working memory, cognitive flexibility, inhibitory control, problem solving, and decision making) and information processing speed for a period of up to 2 hours after exercising.[61]		Aerobic exercise induces short- and long-term effects on mood and emotional states by promoting positive affect, inhibiting negative affect, and decreasing the biological response to acute psychological stress.[61] Over the short-term, aerobic exercise functions as both an antidepressant and euphoriant,[62][63][64][65] whereas consistent exercise produces general improvements in mood and self-esteem.[66][67]		Regular aerobic exercise improves symptoms associated with a variety of central nervous system disorders and may be used as an adjunct therapy for these disorders. There is clear evidence of exercise treatment efficacy for major depressive disorder and attention deficit hyperactivity disorder.[59][64][68][69][70][71] A large body of preclinical evidence and emerging clinical evidence supports the use of exercise therapy for treating and preventing the development of drug addictions.[72][73][74][75][76] Reviews of clinical evidence also support the use of exercise as an adjunct therapy for certain neurodegenerative disorders, particularly Alzheimer’s disease and Parkinson's disease.[77][78][79][80][81][82] Regular exercise is also associated with a lower risk of developing neurodegenerative disorders.[80][83] Regular exercise has also been proposed as an adjunct therapy for brain cancers.[84]		Physical exercise has established efficacy as an antidepressant in individuals with depression and current medical evidence supports the use of exercise as both a preventive measure against and an adjunct therapy with antidepressant medication for depressive disorders.[59][64][65][68][69] A July 2016 meta-analysis concluded that physical exercise improves overall quality of life in individuals with depression relative to controls.[59] One systematic review noted that yoga may be effective in alleviating symptoms of prenatal depression.[85] The biomolecular basis for exercise-induced antidepressant effects is believed to be a result of increased neurotrophic factor signaling, particularly brain-derived neurotrophic factor.[68][56]		Continuous aerobic exercise can induce a transient state of euphoria, colloquially known as a "runner's high" in distance running or a "rower's high" in crew, through the increased biosynthesis of at least three euphoriant neurochemicals: anandamide (an endocannabinoid),[86] β-endorphin (an endogenous opioid),[87] and phenethylamine (a trace amine and amphetamine analog).[88][89][90]		A systematic review noted that, although limited, some evidence suggests that the duration of engagement in a sedentary lifestyle is positively correlated with a risk of developing an anxiety disorder or experiencing anxiety symptoms.[91] It noted that additional research is needed in order to confirm these findings.[91]		A 2010 review of published scientific research suggested that exercise generally improves sleep for most people, and helps sleep disorders such as insomnia. The optimum time to exercise may be 4 to 8 hours before bedtime, though exercise at any time of day is beneficial, with the possible exception of heavy exercise taken shortly before bedtime, which may disturb sleep. There is, in any case, insufficient evidence to draw detailed conclusions about the relationship between exercise and sleep.[92]		According to a 2005 study, exercise is the most recommended alternative to sleeping pills for resolving insomnia. Sleeping pills are more costly than to make time for a daily routine of staying fit, and may have dangerous side effects in the long run. Exercise can be a healthy, safe and inexpensive way to achieve more and better sleep.[93]		Too much exercise can be harmful. Without proper rest, the chance of stroke or other circulation problems increases,[94] and muscle tissue may develop slowly. Extremely intense, long-term cardiovascular exercise, as can be seen in athletes who train for multiple marathons, has been associated with scarring of the heart and heart rhythm abnormalities.[95][96][97] Specifically, high cardiac output has been shown to cause enlargement of the left and right ventricle volumes, increased ventricle wall thickness, and greater cardiac mass. These changes further result in myocardial cell damage in the lining of the heart, leading to scar tissue and thickened walls. During these processes, the protein troponin increases in the bloodstream, indicating cardiac muscle cell death and increased stress on the heart itself.[98]		Inappropriate exercise can do more harm than good, with the definition of “inappropriate” varying according to the individual. For many activities, especially running and cycling, there are significant injuries that occur with poorly regimented exercise schedules. Injuries from accidents also remain a major concern,[99] whereas the effects of increased exposure to air pollution seem only a minor concern.[100][101]		In extreme instances, over-exercising induces serious performance loss. Unaccustomed overexertion of muscles leads to rhabdomyolysis (damage to muscle) most often seen in new army recruits.[102] Another danger is overtraining, in which the intensity or volume of training exceeds the body's capacity to recover between bouts. One sign of Overtraining Syndrome (OTS) is suppressed immune function, with an increased incidence of upper respiratory tract infection (URTI). An increased incidence of URTIs is also associated with high volume/intensity training, as well as with excessive exercise (EE), such as in a marathon.[103]		Stopping excessive exercise suddenly may create a change in mood. Exercise should be controlled by each body's inherent limitations. While one set of joints and muscles may have the tolerance to withstand multiple marathons, another body may be damaged by 20 minutes of light jogging. This must be determined for each individual.		Too much exercise may cause a woman to miss her periods, a symptom known as amenorrhea.[104] This is a very serious condition which indicates a woman is pushing her body beyond its natural boundaries.[105]		Resistance training and subsequent consumption of a protein-rich meal promotes muscle hypertrophy and gains in muscle strength by stimulating myofibrillar muscle protein synthesis (MPS) and inhibiting muscle protein breakdown (MPB).[106][107] The stimulation of muscle protein synthesis by resistance training occurs via phosphorylation of the mechanistic target of rapamycin (mTOR) and subsequent activation of mTORC1, which leads to protein biosynthesis in the ribosome via phosphorylation of mTORC1's immediate targets (the p70S6 kinase and the translation repressor protein 4EBP1).[106][108] The suppression of muscle protein breakdown following food consumption occurs primarily via increases in plasma insulin;[106][109] however, a suppression of MPB of comparable magnitude has also been shown to occur in humans from a sufficient elevation of plasma β-hydroxy β-methylbutyric acid.[106][109][110]		Aerobic exercise induces mitochondrial biogenesis and an increased capacity for oxidative phosphorylation in the mitochondria of skeletal muscle, which is one mechanism by which aerobic exercise enhances submaximal endurance performance.[106][111] These effects occur via an exercise-induced increase in the intracellular AMP:ATP ratio, thereby triggering the activation of AMP-activated protein kinase (AMPK) which subsequently phosphorylates peroxisome proliferator-activated receptor gamma coactivator-1α (PGC-1α), the master regulator of mitochondrial biogenesis.[106][111][112]		Developing research has demonstrated that many of the benefits of exercise are mediated through the role of skeletal muscle as an endocrine organ. That is, contracting muscles release multiple substances known as myokines which promote the growth of new tissue, tissue repair, and multiple anti-inflammatory functions, which in turn reduce the risk of developing various inflammatory diseases.[126] Exercise reduces levels of cortisol, which causes many health problems, both physical and mental.[127] Endurance exercise before meals lowers blood glucose more than the same exercise after meals.[128] There is evidence that vigorous exercise (90–95% of VO2 max) induces a greater degree of physiological cardiac hypertrophy than moderate exercise (40 to 70% of VO2 max), but it is unknown whether this has any effects on overall morbidity and/or mortality.[129] Both aerobic and anaerobic exercise work to increase the mechanical efficiency of the heart by increasing cardiac volume (aerobic exercise), or myocardial thickness (strength training). Ventricular hypertrophy, the thickening of the ventricular walls, is generally beneficial and healthy if it occurs in response to exercise.		The persistent long-term neurobiological effects of regular physical exercise[note 1] are believed to be mediated by transient exercise-induced increases in the concentration of neurotrophic factors (e.g., BDNF, IGF-1, VEGF, and GDNF) and other biomolecules in peripheral blood plasma, which subsequently cross the blood–brain barrier and blood–cerebrospinal fluid barrier and bind to their associated receptors in the brain.[50][66][130][131] Upon binding to their receptors in cerebral vasculature and brain cells (i.e., neurons and glial cells), these biomolecules trigger intracellular signaling cascades that lead to neuroplastic biological responses – such as neurogenesis, synaptogenesis, oligodendrogenesis, and angiogenesis, among others – which ultimately mediate the exercise-induced improvements in cognitive function.[50][53][130][132][133]		Multiple component community-wide campaigns are frequently used in an attempt to increase a population's level of physical activity. A 2015 Cochrane review, however, did not find evidence supporting a benefit.[134] The quality of the underlying evidence was also poor.[134] However, there is some evidence that school-based interventions can increase activity levels and fitness in children.[24] Another Cochrane review found some evidence that certain types of exercise programmes, such as those involving gait, balance, co-ordination and functional tasks, can improve balance in older adults.[135] Following progressive resistance training, older adults also respond with improved physical function.[136] Survey of brief interventions promoting physical activity found that they are cost-effective, although there are variations between studies.[137]		Environmental approaches appear promising: signs that encourage the use of stairs, as well as community campaigns, may increase exercise levels.[138] The city of Bogotá, Colombia, for example, blocks off 113 kilometers (70 mi) of roads on Sundays and holidays to make it easier for its citizens to get exercise. These pedestrian zones are part of an effort to combat chronic diseases, including obesity.[139]		To identify which public health strategies are effective, a Cochrane overview of reviews is in preparation.[140]		Physical exercise was said to decrease healthcare costs, increase the rate of job attendance, as well as increase the amount of effort women put into their jobs.[141]		Children will mimic the behavior of their parents in relation to physical exercise. Parents can thus promote physical activity and limit the amount of time children spend in front of screens which may decrease the risk of childhood obesity.[142]		Overweight children who participate in physical exercise experience greater loss of body fat and increased cardiovascular fitness. According to the Centers for Disease Control and Prevention in the United States, both children and adults should do 60 minutes or more of physical activity each day.[143] Implementing physical exercise in the school system and ensuring an environment in which children can reduce barriers to maintain a healthy lifestyle is essential.		Worldwide there has been a large shift towards less physically demanding work.[144] This has been accompanied by increasing use of mechanized transportation, a greater prevalence of labor saving technology in the home, and fewer active recreational pursuits.[144] Personal lifestyle changes however can correct the lack of physical exercise.		Research in 2015 indicates integrating mindfulness to physical exercise interventions increases exercise adherence, self-efficacy and also has positive effects both psychologically and physiologically.[145]		Exercising looks different in every country, as do the motivations behind exercising.[9] In some countries, people exercise primarily indoors, and in others, people exercise primarily outdoors. People may exercise for personal enjoyment, health and well-being, social interactions, competition or training, etc. These differences could potentially be attributed to geographic location, social tendencies, or otherwise.		In Colombia, citizens value and celebrate the outdoor environments of their country. In many instances, they utilize outdoor activities as social gatherings to enjoy nature and their communities. In Bogotá, Colombia, a 70-mile stretch of road known as the Ciclovía is shut down each Sunday for bicyclists, runners, rollerbladers, skateboarders and other exercisers to work out and enjoy their surroundings.[146]		Similarly to Colombia, citizens of Cambodia tend to exercise socially outside. In this country, public gyms have become quite popular. People will congregate at these outdoor gyms not only to utilize the public facilities, but also to organize aerobics and dance sessions, which are open to the public.[147]		Sweden has also begun developing outdoor gyms, called utegym. These gyms are free to the public and are often placed in beautiful, picturesque environments. People will swim in rivers, use boats, and run through forests to stay healthy and enjoy the natural world around them. This is especially possible in Sweden due to its geographical location.[148]		Chinese exercise, particularly in the retired community, seems to be socially grounded. In the mornings, dances are held in public parks; these gatherings may include Latin dancing, ballroom dancing, tango, or even the jitterbug. Dancing in public allows people to interact with those with whom they would not normally interact, allowing for both health benefits and social benefits.[149]		These sociocultural variations in physical exercise show how people in different geographic locations and social climates have varying motivations and methods of exercising. Physical exercise can improve health and well-being, as well as enhance community ties and appreciation of natural beauty.[9]		Proper nutrition is as important to health as exercise. When exercising, it becomes even more important to have a good diet to ensure that the body has the correct ratio of macronutrients while providing ample micronutrients, in order to aid the body with the recovery process following strenuous exercise.[150]		Active recovery is recommended after participating in physical exercise because it removes lactate from the blood more quickly than inactive recovery. Removing lactate from circulation allows for an easy decline in body temperature, which can also benefit the immune system, as an individual may be vulnerable to minor illnesses if the body temperature drops too abruptly after physical exercise.[151]		The benefits of exercise have been known since antiquity. Marcus Cicero, around 65 BCE, stated: "It is exercise alone that supports the spirits, and keeps the mind in vigor."[152]		Several mass exercise movements were started in the early twentieth century to realise the benefits of exercise. The first and most significant of these in the UK was the Women's League of Health and Beauty founded in 1930 by Mary Bagot Stack that had 166,000 members in 1937.[153]		However, the link between physical health and exercise (or lack of it) was only discovered in 1949 and reported in 1953 by a team led by Jerry Morris.[154][155] Dr. Morris noted that men of similar social class and occupation (bus conductors versus bus drivers) had markedly different rates of heart attacks, depending on the level of exercise they got: bus drivers had a sedentary occupation and a higher incidence of heart disease, while bus conductors were forced to move continually and had a lower incidence of heart disease.[155] This link had not previously been noted and was later confirmed by other researchers.		Physical exercise has been shown to benefit a wide range of other mammals, as well as salmon, juvenile crocodiles, and at least one species of bird.[156]		However, several studies have shown that lizards display no benefit from exercise, leading them to be termed "metabolically inflexible".[157] Indeed, damage from overtraining may occur following weeks of forced treadmill exercise in lizards.[157]		A number of studies of both rodents and humans have demonstrated that individual differences in both ability and propensity for exercise (i.e., voluntary exercise) have some genetic basis.[158][159]		Several studies of rodents have demonstrated that maternal [160] or juvenile access to wheels that allow voluntary exercise can increase the propensity to run as adults.[161] These studies further suggest that physical activity may be more "programmable" (for discusison, see Thrifty phenotype) than food intake.[162]		|group2 = See also |list2 =		|below = }}		
The social determinants of health (SDOH) are the economic and social conditions and their distribution among the population that influence individual and group differences in health status. They are health promoting factors found in one's living and working conditions (such as the distribution of income, wealth, influence, and power), rather than individual risk factors (such as behavioural risk factors or genetics) that influence the risk for a disease, or vulnerability to disease or injury. According to some viewpoints, the distributions of social determinants are shaped by public policies that reflect the influence of prevailing political ideologies of those governing a jurisdiction.[1] The World Health Organization says, "This unequal distribution of health-damaging experiences is not in any sense a 'natural' phenomenon but is the result of a toxic combination of poor social policies, unfair economic arrangements [where the already well-off and healthy become even richer and the poor who are already more likely to be ill become even poorer], and bad politics."[2]						There is no single definition of the social determinants of health, but there are commonalities, and many governmental and non-governmental organizations recognize that there are social factors which impact the health of individuals.		In 2003, the World Health Organisation (WHO) Europe suggested that the social determinants of health included:[3]		In Canada, these social determinants of health have gained wide usage.[4]		These SDOH are clearly related to health outcomes, are closely tied to public policy, and are clearly understandable by the public. They tend to cluster together – for example, those living in poverty also experience numerous other adverse social determinants. The quality and equitable distribution of these social determinants in Canada and the USA are clearly well below the standards seen in other developed nations.[4]		The WHO later developed a Commission on Social Determinants of Health, which in 2008 published a report entitled "Closing the Gap in a Generation".[2] This report identified two broad areas of social determinants of health that needed to be addressed. The first area was daily living conditions, which included healthy physical environments, fair employment and decent work, social protection across the lifespan, and access to health care. The second major area was distribution of power, money, and resources, including equity in health programs, public financing of action on the social determinants, economic inequalities, resource depletion, healthy working conditions, gender equity, political empowerment, and a balance of power and prosperity of nations.[2]		The 2011 World Conference on Social Determinants of Health brought together delegations from 125 member states and resulted in the Rio Political Declaration on Social Determinants of Health. This declaration involved an affirmation that health inequities are unacceptable, and noted that these inequities arise from the societal conditions in which people are born, grow, live, work, and age, including early childhood development, education, economic status, employment and decent work, housing environment, and effective prevention and treatment of health problems.[5]		The United States Centers for Disease Control defines social determinants of health as "life-enhancing resources, such as food supply, housing, economic and social relationships, transportation, education, and health care, whose distribution across populations effectively determines length and quality of life".[6] These include access to care and resources such as food, insurance coverage, income, housing, and transportation.[6] Social determinants of health influence health-promoting behaviours, and health equity among the population is not possible without equitable distribution of social determinants among groups.[6]		Woolf states, "The degree to which social conditions affect health is illustrated by the association between education and mortality rates".[7] Reports in 2005 revealed the mortality rate was 206.3 per 100,000 for adults aged 25 to 64 years with little education beyond high school, but was twice as great (477.6 per 100,000) for those with only a high school education and 3 times as great (650.4 per 100,000) for those less educated. Based on the data collected, the social conditions such as education, income, and race were very much dependent on one another, but these social conditions also apply independent health influences.[7]		Marmot and Bell found that in wealthy countries, income and mortality are correlated as a marker of relative position within society, and this relative position is related to social conditions that are important for health including good early childhood development, access to good quality education, rewarding work with some degree of autonomy, decent housing, and a clean and safe living environment. The social condition of autonomy, control, and empowerment turns are important influences on health and disease, and individuals who lack social participation and control over their lives are at a greater risk for heart disease and mental illness.[8]		Even in the wealthiest countries, there are economic inequalities in health between the rich and the poor.[3] Canadian authors Labonte and Schrecker from the University of Ottawa note that globalization is a key context for the study of the social determinants of health, and as Bushra (2011) the impacts of globalization are asymmetric.[9] As a result, there is an uneven distribution of wealth and influence both within and across national borders, leading to negative impacts on the social determinants of health. The Organization for Economic Cooperation and Development found significant differences among developed nations in health status indicators such as life expectancy, infant mortality, incidence of disease, and death from injuries.[10] International labor migration and the policies that attempt to regulate it can also impact the health of only those that migrate but their family members who stay behind.[11]		These inequalities may exist in the context of the health care system, or in broader social approaches. According to the WHO's Commission on Social Determinants of Health, access to health care is essential for equitable health, and it argued that health care should be a common good rather than a market commodity.[2] However, there is substantial variation in health care systems and coverage from country to country. The Commission also calls for government action on such things as access to clean water and safe, equitable working conditions, and it notes that dangerous working conditions exist even in some wealthy countries.[2][12] In the Rio Political Declaration on Social Determinants of Health, several key areas of action were identified to address inequalities, including promotion of participatory policy-making processes, strengthening global governance and collaboration, and encouraging developed countries to reach a target of 0.7% of gross national product (GNP) for official development assistance.[5]		The UK Black and The Health Divide reports considered two primary mechanisms for understanding the process by which the social determinants influence health: cultural/ behavioural and materialist/structuralist.[13] The cultural/behavioural explanation was that individuals' behavioural choices (e.g., tobacco and alcohol use, diet, physical activity, etc.) were responsible for their developing and dying from a variety of diseases. However, both the Black and Health divide reports found that behavioural choices are heavily structured by one's material conditions of life, and these behavioural risk factors account for a relatively small proportion of variation in the incidence and death from various diseases.		The materialist/structuralist explanation emphasizes the material conditions under which people live. These conditions include availability of resources to access the amenities of life, working conditions, and quality of available food and housing among others. Within this view, three frameworks have been developed to explain how social determinants influence health.[14] These frameworks are: (a) materialist; (b) neo-materialist; and (c) psychosocial comparison. The materialist explanation is about how living conditions – and the social determinants of health that constitute these living conditions – shape health. The neo-materialist explanation extends the materialist analysis by asking how these living conditions come about. The psychosocial comparison explanation considers whether people compare themselves to others and how these comparisons affect health and wellbeing.		The wealth of nations is a strong indicator of population health. But within nations, socio-economic position is a powerful predictor of health as it is an indicator of material advantage or disadvantage over the lifespan.[15] Material conditions of life determine health by influencing the quality of individual development, family life and interaction, and community environments. Material conditions of life lead to differing likelihood of physical (infections, malnutrition, chronic disease, and injuries), developmental (delayed or impaired cognitive, personality, and social development), educational (learning disabilities, poor learning, early school leaving), and social (socialization, preparation for work, and family life) problems.[16] Overall wealth of nations is a strong indicator of population health. But within nations, socio-economic position is a powerful predictor of health as it is an indicator of material advantage or disadvantage over the lifespan.[15] Material conditions of life also lead to differences in psychosocial stress.[17] The fight-or-flight reaction – chronically elicited in response to threats such as income, housing, and food insecurity, among others – weakens the immune system, leads to increased insulin resistance, greater incidence of lipid and clotting disorders, and other biomedical insults that are precursors to adult disease.		Adoption of health-threatening behaviours is also influenced by material deprivation and stress.[18] Environments influence whether individuals take up tobacco, use alcohol, experience poor diets, and have low levels of physical activity. Tobacco and excessive alcohol use, and carbohydrate-dense diets are also means of coping with difficult circumstances.[19][18] The materialist approach offers insight into the sources of health inequalities among individuals and nations and the role played by the social determinants of health.		The neo-materialist approach is concerned with how nations, regions, and cities differ on how economic and other resources are distributed among the population.[20] This distribution of resources can vary widely from country to country. The neo-materialist view therefore, directs attention to both the effects of living conditions – the social determinants of health – on individuals' health and the societal factors that determine the quality of the distribution of these social determinants of health. How a society decides to distribute resources among citizens is especially important.		The social comparison approach holds that the social determinants of health play their role through citizens' interpretations of their standings in the social hierarchy.[21] There are two mechanisms by which this occurs. At the individual level, the perception and experience of one's status in unequal societies lead to stress and poor health. Feelings of shame, worthlessness, and envy can lead to harmful effects upon neuro-endocrine, autonomic and metabolic, and immune systems.[17] Comparisons to those of a higher social class can also lead to attempts to alleviate such feelings by overspending, taking on additional employment that threaten health, and adopting health-threatening coping behaviours such as overeating and using alcohol and tobacco.[21] At the communal level, widening and strengthening of hierarchy weakens social cohesion, which is a determinant of health.[22] The social comparison approach directs attention to the psychosocial effects of public policies that weaken the social determinants of health. However, these effects may be secondary to how societies distribute material resources and provide security to its citizens, which are described in the materialist and neo-materialist approaches.[citation needed]		Life-course approaches emphasize the accumulated effects of experience across the life span in understanding the maintenance of health and the onset of disease. The economic and social conditions – the social determinants of health – under which individuals live their lives have a cumulative effect upon the probability of developing any number of diseases, including heart disease and stroke [23][24] Studies into the childhood and adulthood antecedents of adult-onset diabetes show that adverse economic and social conditions across the life span predispose individuals to this disorder.[25][26]		Hertzman outlines three health effects that have relevance for a life-course perspective.[27] Latent effects are biological or developmental early life experiences that influence health later in life. Low birth weight, for instance, is a reliable predictor of incidence of cardiovascular disease and adult-onset diabetes in later life. Experience of nutritional deprivation during childhood has lasting health effects.		Pathway effects are experiences that set individuals onto trajectories that influence health, well-being, and competence over the life course. As one example, children who enter school with delayed vocabulary are set upon a path that leads to lower educational expectations, poor employment prospects, and greater likelihood of illness and disease across the lifespan. Deprivation associated with poor-quality neighbourhoods, schools, and housing sets children off on paths that are not conducive to health and well-being.[28][citation needed]		Cumulative effects are the accumulation of advantage or disadvantage over time that manifests itself in poor health. These involve the combination of latent and pathways effects. Adopting a life-course perspective directs attention to how social determinants of health operate at every level of development – early childhood, childhood, adolescence, and adulthood – to both immediately influence health and provide the basis for health or illness later in life.[29][citation needed]		Stress is hypothesised to be a major influence in the social determinants of health. There is a relationship between experience of chronic stress and negative health outcomes.[30] There are two ways that this relationship is explained, through both direct and indirect effects of chronic stress on health outcomes.		One possible reason for the relationship between chronic stress and health outcomes is the effect that stress has on the physiology of a person. This is referred to as a direct relationship between chronic stress and health. The long term stress hormone, cortisol, is believed to be the key driver in this relationship.[31] Chronic stress has been found to be significantly associated with chronic low grade inflammation, slower wound healing, increased susceptibility to infections, and poorer responses to vaccines.[30] For example, a study of otherwise healthy adult males found that those with high levels of perceived stress in their lives had significantly longer wound healing times, and that elevated cortisol levels, rather than health behaviour responses, appeared to be the main reason for this relationship.[32] Meta-analysis of healing studies has found that there is a robust relationship between elevated stress levels and slower healing for many different acute and chronic conditions[33] However, it is also important to note that certain factors, such as coping styles and social support, can mitigate the relationship between chronic stress and health outcomes.[34][35]		Stress can also be seen to have an indirect effect on health status. One way this happens is due to the strain on the psychological resources of the stressed individual. Chronic stress is common in those of a low socio-economic status, who are having to balance worries about financial security, how they will feed their families, housing status, and many other concerns.[36] Therefore, individuals with these kinds of worries may lack the emotional resources to adopt positive health behaviours. Chronically stressed individuals may therefore be less likely to be able to prioritise their health.		In addition to this, the way that an individual responds to stress can influence their health status. Often, individuals responding to chronic stress will develop coping behaviours, some of which have a positive influences on health and others which have a negative influence. People who cope with stress through positive behaviours such as exercise or social connections may not be as affected by the relationship between stress and health, whereas those with a coping style more prone to over consumption (i.e. emotional eating, drinking, smoking or drug use) are more likely to be see negative health effects of stress.[34] For example, a laboratory study has shown that, while stress did not alter calorie intake or food type eaten when averaged across the sample as a whole, when looking at a subset of the sample who were classed as emotional eaters, high stress levels were associated with eating more high fat and sweet food and a more calorie dense meal overall.[37] Therefore, the authors conclude that stress may compromise the health of certain susceptible individuals (I.e those with an emotional eating coping style).[37]		The detrimental effects of stress on health outcomes are hypothesised to partly explain why countries that have high levels of income inequality have poorer health outcomes compared to more equal countries.[38] Wilkinson and Picket hypothesise in their book The Spirit Level that the stressors associated with low social status are amplified in societies where others are clearly far better off.[38]		Reducing the health gap in a generation requires that governments build systems that allow a healthy standard of living where no one should fall below due to circumstances beyond his or her control. Social protection schemes can be instrumental in realizing developmental goals rather than being dependent on achieving those goals. They can be effective ways to reduce poverty and local economies can benefit.[2]		Policies to reduce child poverty are particularly important, as elevated stress hormones in children interfere with the development of brain circuitry and connections, causing long term chemical damage.[39] Studies showed that the immune system of participants were stronger if their parents had the security of home ownership while the participants were growing up.[40][citation needed] In most wealthy countries, the relative child poverty rate is 10 percent or less; in the United States, it is 21.9 percent.[41] The lowest poverty rates are more common in smaller well-developed and high-spending welfare states like Sweden and Finland, with about 5 or 6 percent.[41] Middle-level rates are found in major European countries where unemployment compensation is more generous and social policies provide more generous support to single mothers and working women (through paid family leave, for example), and where social assistance minimums are high. For instance, the Netherlands, Austria, Belgium and Germany have poverty rates that are in the 7 to 8 percent range.[42]		The Commission on Social Determinants of Health (CSDH) in 2005 made recommendations for action to promote health equity based on 3 principles of action: "improve the circumstances in which people are born, grow, live, work, and age; tackle the inequitable distribution of power, money, and resources, the structural drivers of conditions of daily life, globally, nationally, and locally; and measure the problem, evaluate action, and expand the knowledge base."[8] These recommendations would involve providing resources such as quality education, decent housing, access to affordable health care, access to healthy food, and safe places to exercise for everyone despite gaps in affluence. Expansion of knowledge of the social determinants of health, including among healthcare workers, can improve the quality and standard of care for people who are marginalized, poor or living in developing nations by preventing early death and disability while working to improve quality of life.[43]		The Rio Political Declaration on Social Determinants of Health embraces a transparent, participatory model of policy development that, among other things, addresses the social determinants of health leading to persistent health inequalities for indigenous peoples.[5]		The United States Department of Health and Human Services includes social determinants in its model of population health, and one of its missions is to strengthen policies which are backed by the best available evidence and knowledge in the field [44] Social determinants of health do not exist in a vacuum. Their quality and availability to the population are usually a result of public policy decisions made by governing authorities. For example, early life is shaped by availability of sufficient material resources that assure adequate educational opportunities, food and housing among others. Much of this has to do with the employment security and the quality of working conditions and wages. The availability of quality, regulated childcare is an especially important policy option in support of early life.[45] These are not issues that usually come under individual control but rather they are socially constructed conditions which require institutional responses.[46] A policy-oriented approach places such findings within a broader policy context. In this context, Health in All Policies has seen as a response to incorporate health and health equity into all public policies as means to foster synergy between sectors and ultimately promote health.		Yet it is not uncommon to see governmental and other authorities individualize these issues. Governments may view early life as being primarily about parental behaviours towards their children. They then focus upon promoting better parenting, assist in having parents read to their children, or urge schools to foster exercise among children rather than raising the amount of financial or housing resources available to families. Indeed, for every social determinant of health, an individualized manifestation of each is available. There is little evidence to suggest the efficacy of such approaches in improving the health status of those most vulnerable to illness in the absence of efforts to modify their adverse living conditions.[47]		One of the recommendations by the CSDH is expanding knowledge – particularly to health care workers.[43]		A recent article outlines the role that "raw power" plays in shaping the distribution of the social determinants of health through public policy action.[48] Here the business sector is seen as shaping public policy to increase profits at the expense of the population's health. This is a political economy approach that is typically ignored in discussions of the social determinants of health.		
Systole /ˈsɪstəliː/ is the part of the cardiac cycle when a heart chamber contracts.[1] The term "systole" originates from New Latin, from Ancient Greek συστολή (sustolē), from συστέλλειν (sustellein, "to contract"), from σύν (syn, "together") + στέλλειν (stellein, "send").		The mammalian heart has 4 chambers: the left atrium, the left ventricle, the right atrium and the right ventricle.		When the smaller, upper atria chambers contract in late diastole, they send blood down to the larger, lower ventricle chambers. When the lower chambers are filled and the valves to the atria are closed, the ventricles undergo isovolumetric contraction (contraction of the ventricles while all valves are closed), marking the first stage of systole. The second phase of systole sends blood from the left ventricle to the aorta and body extremities, and from the right ventricle to the lungs. Thus, the atria and ventricles contract in alternating sequence. The left and right atria feed blood, at the same time, into the ventricles. Then, the left and right ventricles contract simultaneously as well.		Cardiac systole is the contraction of the cardiac muscle in response to an electrochemical stimulus to the heart's cells (cardiomyocytes).		The cardiac output (CO) is the volume of blood pumped by the left ventricle in one minute. The ejection fraction (EF) is the volume of blood pumped divided by the total volume of blood in the left ventricle.[2]						Cardiac electrical systole is staged and first derived from sympathetic charge from the sinoatrial node (SA node). Subsequent physiologic discharge from the SA node then finds its way through the atrial mass, eventually meeting at the atrioventricular node to be gated through the available channels from the atria .		Electrical systole opens voltage-gated sodium, potassium and calcium channels. A rise in intracellular calcium triggers the interaction of actin and myosin in the presence of ATP that generates force (see Physiological mechanism below). The muscular contraction of myocardium generates an active stress that increases intra-ventricular pressure and when intra-ventricular pressure exceeds aortic pressure there is ejection of blood. Mechanical systole is the origin of the pulse. The pulse is readily palpated (felt) at many points on the body and represents a universally accepted tactile (and sometimes visual) method of observing peak or systolic blood pressure. Mechanical forces resulting from electrical systole cause rotation of the muscle mass around the long and short axes; a process envisaged as "wringing" of the ventricles.		Atrial systole represents the contraction of myocardium of the left and right atria. Atrial systole occurs late in ventricular diastole. One force driving blood from the atria to the ventricles is the decrease in ventricular pressure that occurs during ventricular diastole. The drop in ventricular pressure that occurs during ventricular diastole allows the atrioventricular valves to open, emptying the contents of the atria into the ventricles. Contraction of the atrium confers a relatively minor, additive effect toward ventricular filling; atrial contraction becomes significant in left ventricular hypertrophy, in which the ventricle does not fully relax during ventricular diastole. Loss of normal electrical conduction in the heart, as seen during atrial fibrillation, atrial flutter, and complete heart block, may abolish atrial systole. The aortic valve and pulmonary valve remain closed, while the atrioventricular mitral and tricuspid valves remain open because the pressure gradient between the atrium and ventricle is preserved during late ventricular diastole.		Contraction of the atria follows depolarization, represented by the P wave of the ECG. As the atrial muscles contract from the superior portion of the atria toward the atrioventricular septum, pressure rises within the atria and blood is pumped into the ventricles through the open atrioventricular (tricuspid, and mitral or bicuspid) valves. At the start of atrial systole, the ventricles are normally filled with approximately 70–80 percent of their capacity due to inflow during diastole. Atrial contraction, also referred to as the "atrial kick," contributes the remaining 20–30 percent of filling. Atrial systole lasts approximately 100 ms and ends prior to ventricular systole, as the atrial muscle returns to diastole.[3]		The ventricles are histologically and electrically isolated from the atria by the unique and electrically impermeable collagen layers of connective tissue known as the cardiac skeleton. The bulwarks of this entity stem from the central body to form the four valve rings. Collagen extensions from the valve rings seal and limit atrial electrical influence from ventricular electrical influence to the SA/AV/Purkinje pathways. Exceptions such as accessory pathways may occur in this firewall between atrial and ventricular electrical influence but are rare. The compromised load of atrial fibrillation detracts from overall performance but the ventricles continue to work as a physiologically effective pump. Given this pathology, ejection fraction may deteriorate by ten to thirty percent. Uncorrected atrial fibrillation can lead to heart rates approaching 200 beats per minute. If one can slow this rate down to a normal range of approximately 80 beats per minute, the filling time of the heart cycle is longer and confers additional benefit to the pumping ability of the heart. Breathless individuals with uncontrolled atrial fibrillation can be rapidly returned to normal breathing when conversion with medication or electrical cardioversion is attempted. Pharmacological manipulation of rate control, for example, by beta blocker|beta adrenoceptor antagonists, non-dyhydropyridine calcium channel blockers and digoxin are important historical interventions in this condition. Individuals prone to a hypercoagulable state are at a decided risk of thromboembolism, thus requiring therapy with warfarin for life if the defined pathology cannot be corrected.		Right atrial systole coincides with right ventricular diastole, driving the blood through the tricuspid valve (TV), into the right ventricle. The time variable of right atrial systole, is (TV) open to (TV) close.		Left atrial systole coincides with left ventricular diastole, driving blood through the mitral valve (MV) (also known as the bicuspid valve), into the left ventricle. The time variable of left atrial systole is (MV) open to (MV) close. The atria contains two valves, the mitral (bicuspid) and the tricuspid valves which open during the late stages of diastole		Atrial fibrillation represents a common electrical malady apparent during the time interval of atrial systole. Theory suggests that an ectopic focus, usually within the pulmonary trunks, competes with the sinoatrial node for electrical control of the atrial chambers to the detriment of atrial myocardial performance. Ordered sinoatrial control of atrial electrical activity is lost, as a result coordinated pressure generation does not occur in the upper cardiac chambers. Atrial fibrillation represents an electrically disordered but well blood perfused atrial mass working in an uncoordinated fashion with an electrically (comparatively) healthy ventricle.		Ventricular systole is a written description of the contraction of the myocardium of the left and right ventricles. Ventricular systole induces increased pressure in the left and right ventricles. Pressure in the ventricles rises to a level above that of the atria, thus closing the tricuspid and mitral valves, which are prevented from inverting by chordae tendineae and associated papillary muscles. Ventricular pressure continues to rise in isovolumetric contraction with maximal pressure generation (max dP/dt) occurring during this phase, until the pulmonary and aortic valves open in the ejection phase. In the ejection phase, blood flows down its pressure gradient through the aorta and pulmonary artery from left and right ventricles respectively. It is important to note that cardiac muscle perfusion through coronary vessels does not occur during ventricular systole, but occurs during ventricular diastole.		Ventricular systole is the origin of the pulse.		Right ventricular systole drives blood through the pulmonary valve (PV) into the lungs. Right ventricular systole is volumetrically defined as right ventricular ejection fraction (RVEF). The time variable of right ventricular systole is PV open to PV close. Increased RVEF is indicative of pulmonary hypertension.		Left ventricular systole drives blood through the aortic valve (AoV) to the body and organs excluding the lungs. Left ventricular systole is volumetrically defined as left ventricular ejection fraction (LVEF). The time variable of left ventricular systole is AoV open to AoV close.		Systole of the heart is initiated by the electrically excitable cells of the sinoatrial node. These cells are activated spontaneously by depolarization of their representative membranes beyond a given threshold for excitation. At this point, voltage-gated calcium channels on the cell membrane open and allow calcium ions to pass through, into the sarcoplasm of the cardiac muscle cell. Calcium ions bind to ryanodine receptors on the sarcoplasmic reticulum causing a flux of calcium ions to the sarcoplasm.		Calcium ions bind to troponin C, causing a conformational change in the troponin-tropomyosin complex, and thus allowing myosin head binding sites on F-Actin to be exposed. This transition allows cross bridge cycling to occur. The cardiac action potential spreads distally to the small branches of the Purkinje tree via the flux of cations through gap junctions that connect the sarcoplasm of adjacent myocytes. The electrical activity of ventricular systole is coordinated by the atrioventricular node, this discrete collection of cells receives electrical stimulation from the atrium, but also has a slower intrinsic pacemaker activity. The cardiac action potential is propagated down the bundle of His to Purkinje fibres which rapidly causes coordinated depolarisation, and excitation-contraction coupling from the apex of the heart up to the roots of the great vessels.		When blood pressure is stated for medical purposes, it is usually written with the systolic and diastolic pressure separated by a slash; for example: 120/80 mmHg. This has nothing to do with the mathematical notation for a fraction or ratio: it is not a display of a numerator over a denominator but rather a medical notation used for quickly showing the two clinically significant pressures involved and cannot be reduced into lower terms.		
Neurotransmitters, also known as chemical messengers, are endogenous chemicals that enable neurotransmission. They transmit signals across a chemical synapse, such as a neuromuscular junction, from one neuron (nerve cell) to another "target" neuron, muscle cell, or gland cell.[1] Neurotransmitters are released from synaptic vesicles in synapses into the synaptic cleft, where they are received by receptors on the target cells. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available from the diet and only require a small number of biosynthetic steps for conversion. Neurotransmitters play a major role in shaping everyday life and functions. Their exact numbers are unknown, but more than 100 chemical messengers have been uniquely identified.[2]						Neurotransmitters are stored in a synapse in synaptic vesicles, clustered beneath the membrane in the axon terminal located at the presynaptic side of the synapse. Neurotransmitters are released into and diffused across the synaptic cleft, where they bind to specific receptors in the membrane on the postsynaptic side of the synapse.[3]		Most neurotransmitters are about the size of a single amino acid, however, some neurotransmitters may be the size of larger proteins or peptides. A released neurotransmitter is typically available in the synaptic cleft for a short time before it is metabolized by enzymes, pulled back into the presynaptic neuron through reuptake, or bound to a postsynaptic receptor. Nevertheless, short-term exposure of the receptor to a neurotransmitter is typically sufficient for causing a postsynaptic response by way of synaptic transmission.		In response to a threshold action potential or graded electrical potential, a neurotransmitter is released at the presynaptic terminal. Low level "baseline" release also occurs without electrical stimulation. The released neurotransmitter may then move across the synapse to be detected by and bind with receptors in the postsynaptic neuron. Binding of neurotransmitters may influence the postsynaptic neuron in either an inhibitory or excitatory way. This neuron may be connected to many more neurons, and if the total of excitatory influences are greater than those of inhibitory influences, the neuron will also "fire". Ultimately it will create a new action potential at its axon hillock to release neurotransmitters and pass on the information to yet another neighboring neuron.[4]		Until the early 20th century, scientists assumed that the majority of synaptic communication in the brain was electrical. However, through the careful histological examinations by Ramón y Cajal (1852–1934), a 20 to 40 nm gap between neurons, known today as the synaptic cleft, was discovered. The presence of such a gap suggested communication via chemical messengers traversing the synaptic cleft, and in 1921 German pharmacologist Otto Loewi (1873–1961) confirmed that neurons can communicate by releasing chemicals. Through a series of experiments involving the vagus nerves of frogs, Loewi was able to manually slow the heart rate of frogs by controlling the amount of saline solution present around the vagus nerve. Upon completion of this experiment, Loewi asserted that sympathetic regulation of cardiac function can be mediated through changes in chemical concentrations. Furthermore, Otto Loewi is credited with discovering acetylcholine (ACh)—the first known neurotransmitter.[5] Some neurons do, however, communicate via electrical synapses through the use of gap junctions, which allow specific ions to pass directly from one cell to another.[6]		There are four main criteria for identifying neurotransmitters:		However, given advances in pharmacology, genetics, and chemical neuroanatomy, the term "neurotransmitter" can be applied to chemicals that:		The anatomical localization of neurotransmitters is typically determined using immunocytochemical techniques, which identify either the location of either the transmitter substances themselves, or of the enzymes that are involved in their synthesis. Immunocytochemical techniques have also revealed that many transmitters, particularly the neuropeptides, are co-localized, that is, one neuron may release more than one transmitter from its synaptic terminal.[7] Various techniques and experiments such as staining, stimulating, and collecting can be used to identify neurotransmitters throughout the central nervous system.[8]		There are many different ways to classify neurotransmitters. Dividing them into amino acids, peptides, and monoamines is sufficient for some classification purposes.		Major neurotransmitters:		In addition, over 50 neuroactive peptides have been found, and new ones are discovered regularly. Many of these are "co-released" along with a small-molecule transmitter. Nevertheless, in some cases a peptide is the primary transmitter at a synapse. β-endorphin is a relatively well-known example of a peptide neurotransmitter because it engages in highly specific interactions with opioid receptors in the central nervous system.		Single ions (such as synaptically released zinc) are also considered neurotransmitters by some,[10] as well as some gaseous molecules such as nitric oxide (NO), carbon monoxide (CO), and hydrogen sulfide (H2S).[11] The gases are produced in the neural cytoplasm and are immediately diffused through the cell membrane into the extracellular fluid and into nearby cells to stimulate production of second messengers. Soluble gas neurotransmitters are difficult to study, as they act rapidly and are immediately broken down, existing for only a few seconds.		The most prevalent transmitter is glutamate, which is excitatory at well over 90% of the synapses in the human brain.[4] The next most prevalent is Gamma-Aminobutyric Acid, or GABA, which is inhibitory at more than 90% of the synapses that do not use glutamate. Although other transmitters are used in fewer synapses, they may be very important functionally: the great majority of psychoactive drugs exert their effects by altering the actions of some neurotransmitter systems, often acting through transmitters other than glutamate or GABA. Addictive drugs such as cocaine and amphetamines exert their effects primarily on the dopamine system. The addictive opiate drugs exert their effects primarily as functional analogs of opioid peptides, which, in turn, regulate dopamine levels.		Neurons form elaborate networks through which nerve impulses—action potentials—travel. Each neuron has as many as 15,000 connections with neighboring neurons.		Neurons do not touch each other (except in the case of an electrical synapse through a gap junction); instead, neurons interact at contact points called synapses: a junction within two nerve cells, consisting of a miniature gap which impulses pass by a neurotransmitter. A neuron transports its information by way of a nerve impulse called an action potential. When an action potential arrives at the synapse's presynaptic terminal button, it may stimulate the release of neurotransmitters. These neurotransmitters are released into the synaptic cleft to bind onto the receptors of the postsynaptic membrane and influence another cell, either in an inhibitory or excitatory way. The next neuron may be connected to many more neurons, and if the total of excitatory influences minus inhibitory influences is great enough, it will also "fire". That is to say, it will create a new action potential at its axon hillock, releasing neurotransmitters and passing on the information to yet another neighboring neuron.		A neurotransmitter can influence the function of a neuron through a remarkable number of mechanisms. In its direct actions in influencing a neuron’s electrical excitability, however, a neurotransmitter acts in only one of two ways: excitatory or inhibitory. A neurotransmitter influences trans-membrane ion flow either to increase (excitatory) or to decrease (inhibitory) the probability that the cell with which it comes in contact will produce an action potential. Thus, despite the wide variety of synapses, they all convey messages of only these two types, and they are labeled as such. Type I synapses are excitatory in their actions, whereas type II synapses are inhibitory. Each type has a different appearance and is located on different parts of the neurons under its influence. Each neuron receives thousands of excitatory and inhibitory signals every second[citation needed].		Type I (excitatory) synapses are typically located on the shafts or the spines of dendrites, whereas type II (inhibitory) synapses are typically located on a cell body. In addition, Type I synapses have round synaptic vesicles, whereas the vesicles of type II synapses are flattened. The material on the presynaptic and post-synaptic membranes is denser in a Type I synapse than it is in a type II, and the type I synaptic cleft is wider. Finally, the active zone on a Type I synapse is larger than that on a Type II synapse.		The different locations of type I and type II synapses divide a neuron into two zones: an excitatory dendritic tree and an inhibitory cell body. From an inhibitory perspective, excitation comes in over the dendrites and spreads to the axon hillock to trigger an action potential. If the message is to be stopped, it is best stopped by applying inhibition on the cell body, close to the axon hillock where the action potential originates. Another way to conceptualize excitatory–inhibitory interaction is to picture excitation overcoming inhibition. If the cell body is normally in an inhibited state, the only way to generate an action potential at the axon hillock is to reduce the cell body’s inhibition. In this "open the gates" strategy, the excitatory message is like a racehorse ready to run down the track, but first the inhibitory starting gate must be removed.[15]		As explained above, the only direct action of a neurotransmitter is to activate a receptor. Therefore, the effects of a neurotransmitter system depend on the connections of the neurons that use the transmitter, and the chemical properties of the receptors that the transmitter binds to.		Here are a few examples of important neurotransmitter actions:		Neurons expressing certain types of neurotransmitters sometimes form distinct systems, where activation of the system affects large volumes of the brain, called volume transmission. Major neurotransmitter systems include the noradrenaline (norepinephrine) system, the dopamine system, the serotonin system, and the cholinergic system, among others. It should be noted that trace amines, primarily via TAAR1 activation, have a very significant effect on neurotransmission in monoamine pathways (i.e., dopamine, histamine, norepinephrine, and serotonin pathways) throughout the brain.[22][23] A brief comparison of these systems follows:		Caudal nuclei (CN): Raphe magnus, raphe pallidus, and raphe obscurus		Rostral nuclei (RN): Nucleus linearis, dorsal raphe, medial raphe, and raphe pontis		Forebrain cholinergic nuclei (FCN): Nucleus basalis of Meynert, medial septal nucleus, and diagonal band		Brainstem cholinergic nuclei (BCN): Pedunculopontine nucleus, laterodorsal tegmentum, medial habenula, and parabigeminal nucleus		Understanding the effects of drugs on neurotransmitters comprises a significant portion of research initiatives in the field of neuroscience. Most neuroscientists involved in this field of research believe that such efforts may further advance our understanding of the circuits responsible for various neurological diseases and disorders, as well as ways to effectively treat and someday possibly prevent or cure such illnesses.[34]		Drugs can influence behavior by altering neurotransmitter activity. For instance, drugs can decrease the rate of synthesis of neurotransmitters by affecting the synthetic enzyme(s) for that neurotransmitter. When neurotransmitter syntheses are blocked, the amount of neurotransmitters available for release becomes substantially lower, resulting in a decrease in neurotransmitter activity. Some drugs block or stimulate the release of specific neurotransmitters. Alternatively, drugs can prevent neurotransmitter storage in synaptic vesicles by causing the synaptic vesicle membranes to leak. Drugs that prevent a neurotransmitter from binding to its receptor are called receptor antagonists. For example, drugs used to treat patients with schizophrenia such as haloperidol, chlorpromazine, and clozapine are antagonists at receptors in the brain for dopamine. Other drugs act by binding to a receptor and mimicking the normal neurotransmitter. Such drugs are called receptor agonists. An example of a receptor agonist is Valium, a benzodiazepine that mimics effects of the endogenous neurotransmitter gamma-aminobutyric acid (GABA) to decrease anxiety. Other drugs interfere with the deactivation of a neurotransmitter after it has been released, thereby prolonging the action of a neurotransmitter. This can be accomplished by blocking re-uptake or inhibiting degradative enzymes. Lastly, drugs can also prevent an action potential from occurring, blocking neuronal activity throughout the central and peripheral nervous system. Drugs such as tetrodotoxin that block neural activity are typically lethal.		Drugs targeting the neurotransmitter of major systems affect the whole system, which can explain the complexity of action of some drugs. Cocaine, for example, blocks the re-uptake of dopamine back into the presynaptic neuron, leaving the neurotransmitter molecules in the synaptic gap for an extended period of time. Since the dopamine remains in the synapse longer, the neurotransmitter continues to bind to the receptors on the postsynaptic neuron, eliciting a pleasurable emotional response. Physical addiction to cocaine may result from prolonged exposure to excess dopamine in the synapses, which leads to the downregulation of some post-synaptic receptors. After the effects of the drug wear off, an individual can become depressed due to decreased probability of the neurotransmitter binding to a receptor. Fluoxetine is a selective serotonin re-uptake inhibitor (SSRI), which blocks re-uptake of serotonin by the presynaptic cell which increases the amount of serotonin present at the synapse and furthermore allows it to remain there longer, providing potential for the effect of naturally released serotonin.[35] AMPT prevents the conversion of tyrosine to L-DOPA, the precursor to dopamine; reserpine prevents dopamine storage within vesicles; and deprenyl inhibits monoamine oxidase (MAO)-B and thus increases dopamine levels.		An agonist is a chemical capable of binding to a receptor, such as a neurotransmitter receptor, and initiating the same reaction typically produced by the binding of the endogenous substance.[36] An agonist of a neurotransmitter will thus initiate the same receptor response as the transmitter. In neurons, an agonist drug may activate neurotransmitter receptors either directly or indirectly. Direct-binding agonists can be further characterized as full agonists, partial agonists, inverse agonists.[citation needed]		Direct agonists act similar to a neurotransmitter by binding directly to its associated receptor site(s), which may be located on the presynaptic neuron or postsynaptic neuron, or both.[37] Typically, neurotransmitter receptors are located on the postsynaptic neuron, while neurotransmitter autoreceptors are located on the presynaptic neuron, as is the case for monoamine neurotransmitters;[22] in some cases, a neurotransmitter utilizes retrograde neurotransmission, a type of feedback signaling in neurons where the neurotransmitter is released postsynaptically and binds to target receptors located on the presynaptic neuron.[38][note 1] Nicotine, a compound found in tobacco, is a direct agonist of most nicotinic acetylcholine receptors, mainly located in cholinergic neurons.[39] Opiates, such as morphine, heroin, hydrocodone, oxycodone, codeine, and methadone, are μ-opioid receptor agonists; this action mediates their euphoriant and pain relieving properties.[39]		Indirect agonists increase the binding of neurotransmitters at their target receptors by stimulating the release or preventing the reuptake of neurotransmitters.[37] Some indirect agonists trigger neurotransmitter release and prevent neurotransmitter reuptake. Amphetamine, for example, is an indirect agonist of postsynaptic dopamine, norepinephrine, and serotonin receptors in each their respective neurons;[22][23] it produces both neurotransmitter release into the presynaptic neuron and subsequently the synaptic cleft and prevents their reuptake from the synaptic cleft by activating TAAR1, a presynaptic G protein-coupled receptor, and binding to a site on VMAT2, a type of monoamine transporter located on synaptic vesicles within monoamine neurons.[22][23]		An antagonist is a chemical that acts within the body to reduce the physiological activity of another chemical substance (as an opiate); especially one that opposes the action on the nervous system of a drug or a substance occurring naturally in the body by combining with and blocking its nervous receptor.[40]		There are two main types of antagonist: direct-acting Antagonist and indirect-acting Antagonists:		An antagonist drug is one that attaches (or binds) to a site called a receptor without activating that receptor to produce a biological response. It is therefore said to have no intrinsic activity. An antagonist may also be called a receptor "blocker" because they block the effect of an agonist at the site. The pharmacological effects of an antagonist therefore result in preventing the corresponding receptor site's agonists (e.g., drugs, hormones, neurotransmitters) from binding to and activating it. Antagonists may be "competitive" or "irreversible".		A competitive antagonist competes with an agonist for binding to the receptor. As the concentration of antagonist increases, the binding of the agonist is progressively inhibited, resulting in a decrease in the physiological response. High concentration of an antagonist can completely inhibit the response. This inhibition can be reversed, however, by an increase of the concentration of the agonist, since the agonist and antagonist compete for binding to the receptor. Competitive antagonists, therefore, can be characterized as shifting the dose-response relationship for the agonist to the right. In the presence of a competitive antagonist, it takes an increased concentration of the agonist to produce the same response observed in the absence of the antagonist.		An irreversible antagonist binds so strongly to the receptor as to render the receptor unavailable for binding to the agonist. Irreversible antagonists may even form covalent chemical bonds with the receptor. In either case, if the concentration of the irreversible antagonist is high enough, the number of unbound receptors remaining for agonist binding may be so low that even high concentrations of the agonist do not produce the maximum biological response.[41]		While intake of neurotransmitter precursors does increase neurotransmitter synthesis, evidence is mixed as to whether neurotransmitter release and postsynaptic receptor firing is increased. Even with increased neurotransmitter release, it is unclear whether this will result in a long-term increase in neurotransmitter signal strength, since the nervous system can adapt to changes such as increased neurotransmitter synthesis and may therefore maintain constant firing.[45] Some neurotransmitters may have a role in depression and there is some evidence to suggest that intake of precursors of these neurotransmitters may be useful in the treatment of mild and moderate depression.[45][46]		L-DOPA, a precursor of dopamine that crosses the blood–brain barrier, is used in the treatment of Parkinson's disease. For depressed patients where low activity of the neurotransmitter norepinephrine is implicated, there is only little evidence for benefit of neurotransmitter precursor administration. L-phenylalanine and L-tyrosine are both precursors for dopamine, norepinephrine, and epinephrine. These conversions require vitamin B6, vitamin C, and S-adenosylmethionine. A few studies suggest potential antidepressant effects of L-phenylalanine and L-tyrosine, but there is much room for further research in this area.[45]		Administration of L-tryptophan, a precursor for serotonin, is seen to double the production of serotonin in the brain. It is significantly more effective than a placebo in the treatment of mild and moderate depression.[45] This conversion requires vitamin C.[21] 5-hydroxytryptophan (5-HTP), also a precursor for serotonin, is more effective than a placebo.[45]		Diseases and disorders may also affect specific neurotransmitter systems. For example, problems in producing dopamine can result in Parkinson's disease, a disorder that affects a person's ability to move as they want to, resulting in stiffness, tremors or shaking, and other symptoms. Some studies suggest that having too little or too much dopamine or problems using dopamine in the thinking and feeling regions of the brain may play a role in disorders like schizophrenia or attention deficit hyperactivity disorder (ADHD). Similarly, after some research suggested that drugs that block the recycling, or reuptake, of serotonin seemed to help some people diagnosed with depression, it was theorized that people with depression might have lower-than-normal serotonin levels. Though widely popularized, this theory was not borne out in subsequent research.[47] Furthermore, problems with producing or using glutamate have been suggestively and tentatively linked to many mental disorders, including autism, obsessive compulsive disorder (OCD), schizophrenia, and depression.[48]		Generally, there are no scientifically established "norms" for appropriate levels or "balances" of different neurotransmitters. It is in most cases pragmatically impossible to even measure levels of neurotransmitters in a brain or body at any distinct moments in time. Neurotransmitters regulate each other's release, and weak consistent imbalances in this mutual regulation were linked to temperament in healthy people .[49][50][51][52][53] Strong imbalances or disruptions to neurotransmitter systems have been associated with many diseases and mental disorders. These include Parkinson's, depression, insomnia, Attention Deficit Hyperactivity Disorder (ADHD), anxiety, memory loss, dramatic changes in weight and addictions. Chronic physical or emotional stress can be a contributor to neurotransmitter system changes. Genetics also plays a role in neurotransmitter activities. Apart from recreational use, medications that directly and indirectly interact one or more transmitter or its receptor are commonly prescribed for psychiatric and psychological issues. Notably, drugs interacting with serotonin and norepinephrine are prescribed to patients with problems such as depression and anxiety—though the notion that there is much solid medical evidence to support such interventions has been widely criticized.[54]		A neurotransmitter must be broken down once it reaches the post-synaptic cell to prevent further excitatory or inhibitory signal transduction. This allows new signals to be produced from the adjacent nerve cells. When the neurotransmitter has been secreted into the synaptic cleft, it binds to specific receptors on the postsynaptic cell, thereby generating a postsynaptic electrical signal. The transmitter must then be removed rapidly to enable the postsynaptic cell to engage in another cycle of neurotransmitter release, binding, and signal generation. Neurotransmitters are terminated in three different ways:		For example, choline is taken up and recycled by the pre-synaptic neuron to synthesize more ACh. Other neurotransmitters such as dopamine are able to diffuse away from their targeted synaptic junctions and are eliminated from the body via the kidneys, or destroyed in the liver. Each neurotransmitter has very specific degradation pathways at regulatory points, which may be targeted by the body's regulatory system or by recreational drugs.		
Swimming is an individual or team sport that involves using arms and legs to move the body through water. Typically, the sport takes place in pools or in open-water (e.g., in a sea or lake). Competitive swimming is one of the most popular Olympic sports,[1] with events in butterfly, backstroke, breaststroke, freestyle, and individual medley. In addition to these individual events, swimmers also take part in relays. Swimming each stroke requires specific techniques, and in competition, there are specific regulations concerning the acceptable form for different strokes.[2] There are also rules put in place to regulate what types of swimsuits are allowed at competitions. Although it is possible for competitive swimmers to incur several injuries from the sport, there are also multiple health benefits associated with the sport.						Evidence of recreational swimming in prehistoric times has been found, with the earliest evidence dating to Stone Age paintings from around 10000 years ago. Written references date from 2000 BC, with some of the earliest references to swimming including the Iliad, the Odyssey, the Bible, Beowulf, the Quran and others. In 1538, Nikolaus Wynmann, a German professor of languages, wrote the first swimming book, The Swimmer or A Dialogue on the Art of Swimming (Der Schwimmer oder ein Zweigespräch über die Schwimmkunst).		Swimming emerged as a competitive recreational activity in the 1830s in England. In 1828, the first indoor swimming pool, St George's Baths was opened to the public.[3] By 1837, the National Swimming Society was holding regular swimming competitions in six artificial swimming pools, built around London. The recreational activity grew in popularity and by 1880, when the first national governing body, the Amateur Swimming Association, was formed, there were already over 300 regional clubs in operation across the country.[4]		In 1844 two Native American participants at a swimming competition in London introduced the front crawl to a European audience. Sir John Arthur Trudgen picked up the hand-over stroke from some South American natives and successfully debuted the new stroke in 1873, winning a local competition in England. His stroke is still regarded as the most powerful to use today.[5]		Captain Matthew Webb was the first man to swim the English Channel (between England and France), in 1875. Using the breaststroke technique, he swam the channel 21.26 miles (34.21 km) in 21 hours and 45 minutes. His feat was not replicated or surpassed for the next 36 years, until T.W. Burgess made the crossing in 1911.		Other European countries also established swimming federations; Germany in 1882, France in 1890 and Hungary in 1896. The first European amateur swimming competitions were in 1889 in Vienna. The world's first women's swimming championship was held in Scotland in 1892.[6]		Men's swimming became part of the first modern Olympic Games in 1896 in Athens. In 1902, the Australian Richmond Cavill introduced the front crawl to the Western world. In 1908, the world swimming association, Fédération Internationale de Natation (FINA), was formed. Women's swimming was introduced into the Olympics in 1912; the first international tournament for women outside the Olympics was the 1922 Women's Olympiad. Butterfly was developed in the 1930s and was at first a variant of breaststroke, until it was accepted as a separate style in 1952.		Competitive swimming became popular in the 19th century. The goal of competitive swimming is to break personal or world records while beating competitors in any given event. Swimming in competition should create the least resistance in order to obtain maximum speed. However, some professional swimmers who do not hold a national or world ranking are considered the best in regard to their technical skills. Typically, an athlete goes through a cycle of training in which the body is overloaded with work in the beginning and middle segments of the cycle, and then the workload is decreased in the final stage as the swimmer approaches competition.		The practice of reducing exercise in the days just before an important competition is called tapering. A final stage is often referred to as "shave and taper": the swimmer shaves off all exposed hair for the sake of reducing drag and having a sleeker and more hydrodynamic feel in the water.[7] Additionally, the "shave and taper" method refers to the removal of the top layer of "dead skin", which exposes the newer and richer skin underneath.[8]		Swimming is an event at the Summer Olympic Games, where male and female athletes compete in 16 of the recognized events each. Olympic events are held in a 50-meter pool, called a long course pool.		There are forty officially recognized individual swimming events in the pool; however the International Olympic Committee only recognizes 32 of them. The international governing body for competitive swimming is the Fédération Internationale de Natation ("International Swimming Federation"), better known as FINA.		In open water swimming, where the events are swum in a body of open water (lake or sea), there are also 5 km, 10 km and 25 km events for men and women. However, only the 10 km event is included in the Olympic schedule, again for both men and women. Open-water competitions are typically separate to other swimming competitions with the exception of the World Championships and the Olympics.		In competitive swimming, four major styles have been established. These have been relatively stable over the last 30–40 years with minor improvements. They are:		In competition, only one of these styles may be used except in the case of the individual medley, or IM, which consists of all four. In this latter event, swimmers swim equal distances of butterfly, then backstroke, breaststroke, and finally, freestyle.[9] In Olympic competition, this event is swum in two distances – 200 and 400 meters. Some short course competitions also include the 100-yard or 100-meter IM – particularly, for younger swimmers (typically under 14 years) involved in club swimming, or masters swimming (over 18).		Since the 1990s, the most drastic change in swimming has been the addition of the underwater dolphin kick. This is used to maximize the speed at the start and after the turns in all styles. The first successful use of it was by David Berkoff. At the 1988 Olympics, he swam most of the 100 m backstroke race underwater and broke the world record on the distance during the preliminaries. Another swimmer to use the technique was Denis Pankratov at the 1996 Olympics in Atlanta, where he completed almost half of the 100 m butterfly underwater to win the gold medal. In the past decade, American competitive swimmers have shown the most use of the underwater dolphin kick to gain advantage, most notably Olympic and World medal winners Michael Phelps and Ryan Lochte; however currently swimmers are not allowed to go any further than fifteen metres underwater due to rule changes by FINA.[10]		While the dolphin kick is mostly seen in middle-distance freestyle events and in all distances of backstroke and butterfly, it is not usually used to the same effect in freestyle sprinting. That changed with the addition of the so-called "technical" suits around the European Short Course Championships in Rijeka, Croatia in December 2008. There, Amaury Leveaux set new world records of 44.94 seconds in the 100 m freestyle, 20.48 seconds in the 50 m freestyle and 22.18 in the 50 m butterfly. Unlike the rest of the competitors in these events, he spent at least half of each race submerged using the dolphin kick.[11]		World Championship pools must be 50 metres (160 ft) (long course) long and 25 metres (82 ft) wide, with ten lanes labelled zero to nine (or one to ten in some pools; zero and nine (or one and ten) are usually left empty in semi-finals and finals); the lanes must be at least 2.5 metres (8.2 ft) wide. They will be equipped with starting blocks at both ends of the pool and most will have Automatic Officiating Equipment, including touch pads to record times and sensors to ensure the legality of relay take overs. The pool must have a minimum depth of two metres.[12]		Other pools which host events under FINA regulations are required to meet some but not all of these requirements. Many of these pools have eight instead of ten lanes and some will be 25 metres (82 ft) long, making them Short course. World records that are set in short course pools are kept separate from those set in long course pools because it may be an advantage or disadvantage to swimmers to have more or less turns in a race.		Competitive swimming, from the club through to international level, tends to have an autumn and winter season competing in short course (25 metre or yard) pools and a spring and summer season competing in long course (50 metre) pools and in open water.		In international competition and in club swimming in Europe, the short course (25m) season lasts from September to December, and the long course (50m) season from January to August with open water in the summer months.		In club, school, and college swimming in the United States, the short course (25 yard) season is much longer, from September to March. The long-course season takes place in 50-meter pools and lasts from April to the end of August with open water in the summer months.		In club swimming in Australasia, the short course (25m) season lasts from April to September, and the long course (50m) season from October to March with open water in the summer months.		Outside the United States, meters is the standard in both short and long course swimming, with the same distances swum in all events. In the American short course season, the 500 yard, 1000 yard, and 1650-yard freestyle events are swum as a yard is much shorter than a meter (100 yards equals 91.44 meters), while during the American long course season the 400 meter, 800 meter, and 1500-meter freestyle events are swum instead.		Beginning each swimming season racing in short course allows for shorter distance races for novice swimmers. For example, in the short course season if a swimmer wanted to compete in a stroke they had just learned, a 25-yard/meter race is available to them, opposed to the long course season when they would need to be able to swim at least 50 meters of that new stroke in order to compete.		There are several types of officials,[13] which are needed to manage the competition.[14]		Referee: The referee has full control and authority over all officials. The referee will enforce all rules and decisions of FINA and shall decide all questions relating to the actual conduct of the meet, and event or the competition, the final settlement of which is not otherwise covered by the rules. The referee takes overall responsibility for running the race and makes the final decisions as to who wins the competition. Referees call swimmers to the blocks with short blasts of his or her whistle. This is the signal for the swimmers to stand next to their blocks. Then the referee will blow a long whistle that will tell the swimmers to step on the block. For backstroke events, the long whistle is the signal for the swimmers to step in the water. The referee will then blow another long whistle, signalling the swimmers to grab the gutter or the provided block handle. The referee will then hand over the rest to the starter by directing his or her hand to the starter.		Starter: The starter has full control of the swimmers from the time the referee turns the swimmers over to him/her until the race commences. A starter begins the race by saying, "Take your mark." At this point, the swimmers will get into stationary positions in which they would like to start their race. After all swimmers have assumed their stationary position, the starter will push a button on the starting system, signaling the start of a race with a loud noise (usually a beep or a horn) and flash from a strobe light. A starter sends the swimmers off the blocks and may call a false start if a swimmer leaves the block before the starter sends them. A starter may also choose to recall the race after the start for any reason or request the swimmers to "stand" or "relax" (for backstroke/backcrawl events only) if he or she believes that (a) particular swimmer(s) has gotten an unfair advantage at the start.		Clerk of course: The clerk of course (also called the "bullpen") assembles swimmers prior to each event, and is responsible for organizing ("seeding") swimmers into heats based on their times. Heats are generally seeded from slowest to fastest, where swimmers with no previous time for an event are assumed to be the slowest. The clerk of the course is also responsible for recording and reporting swimmers who have chosen to "scratch" (not swim) their events after they have signed up or qualified to a semifinal or final. The clerk is also responsible for enforcing rules of the swim meet if a swimmer chooses to not show up ("No show" - NS) his or her events.		Timekeepers: Each timekeeper takes the time of the swimmers in the lane assigned to him/her. Unless a video backup system is used, it may be necessary to use the full complement of timekeepers even when automatic officiating equipment is used. A chief timekeeper assigns the seating positions for all timekeepers and the lanes for which they are responsible. In most competitions there will be one or more timekeepers per lane. In international competitions where full automatic timing and video placing equipment is in use timekeepers may not be required.		Inspectors of turns: One inspector of turns is assigned to one or more lanes at each end of the pool. Each inspector of turns ensures that swimmers comply with the relevant rules for turning as well as the relevant rules for start and finish of the race. Inspectors of turns shall report any violation on disqualification reports detailing the event, lane number, and the infringement delivered to the chief inspector of turns who will immediately convey the report to the referee.		Judges of Stroke: Judges of stroke are located on each side of the pool. They follow the swimmers during their swim back and forth across the pool. They ensure that the rules related to the style of swimming designated for the event are being observed, and observe the turns and the finishes to assist the inspectors of turns.		Finish judges: Finish judges determine the order of finish and make sure the swimmers finish in accordance with the rules (two hands simultaneously for breaststroke and butterfly, on the back for backstroke, etc.)		If an official observes a swimmer breaking a rule concerning the stroke he or she is swimming, the official will report what they have seen to the referee. The referee can disqualify (or DQ) any swimmer for any violation of the rules that he/she personally observes or for any violation reported to them by other authorised officials. All disqualifications are subject to the decision and discretion of the referee.		Those who are disqualified may choose to protest their disqualification . Protests are reviewed by a panel of officials instead of the deck referee or stroke judges who may have made the initial disqualification report.		Brands such as Arena, Speedo, Nike, and Adidas are popular regular swimwear brands. The most durable material for regular swimming is Polyester. The main difference between competition and regular swimwear is that competition swimwear is tighter and compresses the muscles of the swimmers. Regular swimwear is easier to put on and more comfortable for leisure activities.		Men's most used practice swimwear include briefs and jammers. Males generally swim barechested.		There was much controversy after the Beijing Olympic Games in 2008, when many Olympic swimmers broke records an unprecedented number of times using revolutionary swimsuits. To highlight the issue, in 2008, 70 world records were broken in one year, and 66 Olympic records were broken in one Olympic Games (there were races in Beijing where the first five finishers were swimming faster than the old world record).		As of January 1, 2010, men are only allowed to wear suits from the waist to above the knees.[19] They are also only permitted to wear one piece of swimwear; they cannot wear briefs underneath jammers. This rule was enacted after the controversy in the Beijing Olympics and Rome World Championships.		Women wear one-piece suits with different backs for competition, though two-piece suits can also be worn. Backs vary mainly in strap thickness and geometric design. Most common styles include: racerback, axel back, corset, diamondback, and butterfly-back/Fly-Back. There are also different style lengths: three-quarter length (reaches the knees), regular length (shoulders to hips), and bikini style (two-piece). Also as of January 1, 2010, in competition, women are only allowed to wear suits that do not go past the knees or shoulders.		Drag suits are used to increase water resistance against the swimmer to help them train for competitions. Other forms of drag wear include nylons, old suits, and T-shirts: articles that increase friction in the water to build strength during training, and thus increase speed once drag items are removed for competition.		Some swimmers also shave areas of exposed skin before end-of-season competitions to reduce friction in the water. The practice gained popularity after the 1956 Olympics, when Murray Rose and Jon Henricks came shaved and won gold medals for Australia.[20] Freshly shaven skin is less resistant when in the water. In addition, a 1989 study demonstrated that shaving improves a swimmer's overall performance by reducing drag.[21]		Wearing drag suits during training also improves mental performance during competitions[citation needed]. Drag makes a swimmer feel slower and more resistant during training with the added friction. Then on the day of the competition, a shaven swimmer wearing only a fast competition suit will feel an improvement in how fast and smooth they feel in the water.		The disadvantages of using a drag suit include the depletion of proper stroke. This is caused by the swimmer's own fatigue. When the swimmer becomes more fatigued, different muscle groups become more tired. Consequently, the swimmer will try to engage another group of muscle to do the same thing, which can cause the stroke efficiency to drop.		Elite and international swimming comprises the highest level of competition available to swimmers, including competitions such as the Olympic Games and FINA World Aquatics Championships.		It is not a straightforward professional sport as almost all the money for elite and international swimming is held by national governing bodies for the sport because it is generated by amateurs' subscription fees, government grants, the Olympic Games and the World Aquatics Championships (and in each country, the national trials to qualify for the preceding). This results in a mix of fully professional, semi-professional, and amateur swimmers at this level. Fully professional swimmers will typically get a salary both from their national governing body and from outside sponsors, semi-professionals a small stipend from their national governing body, and amateurs receive no funding. Outside of these major championships prize money is low – the 2015 FINA World Cup series has a total prize fund of $3,000 per race shared between the top three[22] and the 2014–15 USA Grand Prix Series $1,800[23] compared to the 2015 World Aquatics Championships fund of $60,000 per race shared between the top eight.[24]		Open water swimming is swimming outside a regular pool, usually in a lake, or sometimes ocean. Popularity of the sport has grown in recent years, particularly since the 10 km open water event was added as an Olympic event in 2005, contested for the first time in the 2008 Olympic Games in Beijing.[25]		New recent technology has developed much faster swimsuits. Full body suits have been banned, but swimmers at the very top levels still wear suits that have been lasered together because stitching creates drag. The disadvantage of these suits is that they are sometimes uncomfortable and tight.		The largest Ocean Swim's in terms of numbers of participants are in Australia, with the Pier to Pub, Cole Classic and Melbourne Swim Classic all with roughly 5000 swimming participants.		Swimming times have dropped over the years due to better training techniques and to new developments.		The first four Olympics were not held in pools, but in open water (1896 – the Mediterranean, 1900 – the Seine river, 1904 – an artificial lake, 1906 – the Mediterranean). The 1904 Olympics' freestyle race was the only one ever measured at 100 yards, instead of the usual 100 meters. A 100-meter pool was built for the 1908 Olympics and sat in the center of the main stadium's track and field oval. The 1912 Olympics, held in the Stockholm harbor, marked the beginning of electronic timing.[clarification needed]		Male swimmers wore full-body suits until the 1940s, which caused more drag in the water than their modern swimwear counterparts did. Competition suits now include engineered fabric and designs to reduce swimmers' drag in the water and prevent athlete fatigue. In addition, over the years, pool designs have lessened the drag. Some design considerations allow for the reduction of swimming resistance, making the pool faster. Namely, proper pool depth, elimination of currents, increased lane width, energy absorbing racing lane lines and gutters, and the use of other innovative hydraulic, acoustic, and illumination designs. There have been major changes in starting blocks over the past years. Starting blocks used to be small, narrow and straight [26] but throughout time they have become bigger and wider and nowadays the surface of the block is angled towards the swimming pool.[27] In addition, starting blocks now have a "wedge" which is a raised, slanting platform situated at the rear of the main block. This enables the swimmer to adopt a crouched position at a 90 degrees angle and push off quicker with the rear leg to increase their launch power.[28]		The 1924 Summer Olympics were the first to use the standard 50-meter pool with marked lanes. In the freestyle, swimmers originally dove from the pool walls, but diving blocks were incorporated at the 1936 Summer Olympics. The tumble turn was developed by the 1950s and goggles were first used in the 1976 Olympics.		There were also changes in the late 20th century in terms of technique. Breaststrokers are now allowed to dip their heads completely under water, which allows for a longer stroke and faster time. However, the breaststrokers must bring their heads up at the completion of each cycle. In addition, a key hole pull in the breaststroke start and turns has been added to help speed up the stroke. There have been some other changes added recently[when?] as well. Now off the start and turns, breaststrokers are allowed one butterfly kick to help increase their speed. Backstrokers are now allowed to turn on their stomachs before the wall in order to perform a "flip-turn". Previously, they had to reach and flip backwards and a variation of it, known as a "bucket turn" or a "suicide turn", is sometimes used in individual medley events to transition from backstroke to breaststroke.		The foundation of FINA in 1908 signalled the commencement of recording the first official world records in swimming.[29] At that time records could be established in any swimming pool of length not less than 25 yards, and records were also accepted for intermediate distance split times from longer distance events. Today World Records will only be accepted when times are reported by Automatic Officiating Equipment, or Semi-Automatic Officiating Equipment in the case of Automatic Officiating Equipment system malfunction.[30]		Records in events such as 300 yd, 300 m, 1000 yd, and 1000 m freestyle, 400 m backstroke, and 400 m and 500 m breaststroke were no longer ratified from 1948. A further removal of the 500 yd and 500 m freestyle, 150 m backstroke, and 3×100 m medley relay from the record listings occurred in 1952.		In 1952, the national federations of the United States and Japan proposed at the FINA Congress the separation of records achieved in long-course and short-course pools, however it was four more years before action came into effect with Congress deciding to retain only records held in 50 m pools as the official world record listings.		By 1969 there were thirty-one events in which FINA recognised official world records – 16 for men, 15 for women – closely resembling the event schedule that was in use at the Olympic Games.		The increase in accuracy and reliability of electronic timing equipment led to the introduction of hundredths of a second to the time records from 21 August 1972.		Records in short course (25 m) pools began to be officially approved as "short course world records" from 3 March 1991. Prior to this date, times in short course (25 m) pools were not officially recognised, but were regarded a "world best time" (WBT). From 31 October 1994 times in 50 m backstroke, breaststroke, and butterfly were added to the official record listings.		FINA currently recognises world records in the following events for both men and women.[31]		Swimming is a healthy workout that can be done for a lifetime. It is a low-impact activity that has several mental and bodily health benefits, that is a recreational motion for everyone. Swimming can provide a low-impact workout. Swimming builds endurance, muscle strength, and cardiovascular fitness.[32]		The US Census Bureau reports that two and a half hours per week of aerobic physical activity such as swimming can decrease the risk of chronic illnesses. Along with this, swimming is linked to better cognitive function, lower risk of type 2 diabetes, lower risk of high blood pressure, and lower risk of stroke. People are typically able to exercise longer in water than on land without increased effort, and minimal joint or muscle pain.[33][34][35]		Due to continuous rotation and usage, the shoulder (rotator cuff) is the joint most susceptible to injury in swimmers. As opposed to a single incident, injury to the rotator cuff in swimmers is a result of repeated trauma and overuse. The joint is most prone to injury when the arm is repetitively used in a position above the horizontal. This position occurs in each of the four swimming strokes in every cycle of the arms. Of the four muscles and tendons of the rotator cuff, the injury, or tear, is most likely to occur in the tendon of the supraspinatus. Rotator cuff impingement is due to pressure on the rotator cuff from part of the scapula as the arm is raised.		The best way to prevent injury is to diagnose the issue early. Typically, poor technique and excessive use without rest are the primary causes of injury. Through communication between swimmers, coaches, parents, and medical professionals, any issue can be diagnosed prior to more serious injury. Additionally, proper warm-up and strength training exercises should be completed before any rigorous movements.		In treating a rotator cuff injury, the most important factor is time. Due to the nature of the joint being primarily stabilized by muscle and tendon, the injury must be fully healed to prevent recurrence. Returning to swimming or other demanding exercises too soon will likely result in degeneration of a tendon eventually resulting in a rupture. During the rehabilitation period, focus should be placed on rotator cuff and scapular strengthening.[36]		
Well-being, wellbeing, or wellness is a general term for the condition of an individual or group. A high level of well-being means in some sense the individual or group's condition is positive.						According to Naci and Ioannidis,		Wellness refers to diverse and interconnected dimensions of physical, mental, and social well-being that extend beyond the traditional definition of health. It includes choices and activities aimed at achieving physical vitality, mental alacrity, social satisfaction, a sense of accomplishment, and personal fulfillment.[1]		Three subdisciplines in psychology are critical for the study of psychological well-being:[2]		There are two approaches typically taken to understand psychological well-being:		According to Guttman and Levy (1982) well-being is “...a special case of attitude”.[4] This approach serves two purposes in the study of well-being: "developing and testing a [systematic] theory for the structure of [interrelationships] among varieties of well-being, and integration of well-being theory with the ongoing[when?] cumulative theory [clarification needed] development in the fields of attitude of related research”.[4]		Diener's tripartite model of subjective well-being is one of the most comprehensive models of well-being in psychology. It was synthesized by Diener in 1984, positing "three distinct but often related components of wellbeing: frequent positive affect, infrequent negative affect, and cognitive evaluations such as life satisfaction."[5]		Cognitive, affective and contextual factors contribute to subjective well-being.[6] According to Diener and Suh, subjective well-being is "...based on the idea that how each person thinks and feels about his or her life is important."[7]		Carol Ryff's multidimensional model of psychological well-being postlated six factors which are key for well-being:[web 1]		According to Corey Keyes, who collaborated with Carol Ryff, mental well-being has three components, namely emotional or subjective well-being (also called hedonic well-being),[8] psychological well-being, and social well-being (together also called eudaimonic well-being).[9] emotional well-being concerns subjective aspects of well-being, in concreto, feeling well, whereas psychological and social well-being concerns skills, abilities, and psychological and social functioning.[10]		Keyes model of mental well-being has received extensive empirical support across cultures.[10][8][11][12]		Well-being is a central concept in positive psychology. Positive psychology is concerned with eudaimonia, "the good life", reflection about what holds the greatest value in life – the factors that contribute the most to a well-lived and fulfilling life. While not attempting a strict definition of the good life, positive psychologists agree that one must live a happy, engaged, and meaningful life in order to experience "the good life". Martin Seligman referred to "the good life" as "using your signature strengths every day to produce authentic happiness and abundant gratification".[13]		In Authentic Happiness (2002) Seligman proposed three kinds of a happy life which can be investigated:[14][15]		These categories appear neither widely disputed nor adopted by researchers across the 12 years that this academic area has been in existence.		In Flourish (2011) Seligman argued that the last category, "meaningful life", can be considered as 3 different categories. The resulting acronym is PERMA: Positive Emotions, Engagement, Relationships, Meaning and purpose, and Accomplishments. It is a mnemonic for the five elements of Martin Seligman's well-being theory:[15][18]		Research on positive psychology, well-being, eudaimonia and happiness, and the theories of Diener, Ryff, Keyes and Seligmann covers a broad range of levels and topics, including "the biological, personal, relational, institutional, cultural, and global dimensions of life."[27]		
A fitness professional is a professional in the field of fitness and exercise, most often instruction (fitness instructor), including aerobics and yoga instructors and authors of fitness instruction books or manuals. Fitness topics may also include nutrition, weight-loss, and self-help. Fitness careers are distinguished from exercise science careers such as athletic training, however the various types of fitness certifications[1] have more and more in common: the, "distinctions...have become blurred, with more similarities than differences given the common background that all fitness professionals must possess."[2]		Fitness professionals screen participants for exercise programs, evaluate various fitness components, prescribe exercise to improve these components, and may also help people with specific or chronic conditions.[2]		Notable fitness professionals or former fitness professionals include Richard Simmons, Susan Powter, John Sitaras and Gov. Arnold Schwarzenegger (Arnold Schwarzenegger's Total Body Workout).		Certified fitness professionals must maintain up-to-date on all certifications in order to instruct at particular health clubs and gyms. Often, fitness professionals will have some education in kinesiology, anatomy, and biomechanics to aid in their fitness career.		In Canada, Canadian Fitness Education Services (CFES) provides national fitness leadership program modules to take candidates through the steps in Aquafit, Group Fitness and/or Weight Training Instructor and Personal Trainer national certification.		Personal training, Athletic training, and physical therapy are all technically distinct specialties with different processes and requirements for certification.[3] In the United States the main certifying agency for personal trainers is ACSM (the American College of Sports Medicine),[4] while the main certifying agency for athletic trainers is NATA (the National Athletic Trainers' Association). Obtaining certification or licensure as a physical therapist requires that you attend and graduate from a masters or doctoral program in physical therapy.[5]		
U.S. Department of Defense		The United States Navy (USN) is the naval warfare service branch of the United States Armed Forces and one of the seven uniformed services of the United States. The U.S. Navy is the largest, most capable navy in the world,[5][6][7] with the highest combined battle fleet tonnage.[8][9] The U.S. Navy has the world's largest aircraft carrier fleet, with eleven in service, two in the reserve fleet,[10] and two new carriers under construction.[11] The service has 322,421 personnel on active duty and 107,577 in the Navy Reserve. It has 276 deployable combat vessels and more than 3,700 operational aircraft as of June 2017[update].[2]		The U.S. Navy traces its origins to the Continental Navy, which was established during the American Revolutionary War and was effectively disbanded as a separate entity shortly thereafter. It played a major role in the American Civil War by blockading the Confederacy and seizing control of its rivers. It played the central role in the World War II defeat of Imperial Japan. The 21st century U.S. Navy maintains a sizable global presence, deploying in strength in such areas as the Western Pacific, the Mediterranean, and the Indian Ocean. It is a blue-water navy with the ability to project force onto the littoral regions of the world, engage in forward areas[clarification needed] during peacetime, and rapidly respond to regional crises, making it a frequent actor in U.S. foreign and military policy.		The Navy is administratively managed by the Department of the Navy, which is headed by the civilian Secretary of the Navy. The Department of the Navy is itself a division of the Department of Defense, which is headed by the Secretary of Defense. The Chief of Naval Operations (CNO) is a four-star admiral and the senior naval officer of the Department of the Navy.[12]						The mission of the Navy is to maintain, train and equip combat-ready Naval forces capable of winning wars, deterring aggression and maintaining freedom of the seas.		The U.S. Navy is a seaborne branch of the military of the United States. The Navy's three primary areas of responsibility:[14]		U.S. Navy training manuals state that the mission of the U.S. Armed Forces is "to prepare and conduct prompt and sustained combat operations in support of the national interest. "As part of that establishment, the U.S. Navy's functions comprise sea control, power projection and nuclear deterrence, in addition to "sealift" duties.[15]		It follows then as certain as that night succeeds the day, that without a decisive naval force we can do nothing definitive, and with it, everything honorable and glorious.		The Navy was rooted in the colonial seafaring tradition, which produced a large community of sailors, captains, and shipbuilders.[17] In the early stages of the American Revolutionary War, Massachusetts had its own Massachusetts Naval Militia. The establishment of a national navy was an issue of debate among the members of the Second Continental Congress. Supporters argued that a navy would protect shipping, defend the coast, and make it easier to seek out support from foreign countries. Detractors countered that challenging the British Royal Navy, then the world's preeminent naval power, was a foolish undertaking. Commander in Chief George Washington resolved the debate when he commissioned the ocean-going schooner USS Hannah to interdict British merchant ships, and reported the captures to the Congress. The U.S. Navy was officially created on 27 March 1794 — the date the 3rd U.S. Congress passed the Act to Provide a Naval Armament,[18] authorizing the purchase of six ships to defend U.S. maritime trade merchants against the Algerine corsairs. In 1972, the Chief of Naval Operations, Admiral Elmo Zumwalt, authorized the Navy to celebrate its birthday on 13 October to honor the establishment of the Continental Navy in 1775[19] — the date of the resolution of the Continental Congress that purchased two vessels to be armed against British ships.[20]		The Continental Navy achieved mixed results; it was successful in a number of engagements and raided many British merchant vessels, but it lost twenty-four of its vessels[21] and at one point was reduced to two in active service.[22] In August 1785, after the Revolutionary War drew to a close, Congress had sold Alliance, the last ship remaining in the Continental Navy due to a lack of funds to maintain the ship or support a navy.[23][24]		The United States was without a navy for nearly a decade, a state of affairs that exposed its merchant ships to a series of attacks by Barbary pirates. The sole armed maritime presence between 1790 and the launching of the U.S. Navy's first warships in 1797 was the U.S. Revenue Cutter Service (USRCS), the primary predecessor of the U.S. Coast Guard. Although the USRCS conducted operations against the pirates, their depredations far outstripped its abilities and Congress passed the Naval Act of 1794 which established a permanent standing navy. The Naval Act ordered the construction and manning of six frigates and, by October 1797;[21] three years later, the first three were brought into service: USS United States, USS Constellation, and USS Constitution. Due to his strong posture on having a strong standing Navy during this period, John Adams is "often called the father of the American Navy".[25] In 1798–99 the Navy was involved in an undeclared Quasi-War with France.[26]		The U.S. Navy saw substantial action in the War of 1812, where it was victorious in eleven single-ship duels with the Royal Navy. It drove all significant British forces off Lake Erie and Lake Champlain and prevented them from becoming British-controlled zones. The result was a major defeat for the British invasion of New York state, and the defeat of the military threat from the Native American allies of the British. Despite this, the U.S. Navy was unable to prevent the British from blockading its ports and landing troops.[27] After the war, the U.S. Navy again focused its attention on protecting American shipping assets, sending squadrons to the Caribbean, the Mediterranean, South America, Africa, and the Pacific.[21] From 1819 to the outbreak of the Civil War the Africa Squadron operated to suppress the slave trade, seizing 36 slave ships, although its contribution was smaller than that of the much larger British Royal Navy.[citation needed]		During the Mexican–American War the U.S. Navy blockaded Mexican ports, capturing or burning the Mexican fleet in the Gulf of California and capturing all major cities in Baja California peninsula. In 1846–1848 the Navy successfully used the Pacific Squadron under Commodore Robert Stockton and its marines and blue-jackets to facilitate the capture of California with large scale land operations coordinated with the local militia organized in the California Battalion. The Navy conducted the U.S. military's first large-scale amphibious joint operation by successfully landing 12,000 army troops with their equipment in one day at Veracruz, Mexico. When larger guns were needed to bombard Veracruz, Navy volunteers landed large guns and manned them in the successful bombardment and capture of the city. This successful landing and capture of Veracruz opened the way for the capture of Mexico City and the end of the war.[27] The U.S. Navy established itself as a player in United States foreign policy through the actions of Commodore Matthew Perry in Japan, which resulted in the Convention of Kanagawa in 1854.[citation needed]		Naval power played a significant role during the American Civil War, in which the Union had a distinct advantage over the Confederacy on the seas.[27] A Union blockade on all major ports shut down exports and the coastal trade, but blockade runners (mostly owned and operated by British companies) provided a thin lifeline. The brown-water Navy's control of the river systems made internal travel difficult for Confederates and easy for the Union. The war saw ironclad warships in combat for the first time at the Battle of Hampton Roads in 1862, which pitted USS Monitor against CSS Virginia.[28] For two decades after the war, however, the U.S. Navy's fleet was neglected and became technologically obsolete.[citation needed]		Our ships are our natural bulwarks.		A modernization program beginning in the 1880s when the first steel hulled warships stimulated the American steel industry, and "the new steel navy" was born.[29] This rapid expansion of the U.S. Navy and its easy victory over the Spanish Navy in 1898 brought a new respect for American technical quality. Rapid building of at first pre-dreadnoughts, then dreadnoughts brought the U.S. in line with the navies of countries such as Britain and Germany. In 1907, most of the Navy's battleships, with several support vessels, dubbed the Great White Fleet, were showcased in a 14-month circumnavigation of the world. Ordered by President Theodore Roosevelt, it was a mission designed to demonstrate the Navy's capability to extend to the global theater.[21] By 1911, the U.S. had begun building the super-dreadnoughts at a pace to eventually become competitive with Britain.[30]		The U.S. Navy saw little action during World War I. It concentrated on mine laying operations against German U-Boats. Hesitation by the senior command meant that naval forces were not contributed until late 1917. Battleship Division Nine was dispatched to Britain and served as the Sixth Battle Squadron of the British Grand Fleet. Its presence allowed the British to decommission some older ships and reuse the crews on smaller vessels. Destroyers and U.S. Naval Air Force units contributed to the anti-submarine operations. The strength of the United States Navy grew under an ambitious ship building program associated with the Naval Act of 1916.[citation needed]		Naval construction, especially of battleships was limited by the Washington Naval Conference of 1921–22. The aircraft carriers USS Saratoga (CV-3) and USS Lexington (CV-2) were built on the hulls of partially built battle cruisers that had been canceled by the treaty. The New Deal used Public Works Administration funds to build warships, such as USS Yorktown (CV-5) and USS Enterprise (CV-6). By 1936, with the completion of USS Wasp (CV-7), the U.S. Navy possessed a carrier fleet of 165,000 tonnes displacement, although this figure was nominally recorded as 135,000 tonnes to comply with treaty limitations. Franklin Roosevelt, the number two official in the Navy Department during World War I, appreciated the Navy and gave it strong support. In return, senior leaders were eager for innovation and experimented with new technologies, such as magnetic torpedoes, and developed a strategy called War Plan Orange for victory in the Pacific in a hypothetical war with Japan that would eventually become reality.[31]		The U.S. Navy grew into a formidable force in the years prior to World War II, with battleship production being restarted in 1937, commencing with USS North Carolina (BB-55). Though ultimately unsuccessful, Japan attempted to neutralize this strategic threat with the surprise attack on Pearl Harbor on 7 December 1941. Following American entry into the war, the U.S. Navy grew tremendously as the United States was faced with a two-front war on the seas. It achieved notable acclaim in the Pacific Theater, where it was instrumental to the Allies' successful "island hopping" campaign.[22] The U.S. Navy participated in many significant battles, including the Battle of the Coral Sea, the Battle of Midway, the Solomon Islands Campaign, the Battle of the Philippine Sea, the Battle of Leyte Gulf, and the Battle of Okinawa. By war's end in 1945, the U.S. Navy had added hundreds of new ships, including 18 aircraft carriers and 8 battleships, and had over 70% of the world's total numbers and total tonnage of naval vessels of 1,000 tons or greater.[32][33] At its peak, the U.S. Navy was operating 6,768 ships on V-J Day in August 1945.[34]		Doctrine had significantly shifted by the end of the war. The U.S. Navy had followed in the footsteps of the navies of Great Britain and Germany which favored concentrated groups of battleships as their main offensive naval weapons.[35] The development of the aircraft carrier and its devastating utilization by the Japanese against the U.S. at Pearl Harbor, however, shifted U.S. thinking. The Pearl Harbor attack destroyed or took out of action a significant number of U.S. Navy battleships. This placed much of the burden of retaliating against the Japanese on the small number of aircraft carriers.[36]		The potential for armed conflict with the Soviet Union during the Cold War pushed the U.S. Navy to continue its technological advancement by developing new weapons systems, ships, and aircraft. U.S. naval strategy changed to that of forward deployment in support of U.S. allies with an emphasis on carrier battle groups.[37]		The navy was a major participant in the Vietnam War, blockaded Cuba during the Cuban Missile Crisis, and, through the use of ballistic missile submarines, became an important aspect of the United States' nuclear strategic deterrence policy. The U.S. Navy conducted various combat operations in the Persian Gulf against Iran in 1987 and 1988, most notably Operation Praying Mantis. The Navy was extensively involved in Operation Urgent Fury, Operation Desert Shield, Operation Desert Storm, Operation Deliberate Force, Operation Allied Force, Operation Desert Fox and Operation Southern Watch.[citation needed]		The U.S. Navy has also been involved in search and rescue/search and salvage operations, sometimes in conjunction with vessels of other countries as well as with U.S. Coast Guard ships. Two examples are the 1966 Palomares B-52 crash incident and search for the nuclear bombs, and Task Force 71 of the Seventh Fleet operation in search for Korean Air Lines Flight 007 shot down by the Soviets on 1 September 1983.[citation needed]		When a crisis confronts the nation, the first question often asked by policymakers is: 'What naval forces are available and how fast can they be on station?'		The U.S. Navy continues to be a major support to U.S. interests in the 21st century. Since the end of the Cold War, it has shifted its focus from preparations for large-scale war with the Soviet Union to special operations and strike missions in regional conflicts.[39] The navy participated in Operation Enduring Freedom, Operation Iraqi Freedom, and is a major participant in the ongoing War on Terror, largely in this capacity. Development continues on new ships and weapons, including the Gerald R. Ford-class aircraft carrier and the Littoral combat ship. Because of its size, weapons technology, and ability to project force far from U.S. shores, the current U.S. Navy remains a potent asset for the United States. Moreover, it is the principal means through which the U.S. maintains international global order, namely by safeguarding global trade and protecting allied nations.[40]		In 2007, the U.S. Navy joined with the U.S. Marine Corps and U.S. Coast Guard to adopt a new maritime strategy called A Cooperative Strategy for 21st Century Seapower that raises the notion of prevention of war to the same philosophical level as the conduct of war. The strategy was presented by the Chief of Naval Operations, the Commandant of the Marine Corps, and Commandant of the Coast Guard at the International Sea Power Symposium in Newport, RI on 17 October 2007.[41] The strategy recognized the economic links of the global system and how any disruption due to regional crises—man-made or natural—can adversely impact the U.S. economy and quality of life. This new strategy charts a course for the Navy, Coast Guard, and Marine Corps to work collectively with each other and international partners to prevent these crises from occurring or reacting quickly should one occur to prevent negative impacts on the U.S.[citation needed]		In 2010, Chief of Naval Operations, Admiral Gary Roughead, noted that demands on the Navy have grown as the fleet has shrunk and that in the face of declining budgets in the future, the U.S. Navy must rely even more on international partnerships.[42]		In its 2013 budget request, the navy focused on retaining all eleven big deck carriers, at the expense of cutting numbers of smaller ships and delaying the SSBN replacement.[43] By the next year the USN found itself unable to maintain eleven aircraft carriers in the face of the expiration of budget relief offered by the Bipartisan Budget Act of 2013 and CNO Jonathan Greenert said that a ten ship carrier fleet would not be able to sustainably support military requirements.[44] The British First Sea Lord George Zambellas said that [45] the USN had switched from "outcome-led to resource-led" planning.[46]		One significant change in U.S. policymaking that is having a major effect on naval planning is the Pivot to East Asia. In response this, Secretary of the Navy Ray Mabus has stated that 60 percent of the total U.S. fleet will be deployed to the Pacific by the year 2020.[47] The Navy's most recent 30-year shipbuilding plan, published in 2016, calls for a future fleet of 308 ships in order to meet the challenges of an increasingly competitive international environment.[45]		U.S. Navy officers aboard the aircraft carrier USS Abraham Lincoln (CVN-72) monitor defense systems during early 2010s maritime security operations exercises.		The USS America Amphibious assault ship, launched in 2012.		U.S. Navy patrol boat near Kuwait Naval Base in 2009		The U.S. Navy falls under the administration of the Department of the Navy, under civilian leadership of the Secretary of the Navy (SECNAV). The most senior naval officer is the Chief of Naval Operations (CNO), a four-star admiral who is immediately under and reports to the Secretary of the Navy. At the same time, the Chief of Naval Operations is one of the Joint Chiefs of Staff, which is the second-highest deliberatory body of the armed forces after the United States National Security Council, although it only plays an advisory role to the President and does not nominally form part of the chain of command. The Secretary of the Navy and Chief of Naval Operations are responsible for organizing, recruiting, training, and equipping the Navy so that it is ready for operation under the command of the unified combat command commanders.[citation needed]		There are nine components in the operating forces of the U.S. Navy: the United States Fleet Forces Command (formerly United States Atlantic Fleet), United States Pacific Fleet, United States Naval Forces Central Command, United States Naval Forces Europe, Naval Network Warfare Command, Navy Reserve, United States Naval Special Warfare Command, Operational Test and Evaluation Force, and Military Sealift Command. Fleet Forces Command controls a number of unique capabilities, including Military Sealift Command, Naval Expeditionary Combat Command, and Navy Cyber Forces.[citation needed]		The United States Navy has six active numbered fleets – Third, Fifth, Sixth, Seventh Fleet and Tenth Fleets are each led by a vice admiral, and the Fourth Fleet is led by a rear admiral. These six fleets are further grouped under Fleet Forces Command (the former Atlantic Fleet), Pacific Fleet, Naval Forces Europe-Africa, and Naval Forces Central Command, whose commander also doubles as Commander Fifth Fleet; the first three commands being led by four-star admirals. The United States First Fleet existed after the Second World War from 1947, but it was redesignated the Third Fleet in early 1973. In early 2008, the navy reactivated the United States Fourth Fleet to control operations in the area controlled by Southern Command, which consists of US assets in and around Central and South America.[48]		Shore establishments exist to support the mission of the fleet through the use of facilities on land. Among the commands of the shore establishment, as of April 2011[update], are the Naval Education and Training Command, the Naval Meteorology and Oceanography Command, the Space and Naval Warfare Systems Command, the Naval Facilities Engineering Command, the Naval Supply Systems Command, the Naval Air Systems Command, the Naval Sea Systems Command, the Bureau of Medicine and Surgery, the Bureau of Naval Personnel, the United States Naval Academy, the Naval Safety Center, the Naval Strike and Air Warfare Center, and the United States Naval Observatory.[49] Official Navy websites list the Office of the Chief of Naval Operations and the chief of naval operations as part of the shore establishment, but these two entities effectively sit superior to the other organizations, playing a coordinating role.[citation needed]		In 1834, the United States Marine Corps came under the Department of the Navy.[50] Historically, the Navy has had a unique relationship with the USMC, partly because they both specialize in seaborne operations. Together the Navy and Marine Corps form the Department of the Navy and report to the Secretary of the Navy. However, the Marine Corps is a distinct, separate service branch[51] with its own uniformed service chief – the Commandant of the Marine Corps, a four-star general.[citation needed]		The Marine Corps depends on the Navy for medical support (dentists, doctors, nurses, medical technicians known as corpsmen) and religious support (chaplains). Thus Navy officers and enlisted sailors fulfill these roles. When attached to Marine Corps units deployed to an operational environment they generally wear Marine camouflage uniforms, but otherwise they wear Navy dress uniforms unless they opt to conform to Marine Corps grooming standards.[citation needed]		In the operational environment, as an expeditionary force specializing in amphibious operations, Marines often embark on Navy ships to conduct operations from beyond territorial waters. Marine units deploying as part of a Marine Air-Ground Task Force (MAGTF) operate under the command of the existing Marine chain of command. Although Marine units routinely operate from amphibious assault ships, the relationship has evolved over the years much as the Commander of the Carrier Air Group/Wing (CAG) does not work for the carrier commanding officer, but coordinates with the ship's CO and staff. Some Marine aviation squadrons, usually fixed-wing assigned to carrier air wings train and operate alongside Navy squadrons; they fly similar missions and often fly sorties together under the cognizance of the CAG. Aviation is where the Navy and Marines share the most common ground, since aircrews are guided in their use of aircraft by standard procedures outlined in series of publications known as NATOPS manuals.[citation needed]		The United States Coast Guard, in its peacetime role with the Department of Homeland Security, fulfills its law enforcement and rescue role in the maritime environment. It provides Law Enforcement Detachments (LEDETs) to Navy vessels, where they perform arrests and other law enforcement duties during naval boarding and interdiction missions. In times of war, the Coast Guard operates as a service in the Navy.[52] At other times, Coast Guard port security units are sent overseas to guard the security of ports and other assets. The Coast Guard also jointly staffs the Navy's naval coastal warfare groups and squadrons (the latter of which were known as harbor defense commands until late-2004), which oversee defense efforts in foreign littoral combat and inshore areas.[citation needed]		The United States Navy has nearly 500,000 personnel, approximately a quarter of whom are in ready reserve. Of those on active duty, more than eighty percent are enlisted sailors, and around fifteen percent are commissioned officers; the rest are midshipmen of the United States Naval Academy and midshipmen of the Naval Reserve Officer Training Corps at over 180 universities around the country and officer candidates at the Navy's Officer Candidate School.[2]		Enlisted sailors complete basic military training at boot camp and then are sent to complete training for their individual careers.[citation needed]		Sailors prove they have mastered skills and deserve responsibilities by completing Personnel Qualification Standards (PQS) tasks and examinations. Among the most important is the "warfare qualification", which denotes a journeyman level of capability in Surface Warfare, Aviation Warfare, Information Dominance Warfare, Naval Aircrew, Special Warfare, Seabee Warfare, Submarine Warfare or Expeditionary Warfare. Many qualifications are denoted on a sailor's uniform with U.S. Navy badges and insignia.[citation needed]		The uniforms of the U.S. Navy have evolved gradually since the first uniform regulations for officers were issued in 1802 on the formation of the Navy Department. The predominant colors of U.S. Navy uniforms are navy blue and white. U.S. Navy uniforms were based on Royal Navy uniforms of the time, and have tended to follow that template.[53]		The commissioned officer ranks of the U.S. Navy are divided into three categories: junior officers, senior officers, and flag officers. Junior officers are those officers in pay grades O-1 to O-4, while senior officers are those in pay grades O-5 and O-6, and flag officers are those in pay grades of O-7 and above.[54]		Sailors in pay grades E-1 through E-3 are considered to be in apprenticeships.[55] They are divided into five definable groups, with colored group rate marks designating the group to which they belong: Seaman/Seawoman, Fireman, Airman, Constructionman, and Hospitalman. E-4 to E-6 are non-commissioned officers (NCOs), and are specifically called Petty Officers in the Navy.[56] Petty Officers perform not only the duties of their specific career field but also serve as leaders to junior enlisted personnel. E-7 to E-9 are still considered Petty Officers, but are considered a separate community within the Navy. They have separate berthing and dining facilities (where feasible), wear separate uniforms, and perform separate duties.		After attaining the rate of Master Chief Petty Officer, a service member may choose to further his or her career by becoming a Command Master Chief Petty Officer (CMC). A CMC is considered to be the senior-most enlisted service member within a command, and is the special assistant to the Commanding Officer in all matters pertaining to the health, welfare, job satisfaction, morale, utilization, advancement and training of the command's enlisted personnel.[57][58] CMCs can be Command level (within a single unit, such as a ship or shore station), Fleet level (squadrons consisting of multiple operational units, headed by a flag officer or commodore), or Force level (consisting of a separate community within the Navy, such as Subsurface, Air, Reserves).[59]		CMC insignia are similar to the insignia for Master Chief, except that the rating symbol is replaced by an inverted five-point star, reflecting a change in their rating from their previous rating (i.e., MMCM) to CMDCM. The stars for Command Master Chief are silver, while stars for Fleet or Force Master Chief are gold. Additionally, CMCs wear a badge, worn on their left breast pocket, denoting their title (Command/Fleet/Force).[58][60]		Insignia and badges of the United States Navy are military "badges" issued by the United States Department of the Navy to naval service members who achieve certain qualifications and accomplishments while serving on both active and reserve duty in the United States Navy. Most naval aviation insignia are also permitted for wear on uniforms of the United States Marine Corps.		As described in Chapter 5 of U.S. Navy Uniform Regulations,[61] "badges" are categorized as breast insignia (usually worn immediately above and below ribbons) and identification badges (usually worn at breast pocket level).[62] Breast insignia are further divided between command and warfare and other qualification.[63]		Insignia come in the form of metal "pin-on devices" worn on formal uniforms and embroidered "tape strips" worn on work uniforms. For the purpose of this article, the general term "insignia" shall be used to describe both, as it is done in Navy Uniform Regulations. The term "badge", although used ambiguously in other military branches and in informal speak to describe any pin, patch, or tab, is exclusive to identification badges[64] and authorized marksmanship awards[65] according to the language in Navy Uniform Regulations, Chapter 5. Below are just a few of the many badges maintained by the Navy. The rest can be seen in the article cited at the top of this section:		Naval Aviator Badge		Submarine Officer and Enlisted		Surface Warfare Officer Insignia		The size, complexity, and international presence of the United States Navy requires a large number of navy installations to support its operations. While the majority of bases are located inside the United States itself, the navy maintains a significant number of facilities abroad, either in U.S.-controlled territories or in foreign countries under a Status of Forces Agreement (SOFA).[citation needed]		The second largest concentration of installations is at Hampton Roads, Virginia, where the navy occupies over 36,000 acres (15,000 ha) of land. Located at Hampton Roads are Naval Station Norfolk, homeport of the Atlantic Fleet; Naval Air Station Oceana, a Master Jet Base; Naval Amphibious Base Little Creek; and Training Support Center Hampton Roads as well as a number of Navy and commercial shipyards that service navy vessels. The Aegis Training and Readiness Center is located at the Naval Support Activity South Potomac in Dahlgren, Virginia. Maryland is home to NAS Patuxent River, which houses the Navy's Test Pilot School. Also located in Maryland is the United States Naval Academy, situated in Annapolis. NS Newport in Newport, Rhode Island is home to many schools and tenant commands, including the Officer Candidate School, Naval Undersea Warfare Center, and more, and also maintains inactive ships.[citation needed]		There is also a naval base in Charleston, South Carolina. This is home to the Nuclear A-School, and the Nuclear Field Power school, and one of two nuclear 'Prototype' Schools. The state of Florida is the location of three major bases, NS Mayport, the Navy's fourth largest, in Jacksonville, Florida; NAS Jacksonville, a Master Air Anti-submarine Warfare base; and NAS Pensacola; home of the Naval Education and Training Command, the Naval Air Technical Training Center that provides specialty training for enlisted aviation personnel and is the primary flight training base for Navy and Marine Corps Naval Flight Officers and enlisted Naval Aircrewmen. There is also NSA Panama City, Florida which is home to the Navy Diving and Salvage Training Center.[citation needed]		The main U.S. Navy submarine bases on the east coast are located in Naval Submarine Base New London in Groton, Connecticut and NSB Kings Bay in Kings Bay, Georgia. The Portsmouth Naval Shipyard near Portsmouth, New Hampshire,[66] which repairs naval submarines.[2] NS Great Lakes, north of Chicago, Illinois is the home of the Navy's boot camp for enlisted sailors.[citation needed]		The Washington Navy Yard in Washington, DC is the Navy's oldest shore establishment and serves as a ceremonial and administrative center for the U.S. Navy, home to the Chief of Naval Operations, and is headquarters for numerous commands.[citation needed]		The navy's largest complex is Naval Air Weapons Station China Lake, California, which covers 1.1 million acres (4,500 km2) of land, or approximately 1/3 of the United States Navy's total land holdings.[2]		Naval Base San Diego, California, is the main homeport of the Pacific Fleet (although its headquarters is located in Pearl Harbor, Hawaii). NAS North Island is located on the north side of Coronado, and is home to Headquarters for Naval Air Forces and Naval Air Force Pacific, the bulk of the Pacific Fleet's helicopter squadrons, and part of the West Coast aircraft carrier fleet. NAB Coronado is located on the southern end of the Coronado Island and is home to the navy's west coast SEAL teams and special boat units. NAB Coronado is also home to the Naval Special Warfare Center, the primary training center for SEALs.[citation needed]		The other major collection of naval bases on the west coast is in Puget Sound, Washington. Among them, NS Everett is one of the newer bases and the navy states that it is its most modern facility.[67]		NAS Fallon, Nevada serves as the primary training ground for navy strike aircrews, and is home to the Naval Strike Air Warfare Center. Master Jet Bases are also located at NAS Lemoore, California and NAS Whidbey Island, Washington, while the carrier-based airborne early warning aircraft community and major air test activities are located at NAS Point Mugu, California. The naval presence in Hawaii is centered on NS Pearl Harbor, which hosts the headquarters of the Pacific Fleet and many of its subordinate commands.[citation needed]		Guam, an island strategically located in the Western Pacific Ocean, maintains a sizable U.S. Navy presence, including NB Guam. The westernmost U.S. territory, it contains a natural deep water harbor capable of harboring aircraft carriers in emergencies.[68] Its naval air station was deactivated[69] in 1995 and its flight activities transferred to nearby Andersen Air Force Base.		Puerto Rico in the Caribbean formerly housed NS Roosevelt Roads, which was shut down in 2004 shortly after the controversial closure of the live ordnance training area on nearby Vieques Island.[2]		The largest overseas base is the United States Fleet Activities Yokosuka, Japan,[70] which serves as the home port for the navy's largest forward-deployed fleet and is a significant base of operations in the Western Pacific.[citation needed]		European operations revolve around facilities in Italy (NAS Sigonella and Naval Computer and Telecommunications Station Naples) with NSA Naples as the homeport for the Sixth Fleet and Command Naval Region Europe, Africa, Southwest Asia (CNREURAFSWA), and additional facilities in nearby Gaeta. There is also NS Rota in Spain and NSA Souda Bay in Greece.[citation needed]		In the Middle East, naval facilities are located almost exclusively in countries bordering the Persian Gulf, with NSA Bahrain serving as the headquarters of U.S. Naval Forces Central Command and U.S. Fifth Fleet.		NS Guantanamo Bay in Cuba is the oldest overseas facility and has become known in recent years as the location of a detention camp for suspected al-Qaeda operatives.[71]		As of 2013[update], the navy operates over 280 ships, 3,650+ aircraft, 50,000 non-combat vehicles and owns 75,200 buildings on 3,300,000 acres (13,000 km2). In addition, the Navy has more than one hundred vessels operated by the Military Sealift Command (MSC) crewed by a combination of civilian contractors and a small number of uniformed Naval personnel.[citation needed]		The names of commissioned ships of the U.S. Navy are prefixed with the letters "USS", designating "United States Ship".[72] Non-commissioned, civilian-manned vessels of the navy have names that begin with "USNS", standing for "United States Naval Ship" The names of ships are officially selected by the secretary of the navy, often to honor important people or places.[73] Additionally, each ship is given a letter-based hull classification symbol (for example, CVN or DDG) to indicate the vessel's type and number. All ships in the navy inventory are placed in the Naval Vessel Register, which is part of "the Navy List" (required by article 29 of the United Nations Convention on the Law of the Sea).[dubious – discuss] The register tracks data such as the current status of a ship, the date of its commissioning, and the date of its decommissioning. Vessels that are removed from the register prior to disposal are said to be stricken from the register. The navy also maintains a reserve fleet of inactive vessels that are maintained for reactivation in times of need.[citation needed]		The U.S. Navy was one of the first to install nuclear reactors aboard naval vessels;[74] today, nuclear energy powers all active U.S. aircraft carriers and submarines. In the case of the Nimitz-class carrier, two naval reactors give the ship almost unlimited range and provide enough electrical energy to power a city of 100,000 people.[75] The U.S. Navy previously operated nuclear-powered cruisers, but all have been decommissioned.[citation needed]		The U.S. Navy had identified a need for 313 combat ships in early 2010s, but under its plans at the time could only afford 232 to 243.[76] In March 2014, the Navy started counting self-deployable support ships such as minesweepers, surveillance craft, and tugs in the "battle fleet" in order to reach a count of 272 as of October 2016,[77][78] and it includes ships that have been put in "shrink wrap".[79]		An aircraft carrier is typically deployed along with a host of additional vessels, forming a carrier strike group. The supporting ships, which usually include three or four Aegis-equipped cruisers and destroyers, a frigate, and two attack submarines, are tasked with protecting the carrier from air, missile, sea, and undersea threats as well as providing additional strike capabilities themselves. Ready logistics support for the group is provided by a combined ammunition, oiler, and supply ship. Modern carriers are named after American admirals and politicians, usually presidents.		The Navy has a statutory requirement for a minimum of 11 aircraft carriers.[80] Currently there are 10 that are deployable and one, the USS Gerald R. Ford (CVN-78), is currently undergoing extensive systems and technologies testing until around 2021.[81]		Amphibious assault ships are the centerpieces of US amphibious warfare and fulfill the same power projection role as aircraft carriers except that their striking force centers on land forces instead of aircraft. They deliver, command, coordinate, and fully support all elements of a 2,200-strong Marine Expeditionary Unit in an amphibious assault using both air and amphibious vehicles. Resembling small aircraft carriers, amphibious assault ships are capable of V/STOL, STOVL, VTOL, tiltrotor, and rotary wing aircraft operations. They also contain a well deck to support the use of Landing Craft Air Cushion (LCAC) and other amphibious assault watercraft. Recently, amphibious assault ships have begun to be deployed as the core of an expeditionary strike group, which usually consists of an additional amphibious transport dock and dock landing ship for amphibious warfare and an Aegis-equipped cruiser and destroyer, frigate, and attack submarine for group defense. Amphibious assault ships are typically named after World War II aircraft carriers.[citation needed]		Amphibious transport docks are warships that embark, transport, and land Marines, supplies, and equipment in a supporting role during amphibious warfare missions. With a landing platform, amphibious transport docks also have the capability to serve as secondary aviation support for an expeditionary group. All amphibious transport docks can operate helicopters, LCACs, and other conventional amphibious vehicles while the newer San Antonio class of ships has been explicitly designed to operate all three elements of the Marines' "mobility triad": Expeditionary Fighting Vehicles (EFVs), the V-22 Osprey tiltrotor aircraft, and LCACs. Amphibious transport docks are named after U.S. cities, with the exception of the USS John P. Murtha (LPD-26), named after a former Congressman and USMC Officer and USS Mesa Verde (LPD-19), named for Mesa Verde National Park in Colorado.[citation needed]		The dock landing ship is a medium amphibious transport that is designed specifically to support and operate LCACs, though it is able to operate other amphibious assault vehicles in the United States inventory as well. Dock landing ships are normally deployed as a component of an expeditionary strike group's amphibious assault contingent, operating as a secondary launch platform for LCACs. All dock landing ships are named after cities or important places in U.S. and U.S. Naval history.[citation needed]		Cruisers are large surface combat vessels that conduct anti-air/anti-missile warfare, surface warfare, anti-submarine warfare, and strike operations independently or as members of a larger task force. Modern guided missile cruisers were developed out of a need to counter the anti-ship missile threat facing the United States Navy. This led to the development of the AN/SPY-1 phased array radar and the Standard missile with the Aegis combat system coordinating the two. Ticonderoga-class cruisers were the first to be equipped with Aegis and were put to use primarily as anti-air and anti-missile defense in a battle force protection role. Later developments of vertical launch systems and the Tomahawk missile gave cruisers additional long-range land and sea strike capability, making them capable of both offensive and defensive battle operations. The Ticonderoga class is the only active class of cruiser. All cruisers in this class are named after battles.[citation needed]		Destroyers are multi-mission medium surface ships capable of sustained performance in anti-air, anti-submarine, anti-ship, and offensive strike operations. Like cruisers, guided missile destroyers are primarily focused on surface strikes using Tomahawk missiles and fleet defense through Aegis and the Standard missile. Destroyers additionally specialize in anti-submarine warfare and are equipped with VLA rockets and LAMPS Mk III Sea Hawk helicopters to deal with underwater threats. When deployed with a carrier strike group or expeditionary strike group, destroyers and their fellow Aegis-equipped cruisers are primarily tasked with defending the fleet while providing secondary strike capabilities. With very few exceptions, destroyers are named after U.S. Navy, Marine Corps, and Coast Guard heroes.[citation needed]		Modern U.S. frigates mainly perform anti-submarine warfare for carrier and expeditionary strike groups and provide armed escort for supply convoys and merchant shipping. They are designed to protect friendly ships against hostile submarines in low to medium threat environments, using torpedoes and LAMPS helicopters. Independently, frigates are able to conduct counterdrug missions and other maritime interception operations. As in the case of destroyers, frigates are named after U.S. Navy, Marine Corps, and Coast Guard heroes. As of autumn 2015, the U.S. Navy has retired its most recent class of frigates, and expects that by 2020 the Littoral Combat Ships (LCS) will assume many of the duties the frigate had with the fleet. The LCS is a class of relatively small surface vessels intended for operations in the littoral zone (close to shore). It was "envisioned to be a networked, agile, stealthy surface combatant capable of defeating anti-access and asymmetric threats in the littorals". They have the capabilities of a small assault transport, including a flight deck and hangar for housing two helicopters, a stern ramp for operating small boats, and the cargo volume and payload to deliver a small assault force with fighting vehicles to a roll-on/roll-off port facility. The ship is easy to reconfigure for different roles, including anti-submarine warfare, mine countermeasures, anti-surface warfare, intelligence, surveillance and reconnaissance, homeland defense, maritime intercept, special operations, and logistics, all by swapping mission-specific modules as needed. The LCS program is still relatively new as of 2015 with only a few active ships, but the navy has announced plans for up to 32 ships. (See: List of littoral combat ships) The navy has announced that a further 20 vessels to be built after that will be redesignated as 'frigates'.[82]		Mine countermeasures vessels are a combination of minehunters, a naval vessel that actively detects and destroys individual naval mines, and minesweepers, which clear mined areas as a whole, without prior detection of the mines. The navy has approximately a dozen of these in active service, but the mine countermeasure (MCM) role is also being assumed by the incoming classes of littoral combat ships. MCM vessels have mostly legacy names of previous US Navy ships, especially WWII-era minesweepers.[citation needed]		A patrol boat is a relatively small naval vessel generally designed for coastal defense duties. There have been many designs for patrol boats, though the navy currently only has a single class. They may be operated by a nation's navy or coast guard, and may be intended for marine ("blue water") and/or estuarine or river ("brown water") environments. The Navy has approximately a dozen in active service, which are mainly used in the littoral regions of the Persian Gulf, but have also been used for home port patrols and drug interdiction missions. The navy's current class of patrol boats have names based on weather phenomena.[citation needed]		All current and planned U.S. Navy submarines are nuclear-powered, as only nuclear propulsion allows for the combination of stealth and long duration, high-speed sustained underwater movement that makes modern nuclear submarines so vital to a modern blue-water navy. The U.S. Navy operates three types: ballistic missile submarines, guided missile submarines, and attack submarines. U.S. Navy (nuclear) ballistic missile submarines carry the stealthiest leg of the U.S. strategic triad (the other legs are the land-based U.S. strategic missile force and the air-based U.S. strategic bomber force). These submarines have only one mission: to carry and, if called upon, to launch the Trident nuclear missile. The primary missions of attack and guided missile submarines in the U.S. Navy are peacetime engagement, surveillance and intelligence, special operations, precision strikes, and control of the seas.[83] To these, attack submarines also add the battlegroup operations mission. Attack and guided missile submarines have several tactical missions, including sinking ships and other subs, launching cruise missiles, gathering intelligence, and assisting in special operations.[citation needed]		As with other classes of naval vessels, most U.S. submarines (or "boats") are named according to specific conventions. The boats of the current U.S. ballistic missile submarine class, Ohio-class, are named after U.S. states. As the four current U.S. guided missile submarines are converted Ohio-class boats, they have retained their U.S. state names. The members of the oldest currently-commissioned attack submarine class, the Los Angeles class, are typically named for cities. The follow-on Seawolf-class' three submarines—Seawolf, Connecticut and Jimmy Carter—share no consistent naming scheme. With the current Virginia-class class attack submarines, the U.S. Navy has extended the Ohio class' state-based naming scheme to these submarines. Attack submarines prior to the Los Angeles class were named for denizens of the deep, while pre-Ohio-class ballistic missile submarines were named for famous Americans and foreigners with notable connections to the United States.[citation needed]		Carrier-based aircraft are able to strike air, sea, and land targets far from a carrier strike group while protecting friendly forces from enemy aircraft, ships, and submarines. In peacetime, aircraft's ability to project the threat of sustained attack from a mobile platform on the seas gives United States leaders significant diplomatic and crisis-management options. Aircraft additionally provide logistics support to maintain the navy's readiness and, through helicopters, supply platforms with which to conduct search and rescue, special operations, anti-submarine warfare (ASW), and anti-surface warfare (ASuW).[citation needed]		The U.S. Navy began to research the use of aircraft at sea in the 1910s, with Lieutenant Theodore G. "Spuds" Ellyson becoming the first naval aviator on 28 January 1911, and commissioned its first aircraft carrier, USS Langley (CV-1), in 1922.[84] United States naval aviation fully came of age in World War II, when it became clear following the Attack on Pearl Harbor, the Battle of the Coral Sea, and the Battle of Midway that aircraft carriers and the planes that they carried had replaced the battleship as the greatest weapon on the seas. Leading navy aircraft in World War II included the Grumman F4F Wildcat, the Grumman F6F Hellcat, the Chance Vought F4U Corsair, the Douglas SBD Dauntless, and the Grumman TBF Avenger. Navy aircraft also played a significant role in conflicts during the following Cold War years, with the F-4 Phantom II and the F-14 Tomcat becoming military icons of the era. The navy's current primary fighter and attack airplanes are the multi-mission F/A-18C/D Hornet and its newer cousin, the F/A-18E/F Super Hornet. The F-35 Lightning II is presently under development and was scheduled to replace the C and D versions of the Hornet beginning in 2012.[85] Initial operational capability of the F-35C is now expected to be February 2019.[86] The Navy is also looking to eventually replace its F/A-18E/F Super Hornets with the F/A-XX program.[citation needed]		The Aircraft Investment Plan sees naval aviation growing from 30 percent of current aviation forces to half of all procurement funding over the next three decades.[87]		Current U.S. Navy shipboard weapons systems are almost entirely focused on missiles, both as a weapon and as a threat. In an offensive role, missiles are intended to strike targets at long distances with accuracy and precision. Because they are unmanned weapons, missiles allow for attacks on heavily defended targets without risk to human pilots. Land strikes are the domain of the BGM-109 Tomahawk, which was first deployed in the 1980s and is continually being updated to increase its capabilities. For anti-ship strikes, the navy's dedicated missile is the Harpoon Missile. To defend against enemy missile attack, the navy operates a number of systems that are all coordinated by the Aegis combat system. Medium-long range defense is provided by the Standard Missile 2, which has been deployed since the 1980s. The Standard missile doubles as the primary shipboard anti-aircraft weapon and is undergoing development for use in theater ballistic missile defense. Short range defense against missiles is provided by the Phalanx CIWS and the more recently developed RIM-162 Evolved Sea Sparrow Missile. In addition to missiles, the navy employs Mark 46 and Mark 50 torpedoes and various types of naval mines.[citation needed]		Naval fixed-wing aircraft employ much of the same weapons as the United States Air Force for both air-to-air and air-to-surface combat. Air engagements are handled by the heat-seeking Sidewinder and the radar guided AMRAAM missiles along with the M61 Vulcan cannon for close range dogfighting. For surface strikes, navy aircraft utilize a combination of missiles, smart bombs, and dumb bombs. On the list of available missiles are the Maverick, SLAM-ER and JSOW. Smart bombs include the GPS-guided JDAM and the laser-guided Paveway series. Unguided munitions such as dumb bombs and cluster bombs make up the rest of the weapons deployed by fixed-wing aircraft.[citation needed]		Rotary aircraft weapons are focused on anti-submarine warfare (ASW) and light to medium surface engagements. To combat submarines, helicopters use Mark 46 and Mark 50 torpedoes. Against small watercraft, they utilize Hellfire and Penguin air to surface missiles. Helicopters also employ various types of mounted anti-personnel machine guns, including the M60, M240, GAU-16/A, and GAU-17/A.[citation needed]		Nuclear weapons in the U.S. Navy arsenal are deployed through ballistic missile submarines and aircraft. The Ohio-class submarine carries the latest iteration of the Trident missile, a three-stage, submarine-launched ballistic missile (SLBM) with MIRV capability; the current Trident II (D5) version is expected to be in service past 2020.[88] The navy's other nuclear weapon is the air-deployed B61 nuclear bomb. The B61 is a thermonuclear device that can be dropped by strike aircraft such as the F/A-18 Hornet and Super Hornet at high speed from a large range of altitudes. It can be released through free-fall or parachute and can be set to detonate in the air or on the ground.[citation needed]		The current naval jack of the United States is the First Navy Jack, traditionally regarded as having been used during the American Revolutionary War. On 31 May 2002, Secretary of the Navy Gordon R. England directed all U.S. naval ships to fly the First Navy Jack for the duration of the "War on Terror". Many ships chose to shift colors later that year on the first anniversary of the September 11, 2001 attacks. The previous naval jack was a blue field with 50 white stars, identical to the canton of the ensign (the flag of the United States) both in appearance and size, and remains in use with vessels of the U.S. Coast Guard and National Oceanic and Atmospheric Administration. A jack of similar design was used in 1794, though with 13 stars arranged in a 3–2–3–2–3 pattern. When a ship is moored or anchored, the jack is flown from the bow of the ship while the ensign is flown from the stern. When underway, the ensign is raised on the mainmast. The First Naval Jack, however, has always been flown on the oldest ship in the active American fleet, which is currently USS Blue Ridge (LCC-19).[citation needed]		Many past and present United States historical figures have served in the navy. Notable officers include John Paul Jones, John Barry (Continental Navy officer and first flag officer of the United States Navy),[89] Edward Preble, James Lawrence (whose last words "don't give up the ship" are memorialized in Bancroft Hall at the United States Naval Academy), Stephen Decatur, Jr., David Farragut, David Dixon Porter, Oliver Hazard Perry, Commodore Matthew Perry (whose Black Ships forced the opening of Japan), George Dewey (the only person in the history of the United States to have attained the rank of Admiral of the Navy), and the officers who attained the rank of Fleet Admiral during World War II: William D. Leahy, Ernest J. King, Chester W. Nimitz, and William F. Halsey, Jr..[citation needed]		The first American president who served in the navy was John F. Kennedy (who commanded the famous PT-109). Others included Lyndon B. Johnson, Richard Nixon, Gerald Ford, Jimmy Carter, and George H. W. Bush. Both Theodore Roosevelt and Franklin D. Roosevelt were the assistant secretary of the navy prior to their presidencies. Many members of Congress served in the navy, notably U.S. Senators Bob Kerrey, John McCain, and John Kerry. Other notable former members of the U.S. Navy include astronauts, entertainers, authors and professional athletes.[citation needed]		
Military service is service by an individual or group in an army or other militia, whether as a chosen job or as a result of an involuntary draft (conscription). Some nations (e.g., Mexico) require a specific amount of military service from every citizen (except for special cases, such as physical or mental disorders or religious beliefs, and most countries that have conscription only conscript men). A nation with a fully volunteer military does not normally require mandatory military service from its citizens, unless it is faced with a recruitment crisis during a time of war.						In this summary, 195 countries are included.[1][2][3][4]		The following nineteen countries have been identified as having no defence forces or as having no standing army but having very limited military forces:		The following 105 countries have been identified as having no enforced conscription:		The following ten countries and colonies have been identified as having both compulsory and voluntary military service:		The following 13 countries have been identified as having selective conscription:		The following thirteen countries have been identified as having a civilian, unarmed or non-combatant service optional alternative to compulsory military service:		The following twenty-one countries have been identified as having compulsory military service limited to 1 year or less:		The following 8 countries have been identified to having compulsory military service limited to 18 months or less:		The following thirty-two countries have been identified as having compulsory military service terms longer than 18 months:		As of 2015[update], three countries have been identified as intending to abolish conscription in the near future:				Compulsory military service has declined considerably since 1970. A 2016 study finds "that the probability of a shorter military service time is positively associated with smaller country populations, smaller lagged army sizes, increases in primary schooling among young males, and having common law legal origins."[15]		Albania had compulsory military service. Albania's armed forces announced an objective to create a professional army by the end of 2010.[16]		Argentina suspended military conscription in 1995 and replaced it with a voluntary military service, yet those already in service had to finish their time in service.[17]		This came as a result of political and social distrust of the military, dwindling budgets which forced the military to induct fewer conscripts every year, the experience of the 1982 Falklands War which proved the superiority of professional servicemen over conscripts and a series of conscription-related brutality scandals which came to a head with the murder of Private Omar Carrasco at an Army base in 1994, following a brutal disciplinary action.		It should be noted that military conscription has not been abolished; the Mandatory Military Service Law is still in the books and might be enforced in times of war, crisis or national emergency.		Conscription was known in Argentina as la colimba. The word colimba is a composite word made from the initial syllables of the verbs correr (to run), limpiar (to clean) and barrer (to sweep), as it was perceived that all a conscript did during service was running, cleaning and sweeping. Conscripts themselves were known and referred to as "colimbas".		Voluntary service in the Boer War (1899-1902) was initially from a number of the separate colonies before federation in 1901 and later volunteers were deployed as an Australian force. Two conscription referendums were defeated during World War 1. Military service during WW1 was voluntary (1st AIF) as was service in WW2 (2nd AIF). Volunteer militia units (part-time civilian soldiers) were to be used only within the Commonwealth of Australia but in 1942 some militia units were deployed to Papua New Guinea, as it was considered part of Australia at that time, to fight the advancing and later withdrawing Japanese invasion army. Various levels of conscription (National Service) were in force during the 1950s but only for service in Australia during times of conflicts but the (Vietnam War) saw NS deployed to war with over 500 KIA and thousands WIA with about half of the casualties being NS. The Vietnam War was lost on 1 May 1975 over three years after the ADF withdrew in late 1971. All forms of conscription were abolished by the Whitlam Government in later 1972.[18]		Barbados has no conscription.[19] The country has set the minimum age for voluntary recruitment into the Barbados Defence Force at 18. Younger recruits may be conscripted with parental consent.		Belgium suspended conscription on 31 December 1992 by amending the 1962 Law on Conscription, which became applicable only to conscripts drafted in 1993 and earlier. In practice this meant that the law no longer applied to those born in 1975 and later. Since 1 March 1995 the Belgian armed forces consist of professional volunteers only.[20]		Belize has set minimum age for voluntary recruitment into the Armed Forces at 18. (According to the Section 16 of the Defense Act of the Defence Ordinance of 1977.) Conscription has never been prescribed in the Defense Act, but is at the Governor General’s discretion.		Bosnia and Herzegovina abolished compulsory military service as of 1 January 2007.[21]		Bulgaria abolished compulsory military service. The last conscripts were sent home on 25 November 2007.[22][23][24]		Previously there was mandatory military service for male citizens from eighteen to twenty-seven years of age. Duration of the service depended on the degree of education. For citizens studying for or holding a bachelor's degree or higher the service was six months, and for citizens with no higher education it was nine months.[25] The duration of service was two years in 1992, and was dropping steadily, until it was finally abolished.		In Canada, conscription has never taken place in peacetime. Conscription became a very controversial issue during both World War I and World War II, especially in the province of Quebec.		All Chilean men between 17 and 24 years are eligible for military service. At first instance, military service is voluntary and then mandatory if quotas necessary for the armed forces are not completed.The general direction of national mobilization (In Spanish Dirección general de Movilización Nacional DGMN) is responsible for the recruitment of volunteers and conscripts.[26]		At present, military conscription only exists in theory and has done so since the establishment of the People's Republic of China in 1949. Due to China's huge population and therefore[citation needed] the large number of individuals who volunteer to join the regular armed forces, universal military conscription has never been enforced.		Every Chinese citizen, both male and female, who attend further education are required to attend a military training period of around 20 days as a part of a military education.[clarification needed]		Conscription is enshrined in Article 55 of the Constitution, which states: "It is a sacred duty of every citizen of the People's Republic of China to defend his or her motherland and resist invasion. It is an honored Obligation of the citizens of the People's Republic of China to perform military service and to join the militia forces".[27]		As of 1998, the legal basis of conscription was stated to be the 1984 Military Service Law, which describes military service as a duty for "all citizens without distinction of race (...) and religious creed".[citation needed] This law has not been amended since it came into effect.[27] Military service is normally performed in the regular armed forces but the 1984 law does allow for conscription into the reserve forces in times of national emergency.[citation needed]		Citizens of the Special Administrative Regions of Hong Kong and Macau, as of 1997 and 1999, respectively, are not permitted to join the Chinese military; however, the defence of these two regions are protected by the Chinese military.[citation needed]		Costa Rica abolished its military in 1948. See Military of Costa Rica		On 3 October 2007, the government proposed to the parliament of the Republic of Croatia a decision to suspend all compulsory military service. This was supported by President Stjepan Mesić, and after a vote in the parliament on 5 October 2007, the decision became official. As of 1 January 2008, obligatory military (or civil) service is replaced with voluntary military service.[28]		The Czech Republic abolished compulsory military service on 31 December 2004.[29][30]		Modern conscription was invented during the French Revolution, when the Republic wanted a stronger defense and to expand its radical ideas throughout Europe. The 1798 Jourdan Act stated: "Any Frenchman is a soldier and owes himself to the defense of the nation". Thus Napoleon Bonaparte could create afterward the Grande Armée with which he set out on the first large intra-European war.		France suspended peacetime military conscription in 1996, while those born before 1979 had to complete their service;[31] since the Algerian War (1954–62), conscripts had not been deployed abroad or in war zones, except those volunteering for such deployments.		On 15 November 2010, the German government voted in favour of suspending universal conscription with the aim of establishing a professional army by 1 July 2011. The last conscripts were drafted on 1 January 2011.[32]		Hungary abolished mandatory military service by November 2004, after the parliament had modified the constitution, ending a long-standing political dispute. To restore drafting, a two-thirds vote in parliament is needed, which is unlikely in the short term. As of 2011[update], the country is developing a professional army, with strong emphasis on "contract soldiers" who voluntarily serve 4+4 years for a wage.[citation needed]		In December 2011, the National Assembly re-established the possibility of mandatory military service for every male citizen - with Hungarian address - between the age of 18 and the age of 40. Even though drafting is still banned in peace time, the listing of citizens fit for military service starts in January 2012. According to the legislation, the conscripts can only be drafted in "state of emergency" or as defensive measure, the National Assembly can authorise drafting.[33]		India has never had mandatory military service, either under British rule or since independence in 1947. In WWII the Indian Army became the largest all-volunteer force in history, rising to over 2.5 million men in size. And it has since maintained the world's third largest army and the world's largest all volunteer army.		Saddam Hussein's large Iraqi army was largely composed of conscripts, except for the elite Republican Guard. About 20,000-35,000 conscripts died during the First Persian Gulf War, also known as Operation Desert Storm. In the intervening years, Iraq's military suffered from decay and poor leadership, but there was still compulsory service. Note: One of voluntary program was "Ashbal Saddam" known as "Saddam's Cubs" where children were trained to defend Iraq through "toughening" exercises such as firearms training and dismembering live chickens with their teeth. Following the Second Persian Gulf War where the original military was disbanded, the Iraqi Army was recreated as a volunteer force with training overseen at first by the Coalition Provisional Authority and later by the American presence.		The whole island of Ireland was exempted from UK First World War conscription in 1916, but in April 1918 new legislation empowered the UK government to extend it to Ireland. Although the government never implemented this legislation, it led to a Conscription Crisis in Ireland and politically pushed the country further to seek its independence from the UK. Since independence in 1922, the Irish Defence Forces have always been fully voluntary, and the state's foreign policy includes "military neutrality".[34]		Italy had mandatory military service, for men only, until 31 December 2004. The right to conscientious objection was legally recognized in 1972 so that a "non armed military service", or a community service, could be authorised as an alternative to those who required it.[35]		The Italian Parliament approved the suspension of the mandatory military service in August 2004, with effect starting from 1 January 2005, and the Italian armed forces will now be entirely composed of professional volunteer troops, both male and female,[36] except in case of war or serious international military crisis, when conscription can be implemented.		In Jamaica the military service is voluntary from 18 years of age up. Younger recruits may be conscripted with parental consent.		Conscription was enforced during the Japanese Militarism in Second World War . Japan's Self Defense Forces have been a volunteer force since their establishment in the 1950s, following the end of the Allied occupation.		Latvia abolished compulsory military service on 1 January 2007.[37]		Lebanon previously had mandatory military service of one year for men. On 4 May 2005, a new conscription system was adopted, making for a six-month service, and pledging to end conscription within two years. By 10 February 2007 it did.		Luxembourg has a military ground force (army) composed of career officers and volunteers.[38] Compulsory military service (conscription) existed between 1944 and 1967.[39]		Republic of Macedonia abolished compulsory military service as of October 2006.[40]		Mauritius does not have a standing army but operates a paramilitary service manned by volunteering Police officers known as the Special Mobile Force.		President of Montenegro Filip Vujanović has, as of 30 August 2006, abolished conscription for the military.		Morocco eliminated compulsory military service as of 31 August 2006.[41]		The Netherlands established conscription for a territorial militia in 1814, simultaneously establishing a standing army which was to be manned by volunteers only. However, lack of sufficient volunteers caused the two components to be merged in 1819 into a "cadre-militia" army, in which the bulk of troops were conscripts, led by professional officers and NCOs. This system remained in use until the end of the Cold War. Between 1991 and 1996, the Dutch armed forces phased out their conscript personnel and converted to an all-volunteer force. The last conscript troops were inducted in 1995 and demobilized in 1996. Formally, the Netherlands has not abolished conscription; that is to say, the laws and systems which provide for the conscription of armed forces personnel remain in place, and Dutch citizens can still, theoretically, be mobilized in the event of a national emergency.		Conscription of men into the armed forces of New Zealand came into effect in two periods, from 1912 to 1930 and from 1940 until it was abolished in 1972.		Like India, Pakistan has always maintained a purely volunteer military. However, in the immediate aftermath of independence, and the 1948 war; at a time when the army was just reorganising from a colonial force to a new national army; militias raised for service from, the Frontier, Punjab and Kashmir were often raised from locals tribe; each tribe was given a quota and many of the individuals sent did not "volunteer" in the strictest sense (though many did).		Panama officially abolished the entire military in 1992, and transformed it into National Police.		Peru abolished conscription in 1999.[42]		Poland suspended compulsory military service on 5 December 2008 by the order of the Minister of Defence. Compulsory military service was formally abolished when the Polish parliament amended the conscription law on 9 January 2009; the law came into effect on 11 February 2009.		Portugal abolished compulsory military service on 19 November 2004.[43]		Romania suspended compulsory military service on 23 October 2006.[44] This came about due to a 2003 constitutional amendment which allowed the parliament to make military service optional. The Romanian Parliament voted to abolish conscription in October 2005, with the vote formalising one of many military modernisation and reform programmes that Romania agreed to when it joined NATO.		Serbia abolished compulsory military service on 1 January 2011. Before that, Serbia had compulsory national service for all men aged between 19 and 35. In practice, men over 27 were seldom called up. Service was usually performed after university studies had been completed. The length of service was 12 months, then reduced to 9 months but was reduced to 6 months in 2006. There was also an alternative for conscientious objectors which lasted 9 months. Serbian nationals living outside of the country were still expected to complete national service; however, they could defer it if it would have seriously affected their career in the country where they then resided. This could be done by contacting the embassy in the country of residence (if under 27), or done by contacting the army directly (if over 27).		Slovakia abolished compulsory military service on 1 January 2006.		Slovenia's Government of Prime Minister Anton Rop abolished mandatory military service on 9 September 2003.[45]		South Africa under the apartheid system had two years of compulsory military service for white men, followed by camps at intervals. This was abolished in 1994. See End Conscription Campaign.		Spain abolished compulsory military service in 2001.[46] Military and alternative service was nine months long and in recent years the majority of conscripts chose to perform alternative, rather than military, service.		Sri Lanka has never had mandatory military service, either under British rule or since independence in 1948. It maintains an all-volunteer military.		Military Service was mandatory in Sweden from 1901 until 1 July 2010,[47] when conscription was officially suspended.[48] Until 2010, all Swedish men aged between 18 and 47 years old were eligible to serve with the armed forces over a period ranging from 80 to 450 days. The right to Conscientious Objection was legally recognised in 1920. An alternative community service for Conscientious Objectors was easily available instead of military service. The number of those seeking conscientious objector status declined as actual conscript recruitment continued to decline.[49] In the years running up to 2010 roughly 6000 - 8000 people out of an annual cohort of 100,000 - 120,000 potential recruits actually completed military service.[50][50] Sweden now looks to reintroduce Compulsory Military Service with the idea to be decided on by the end of 2017 and if it is passed begin training troops in 2018.[51]		Tanzania officially never had conscription since it became independent.[52]		The United Kingdom historically was the only European state with a volunteer army, and remained so through the first half of World War I. As the war dragged on into deadlocked trench warfare, the amount of volunteers dried up, which led the government to introduce conscription under the Military Service Act. The Act exempted the whole of Ireland from conscription, and although an amending Act in 1918 empowered the government to extend conscription to Ireland, the extension was never implemented. Conscientious objectors were formally recognised under the Military Service Act, with the possibilities of absolute exemption from military service, exemption conditional upon performing civilian "work of national importance", or non-combatant service within the Army; some of those who refused the latter alternatives, or engaged in anti-war protests, went to jail. Conscription was ended in 1920.		Military conscription was reintroduced in Britain (Northern Ireland being exempted) in May 1939 in anticipation of World War II. A form of "industrial conscription" was also used to increase output in coal mining (see the "Bevin Boys") and other dimensions of the war effort. Later in the war both forms of conscription were extended in a limited way to women, such as the Women's Land Army to help with agricultural production. Conscientious objectors were treated more leniently than in WWI, but could still go to prison if they refused war-related work. For example, the scientist Kathleen Lonsdale was sentenced to a month in Holloway prison in 1943 for refusing to register for war duties and refusing to pay a resulting fine of two pounds.[53]		Conscription of men into the Armed Forces continued ad hoc from 1945 until the National Service Act 1948 established a system from 1 January 1949 of calling up men in Britain reaching the age of 18 for eighteen months full-time service followed by four years reserve service. In 1950, as a result of the Korean War, full-time service was increased to two years, and reserve service reduced to three and a half years. In 1957, phasing-out of the system was announced, the last men being called up in 1960, and the last conscript being discharged in May 1963, after allowance for deferment of service.		The British royal family is a unique exception to the country's tradition of volunteerism, as all male royals are required to perform military service.[citation needed]		The United States first introduced conscription during the American Civil War. With insufficient volunteers coming in by 1863, the Lincoln Administration was forced to begin drafting despite widespread complaints that it was unconstitutional and undermined states' rights (some states had conscripted men up to this point, but not the federal government). In July 1863, New York City erupted in the draft riots over the draft. However, anyone could get out by paying a $300 fee or hiring a substitute. Many conscripts and substitutes were criminals or men with debilitating health problem, and thus largely useless. The Confederate government had begun drafting men in early 1862.		Conscription was next used after the United States entered World War I in 1917. The first peacetime conscription came with the Selective Training and Service Act of 1940. When World War II ended, so did the draft. It was quickly reinstated with the Korean War and retained for the next 20 years, especially in the Vietnam War. Active conscription ("the draft") ended in 1973.		In 1979, President Jimmy Carter, a Democrat, brought back draft registration. All males up to the age of 26 are required to register with the Selective Service System, whose mission is "to provide manpower to the armed forces in an emergency" including a "Health Care Personnel Delivery System"[54] and "to run an Alternative Service Program for men classified as conscientious objectors during a draft." No one has been prosecuted for violating the conscription law in the USA since 1986, but registration is required for certain benefits such as federal college aid, or in certain states, state college aid, or even driver's license. Women do not currently register for Selective Service in the United States; however they may still enlist for voluntary service.		Armenia has compulsory military service for two years for males from 18 to 27 years old.		Austria has mandatory military service for all able bodied male citizens up to 35 years of age. Since 2006, the period of service has been six months. Conscientious objectors can join the civilian service (called Zivildienst) for nine months. A 12-month participation in the Austrian Holocaust Memorial Service, the Austrian Social Service or the Austrian Peace Service is regarded as an equivalent to the civilian service.		Since 1 January 1998, females can join the military service voluntarily. The Austrian conscription referendum, 2013 resulted in the rejection of a proposal that would have ended conscription. Although the referendum was non-binding, both parties in government pledged to honour the results.		Belarus has mandatory military service for all fit men from eighteen to twenty-seven years of age. Military service lasts for eighteen months for those without higher education, and for twelve months for those with higher education.		Azerbaijan has mandatory military service for all fit men, who are at least at the age of 18. Military service lasts for eighteen months for those without higher education, and for twelve months for those with higher education.		Bermuda, although an overseas territory of the United Kingdom, still maintains conscription for its local force. Males between the ages of eighteen and thirty-two are drawn by lottery to serve in The Bermuda Regiment for a period of thirty-eight months. The commitment is only on a part-time basis, however. Anyone who objects to this has the right to have his case heard by an exemption tribunal. The rights that applied for conscientious objection during National Service in the United Kingdom apply in Bermuda. The local government, as of 2013, has committed to ending conscription, although it is likely to be phased out gradually in order to prevent the manpower of the battalion (which had already seen its numbers fall below strength, from four to three companies, as a delayed result of birth rates decreasing following the Baby boom generation) plummeting.[55] Currently, three-quarters of the strength of the Bermuda Regiment is made up of conscripts, although many soldiers, whether they initially volunteered or were conscripted, elect to re-engage annually after their initial three years and two months term of service has been completed, with some serving for decades.[56]		Males in Brazil are required to serve 12 months of military service upon their 18th birthday. While de jure all males are required to serve, numerous exceptions mean military service is de facto limited mostly to volunteers, with an average of between 5 and 10% of those reporting for duty actually being inducted.[57] Most often, the service is performed in military bases as close as possible to the person's home. The government does not usually require those planning to attend college or holding a permanent job to serve. There are also several other exceptions, including health reasons, for which one may not have to serve. Recruits accepted at a university may also choose to train under a program similar to the American ROTC, and satisfy their military requirement this way. Direct entrance to one of the military academies will also substitute for this requirement.		The Burmese junta requires able-bodied persons aged 18 and over to register with local authorities.[58] In 2011, civil servants, students, those serving prison terms, and those caring for an elderly parent were excluded from the draft, but they could be later called to serve. Totally exempt are members of religious orders, disabled persons, and married or divorced women with children. Those who fail to report for military service could be imprisoned for three years, and face fines. Those who deliberately inflict injury upon themselves to avoid conscription could be imprisoned for up to five years, as well as fined.		But the conscription has never been activated under the military junta which ruled the country from 1988-2010. Myanmar's constitution states that male citizens over 18 could be called to serve in military, but it has not been activated either. As of 2013, Myanmar citizens are not required to serve in military.[citation needed]		Cyprus has compulsory military service for all Greek Cypriot men between the ages of seventeen and fifty. Additionally, from 2008 onwards, all men belonging to the religious groups of Armenians, Latins and Maronites, also serve their military service. Military service lasted for twenty-four months. From 2016 a new law was implied and military service now lasts for fourteen months. After that, ex-soldiers are considered reservists and participate in military exercises for a few days every year. Conscientious objectors can either do thirty-three months’ unarmed service in the army or thirty-eight months’ community work.[59] In 2016, however, the Cypriot parliament had voted to reduce its mandatory service to 14 months and make up for lost manpower by hiring professional soldiers.		As described in the Constitution of Denmark,[60] § 81, Denmark has mandatory service for all able men. Normal service is four months, and is normally served by men in the age of eighteen to twenty-seven. Some special services will take longer. Danish men will typically receive a letter around the time of their 18th birthday, asking when their current education (if any) ends, and some time later, depending on when, they will receive a notice on when to attend to the draft office to be tested physically and psychologically. However, some may be deemed unfit for service and not be required to show up.		Even if a person is deemed fit, or partially fit for service, he may avoid having to serve if he draws a high enough number randomly. Persons who are deemed partly fit for service will however be placed lower than those who are deemed fit for service, and therefore have a very low chance of being drafted. Men deemed fit can be called upon for service until their 50th birthday in case of national crisis, regardless of whether normal conscription has been served. This right is very rarely exercised by Danish authorities.		Conscientious objectors can choose to instead serve six months in a non-military position, for example in Beredskabsstyrelsen (dealing with non-military disasters like fires, flood, pollution, etc.) or foreign aid work in a third world country.[61]		Egypt has a mandatory military service program for males between the ages of eighteen and thirty. Conscription is regularly postponed for students until the end of their studies, as long as they apply before they turn twenty-eight years of age. By the age of thirty, a male is considered unfit to join the army and pays a fine. Males with no brothers, or those supporting parents are exempted from the service. Former President Sadat added that any Egyptian who has dual nationality is exempted from military service and this is still in effect. Males serve for a period ranging from fourteen months to thirty-six months, depending on their education; high school drop-outs serve for thirty-six months. College graduates serve for lesser periods of time, depending on their education; college graduates with special skills are still conscripted yet at a different rank and at a different pay scale with the option of remaining with the service as a career. Some Egyptians evade conscription and travel overseas until they reach the age of thirty, at which point they are tried, pay a $580 fine (as of 2004), and are dishonorably discharged. Such an offense, legally considered an offense of "bad moral character", prevents the "unpatriotic" citizen from ever holding public office.		Finland has mandatory military service for men of a minimum duration of five and half months (165 days); depending on the assigned position: those trained as officers or NCOs serve for eleven and half months (347 days), specialist troops serve for eight and half (255 days) or eleven and half months, while rank and file serve for the minimum period. Unarmed service is also possible, and lasts eight and half months (270 days) or eleven and half (347 days).[62] All males are required to participate in the drafting event (Finnish: kutsunnat) in their municipality of domicile in the year that they turn 18. The fitness for service and the actual induction to the service then takes place at the time and place decided individually for each conscript during the drafting event. The induction takes place usually at the age of 19 but the allowed age range is 18–29. The delayed induction is permissible for serious personal reasons, such as studies, but induction cannot be delayed beyond 29. The military strives to accommodate the wishes of the future conscript when determining the time of induction and the duty location, but these are ultimately determined by the needs of service.[63]		Since 1995, women have been able to volunteer for military service. During the first 45 days, women have an option to quit at will.[64] Having served for 45 days, they fall under the same obligation to serve as men except for medical reasons. A pregnancy during service would interrupt the service but not automatically cause a medical discharge.		Belonging in a sexual minority does not result in an exemption.[65] Transsexuals usually get their service postponed until they have undergone sex reassignment surgery.		Non-military service of twelve months is available for men whose conscience prevents them from serving in the military. Men who refuse to serve at all are sent to prison for six months or half the time of their remaining non-military service at the time of refusal. In theory, male citizens from the demilitarized Åland region are to serve in customs offices or lighthouses, but since this service has not been arranged, they are always exempted in practice. Jehovah's Witnesses' service is deferred for three years, if they present a written testimony, not older than two months, from the congregation of their status as baptized and active members of the congregation. Jehovah's Witnesses will be exempted from peace time duty at the beginning of the age 29. Military service has been mandatory for men throughout the history of independent Finland since 1917. Soldiers and civilian servicemen receive a daily allowance of €5 (days 1 – 165), €8.35 (days 165 – 255), or €11.70 (onward from day 255).[63]		Approximately 20% are trained as NCOs (corporals, sergeants), and 10% are trained as officers-in-reserve (second lieutenant). In wartime, it is expected that the officers-in-reserve fulfill most platoon leader and company commander positions. At the beginning of the service, all men go through same basic training of eight weeks. After this eight-week period it is decided who will be trained as NCOs or officers.		Having completed the initial part of the service as a conscript, the soldier is placed in the reserve. Reservists may be called for mandatory refresher exercises. Rank and file serve a maximum of 40 days, specialists 75 days and officers and NCOs 100 days. Per refresher course day, the reservists receive a taxable salary of about fifty euros. The salary depends slightly on the military rank: officers receive €56, NCOs €53 and rank-and file €51 per day. The service is mandatory; it is not possible to refuse an order to attend the refresher exercise, only to apply for a postponement of the service if personal or employer's urgent and non-avoidable needs require this. The postponement is not always granted. If the reservist experiences a crisis of conscience that prevemts him or her from further execution of military service, the reservist can apply for civilian service. The civilian service for reservists takes a form of a five-day course at Lapinjärvi Civilian Service Institution. After the course, the reservist is permanently freed of military service during peace and war.		There are no general exemptions for the conscription. The law requires employers, landlords etc. to continue any pre-existing contracts after the service. For medical reasons, exemption or postponing is given by the military authority after an examination by a military or military-appointed doctor. If the disability is expected to be cured, the exemption is temporary, and the service is postponed. If the disability continues until the male turns 25, he is exempted. The basic doctrine is that the great majority of each age cohort serve, and the size of the active army can be adjusted by changing the maximum age of reservists to be called up, instead of using selective service.		The option to military service is civilian service (available to females after completing 45 days of military service), where an individual subject to conscript finds a job at some public institution, where they serve for 11 and half months, the same as the longest rank-and-file service (drivers). Before 2008, the law required 13 months, which was criticized for being punitive internationally by the Human Rights Committee of the United Nations, The European Committee of Social Rights, The Commissioner for Human Rights of the Council of Europe and Amnesty International.[66]		The national security policy of Finland is based on a credible independent defence of all Finnish territory. The maximum number of military personnel abroad is limited to 2,000 (out of the 900,000 available reserve). Contributions to the UN troops comprise only professional soldiers and trained, paid reservists who have specifically applied to such operations. Therefore, no "expeditionary wars" argument can be made against conscription.		Draft dodging is nearly non-existent, as failure to show up to a drafting event immediately leads to an arrest warrant, and the delinquent is brought by the police to either to the drafting event (if still in progress) or to the regional military office for a physical examination and subsequent determination of induction time. Disobeying the induction order also causes the arrest warrant to be issued and is prosecuted as absence without leave, or after five days of absence, as desertion. Proof of military or civil service is generally required to obtain a 5-year passport (some exceptions can apply). Without proof, the passport is valid, at the longest, until the applicant's 28th birthday.[67]		Military rank, either as NCO or reserve officer, is highly valued as a merit in Finland by employers when recruiting a male employee.[68][69] In general, serving as an NCO or, especially, as a reserve officer, has a clear positive impact on future earnings and achievement of a high career position of the individual.[70][71]		As of 2009[update], Greece (Hellenic Republic) has mandatory military service of 9 months for men in the Army and 12 months for the Navy and Air Force. Some are entitled to reduced service due to serious family reasons (single parent families, parent serving in the army etc.). Although Greece is developing a professional army system, it continues to enforce the 9-month mandatory military service. It has been stated that the draft is to be reduced to six months in future. Women are accepted into the Greek army as salaried professionals, but are not obliged to mandatory conscript service. Conscript soldiers receive full health insurance and a nominal salary of €9 per month for privates and €12 for the rank of draft corporal and draft sergeant. There is the option of serving as a non-regular officer designate. In that case the received salary is €569 (with an additional €150 if the cadet is sent far from his home) after basic training is over. The duration of cadet training is roughly 4 months and 9 more months are dedicated for the actual service. Adding the 1 month spent in rookie training, a cadet's conscription will last a total of 14 months. In the last month of his service the cadet takes the rank of second lieutenant. The minimum wage for an unskilled worker stands at around €650 per month in Greece, while professional soldiers are paid upwards of €800. This results in reservist corporals and sergeants receiving a wage that is 1/70th that of a professional soldier, whom they outrank. This inconsistency was partly dealt with by abolishing the rank of sergeant for conscripts.		The length of alternative civilian service for conscientious objectors to military service is 15 months.[dubious – discuss] Amnesty International was also concerned that the determination of conscientious objector status fell under the jurisdiction of the Ministry of Defense, which breaches international standards that stipulate that the entire institution of alternative service should have a civilian character.		Iran has mandatory military service for men which starts at the age of 18. Duration of military service is normally 24 months but it can be also varying according to some conditions and circumstances. There is a 24-month military service for general, 22 months for destitute areas and 20 months for boundary areas; there is two months for military education. There are exceptions for those who cannot serve because of physical or mental health problems or disabilities. Students are exempt as long as they are attending school. The higher the education of a man, the higher his rank will be in the military service. Since 2008 and the commence of Iran's National Elites Foundation (Bonyade Mellie Nokhbegan[72]), students or university graduates who are accepted as members of this organization (because of their special achievements, e.g., recognized researchers with proven accomplishments, national and international olympiad medalists and winners of invention competitions) can have a "scientific research" substitution instead of mandatory military service, and the research grant is given to these members from military universities, otherwise, formally these members are regarded as "soldiers" who are spending the mandatory military service program, and in any publication related to that research, their citations have to be that of the military university giving the research grant. The 45-day mandatory military training is applicable even for those who are members of Iran's National Elites Foundation.		Exemptions from the Iranian military service, but also military duty in case of war include:		Prisoners may be excused of their sentence to serve in the military at a time of war or to complete military service in exchange for a reduced sentence dependent on the nature of the crime committed.[73]		Men reaching 19 years old who are not granted exemption from the military service are not able to apply for a drivers license, passport, any form of employment, leave the country or collect any completed academic certificate.		Israel drafts both men and women. All Israeli citizens are conscripted at age 18, with the following exceptions:		Typically, men are required to serve for 2 years and 8 months, while women for 2 years.[6] Officers and other soldiers in certain voluntary units such as Nahal and Hesder are required to sign on for additional service. Those studying in a "Mechina" (pre-induction preparatory course) defer service until the conclusion of the program, typically one academic year. An additional program (called "Atuda'i") for qualified applicants allows post-secondary academic studies prior to induction. See also: Israel Defense Forces.		There is a very limited amount of conscientious objection to conscription into the IDF. More common is refusal by reserve soldiers to serve in the West Bank and Gaza. Some of these conscientious objectors may be assigned to serve elsewhere, or are sentenced to brief prison terms lasting a few months to a year and may subsequently receive dishonourable discharges. See also: Refusal to serve in the Israeli military.		After a year their period of regular army service, men are liable for up to 30 days (much less on average) per year of reserve duty ("miluim") until they are in their early forties. Women in certain positions of responsibility are liable for reserve duty under the same terms as men, but are exempt once they are pregnant or with children.		North Korea in 1993 had a mandatory military service, for all men, of 120 months as normal troops and 156 months as special forces.		South Korea has mandatory military service of 21 (army, auxiliary police), 23 (navy) and 24 (air force, special civil service) months. There are no alternatives for conscientious objectors except imprisonment.[74] In general, with very few exceptions, most South Korean males serve in the military. The duration of service varies from branch to branch of the military.		Exemptions are granted to Korean male citizens with physical disabilities or whose mental status is unstable or questionable. When a Korean man becomes of legal age, he is required to take a physical check-up to determine whether he is suitable for military service.		There are some controversies portrayed in Korean media concerning special treatment given to celebrities. Some celebrities are given exemptions to their mandatory military service, even though they clearly have no physical disabilities. The government has begun implementing tougher sanctions to those who attempt to avoid their military duty. It is considered shameful, undutiful, and treasonous for a man to take measures to avoid his military service when he is healthy and capable of fulfilling his 21-month requirement.[citation needed] In 2002 Yoo Seung Jun, a Korean pop singer, became a naturalized American citizen to avoid his military duty in Korea. For this reason, Korea has banned Yoo from the country and actually deported him.[75][76] Another recent example is MC Mong, a popular singer/rapper accused of avoiding his military service by having his molar teeth removed. On 11 April 2011, MC Mong was sentenced to a suspended jail term of 6 months, probation for one year, and 120 hours of community service.[77]		The following data is from 'Regulation on Public Servant Compensation', implemented on 6 January 2012.[78] Exchange rate as of 8 June 2012 (₩1173.84 to $1.00)		As of 2011[update], all males reaching eighteen years of age must register for military service (Servicio Militar Nacional, or SMN) for one year, though selection is made by a lottery system using the following color scheme: whoever draws a black ball must serve as a "disponibility reservist", that is, he must not follow any activities whatsoever and get his discharge card at the end of the year. The ones who get a white ball serve Saturdays in a Batallón del Servicio Militar Nacional (National Military Service Battalion) composed entirely of one-year SMN conscripts. Those with a community service interest may participate in Literacy Campaigns as teachers or as physical education instructors. Military service is also (voluntarily) open to women. In certain cities, such as Mexico City and Veracruz, there is a third option: a red ball (Mexico City) and a Blue ball (Veracruz), which entails serving a full year as a recruit in a Paratrooper Battalion in the case of Mexico City residents, or an Infantería de Marina unit (Navy Marines) in Veracruz. In other cities which have a Navy HQ (such as Ciudad Madero), it is the Navy which takes charge of the conscripts, instead of the Army.		A "liberated" military ID is a requirement to join the Mexican local, state, and federal police forces, also to apply for some government jobs, Draft dodging was an uncommon occurrence in Mexico until 2002, since a "liberated" military ID card was needed for a Mexican male to obtain a passport, but since this requirement was dropped by the Mexican government, absenteeism from military service has become more common.		Norway has mandatory military service of nineteen months for men and women between the ages of 19 (18 in war time) and 44 (55 in case of officers and NCOs). The actual draft time is six months for the home guard, and twelve months for the regular army, air force and navy. In October 2014, Norway extended compulsory military service to women.[79]		The remaining months are supposed to be served in annual exercises, but very few conscripts do this because of lack of funding for the Norwegian Armed Forces. As a result of this decreased funding and greater reliance on high technology, the Armed Forces are aiming towards drafting only 10,000 conscripts a year. As of 2011[update], an average of 27% of conscripts actually complete military service each year.[80] The remainder, for the most part, either are formally dismissed after medical tests or obtain deferral from the service because of studies or stays abroad.		Some, such as those who choose vocational course paths during high school (for example, carpenters and electricians) can choose to complete their required apprenticeships within the military. While some Norwegians consider it unfair that they have to complete the compulsory military duty when so many others are dismissed, others see it as a privilege and there is normally high competition to be allowed to join some branches of the service.[citation needed]		The Norwegian Armed Forces will normally not draft a person who has reached the age of 28. In Norway, certain voluntary specialist training programs and courses entail extended conscription of one to eight years. Pacifists can apply for non-military service, which lasts 12 months.		The conscription system was introduced into Imperial Russia by Dmitry Milyutin on 1 January 1874. As of 2008, the Russian Federation has a mandatory 12 months draft. Some examples of how people avoid being drafted are:		In Russia, all males are liable for one year of compulsory military service up to the age of 27. In 2006, the Russian government and State Duma gradually reduced the term of service to 18 months from 24 for those who will be conscripted in 2007 and to 12 months from 2008 and dropped some legal excuses for non-conscription from the law (such as non-conscription of rural doctors and teachers, of men who have a child younger than 3 years, etc.) from 1 January 2008. Also full-time students graduated from civil university with military education will be free from conscription from 1 January 2008.		According to the Russian federal law, the Armed Forces, the National Guard, the Federal Protective Service (FSO), the Foreign Intelligence Service (SVR), and civil defence of the Ministry of Emergency Situations (EMERCOM) are considered as military service.		After Singapore gained its sovereign independence as an island-city nation, the NS (Amendment) Act was passed on 14 March 1967, under which all able-bodied male citizens and permanent residents of at least 18 years of age were obliged by law to serve 20 months of compulsory national service in the Singapore Armed Forces, the Singapore Police Force, or the Singapore Civil Defence Force to defend and protect the country as a sacred, honorable national duty above one's self. Upon completion of the mandatory active full-time NS, they will later also have reservist in-camp training cycles of up to 40 days annually over a 10-years period upon deployment to operationally-ready reservist units.[81]		The majority of conscripts serve in the SAF due to its larger manpower requirement. In practice, all conscripts undergo basic military training before being deployed to the various military units of the SAF, the Police Force (SPF), or Civil Defence (SCDF). During Basic Military Training, conscripts, known as National Servicemen, are assessed on their leadership capabilities (over and above basic military skills). All capable conscripts will undergo further vocational trainings to their trained roles and appointments for them to gain experience to move up the NS ranks.		In practice, conscripts cannot pick and choose their desired or preferred vocations due to manpower constraints and quotas. Since 2004, Singapore cut its mandatory military service period of 30 months to between 22 and 24 months, depending on medical health and physical fitness. NSmen make up >80% of its military defense system and form the backbone of the SAF. NSmen represent the collective will of Singapore to stand up for itself and to ensure the security of the nation.[81]		Military service for Swiss men is obligatory according to the Federal Constitution, and includes 18 or 21 weeks of basic training (depending on troop category) as well as annual 3-week-refresher courses until a number of service days which increases with rank (260 days for privates) is reached. (It is also possible to serve the whole requirement at one piece, meaning no refresher courses are required.) Service for women is voluntary, but identical in all respects. Conscientious objectors can choose 390 days of community service instead of military service. Medical deferments and dismissals from basic training (often on somewhat dubious grounds) lead to about 60% to 65% of Swiss men actually complete basic training. In 2013, the socialists wanted to replace the mandatory conscription for men, by a voluntary based service. By referundum, the Swiss population refused the project with more than 73% and decided to keep the centuries-old milicia tradition.[82]		The Republic of China has had mandatory military service for all males since 1949. Females from the outlying islands of Fuchien were also required to serve in a civil defense role, although this requirement has been dropped since the lifting of martial law. In October 1999, the mandatory service was shortened from twenty-four months to twenty-two months; from January 2004 it was shortened further to eighteen months, and from 1 January 2006 the duration has decreased to sixteen months. The ROC Defense Ministry had announced that should voluntary enlistment reach sufficient numbers, the compulsory service period for draftees will be shortened to fourteen months in 2007, and further to twelve months in 2009.[83][84]		ROC nationals with Overseas Chinese status are exempt from service. Draftees may also request alternative service, usually in community service areas, although the required service period would be longer than military service. Qualified draftees with graduate degrees in the sciences or engineering who pass officer candidate exams may also apply to fulfil their obligations in a national defense service option which involves three months of military training, followed by an officer commission in the reserves and four years working in technical jobs in the defense industry or government research institutions.		The Ministry of Interior is responsible for administering the National Conscription Agency.[85]		On 1 August 2008, the Defence Minister announced that from 2014 on, Taiwan would have a purely volunteer professional force. However, males who opt not to volunteer will be subjected to three to four-month military training. Those who do not have a tertiary education will have a three-month training when reaching military age, whereas those who are receiving tertiary education will have to complete the training in summer vacations.[86]		Should this policy remain unchanged, although Taiwan will have a purely volunteer professional force, every male will still be conscripted to receive a three- to four-month military training. Thus, after 2014, compulsory military service will still remain in practice in Taiwan.		The Military Service Act B.E. 2497 (1954) states that all male citizens of Thailand are obliged to serve in the military upon reaching 21 years of age. High school students have the option of enrolling in the three-year Reserve Officers Training Corps (ROTC) during Matthayom 4–6 (i.e., Grade 10–12). ROTC students drill at a local military installation once per week during the school year, with a field training exercise at the end of the third year. The ROTC program is operated nearly exclusively by the Royal Thai Army. Those who complete the three-year program are exempted from conscription and receive the rank of acting sergeant (E-6) upon graduation from high school. Students who do not complete the program or wish to enroll in the commissioned officer phase of the program can do so at their post-secondary institution. Those who do not complete the ROTC program will be required to report for conscription in early-April of the year in which they reach 21 years of age.[87]		Military service selection is done at a designated date and time at a local school or assembly hall. Each selection station has a quota for recruitment. The process begins with a call for volunteers. Those who volunteer will have the option to choose the branch of service and their date of induction. If the number of volunteers is fewer than the quota for the selection station, the remaining men will be asked to draw a card from an opaque box. The box contains red cards and black cards. Drawing a black card results in exemption from military service. Drawing a red card results in conscription in the branch of service and induction date on the card.		Those who volunteer for military service are free to choose from the three branches of the armed forces (Royal Thai Army, Royal Thai Navy, Royal Thai Air Force). Service obligation varies by educational qualification. Those with a high school diploma or the equivalent and those who have one year of military service education are required to serve for two years if they draw the red card, but if these same individuals volunteer, the service obligation is reduced by half, i.e., reduced to only one year. Those with an associate degree (or equivalent) or higher are required to serve for one year if drafted, but the requirement is reduced to only six months if they volunteered. University students can defer their service as long as they maintain the student status until reaching the age of 27 or obtaining a master's degree or the equivalent, whichever comes first. Undergraduate and graduate students who decide to volunteer are allowed by their institution to put their studies on suspension until the end of term of service. All conscripts, regardless of educational qualification, undergo the same training and receive the same grade and rank upon completion of basic training: private, seaman, or airman (E-1).		In recent years the government has issued new guidelines for better treatment of transgender recruits.[88]		In Turkey, compulsory military service applies to all male citizens from twenty to forty-one years of age (with some exceptions). Those who are engaged in higher education or vocational training programs prior to their military drafting are allowed to delay service until they have completed the programs, or reach a certain age, depending on the program (e.g. 29 years of age for undergraduate degrees). The duration of the basic military service varies. As of July 2003, the reduced durations are as follows: twelve months for privates (previously fifteen months), twelve months for reserve officers (previously sixteen months) and six months for short-term privates, which denotes those who have earned a university degree and have not been enlisted as reserve officers (previously eight months).		Turkish citizens who have lived or worked abroad for at least three years can be exempt from military service in exchange for a certain fee in foreign currencies. Also, when the General Staff assesses that the military reserve exceeds the required amount, paid military service of one-month's basic training is established by law as a stopgap measure, but has only been practiced in reality once so far, and only applied to men of a certain age (born in or prior to 1973). This was done in order to generate funds to recover from the aftermath of the 1999 İzmit earthquake, which took place in the highly industrialized Marmara region of the country, and had a considerable negative impact on the Turkish economy due to the severe damage it caused to a significant number of residential and industrial structures.		Although women in principle are not obliged to serve in the military, they are allowed to become military officers.		Conscientious objection of military service is illegal in Turkey and punishable with imprisonment by law. Many conscientious objectors flee abroad mainly to neighbouring countries or the European Union (as asylum seekers or guest workers).		The options are either reserve officer training for two years (offered in universities as a part of a program which means not having to join the army), or one-year regular service. In Ukraine, a person could not be conscripted after he turned 27 years of age. The Ukrainian army had similar problems with dedovshchina as the Russian army did until very recently, but in Ukraine the problem is getting less severe compared to Russia, due to cuts in the conscript terms (from 24 months to 18 months in the early 2000s and then to 12 months in 2004) and cuts in total conscription numbers (due to the switching of the army into a full-time professional army) since the last conscripts are being drafted at the end of 2013.[89]		The United Arab Emirates started its national service requirement in September 2014. This is the first time the UAE has required mandatory national service. It is compulsory for all male citizens over 18 and under 30 years of age to report for military service. Foreign male residents are not required to serve in the military service. It is optional for females to register for military service, and which they serve for 9 months.[90] Males who hold a high school diploma must complete 12-months of military service, whilst males who have not completed high school must complete two years.[90] All males must register for compulsory military service after graduating high school. However, males who obtain a high school graduation grade over 90% are able to postpone their military service until after graduation from a college. Males whose high school graduation grade is under 90% must register for military service and cannot go to college until it has been completed.[91]		Some religious organizations that either oppose militarism or are politically neutral prohibits their members from joining any program directly related to the armed forces, notable faiths are the Jehova's Witnesses, and numerous Neo-Pagan pacifist congregations.		
The 100 metres, or 100-metre dash, is a sprint race in track and field competitions. The shortest common outdoor running distance, it is one of the most popular and prestigious events in the sport of athletics. It has been contested at the Summer Olympics since 1896 for men and since 1928 for women.		The reigning 100 m Olympic champion is often named "the fastest runner in the world." The World Championships 100 metres has been contested since 1983. American Justin Gatlin and Tori Bowie are the reigning world champions; Usain Bolt and Elaine Thompson are the Olympic champions in the men's and women's 100 metres, respectively.		On an outdoor 400 metres running track, the 100 m is run on the home straight, with the start usually being set on an extension to make it a straight-line race. Runners begin in the starting blocks and the race begins when an official fires the starter's pistol. Sprinters typically reach top speed after somewhere between 50–60 m. Their speed then slows towards the finish line.		The 10-second barrier has historically been a barometer of fast men's performances, while the best female sprinters take eleven seconds or less to complete the race. The current men's world record is 9.58 seconds, set by Jamaica's Usain Bolt in 2009, while the women's world record of 10.49 seconds set by American Florence Griffith-Joyner in 1988 remains unbroken.		The 100 m (109.361 yards) emerged from the metrication of the 100 yards (91.44 m), a now defunct distance originally contested in English-speaking countries. The event is largely held outdoors as few indoor facilities have a 100 m straight.		US athletes have won the men's Olympic 100 metres title more times than any other country, 16 out of the 28 times that it has been run. US women have also dominated the event winning 9 out of 21 times.		At the start, some athletes play psychological games such as trying to be last to the starting blocks.[1][2][3]		At high level meets, the time between the gun and first kick against the starting block is measured electronically, via sensors built in the gun and the blocks. A reaction time less than 0.1 s is considered a false start. The 0.2-second interval accounts for the sum of the time it takes for the sound of the starter's pistol to reach the runners' ears, and the time they take to react to it.		For many years a sprinter was disqualified if responsible for two false starts individually. However, this rule allowed some major races to be restarted so many times that the sprinters started to lose focus. The next iteration of the rule, introduced in February 2003, meant that one false start was allowed among the field, but anyone responsible for a subsequent false start was disqualified.		This rule led to some sprinters deliberately false-starting to gain a psychological advantage: an individual with a slower reaction time might false-start, forcing the faster starters to wait and be sure of hearing the gun for the subsequent start, thereby losing some of their advantage. To avoid such abuse and to improve spectator enjoyment, the IAAF implemented a further change in the 2010 season – a false starting athlete now receives immediate disqualification.[4] This proposal was met with objections when first raised in 2005, on the grounds that it would not leave any room for innocent mistakes. Justin Gatlin commented, "Just a flinch or a leg cramp could cost you a year's worth of work."[5] The rule had a dramatic impact at the 2011 World Championships, when current world record holder Usain Bolt was disqualified.[6][7]		Runners normally reach their top speed just past the halfway point of the race and they progressively decelerate in the later stages of the race. Maintaining that top speed for as long as possible is a primary focus of training for the 100 m.[8] Pacing and running tactics do not play a significant role in the 100 m, as success in the event depends more on pure athletic qualities and technique.		The winner, by IAAF Competition Rules, is determined by the first athlete with his or her torso (not including limbs, head, or neck) over the nearer edge of the finish line.[9] When the placing of the athletes is not obvious, a photo finish is used to distinguish which runner was first to cross the line.		Climatic conditions, in particular air resistance, can affect performances in the 100 m. A strong head wind is very detrimental to performance, while a tail wind can improve performances significantly. For this reason, a maximum tail wind of 2.0 m/s is allowed for a 100 m performance to be considered eligible for records, or "wind legal."		Furthermore, sprint athletes perform better at high altitudes because of the thinner air, which provides less air resistance. In theory, the thinner air would also make breathing slightly more difficult (due to the partial pressure of oxygen being lower), but this difference is negligible for sprint distances where all the oxygen needed for the short dash is already in the muscles and bloodstream when the race starts. While there are no limitations on altitude, performances made at altitudes greater than 1000 m above sea level are marked with an "A."[10]		Only male sprinters have beaten the 100 m 10-second barrier, nearly all of them being of West African descent. Namibian (formerly South-West Africa) Frankie Fredericks became the first man of non-West African heritage to achieve the feat in 1991 and in 2003 Australia's Patrick Johnson (an Indigenous Australian with Irish heritage) became the first sub-10-second runner without an African background.[11][12][13][14]		In 2010, French sprinter Christophe Lemaitre became the first Caucasian to break the 10-second barrier. In the Prefontaine Classic 2015 Diamond League meet at Eugene, Su Bingtian ran a time of 9.99 seconds, becoming the first Asian athlete to officially break the 10-second barrier. In the 2015 Birmingham Grand Prix Diamond League meet, British athlete Adam Gemili, who is of mixed Iranian and Moroccan descent, ran a time of 9.97 seconds on home soil, becoming the first athlete with either North African or Middle Eastern heritage to break the ten-second barrier.		Top sprinters of differing ancestry, such as Christophe Lemaitre, are believed to be exceptions in that they too likely have the genes favourable for sprinting.[15] Colin Jackson, an athlete with mixed ethnic background and former world record holder in the 110 metre hurdles,[16] noted that both his parents were talented athletes and suggested that biological inheritance was the greatest influence, rather than any perceived racial factor. Furthermore, successful black role models in track events may reinforce the racial disparity.[17]		Major 100 m races, such as at the Olympic Games, attract much attention, particularly when the world record is thought to be within reach.		The men's world record has been improved upon twelve times since electronic timing became mandatory in 1977.[18] The current men's world record of 9.58 s is held by Usain Bolt of Jamaica, set at the 2009 World Athletics Championships final in Berlin, Germany on 16 August 2009, breaking his own previous world record by 0.11 s.[19] The current women's world record of 10.49 s was set by Florence Griffith-Joyner of the US, at the 1988 United States Olympic Trials in Indianapolis, Indiana, on 16 July 1988[20] breaking Evelyn Ashford's four-year-old world record by .27 seconds. The extraordinary nature of this result and those of several other sprinters in this race raised the possibility of a technical malfunction with the wind gauge which read at 0.0 m/s- a reading which was at complete odds to the windy conditions on the day with high wind speeds being recorded in all other sprints before and after this race as well as the parallel long jump runway at the time of the Griffith-Joyner performance. All scientific studies commissioned by the IAAF and independent organizations since have confirmed there was certainly an illegal tailwind of between 5 m/s - 7 m/s at the time. This should have annulled the legality of this result, although the IAAF has chosen not to take this course of action. The legitimate next best wind legal performance would therefore be Griffith-Joyner's 10.61s performance in the final the next day.[21]		Some records have been marred by prohibited drug use – in particular, the scandal at the 1988 Summer Olympics when the winner, Canadian Ben Johnson was stripped of his medal and world record.		Jim Hines, Ronnie Ray Smith and Charles Greene were the first to break the 10-second barrier in the 100 m, all on 20 June 1968, the Night of Speed. Hines also recorded the first legal electronically timed sub-10 second 100 m in winning the 100 metres at the 1968 Olympics. Bob Hayes ran a wind-assisted 9.91 seconds at the 1964 Olympics.		Updated 5 July 2015.[22]		Any performance with a following wind of more than 2.0 metres per second is not counted for record purposes. Below is a list of the fastest wind-assisted times (9.80 or better). Only times that are superior to legal bests are shown.		Below is a list of all other legal times equal or superior to 10.77:		Any performance with a following wind of more than 2.0 metres per second is not counted for record purposes. Below is a list of the fastest wind-assisted times (10.82 or better). Only times that are superior to legal bests are shown.		Updated 24 June 2017[update][39]		Updated 30 June 2017[update]		Updated 31 March 2017[update]		Updated 20 June 2015[update]		Updated to April 2017[53]		Updated to April 2017[59]		
U.S. Department of Defense		The United States Army (USA) is the largest branch of the United States Armed Forces and performs land-based military operations. It is one of the seven uniformed services of the United States and is designated as the Army of the United States in the United States Constitution, Article 2, Section 2, Clause 1 and United States Code, Title 10, Subtitle B, Chapter 301, Section 3001. As the oldest and most senior (in official precedence)[7] branch of the U.S. military, the modern U.S. Army has its roots in the Continental Army, which was formed (14 June 1775) to fight the American Revolutionary War (1775–1783)—before the U.S. was established as a country.[8] After the Revolutionary War, the Congress of the Confederation created the United States Army on 3 June 1784, to replace the disbanded Continental Army.[9][10] The United States Army considers itself descended from the Continental Army, and dates its institutional inception from the origin of that armed force in 1775.[8]		As a uniformed military service, the Army is part of the Department of the Army, which is one of the three military departments of the Department of Defense. The U.S. Army is headed by a civilian senior appointed civil servant, the Secretary of the Army (SECARMY), and by a chief military officer, the Chief of Staff of the Army (CSA) who is also a member of the Joint Chiefs of Staff. In the fiscal year 2017, the projected end strength for the Regular Army (USA) was 460,000 soldiers; the Army National Guard (ARNG) had 335,000 soldiers, and the United States Army Reserve (USAR) had 195,000 soldiers; the combined-component strength of the U.S. Army was 990,000 soldiers.[3] As a branch of the armed forces, the mission of the U.S. Army is "to fight and win our Nation's wars, by providing prompt, sustained, land dominance, across the full range of military operations and the spectrum of conflict, in support of combatant commanders."[11] The branch participates in conflicts worldwide and is the major ground-based offensive and defensive force of the United States.						The United States Army serves as the land-based branch of the U.S. Armed Forces. Section 3062 of Title 10, U.S. Code defines the purpose of the army as:[12][13]		The Continental Army was created on 14 June 1775 by the Continental Congress[14] as a unified army for the colonies to fight Great Britain, with George Washington appointed as its commander.[8][15][16][17] The army was initially led by men who had served in the British Army or colonial militias and who brought much of British military heritage with them. As the Revolutionary War progressed, French aid, resources, and military thinking influenced the new army. A number of European soldiers came on their own to help, such as Friedrich Wilhelm von Steuben, who taught Prussian Army tactics and organizational skills.		The army fought numerous pitched battles and in the South in 1780–81 sometimes used the Fabian strategy and hit-and-run tactics, hitting where the British were weakest, to wear down their forces. Washington led victories against the British at Trenton and Princeton, but lost a series of battles in the New York and New Jersey campaign in 1776 and the Philadelphia campaign in 1777. With a decisive victory at Yorktown, and the help of the French, the Continental Army prevailed against the British.		After the war, though, the Continental Army was quickly given land certificates and disbanded in a reflection of the republican distrust of standing armies. State militias became the new nation's sole ground army, with the exception of a regiment to guard the Western Frontier and one battery of artillery guarding West Point's arsenal. However, because of continuing conflict with Native Americans, it was soon realized that it was necessary to field a trained standing army. The Regular Army was at first very small, and after General St. Clair's defeat at the Battle of the Wabash, the Regular Army was reorganized as the Legion of the United States, which was established in 1791 and renamed the "United States Army" in 1796.		The War of 1812, the second and last war between the United States and Great Britain, had mixed results. The Army did not conquer Canada but it did destroy Native American resistance to expansion in the Old Northwest, and it validated U.S. independence by stopping two major British invasions in 1814 and 1815. After taking control of Lake Erie in 1813, the U.S.Army seized parts of western Upper Canada, burned York and defeated Tecumseh, which caused his Western Confederacy to collapse. Following U.S.victories in the Canadian province of Upper Canada, British troops, who had dubbed the U.S. Army "Regulars, by God!", were able to capture and burn Washington, which was defended by militia, in 1814. The regular army, however, proved they were professional and capable of defeating the British army during the invasions of Plattsburgh and Baltimore, prompting British agreement on the previously rejected terms of a status quo ante bellum. Two weeks after a treaty was signed (but not ratified), Andrew Jackson defeated the British in the Battle of New Orleans and Siege of Fort St. Philip, and became a national hero. U.S. troops and sailors captured HMS Cyane, Levant, and Penguin in the final engagements of the war. Per the treaty, both sides, the United States and Great Britain, returned to the geographical status quo. Both navies kept the warships they had seized during the conflict.		The army's major campaign against the Indians was fought in Florida against Seminoles. It took long wars (1818–58) to finally defeat the Seminoles and move them to Oklahoma. The usual strategy in Indian wars was to seize control of the Indians' winter food supply, but that was no use in Florida where there was no winter. The second strategy was to form alliances with other Indian tribes, but that too was useless because the Seminoles had destroyed all the other Indians when they entered Florida in the late eighteenth century.[18]		The U.S. Army fought and won the Mexican–American War (1846–1848), which was a defining event for both countries.[19] The U.S. victory resulted in acquisition of territory that eventually became all or parts of the states of California, Nevada, Utah, Colorado, Arizona, Wyoming and New Mexico.		The American Civil War was the costliest war for the U.S. in terms of casualties. After most slave states, located in the southern U.S., formed the Confederate States, the Confederate States Army, led by former U.S. Army officers, mobilized a large fraction of Southern white manpower. Forces of the United States (the "Union" or "the North") formed the Union Army, consisting of a small body of regular army units and a large body of volunteer units raised from every state, north and south, except South Carolina.[20]		For the first two years Confederate forces did well in set battles but lost control of the border states.[21] The Confederates had the advantage of defending a large territory in an area where disease caused twice as many deaths as combat. The Union pursued a strategy of seizing the coastline, blockading the ports, and taking control of the river systems. By 1863, the Confederacy was being strangled. Its eastern armies fought well, but the western armies were defeated one after another until the Union forces captured New Orleans in 1862 along with the Tennessee River. In the Vicksburg Campaign of 1862–1863, General Ulysses Grant seized the Mississippi River and cut off the Southwest. Grant took command of Union forces in 1864 and after a series of battles with very heavy casualties, he had General Robert E. Lee under siege in Richmond as General William T. Sherman captured Atlanta and marched through Georgia and the Carolinas. The Confederate capital was abandoned in April 1865 and Lee subsequently surrendered his army at Appomattox Court House; all other Confederate armies surrendered within a few months.		The war remains the deadliest conflict in American history, resulting in the deaths of 620,000 soldiers. Based on 1860 census figures, 8% of all white males aged 13 to 43 died in the war, including 6.4% in the North and 18% in the South.[22]		Following the Civil War, the U.S. Army had the mission of containing western tribes of Native Americans on the Indian reservations. They set up many forts, and engaged in the last of the American Indian Wars. U.S. Army troops also occupied several Southern states during the Reconstruction Era to protect freedmen.		The key battles of the Spanish–American War of 1898 were fought by the Navy. Using mostly new volunteers, the U.S. Army defeated Spain in land campaigns in Cuba and played the central role in the Philippine–American War.		Starting in 1910, the army began acquiring fixed-wing aircraft.[23] In 1910, Mexico was having a civil war, peasant rebels fighting government soldiers. The army was deployed to American towns near the border to ensure safety to lives and property. In 1916, Pancho Villa, a major rebel leader, attacked Columbus, New Mexico, prompting a U.S. intervention in Mexico until 7 February 1917. They fought the rebels and the Mexican federal troops until 1918.		The United States joined World War I in 1917 on the side of Britain, France, Russia, Italy and other allies. U.S. troops were sent to the Western Front and were involved in the last offensives that ended the war. With the armistice in November 1918, the army once again decreased its forces.		The United States joined World War II in December 1941 after the Japanese attack on Pearl Harbor. On the European front, U.S. Army troops formed a significant portion of the forces that captured North Africa and Sicily, and later fought in Italy. On D-Day, 6 June 1944, and in the subsequent liberation of Europe and defeat of Nazi Germany, millions of U.S. Army troops played a central role. In the Pacific War, U.S. Army soldiers participated alongside the United States Marine Corps in capturing the Pacific Islands from Japanese control. Following the Axis surrenders in May (Germany) and August (Japan) of 1945, army troops were deployed to Japan and Germany to occupy the two defeated nations. Two years after World War II, the Army Air Forces separated from the army to become the United States Air Force in September 1947. In 1948, the army was desegregated by order of President Harry S. Truman.		The end of World War II set the stage for the East–West confrontation known as the Cold War. With the outbreak of the Korean War, concerns over the defense of Western Europe rose. Two corps, V and VII, were reactivated under Seventh United States Army in 1950 and American strength in Europe rose from one division to four. Hundreds of thousands of U.S. troops remained stationed in West Germany, with others in Belgium, the Netherlands and the United Kingdom, until the 1990s in anticipation of a possible Soviet attack.[24]:minute 9:00-10:00		During the Cold War, American troops and their allies fought communist forces in Korea and Vietnam. The Korean War began in 1950, when the Soviets walked out of a U.N. Security Council meeting, removing their possible veto. Under a United Nations umbrella, hundreds of thousands of U.S. troops fought to prevent the takeover of South Korea by North Korea, and later, to invade the northern nation. After repeated advances and retreats by both sides, and the Chinese People's Volunteer Army's entry into the war, the Korean Armistice Agreement returned the peninsula to the status quo in 1953.		The Vietnam War is often regarded as a low point for the U.S. Army due to the use of drafted personnel, the unpopularity of the war with the American public, and frustrating restrictions placed on the military by American political leaders. While American forces had been stationed in the Republic of Vietnam since 1959, in intelligence and advising/training roles, they were not deployed in large numbers until 1965, after the Gulf of Tonkin Incident. American forces effectively established and maintained control of the "traditional" battlefield, however they struggled to counter the guerrilla hit and run tactics of the communist Viet Cong and the North Vietnamese Army. On a tactical level, American soldiers (and the U.S. military as a whole) did not lose a sizable battle.[25]		During the 1960s the Department of Defense continued to scrutinize the reserve forces and to question the number of divisions and brigades as well as the redundancy of maintaining two reserve components, the Army National Guard and the Army Reserve.[26] In 1967 Secretary of Defense Robert McNamara decided that 15 combat divisions in the Army National Guard were unnecessary and cut the number to eight divisions (one mechanized infantry, two armored, and five infantry), but increased the number of brigades from seven to 18 (one airborne, one armored, two mechanized infantry, and 14 infantry). The loss of the divisions did not sit well with the states. Their objections included the inadequate maneuver element mix for those that remained and the end to the practice of rotating divisional commands among the states that supported them. Under the proposal, the remaining division commanders were to reside in the state of the division base. No reduction, however, in total Army National Guard strength was to take place, which convinced the governors to accept the plan. The states reorganized their forces accordingly between 1 December 1967 and 1 May 1968.		The Total Force Policy was adopted by Chief of Staff of the Army General Creighton Abrams in the aftermath of the Vietnam War and involves treating the three components of the army – the Regular Army, the Army National Guard and the Army Reserve as a single force.[27] Believing that no U.S. president should be able to take the United States (and more specifically the U.S. Army) to war without the support of the American people, General Abrams intertwined the structure of the three components of the army in such a way as to make extended operations impossible, without the involvement of both the Army National Guard and the Army Reserve.[28]		The 1980s was mostly a decade of reorganization. The army converted to an all-volunteer force with greater emphasis on training and technology. The Goldwater-Nichols Act of 1986 created unified combatant commands bringing the army together with the other four military services under unified, geographically organized command structures. The army also played a role in the invasions of Grenada in 1983 (Operation Urgent Fury) and Panama in 1989 (Operation Just Cause).		By 1989 Germany was nearing reunification and the Cold War was coming to a close. Army leadership reacted by starting to plan for a reduction in strength. By November 1989 Pentagon briefers were laying out plans to reduce army end strength by 23%, from 750,000 to 580,000.[29] A number of incentives such as early retirement were used.		In 1990 Iraq invaded its smaller neighbor, Kuwait, and U.S. land forces, quickly deployed to assure the protection of Saudi Arabia. In January 1991 Operation Desert Storm commenced, a U.S.-led coalition which deployed over 500,000 troops, the bulk of them from U.S. Army formations, to drive out Iraqi forces. The campaign ended in total victory, as Western coalition forces routed the Iraqi Army, organized along Soviet lines, in just one hundred hours.		After Operation Desert Storm, the army did not see major combat operations for the remainder of the 1990s but did participate in a number of peacekeeping activities. In 1990 the Department of Defense issued guidance for "rebalancing" after a review of the Total Force Policy,[30] but in 2004, Air War College scholars concluded the guidance would reverse the Total Force Policy which is an "essential ingredient to the successful application of military force."[31]		On 11 September 2001, 53 Army civilians (47 employees and six contractors) and 22 soldiers were among the 125 victims killed in the Pentagon in a terrorist attack when American Airlines Flight 77 commandeered by five Al-Qaeda hijackers slammed into the western side of the building, as part of the September 11 attacks.[32] Lieutenant General Timothy Maude was the highest-ranking military official killed at the Pentagon, and the most senior U.S. Army officer killed by foreign action since the death of Lieutenant General Simon B. Buckner, Jr. on 18 June 1945, in the Battle of Okinawa during World War II.[33]		In response to the 11 September attacks, and as part of the Global War on Terror, U.S. and NATO forces invaded Afghanistan in October 2001, displacing the Taliban government. The U.S. Army also led the combined U.S. and allied invasion of Iraq in 2003. It served as the primary source for ground forces with its ability to sustain short and long-term deployment operations. In the following years the mission changed from conflict between regular militaries to counterinsurgency, resulting in the deaths of more than 4,000 U.S service members (as of March 2008) and injuries to thousands more.[34][35] 23,813 insurgents[36] were killed in Iraq between 2003–2011.		Until 2009 the army's chief modernization plan, its most ambitious since World War II,[37] was the FCS program. In 2009 many systems were canceled and the remaining were swept into the BCT modernization program.[38] In response to Budget sequestration in 2013 the army is planned to shrink to a size not seen since the WWII buildup.[39] From 2016 to 2017 the army retired hundreds of OH-58 Kiowa Warrior observation helicopters without an adequate successor.[40] The 2015 expenditure for Army research, development and acquisition changed from $32 billion projected in 2012 for FY15, to $21 billion for FY15 expected in 2014.[41] By 2017 the Brigade Modernization project was completed, and its headquarters, the Brigade Modernization Command, was renamed the Joint Modernization Command, or JMC, to reflect its evolving mission at TRADOC.[42] (TRADOC is the Army Command whose mission is to define the architecture and organization of the Army, to train and supply soldiers to FORSCOM, and to design hardware, as well as to define materiel for AMC. [43]:minutes 2:30-15:00 [24])		The task of organizing the U.S. Army commenced in 1775.[45] In the first one hundred years of its existence, the United States Army was maintained as a small peacetime force to man permanent forts and perform other non-wartime duties such as engineering and construction works. During times of war, the U.S. Army was augmented by the much larger United States Volunteers which were raised independently by various state governments. States also maintained full-time militias which could also be called into the service of the army.		By the twentieth century, the U.S. Army had mobilized the U.S. Volunteers on four separate occasions during each of the major wars of the nineteenth century. During World War I, the "National Army" was organized to fight the conflict, replacing the concept of U.S. Volunteers.[46] It was demobilized at the end of World War I, and was replaced by the Regular Army, the Organized Reserve Corps, and the State Militias. In the 1920s and 1930s, the "career" soldiers were known as the "Regular Army" with the "Enlisted Reserve Corps" and "Officer Reserve Corps" augmented to fill vacancies when needed.[47]		In 1941, the "Army of the United States" was founded to fight World War II. The Regular Army, Army of the United States, the National Guard, and Officer/Enlisted Reserve Corps (ORC and ERC) existed simultaneously. After World War II, the ORC and ERC were combined into the United States Army Reserve. The Army of the United States was re-established for the Korean War and Vietnam War and was demobilized upon the suspension of the draft.[47]		Currently, the army is divided into the Regular Army, the Army Reserve, and the Army National Guard.[46] The army is also divided into major branches such as Air Defense Artillery, Infantry, Aviation, Signal Corps, Corps of Engineers, and Armor. Before 1903 members of the National Guard were considered state soldiers unless federalized (i.e., activated) by the President. Since the Militia Act of 1903 all National Guard soldiers have held dual status: as National Guardsmen under the authority of the governor of their state or territory and, when activated, as a reserve of the U.S. Army under the authority of the President.		Since the adoption of the total force policy, in the aftermath of the Vietnam War, reserve component soldiers have taken a more active role in U.S. military operations. For example, Reserve and Guard units took part in the Gulf War, peacekeeping in Kosovo, Afghanistan, and the 2003 invasion of Iraq.		Headquarters, United States Department of the Army (HQDA):		Source: U.S. Army organization[66]		See Structure of the United States Army for detailed treatment of the history, components, administrative and operational structure, and the branches and functional areas of the Army.		The United States Army is made up of three components: the active component, the Regular Army; and two reserve components, the Army National Guard and the Army Reserve. Both reserve components are primarily composed of part-time soldiers who train once a month, known as battle assemblies or unit training assemblies (UTAs), and conduct two to three weeks of annual training each year. Both the Regular Army and the Army Reserve are organized under Title 10 of the United States Code, while the National Guard is organized under Title 32. While the Army National Guard is organized, trained and equipped as a component of the U.S. Army, when it is not in federal service it is under the command of individual state and territorial governors; the District of Columbia National Guard, however, reports to the U.S. President, not the district's mayor, even when not federalized. Any or all of the National Guard can be federalized by presidential order and against the governor's wishes.[67]		The army is led by a civilian Secretary of the Army, who has the statutory authority to conduct all the affairs of the army under the authority, direction and control of the Secretary of Defense.[68] The Chief of Staff of the Army, who is the highest-ranked military officer in the army, serves as the principal military adviser and executive agent for the Secretary of the Army, i.e., its service chief; and as a member of the Joint Chiefs of Staff, a body composed of the service chiefs from each of the four military services belonging to the Department of Defense who advise the President of the United States, the Secretary of Defense, and the National Security Council on operational military matters, under the guidance of the Chairman and Vice Chairman of the Joint Chiefs of Staff.[69][70] In 1986, the Goldwater–Nichols Act mandated that operational control of the services follows a chain of command from the President to the Secretary of Defense directly to the unified combatant commanders, who have control of all armed forces units in their geographic or function area of responsibility. Thus, the secretaries of the military departments (and their respective service chiefs underneath them) only have the responsibility to organize, train and equip their service components. The army provides trained forces to the combatant commanders for use as directed by the Secretary of Defense.[71]		By 2013, the army shifted to six geographical commands that align with the six geographical unified combatant commands (COCOM):		The army also transformed its base unit from divisions to brigades. Division lineage will be retained, but the divisional headquarters will be able to command any brigade, not just brigades that carry their divisional lineage. The central part of this plan is that each brigade will be modular, i.e., all brigades of the same type will be exactly the same, and thus any brigade can be commanded by any division. As specified before the 2013 end-strength re-definitions, the three major types of ground combat brigades are:		In addition, there are combat support and service support modular brigades. Combat support brigades include aviation (CAB) brigades, which will come in heavy and light varieties, fires (artillery) brigades (now transforms to division artillery), and battlefield surveillance brigades. Combat service support brigades include sustainment brigades and come in several varieties and serve the standard support role in an army.		The U.S. Army currently consists of 10 active divisions and one deployable division headquarters (7th Infantry Division) as well as several independent units. The force is in the process of contracting after several years of growth. In June 2013, the Army announced plans to downsize to 32 active combat brigade teams by 2015 to match a reduction in active duty strength to 490,000 soldiers. Army Chief of Staff Raymond Odierno projected that the Army was to shrink to "450,000 in the active component, 335,000 in the National Guard and 195,000 in U.S. Army Reserve" by 2018.[72] This plan however was scrapped by the new administration, and now the Army plans to grow by 16,000 soldiers to a total of 476,000 by October 2017. The National Guard and the Army Reserve will see a smaller expansion.[73][74]		Within the Army National Guard and United States Army Reserve there are a further 8 divisions, over 15 maneuver brigades, additional combat support and combat service support brigades, and independent cavalry, infantry, artillery, aviation, engineer, and support battalions. The Army Reserve in particular provides virtually all psychological operations and civil affairs units.		United States Army Forces Command (FORSCOM)		For a description of U.S. Army tactical organizational structure, see: a U.S. context, and also a global context.		United States Army Special Operations Command (Airborne) (USASOC):[80]		These are the U.S. Army ranks authorized for use today and their equivalent NATO designations. Although no living officer currently holds the rank of General of the Army, it is still authorized by Congress for use in wartime.		There are several paths to becoming a commissioned officer[81] including the United States Military Academy, Reserve Officers' Training Corps, and Officer Candidate School. Regardless of which road an officer takes, the insignia are the same. Certain professions, including physicians, pharmacists, nurses, lawyers, and chaplains are commissioned directly into the army and are designated by insignia unique to their staff community.		Most army commissioned officers are promoted based on an "up or out" system. The Defense Officer Personnel Management Act of 1980 establishes rules for timing of promotions and limits the number of officers that can serve at any given time.		Army regulations call for addressing all personnel with the rank of general as 'General (last name)' regardless of the number of stars. Likewise, both colonels and lieutenant colonels are addressed as 'Colonel (last name)' and first and second lieutenants as 'Lieutenant (last name).'[82]		Warrant officers[81] are single track, specialty officers with subject matter expertise in a particular area. They are initially appointed as warrant officers (in the rank of WO1) by the Secretary of the Army, but receive their commission upon promotion to chief warrant officer two (CW2).		By regulation, warrant officers are addressed as 'Mr. (last name)' or 'Ms. (last name)' by senior officers, and as "sir" or "ma'am" by all enlisted personnel.[82] However, many personnel address warrant officers as 'Chief (last name)' within their units regardless of rank.		Sergeants and corporals are referred to as NCOs, short for non-commissioned officers.[81][84] This distinguishes corporals from the more numerous specialists, who have the same pay grade but do not exercise leadership responsibilities.		Privates (E1 and E2) and privates first class (E3) are addressed as 'Private (last name)', specialists as 'Specialist (last name)', corporals as 'Corporal (last name)', and sergeants, staff sergeants, sergeants first class, and master sergeants all as 'Sergeant (last name).' First sergeants are addressed as 'First Sergeant (last name)', and sergeants major and command sergeants major are addressed as 'Sergeant Major (last name)'.[82]		Training in the U.S. Army is generally divided into two categories – individual and collective. Basic training consists of 10 weeks for most recruits followed by Advanced Individualized Training (AIT) where they receive training for their military occupational specialties (MOS). Some individuals MOSs range anywhere from 14–20 weeks of One Station Unit Training (OSUT), which combines Basic Training and AIT. The length of AIT school varies by the MOS The length of time spent in AIT depends on the MOS of the soldier, and some highly technical MOS training may require many months (e.g., foreign language translators). Depending on the needs of the army, Basic Combat Training for combat arms soldiers is conducted at a number of locations, but two of the longest-running are the Armor School and the Infantry School, both at Fort Benning, Georgia.		Following their basic and advanced training at the individual-level, soldiers may choose to continue their training and apply for an "additional skill identifier" (ASI). The ASI allows the army to take a wide-ranging MOS and focus it into a more specific MOS. For example, a combat medic, whose duties are to provide pre-hospital emergency treatment, may receive ASI training to become a cardiovascular specialist, a dialysis specialist, or even a licensed practical nurse. For commissioned officers, ASI training includes pre-commissioning training either at USMA, or via ROTC, or by completing OCS. After commissioning, officers undergo branch specific training at the Basic Officer Leaders Course, (formerly called Officer Basic Course), which varies in time and location according their future assignments. Further career development is available through the Army Correspondence Course Program.		Collective training at the unit level takes place at the unit's assigned station, but the most intensive training at higher echelons is conducted at the three combat training centers (CTC); the National Training Center (NTC) at Fort Irwin, California, the Joint Readiness Training Center (JRTC) at Fort Polk, Louisiana, and the Joint Multinational Training Center (JMRC) at the Hohenfels Training Area in Hohenfels, Germany. ARFORGEN is the Army Force Generation process approved in 2006 to meet the need to continuously replenish forces for deployment, at unit level, and for other echelons as required by the mission. Individual-level replenishment still requires training at a unit level, which is conducted at the continental US (CONUS) replacement center at Fort Bliss, in New Mexico and Texas, before their individual deployment.		The army employs various individual weapons to provide light firepower at short ranges. The most common weapons used by the army are the compact variant of the M16 rifle, the M4 carbine,[86] as well as the 7.62×51mm variant of the FN SCAR for Army Rangers. The primary sidearm in the U.S. Army is the 9 mm M9 pistol; the M11 pistol is also used. Both handguns are to be replaced by the M17[87] through the Modular Handgun System program.[88] Soldiers are also equipped with various hand grenades, such as the M67 fragmentation grenade and M18 smoke grenade.		Many units are supplemented with a variety of specialized weapons, including the M249 SAW (Squad Automatic Weapon), to provide suppressive fire at the squad level.[89] Indirect fire is provided by the M203 grenade launcher. The M1014 Joint Service Combat Shotgun or the Mossberg 590 Shotgun are used for door breaching and close-quarters combat. The M14EBR is used by designated marksmen. Snipers use the M107 Long Range Sniper Rifle, the M2010 Enhanced Sniper Rifle, and the M110 Semi-Automatic Sniper Rifle.		The army employs various crew-served weapons to provide heavy firepower at ranges exceeding that of individual weapons.		The M240 is the U.S. Army's standard Medium Machine Gun.[90] The M2 heavy machine gun is generally used as a vehicle-mounted machine gun. In the same way, the 40 mm MK 19 grenade machine gun is mainly used by motorized units.[91]		The U.S. Army uses three types of mortar for indirect fire support when heavier artillery may not be appropriate or available. The smallest of these is the 60 mm M224, normally assigned at the infantry company level.[92] At the next higher echelon, infantry battalions are typically supported by a section of 81 mm M252 mortars.[93] The largest mortar in the army's inventory is the 120 mm M120/M121, usually employed by mechanized units.[94]		Fire support for light infantry units is provided by towed howitzers, including the 105 mm M119A1[95] and the 155 mm M777 (which will replace the M198).[96]		The U.S. Army utilizes a variety of direct-fire rockets and missiles to provide infantry with an Anti-Armor Capability. The AT4 is an unguided projectile that can destroy armor and bunkers at ranges up to 500 meters. The FIM-92 Stinger is a shoulder-launched, heat seeking anti-aircraft missile. The FGM-148 Javelin and BGM-71 TOW are anti-tank guided missiles.		U.S. Army doctrine puts a premium on mechanized warfare. It fields the highest vehicle-to-soldier ratio in the world as of 2009.[97]		The army's most common vehicle is the High Mobility Multipurpose Wheeled Vehicle (HMMWV), commonly called the Humvee, which is capable of serving as a cargo/troop carrier, weapons platform, and ambulance, among many other roles.[98] While they operate a wide variety of combat support vehicles, one of the most common types centers on the family of HEMTT vehicles. The M1A2 Abrams is the army's main battle tank,[99] while the M2A3 Bradley is the standard infantry fighting vehicle.[100] Other vehicles include the Stryker,[101] and the M113 armored personnel carrier,[102] and multiple types of Mine Resistant Ambush Protected (MRAP) vehicles.		The Pentagon bought 25,000 MRAP vehicles since 2007 in 25 variants through rapid acquisition with no long-term plans for the platforms. The Army plans to divest 7,456 vehicles and retain 8,585. Of the total number of vehicles the Army will keep, 5,036 will be put in storage, 1,073 will be used for training, and the remainder will be spread across the active force. The Oshkosh M-ATV will be kept the most at 5,681 vehicles, as it is smaller and lighter than other MRAPs for off-road mobility. The other most retained vehicle will be the Navistar MaxxPro Dash with 2,633 vehicles, plus 301 Maxxpro ambulances. Thousands of other MRAPs like the Cougar, BAE Caiman, and larger MaxxPros will be disposed of.[103]		The U.S. Army's principal artillery weapons are the M109A6 Paladin self-propelled howitzer[104] and the M270 Multiple Launch Rocket System (MLRS),[105] both mounted on tracked platforms and assigned to heavy mechanized units.		While the United States Army Aviation Branch operates a few fixed-wing aircraft, it mainly operates several types of rotary-wing aircraft. These include the AH-64 Apache attack helicopter,[106] the OH-58D Kiowa Warrior armed reconnaissance/light attack helicopter,[107] the UH-60 Black Hawk utility tactical transport helicopter,[108] and the CH-47 Chinook heavy-lift transport helicopter.[109] Restructuring plans call for reduction of 750 aircraft and from 7 to 4 types.[110]		Under the Johnson-McConnell agreement of 1966, the Army agreed to limit its fixed-wing aviation role to administrative mission support (light unarmed aircraft which cannot operate from forward positions). For UAVs, the Army is deploying at least one company of drone MQ-1C Gray Eagles to each Active Army division.[111]		The Army Combat Uniform, or ACU, currently features a digital Universal Camouflage Pattern (UCP) and is designed for use in woodland, desert, and urban environments. However, soldiers operating in Afghanistan are being issued a fire-resistant ACU with the "MultiCam" pattern, officially known as Operation Enduring Freedom Camouflage Pattern or "OCP".[112]		The standard garrison service uniform is the Army Service Uniform, which functions as both a garrison uniform (when worn with a white shirt and necktie) and a dress uniform (when worn with a white shirt and either a necktie for parades or a bow tie for after six p.m. or black tie events).		The U.S. Army's black beret is no longer worn with the new ACU for garrison duty, having been permanently replaced with the patrol cap. After years of complaints that it wasn't suited well for most work conditions, Army Chief of Staff General Martin Dempsey eliminated it for wear with the ACU in June 2011. U.S. soldiers still wear berets who are currently in a unit in jump status, whether the wearer is parachute-qualified, or not (maroon beret), Members of the 75th Ranger Regiment and the Airborne and Ranger Training Brigade (tan beret), and Special Forces (rifle green beret) and may wear it with the Army Service Uniform for non-ceremonial functions. Unit commanders may still direct the wear of patrol caps in these units in training environments or motor pools.		The army has relied heavily on tents to provide the various facilities needed while on deployment. The most common tent uses for the military are as temporary barracks (sleeping quarters), DFAC buildings (dining facilities), forward operating bases (FOBs), after action review (AAR), tactical operations center (TOC), morale, welfare, and recreation (MWR) facilities, and security checkpoints. Furthermore, most of these tents are set up and operated through the support of Natick Soldier Systems Center.		The U.S. Army is beginning to use a more modern tent called the deployable rapid assembly shelter or DRASH. In 2008, DRASH became part of the Army's Standard Integrated Command Post System.[113]		Tomb of the Unknowns is a tomb that soldiers walk and salute every day in any weather.[114]		In November 2012, the United States Army developed a tactical 3D printing capability to allow it to rapidly manufacture critical components on the battlefield.[115]		
Cardiovascular disease (CVD) is a class of diseases that involve the heart or blood vessels.[2] Cardiovascular disease includes coronary artery diseases (CAD) such as angina and myocardial infarction (commonly known as a heart attack).[2] Other CVDs include stroke, heart failure, hypertensive heart disease, rheumatic heart disease, cardiomyopathy, heart arrhythmia, congenital heart disease, valvular heart disease, carditis, aortic aneurysms, peripheral artery disease, thromboembolic disease, and venous thrombosis.[2][3]		The underlying mechanisms vary depending on the disease in question. Coronary artery disease, stroke, and peripheral artery disease involve atherosclerosis. This may be caused by high blood pressure, smoking, diabetes, lack of exercise, obesity, high blood cholesterol, poor diet, and excessive alcohol consumption, among others. High blood pressure results in 13% of CVD deaths, while tobacco results in 9%, diabetes 6%, lack of exercise 6% and obesity 5%. Rheumatic heart disease may follow untreated strep throat.[2]		It is estimated that 90% of CVD is preventable.[5] Prevention of atherosclerosis involves improving risk factors through: healthy eating, exercise, avoidance of tobacco smoke and limiting alcohol intake.[2] Treating risk factors, such as high blood pressure, blood lipids and diabetes is also beneficial.[2] Treating people who have strep throat with antibiotics can decrease the risk of rheumatic heart disease.[6] The effect of the use of aspirin in people who are otherwise healthy is of unclear benefit.[7][8]		Cardiovascular diseases are the leading cause of death globally.[2] This is true in all areas of the world except Africa.[2] Together they resulted in 17.9 million deaths (32.1%) in 2015 up from 12.3 million (25.8%) in 1990.[4][3] Deaths, at a given age, from CVD are more common and have been increasing in much of the developing world, while rates have declined in most of the developed world since the 1970s.[9][10] Coronary artery disease and stroke account for 80% of CVD deaths in males and 75% of CVD deaths in females.[2] Most cardiovascular disease affects older adults. In the United States 11% of people between 20 and 40 have CVD, while 37% between 40 and 60, 71% of people between 60 and 80, and 85% of people over 80 have CVD.[1] The average age of death from coronary artery disease in the developed world is around 80 while it is around 68 in the developing world.[9] Disease onset is typically seven to ten years earlier in men as compared to women.[11]						There are many cardiovascular diseases involving the blood vessels. They are known as vascular diseases.		There are also many cardiovascular diseases that involve the heart.		There are many risk factors for heart diseases: age, gender, tobacco use, physical inactivity, excessive alcohol consumption, unhealthy diet, obesity, genetic predisposition and family history of cardiovascular disease, raised blood pressure (hypertension), raised blood sugar (diabetes mellitus), raised blood cholesterol (hyperlipidemia), psychosocial factors, poverty and low educational status, and air pollution.[13][14][15][16] While the individual contribution of each risk factor varies between different communities or ethnic groups the overall contribution of these risk factors is very consistent.[17] Some of these risk factors, such as age, gender or family history/genetic predisposition, are immutable; however, many important cardiovascular risk factors are modifiable by lifestyle change, social change, drug treatment (for example prevention of hypertension, hyperlipidemia, and diabetes).[18] People with obesity are at increased risk of atherosclerosis of the coronary arteries.[19]		Genetic factors influence the development of cardiovascular disease in men who are less than 55 years-old and in women who are less than 65 years old.[18] Cardiovascular disease in a person's parents increases their risk by 3 fold.[20] Multiple single nucleotide polymorphisms (SNP) have been found to be associated with cardiovascular disease in genetic association studies,[21][22] but usually their individual influence is small, and genetic contributions to cardiovascular disease are poorly understood.[22]		Age is by far the most important risk factor in developing cardiovascular or heart diseases, with approximately a tripling of risk with each decade of life.[23] Coronary fatty streaks can begin to form in adolescence.[24] It is estimated that 82 percent of people who die of coronary heart disease are 65 and older.[25] At the same time, the risk of stroke doubles every decade after age 55.[26]		Multiple explanations have been proposed to explain why age increases the risk of cardiovascular/heart diseases. One of them is related to serum cholesterol level.[27] In most populations, the serum total cholesterol level increases as age increases. In men, this increase levels off around age 45 to 50 years. In women, the increase continues sharply until age 60 to 65 years.[27]		Aging is also associated with changes in the mechanical and structural properties of the vascular wall, which leads to the loss of arterial elasticity and reduced arterial compliance and may subsequently lead to coronary artery disease.[28]		Men are at greater risk of heart disease than pre-menopausal women.[23][29] Once past menopause, it has been argued that a woman's risk is similar to a man's[29] although more recent data from the WHO and UN disputes this.[23] If a female has diabetes, she is more likely to develop heart disease than a male with diabetes.[30]		Coronary heart diseases are 2 to 5 times more common among middle-aged men than women.[27] In a study done by the World Health Organization, sex contributes to approximately 40% of the variation in sex ratios of coronary heart disease mortality.[31] Another study reports similar results finding that gender differences explains nearly half the risk associated with cardiovascular diseases[27] One of the proposed explanations for gender differences in cardiovascular diseases is hormonal difference.[27] Among women, estrogen is the predominant sex hormone. Estrogen may have protective effects on glucose metabolism and hemostatic system, and may have direct effect in improving endothelial cell function.[27] The production of estrogen decreases after menopause, and this may change the female lipid metabolism toward a more atherogenic form by decreasing the HDL cholesterol level while increasing LDL and total cholesterol levels.[27]		Among men and women, there are notable differences in body weight, height, body fat distribution, heart rate, stroke volume, and arterial compliance.[28] In the very elderly, age-related large artery pulsatility and stiffness is more pronounced among women than men.[28] This may be caused by the women's smaller body size and arterial dimensions which are independent of menopause.[28]		Cigarettes are the major form of smoked tobacco.[2] Risks to health from tobacco use result not only from direct consumption of tobacco, but also from exposure to second-hand smoke.[2] Approximately 10% of cardiovascular disease is attributed to smoking;[2] however, people who quit smoking by age 30 have almost as low a risk of death as never smokers.[32]		Insufficient physical activity (defined as less than 5 x 30 minutes of moderate activity per week, or less than 3 x 20 minutes of vigorous activity per week) is currently the fourth leading risk factor for mortality worldwide.[2] In 2008, 31.3% of adults aged 15 or older (28.2% men and 34.4% women) were insufficiently physically active.[2] The risk of ischemic heart disease and diabetes mellitus is reduced by almost a third in adults who participate in 150 minutes of moderate physical activity each week (or equivalent).[33] In addition, physical activity assists weight loss and improves blood glucose control, blood pressure, lipid profile and insulin sensitivity. These effects may, at least in part, explain its cardiovascular benefits.[2]		High dietary intakes of saturated fat, trans-fats and salt, and low intake of fruits, vegetables and fish are linked to cardiovascular risk, although whether all these associations are a cause is disputed. The World Health Organization attributes approximately 1.7 million deaths worldwide to low fruit and vegetable consumption.[2] The amount of dietary salt consumed is also an important determinant of blood pressure levels and overall cardiovascular risk.[2] Frequent consumption of high-energy foods, such as processed foods that are high in fats and sugars, promotes obesity and may increase cardiovascular risk.[2] A Cochrane review found that replacing saturated fat with polyunsaturated fat (plant based oils) reduced cardiovascular disease risk. Cutting down on saturated fat reduced risk of cardiovascular disease by 17% including heart disease and stroke.[34] High trans-fat intake has adverse effects on blood lipids and circulating inflammatory markers,[35] and elimination of trans-fat from diets has been widely advocated.[36] There is evidence that higher consumption of sugar is associated with higher blood pressure and unfavorable blood lipids,[37] and sugar intake also increases the risk of diabetes mellitus.[38] High consumption of processed meats is associated with an increased risk of cardiovascular disease, possibly in part due to increased dietary salt intake.[39]		The relationship between alcohol consumption and cardiovascular disease is complex, and may depend on the amount of alcohol consumed. There is a direct relationship between high levels of alcohol consumption and risk of cardiovascular disease.[2] Drinking at low levels without episodes of heavy drinking may be associated with a reduced risk of cardiovascular disease.[40] Overall alcohol consumption at the population level is associated with multiple health risks that exceed any potential benefits.[2][41]		Cardiovascular disease affects low- and middle-income countries even more than high-income countries.[42] There is relatively little information regarding social patterns of cardiovascular disease within low- and middle-income countries,[42] but within high-income countries low income and low educational status are consistently associated with greater risk of cardiovascular disease.[43] Policies that have resulted in increased socio-economic inequalities have been associated with greater subsequent socio-economic differences in cardiovascular disease[42] implying a cause and effect relationship. Psychosocial factors, environmental exposures, health behaviours, and health-care access and quality contribute to socio-economic differentials in cardiovascular disease. [44] The Commission on Social Determinants of Health recommended that more equal distributions of power, wealth, education, housing, environmental factors, nutrition, and health care were needed to address inequalities in cardiovascular disease and non-communicable diseases.[45]		Particulate matter has been studied for its short- and long-term exposure effects on cardiovascular disease. Currently, PM2.5 is the major focus, in which gradients are used to determine CVD risk. For every 10 μg/m3 of PM2.5 long-term exposure, there was an estimated 8–18% CVD mortality risk.[46] Women had a higher relative risk (RR) (1.42) for PM2.5 induced coronary artery disease than men (0.90) did.[46] Overall, long-term PM exposure increased rate of atherosclerosis and inflammation. In regards to short-term exposure (2 hours), every 25 μg/m3 of PM2.5 resulted in a 48% increase of CVD mortality risk.[47] In addition, after only 5 days of exposure, a rise in systolic (2.8 mmHg) and diastolic (2.7 mmHg) blood pressure occurred for every 10.5 μg/m3 of PM2.5.[47] Other research has implicated PM2.5 in irregular heart rhythm, reduced heart rate variability (decreased vagal tone), and most notably heart failure.[47][48] PM2.5 is also linked to carotid artery thickening and increased risk of acute myocardial infarction.[47][48]		Existing cardiovascular disease or a previous cardiovascular event, such as a heart attack or stroke, is the strongest predictor of a future cardiovascular event.[49] Age, sex, smoking, blood pressure, blood lipids and diabetes are important predictors of future cardiovascular disease in people who are not known to have cardiovascular disease.[50] These measures, and sometimes others, may be combined into composite risk scores to estimate an individual's future risk of cardiovascular disease.[49] Numerous risk scores exist although their respective merits are debated.[51] Other diagnostic tests and biomarkers remain under evaluation but currently these lack clear-cut evidence to support their routine use. They include family history, coronary artery calcification score, high sensitivity C-reactive protein (hs-CRP), ankle–brachial pressure index, lipoprotein subclasses and particle concentration, lipoprotein(a), apolipoproteins A-I and B, fibrinogen, white blood cell count, homocysteine, N-terminal pro B-type natriuretic peptide (NT-proBNP), and markers of kidney function.[52][53] High blood phosphorus is also linked to an increased risk.[54]		Little is known about the relationship between work and cardiovascular disease, but links have been established between certain toxins, extreme heat and cold, exposure to tobacco smoke, and mental health concerns such as stress and depression.[55]		A 2015 SBU-report looking at non-chemical factors found an association for those:[56]		Specifically the risk of stroke was also increased by exposure to ionizing radiation.[56] Hypertension develops more often in those who experience job strain and who have shift-work.[56] Differences between women and men in risk are small, however men risk suffering and dying of heart attacks or stroke twice as often as women during working life.[56]		A 2017 SBU report found evidence that workplace exposure to silica dust, engine exhaust or welding fumes is associated with heart disease.[57] Associations also exist for exposure to arsenic, benzopyrenes, lead, dynamite, carbon disulphide, carbon monoxide, metalworking fluids and occupational exposure to tobacco smoke.[57] Working with the electrolytic production of aluminium or the production of paper when the sulphate pulping process is used is associated with heart disease.[57] An association was also found between heart disease and exposure to compounds which are no longer permitted in certain work environments, such as phenoxy acids containing TCDD (dioxin) or asbestos.[57]		Workplace exposure to silica dust or asbestos is also associated with pulmonary heart disease.There is evidence that workplace exposure to lead, carbon disulphide, phenoxyacids containing TCDD, as well as working in an environment where aluminium is being electrolytically produced, is associated with stroke.[57]		As of 2017, evidence suggests that certain leukemia-associated mutations in blood cells may also lead to increased risk of cardiovascular disease. Several large-scale research projects looking at human genetic data have found a robust link between the presence of these mutations, a condition known as clonal hematopoiesis, and cardiovascular disease-related incidents and mortality.[58]		Population-based studies show that atherosclerosis, the major precursor of cardiovascular disease, begins in childhood. The Pathobiological Determinants of Atherosclerosis in Youth (PDAY) study demonstrated that intimal lesions appear in all the aortas and more than half of the right coronary arteries of youths aged 7–9 years.[60]		This is extremely important considering that 1 in 3 people die from complications attributable to atherosclerosis. In order to stem the tide, education and awareness that cardiovascular disease poses the greatest threat, and measures to prevent or reverse this disease must be taken.		Obesity and diabetes mellitus are often linked to cardiovascular disease,[61] as are a history of chronic kidney disease and hypercholesterolaemia.[62] In fact, cardiovascular disease is the most life-threatening of the diabetic complications and diabetics are two- to four-fold more likely to die of cardiovascular-related causes than nondiabetics.[63][64][65]		Screening ECGs (either at rest or with exercise) are not recommended in those without symptoms who are at low risk.[66] This includes those who are young without risk factors.[67] In those at higher risk the evidence for screening with ECGs is inconclusive.[66]		Additionally echocardiography, myocardial perfusion imaging, and cardiac stress testing is not recommended in those at low risk who do not have symptoms.[68]		Some biomarkers may add to conventional cardiovascular risk factors in predicting the risk of future cardiovascular disease; however, the clinical value of some biomarkers is questionable.[69][70]		The NIH recommends lipid testing in children beginning at the age of 2 if there is a family history of heart disease or lipid problems.[71] It is hoped that early testing will improve lifestyle factors in those at risk such as diet and exercise.[72]		Screening and selection for primary prevention interventions has traditionally been done through absolute risk using a variety of scores (ex. Framingham or Reynolds risk scores).[73] This stratification has separated people who receive the lifestyle interventions (generally lower and intermediate risk) from the medication (higher risk). The number and variety of risk scores available for use has multiplied, but their efficacy according to a 2016 review was unclear due to lack of external validation or impact analysis.[74] Risk stratification models often lack sensitivity for population groups and do not account for the large number of negative events among the intermediate and low risk groups.[73] As a result, future preventative screening appears to shift toward applying prevention according to randomized trial results of each intervention rather than large-scale risk assessment.		Up to 90% of cardiovascular disease may be preventable if established risk factors are avoided.[75][76] Currently practiced measures to prevent cardiovascular disease include:		Most guidelines recommend combining preventive strategies. A 2015 Cochrane Review found some evidence that interventions aiming to reduce more than one cardiovascular risk factor may have favourable effects on blood pressure, body mass index and waist circumference; however, evidence was limited and the authors were unable to draw firm conclusions on the effects on cardiovascular events and mortality.[104] For adults without a known diagnosis of hypertension, diabetes, hyperlipidemia, or cardiovascular disease, routine counseling to advise them to improve their diet and increase their physical activity has not been found to significantly alter behavior, and thus is not recommended.[105] Another Cochrane review suggested that simply providing people with a cardiovascular disease risk score may reduce cardiovascular disease risk factors by a small amount compared to usual care.[106] However, there was some uncertainty as to whether providing these scores had any effect on cardiovascular disease events. It is unclear whether or not dental care in those with periodontitis affects their risk of cardiovascular disease.[107]		A diet high in fruits and vegetables decreases the risk of cardiovascular disease and death.[108] Evidence suggests that the Mediterranean diet may improve cardiovascular outcomes.[109] There is also evidence that a Mediterranean diet may be more effective than a low-fat diet in bringing about long-term changes to cardiovascular risk factors (e.g., lower cholesterol level and blood pressure).[110] The DASH diet (high in nuts, fish, fruits and vegetables, and low in sweets, red meat and fat) has been shown to reduce blood pressure,[111] lower total and low density lipoprotein cholesterol[112] and improve metabolic syndrome;[113] but the long-term benefits outside the context of a clinical trial have been questioned.[114] A high fiber diet appears to lower the risk.[115]		Total fat intake does not appear to be an important risk factor.[116][117] A diet high in trans fatty acids, however, does increase rates of cardiovascular disease.[117][118] Worldwide, dietary guidelines recommend a reduction in saturated fat.[119] However, there are some questions around the effect of saturated fat on cardiovascular disease in the medical literature.[118][120] Reviews from 2014 and 2015 did not find evidence of harm from saturated fats.[118][120] A 2012 Cochrane review found suggestive evidence of a small benefit from replacing dietary saturated fat by unsaturated fat.[121] A 2013 meta analysis concludes that substitution with omega 6 linoleic acid (a type of unsaturated fat) may increase cardiovascular risk.[119] Replacement of saturated fats with carbohydrates does not change or may increase risk.[122][123] Benefits from replacement with polyunsaturated fat appears greatest;[117][124] however, supplementation with omega-3 fatty acids (a type of polysaturated fat) does not appear to have an effect.[125]		The effect of a low-salt diet is unclear. A Cochrane review concluded that any benefit in people with high or normal blood pressure is small if present.[126] In addition, the review suggested that a low-salt diet may be harmful in those with congestive heart failure.[126] However, the review was criticized in particular for not excluding a trial in heart failure where people had low-salt and -water levels due to diuretics.[127] When this study is left out, the rest of the trials show a trend to benefit.[127][128] Another review of dietary salt concluded that there is strong evidence that high dietary salt intake increases blood pressure and worsens hypertension, and that it increases the number of cardiovascular disease events; both as a result of the increased blood pressure and, quite likely, through other mechanisms.[129][130] Moderate evidence was found that high salt intake increases cardiovascular mortality; and some evidence was found for an increase in overall mortality, strokes, and left ventricular hypertrophy.[129]		Blood pressure medication reduces cardiovascular disease in people at risk,[90] irrespective of age,[131] the baseline level of cardiovascular risk,[132] or baseline blood pressure.[133] The commonly-used drug regimens have similar efficacy in reducing the risk of all major cardiovascular events, although there may be differences between drugs in their ability to prevent specific outcomes.[134] Larger reductions in blood pressure produce larger reductions in risk,[134] and most people with high blood pressure require more than one drug to achieve adequate reduction in blood pressure.[135]		Statins are effective in preventing further cardiovascular disease in people with a history of cardiovascular disease.[136] As the event rate is higher in men than in women, the decrease in events is more easily seen in men than women.[136] In those at risk, but without a history of cardiovascular disease (primary prevention), statins decrease the risk of death and combined fatal and non-fatal cardiovascular disease.[137] A United States guideline recommends statins in those who have a 12% or greater risk of cardiovascular disease over the next ten years.[138] Niacin, fibrates and CETP Inhibitors, while they may increase HDL cholesterol do not affect the risk of cardiovascular disease in those who are already on statins.[139]		Anti-diabetic medication may reduce cardiovascular risk in people with Type 2 Diabetes, although evidence is not conclusive.[140] A meta-analysis in 2009 including 27,049 participants and 2,370 major vascular events showed a 15% relative risk reduction in cardiovascular disease with more-intensive glucose lowering over an average follow-up period of 4.4 years, but an increased risk of major hypoglycemia.[141]		Aspirin has been found to be of only modest benefit in those at low risk of heart disease as the risk of serious bleeding is almost equal to the benefit with respect to cardiovascular problems.[142] In those at very low risk it is not recommended.[143] The United States Preventive Services Task Force recommends against use of aspirin for prevention in women less than 55 and men less than 45 years old; however, in those who are older it is recommends in some individuals.[144]		The use of vasoactive agents for people with pulmonary hypertension with left heart disease or hypoxemic lung diseases may cause harm and unnecessary expense.[145]		A systematic review estimated that inactivity is responsible for 6% of the burden of disease from coronary heart disease worldwide.[146] The authors estimated that 121,000 deaths from coronary heart disease could have been averted in Europe in 2008, if physical inactivity had been removed. A Cochrane review found some evidence that yoga has favourable effects on blood pressure and cholesterol, but studies included in this review were of low quality.[147]		While a healthy diet is beneficial, the effect of antioxidant supplementation (vitamin E, vitamin C, etc.) or vitamins has not been shown to protect against cardiovascular disease and in some cases may possibly result in harm.[148][149] Mineral supplements have also not been found to be useful.[150] Niacin, a type of vitamin B3, may be an exception with a modest decrease in the risk of cardiovascular events in those at high risk.[151][152] Magnesium supplementation lowers high blood pressure in a dose dependent manner.[153] Magnesium therapy is recommended for people with ventricular arrhythmia associated with torsades de pointes who present with long QT syndrome as well as for the treatment of people with digoxin intoxication-induced arrhythmias.[154] There is no evidence to support omega-3 fatty acid supplementation.[155]		Cardiovascular disease is treatable with initial treatment primarily focused on diet and lifestyle interventions.[2] Influenza may make heart attacks and strokes more likely and therefore influenza vaccination may decrease the chance of cardiovascular events and death in people with heart disease.[156]		Proper CVD management necessitates a focus on MI and stroke cases due to their combined high mortality rate, keeping in mind the cost-effectiveness of any intervention, especially in developing countries with low or middle income levels.[73] Regarding MI, strategies using aspirin, atenolol, streptokinase or tissue plasminogen activator have been compared for quality-adjusted life-year (QALY) in regions of low and middle income. The costs for a single QALY for aspirin, atenolol, streptokinase, and t-PA were $25, $630-$730, and $16,000, respectively. Aspirin, ACE inhibitors, beta blockers, and statins used together for secondary CVD prevention in the same regions showed single QALY costs of $300-400.		Cardiovascular diseases are the leading cause of death worldwide and in all regions except Africa.[158] In 2008, 30% of all global death was attributed to cardiovascular diseases. Death caused by cardiovascular diseases are also higher in low- and middle-income countries as over 80% of all global deaths caused by cardiovascular diseases occurred in those countries. It is also estimated that by 2030, over 23 million people will die from cardiovascular diseases each year.		It is estimated that 60% of the world's cardiovascular disease burden will occur in the South Asian subcontinent despite only accounting for 20% of the world's population. This may be secondary to a combination of genetic predisposition and environmental factors. Organizations such as the Indian Heart Association are working with the World Heart Federation to raise awareness about this issue.[159]		There is evidence that cardiovascular disease existed in pre-history,[160] and research into cardiovascular disease dates from at least the 18th century.[161] The causes, prevention, and/or treatment of all forms of cardiovascular disease remain active fields of biomedical research, with hundreds of scientific studies being published on a weekly basis.		Recent areas of research include the link between inflammation and atherosclerosis[162] the potential for novel therapeutic interventions,[163] and the genetics of coronary heart disease.[164]						
Hiking is the preferred term, in Canada and the United States, for a long, vigorous walk, usually on trails (footpaths), in the countryside, while the word walking is used for shorter, particularly urban walks. On the other hand, in the United Kingdom, and the Republic of Ireland, the word "walking" is acceptable to describe all forms of walking, whether it is a walk in the park or backpacking in the Alps. The word hiking is also often used in the UK, along with rambling (a slightly old-fashioned term), hillwalking, and fell walking (a term mostly used for hillwalking in northern England). The term "bushwalking" is endemic to Australia, having been adopted by the Sydney Bush Walkers club in 1927.[1] In New Zealand a long, vigorous walk or hike is called tramping.[2] It is a popular activity with numerous hiking organizations worldwide, and studies suggest that all forms of walking have health benefits.[3][4]						In the United States, Canada, the Republic of Ireland, and United Kingdom, hiking means walking outdoors on a trail, or off trail, for recreational purposes.[5] A day hike refers to a hike that can be completed in a single day. However, in the United Kingdom, the word walking is also used, as well as rambling, while walking in mountainous areas is called hillwalking. In Northern England, Including the Lake District and Yorkshire Dales, fellwalking describes hill or mountain walks, as fell is the common word for both features there.		Hiking sometimes involves bushwhacking and is sometimes referred to as such. This specifically refers to difficult walking through dense forest, undergrowth, or bushes, where forward progress requires pushing vegetation aside. In extreme cases of bushwhacking, where the vegetation is so dense that human passage is impeded, a machete is used to clear a pathway. The Australian term bushwalking refers to both on and off-trail hiking.[6] Common terms for hiking used by New Zealanders are tramping (particularly for overnight and longer trips),[7] walking or bushwalking. Trekking is the preferred word used to describe multi-day hiking in the mountainous regions of India, Pakistan, Nepal, North America, South America, Iran and in the highlands of East Africa. Hiking a long-distance trail from end-to-end is also referred to as trekking and as thru-hiking in some places.[8][9] In North America, multi-day hikes, usually with camping, are referred to as backpacking.[5]		The idea of taking a walk in the countryside for pleasure developed in the 18th-century, and arose because of changing attitudes to the landscape and nature, associated with the Romantic movement.[10] In earlier times walking generally indicated poverty and was also associated with vagrancy.[11]		Thomas West, an English priest, popularized the idea of walking for pleasure in his guide to the Lake District of 1778. In the introduction he wrote that he aimed		to encourage the taste of visiting the lakes by furnishing the traveller with a Guide; and for that purpose, the writer has here collected and laid before him, all the select stations and points of view, noticed by those authors who have last made the tour of the lakes, verified by his own repeated observations.[12]		To this end he included various 'stations' or viewpoints around the lakes, from which tourists would be encouraged to enjoy the views in terms of their aesthetic qualities.[13] Published in 1778 the book was a major success.[14]		Another famous early exponent of walking for pleasure, was the English poet William Wordsworth. In 1790 he embarked on an extended tour of France, Switzerland, and Germany, a journey subsequently recorded in his long autobiographical poem The Prelude (1850). His famous poem Tintern Abbey was inspired by a visit to the Wye Valley made during a walking tour of Wales in 1798 with his sister Dorothy Wordsworth. Wordsworth's friend Coleridge was another keen walker and in the autumn of 1799, he and Wordsworth undertook a three weeks tour of the Lake District. John Keats, who belonged to the next generation of Romantic poets began, in June 1818, a walking tour of Scotland, Ireland, and the Lake District with his friend Charles Armitage Brown.		More and more people undertook walking tours through the 19th-century, of which the most famous is probably Robert Louis Stevenson's journey through the Cévennes in France with a donkey, recorded in his Travels with a Donkey (1879). Stevenson also published in 1876 his famous essay "Walking Tours". The subgenre of travel writing produced many classics in the subsequent 20th-century. An early American example of a book that describes an extended walking tour is naturalist John Muir's A Thousand Mile Walk to the Gulf (1916), a posthumous published account of a long botanizing walk, undertaken in 1867.		Due to industrialisation in England, people began to migrate to the cities where living standards were often cramped and unsanitary. They would escape the confines of the city by rambling about in the countryside. However, the land in England, particularly around the urban areas of Manchester and Sheffield, was privately owned and trespass was illegal. Rambling clubs soon sprang up in the north and began politically campaigning for the legal 'right to roam'. One of the first such clubs, was 'Sunday Tramps' founded by Leslie White in 1879. The first national grouping, the Federation of Rambling Clubs, was formed in London in 1905 and was heavily patronized by the peerage.[15]		Access to Mountains bills, that would have legislated the public's 'right to roam' across some private land, were periodically presented to Parliament from 1884 to 1932 without success. Finally, in 1932, the Rambler’s Right Movement organized a mass trespass on Kinder Scout in Derbyshire. Despite attempts on the part of the police to prevent the trespass from going ahead it was successfully achieved due to massive publicity. However the Mountain Access Bill that was passed in 1939 was opposed by many walkers' organizations, including The Ramblers, who felt that it did not sufficiently protect their rights, and it was eventually repealed.[16]		The effort to improve access led after World War II to the National Parks and Access to the Countryside Act 1949, and in 1951 to the creation of the first national park in the UK, the Peak District National Park.[17] The establishment of this and similar national parks helped to improve access for all outdoors enthusiasts.[18] The Countryside and Rights of Way Act 2000 considerably extended the right to roam in England and Wales.		An early example of an interest in hiking in the United States, is Abel Crawford and his son Ethan's clearing of a trail to the summit of Mount Washington, New Hampshire in 1819.[19] This 8.5 mile path is the oldest continually used hiking trail in the United States. The influence of British and European Romanticism reached North America through the transcendentalist movement, and both Ralph Waldo Emerson (1803–82) and Henry David Thoreau (1817-62) were important influences on the outdoors movement in North America. Thoreau's writing on nature and on walking include the posthumously published "Walking" (1862)".[20] While an earlier essay "A Walk to Wachusett" (1842) describes a four-day walking tour he took with a companion from Concord, Massachusetts to the summit of Mount Wachusett, Princeton, Massachusetts and back. In 1876 the Appalachian Mountain Club, America’s earliest recreation organization, was founded to protect the trails and mountains in the northeastern United States.		The Scottish-born, American naturalist John Muir (1838 –1914), was another important early advocate of the preservation of wilderness in the United States. He petitioned the U.S. Congress for the National Park bill that was passed in 1890, establishing Yosemite and Sequoia National Parks. The Sierra Club, which he founded, is now one of the most important conservation organizations in the United States. The spiritual quality and enthusiasm toward nature expressed in his writings inspired others, including presidents and congressmen, to take action to help preserve large areas of undeveloped countryside.[21] He is today referred to as the "Father of the National Parks".[22] In 1916, the National Park Service was created to protect national parks and monuments.		In 1921, Benton MacKaye, a forester, conceived the idea of the America's first National trail, the Appalachian trail, and this was completed in August 1937, running from Sugarloaf Mountain in Maine to Georgia.[23] The Pacific Crest Trail ("PCT") was first explored in the 1930s by the YMCA hiking groups and was eventually registered as a complete border to border trail from Mexico to Canada.[24]		See also: National Park; National Parks of England and Wales; of Canada; of New Zealand, of South Africa, etc.		In Continental Europe amongst the most popular areas for hiking are the Alps, and in the United Kingdom the Lake District, Snowdonia, and the Scottish Highlands. In the US the National Park system generally is popular, whereas in Canada the Rockies of Alberta and British Columbia are the most popular hiking areas. The most visited hiking area in Asia is probably Nepal. The Inca Trail to Machu Picchu is possibly the most hiked short trail in South America.		Frequently nowadays long-distance hikes (walking tours) are undertaken along long-distance paths, including the National Trails in England and Wales, the Kungsleden (Sweden) and the National Trail System in the United States. The Grande Randonnée (France), Grote Routepaden, or Lange-afstand-wandelpaden (Holland), Grande Rota (Portugal), Gran Recorrido (Spain) is a network of long-distance footpaths in Europe, mostly in France, Belgium, the Netherlands and Spain. There are extensive networks in other European countries of long-distance trails, as well as in Canada, Australia, New Zealand, Nepal, and to a lesser extent other Asiatic countries, like Turkey, Israel, and Jordan. In the Alps of Austria, Slovenia, Switzerland, Germany, France, and Italy walking tours are often made from 'hut-to-hut', using an extensive system of mountain huts.		In the late 20th-century there has been a proliferation of official and unofficial long-distance routes, which mean that hikers now are more likely to refer to using a long-distance way (Britain), trail (US), The Grande Randonnée (France), etc., than setting out on a walking tour. Early examples of long-distance paths include the Appalachian Trail in the US and the Pennine Way in Britain. Pilgrimage routes are now treated, by some walkers, as long-distance routes, and the route taken by the British National Trail the North Downs Way closely follows that of the Pilgrims' Way to Canterbury.		The equipment required for hiking depends on the length of the hike, but day hikers generally carry at least water, food, a map, and rain-proof gear.[5] Hikers usually wear sturdy hiking boots for mountain walking and backpacking, as protection from the rough terrain, as well as providing increased stability.[5] The Mountaineers club recommends a list of "Ten Essentials" equipment for hiking, including a compass, a trekking pole, sunglasses, sunscreen, a flashlight, a first aid kit, a fire starter, and a knife.[26] Other groups recommend items such as hat, gloves, insect repellent, and an emergency blanket.[27] A GPS navigation device can also be helpful and route cards may be used as a guide.		Proponents of ultralight backpacking argue that long lists of required items for multi-day hikes increases pack weight, and hence fatigue and the chance of injury.[28] Instead, they recommend reducing pack weight, in order to make hiking long distances easier. Even the use of hiking boots on long-distances hikes is controversial among ultralight hikers, because of their weight.[28]		Hiking times can be estimated by Naismith's rule or Tobler's hiking function, While distances can and measured on a map with an opisometer. A pedometer is a device that records the distance walked.		Natural environments are often fragile, and may be accidentally damaged, especially when a large number of hikers are involved. For example, years of gathering wood can strip an alpine area of valuable nutrients, and can cause deforestation.[29] and some species, such as martens or bighorn sheep, are very sensitive to the presence of humans, especially around mating season. Generally, protected areas such as parks have regulations in place to protect the environment, so as to minimize such impact.[29] Such regulations include banning wood fires, restricting camping to established camp sites, disposing or packing out faecal matter, and imposing a quota on the number of hikers. Many hikers espouse the philosophy of Leave No Trace, following strict practices on dealing with food waste, food packaging, and other impact on the environment.[30]		Human waste is often a major source of environmental impact from hiking,[29] and can contaminate the watershed and make other hikers ill. 'Catholes' dug 10 to 25 cm (4 to 10 inches) deep, depending on local soil composition and covered after use, at least 60 m (200 feet) away from water sources and trails, are recommended to reduce the risk of bacterial contamination.		Fire is a particular source of danger, and an individual hiker can have a large impact on an ecosystem. For example, in 2005, a Czech backpacker burned 7% of Torres del Paine National Park in Chile by knocking over a portable stove.[31]		Sometimes the action of hikers may come into conflict with other users of the land. Hiking etiquette has developed to minimize such interference. Common hiking etiquette includes:		As discussed in Hazards of outdoor recreation, hiking may produce threats to personal safety, from such causes as hazardous terrain, inclement weather, becoming lost, or exacerbation of pre-existing medical conditions. These dangerous circumstances and/or specific accidents or ailments that hikers face may include, for example, diarrhea, one of the most common illnesses afflicting long-distance hikers in the United States.[33] (See Wilderness acquired diarrhea.)		Additional potential hazards involving physical ailments may include dehydration, frostbite, hypothermia, sunburn, or sunstroke, or such injuries as ankle sprains, or broken bones.[34]		Other threats may be posed attacks by animals (such as mammals (e.g., bears), reptiles (e.g., snakes), or insects) or contact with noxious plants that can cause rashes (e.g., poison ivy, poison oak, poison sumac, or stinging nettles). Attacks by humans are also a reality in some places, and lightning is also a threat, especially on high ground.		The crossing of glaciers is potentially hazardous because of the potential for crevasses. These giant cracks in the ice are not always visible as snow can be blown and freeze over the top to make a snowbridge. To cross a glacier the use of a rope, crampons and ice axes are usually required. Deep, fast flowing rivers pose another danger that can be mitigated with ropes.		In various countries, borders may be poorly marked. In 2009, Iran imprisoned three Americans for hiking across the Iran-Iraq border.[35] It is illegal to cross into the US on the Pacific Crest Trail from Canada. Going south to north it is more straightforward and a crossing can be made, if advanced arrangements are made with Canada Border Services. Within the Schengen Area, which includes most of the E.U., and associated nations like Switzerland and Norway, there are no impediments to crossing by path, and borders are not always obvious.[36]		See: List of long-distance footpaths		|group2 = See also |list2 =		|below = }}		
Aquajogging is a water sport and activity. Unlike a swimmer, an aquajogger moves in the water in an upright position. A purpose-made buoyancy belt is usually used to help maintain the position. Aquajogging is often done in a swimming pool.						Aquajogging started in America as an exercise method for pre- and post-surgery patients. It has proven a suitable form of exercise for elderly people and overweight people because of its low impact to the joints and its effectiveness due to the water resistance. This combination avoids muscle soreness, stress fractures and aching joints.		From the start of the 21st century, aquajogging has been increasingly popular as physical exercise and even competitive sport, outside its therapeutic use.		Aquajogging World Championships The Aquajogging World Championships are a yearly event, which has been organized since 2004 in inland. There are different categories: Relay (3 x 50 meter), Marathon (1000 meter) and individual competitions (50 meter).		The current world record for the relay is from 2006 by Katja and the Girls, Katja Backman, Jenni Salonen, Reetta Salminen in a time of 03:46 (min:sec). Jouni Laukkanen holds the world record at the marathon competition with 27:33 (min:sec) (2006). He also holds the world record for the individual competitions 00:54 (min:sec) (2007). Also in 2008, the Aquajogging World Championships will be held, this time it will be possible for anybody in the world to participate.		International records are held by the Netherlands, relay (3x50m) with a time of 06:59 (min:sec). Team: Dutch Association in Finland: Jos Helmich/Kaija Helmich/Marina Nijhuis. Sweden holds a record in the relay (3x50m) as well, with a time of 10:01,6 (team: Swedish aquajogging society: Pierre Lindgren, Eva Nordlander, Tanja Sergeeva). Germany has a record in the marathon by Erik Hass with a time of 35.09. Finally, Hanna Wierenga from the Netherlands holds the record in the individual competition for girls until 18 (25 meter) with a time of 00:56 (min:sec). [1]				
Long-distance running, or endurance running, is a form of continuous running over distances of at least three kilometres (1.86 miles). Physiologically, it is largely aerobic in nature and requires stamina as well as mental strength.[1]		Among mammals, humans are well adapted for running significant distances, and particularly so among primates. The endurance running hypothesis suggests that running endurance in the Homo genus arose because travelling over large areas improved scavenging opportunities and allowed persistence hunting. The capacity for endurance running is also found in migratory ungulates and a limited number of terrestrial carnivores, such as dogs, wolves and hyenas.[2][3]		In modern human society, long-distance running has multiple purposes: people may engage in it for physical exercise, for recreation, as a means of travel, for economic reasons, or for cultural reasons. Long distance running can also be used as a means to improve cardiovascular health.[4] Running improves aerobic fitness by increasing the activity of enzymes and hormones that stimulate the muscles and the heart to work more efficiently.[5] Endurance running is often a component of physical military training and has been so historically. Professional running is most commonly found in the field of sports, although in pre-industrial times foot messengers would run to deliver information to distant locations. Long-distance running as a form of tradition or ceremony is known among the Hopi and Tarahumara people, among others.[6][7] Distance running can also serve as a bonding exercise for family, friends,[8] colleagues, and has even been associated with nation-building.[9] The social element of distance running has been linked with improved performance.[10]		In the sport of athletics, long-distance events are defined as races covering three kilometres (1.86 miles) and above. The three most common types are track running, road running and cross country running, all of which are defined by their terrain – all-weather tracks, roads and natural terrain, respectively. Typical long-distance track races range from 3000 metres to 10,000 metres (6.2 miles), cross country races usually cover 5 to 12 km (3 to 7½ miles), while road races can be significantly longer, reaching 100 kilometres (60 miles) and beyond. In collegiate cross country races in the United States, men race 8000 or 10000 meters, depending on their division, whereas women race 6000 meters [2]. The Summer Olympics features three long-distance running events: the 5000 metres, 10,000 metres and marathon (42.195 kilometres, or 26 miles and 385 yards). Since the late 1980s, Kenyans, Moroccans and Ethiopians have dominated in major international long-distance competitions.[11]						Anthropological observations of modern hunter-gatherer communities have provided accounts for long distance running as a method for hunting among the San of the Kalahari,[12] American Indians,[13] and the Australian Aborigines.[14] In this method, the hunter would run at a slow and steady pace between one hour and a few days, in an area where the animal has no place to hide. The animal, running in spurts, has to stop to pant in order to cool itself, but as the chase goes on it would not have enough time before it has to start running again, and after a while would collapse from exhaustion and heat.[15] The body structure of a skeleton of a 12 years old Nariokatome boy is suggested to prove that early humans from 1.5 million years ago were eating more meat and less plants, and hunted by running down animals.[16][17]		With developments in agriculture and culture, long distance running took more and more purposes other than hunting: religious ceremonies, delivering messages for military and political purposes, and sport.[15]		The Old Testament has a few mentions of messengers running to deliver messages. For example, in 2 Samuel 18, two runners, Ahimaaz son of Zadok and a Cushite run to deliver King David the message of the death of his son Absalom. In Jeremia 51:31-32, two running messengers meet each other halfway to deliver the message about the loss of Babylon:		Running messengers are reported from early Sumer, were named lasimu[18] as military men as well as the king’s officials who disseminated documents throughout the kingdom by running.[19] Ancient Greece was famous for its running messengers, who were named hemerodromoi, meaning “day runners”.[20] One of the most famous running messengers is Pheidippides, who according to the legend ran from Marathon to Athens to announce the victory of the Greek over the Persians in the Battle of Marathon in 490 B.C. He collapsed and died as he delivered the message “we won”.[21] While there are debates around the accuracy of this historical legend,[22] whether Pheidippides actually ran from Marathon to Athens or between other cities, how far this was, and if he was the one to deliver the victory message,[23] the marathon running event of 26.2 miles / 42.195 km is based on this legend.		Humans are considered among the best distance runners among all running animals: game animals are faster over short distances, but they have less endurance than humans.[17] Unlike other primates whose bodies are designed to walk on four legs or climb trees, the human body has evolved into upright walking and running around 2-3 million years ago.[24] The human body can endure long distance running through the following attributes:		One distinction between upright walking and running is energy consumption during locomotion. While walking, humans use about half the energy needed to run.[25] Evolutionary biologists believe that the human ability to run over long distances has helped meat-eating humans to compete with other carnivores.[26] Persistence hunting is a method in which hunters use a combination of running, walking,[27] and tracking to pursue prey to the point of exhaustion. While humans can sweat to reduce body heat, their quadrupedal prey would need to slow from a gallop in order to pant.[28] The persistence hunt is still practised by hunter-gatherers in the central Kalahari Desert in Southern Africa, and David Attenborough's documentary The Life of Mammals (program 10, "Food For Thought") showed a bushman hunting a kudu antelope until it collapsed.[29]		The impact of long-distance running on human health is generally positive. Various organs and systems in the human body are improved: bone mineral density is increased,[30] cholesterol is lowered.[31] However, beyond a certain point, negative consequences might occur. Male runners who run more than 40 miles (64 kilometers) per week face reduced testosterone levels, although they are still in the normal range.[32] Running a marathon lowers testosterone levels by 50% in men, and more than doubles cortisol levels for 24 hours.[33] Low testosterone is thought to be a physiological adaptation to the sport, as excess muscle caused may be shed through lower testosterone, yielding a more efficient runner. Veteran, lifelong endurance athletes have been found to have more heart scarring than controls groups, but replication studies and larger studies should be done to firmly establish the link, which may or may not be causal.[34] Some studies find that running more than 20 miles (32 kilometers) per week yields no lower risk for all-cause mortality than non-runners,[35] however these studies are in conflict with large studies that show longer lifespans for any increase in exercise volume.[36]		Many sporting activities feature significant levels of running under prolonged periods of play, especially during ball sports like association football and rugby league. However, continuous endurance running is exclusively found in racing sports. Most of these are individual sports, although team and relay forms also exist.		The most prominent long-distance running sports are grouped within the sport of athletics, where running competitions are held on strictly defined courses and the fastest runner to complete the distance wins. The foremost types are long-distance track running, road running and cross-country running. Both track and road races are usually timed, while cross country races are not always timed and typically only the placing is of importance.[37] Other less popular variants such as fell running, trail running, mountain running and tower running combine the challenge of distance with a significant incline or change of elevation as part of the course.[38][39]		Multisport races frequently include endurance running. Triathlon, as defined by the International Triathlon Union, may feature running sections ranging from five kilometres (3.1 mi) to the marathon distance (42.195 kilometres, or 26 miles and 385 yards), depending on the race type.[40] The related sport of duathlon is a combination of cycling and distance running.[41] Previous versions of the modern pentathlon incorporated a three or four kilometre (1.9–2.5 mi) run, but changes to the official rules in 2008 meant the running sections are now divided into three separate legs of one kilometre each (0.6 mi).[42]		Depending on the rules and terrain, navigation sports such as foot orienteering and rogaining may contain periods of endurance running within the competition.[43] Variants of adventure racing may also combine navigational skills and endurance running in this manner.[44]		The history of long-distance track running events is tied into the track and field stadia where they are held. Oval circuits allow athletes to cover long distances in a confined area. Early tracks were usually on flattened earth or were simply marked areas of grass. The style of running tracks became refined during the 20th century: the oval running tracks were standardised to 400 metres in distance and cinder tracks were replaced by synthetic all-weather running track of asphalt and rubber from the mid-1960s onwards. It was not until the 1912 Stockholm Olympics that the standard long-distance track events of 5000 metres and 10,000 metres were introduced.		Long-distance road running competitions are mainly conducted on courses of paved or tarmac roads, although major events often finish on the track of a main stadium. In addition to being a common recreational sport, the elite level of the sport – particularly marathon races – are one of the most popular aspects of athletics. Road racing events can be of virtually any distance, but the most common and well known are the marathon, half marathon and 10 km run.		The sport of road running finds its roots in the activities of footmen: male servants who ran alongside the carriages of aristocrats around the 18th century, and who also ran errands over distances for their masters. Foot racing competitions evolved from wagers between aristocrats, who pitted their footman against that of another aristocrat in order to determine a winner. The sport became professionalised as footmen were hired specifically on their athletic ability and began to devote their lives to training for the gambling events. The amateur sports movement in the late 19th century marginalised competitions based on the professional, gambling model. The 1896 Summer Olympics saw the birth of the modern marathon and the event led to the growth of road running competitions through annual public events such as the Boston Marathon (first held in 1897) and the Lake Biwa Marathon and Fukuoka Marathons, which were established in the 1940s. The 1970s running boom in the United States made road running a common pastime and also increased its popularity at the elite level.[45]		The marathon is the only road running event featured at the IAAF World Championships in Athletics and the Summer Olympics, although there is also the IAAF World Half Marathon Championships held every two years. The marathon is also the only road running event featured at the IPC Athletics World Championships and the Summer Paralympics. The World Marathon Majors series includes the six most prestigious marathon competitions at the elite level – the Berlin, Boston, Chicago, London, Tokyo, and New York City marathons. The Tokyo Marathon was most recently added to the World Marathon Majors in 2012.[46] (See also: List of marathon races)		Ekiden contests – which originated in Japan and remain very popular there – are a relay race variation on the marathon, being in contrast to the typically individual sport of road running.		Cross country running is the most naturalistic form of long-distance running in athletics as competitions take place on open-air courses over surfaces such as grass, woodland trails, earth or mountains. In contrast to the relatively flat courses in track and road races, cross country usually incorporates obstacles such as muddy sections, logs and mounds of earth. As a result of these factors, weather can play an integral role in the racing conditions. Cross country is both an individual and team sport, as runners are judged on an individual basis and a points scoring method is used for teams. Competitions are typically races of 4 km (2.5 mi) or more which are usually held in autumn and winter. Cross country's most successful athletes often compete in long-distance track and road events as well.		The history of the sport is linked with the game of paper chase, or hare and hounds, where a group of runners would cover long distances to chase a leading runner, who left a trail of paper to follow. The Crick Run in England in 1838 was the first recorded instance of an organised cross country competition. The sport gained popularity in British, then American schools in the 19th century and culminated in the creation of the first International Cross Country Championships in 1903.[47] The annual IAAF World Cross Country Championships was inaugurated in 1973 and this remains the highest level of competition for the sport. A number of continental cross country competitions are held, with championships taking place in Africa, Asia, Europe, Oceania, North America and South America. The sport has retained its status at the scholastic level, particularly in the United Kingdom and United States. At the professional level, the foremost competitions come under the banner of the IAAF Cross Country Permit Meetings.		While cross country competitions are no longer held at the Olympics, having featured in the athletics programme from 1912–1924, it has been present as one of the events within the modern pentathlon competition since the 1912 Summer Olympics.		Fell running, trail running and mountain running can all be considered variations on traditional cross country which incorporate significant uphill and/or downhill sections as an additional challenge to the course.		A number of events, records and achievements exist for long distance running, outside the context of track and field sports events. These include multiday races, ultramarathons, and long distance races in extreme conditions or measuring hundreds or thousands of miles.		Beyond these, records and stand-alone achievements, rather than regular events, exist for individuals who have achieved running goals of a unique nature, such as running across or around continents (see lists of runners: America, Australia) or running around the world.		|group2 = See also |list2 =		|below = }}		
Interval training is a type of training that involves a series of low- to high-intensity workouts interspersed with rest or relief periods.[1] The high-intensity periods are typically at or close to anaerobic exercise, while the recovery periods involve activity of lower intensity.[2] Varying the intensity of effort exercises the heart muscle, providing a cardiovascular workout, improving aerobic capacity and permitting the person to exercise for longer and/or at more intense levels.[3]		Interval training can refer to the organization of any cardiovascular workout (e.g., cycling, running, rowing). It is prominent in training routines for many sports, but is particularly employed by runners.						Fartlek training, developed in Sweden, incorporates aspects of interval training with regular distance running. The name means 'speed play', and consists of distance running with "bursts of harder running at more irregular points, lengths and speeds compared with interval training".[2] For example, a Fartlek training session might consist of a warm-up for 5–10 minutes; running at a steady, hard speed for 2 km; rapid walking for 5 minutes (recovery); sprints of 50-60m interspersed with easy running; full-speed uphill for 200 m; rapid walking for one minute; repeating this routine until the time schedule has elapsed (a minimum of 45 minutes). [2] The development of aerobic and anaerobic capacities, and the adaptability of Fartlek - to mimic running during specific sports - are characteristics it shares with other types of interval training. [2]		"Walk-back sprinting" is one example of interval training for runners, in which one sprints a short distance (anywhere from 100 to 800 metres), then walks back to the starting point (the recovery period), to repeat the sprint a certain number of times. To add challenge to the workout, each of these sprints may start at predetermined time intervals - e.g. 200 metre sprint, walk back, and sprint again, every 3 minutes. The time interval is intended to provide just enough recovery time. A runner will use this method of training mainly to add speed to their race and give them a finishing kick.		High-intensity interval training attempts to decrease the overall volume of training by increasing the effort expended during the high-intensity intervals. The acronym DIRT is sometimes used to denote the variables : D = Distance of each speed interval, I = Interval of recovery between speed intervals, R = Repetitions of speed intervals, and T = Time of each.[4]		Some experts[who?] believe aerobic interval training may benefit exercisers by allowing them to burn more calories in a shorter period, and by improving aerobic capability at a faster rate, when compared with continuous-intensity exercise. In overweight and obese individuals, high intensity interval training employing 4 sets of 4-minute intervals has been shown to improve VO2max to a greater extent than isocaloric moderate continuous training, as well as to a greater extent than with a protocol using shorter, 1-minute intervals [5] Some exercisers find interval training less monotonous than continuous-intensity exercise.[6] A number of studies confirm that in young and healthy individuals, sprint interval training appears to be as effective as continuous endurance training of moderate intensity, and has the benefit of requiring a reduced time commitment.[7] There is some evidence that interval training is also beneficial for older individuals and for those with coronary artery disease, but further study is required.[7][8]		Interval training can improve many aspects of human physiology. In athletes, it can enhance lactate threshold and increase VO2max. Lactate threshold has been shown to be a significant factor in determining performance for long distance running events. An increase in an athlete's VO2max allows them to intake more oxygen while exercising, enhancing the capability to sustain larger spans of aerobic effort.[9][10] Studies have also shown interval training can induce endurance-like adaptions, corresponding to increased capacity for whole body and skeletal muscle lipid oxidation and enhanced peripheral vascular structure and function.[11]		There is increasing evidence that interval training assists in managing risk factors of many diseases, including metabolic syndrome, cardiovascular disease, obesity and diabetes. It does this by improving insulin action and sensitivity. Generating higher insulin sensitivity results in lower levels of insulin needed to lower glucose levels in the blood. This helps individuals with type 2 diabetes or metabolic syndrome control their glucose levels.[9][12] A combination of interval training and continuous exercise increases cardiovascular fitness and raises HDL-cholesterol, which reduces the risk of cardiovascular disease.[13] [14] This type of training also decreases waist circumference, waist-to-hip ratio(WRH), and the sum of skin folds on the body.[11]		This method of training may be more effective at inducing fat loss than simply training at a moderate intensity for the same duration. This is due to the metabolism-boosting effects of high intensity intervals.[15][16][17]		|group2 = See also |list2 =		|below = }}		
Endorphins (contracted from "endogenous morphine"[note 1]) are endogenous opioid neuropeptides and peptide hormones in humans and other animals. They are produced by the central nervous system and the pituitary gland. The term "endorphins" implies a pharmacological activity (analogous to the activity of the corticosteroid category of biochemicals) as opposed to a specific chemical formulation. It consists of two parts: endo- and -orphin; these are short forms of the words endogenous and morphine, intended to mean "a morphine-like substance originating from within the body".[2] The class of endorphin compounds includes α-endorphin, β-endorphin, γ-endorphin, σ-endorphin, α-neo-endorphin, and β-neo-endorphin. The principal function of endorphins is to inhibit the transmission of pain signals; they may also produce a feeling of euphoria very similar to that produced by other opioids.[3]						Opioid neuropeptides were first discovered in 1974 by two independent groups of investigators:		Endorphins are naturally produced in response to pain, but their production can also be triggered by various human activities. Vigorous aerobic exercise can stimulate the release of endorphins in the bloodstream, leading to an effect known as a "runner's high".[9][10] Laughter may also stimulate endorphin production; a 2011 study showed that attendees at a comedy club showed increased resistance to pain.[11]		Endorphins are suspected to play a role in depersonalization disorder. The opioid antagonists naloxone and naltrexone have both been proven to be successful in treating depersonalization.[12][13] To quote a 2001 naloxone study, "In [3] of 14 patients, depersonalization symptoms disappeared entirely, and [7] patients showed a marked improvement. The therapeutic effect of naloxone provides evidence for the role of the endogenous opioid system in the pathogenesis of depersonalization."[non-primary source needed]		From the words ἔνδον / Greek: éndon meaning "within" (endogenous, ἐνδογενής / Greek: endogenes, "proceeding from within") and morphine, from Morpheus (Μορφεύς / Ancient Greek: Morpheús, the god of dreams in the Greek mythology, thus 'endo(genous) (mo)rphine’.		
Cross-training is athletic training in sports other than the athlete's usual sport. The goal is improving overall performance. It takes advantage of the particular effectiveness of one training method to negate the shortcomings of another. The workout is in general performed in a circle and to music.						Cross-training in sports and fitness involves combining exercises to work various parts of the body. Often one particular activity works certain muscle groups, but not others; cross-training aims to eliminate this imbalance.		In Korea and Saudi Arabia, cross-training refers to training in multiple martial arts or fighting systems to become proficient in all the phases of unarmed combat. This training is meant[by whom?] to overcome the shortcomings of one style by practicing another style which is strong in the appropriate area. A typical combination involves a striking-based art such as Muay Thai, combined with a grappling-based art such as wrestling and Brazilian Jiu-Jitsu. Many hybrid martial arts can be considered[by whom?] derivatives of such cross-training - most notably Dan Inosanto's Jeet Kune Do concepts, a hybrid of Filipino martial arts, wing chun and savate,[1] Apolaki Krav Maga & Dirty Boxing, a hybrid martial-art blending Krav Maga, Filipino martial arts, silat and Brazilian jiu-jitsu and kajukenbo, an American hybrid art consisting of karate, tang soo do, jujutsu, kenpo, and boxing.		Modern mixed martial-arts training generally involves cross-training in the different aspects and ranges of fighting.[citation needed]		Cross-training in several military arts or specialties is one of the main distinguishing qualities of élite squads or battalions and special forces. The UK Royal Marines Commandos train using cross-training circuits.[citation needed]		In water sports, cross-training often involves doing exercises and training on land. This is often referred to[by whom?] as "dryland". For swimming, cross-training frequently includes running, stretching, and other resistance and agility training. Diving dryland exercises include various unique exercises such as on-land landing biomechanics training. [2]		|group2 = See also |list2 =		|below = }}		
Exercise equipment is any apparatus or device used during physical activity to enhance the strength or conditioning effects of that exercise by providing either fixed or adjustable amounts of resistance, or to otherwise enhance the experience or outcome of an exercise routine.		Exercise equipment may also include such wearable items as proper footgear, gloves, and hydration packs.						A broad range of different types of exercise equipment are available, including		|group2 = See also |list2 =		|below = }}		
Circuit training is a form of body conditioning or resistance training using high-intensity aerobics. It targets strength building or muscular endurance. An exercise "circuit" is one completion of all prescribed exercises in the program. When one circuit is complete, one begins the first exercise again for the next circuit. Traditionally, the time between exercises in circuit training is short, often with rapid movement to the next exercise.[citation needed]		The program was developed by R.E. Morgan and G.T. Adamson in 1957 at the University of Leeds in England.[1]						A circuit should work each section of the body individually. Typical activities include:[2]		Upper-body		Core & trunk		Lower-body		Total-body		Studies at Baylor University and The Cooper Institute show that circuit training is the most time efficient way to enhance cardiovascular fitness and muscle endurance. Studies show that circuit training helps women to achieve their goals and maintain them longer than other forms of exercise or diet.[3]		Morgan and Anderson claim:		Perhaps a most profound finding of this study, from a health perspective, is that this investigation clearly shows that performance of this circuit of exercises,this level of intensity elicited oxygen consumption values (39% to 51.5% of VO2max) that meet established guidelines of the American College of Sports Medicine (ACSM) for the recommended intensity (40% to 85% of VO2maxR) of exercise for developing and maintaining cardio-respiratory fitness.[4] Thus, this circuit not only provides a suitable muscular fitness stimulus but also helps to meet ACSM cardiovascular guidelines and the newly published Dietary Guidelines for Americans 2005 for physical activity.[1]		One advantage is that reduced station times will encourage the participants to lift heavier weights, which means they can achieve overload with smaller number of repetitions: typically in the range of 25 to 50 depending on their training goals.[5]		
Jogging is a form of trotting or running at a slow or leisurely pace. The main intention is to increase physical fitness with less stress on the body than from faster running, or to maintain a steady speed for longer periods of time. Performed over long distances, it is a form of aerobic endurance training.						Jogging is running at a gentle pace.[1] The definition of jogging as compared with running is not standard. One definition describes jogging as running slower than 6 miles per hour (10 km/h). Running is sometimes defined as requiring a moment of no contact to the ground, whereas jogging often sustains the contact.[2]		Jogging is also distinguished from running by having a wider lateral spacing of foot strikes, creating side-to-side movement that likely adds stability at slower speeds or when coordination is lacking.[citation needed]		The word jog originated in England in the mid-16th century.[3] The etymology of the word is unknown, but it may be related to shog or have been a new invention.[citation needed] In 1593, William Shakespeare wrote in Taming of the Shrew, "you may be jogging whiles your boots are green". At that point, it usually meant to leave.[4]		The term jog was often used in English and North American literature to describe short quick movements, either intentional or unintentional.[citation needed] It is also used to describe a quick, sharp shake or jar.[citation needed] Richard Jefferies, an English naturalist, wrote of "joggers", describing them as quickly moving people who brushed others aside as they passed.[5] This usage became common throughout the British Empire, and in his 1884 novel My Run Home, the Australian author Rolf Boldrewood wrote, "Your bedroom curtains were still drawn as I passed on my morning jog".		In the United States jogging was called "roadwork" when athletes in training, such as boxers, customarily ran several miles each day as part of their conditioning.[citation needed] In New Zealand during the 1960s or 1970s, the word "roadwork" was mostly supplanted by the word "jogging", promoted by coach Arthur Lydiard, who is credited with popularizing jogging. The idea of jogging as an organised activity was mooted in a sports page article in the New Zealand Herald in February 1962, which told of a group of former athletes and fitness enthusiasts who would meet once a week to run for "fitness and sociability". Since they would be jogging, the newspaper suggested that the club "may be called the Auckland Joggers' Club"—which is thought to be the first use of the noun "jogger". University of Oregon track coach Bill Bowerman, after jogging with Lydiard in New Zealand in 1962, published the book Jogging in 1966, popularizing jogging in the United States.		Jogging may also be used as a warm up or cool down for runners, preceding or following a workout or race. It is often used by serious runners as a means of active recovery during interval training. For example, a runner who completes a fast 400 metre repetition at a sub-5-minute mile pace may drop to an 8-minute mile jogging pace for a recovery lap.		Jogging can be used as a method to increase endurance or to provide a means of cardiovascular exercise but with less stress on joints or demand on the circulatory system.		According to a study by Stanford University School of Medicine, jogging is effective in increasing human lifespan, and decreasing the effects of aging,[6] with benefits for the cardiovascular system. Jogging is useful for fighting obesity and staying healthy.		The National Cancer Institute has performed studies that suggest jogging and other types of aerobic exercise can reduce the risk of lung, colon, breast and prostate cancers, among others.[7] It is suggested by the American Cancer Society that jogging for at least 30 minutes five days a week can help in cancer prevention.[8]		While jogging on a treadmill will provide health benefits such as cancer prevention, and aid in weight loss, a study published in BMC Public Health reports that jogging outdoors can have the additional benefits of increased energy and concentration. Jogging outdoors is a better way to improve energy levels and advance mood than using a treadmill at the gym.[9]		Jogging also prevents muscle and bone damage that often occurs with age, improves heart performance and blood circulation and assists in preserving a balanced weight gain.		A Danish study released in 2015 reported that "light" and "moderate" jogging were associated with reduced mortality compared to both non-jogging and "strenuous" jogging. The optimal amount per week was 1 to 2.4 hours, the optimal frequency was 2–3 times per week, and the optimal speed was "slow" or "average".[10]		
A military, is a force authorized to use lethal or deadly force and weapons to support the interests of the state and some or all of its citizens. It typically consists of an Army, Navy, Air Force, and in certain countries the Marines and Coast Guard. The task of the military is usually defined as defense of the state, and its citizens, and the prosecution of war against another state. The military may also have additional sanctioned and non-sanctioned functions within a society, including, the promotion of a political agenda, protecting corporate economic interests, internal population control, construction, emergency services, social ceremonies, and guarding important areas. The military may also function as a discrete subculture within a larger civil society, through the development of separate infrastructures, which may include housing, schools, utilities, logistics, health and medical, law, food production, finance and banking. In broad usage, the terms "armed forces" and "military" are often treated synonymously, although in technical usage a distinction is sometimes made in which a country's armed forces may include both its military and other paramilitary forces. Armed force is the use of armed forces to achieve political objectives. There are various forms of irregular military forces, not belonging to a recognized state; though they share many attributes with regular military forces, they are less often referred to as simply "military".		The profession of soldiering as part of a military is older than recorded history itself. Some of the most enduring images of the classical antiquity portray the power and feats of its military leaders. The Battle of Kadesh in 1274 BC was one of the defining points of Pharaoh Ramses II's reign, and is celebrated in bas-relief on his monuments. A thousand years later, the first emperor of unified China, Qin Shi Huang, was so determined to impress the gods with his military might, he was buried with an army of terracotta soldiers.[1] The Romans were dedicated to military matters, leaving to posterity many treatises and writings, as well as a large number of lavishly carved triumphal arches and victory columns.						The first recorded use of the word military in English, spelled militarie, was in 1585.[2] It comes from the Latin militaris (from Latin miles, meaning 'soldier') through French, but is of uncertain etymology, one suggestion being derived from *mil-it- – going in a body or mass.[3][4] The word is now identified as denoting someone that is skilled in use of weapons, or engaged in military service, or in warfare.[5][6]		As a noun, the military usually refers generally to a country's armed forces, or sometimes, more specifically, to the senior officers who command them.[5][6] In general, it refers to the physicality of armed forces, their personnel, equipment, and physical area which they occupy.		As an adjective, military originally referred only to soldiers and soldiering, but it soon broadened to apply to land forces in general, and anything to do with their profession.[2] The names of both the Royal Military Academy (1741) and United States Military Academy (1802) reflect this. However, at about the time of the Napoleonic Wars, 'military' began to be used in reference to armed forces as a whole,[2] and in the 21st century expressions like 'military service', 'military intelligence', and 'military history' encompass naval, marine and air force aspects. As such, it now connotes any activity performed by armed force personnel.		Military history is often considered to be the history of all conflicts, not just the history of the state militaries. It differs somewhat from the history of war, with military history focusing on the people and institutions of war-making, while the history of war focuses on the evolution of war itself in the face of changing technology, governments, and geography.		Military history has a number of facets. One main facet is to learn from past accomplishments and mistakes, so as to more effectively wage war in the future. Another is to create a sense of military tradition, which is used to create cohesive military forces. Still another may be to learn to prevent wars more effectively. Human knowledge about the military is largely based on both recorded and oral history of military conflicts (war), their participating armies and navies and, more recently, air forces.		There are two types of military history, although almost all texts have elements of both: descriptive history, that serves to chronicle conflicts without offering any statements about the causes, nature of conduct, the ending, and effects of a conflict; and analytical history, that seeks to offer statements about the causes, nature, ending, and aftermath of conflicts – as a means of deriving knowledge and understanding of conflicts as a whole, and prevent repetition of mistakes in future, to suggest better concepts or methods in employing forces, or to advocate the need for new technology.		In the whole history of humanity, every nation had different needs for military forces. How these needs are determined forms the basis of their composition, equipment, and use of facilities. It also determines what military does in terms of peacetime, and wartime activities.		All military forces, whether large or small, are military organizations that have official state, and world recognition as such. Organisations with similar features are paramilitary, civil defense, militia, or other – which are not military. These commonalities of the state's military define them.		Subordinated military personnel, generally known as soldiers, sailors, marines, or airmen, are capable of executing the many specialised operational missions and tasks required for the military to execute policy directives.		Just as in the commercial enterprises where there are, in a corporate setting, directors, managers and various staff that carry out the business of the day as part of business operations or undertake business project management, the military also has its routines and projects.		During peacetime, when military personnel are mostly employed in garrisons or permanent military facilities, they mostly conduct administrative tasks, training and education activities, and technology maintenance. Another role of military personnel is to ensure a continuous replacement of departing servicemen and women through military recruitment, and the maintenance of a military reserve.		The first requirement of the military is to establish it as a force with the capability to execute national defence policy. Invariably, although the policy may be created by policy makers or policy analyst, its implementation requires specific expert knowledge of how the military functions, and how it fulfils roles.		The first of these skills is the ability to create a cohesive force capable of acting on policy as and when required, and therefore the first function of the military is to provide military command. One of the roles of military command is to translate policy into concrete missions and tasks, and to express them in terms understood by subordinates, generally called orders.		Military command make effective and efficient military organisation possible through delegation of authority which encompass organisational structures as large as military districts or military zones, and as small as platoons or units. The command element of the military is often a strong influence on the organisational culture of the forces.		The next requirement comes as a fairly basic need for the military to identify possible threats it may be called upon to face. For this purpose, some of the commanding forces and other military, as well as often civilian personnel participate in identification of these threats. This is at once an organisation, a system and a process collectively called military intelligence (MI).		The difficulty in using military intelligence concepts and military intelligence methods is in the nature of the secrecy of the information they seek, and the clandestine nature that intelligence operatives work in obtaining what may be plans for a conflict escalation, initiation of combat, or an invasion.		An important part of the military intelligence role is the military analysis performed to assess military capability of potential future aggressors, and provide combat modelling that helps to understand factors on which comparison of forces can be made. This helps to quantify and qualify such statements as: "China and India maintain the largest armed forces in the World" or that "the U.S. Military is considered to be the world's strongest".[7]		Although some groups engaged in combat, such as militants or resistance movements, refer to themselves using military terminology, notably 'Army' or 'Front', none have had the structure of a national military to justify the reference, and usually have had to rely on support of outside national militaries. They also use these terms to conceal from the MI their true capabilities, and to impress potential ideological recruits.		Having military intelligence representatives participate in the execution of the national defence policy is important, because it becomes the first respondent and commentator on the policy expected strategic goal, compared to the realities of identified threats. When the intelligence reporting is compared to the policy, it becomes possible for the national leadership to consider allocating resources over and above the officers and their subordinates military pay, and the expense of maintaining military facilities and military support services for them.		Defense economics is the financial and monetary efforts made to resource and sustain militaries, and to finance military operations, including war.		The process of allocating resources is conducted by determining a military budget, which is administered by a military finance organisation within the military. Military procurement is then authorised to purchase or contract provision of goods and services to the military, whether in peacetime at a permanent base, or in a combat zone from local population.		Capability development, which is often referred to as the military 'strength', is arguably one of the most complex activities known to humanity; because it requires determining: strategic, operational, and tactical capability requirements to counter the identified threats; strategic, operational, and tactical doctrines by which the acquired capabilities will be used; identifying concepts, methods, and systems involved in executing the doctrines; creating design specifications for the manufacturers who would produce these in adequate quantity and quality for their use in combat; purchase the concepts, methods, and systems; create a forces structure that would use the concepts, methods, and systems most effectively and efficiently; integrate these concepts, methods, and systems into the force structure by providing military education, training, and practice that preferably resembles combat environment of intended use; create military logistics systems to allow continued and uninterrupted performance of military organisations under combat conditions, including provision of health services to the personnel, and maintenance for the equipment; the services to assist recovery of wounded personnel, and repair of damaged equipment; and finally, post-conflict demobilisation, and disposal of war stocks surplus to peacetime requirements.		Development of military doctrine is perhaps the more important of all capability development activities, because it determines how military forces were, and are used in conflicts, the concepts and methods used by the command to employ appropriately military skilled, armed and equipped personnel in achievement of the tangible goals and objectives of the war, campaign, battle, engagement, action or a duel.[9] The line between strategy and tactics is not easily blurred, although deciding which is being discussed had sometimes been a matter of personal judgement by some commentators, and military historians. The use of forces at the level of organisation between strategic and tactical is called operational mobility.		Because most of the concepts and methods used by the military, and many of its systems are not found in commercial branches, much of the material is researched, designed, developed, and offered for inclusion in arsenals by military science organisations within the overall structure of the military. Military scientists are therefore found to interact with all Arms and Services of the armed forces, and at all levels of the military hierarchy of command.		Although concerned with research into military psychology, and particularly combat stress, and how it affect troop morale, often the bulk of military science activities is directed at military intelligence technology, military communications, and improving military capability through research. The design, development, and prototyping of weapons, military support equipment, and military technology in general, is also an area in which lots of effort is invested – it includes everything from global communication networks and aircraft carriers to paint and food.		Possessing military capability is not sufficient if this capability cannot be deployed for, and employed in combat operations. To achieve this, military logistics are used for the logistics management and logistics planning of the forces military supply chain management, the consumables, and capital equipment of the troops.		Although mostly concerned with the military transport, as a means of delivery using different modes of transport; from military trucks, to container ships operating from permanent military base, it also involves creating field supply dumps at the rear of the combat zone, and even forward supply points in specific unit's Tactical Area of Responsibility.		These supply points are also used to provide military engineering services, such as the recovery of defective and derelict vehicles and weapons, maintenance of weapons in the field, the repair and field modification of weapons and equipment; and in peacetime, the life-extension programmes undertaken to allow continued use of equipment. One of the most important role of logistics is the supply of munitions as a primary type of consumable, their storage, and disposal.		While capability development is about enabling the military to perform its functions and roles in executing the defence policy, how personnel and their equipment are used in engaging the enemy, winning battles, successfully concluding campaigns, and eventually the war – is the responsibility of military operations. Military operations oversees the policy interpretation into military plans, allocation of capability to specific strategic, operational and tactical goals and objectives, change in posture of the armed forces, the interaction of Combat Arms, Combat Support Arms, and Combat Support Services during combat operations, defining of military missions and tasks during the conduct of combat, management of military prisoners and military civil affairs, and the military occupation of enemy territory, seizure of captured equipment, and maintenance of civil order in the territory under its responsibility. Throughout the combat operations process, and during the lulls in combat, combat military intelligence provides reporting on the status of plan completion, and its correlation with desired, expected and achieved satisfaction of policy fulfilment.		The last requirement of the military is for military performance assessment, and learning from it. These two functions are performed by military historians and military theorists who seek to identify failures and success of the armed force, and integrate corrections into the military reform, with the aim of producing an improved force capable of performing adequately, should there be a national defence policy review.		The primary reason for the existence of the military is to engage in combat, should it be required to do so by the national defence policy, and to win. This represents an organisational goal of any military, and the primary focus for military thought through military history. How victory is achieved, and what shape it assumes, is studied by most, if not all, military groups on three levels.		Military strategy is the management of forces in wars and military campaigns by a commander-in-chief, employing large military forces, either national and allied as a whole, or the component elements of armies, navies and air forces; such as army groups, naval fleets, and large numbers of aircraft. Military strategy is a long-term projection of belligerents' policy, with a broad view of outcome implications, including outside the concerns of military command. Military strategy is more concerned with the supply of war and planning, than management of field forces and combat between them. The scope of strategic military planning can span weeks, but is more often months or even years.[9]		Operational mobility is, within warfare and military doctrine, the level of command which coordinates the minute details of tactics with the overarching goals of strategy. A common synonym is operational art.		The operational level is at a scale bigger than one where line of sight and the time of day are important, and smaller than the strategic level, where production and politics are considerations. Formations are of the operational level if they are able to conduct operations on their own, and are of sufficient size to be directly handled or have a significant impact at the strategic level. This concept was pioneered by the German army prior to and during the Second World War. At this level, planning and duration of activities takes from one week to a month, and are executed by Field Armies and Army Corps and their naval and air equivalents.[9]		Military tactics concerns itself with the methods for engaging and defeating the enemy in direct combat. Military tactics are usually used by units over hours or days, and are focused on the specific, close proximity tasks and objectives of squadrons, companies, battalions, regiments, brigades, and divisions, and their naval and air force equivalents.[9]		One of the oldest military publications is The Art of War, by the Chinese philosopher Sun Tzu.[10] Written in the 6th century BCE, the 13-chapter book is intended as military instruction, and not as military theory, but has had a huge influence on Asian military doctrine, and from the late 19th century, on European and United States military planning. It has even been used to formulate business tactics, and can even be applied in social and political areas.[where?]		The Classical Greeks and the Romans wrote prolifically on military campaigning. Among the best-known Roman works are Julius Caesar's commentaries on the Gallic Wars, and the Roman Civil war – written about 50 BC.		Two major works on tactics come from the late Roman period: Taktike Theoria by Aelianus Tacticus, and De Re Militari ('On military matters') by Vegetius. Taktike Theoria examined Greek military tactics, and was most influential in the Byzantine world and during the Golden Age of Islam.		De Re Militari formed the basis of European military tactics until the late 17th century. Perhaps its most enduring maxim is Igitur qui desiderat pacem, praeparet bellum (let he who desires peace prepare for war).		Due to the changing nature of combat with the introduction of artillery in the European Middle Ages, and infantry firearms in the Renaissance, attempts were made to define and identify those strategies, grand tactics, and tactics that would produce a victory more often than that achieved by the Romans in praying to the gods before the battle.		Later this became known as military science, and later still, would adopt the scientific method approach to the conduct of military operations under the influence of the Industrial Revolution thinking. In his seminal book On War, the Prussian Major-General and leading expert on modern military strategy, Carl von Clausewitz defined military strategy as 'the employment of battles to gain the end of war'.[12] According to Clausewitz:		strategy forms the plan of the War, and to this end it links together the series of acts which are to lead to the final decision, that is to say, it makes the plans for the separate campaigns and regulates the combats to be fought in each.[13]		Hence, Clausewitz placed political aims above military goals, ensuring civilian control of the military. Military strategy was one of a triumvirate of 'arts' or 'sciences' that governed the conduct of warfare, the others being: military tactics, the execution of plans and manoeuvring of forces in battle, and maintenance of an army.		The meaning of military tactics has changed over time; from the deployment and manoeuvring of entire land armies on the fields of ancient battles, and galley fleets; to modern use of small unit ambushes, encirclements, bombardment attacks, frontal assaults, air assaults, hit-and-run tactics used mainly by guerrilla forces, and, in some cases, suicide attacks on land and at sea. Evolution of aerial warfare introduced its own air combat tactics. Often, military deception, in the form of military camouflage or misdirection using decoys, is used to confuse the enemy as a tactic.		A major development in infantry tactics came with the increased use of trench warfare in the 19th and 20th centuries. This was mainly employed in World War I in the Gallipoli campaign, and the Western Front. Trench warfare often turned to a stalemate, only broken by a large loss of life, because, in order to attack an enemy entrenchment, soldiers had to run through an exposed 'no man's land' under heavy fire from their opposing entrenched enemy.		As with any occupation, since the ancient times, the military has been distinguished from other members of the society by their tools, the military weapons, and military equipment used in combat. When Stone Age humans first took a sliver of flint to tip the spear, it was the first example of applying technology to improve the weapon.		Since then, the advances made by human societies, and that of weapons, has been irretrievably linked. Stone weapons gave way to Bronze Age weapons, and later, the Iron Age weapons. With each technological change, was realised some tangible increase in military capability, such as through greater effectiveness of a sharper edge in defeating leather armour, or improved density of materials used in manufacture of weapons.		On land, the first really significant technological advance in warfare was the development of the ranged weapons, and notably, the sling. The next significant advance came with the domestication of the horses and mastering of equestrianism.		Arguably, the greatest invention that affected not just the military, but all society, after adoption of fire, was the wheel, and its use in the construction of the chariot. There were no advances in military technology, until, from the mechanical arm action of a slinger, the Greeks, Egyptians, Romans, Persians, Chinese, etc., development the siege engines. The bow was manufactured in increasingly larger and more powerful versions, to increase both the weapon range, and armour penetration performance. These developed into the powerful composite and recurve bows, and crossbows of Ancient China. These proved particularly useful during the rise of cavalry, as horsemen encased in ever-more sophisticated armour came to dominate the battlefield.		Somewhat earlier, in medieval China, gunpowder had been invented, and was increasingly used by the military in combat. The use of gunpowder in the early vase-like mortars in Europe, and advanced versions of the long bow and cross bow, which all had armour-piercing arrowheads, that put an end to the dominance of the armoured knight. After the long bow, which required great skill and strength to use, the next most significant technological advance was the musket, which could be used effectively, with little training. In time, the successors to muskets and cannon, in the form of rifles and artillery, would become core battlefield technology.		As the speed of technological advances accelerated in civilian applications, so too warfare became more industralised. The newly invented machine gun and repeating rifle redefined firepower on the battlefield, and, in part, explains the high casualty rates of the American Civil War. The next breakthrough was the conversion of artillery parks from the muzzle loading guns, to the quicker loading breech loading guns with recoiling barrel that allowed quicker aimed fire and use of a shield. The widespread introduction of low smoke (smokeless) propellant powders since the 1880s also allowed for a great improvement of artillery ranges.		The development of breech loading had the greatest effect on naval warfare, for the first time since the Middle Ages, altering the way weapons are mounted on warships, and therefore naval tactics, now divorced from the reliance on sails with the invention of the internal combustion. A further advance in military naval technology was the design of the submarine, and its weapon, the torpedo.		Main battle tanks, and other heavy equipment such as armoured fighting vehicles, military aircraft, and ships, are characteristic to organised military forces.		During World War I, the need to break the deadlock of trench warfare saw the rapid development of many new technologies, particularly tanks. Military aviation was extensively used, and bombers became decisive in many battles of World War II, which marked the most frantic period of weapons development in history. Many new designs, and concepts were used in combat, and all existing technologies of warfare were improved between 1939 and 1945.		During the war, significant advances were made in military communications through increased use of radio, military intelligence through use of the radar, and in military medicine through use of penicillin, while in the air, the guided missile, jet aircraft, and helicopters were seen for the first time. Perhaps the most infamous of all military technologies was the creation of the atomic bomb, although the exact effects of its radiation were unknown until the early 1950s. Far greater use of military vehicles had finally eliminated the cavalry from the military force structure.		After World War II, with the onset of the Cold War, the constant technological development of new weapons was institutionalised, as participants engaged in a constant 'arms race' in capability development. This constant state of weapons development continues into the present, and remains a constant drain on national resources, which some[who?] blame on the military-industrial complex.		The most significant technological developments that influenced combat have been the guided missiles, which can be used by all branches of the armed services. More recently, information technology, and its use in surveillance, including space-based reconnaissance systems, have played an increasing role in military operations.		The impact of information warfare that focuses on attacking command communication systems, and military databases, has been coupled with the new development in military technology, has been the use of robotic systems in intelligence combat, both in hardware and software applications.		Recently, there has also been a particular focus towards the use of renewable fuels for running military vehicles on. Unlike fossil fuels, renewable fuels can be produced in any country, creating a strategic advantage. The US military has already committed itself to have 50% of its energy consumption come from alternative sources.[14]		For much of military history, the armed forces were considered to be for use by the heads of their societies, until recently, the crowned heads of states. In a democracy or other political system run in the public interest, it is a public force.		The relationship between the military and the society it serves is a complicated and ever-evolving one. Much depends on the nature of the society itself, and whether it sees the military as important, as for example in time of threat or war, or a burdensome expense typified by defence cuts in time of peace.		One difficult matter in the relation between military and society is control and transparency. In some countries, limited information on military operations and budgeting is accessible for the public. However transparency in the military sector is crucial to fight corruption. This showed the Government Defence Anti-corruption Index Transparency International UK published in 2013.[15]		These relationships are seen from the perspective of political-military relations, the military-industrial complex mentioned above, and the socio-military relationship. The last can be divided between those segments of society that offer support for the military, those who voice opposition to the military, the voluntary and involuntary civilians in the military forces, the populations of civilians in a combat zone, and of course the military's self-perception.		Militaries often function as societies within societies, by having their own military communities, economies, education, medicine, and other aspects of a functioning civilian society. Although a 'military' is not limited to nations in of itself as many private military companies (or PMC's) can be used or 'hired' by organisations and figures as security, escort, or other means of protection; where police, agencies, or militaries are absent or not trusted.		Militarist ideology is the society's social attitude of being best served, or being a beneficiary of a government, or guided by concepts embodied in the military culture, doctrine, system, or leaders.		Either because of the cultural memory, national history, or the potentiality of a military threat, the militarist argument asserts that a civilian population is dependent upon, and thereby subservient to the needs and goals of its military for continued independence. Militarism is sometimes contrasted with the concepts of comprehensive national power, soft power and hard power.		Most nations have separate military laws which regulate conduct in war and during peacetime. An early exponent was Hugo Grotius, whose On the Law of War and Peace (1625) had a major impact of the humanitarian approach to warfare development. His theme was echoed by Gustavus Adolphus.		Ethics of warfare have developed since 1945, to create constraints on the military treatment of prisoners and civilians, primarily by the Geneva Conventions; but rarely apply to use of the military forces as internal security troops during times of political conflict that results in popular protests and incitement to popular uprising.		International protocols restrict the use, or have even created international bans on weapons, notably weapons of mass destruction (WMD). International conventions define what constitutes a war crime, and provides for war crimes prosecution. Individual countries also have elaborate codes of military justice, an example being the United States' Uniform Code of Military Justice that can lead to court martial for military personnel found guilty of war crimes.		Military actions are sometimes argued to be justified by furthering a humanitarian cause, such as disaster relief operations, or in defence of refugees. The term military humanism is used to refer to such actions.		Antimilitarism is the society's social attitude opposed to war between states, and in particular, countering arguments based on militarism. Following Georg Wilhelm Friedrich Hegel's exploration of the relationship between history and violence, antimilitarists argue that there are different types of violence, some of which can be said to be legitimate, others non-legitimate. Anarcho-syndicalist Georges Sorel advocated the use of violence as a form of direct action, calling it 'revolutionary violence', which he opposed in Reflections on Violence (1908), to the violence inherent in class struggle. Sorel thus followed the International Workingmen's Association theorisation of propaganda of the deed.		War, as violence, can be distinguished into war between states, and civil war, in which case class struggle is, according to antimilitarists theorists, a primordial component. Hence, Marx's influence on antimilitarist doctrine was not surprising, although making Marx accountable for the antimilitarist tradition is a large overstatement. The belief in the eternal antimilitarist spirit, present in all places and time, is however, a myth, because the modern military as an institution is a historic achievement formed during the 18th and 19th centuries, as a by-product of the modern nation-states. Napoleon's invention of conscription is a fundamental progress in the organisation of state armies. Later, Prussian militarism would be exposed by 19th century social theorists.		A military brat is a colloquial term for a child with at least one parent who served as an active duty member (vice reserve) in the armed forces. Children of armed forces members may move around to different military bases or international postings, which gives them a childhood differing from the norm. Unlike common usage of the term brat, when it is used in this context, it is not necessarily a derogatory term.		Soldiers and armies have been prominent in popular culture since the beginnings of recorded history. In addition to the countless images of military leaders in heroic poses from antiquity, they have been an enduring source of inspiration in war literature. Not all of this has been entirely complementary, and the military have been lampooned or ridiculed as often as they have been idolised. The classical Greek writer Aristophanes, devoted an entire comedy, Lysistrata, to a strike organised by military wives, where they withhold sex from their husbands to prevent them from going to war.		In Medieval Europe, tales of knighthood and chivalry, the officer class of the period captured the popular imagination. Writers and poets like Taliesin, Chrétien de Troyes and Thomas Malory wrote tales of derring-do, featuring Arthur, Guinevere, Lancelot and Galahad. Even in the 21st century, books and films about the Arthurian legend and the Holy Grail continue to appear.		A century or so later, in the hands of writers such as Jean Froissart, Miguel Cervantes and William Shakespeare, the fictional knight Tirant lo Blanch, and the real-life condottieri John Hawkwood would be juxtaposed against the fantastical Don Quixote, and the carousing Sir John Falstaff. In just one play, Henry V, Shakespeare provides a whole range of military characters, from cool-headed and clear-sighted generals, to captains, and common soldiery.		Emperor Augustus Caesar in a martial pose (1st century)		The Flight of Pompey after Pharsalus, by Jean Fouquet		Medieval view: Richard II of England meets rebels		Sir John Hawkwood (fresco in the Duomo, Florence)		Shakespeare's Sir John Falstaff by Eduard von Grützner		'The Cruel Practices of Prince Rupert' (1643)		The rapid growth of movable type in the late 16th century and early 17th century saw an upsurge in private publication. Political pamphlets became popular, often lampooning military leaders for political purposes. A pamphlet directed against Prince Rupert of the Rhine is a typical example. During the 19th century, irreverence towards authority was at its height, and for every elegant military gentleman painted by the master-portraitists of the European courts, for example, Gainsborough, Goya, and Reynolds, there are the sometimes affectionate and sometimes savage caricatures of Rowland and Hogarth.		This continued in the 19th century, with publications like Punch in the British Empire and Le Père Duchesne in France, poking fun at the military establishment. This extended to media other print also. An enduring example is the Major-General's Song from the Gilbert and Sullivan light opera, The Pirates of Penzance, where a senior army officer is satirised for his enormous fund of irrelevant knowledge.		Colonel John Hayes St. Leger (detail) by Sir Joshua Reynolds		Rowlandson often satirised the military		'A modern major general' (The Pirates of Penzance)		Punch: war reporter, W H Russell, Crimean War		The increasing importance of cinema in the early 20th century provided a new platform for depictions of military subjects. During the First World War, although heavily censored, newsreels enabled those at home to see for themselves a heavily sanitised version of life at the front line. About the same time, both pro-war and anti-war films came to the silver screen. One of the first films on military aviation, Hell's Angels, broke all box office records on its release in 1929. Soon, war films of all types were showing throughout the world, notably those of Charlie Chaplin who actively promoted war bonds and voluntary enlistment.		The First World War was also responsible for a new kind of military depiction, through poetry. Hitherto, poetry had been used mostly to glorify or sanctify war. The Charge of the Light Brigade by Alfred, Lord Tennyson, with its galloping hoofbeat rhythm, is a prime late Victorian example of this, though Rudyard Kipling had written a scathing reply, The Last of the Light Brigade, criticising the poverty in which many Light Brigade veterans found themselves in old age. Instead, the new wave of poetry, from the war poets, was written from the point of view of the disenchanted trench soldier.		Leading war poets included Siegfried Sassoon, Wilfred Owen, John McCrae, Rupert Brooke, Isaac Rosenberg, and David Jones. A similar movement occurred in literature, producing a slew of novels on both sides of the Atlantic, including notably: All Quiet on the Western Front, and Johnny Got His Gun. The 1963 English stage musical Oh, What a Lovely War! provided a satirical take on World War I, which was released in a cinematic version directed by Richard Attenborough in 1969.		The propaganda war that accompanied World War II invariably depicted the enemy in unflattering terms. Examples of this exist not only in posters, but also in the films of Leni Riefenstahl and Sergei Eisenstein.		Alongside this, World War II also inspired films as varied as The Dam Busters, 633 Squadron, Bridge on the River Kwai, The Longest Day, Catch-22, Saving Private Ryan, and The Sea Shall Not Have Them. The next major event, the Korean War inspired a long-running television series M*A*S*H. With the Vietnam War, the tide of balance turned, and its films, notably Apocalypse Now, Good Morning, Vietnam, Go Tell the Spartans, Born on the Fourth of July, and We Were Soldiers, have tended to contain critical messages.		There is even a nursery rhyme about war, The Grand Old Duke of York, ridiculing a general for his inability to command any further than marching his men up and down a hill. The huge number of songs focusing on war include And the Band Played Waltzing Matilda and Universal Soldier.		
In strength training and fitness, the squat is a compound, full body exercise that trains primarily the muscles of the thighs, hips and buttocks, quadriceps femoris muscle (vastus lateralis, vastus medialis, vastus intermedius and rectus femoris), hamstrings, as well as strengthening the bones, ligaments and insertion of the tendons throughout the lower body. Squats are considered a vital exercise for increasing the strength and size of the legs and buttocks, as well as developing core strength. Isometrically, the lower back, the upper back, the abdominals, the trunk muscles, the costal muscles, and the shoulders and arms are all essential to the exercise and thus are trained when squatting with the proper form.[1]		The squat is one of the three lifts in the strength sport of powerlifting, together with deadlifts and bench press. It is also considered a staple in many popular recreational exercise programs.						Gluteus maximus (glutes), quadriceps (quads)[2]		The movement begins from a standing position. Weights are often used, either in the hand(dumbbells or kettlebells) or as a bar braced across the trapezius muscle or rear deltoid muscle in the upper back.[3] The movement is initiated by moving the hips back and bending the knees and hips to lower the torso and accompanying weight, then returning to the upright position.		Squats can be performed to varying depths. The competition standard is for the crease of the hip (top surface of the leg at the hip joint) to fall below the top of the knee;[4] this is colloquially known as "parallel" depth.[5] Confusingly, many other definitions for "parallel" depth abound, none of which represents the standard in organized powerlifting. From shallowest to deepest, these other standards are: bottom of hamstring parallel to the ground;[6] the hip joint itself below the top of the knee, or femur parallel to the floor;[7] and the top of the upper thigh (i.e., top of the quadriceps) below the top of the knee.[8]		Squatting below parallel qualifies a squat as deep while squatting above it qualifies as shallow.[3] Some authorities caution against deep squats;[9] though the forces on the ACL and PCL decrease at high flexion, compressive forces on the menisci and articular cartilages in the knee peak at these same high angles.[10] This makes the relative safety of deep versus shallow squats difficult to determine.		As the body gradually descends, the hips and knees undergo flexion, the ankle extends ("dorsiflexes") and muscles around the joint contract eccentrically, reaching maximal contraction at the bottom of the movement while slowing and reversing descent. The muscles around the hips provide the power out of the bottom. If the knees slide forward or cave in then tension is taken from the hamstrings, hindering power on the ascent. Returning to vertical contracts the muscles concentrically, and the hips and knees undergo extension while the ankle plantarflexes.[3] In a weight bearing squat, the heels should always maintain contact with the floor throughout the movement. Weight shifting forward on to the toes, and off the heels creates unnecessary stress on the knee joint. This added stress may lead to inflammation or other overuse injuries.[11]		Two common errors include descending too rapidly and flexing the torso too far forward. Rapid descent risks being unable to complete the lift or causing injury. This occurs when the descent causes the squatting muscles to relax and tightness at the bottom is lost as a result. Over-flexing the torso greatly increases the forces exerted on the lower back, risking a spinal disc herniation.[3]		Another error where health of the knee joint is concerned is when the knee is not aligned with the direction of the toes. If the knee is not tracking over the toes during the movement this results in twisting/shearing of the joint and unwanted torque affecting the ligaments which can soon result in injury. The knee should always follow the toe. Have your toes slightly pointed out in order to track the knee properly.		Various types of equipment can be used to perform squats.		A power cage can be used to reduce risk of injury and eliminate the need for a spotting partner. By putting the bar on a track, the Smith machine reduces the role of hip movement in the squat and in this sense resembles a leg press.[12] The monolift rack allows an athlete to perform a squat without having to take a couple of steps back with weight on as opposed to conventional racks. Not many powerlifting federations allow monolift in competitions (WPO, GPC, IPO).		Other equipment used can include a weight lifting belt to support the torso and boards to wedge beneath the ankles to improve stability and allow a deeper squat (weightlifting shoes also have wooden wedges built into the sole to achieve the same effect). Wrist straps are another piece of recommended equipment; they support the wrist and help to keep it in a straightened position. They should be wrapped around the wrist, above and below the joint, thus limiting movement of the joint. Heel wedges and related equipment are discouraged by some as they are thought to worsen form over the long term.[13] The barbell can also be cushioned with a special padded sleeve.		On 8 October 2011, Jonas Rantanen of Finland performed a squat with a weight of 575 kg (1268 lb) at the Bullfarm Powerlifting Championships in Helsinki, beating the previous record by Donnie Thompson (USA) of 573 kg (1265 lb).[14]		The single-ply squat record is held by Dustin Slepicka (USA) at 500 kg (1102 lb).[15]		The raw world record with knee wraps is 500 kg (1102 lb) performed by Vlad Alhazov on 22 July 2017. [16]		The raw world record without knee wraps belongs to Ray Williams who lifted 1053 lb (477.5 kg) on 4 March 2017, at the Arnold Classic in Columbus, Ohio .[17]		The women world record belongs to Olga Gemaletdinova who lifted 310.0 kg (684 lb) on 5 May 2011.[18]		The most squats with 130 kg in two minutes was Netherlands woman Maria Strik. She squat-lifted a weight of 130 kg, 29 times within two minutes.[19]		The most bodyweight squats performed in one hour is 4,708 by Paddy Doyle (UK) on November 2007.[20]		The most bodyweight sumo squats in one hour is 5,135, and was achieved by Dr. Thienna Ho (Vietnamese) on December 2007.[21]		Silvio Sabba from Italy has :		The squat has a number of variants, some of which can be combined:		Although the squat has long been a basic element of weight training, it has in recent years been the subject of considerable controversy. Some trainers allege that squats are associated with injuries to the lumbar spine and knees.[31] Others, however, continue to advocate the squat as one of the best exercises for building muscle and strength. Some coaches maintain that incomplete squats (those terminating above parallel) are both less effective and more likely to cause injury[1] than a full squat (terminating with hips at or below knee level). A 2013 review concluded that deep squats performed with proper technique do not lead to increased rates of degenerative knee injuries and are an effective exercise. The same review also concluded that shallower squats may lead to degeneration in the lumbar spine and knees in the long-term.[32]		
The immune system is a host defense system comprising many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue. In many species, the immune system can be classified into subsystems, such as the innate immune system versus the adaptive immune system, or humoral immunity versus cell-mediated immunity. In humans, the blood–brain barrier, blood–cerebrospinal fluid barrier, and similar fluid–brain barriers separate the peripheral immune system from the neuroimmune system, which protects the brain.		Pathogens can rapidly evolve and adapt, and thereby avoid detection and neutralization by the immune system; however, multiple defense mechanisms have also evolved to recognize and neutralize pathogens. Even simple unicellular organisms such as bacteria possess a rudimentary immune system in the form of enzymes that protect against bacteriophage infections. Other basic immune mechanisms evolved in ancient eukaryotes and remain in their modern descendants, such as plants and invertebrates. These mechanisms include phagocytosis, antimicrobial peptides called defensins, and the complement system. Jawed vertebrates, including humans, have even more sophisticated defense mechanisms,[1] including the ability to adapt over time to recognize specific pathogens more efficiently. Adaptive (or acquired) immunity creates immunological memory after an initial response to a specific pathogen, leading to an enhanced response to subsequent encounters with that same pathogen. This process of acquired immunity is the basis of vaccination.		Disorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer.[2] Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can either be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. In contrast, autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.						Immunology is a science that examines the structure and function of the immune system. It originates from medicine and early studies on the causes of immunity to disease. The earliest known reference to immunity was during the plague of Athens in 430 BC. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time.[3] In the 18th century, Pierre-Louis Moreau de Maupertuis made experiments with scorpion venom and observed that certain dogs and mice were immune to this venom.[4] This and other observations of acquired immunity were later exploited by Louis Pasteur in his development of vaccination and his proposed germ theory of disease.[5] Pasteur's theory was in direct opposition to contemporary theories of disease, such as the miasma theory. It was not until Robert Koch's 1891 proofs, for which he was awarded a Nobel Prize in 1905, that microorganisms were confirmed as the cause of infectious disease.[6] Viruses were confirmed as human pathogens in 1901, with the discovery of the yellow fever virus by Walter Reed.[7]		Immunology made a great advance towards the end of the 19th century, through rapid developments, in the study of humoral immunity and cellular immunity.[8] Particularly important was the work of Paul Ehrlich, who proposed the side-chain theory to explain the specificity of the antigen-antibody reaction; his contributions to the understanding of humoral immunity were recognized by the award of a Nobel Prize in 1908, which was jointly awarded to the founder of cellular immunology, Elie Metchnikoff.[9]		The immune system protects organisms from infection with layered defenses of increasing specificity. In simple terms, physical barriers prevent pathogens such as bacteria and viruses from entering the organism. If a pathogen breaches these barriers, the innate immune system provides an immediate, but non-specific response. Innate immune systems are found in all plants and animals.[10] If pathogens successfully evade the innate response, vertebrates possess a second layer of protection, the adaptive immune system, which is activated by the innate response. Here, the immune system adapts its response during an infection to improve its recognition of the pathogen. This improved response is then retained after the pathogen has been eliminated, in the form of an immunological memory, and allows the adaptive immune system to mount faster and stronger attacks each time this pathogen is encountered.[11][12]		Both innate and adaptive immunity depend on the ability of the immune system to distinguish between self and non-self molecules. In immunology, self molecules are those components of an organism's body that can be distinguished from foreign substances by the immune system.[13] Conversely, non-self molecules are those recognized as foreign molecules. One class of non-self molecules are called antigens (short for antibody generators) and are defined as substances that bind to specific immune receptors and elicit an immune response.[14]		Microorganisms or toxins that successfully enter an organism encounter the cells and mechanisms of the innate immune system. The innate response is usually triggered when microbes are identified by pattern recognition receptors, which recognize components that are conserved among broad groups of microorganisms,[15] or when damaged, injured or stressed cells send out alarm signals, many of which (but not all) are recognized by the same receptors as those that recognize pathogens.[16] Innate immune defenses are non-specific, meaning these systems respond to pathogens in a generic way.[14] This system does not confer long-lasting immunity against a pathogen. The innate immune system is the dominant system of host defense in most organisms.[10]		Several barriers protect organisms from infection, including mechanical, chemical, and biological barriers. The waxy cuticle of many leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin are examples of mechanical barriers that are the first line of defense against infection.[14] However, as organisms cannot be completely sealed from their environments, other systems act to protect body openings such as the lungs, intestines, and the genitourinary tract. In the lungs, coughing and sneezing mechanically eject pathogens and other irritants from the respiratory tract. The flushing action of tears and urine also mechanically expels pathogens, while mucus secreted by the respiratory and gastrointestinal tract serves to trap and entangle microorganisms.[17]		Chemical barriers also protect against infection. The skin and respiratory tract secrete antimicrobial peptides such as the β-defensins.[18] Enzymes such as lysozyme and phospholipase A2 in saliva, tears, and breast milk are also antibacterials.[19][20] Vaginal secretions serve as a chemical barrier following menarche, when they become slightly acidic, while semen contains defensins and zinc to kill pathogens.[21][22] In the stomach, gastric acid and proteases serve as powerful chemical defenses against ingested pathogens.		Within the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by competing with pathogenic bacteria for food and space and, in some cases, by changing the conditions in their environment, such as pH or available iron.[23] As a result of the symbiotic relationship between commensals and the immune system, the probability that pathogens will reach sufficient numbers to cause illness is reduced. However, since most antibiotics non-specifically target bacteria and do not affect fungi, oral antibiotics can lead to an "overgrowth" of fungi and cause conditions such as a vaginal candidiasis (a yeast infection).[24] There is good evidence that re-introduction of probiotic flora, such as pure cultures of the lactobacilli normally found in unpasteurized yogurt, helps restore a healthy balance of microbial populations in intestinal infections in children and encouraging preliminary data in studies on bacterial gastroenteritis, inflammatory bowel diseases, urinary tract infection and post-surgical infections.[25][26][27]		Inflammation is one of the first responses of the immune system to infection.[28] The symptoms of inflammation are redness, swelling, heat, and pain, which are caused by increased blood flow into tissue. Inflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation, and leukotrienes that attract certain white blood cells (leukocytes).[29][30] Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have anti-viral effects, such as shutting down protein synthesis in the host cell.[31] Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote healing of any damaged tissue following the removal of pathogens.[32]		The complement system is a biochemical cascade that attacks the surfaces of foreign cells. It contains over 20 different proteins and is named for its ability to "complement" the killing of pathogens by antibodies. Complement is the major humoral component of the innate immune response.[33][34] Many species have complement systems, including non-mammals like plants, fish, and some invertebrates.[35]		In humans, this response is activated by complement binding to antibodies that have attached to these microbes or the binding of complement proteins to carbohydrates on the surfaces of microbes. This recognition signal triggers a rapid killing response.[36] The speed of the response is a result of signal amplification that occurs following sequential proteolytic activation of complement molecules, which are also proteases. After complement proteins initially bind to the microbe, they activate their protease activity, which in turn activates other complement proteases, and so on. This produces a catalytic cascade that amplifies the initial signal by controlled positive feedback.[37] The cascade results in the production of peptides that attract immune cells, increase vascular permeability, and opsonize (coat) the surface of a pathogen, marking it for destruction. This deposition of complement can also kill cells directly by disrupting their plasma membrane.[33]		Leukocytes (white blood cells) act like independent, single-celled organisms and are the second arm of the innate immune system.[14] The innate leukocytes include the phagocytes (macrophages, neutrophils, and dendritic cells), innate lymphoid cells, mast cells, eosinophils, basophils, and natural killer cells. These cells identify and eliminate pathogens, either by attacking larger pathogens through contact or by engulfing and then killing microorganisms.[35] Innate cells are also important mediators in lymphoid organ development and the activation of the adaptive immune system.[38]		Phagocytosis is an important feature of cellular innate immunity performed by cells called 'phagocytes' that engulf, or eat, pathogens or particles. Phagocytes generally patrol the body searching for pathogens, but can be called to specific locations by cytokines.[14] Once a pathogen has been engulfed by a phagocyte, it becomes trapped in an intracellular vesicle called a phagosome, which subsequently fuses with another vesicle called a lysosome to form a phagolysosome. The pathogen is killed by the activity of digestive enzymes or following a respiratory burst that releases free radicals into the phagolysosome.[39][40] Phagocytosis evolved as a means of acquiring nutrients, but this role was extended in phagocytes to include engulfment of pathogens as a defense mechanism.[41] Phagocytosis probably represents the oldest form of host defense, as phagocytes have been identified in both vertebrate and invertebrate animals.[42]		Neutrophils and macrophages are phagocytes that travel throughout the body in pursuit of invading pathogens.[43] Neutrophils are normally found in the bloodstream and are the most abundant type of phagocyte, normally representing 50% to 60% of the total circulating leukocytes.[44] During the acute phase of inflammation, particularly as a result of bacterial infection, neutrophils migrate toward the site of inflammation in a process called chemotaxis, and are usually the first cells to arrive at the scene of infection. Macrophages are versatile cells that reside within tissues and: (i) produce a wide array of chemicals including enzymes, complement proteins, and cytokines, while they can also (ii) act as scavengers that rid the body of worn-out cells and other debris, and as antigen-presenting cells that activate the adaptive immune system.[45]		Dendritic cells (DC) are phagocytes in tissues that are in contact with the external environment; therefore, they are located mainly in the skin, nose, lungs, stomach, and intestines.[46] They are named for their resemblance to neuronal dendrites, as both have many spine-like projections, but dendritic cells are in no way connected to the nervous system. Dendritic cells serve as a link between the bodily tissues and the innate and adaptive immune systems, as they present antigens to T cells, one of the key cell types of the adaptive immune system.[46]		Mast cells reside in connective tissues and mucous membranes, and regulate the inflammatory response.[47] They are most often associated with allergy and anaphylaxis.[44] Basophils and eosinophils are related to neutrophils. They secrete chemical mediators that are involved in defending against parasites and play a role in allergic reactions, such as asthma.[48] Natural killer (NK cells) cells are leukocytes that attack and destroy tumor cells, or cells that have been infected by viruses.[49]		Natural killer cells, or NK cells, are lymphocytes and a component of the innate immune system which does not directly attack invading microbes.[50] Rather, NK cells destroy compromised host cells, such as tumor cells or virus-infected cells, recognizing such cells by a condition known as "missing self." This term describes cells with low levels of a cell-surface marker called MHC I (major histocompatibility complex) – a situation that can arise in viral infections of host cells.[35] They were named "natural killer" because of the initial notion that they do not require activation in order to kill cells that are "missing self." For many years it was unclear how NK cells recognize tumor cells and infected cells. It is now known that the MHC makeup on the surface of those cells is altered and the NK cells become activated through recognition of "missing self". Normal body cells are not recognized and attacked by NK cells because they express intact self MHC antigens. Those MHC antigens are recognized by killer cell immunoglobulin receptors (KIR) which essentially put the brakes on NK cells.[51]		The adaptive immune system evolved in early vertebrates and allows for a stronger immune response as well as immunological memory, where each pathogen is "remembered" by a signature antigen.[52] The adaptive immune response is antigen-specific and requires the recognition of specific "non-self" antigens during a process called antigen presentation. Antigen specificity allows for the generation of responses that are tailored to specific pathogens or pathogen-infected cells. The ability to mount these tailored responses is maintained in the body by "memory cells". Should a pathogen infect the body more than once, these specific memory cells are used to quickly eliminate it.		The cells of the adaptive immune system are special types of leukocytes, called lymphocytes. B cells and T cells are the major types of lymphocytes and are derived from hematopoietic stem cells in the bone marrow.[35] B cells are involved in the humoral immune response, whereas T cells are involved in cell-mediated immune response.		Both B cells and T cells carry receptor molecules that recognize specific targets. T cells recognize a "non-self" target, such as a pathogen, only after antigens (small fragments of the pathogen) have been processed and presented in combination with a "self" receptor called a major histocompatibility complex (MHC) molecule. There are two major subtypes of T cells: the killer T cell and the helper T cell. In addition there are regulatory T cells which have a role in modulating immune response. Killer T cells only recognize antigens coupled to Class I MHC molecules, while helper T cells and regulatory T cells only recognize antigens coupled to Class II MHC molecules. These two mechanisms of antigen presentation reflect the different roles of the two types of T cell. A third, minor subtype are the γδ T cells that recognize intact antigens that are not bound to MHC receptors.[53]		In contrast, the B cell antigen-specific receptor is an antibody molecule on the B cell surface, and recognizes whole pathogens without any need for antigen processing. Each lineage of B cell expresses a different antibody, so the complete set of B cell antigen receptors represent all the antibodies that the body can manufacture.[35]		Killer T cells are a sub-group of T cells that kill cells that are infected with viruses (and other pathogens), or are otherwise damaged or dysfunctional.[54] As with B cells, each type of T cell recognizes a different antigen. Killer T cells are activated when their T-cell receptor (TCR) binds to this specific antigen in a complex with the MHC Class I receptor of another cell. Recognition of this MHC:antigen complex is aided by a co-receptor on the T cell, called CD8. The T cell then travels throughout the body in search of cells where the MHC I receptors bear this antigen. When an activated T cell contacts such cells, it releases cytotoxins, such as perforin, which form pores in the target cell's plasma membrane, allowing ions, water and toxins to enter. The entry of another toxin called granulysin (a protease) induces the target cell to undergo apoptosis.[55] T cell killing of host cells is particularly important in preventing the replication of viruses. T cell activation is tightly controlled and generally requires a very strong MHC/antigen activation signal, or additional activation signals provided by "helper" T cells (see below).[55]		Helper T cells regulate both the innate and adaptive immune responses and help determine which immune responses the body makes to a particular pathogen.[56][57] These cells have no cytotoxic activity and do not kill infected cells or clear pathogens directly. They instead control the immune response by directing other cells to perform these tasks.		Helper T cells express T cell receptors (TCR) that recognize antigen bound to Class II MHC molecules. The MHC:antigen complex is also recognized by the helper cell's CD4 co-receptor, which recruits molecules inside the T cell (e.g., Lck) that are responsible for the T cell's activation. Helper T cells have a weaker association with the MHC:antigen complex than observed for killer T cells, meaning many receptors (around 200–300) on the helper T cell must be bound by an MHC:antigen in order to activate the helper cell, while killer T cells can be activated by engagement of a single MHC:antigen molecule. Helper T cell activation also requires longer duration of engagement with an antigen-presenting cell.[58] The activation of a resting helper T cell causes it to release cytokines that influence the activity of many cell types. Cytokine signals produced by helper T cells enhance the microbicidal function of macrophages and the activity of killer T cells.[14] In addition, helper T cell activation causes an upregulation of molecules expressed on the T cell's surface, such as CD40 ligand (also called CD154), which provide extra stimulatory signals typically required to activate antibody-producing B cells.[59]		Gamma delta T cells (γδ T cells) possess an alternative T-cell receptor (TCR) as opposed to CD4+ and CD8+ (αβ) T cells and share the characteristics of helper T cells, cytotoxic T cells and NK cells. The conditions that produce responses from γδ T cells are not fully understood. Like other 'unconventional' T cell subsets bearing invariant TCRs, such as CD1d-restricted Natural Killer T cells, γδ T cells straddle the border between innate and adaptive immunity.[60] On one hand, γδ T cells are a component of adaptive immunity as they rearrange TCR genes to produce receptor diversity and can also develop a memory phenotype. On the other hand, the various subsets are also part of the innate immune system, as restricted TCR or NK receptors may be used as pattern recognition receptors. For example, large numbers of human Vγ9/Vδ2 T cells respond within hours to common molecules produced by microbes, and highly restricted Vδ1+ T cells in epithelia respond to stressed epithelial cells.[53]		A B cell identifies pathogens when antibodies on its surface bind to a specific foreign antigen.[62] This antigen/antibody complex is taken up by the B cell and processed by proteolysis into peptides. The B cell then displays these antigenic peptides on its surface MHC class II molecules. This combination of MHC and antigen attracts a matching helper T cell, which releases lymphokines and activates the B cell.[63] As the activated B cell then begins to divide, its offspring (plasma cells) secrete millions of copies of the antibody that recognizes this antigen. These antibodies circulate in blood plasma and lymph, bind to pathogens expressing the antigen and mark them for destruction by complement activation or for uptake and destruction by phagocytes. Antibodies can also neutralize challenges directly, by binding to bacterial toxins or by interfering with the receptors that viruses and bacteria use to infect cells.[64]		Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T-cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.[65]		When B cells and T cells are activated and begin to replicate, some of their offspring become long-lived memory cells. Throughout the lifetime of an animal, these memory cells remember each specific pathogen encountered and can mount a strong response if the pathogen is detected again. This is "adaptive" because it occurs during the lifetime of an individual as an adaptation to infection with that pathogen and prepares the immune system for future challenges. Immunological memory can be in the form of either passive short-term memory or active long-term memory.		Newborn infants have no prior exposure to microbes and are particularly vulnerable to infection. Several layers of passive protection are provided by the mother. During pregnancy, a particular type of antibody, called IgG, is transported from mother to baby directly across the placenta, so human babies have high levels of antibodies even at birth, with the same range of antigen specificities as their mother.[66] Breast milk or colostrum also contains antibodies that are transferred to the gut of the infant and protect against bacterial infections until the newborn can synthesize its own antibodies.[67] This is passive immunity because the fetus does not actually make any memory cells or antibodies—it only borrows them. This passive immunity is usually short-term, lasting from a few days up to several months. In medicine, protective passive immunity can also be transferred artificially from one individual to another via antibody-rich serum.[68]		Long-term active memory is acquired following infection by activation of B and T cells. Active immunity can also be generated artificially, through vaccination. The principle behind vaccination (also called immunization) is to introduce an antigen from a pathogen in order to stimulate the immune system and develop specific immunity against that particular pathogen without causing disease associated with that organism.[14] This deliberate induction of an immune response is successful because it exploits the natural specificity of the immune system, as well as its inducibility. With infectious disease remaining one of the leading causes of death in the human population, vaccination represents the most effective manipulation of the immune system mankind has developed.[35][69]		Most viral vaccines are based on live attenuated viruses, while many bacterial vaccines are based on acellular components of micro-organisms, including harmless toxin components.[14] Since many antigens derived from acellular vaccines do not strongly induce the adaptive response, most bacterial vaccines are provided with additional adjuvants that activate the antigen-presenting cells of the innate immune system and maximize immunogenicity.[70]		The immune system is a remarkably effective structure that incorporates specificity, inducibility and adaptation. Failures of host defense do occur, however, and fall into three broad categories: immunodeficiencies, autoimmunity, and hypersensitivities.		Immunodeficiencies occur when one or more of the components of the immune system are inactive. The ability of the immune system to respond to pathogens is diminished in both the young and the elderly, with immune responses beginning to decline at around 50 years of age due to immunosenescence.[71][72] In developed countries, obesity, alcoholism, and drug use are common causes of poor immune function.[72] However, malnutrition is the most common cause of immunodeficiency in developing countries.[72] Diets lacking sufficient protein are associated with impaired cell-mediated immunity, complement activity, phagocyte function, IgA antibody concentrations, and cytokine production. Additionally, the loss of the thymus at an early age through genetic mutation or surgical removal results in severe immunodeficiency and a high susceptibility to infection.[73]		Immunodeficiencies can also be inherited or 'acquired'.[14] Chronic granulomatous disease, where phagocytes have a reduced ability to destroy pathogens, is an example of an inherited, or congenital, immunodeficiency. AIDS and some types of cancer cause acquired immunodeficiency.[74][75]		Overactive immune responses comprise the other end of immune dysfunction, particularly the autoimmune disorders. Here, the immune system fails to properly distinguish between self and non-self, and attacks part of the body. Under normal circumstances, many T cells and antibodies react with "self" peptides.[76] One of the functions of specialized cells (located in the thymus and bone marrow) is to present young lymphocytes with self antigens produced throughout the body and to eliminate those cells that recognize self-antigens, preventing autoimmunity.[62]		Hypersensitivity is an immune response that damages the body's own tissues. They are divided into four classes (Type I – IV) based on the mechanisms involved and the time course of the hypersensitive reaction. Type I hypersensitivity is an immediate or anaphylactic reaction, often associated with allergy. Symptoms can range from mild discomfort to death. Type I hypersensitivity is mediated by IgE, which triggers degranulation of mast cells and basophils when cross-linked by antigen.[77] Type II hypersensitivity occurs when antibodies bind to antigens on the patient's own cells, marking them for destruction. This is also called antibody-dependent (or cytotoxic) hypersensitivity, and is mediated by IgG and IgM antibodies.[77] Immune complexes (aggregations of antigens, complement proteins, and IgG and IgM antibodies) deposited in various tissues trigger Type III hypersensitivity reactions.[77] Type IV hypersensitivity (also known as cell-mediated or delayed type hypersensitivity) usually takes between two and three days to develop. Type IV reactions are involved in many autoimmune and infectious diseases, but may also involve contact dermatitis (poison ivy). These reactions are mediated by T cells, monocytes, and macrophages.[77]		It is likely that a multicomponent, adaptive immune system arose with the first vertebrates, as invertebrates do not generate lymphocytes or an antibody-based humoral response.[1] Many species, however, utilize mechanisms that appear to be precursors of these aspects of vertebrate immunity. Immune systems appear even in the structurally most simple forms of life, with bacteria using a unique defense mechanism, called the restriction modification system to protect themselves from viral pathogens, called bacteriophages.[78] Prokaryotes also possess acquired immunity, through a system that uses CRISPR sequences to retain fragments of the genomes of phage that they have come into contact with in the past, which allows them to block virus replication through a form of RNA interference.[79][80] Prokaryotes also possess other defense mechanisms.[81][82] Offensive elements of the immune systems are also present in unicellular eukaryotes, but studies of their roles in defense are few.[83]		Pattern recognition receptors are proteins used by nearly all organisms to identify molecules associated with pathogens. Antimicrobial peptides called defensins are an evolutionarily conserved component of the innate immune response found in all animals and plants, and represent the main form of invertebrate systemic immunity.[1] The complement system and phagocytic cells are also used by most forms of invertebrate life. Ribonucleases and the RNA interference pathway are conserved across all eukaryotes, and are thought to play a role in the immune response to viruses.[84]		Unlike animals, plants lack phagocytic cells, but many plant immune responses involve systemic chemical signals that are sent through a plant.[85] Individual plant cells respond to molecules associated with pathogens known as Pathogen-associated molecular patterns or PAMPs.[86] When a part of a plant becomes infected, the plant produces a localized hypersensitive response, whereby cells at the site of infection undergo rapid apoptosis to prevent the spread of the disease to other parts of the plant. Systemic acquired resistance (SAR) is a type of defensive response used by plants that renders the entire plant resistant to a particular infectious agent.[85] RNA silencing mechanisms are particularly important in this systemic response as they can block virus replication.[87]		Another important role of the immune system is to identify and eliminate tumors. This is called immune surveillance. The transformed cells of tumors express antigens that are not found on normal cells. To the immune system, these antigens appear foreign, and their presence causes immune cells to attack the transformed tumor cells. The antigens expressed by tumors have several sources;[89] some are derived from oncogenic viruses like human papillomavirus, which causes cervical cancer,[90] while others are the organism's own proteins that occur at low levels in normal cells but reach high levels in tumor cells. One example is an enzyme called tyrosinase that, when expressed at high levels, transforms certain skin cells (e.g. melanocytes) into tumors called melanomas.[91][92] A third possible source of tumor antigens are proteins normally important for regulating cell growth and survival, that commonly mutate into cancer inducing molecules called oncogenes.[89][93][94]		The main response of the immune system to tumors is to destroy the abnormal cells using killer T cells, sometimes with the assistance of helper T cells.[92][95] Tumor antigens are presented on MHC class I molecules in a similar way to viral antigens. This allows killer T cells to recognize the tumor cell as abnormal.[96] NK cells also kill tumorous cells in a similar way, especially if the tumor cells have fewer MHC class I molecules on their surface than normal; this is a common phenomenon with tumors.[97] Sometimes antibodies are generated against tumor cells allowing for their destruction by the complement system.[93]		Clearly, some tumors evade the immune system and go on to become cancers.[98] Tumor cells often have a reduced number of MHC class I molecules on their surface, thus avoiding detection by killer T cells.[96] Some tumor cells also release products that inhibit the immune response; for example by secreting the cytokine TGF-β, which suppresses the activity of macrophages and lymphocytes.[99] In addition, immunological tolerance may develop against tumor antigens, so the immune system no longer attacks the tumor cells.[98]		Paradoxically, macrophages can promote tumor growth [100] when tumor cells send out cytokines that attract macrophages, which then generate cytokines and growth factors that nurture tumor development. In addition, a combination of hypoxia in the tumor and a cytokine produced by macrophages induces tumor cells to decrease production of a protein that blocks metastasis and thereby assists spread of cancer cells.		The immune system is involved in many aspects of physiological regulation in the body. The immune system interacts intimately with other systems, such as the endocrine [101][102] and the nervous [103][104][105] systems. The immune system also plays a crucial role in embryogenesis (development of the embryo), as well as in tissue repair and regeneration.		Hormones can act as immunomodulators, altering the sensitivity of the immune system. For example, female sex hormones are known immunostimulators of both adaptive[106] and innate immune responses.[107] Some autoimmune diseases such as lupus erythematosus strike women preferentially, and their onset often coincides with puberty. By contrast, male sex hormones such as testosterone seem to be immunosuppressive.[108] Other hormones appear to regulate the immune system as well, most notably prolactin, growth hormone and vitamin D.[109][110]		When a T-cell encounters a foreign pathogen, it extends a vitamin D receptor. This is essentially a signaling device that allows the T-cell to bind to the active form of vitamin D, the steroid hormone calcitriol. T-cells have a symbiotic relationship with vitamin D. Not only does the T-cell extend a vitamin D receptor, in essence asking to bind to the steroid hormone version of vitamin D, calcitriol, but the T-cell expresses the gene CYP27B1, which is the gene responsible for converting the pre-hormone version of vitamin D, calcidiol into the steroid hormone version, calcitriol. Only after binding to calcitriol can T-cells perform their intended function. Other immune system cells that are known to express CYP27B1 and thus activate vitamin D calcidiol, are dendritic cells, keratinocytes and macrophages.[111][112]		It is conjectured that a progressive decline in hormone levels with age is partially responsible for weakened immune responses in aging individuals.[113] Conversely, some hormones are regulated by the immune system, notably thyroid hormone activity.[114] The age-related decline in immune function is also related to decreasing vitamin D levels in the elderly. As people age, two things happen that negatively affect their vitamin D levels. First, they stay indoors more due to decreased activity levels. This means that they get less sun and therefore produce less cholecalciferol via UVB radiation. Second, as a person ages the skin becomes less adept at producing vitamin D.[115]		The immune system is affected by sleep and rest,[116] and sleep deprivation is detrimental to immune function.[117] Complex feedback loops involving cytokines, such as interleukin-1 and tumor necrosis factor-α produced in response to infection, appear to also play a role in the regulation of non-rapid eye movement (REM) sleep.[118] Thus the immune response to infection may result in changes to the sleep cycle, including an increase in slow-wave sleep relative to REM sleep.[119]		When suffering from sleep deprivation, active immunizations may have a diminished effect and may result in lower antibody production, and a lower immune response, than would be noted in a well-rested individual. Additionally, proteins such as NFIL3, which have been shown to be closely intertwined with both T-cell differentiation and our circadian rhythms, can be affected through the disturbance of natural light and dark cycles through instances of sleep deprivation, shift work, etc. As a result, these disruptions can lead to an increase in chronic conditions such as heart disease, chronic pain, and asthma.[120]		In addition to the negative consequences of sleep deprivation, sleep and the intertwined circadian system have been shown to have strong regulatory effects on immunological functions affecting both the innate and the adaptive immunity. First, during the early slow-wave-sleep stage, a sudden drop in blood levels of cortisol, epinephrine, and norepinephrine induce increased blood levels of the hormones leptin, pituitary growth hormone, and prolactin. These signals induce a pro-inflammatory state through the production of the pro-inflammatory cytokines interleukin-1, interleukin-12, TNF-alpha and IFN-gamma. These cytokines then stimulate immune functions such as immune cells activation, proliferation, and differentiation. It is during this time that undifferentiated, or less differentiated, like naïve and central memory T cells, peak (i.e. during a time of a slowly evolving adaptive immune response). In addition to these effects, the milieu of hormones produced at this time (leptin, pituitary growth hormone, and prolactin) support the interactions between APCs and T-cells, a shift of the Th1/Th2 cytokine balance towards one that supports Th1, an increase in overall Th cell proliferation, and naïve T cell migration to lymph nodes. This milieu is also thought to support the formation of long-lasting immune memory through the initiation of Th1 immune responses.[121]		In contrast, during wake periods differentiated effector cells, such as cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes), peak in order to elicit an effective response against any intruding pathogens. As well during awake active times, anti-inflammatory molecules, such as cortisol and catecholamines, peak. There are two theories as to why the pro-inflammatory state is reserved for sleep time. First, inflammation would cause serious cognitive and physical impairments if it were to occur during wake times. Second, inflammation may occur during sleep times due to the presence of melatonin. Inflammation causes a great deal of oxidative stress and the presence of melatonin during sleep times could actively counteract free radical production during this time.[121][122]		Overnutrition is associated with diseases such as diabetes and obesity, which are known to affect immune function. More moderate malnutrition, as well as certain specific trace mineral and nutrient deficiencies, can also compromise the immune response.[123]		Foods rich in certain fatty acids may foster a healthy immune system.[124] Likewise, fetal undernourishment can cause a lifelong impairment of the immune system.[125]		The immune system, particularly the innate component, plays a decisive role in tissue repair after an insult.[126][127][128][129][130] Key actors include macrophages and neutrophils, but other cellular actors, including γδ T cells, innate lymphoid cells (ILCs), and regulatory T cells (Tregs), are also important. The plasticity of immune cells and the balance between pro-inflammatory and anti-inflammatory signals are crucial aspects of efficient tissue repair.[130] Immune components and pathways are involved in regeneration as well, for example in amphibians. According to one hypothesis, organisms that can regenerate could be less immunocompetent than organisms that cannot regenerate.[131][132]		The immune response can be manipulated to suppress unwanted responses resulting from autoimmunity, allergy, and transplant rejection, and to stimulate protective responses against pathogens that largely elude the immune system (see immunization) or cancer.		Immunosuppressive drugs are used to control autoimmune disorders or inflammation when excessive tissue damage occurs, and to prevent transplant rejection after an organ transplant.[35][133]		Anti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs; however, these drugs can have many undesirable side effects, such as central obesity, hyperglycemia, osteoporosis, and their use must be tightly controlled.[134] Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine. Cytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. However, the killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects.[133] Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.[135]		Cancer immunotherapy covers the medical ways to stimulate the immune system to attack cancer tumours.		Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between "cellular" and "humoral" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes – that were responsible for immune responses. In contrast, the humoral theory of immunity, held, among others, by Robert Koch and Emil von Behring, stated that the active immune agents were soluble components (molecules) found in the organism’s “humors” rather than its cells.[136][137][138]		In the mid-1950s, Frank Burnet, inspired by a suggestion made by Niels Jerne,[139] formulated the clonal selection theory (CST) of immunity.[140] On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: "self" constituents (constituents of the body) do not trigger destructive immune responses, while "nonself" entities (pathogens, an allograft) trigger a destructive immune response.[141] The theory was later modified to reflect new discoveries regarding histocompatibility or the complex "two-signal" activation of T cells.[142] The self/nonself theory of immunity and the self/nonself vocabulary have been criticized,[138][143][144] but remain very influential.[145][146]		More recently, several theoretical frameworks have been suggested in immunology, including "autopoietic" views,[147] "cognitive immune" views,[148] the "danger model" (or "danger theory"),[143] and the "discontinuity" theory.[149][150][151] The danger model, suggested by Polly Matzinger and colleagues, has been very influential, arousing many comments and discussions.[152][153][154][155]		Larger drugs (>500 Da) can provoke a neutralizing immune response, particularly if the drugs are administered repeatedly, or in larger doses. This limits the effectiveness of drugs based on larger peptides and proteins (which are typically larger than 6000 Da). In some cases, the drug itself is not immunogenic, but may be co-administered with an immunogenic compound, as is sometimes the case for Taxol. Computational methods have been developed to predict the immunogenicity of peptides and proteins, which are particularly useful in designing therapeutic antibodies, assessing likely virulence of mutations in viral coat particles, and validation of proposed peptide-based drug treatments. Early techniques relied mainly on the observation that hydrophilic amino acids are overrepresented in epitope regions than hydrophobic amino acids;[156] however, more recent developments rely on machine learning techniques using databases of existing known epitopes, usually on well-studied virus proteins, as a training set.[157] A publicly accessible database has been established for the cataloguing of epitopes from pathogens known to be recognizable by B cells.[158] The emerging field of bioinformatics-based studies of immunogenicity is referred to as immunoinformatics.[159] Immunoproteomics is the study of large sets of proteins (proteomics) involved in the immune response.		The success of any pathogen depends on its ability to elude host immune responses. Therefore, pathogens evolved several methods that allow them to successfully infect a host, while evading detection or destruction by the immune system.[160] Bacteria often overcome physical barriers by secreting enzymes that digest the barrier, for example, by using a type II secretion system.[161] Alternatively, using a type III secretion system, they may insert a hollow tube into the host cell, providing a direct route for proteins to move from the pathogen to the host. These proteins are often used to shut down host defenses.[162]		An evasion strategy used by several pathogens to avoid the innate immune system is to hide within the cells of their host (also called intracellular pathogenesis). Here, a pathogen spends most of its life-cycle inside host cells, where it is shielded from direct contact with immune cells, antibodies and complement. Some examples of intracellular pathogens include viruses, the food poisoning bacterium Salmonella and the eukaryotic parasites that cause malaria (Plasmodium falciparum) and leishmaniasis (Leishmania spp.). Other bacteria, such as Mycobacterium tuberculosis, live inside a protective capsule that prevents lysis by complement.[163] Many pathogens secrete compounds that diminish or misdirect the host's immune response.[160] Some bacteria form biofilms to protect themselves from the cells and proteins of the immune system. Such biofilms are present in many successful infections, e.g., the chronic Pseudomonas aeruginosa and Burkholderia cenocepacia infections characteristic of cystic fibrosis.[164] Other bacteria generate surface proteins that bind to antibodies, rendering them ineffective; examples include Streptococcus (protein G), Staphylococcus aureus (protein A), and Peptostreptococcus magnus (protein L).[165]		The mechanisms used to evade the adaptive immune system are more complicated. The simplest approach is to rapidly change non-essential epitopes (amino acids and/or sugars) on the surface of the pathogen, while keeping essential epitopes concealed. This is called antigenic variation. An example is HIV, which mutates rapidly, so the proteins on its viral envelope that are essential for entry into its host target cell are constantly changing. These frequent changes in antigens may explain the failures of vaccines directed at this virus.[166] The parasite Trypanosoma brucei uses a similar strategy, constantly switching one type of surface protein for another, allowing it to stay one step ahead of the antibody response.[167] Masking antigens with host molecules is another common strategy for avoiding detection by the immune system. In HIV, the envelope that covers the virion is formed from the outermost membrane of the host cell; such "self-cloaked" viruses make it difficult for the immune system to identify them as "non-self" structures.[168]		
The term lifestyle can denote the interests, opinions, behaviours, and behavioural orientations of an individual, group, or culture.[1][2]		The term was introduced by Austrian psychologist Alfred Adler with the meaning of "a person's basic character as established early in childhood"[3], for example in his 1929 book "The Case of Miss R.". The term was introduced in the 1950s as a derivative of that of style in modernist art[4][clarification needed], and the broader sense of "way or style of living" has been documented since 1961.[5] The term refers to a combination of determining intangible or tangible factors. Tangible factors relate specifically to demographic variables, i.e. an individual's demographic profile, whereas intangible factors concern the psychological aspects of an individual such as personal values, preferences, and outlooks.		A rural environment has different lifestyles compared to an urban metropolis. Location is important even within an urban scope. The nature of the neighborhood in which a person resides affects the set of lifestyles available to that person due to differences between various neighborhoods' degrees of affluence and proximity to natural and cultural environments. For example, in areas within a close proximity to the sea, a surf culture or lifestyle can often be present.						A lifestyle typically reflects an individual's attitudes, way of life, values, or world view. Therefore, a lifestyle is a means of forging a sense of self and to create cultural symbols that resonate with personal identity. Not all aspects of a lifestyle are voluntary. Surrounding social and technical systems can constrain the lifestyle choices available to the individual and the symbols she/he is able to project to others and the self.[6]		The lines between personal identity and the everyday doings that signal a particular lifestyle become blurred in modern society.[7] For example, "green lifestyle" means holding beliefs and engaging in activities that consume fewer resources and produce less harmful waste (i.e. a smaller ecological footprint), and deriving a sense of self from holding these beliefs and engaging in these activities.[8] Some commentators argue that, in modernity, the cornerstone of lifestyle construction is consumption behavior, which offers the possibility to create and further individualize the self with different products or services that signal different ways of life.[9]		Lifestyle may include views on politics, religion, health, intimacy, and more. All of these aspects play a role in shaping someone's lifestyle. [10] In the magazine and television industries, "lifestyle" is used to describe a category of publications or programs.		Three main phases can be identified in the history of lifestyles studies:[11]		A healthy or unhealthy lifestyle will most likely be transmitted across generations. According to the study done by Case et al. (2002), when a 0-3 year old child has a mother who practices a healthy lifestyle, this child will be 27% more likely to become healthy and adopt the same lifestyle.[12] For instance, high income parents are more likely to eat organic food, have time to exercise, and provide the best living condition to their children. On the other hand, low income parents are more likely to participate in unhealthy activities such as smoking to help them release poverty-related stress and depression.[13] Parents are the first teacher for every child. Everything that parents do will be very likely transferred to their children through the learning process.		Adults may be drawn together by mutual interest that results in a lifestyle. For example, William Dufty described how pursuing a sugar-free diet led to such associations:[14]		I have come to know hundreds of young people who have found that illness or bingeing on drugs and sugar became the doorway to health. Once they reestablished their own health, we had in common our interest in food. If one can use that overworked word lifestyle, we shared a sugarfree lifestyle. I kept in touch with many of them in campuses and communes, through their travels here and abroad and everywhere. One day you meet them in Boston. The next week you run into them in Southern California.		Lifestyle research can contribute to the question of the relevance of the class concept.[15]		The term lifestyle was introduced in the 1950s as a derivative of that of style in art:[4]		"Life-styles", the culture industry’s recycling of style in art, represent the transformation of an aesthetic category, which once possessed a moment of negativity [shocking, emancipatory], into a quality of commodity consumption.		Theodor W. Adorno noted that there is a "culture industry" in which the mass media is involved, but that the term "mass culture" is inappropriate: [16]		In our drafts, we spoke of "mass culture." We replaced that expression with "culture industry" in order to exclude from the outset the interpretation agreeable to its advocates: that it is a matter of something like a culture that arises spontaneously from the masses themselves, the contemporary form of popular art.		The media culture of advanced capitalism typically creates new "life-styles" to drive the consumption of new commodities:[4]		Diversity is more effectively present in mass media than previously, but this is not an obvious or unequivocal gain. By the late 1950s, the homogenization of consciousness had become counterproductive for the purposes of capital expansion; new needs for new commodities had to be created, and this required the reintroduction of the minimal negativity that had been previously eliminated. The cult of the new that had been the prerogative of art throughout the modernist epoch into the period of post-war unification and stabilization has returned to capital expansion from which it originally sprang. But this negativity is neither shocking nor emancipatory since it does not presage a transformation of the fundamental structures of everyday life. On the contrary, through the culture industry capital has co-opted the dynamics of negation both diachronically in its restless production of new and "different" commodities and synchronically in its promotion of alternative "life-styles."		
Yoga (/ˈjoʊɡə/;[1] Sanskrit, योगः Listen) is a group of physical, mental, and spiritual practices or disciplines which originated in ancient India. There is a broad variety of yoga schools, practices, and goals[2] in Hinduism, Buddhism, and Jainism.[3][4][5] Among the most well-known types of yoga are Hatha yoga and Rāja yoga.[6]		The origins of yoga have been speculated to date back to pre-Vedic Indian traditions; it is mentioned in the Rigveda,[note 1] but most likely developed around the sixth and fifth centuries BCE,[8] in ancient India's ascetic and śramaṇa movements.[9][note 2] The chronology of earliest texts describing yoga-practices is unclear, varyingly credited to Hindu Upanishads.[10] The Yoga Sutras of Patanjali date from the first half of the 1st millennium CE,[11][12] but only gained prominence in the West in the 20th century.[13] Hatha yoga texts emerged around the 11th century with origins in tantra.[14][15]		Yoga gurus from India later introduced yoga to the west,[16] following the success of Swami Vivekananda in the late 19th and early 20th century.[16] In the 1980s, yoga became popular as a system of physical exercise across the Western world.[15] Yoga in Indian traditions, however, is more than physical exercise; it has a meditative and spiritual core.[17] One of the six major orthodox schools of Hinduism is also called Yoga, which has its own epistemology and metaphysics, and is closely related to Hindu Samkhya philosophy.[18]		Many studies have tried to determine the effectiveness of yoga as a complementary intervention for cancer, schizophrenia, asthma, and heart disease.[19][20] The results of these studies have been mixed and inconclusive, with cancer studies suggesting none to unclear effectiveness, and others suggesting yoga may reduce risk factors and aid in a patient's psychological healing process.[19][20] On December 1, 2016, yoga was listed by UNESCO as an Intangible cultural heritage.[21]		In Sanskrit, the word yoga comes from the root yuj which means "to add", "to join", "to unite", or "to attach" in its most common senses. By figurative extension from the yoking or harnessing of oxen or horses, the word took on broader meanings such as "employment, use, application, performance" (compare the figurative uses of "to harness" as in "to put something to some use"). All further developments of the sense of this word are post-Vedic. More prosaic moods such as "exertion", "endeavour", "zeal", and "diligence" are also found in Indian epic poetry.[22]		There are very many compound words containing yoga in Sanskrit. Yoga can take on meanings such as "connection", "contact", "union", "method", "application", "addition" and "performance". In simpler words, Yoga also means "combined". For example, guṇáyoga means "contact with a cord"; chakráyoga has a medical sense of "applying a splint or similar instrument by means of pulleys (in case of dislocation of the thigh)"; chandráyoga has the astronomical sense of "conjunction of the moon with a constellation"; puṃyoga is a grammatical term expressing "connection or relation with a man", etc. Thus, bhaktiyoga means "devoted attachment" in the monotheistic Bhakti movement. The term kriyāyoga has a grammatical sense, meaning "connection with a verb". But the same compound is also given a technical meaning in the Yoga Sutras (2.1), designating the "practical" aspects of the philosophy, i.e. the "union with the supreme" due to performance of duties in everyday life[23]		According to Pāṇini, a 6th-century BCE Sanskrit grammarian, the term yoga can be derived from either of two roots, yujir yoga (to yoke) or yuj samādhau (to concentrate).[24] In the context of the Yoga Sutras of Patanjali, the root yuj samādhau (to concentrate) is considered by traditional commentators as the correct etymology.[25] In accordance with Pāṇini, Vyasa who wrote the first commentary on the Yoga Sutras,[26] states that yoga means samādhi (concentration).[27]		According to Dasgupta, the term yoga can be derived from either of two roots, yujir yoga (to yoke) or yuj samādhau (to concentrate).[24] Someone who practices yoga or follows the yoga philosophy with a high level of commitment is called a yogi (may be applied to a man or a woman) or yogini (traditionally denoting a woman).[28]		The ultimate goal of Yoga is moksha (liberation), although the exact definition of what form this takes depends on the philosophical or theological system with which it is conjugated.		According to Jacobsen, "Yoga has five principal meanings:[29]		According to David Gordon White, from the 5th century CE onward, the core principles of "yoga" were more or less in place, and variations of these principles developed in various forms over time:[30]		White clarifies that the last principle relates to legendary goals of "yogi practice", different from practical goals of "yoga practice," as they are viewed in South Asian thought and practice since the beginning of the Common Era, in the various Hindu, Buddhist, and Jain philosophical schools.[36]		The term "yoga" has been applied to a variety of practices and methods, including Jain and Buddhist practices. In Hinduism these include Jnana Yoga, Bhakti Yoga, Karma Yoga, Laya Yoga and Hatha Yoga.		The so-called Raja Yoga refers to Ashtanga Yoga, the eight limbs to be practiced to attain samadhi, as described in the Yoga Sutras of Pantajali.[37] The term raja yoga originally referred to the ultimate goal of yoga, which is usually samadhi,[38] but was popularised by Vivekananda as the common name for Ashtanga Yoga.[39]		Yoga is considered as a philosophical school in Hinduism.[40] Yoga, in this context, is one of the six āstika schools of Hinduism (those which accept the Vedas as source of knowledge).[41][42]		Due to the influence of Vivekananda, the Yoga Sutras of Patanjali are nowadays considered as the foundational scripture of classical yoga, a status which it only acquired in the 20th century.[39] Before the twentieth century, other works were considered as the most central works, such as the Bhagavad Gita and the Yoga Vasistha,[39] while Tantric Yoga and Hatha Yoga prevailed over Ashtanga Yoga.[39]		Yoga as described in the Yoga Sutras of Patanjali refers to Ashtanga yoga.[39] The Yoga Sutras of Patanjali is considered as a central text of the Yoga school of Hindu philosophy,[44] It is often called "Rāja yoga", "yoga of the kings," a term which originally referred to the ultimate, royal goal of yoga, which is usually samadhi,[38] but was popularised by Vivekananda as the common name for Ashtanga Yoga.[39]		Ashtanga yoga incorporates epistemology, metaphysics, ethical practices, systematic exercises and self-development techniques for body, mind and spirit.[45] Its epistemology (pramanas) is same as the Samkhya school. Both accept three reliable means to knowledge – perception (pratyākṣa, direct sensory observations), inference (anumāna) and testimony of trustworthy experts (sabda, agama). Both these orthodox schools are also strongly dualistic. Unlike the Sāṃkhya school of Hinduism, which pursues a non-theistic/atheistic rationalist approach,[46][47] the Yoga school of Hinduism accepts the concept of a "personal, yet essentially inactive, deity" or "personal god".[48][49] Along with its epistemology and metaphysical foundations, the Yoga school of Hindu philosophy incorporates ethical precepts (yamas and niyamas) and an introspective way of life focused on perfecting one's self physically, mentally and spiritually, with the ultimate goal being kaivalya (liberated, unified, content state of existence).[45][50][51]		Hatha yoga, also called hatha vidyā, is a kind of yoga focusing on physical and mental strength building exercises and postures described primarily in three texts of Hinduism:[53][54][55]		Many scholars also include the preceding Goraksha Samhita authored by Gorakshanath of the 11th century in the above list.[53] Gorakshanath is widely considered to have been responsible for popularizing hatha yoga as we know it today.[57][58][59]		Vajrayana Buddhism, founded by the Indian Mahasiddhas,[60] has a series of asanas and pranayamas, such as tummo (Sanskrit caṇḍālī)[61] and trul khor which parallel hatha yoga.		In Shaivism, yoga is used to unite kundalini with Shiva.[62] See also 'tantra' below.		Buddhist meditation encompasses a variety of meditation techniques that aim to develop mindfulness, concentration, supramundane powers, tranquility, and insight.		Core techniques have been preserved in ancient Buddhist texts and have proliferated and diversified through teacher-student transmissions. Buddhists pursue meditation as part of the path toward Enlightenment and Nirvana.[note 3] The closest words for meditation in the classical languages of Buddhism are bhāvanā[note 4] and jhāna/dhyāna.[note 5]		Jain meditation has been the central practice of spirituality in Jainism along with the Three Jewels.[63] Meditation in Jainism aims at realizing the self, attain salvation, take the soul to complete freedom.[64] It aims to reach and to remain in the pure state of soul which is believed to be pure conscious, beyond any attachment or aversion. The practitioner strives to be just a knower-seer (Gyata-Drashta). Jain meditation can be broadly categorized to the auspicious Dharmya Dhyana and Shukla Dhyana and inauspicious Artta and Raudra Dhyana.[citation needed]		Samuel states that Tantrism is a contested concept.[65] Tantra yoga may be described, according to Samuel, as practices in 9th to 10th century Buddhist and Hindu (Saiva, Shakti) texts, which included yogic practices with elaborate deity visualizations using geometrical arrays and drawings (mandala), fierce male and particularly female deities, transgressive life stage related rituals, extensive use of chakras and mantras, and sexual techniques, all aimed to help one's health, long life and liberation.[65][66]		The origins of yoga are a matter of debate.[67] There is no consensus on its chronology or specific origin other than that yoga developed in ancient India. Suggested origins are the Indus Valley Civilization (3300–1900 BCE)[68] and pre-Vedic Eastern states of India,[69] the Vedic period (1500–500 BCE), and the śramaṇa movement.[70] According to Gavin Flood, continuities may exist between those various traditions:		[T]his dichotomization is too simplistic, for continuities can undoubtedly be found between renunciation and vedic Brahmanism, while elements from non-Brahmanical, Sramana traditions also played an important part in the formation of the renunciate ideal.[71][note 6]		Pre-philosophical speculations of yoga begin to emerge in the texts of c. 500–200 BCE. Between 200 BCE–500 CE philosophical schools of Hinduism, Buddhism and Jainism were taking form and a coherent philosophical system of yoga began to emerge.[73] The Middle Ages saw the development of many satellite traditions of yoga. Yoga came to the attention of an educated western public in the mid 19th century along with other topics of Indian philosophy.		Yoga may have pre-Vedic elements.[68][69] Some state yoga originated in the Indus Valley Civilization.[74] Marshall,[75] Eliade[10] and other scholars suggest that the Pashupati seal discovered in Indus Valley Civilization sites depict figures in positions resembling a common yoga or meditation pose. This interpretation is considered speculative and uncertain by more recent analysis of Srinivasan[10] and may be a case of projecting "later practices into archeological findings".[76]		According to Crangle, some researchers have favoured a linear theory, which attempts "to interpret the origin and early development of Indian contemplative practices as a sequential growth from an Aryan genesis",[77][note 7] just like traditional Hinduism regards the Vedas to be the ultimate source of all spiritual knowledge.[78][note 8] Thomas McEvilley favors a composite model where pre-Aryan yoga prototype existed in the pre-Vedic period and its refinement began in the Vedic period.[81]		Ascetic practices, concentration and bodily postures described in the Vedas may have been precursors to yoga.[82][83] According to Geoffrey Samuel, "Our best evidence to date suggests that [yogic] practices developed in the same ascetic circles as the early sramana movements (Buddhists, Jainas and Ajivikas), probably in around the sixth and fifth centuries BCE."[9]		According to Zimmer, Yoga philosophy is reckoned to be part of the non-Vedic system, which also includes the Samkhya school of Hindu philosophy, Jainism and Buddhism:[69] "[Jainism] does not derive from Brahman-Aryan sources, but reflects the cosmology and anthropology of a much older pre-Aryan upper class of northeastern India [Bihar] – being rooted in the same subsoil of archaic metaphysical speculation as Yoga, Sankhya, and Buddhism, the other non-Vedic Indian systems."[84][note 9]		The first use of the root of word "yoga" is in hymn 5.81.1 of the Rig Veda, a dedication to rising Sun-god in the morning (Savitri), where it has been interpreted as "yoke" or "yogically control".[87][88][note 10]		The earliest evidence of Yogis and Yoga tradition is found in the Keśin hymn 10.136 of the Rigveda, states Karel Werner.[7]		The Yogis of Vedic times left little evidence of their existence, practices and achievements. And such evidence as has survived in the Vedas is scanty and indirect. Nevertheless, the existence of accomplished Yogis in Vedic times cannot be doubted.		Rigveda, however, does not describe yoga and there is little evidence as to what the practices were.[7] Early references to practices that later became part of yoga, are made in Brihadaranyaka Upanishad, the earliest Hindu Upanishad.[note 11] For example, the practice of pranayama (consciously regulating breath) is mentioned in hymn 1.5.23 of Brihadaranyaka Upanishad (c. 900 BCE), and the practice of pratyahara (concentrating all of one's senses on self) is mentioned in hymn 8.15 of Chandogya Upanishad (c. 800–700 BCE).[91][note 12]		Ascetic practices (tapas), concentration and bodily postures used by Vedic priests to conduct yajna (sacrifice), might have been precursors to yoga.[note 13] Vratya, a group of ascetics mentioned in the Atharvaveda, emphasized on bodily postures which may have evolved into yogic asanas.[82] Early Samhitas also contain references to other group ascetics such as munis, the keśin, and vratyas.[94] Techniques for controlling breath and vital energies are mentioned in the Brahmanas (texts of the Vedic corpus, c. 1000–800 BCE) and the Atharvaveda.[82][95] Nasadiya Sukta of the Rig Veda suggests the presence of an early contemplative tradition.[note 14]		Yoga concepts begin to emerge in the texts of c. 500–200 BCE such as the Pali Canon, the middle Upanishads, the Bhagavad Gita and Shanti Parva of the Mahabharata.[98][note 15]		The first known appearance of the word "yoga", with the same meaning as the modern term, is in the Katha Upanishad,[10][101] probably composed between the fifth and third century BCE,[102][103] where it is defined as the steady control of the senses, which along with cessation of mental activity, leading to a supreme state.[94][note 16] Katha Upanishad integrates the monism of early Upanishads with concepts of samkhya and yoga. It defines various levels of existence according to their proximity to the innermost being Ātman. Yoga is therefore seen as a process of interiorization or ascent of consciousness.[105][106] It is the earliest literary work that highlights the fundamentals of yoga. White states:		The earliest extant systematic account of yoga and a bridge from the earlier Vedic uses of the term is found in the Hindu Katha Upanisad (Ku), a scripture dating from about the third century BCE[…] [I]t describes the hierarchy of mind-body constituents—the senses, mind, intellect, etc.—that comprise the foundational categories of Sāmkhya philosophy, whose metaphysical system grounds the yoga of the Yogasutras, Bhagavad Gita, and other texts and schools (Ku3.10–11; 6.7–8).[107]		The hymns in Book 2 of the Shvetashvatara Upanishad, another late first millennium BCE text, states a procedure in which the body is held in upright posture, the breath is restrained and mind is meditatively focussed, preferably inside a cave or a place that is simple, plain, of silence or gently flowing water, with no noises nor harsh winds.[108][106]		The Maitrayaniya Upanishad, likely composed in a later century than Katha and Shvetashvatara Upanishads but before Patanjali's Yoga Sutra, mentions sixfold yoga method – breath control (pranayama), introspective withdrawal of senses (pratyahara), meditation (dhyana), mind concentration (dharana), philosophical inquiry/creative reasoning (tarka), and absorption/intense spiritual union (samadhi).[10][106][109]		In addition to the Yoga discussion in above Principal Upanishads, twenty Yoga Upanishads as well as related texts such as Yoga Vasistha, composed in 1st and 2nd millennium CE, discuss Yoga methods.[110][111]		Yoga is discussed in the ancient foundational Sutras of Hindu philosophy. The Vaiśeṣika Sūtra of the Vaisheshika school of Hinduism, dated to have been composed sometime between 6th and 2nd century BCE discusses Yoga.[112][113][note 17] According to Johannes Bronkhorst, an Indologist known for his studies on early Buddhism and Hinduism and a professor at the University of Lausanne, Vaiśeṣika Sūtra describes Yoga as "a state where the mind resides only in the soul and therefore not in the senses".[115] This is equivalent to pratyahara or withdrawal of the senses, and the ancient Sutra asserts that this leads to an absence of sukha (happiness) and dukkha (suffering), then describes additional yogic meditation steps in the journey towards the state of spiritual liberation.[115]		Similarly, Brahma sutras – the foundational text of the Vedanta school of Hinduism, discusses yoga in its sutra 2.1.3, 2.1.223 and others.[116] Brahma sutras are estimated to have been complete in the surviving form sometime between 450 BCE to 200 CE,[117][118] and its sutras assert that yoga is a means to gain "subtlety of body" and other powers.[116] The Nyaya sutras – the foundational text of the Nyaya school, variously estimated to have been composed between the 6th-century BCE and 2nd-century CE,[119][120] discusses yoga in sutras 4.2.38–50. This ancient text of the Nyaya school includes a discussion of yogic ethics, dhyana (meditation), samadhi, and among other things remarks that debate and philosophy is a form of yoga.[121][122][123]		Alexander the Great reached India in the 4th century BCE. Along with his army, he took Greek academics with him who later wrote memoirs about geography, people and customs they saw. One of Alexander's companion was Onesicritus, quoted in Book 15, Sections 63–65 by Strabo, who describes yogins of India.[124] Onesicritus claims those Indian yogins (Mandanis ) practiced aloofness and "different postures – standing or sitting or lying naked – and motionless".[125]		Onesicritus also mentions his colleague Calanus trying to meet them, who is initially denied audience, but later invited because he was sent by a "king curious of wisdom and philosophy".[125] Onesicritus and Calanus learn that the yogins consider the best doctrine of life as "rid the spirit of not only pain, but also pleasure", that "man trains the body for toil in order that his opinions may be strengthened", that "there is no shame in life on frugal fare", and that "the best place to inhabit is one with scantiest equipment or outfit".[124][125] These principles are significant to the history of spiritual side of yoga.[124] These may reflect the ancient roots of "undisturbed calmness" and "mindfulness through balance" in later works of Hindu Patanjali and Buddhist Buddhaghosa respectively, states Charles Rockwell Lanman;[124] as well as the principle of Aparigraha (non-possessiveness, non-craving, simple living) and asceticism discussed in later Hinduism and Jainism.[citation needed]		Werner states, "The Buddha was the founder of his [Yoga] system, even though, admittedly, he made use of some of the experiences he had previously gained under various Yoga teachers of his time."[126] He notes:[127]		But it is only with Buddhism itself as expounded in the Pali Canon that we can speak about a systematic and comprehensive or even integral school of Yoga practice, which is thus the first and oldest to have been preserved for us in its entirety.[127]		The chronology of completion of these yoga-related Pali Canons, however, is unclear, just like ancient Hindu texts.[128][129] Early known Buddhist sources like the Majjhima Nikāya mention meditation, while the Anguttara Nikāya describes Jhāyins (meditators) that resemble early Hindu descriptions of Muni, Kesins and meditating ascetics,[130] but these meditation-practices are not called yoga in these texts.[131] The earliest known specific discussion of yoga in the Buddhist literature, as understood in modern context, is from the third- to fourth-century CE scriptures of the Buddhist Yogācāra school and fourth- to fifth-century Visuddhimagga of Buddhaghosa.[131]		A yoga system that predated the Buddhist school is Jain yoga. But since Jain sources postdate Buddhist ones, it is difficult to distinguish between the nature of the early Jain school and elements derived from other schools.[127] Most of the other contemporary yoga systems alluded in the Upanishads and some Pali canons are lost to time.[132][133][note 18]		The early Buddhist texts describe meditative practices and states, some of which the Buddha borrowed from the śramaṇa tradition.[135][136] The Pali canon contains three passages in which the Buddha describes pressing the tongue against the palate for the purposes of controlling hunger or the mind, depending on the passage.[137] However, there is no mention of the tongue being inserted into the nasopharynx as in true khecarī mudrā. The Buddha used a posture where pressure is put on the perineum with the heel, similar to even modern postures used to stimulate Kundalini.[138]		Alexander Wynne, author of The Origin of Buddhist Meditation, observes that formless meditation and elemental meditation might have originated in the Upanishadic tradition.[139] The earliest reference to meditation is in the Brihadaranyaka Upanishad, one of the oldest Upanishads.[94] Chandogya Upanishad describes the five kinds of vital energies (prana). Concepts used later in many yoga traditions such as internal sound and veins (nadis) are also described in the Upanishad.[82] Taittiriya Upanishad defines yoga as the mastery of body and senses.[140]		The Bhagavad Gita ('Song of the Lord'), uses the term "yoga" extensively in a variety of ways. In addition to an entire chapter (ch. 6) dedicated to traditional yoga practice, including meditation,[141] it introduces three prominent types of yoga:[142]		The Gita consists of 18 chapters and 700 shlokas (verses),[146] with each chapter named as a different yoga, thus delineating eighteen different yogas.[146][147] Some scholars divide the Gita into three sections, with the first six chapters with 280 shlokas dealing with Karma yoga, the middle six containing 209 shlokas with Bhakti yoga, and the last six chapters with 211 shlokas as Jnana yoga; however, this is rough because elements of karma, bhakti and jnana are found in all chapters.[146]		Description of an early form of yoga called nirodhayoga (yoga of cessation) is contained in the Mokshadharma section of the 12th chapter (Shanti Parva) of the Mahabharata. The verses of the section are dated to c. 300–200 BCE[citation needed]. Nirodhayoga emphasizes progressive withdrawal from the contents of empirical consciousness such as thoughts, sensations etc. until purusha (Self) is realized. Terms like vichara (subtle reflection), viveka (discrimination) and others which are similar to Patanjali's terminology are mentioned, but not described.[148] There is no uniform goal of yoga mentioned in the Mahabharata. Separation of self from matter, perceiving Brahman everywhere, entering into Brahman etc. are all described as goals of yoga. Samkhya and yoga are conflated together and some verses describe them as being identical.[149] Mokshadharma also describes an early practice of elemental meditation.[150]		Mahabharata defines the purpose of yoga as the experience of uniting the individual ātman with the universal Brahman that pervades all things.[149]		This period witnessed many texts of Buddhism, Hinduism and Jainism discussing and systematically compiling yoga methods and practices. Of these, Patanjali's Yoga Sutras are considered as a key work.		During the period between the Mauryan and the Gupta eras (c. 200 BCE–500 CE) philosophical schools of Hinduism, Buddhism and Jainism were taking form and a coherent philosophical system of yoga began to emerge.[73]		Yoga as a philosophy is mentioned in Sanskrit texts dated to be completed between 200 BCE–200 CE. Kauṭilya's Arthashastra in verse 1.2.10, for example, states that there are three categories of anviksikis (philosophies) – Samkhya (nontheistic), Yoga (theistic) and Cārvāka (atheistic materialism).[151][152]		Many traditions in India began to adopt systematic methodology by about first century CE. Of these, Samkhya was probably one of the oldest philosophies to begin taking a systematic form.[153] Patanjali systematized Yoga, building them on the foundational metaphysics of Samkhya. In the early works, the Yoga principles appear together with the Samkhya ideas. Vyasa's commentary on the Yoga Sutras, also called the Samkhyapravacanabhasya (Commentary on the Exposition of the Sankhya Philosophy), describes the relation between the two systems.[154] The two schools have some differences as well. Yoga accepted the conception of "personal god", while Samkhya developed as a rationalist, non-theistic/atheistic system of Hindu philosophy.[46][155][156] Sometimes Patanjali's system is referred to as Seshvara Samkhya in contradistinction to Kapila's Nirivara Samkhya.[157]		The parallels between Yoga and Samkhya were so close that Max Müller says that "the two philosophies were in popular parlance distinguished from each other as Samkhya with and Samkhya without a Lord."[158]		In Hindu philosophy, yoga is the name of one of the six orthodox (which accept the testimony of Vedas) philosophical schools.[160][161] Karel Werner, author of Yoga And Indian Philosophy, believes that the process of systematization of yoga which began in the middle and Yoga Upanishads culminated with the Yoga Sutras of Patanjali.[note 19]		There are numerous parallels in the concepts in ancient Samkhya, Yoga and Abhidharma Buddhist schools of thought, particularly from 2nd century BCE to 1st century AD, notes Larson.[163] Patanjali's Yoga Sutras is a synthesis of these three traditions. From Samkhya, Yoga Sutras adopt the "reflective discernment" (adhyavasaya) of prakrti and purusa (dualism), its metaphysical rationalism, as well its three epistemic methods to gaining reliable knowledge.[163] From Abhidharma Buddhism's idea of nirodhasamadhi, suggests Larson, Yoga Sutras adopt the pursuit of altered state of awareness, but unlike Buddhist's concept of no self nor soul, Yoga is physicalist and realist like Samkhya in believing that each individual has a self and soul.[163] The third concept Yoga Sutras synthesize into its philosophy is the ancient ascetic traditions of meditation and introspection, as well as the yoga ideas from middle Upanishads such as Katha, Shvetashvatara and Maitri.[163]		Patanjali's Yoga Sutras are widely regarded as the first compilation of the formal yoga philosophy.[164] The verses of Yoga Sutras are terse. Many later Indian scholars studied them and published their commentaries, such as the Vyasa Bhashya (c. 350–450 CE).[165] Patanjali's yoga is also referred to as Raja yoga.[166] Patanjali defines the word "yoga" in his second sutra:		योगश्‍चित्तवृत्तिनिरोधः (yogaś citta-vṛtti-nirodhaḥ) - Yoga Sutras 1.2		This terse definition hinges on the meaning of three Sanskrit terms. I. K. Taimni translates it as "Yoga is the inhibition (nirodhaḥ) of the modifications (vṛtti) of the mind (citta)".[167] Swami Vivekananda translates the sutra as "Yoga is restraining the mind-stuff (Citta) from taking various forms (Vrittis)."[168] Edwin Bryant explains that, to Patanjali, "Yoga essentially consists of meditative practices culminating in attaining a state of consciousness free from all modes of active or discursive thought, and of eventually attaining a state where consciousness is unaware of any object external to itself, that is, is only aware of its own nature as consciousness unmixed with any other object."[45][169][170]		If the meaning of yoga is understood as the practice of nirodha (mental control), then its goal is "the unqualified state of niruddha (the perfection of that process)",[171] according to Baba Hari Dass. In that context, "yoga (union) implies duality (as in joining of two things or principles); the result of yoga is the nondual state", and "as the union of the lower self and higher Self. The nondual state is characterized by the absence of individuality; it can be described as eternal peace, pure love, Self-realization, or liberation."[172]		Patanjali's writing also became the basis for a system referred to as "Ashtanga Yoga" ("Eight-Limbed Yoga"). This eight-limbed concept is derived from the 29th Sutra of the Book 2 of Yoga Sutras. They are:		Yoga and Vedanta are the two largest surviving schools of Hindu traditions. They share many thematic principles, concepts and belief in self/soul, but diverge in degree, style and some of their methods. Epistemologically, Yoga school accepts three means to reliable knowledge, while Advaita Vedanta accepts six ways.[180] Yoga disputes the monism of Advaita Vedanta.[181] Yoga school believes that in the state of moksha, each individual discovers the blissful, liberating sense of himself or herself as an independent identity; Advaita Vedanta, in contrast, believes that in the state of moksha, each individual discovers the blissful, liberating sense of himself or herself as part of Oneness with everything, everyone and the Universal Self. They both hold that the free conscience is aloof yet transcendent, liberated and self-aware. Further, Advaita Vedanta school enjoins the use of Patanjali's yoga practices and the reading of Upanishads for those seeking the supreme good, ultimate freedom and jivanmukti.[181]		The Yoga Yajnavalkya is a classical treatise on yoga attributed to the Vedic sage Yajnavalkya. It takes the form of a dialogue between Yajnavalkya and Gargi, a renowned philosopher.[183] The text contains 12 chapters and its origin has been traced to the period between the second century BCE and fourth century CE.[184] Many yoga texts like the Hatha Yoga Pradipika, the Yoga Kundalini and the Yoga Tattva Upanishads have borrowed verses from or make frequent references to the Yoga Yajnavalkya.[185] The Yoga Yajnavalkya discusses eight yoga Asanas – Swastika, Gomukha, Padma, Vira, Simha, Bhadra, Mukta and Mayura,[186] numerous breathing exercises for body cleansing,[187] and meditation.[188]		According to Tattvarthasutra, 2nd century CE Jain text, yoga is the sum of all the activities of mind, speech and body.[5] Umasvati calls yoga the cause of "asrava" or karmic influx[189] as well as one of the essentials—samyak caritra—in the path to liberation.[189] In his Niyamasara, Acarya Kundakunda, describes yoga bhakti—devotion to the path to liberation—as the highest form of devotion.[190] Acarya Haribhadra and Acarya Hemacandra mention the five major vows of ascetics and 12 minor vows of laity under yoga. This has led certain Indologists like Prof. Robert J. Zydenbos to call Jainism, essentially, a system of yogic thinking that grew into a full-fledged religion.[191] The five yamas or the constraints of the Yoga Sutras of Patanjali bear a resemblance to the five major vows of Jainism, indicating a history of strong cross-fertilization between these traditions.[192][note 20]		Mainstream Hinduism's influence on Jain yoga can be see in Haribhadra's Yogadṛṣṭisamuccaya which outlines an eightfold yoga influenced by Patanjali's eightfold yoga.[194]		In the late phase of Indian antiquity, on the eve of the development of Classical Hinduism, the Yogacara movement arises during the Gupta period (4th to 5th centuries). Yogacara received the name as it provided a "yoga," a framework for engaging in the practices that lead to the path of the bodhisattva.[195] The yogacara sect teaches "yoga" as a way to reach enlightenment.[196]		Middle Ages saw the development of many satellite traditions of yoga. Hatha yoga emerged in this period.[197]		The Bhakti movement was a development in medieval Hinduism which advocated the concept of a personal God (or "Supreme Personality of Godhead"). The movement was initiated by the Alvars of South India in the 6th to 9th centuries, and it started gaining influence throughout India by the 12th to 15th centuries.[198] Shaiva and Vaishnava bhakti traditions integrated aspects of Yoga Sutras, such as the practical meditative exercises, with devotion.[199] Bhagavata Purana elucidates the practice of a form of yoga called viraha (separation) bhakti. Viraha bhakti emphasizes one pointed concentration on Krishna.[200]		Tantra is a genre of yoga that arose in India no later than the 5th century CE.[201][note 21] George Samuel states, "Tantra" is a contested term, but may be considered as a school whose practices appeared in mostly complete form in Buddhist and Hindu texts by about 10th century CE.[65] Over its history, some ideas of Tantra school influenced the Hindu, Bon, Buddhist, and Jain traditions. Elements of Tantric yoga rituals were adopted by and influenced state functions in medieval Buddhist and Hindu kingdoms in East and Southeast Asia.[203][204]		By the turn of the first millennium, hatha yoga emerged from tantra.[14][15]		Vajrayana is also known as Tantric Buddhism and Tantrayāna. Its texts were compiled starting with 7th century and Tibetan translations were completed in 8th century CE. These tantra yoga texts were the main source of Buddhist knowledge that was imported into Tibet.[205] They were later translated into Chinese and other Asian languages, helping spread ideas of Tantric Buddhism. The Buddhist text Hevajra Tantra and Caryāgiti introduced hierarchies of chakras.[206] Yoga is a significant practice in Tantric Buddhism.[61][207][208]		The earliest references to hatha yoga are in Buddhist works dating from the eighth century.[209] The earliest definition of hatha yoga is found in the 11th century Buddhist text Vimalaprabha, which defines it in relation to the center channel, bindu etc.[210] Hatha yoga synthesizes elements of Patanjali's Yoga Sutras with posture and breathing exercises.[211] It marks the development of asanas (plural) into the full body 'postures' now in popular usage[212] and, along with its many modern variations, is the style that many people associate with the word yoga today.[213]		Various yogic groups had become prominent in Punjab in the 15th and 16th century, when Sikhism was in its nascent stage. Compositions of Guru Nanak, the founder of Sikhism, describe many dialogues he had with Jogis, a Hindu community which practiced yoga.[214] Guru Nanak rejected the austerities, rites and rituals connected with Hatha Yoga.[215] He propounded the path of Sahaja yoga or Nama yoga (meditation on the name) instead.[216] The Guru Granth Sahib states:		Listen "O Yogi, Nanak tells nothing but the truth. You must discipline your mind. The devotee must meditate on the Word Divine. It is His grace which brings about the union. He understands, he also sees. Good deeds help one merge into Divination."[217]		Yoga came to the attention of an educated western public in the mid-19th century along with other topics of Indian philosophy. In the context of this budding interest, N. C. Paul published his Treatise on Yoga Philosophy in 1851.		The first Hindu teacher to actively advocate and disseminate aspects of yoga to a western audience, Swami Vivekananda, toured Europe and the United States in the 1890s.[218] The reception which Swami Vivekananda received built on the active interest of intellectuals, in particular the New England Transcendentalists, among them Ralph Waldo Emerson (1803–1882), who drew on German Romanticism and the interest of philosophers and scholars like G. W. F. Hegel (1770–1831), the brothers August Wilhelm Schlegel (1767–1845) and Karl Wilhelm Friedrich Schlegel (1772–1829), Max Mueller (1823–1900), Arthur Schopenhauer (1788–1860), and others who had (to varying degrees) interests in things Indian.[219][220]		Australia's Bette Calman is the oldest female yoga teacher at 83 years old. She teaches at the Indian mental and physical discipline of yoga.[221]		Theosophists also had a large influence on the American public's view of Yoga.[222] Esoteric views current at the end of the 19th century provided a further basis for the reception of Vedanta and of Yoga with its theory and practice of correspondence between the spiritual and the physical.[223] The reception of Yoga and of Vedanta thus entwined with each other and with the (mostly Neoplatonism-based) currents of religious and philosophical reform and transformation throughout the 19th and early 20th centuries. M. Eliade, himself rooted in the Romanian currents of these traditions,[citation needed] brought a new element into the reception of Yoga with the strong emphasis on Tantric Yoga in his seminal book: Yoga: Immortality and Freedom.[note 22] With the introduction of the Tantra traditions and philosophy of Yoga, the conception of the "transcendent" to be attained by Yogic practice shifted from experiencing the "transcendent" ("Atman-Brahman" in Advaitic theory) in the mind to the body itself.[224]		The American born yogi by the name of Pierre Arnold Bernard, after his travels through the lands of Kashmir and Bengal, founded the Tantrik Order of America in 1905. His teachings gave many westerners their first glimpse into the practices of yoga and tantra.[225]		The modern scientific study of yoga began with the works of N. C. Paul and Major D. Basu in the late 19th century, and then continued in the 20th century with Sri Yogendra (1897–1989) and Swami Kuvalayananda.[226] Western medical researchers came to Swami Kuvalayananda's Kaivalyadhama Health and Yoga Research Center, starting in 1928, to study Yoga as a science.[227]		Outside of Buddhist, Hindu and Jain traditions in Asia, the term "yoga" has been usually synonymous with its asanas (postures) or as a form of exercise.[228] This aspect of Yoga was adopted as a cultural trend in Europe and North America starting in the first half of the 20th century. There were periods of criticism and paranoia against yoga as well.[222] By the 1960s, western interest in Hindu spirituality reached its peak, giving rise to a great number of Neo-Hindu schools specifically advocated to a western public. During this period, most of the influential Indian teachers of yoga came from two lineages, those of Sivananda Saraswati (1887–1963) and of Tirumalai Krishnamacharya (1888–1989).[229] Teachers of Hatha yoga who were active in the west in this period included B.K.S. Iyengar (1918–2014), K. Pattabhi Jois (1915–2009), Swami Vishnu-devananda (1927–1993), and Swami Satchidananda (1914–2002).[230][231][232] Yogi Bhajan brought Kundalini Yoga to the United States in 1969.[233] Comprehensive, classical teachings of Ashtanga Yoga, Samkhya, the subtle body theory, Fitness Asanas, and tantric elements were included in the yoga teachers training by Baba Hari Dass (1923–), in the United States and Canada.[234]		A second "yoga boom" followed in the 1980s, as Dean Ornish, a follower of Swami Satchidananda, connected yoga to heart health, legitimizing yoga as a purely physical system of health exercises outside of counter-culture or esotericism circles, and unconnected to any religious denomination.[218] Numerous asanas seemed modern in origin, and strongly overlapped with 19th and early-20th century Western exercise traditions.[235]		Since 2001, the popularity of yoga in the USA has expanded. The number of people who practiced some form of yoga has grown from 4 million (in 2001) to 20 million (in 2011). It has drawn support from world leaders such as Barack Obama who stated, "Yoga has become a universal language of spiritual exercise in the United States, crossing many lines of religion and cultures,... Every day, millions of people practice yoga to improve their health and overall well-being. That's why we're encouraging everyone to take part in PALA (Presidential Active Lifestyle Award), so show your support for yoga and answer the challenge".[236]		The American College of Sports Medicine supports the integration of yoga into the exercise regimens of healthy individuals as long as properly-trained professionals deliver instruction. The College cites yoga's promotion of "profound mental, physical and spiritual awareness" and its benefits as a form of stretching, and as an enhancer of breath control and of core strength.[237]		Yoga has been studied and is increasingly recommended to promote relaxation, reduce stress and some medical conditions such as premenstrual syndrome in Europe as well as in the United States.[238] According to Dupler and Frey, Yoga is a low-impact activity that can provide the same benefits as "any well-designed exercise program, increasing general health and stamina, reducing stress, and improving those conditions brought about by sedentary lifestyles". It is particularly suited, add Dupler and Frey, as a physical therapy routine, and as a regimen to strengthen and balance all parts of the body.[238] Yoga has also been used as a complete exercise program and physical therapy routine.[238]		In 2015 the Australian Government's Department of Health published the results of a review of alternative therapies that sought to determine if any were suitable for being covered by health insurance; Yoga was one of 17 practices evaluated for which no clear evidence of effectiveness was found, with the caveat that "Reviewers were limited in drawing definite conclusions, not only due to a lack of studies for some clinical conditions, but also due to the lack of information reported in the reviews and potentially in the primary studies."[239]		While the practice of yoga continues to rise in contemporary American culture, sufficient and adequate knowledge of the practice's origins does not. According to Andrea R. Jain, Yoga is being marketed as a supplement to a cardio routine with health benefits, but in Hinduism it is more than exercise and incorporates meditation with spiritual benefits.[240]		While much of the medical community regards the results of yoga research as significant, others point to many flaws which undermine results. Much of the research on yoga has taken the form of preliminary studies or clinical trials of low methodological quality, including small sample sizes, inadequate blinding, lack of randomization, and high risk of bias.[241][242][243] A 2013 systematic review found strong evidence that yoga was effective for low back pain in the short-term, and moderate evidence that it was effective in the long-term.[244]		There has been an emergence of studies investigating yoga as a complementary intervention for cancer patients. Yoga is used for treatment of cancer patients to decrease depression, insomnia, pain, and fatigue and to increase anxiety control.[245] Mindfulness Based Stress Reduction (MBSR) programs include yoga as a mind-body technique to reduce stress. A study found that after seven weeks the group treated with yoga reported significantly less mood disturbance and reduced stress compared to the control group. Another study found that MBSR had showed positive effects on sleep anxiety, quality of life, and spiritual growth in cancer patients.[246]		Yoga has also been studied as a treatment for schizophrenia.[247] Some encouraging, but inconclusive, evidence suggests that yoga as a complementary treatment may help alleviate symptoms of schizophrenia and improve health-related quality of life.[20]		Implementation of the Kundalini Yoga Lifestyle has shown to help substance abuse addicts increase their quality of life according to psychological questionnaires like the Behavior and Symptom Identification Scale and the Quality of Recovery Index.[248]		Yoga has been shown in a study to have some cognitive functioning (executive functioning, including inhibitory control) acute benefit.[249]		A 2016 systematic review and meta-analysis found no evidence that yoga was effective for metabolic syndrome.[250]		A small percentage of yoga practitioners each year suffer physical injuries analogous to sports injuries;[251] therefore, caution and common sense are recommended.[252] Yoga has been criticized for being potentially dangerous and being a cause for a range of serious medical conditions including thoracic outlet syndrome, degenerative arthritis of the cervical spine, spinal stenosis, retinal tears, damage to the common fibular nerve, "Yoga foot drop,"[253] etc. An exposé of these problems by William Broad published in January, 2012 in The New York Times Magazine[254] resulted in controversy within the international yoga community. Broad, a science writer, yoga practitioner, and author of The Science of Yoga: The Risks and the Rewards,[255] had suffered a back injury while performing a yoga posture.[256] Torn muscles, knee injuries,[257] and headaches are common ailments which may result from yoga practice.[258]		An extensive survey of yoga practitioners in Australia showed that about 20% had suffered some physical injury while practicing yoga. In the previous 12 months 4.6% of the respondents had suffered an injury producing prolonged pain or requiring medical treatment. Headstands, shoulder stands, lotus and half lotus (seated cross-legged position), forward bends, backward bends, and handstands produced the greatest number of injuries.[251]		Some yoga practitioners do not recommend certain yoga exercises for women during menstruation, for pregnant women, or for nursing mothers. However, meditation, breathing exercises, and certain postures which are safe and beneficial for women in these categories are encouraged.[259]		Among the main reasons that experts cite for causing negative effects from yoga are beginners' competitiveness and instructors' lack of qualification. As the demand for yoga classes grows, many people get certified to become yoga instructors, often with relatively little training. Not every newly certified instructor can evaluate the condition of every new trainee in their class and recommend refraining from doing certain poses or using appropriate props to avoid injuries. In turn, a beginning yoga student can overestimate the abilities of their body and strive to do advanced poses before their body is flexible or strong enough to perform them.[254][258]		Vertebral artery dissection, a tear in the arteries in the neck which provide blood to the brain can result from rotation of the neck while the neck is extended. This can occur in a variety of contexts, but is an event which could occur in some yoga practices. This is a very serious condition which can result in a stroke.[260][261]		Acetabular labral tears, damage to the structure joining the femur and the hip, have been reported to have resulted from yoga practice.[262]		It is claimed that yoga can be an excellent training for children and adolescents, both as a form of physical exercise and for breathing, focus, mindfulness, and stress relief: many school districts have considered incorporating yoga into their P.E. programs. The Encinitas, California school district gained a San Diego Superior Court Judge's approval to use yoga in P.E., holding against the parents who claimed the practice was intrinsically religious and hence should not be part of a state funded program.[263]		Over time, an extended yoga physiology developed, especially within the tantric tradition and hatha yoga. It pictures humans as composed of three bodies or five sheaths which cover the atman. The three bodies are described within the Mandukya Upanishad, which adds a fourth state, turiya, while the five sheaths (pancha-kosas) are described in the Taittiriya Upanishad.[264] They are often integrated:		Within the subtle body energy flows through the nadis or channels, and is concentrated within the chakras.		Zen, the name of which derives from the Sanskrit "dhyāna" via the Chinese "ch'an"[note 23] is a form of Mahayana Buddhism. The Mahayana school of Buddhism is noted for its proximity with yoga.[267] In the west, Zen is often set alongside yoga; the two schools of meditation display obvious family resemblances.[268] This segregation deserves attention because yogic practices integrally exist within the Zen Buddhist school.[note 24] Certain essential elements of yoga are important both for Buddhism in general and for Zen in particular.[269]		In the Nyingma tradition, the path of meditation practice is divided into nine yanas, or vehicles, which are said to be increasingly profound.[270] The last six are described as "yoga yanas": "Kriya yoga", "Upa yoga," "Yoga yana," "Mahā yoga," "Anu yoga" and the ultimate practice, "Ati yoga."[271] The Sarma traditions also include Kriya, Upa (called "Charya"), and Yoga, with the Anuttara yoga class substituting for Mahayoga and Atiyoga.[272]		Other tantra yoga practices include a system of 108 bodily postures practiced with breath and heart rhythm. The Nyingma tradition also practices Yantra yoga (Tib. "Trul khor"), a discipline that includes breath work (or pranayama), meditative contemplation and precise dynamic movements to centre the practitioner.[273] The body postures of Tibetan ancient yogis are depicted on the walls of the Dalai Lama's summer temple of Lukhang. A semi-popular account of Tibetan yoga by Chang (1993) refers to caṇḍalī (Tib. "tummo"), the generation of heat in one's own body, as being "the very foundation of the whole of Tibetan yoga."[274] Chang also claims that Tibetan yoga involves reconciliation of apparent polarities, such as prana and mind, relating this to theoretical implications of tantrism.		Some Christians integrate yoga and other aspects of Eastern spirituality with prayer and meditation. This has been attributed to a desire to experience God in a more complete way.[275] In 2013, Monsignor Raffaello Martinelli, servicing Congregation for the Doctrine of the Faith, having worked for over 23 years with Cardinal Joseph Ratzinger (Pope Benedict XVI),[276] said that for his Meditation, a Christian can learn from other religious traditions (zen, yoga, controlled respiration, Mantra), quoting Aspects of Christian meditation: "Just as "the Catholic Church rejects nothing of what is true and holy in these religions," neither should these ways be rejected out of hand simply because they are not Christian. On the contrary, one can take from them what is useful so long as the Christian conception of prayer, its logic and requirements are never obscured. It is within the context of all of this that these bits and pieces should be taken up and expressed anew."[277] Previously, the Roman Catholic Church, and some other Christian organizations have expressed concerns and disapproval with respect to some eastern and New Age practices that include yoga and meditation.[278][279][280]		In 1989 and 2003, the Vatican issued two documents: Aspects of Christian meditation and "A Christian reflection on the New Age," that were mostly critical of eastern and New Age practices. The 2003 document was published as a 90-page handbook detailing the Vatican's position.[281] The Vatican warned that concentration on the physical aspects of meditation "can degenerate into a cult of the body" and that equating bodily states with mysticism "could also lead to psychic disturbance and, at times, to moral deviations." Such has been compared to the early days of Christianity, when the church opposed the gnostics' belief that salvation came not through faith but through a mystical inner knowledge.[275] The letter also says, "one can see if and how [prayer] might be enriched by meditation methods developed in other religions and cultures"[282] but maintains the idea that "there must be some fit between the nature of [other approaches to] prayer and Christian beliefs about ultimate reality."[275] Some fundamentalist Christian organizations consider yoga to be incompatible with their religious background, considering it a part of the New Age movement inconsistent with Christianity.[283]		Another view holds that Christian meditation can lead to religious pluralism. This is held by an interdenominational association of Christians that practice it. "The ritual simultaneously operates as an anchor that maintains, enhances, and promotes denominational activity and a sail that allows institutional boundaries to be crossed." [284]		In early 11th century, the Persian scholar Al Biruni visited India, lived with Hindus for 16 years, and with their help translated several significant Sanskrit works into Arabic and Persian languages. One of these was Patanjali's Yogasutras.[285][286] Al Biruni's translation preserved many of the core themes of Patañjali 's Yoga philosophy, but certain sutras and analytical commentaries were restated making it more consistent with Islamic monotheistic theology.[285][287] Al Biruni's version of Yoga Sutras reached Persia and Arabian peninsula by about 1050 AD. Later, in the 16th century, the hath yoga text Amritakunda was translated into Arabic and then Persian.[288] Yoga was, however, not accepted by mainstream Sunni and Shia Islam. Minority Islamic sects such as the mystic Sufi movement, particularly in South Asia, adopted Indian yoga practises, including postures and breath control.[289][290] Muhammad Ghawth, a Shattari Sufi and one of the translators of yoga text in 16th century, drew controversy for his interest in yoga and was persecuted for his Sufi beliefs.[291]		Malaysia's top Islamic body in 2008 passed a fatwa, prohibiting Muslims from practicing yoga, saying it had elements of Hinduism and that its practice was blasphemy, therefore haraam.[292] Some Muslims in Malaysia who had been practicing yoga for years, criticized the decision as "insulting."[293] Sisters in Islam, a women's rights group in Malaysia, also expressed disappointment and said yoga was just a form of exercise.[294] This fatwa is legally enforceable.[295] However, Malaysia's prime minister clarified that yoga as physical exercise is permissible, but the chanting of religious mantras is prohibited.[296]		In 2009, the Council of Ulemas, an Islamic body in Indonesia, passed a fatwa banning yoga on the grounds that it contains Hindu elements.[297] These fatwas have, in turn, been criticized by Darul Uloom Deoband, a Deobandi Islamic seminary in India.[298] Similar fatwas banning yoga, for its link to Hinduism, were issued by the Grand Mufti Ali Gomaa in Egypt in 2004, and by Islamic clerics in Singapore earlier.[299]		In Iran, as of May 2014, according to its Yoga Association, there were approximately 200 yoga centres in the country, a quarter of them in the capital Tehran, where groups can often be seen practising in parks. This has been met by opposition among conservatives.[300] In May 2009, Turkey's head of the Directorate of Religious Affairs, Ali Bardakoğlu, discounted personal development techniques such as reiki and yoga as commercial ventures that could lead to extremism. His comments were made in the context of reiki and yoga possibly being a form of proselytization at the expense of Islam.[301]		On 11 December 2014, The 193-member United Nations General Assembly approved by consensus, a resolution establishing 21 June as "International Day of Yoga".[302] The declaration of this day came after the call for its adoption as International Day of Yoga by Indian Prime Minister Narendra Modi during his address to UN General Assembly on 27 September 2014.[303][304][305][306][307] In suggesting 21 June, which is one of the two solstices, as the International Day of Yoga, Narendra Modi had said that the date is the longest day of the year in the Northern Hemisphere and has special significance in many parts of the world.[308]		The first International Day of Yoga was observed world over on 21 June 2015. About 35,000 people, including Indian Prime Minister Narendra Modi and a large number of dignitaries, performed 21 yoga asanas (yoga postures) for 35 minutes at Rajpath in New Delhi. The day devoted to yoga was observed by millions across the world.[309] The event at Rajpath established two Guinness records – largest Yoga Class with 35,985 people and the record for the most nationalities participating in it—eighty four.[310]		|group2 = See also |list2 =		|below = }}		
Walking (also known as ambulation) is one of the main gaits of locomotion among legged animals. Walking is typically slower than running and other gaits. Walking is defined by an 'inverted pendulum' gait in which the body vaults over the stiff limb or limbs with each step. This applies regardless of the number of limbs—even arthropods, with six, eight or more limbs, walk.						The word walk is descended from the Old English wealcan "to roll". In humans and other bipeds, walking is generally distinguished from running in that only one foot at a time leaves contact with the ground and there is a period of double-support. In contrast, running begins when both feet are off the ground with each step. This distinction has the status of a formal requirement in competitive walking events. For quadrupedal species, there are numerous gaits which may be termed walking or running, and distinctions based upon the presence or absence of a suspended phase or the number of feet in contact any time do not yield mechanically correct classification.[1] The most effective method to distinguish walking from running is to measure the height of a person's centre of mass using motion capture or a force plate at midstance. During walking, the centre of mass reaches a maximum height at midstance, while during running, it is then at a minimum. This distinction, however, only holds true for locomotion over level or approximately level ground. For walking up grades above 9%, this distinction no longer holds for some individuals. Definitions based on the percentage of the stride during which a foot is in contact with the ground (averaged across all feet) of greater than 50% contact corresponds well with identification of 'inverted pendulum' mechanics and are indicative of walking for animals with any number of limbs, although this definition is incomplete.[1] Running humans and animals may have contact periods greater than 50% of a gait cycle when rounding corners, running uphill or carrying loads.		Speed is another factor that distinguishes walking from running. Although walking speeds can vary greatly depending on many factors such as height, weight, age, terrain, surface, load, culture, effort, and fitness, the average human walking speed is about 5.0 kilometres per hour (km/h), or about 3.1 miles per hour (mph). Specific studies have found pedestrian walking speeds ranging from 4.51 kilometres per hour (2.80 mph) to 4.75 kilometres per hour (2.95 mph) for older individuals and from 5.32 kilometres per hour (3.31 mph) to 5.43 kilometres per hour (3.37 mph) for younger individuals;[2][3] a brisk walking speed can be around 6.5 kilometres per hour (4.0 mph).[4] Champion racewalkers can average more than 14 kilometres per hour (8.7 mph) over a distance of 20 kilometres (12 mi).		An average human child achieves independent walking ability at around 11 months old.[5]		Regular, brisk exercise of any kind can improve confidence, stamina, energy, weight control and life expectancy and reduce stress.[citation needed] It can also reduce the risk of coronary heart disease, strokes, diabetes, high blood pressure, bowel cancer and osteoporosis.[citation needed] Scientific studies have also shown that walking, besides its physical benefits, is also beneficial for the mind, improving memory skills, learning ability, concentration and abstract reasoning,[citation needed] as well as ameliorating spirits.[clarification needed] Sustained walking sessions for a minimum period of thirty to sixty minutes a day, five days a week, with the correct walking posture,[6] reduce health risks and have various overall health benefits, such as reducing the chances of cancer, type 2 diabetes, heart disease, anxiety disorder and depression.[7] Life expectancy is also increased even for individuals suffering from obesity or high blood pressure. Walking also improves bone health, especially strengthening the hip bone, and lowering the harmful low-density lipoprotein (LDL) cholesterol, and raising the useful high-density lipoprotein (HDL) cholesterol.[8] Studies have found that walking may also help prevent dementia and Alzheimer's.[9]		The Centers for Disease Control and Prevention's fact sheet on the "Relationship of Walking to Mortality Among U.S. Adults with Diabetes" states that those with diabetes who walked for 2 or more hours a week lowered their mortality rate from all causes by 39 per cent. "Walking lengthened the life of people with diabetes regardless of age, sex, race, body mass index, length of time since diagnosis, and presence of complications or functional limitations."[10] It has been suggested that there is a relationship between the speed of walking and health, and that the best results are obtained with a speed of more than 2.5 mph (4 km/h).[11]		Governments now recognize the benefits of walking for mental and physical health and are actively encouraging it. This growing emphasis on walking has arisen because people walk less nowadays than previously. In the UK, a Department of Transport report[12] found that between 1995/97 and 2005 the average number of walk trips per person fell by 16%, from 292 to 245 per year. Many professionals in local authorities and the NHS are employed to halt this decline by ensuring that the built environment allows people to walk and that there are walking opportunities available to them. Professionals working to encourage walking come mainly from six sectors: health, transport, environment, schools, sport and recreation, and urban design.		One programme to encourage walking is "The Walking the Way to Health Initiative", organized by the British walkers association The Ramblers, which is the largest volunteer led walking scheme in the United Kingdom. Volunteers are trained to lead free Health Walks from community venues such as libraries and doctors' surgeries. The scheme has trained over 35,000 volunteers and have over 500 schemes operating across the UK, with thousands of people walking every week.[13] A new organization called "Walk England" launched a web site in June 2008 to provide these professionals with evidence, advice and examples of success stories of how to encourage communities to walk more. The site has a social networking aspect to allow professionals and the public to ask questions, post news and events and communicate with others in their area about walking, as well as a "walk now" option to find out what walks are available in each region. Similar organizations exist in other countries and recently a "Walking Summit" was held in the United States. This "assembl[ed] thought-leaders and influencers from business, urban planning and real estate, [along with] physicians and public health officials," and others, to discuss how to make American cities and communities places where "people can and want to walk".[14]		It is theorized that "walking" among tetrapods originated underwater with air-breathing fish that could "walk" underwater, giving rise to the plethora of land-dwelling life that walk on four or two limbs.[15] While terrestrial tetrapods are theorised to have a single origin, arthropods and their relatives are thought to have independently evolved walking several times, specifically in insects, myriapods, chelicerates, tardigrades, onychophorans, and crustaceans.[16]		Judging from footprints discovered on a former shore in Kenya, it is thought possible that ancestors of modern humans were walking in ways very similar to the present activity as many as 1.5 million years ago.[17][18]		Human walking is accomplished with a strategy called the double pendulum. During forward motion, the leg that leaves the ground swings forward from the hip. This sweep is the first pendulum. Then the leg strikes the ground with the heel and rolls through to the toe in a motion described as an inverted pendulum. The motion of the two legs is coordinated so that one foot or the other is always in contact with the ground. The process of walking recovers approximately sixty per cent of the energy used due to pendulum dynamics and ground reaction force.[25][26]		Walking differs from a running gait in a number of ways. The most obvious is that during walking one leg always stays on the ground while the other is swinging. In running there is typically a ballistic phase where the runner is airborne with both feet in the air (for bipedals).		Another difference concerns the movement of the centre of mass of the body. In walking the body "vaults" over the leg on the ground, raising the centre of mass to its highest point as the leg passes the vertical, and dropping it to the lowest as the legs are spread apart. Essentially kinetic energy of forward motion is constantly being traded for a rise in potential energy. This is reversed in running where the centre of mass is at its lowest as the leg is vertical. This is because the impact of landing from the ballistic phase is absorbed by bending the leg and consequently storing energy in muscles and tendons. In running there is a conversion between kinetic, potential, and elastic energy.		There is an absolute limit on an individual's speed of walking (without special techniques such as those employed in speed walking) due to the upwards acceleration of the centre of mass during a stride – if it's greater than the acceleration due to gravity the person will become airborne as they vault over the leg on the ground. Typically however, animals switch to a run at a lower speed than this due to energy efficiencies.		Many people enjoy walking as a recreation in the mainly urban modern world, and it is one of the best forms of exercise.[27] For some, walking is a way to enjoy nature and the outdoors; and for others the physical, sporting and endurance aspect is more important.		There are a variety of different kinds of walking, including bushwalking, racewalking, beach walking, hillwalking, volksmarching, Nordic walking, trekking and hiking. Some people prefer to walk indoors on a treadmill, or in a gym, and fitness walkers and others may use a pedometer to count their steps. Hiking is the usual word used in Canada, the United States and South Africa for long vigorous walks; similar walks are called tramps in New Zealand, or hill walking or just walking in Australia, the UK and the Irish Republic. Australians also bushwalk. In English-speaking parts of North America the term walking is used for short walks, especially in towns and cities. Snow shoeing is walking in snow; a slightly different gait is required compared with regular walking.		In terms of tourism the possibilities range from guided walking tours in cities, to organized trekking holidays in the Himalayas. In the UK the term walking tour also refers to a multi-day walk or hike undertaken by a group or individual. Well-organized systems of trails exist in many other European counties, as well as Canada, United States, New Zealand, and Nepal. Systems of lengthy waymarked walking trails now stretch across Europe from Norway to Turkey, Portugal to Cyprus.[28] Many also walk the traditional pilgrim routes, of which the most famous is El Camino de Santiago, The Way of St. James.		Numerous walking festivals and other walking events take place each year in many countries. The world's largest multi-day walking event is the International Four Days Marches Nijmegen in the Netherlands. The "Vierdaagse" (Dutch for "Four day Event") is an annual walk that has taken place since 1909; it has been based at Nijmegen since 1916. Depending on age group and category, walkers have to walk 30, 40 or 50 kilometers each day for four days.[citation needed] Originally a military event with a few civilians, it now is a mainly civilian event. Numbers have risen in recent years, with over 40,000 now taking part, including about 5,000 military personnel.[citation needed] Due to crowds on the route, since 2004 the organizers have limited the number of participants. In the U.S., there is the annual Labor Day walk on Mackinac Bridge, Michigan, which draws over 60,000 participants; it is the largest single-day walking event;[citation needed] while the Chesapeake Bay Bridge Walk in Maryland draws over 50,000 participants each year.[citation needed] There are also various walks organised as charity events, with walkers sponsored for a specific cause. These walks range in length from two miles (3 km) or five km to 50 miles (80 km). The MS Challenge Walk is an 80 km or 50 mile walk which raises money to fight multiple sclerosis, while walkers in the Oxfam Trailwalker cover 100 km or 60 miles.		In Britain, The Ramblers, a registered charity, is the largest organisation that looks after the interests of walkers, with some 139,000 members.[citation needed] Its "Get Walking Keep Walking" project provides free route guides, led walks, as well as information for people new to walking.[29] The Long Distance Walkers Association in the UK is for the more energetic walker, and organizes lengthy challenge hikes of 20 or even 50 miles (30 to 80 km) or more in a day. The LDWA's annual "Hundred" event, entailing walking 100 miles or 160 km in 48 hours, takes place each British Spring Bank Holiday weekend.[30]		There has been a recent focus among urban planners in some communities to create pedestrian-friendly areas and roads, allowing commuting, shopping and recreation to be done on foot. The concept of walkability has arisen as a measure of the degree to which an area is friendly to walking. Some communities are at least partially car-free, making them particularly supportive of walking and other modes of transportation. In the United States, the active living network is an example of a concerted effort to develop communities more friendly to walking and other physical activities.		An example of such efforts to make urban development more pedestrian friendly is the pedestrian village. This is a compact, pedestrian-oriented neighborhood or town, with a mixed-use village center, that follows the tenets of New Pedestrianism.[31][32] Shared-use lanes for pedestrians and those using bicycles, Segways, wheelchairs, and other small rolling conveyances that do not use internal combustion engines. Generally, these lanes are in front of the houses and businesses, and streets for motor vehicles are always at the rear. Some pedestrian villages might be nearly car-free with cars either hidden below the buildings or on the periphery of the village. Venice, Italy is essentially a pedestrian village with canals. The canal district in Venice, California, on the other hand, combines the front lane/rear street approach with canals and walkways, or just walkways.[31][33][34]		Walking is also considered to be a clear example of a sustainable mode of transport, especially suited for urban use and/or relatively shorter distances. Non-motorised transport modes such as walking, but also cycling, small-wheeled transport (skates, skateboards, push scooters and hand carts) or wheelchair travel are often key elements of successfully encouraging clean urban transport.[35] A large variety of case studies and good practices (from European cities and some worldwide examples) that promote and stimulate walking as a means of transportation in cities can be found at Eltis, Europe's portal for local transport.[36]		The development of specific rights of way with appropriate infrastructure can promote increased participation and enjoyment of walking. Examples of types of investment include pedestrian malls, and foreshoreways such as oceanways and also river walks.		The first purpose-built pedestrian street in Europe is the Lijnbaan in Rotterdam, opened in 1953. The first pedestrianised shopping centre in the United Kingdom was in Stevenage in 1959. A large number of European towns and cities have made part of their centres car-free since the early 1960s. These are often accompanied by car parks on the edge of the pedestrianised zone, and, in the larger cases, park and ride schemes. Central Copenhagen is one of the largest and oldest: It was converted from car traffic into pedestrian zone in 1962.		The first successful attempts at walking robots tended to have six legs. The number of legs was reduced as microprocessor technology advanced, and there are now a number of robots that can walk on two legs. One, for example, is ASIMO. Although robots have taken great strides in advancement, they still don't walk nearly as well as human beings as they often need to keep their knees bent permanently in order to improve stability.		In 2009, Japanese roboticist Tomotaka Takahashi developed a robot that can jump three inches off the ground. The robot, named Ropid, is capable of getting up, walking, running, and jumping.[37]		The walk is a four-beat gait that averages about 4 miles per hour (6.4 km/h). When walking, a horse's legs follow this sequence: left hind leg, left front leg, right hind leg, right front leg, in a regular 1-2-3-4 beat. At the walk, the horse will always have one foot raised and the other three feet on the ground, save for a brief moment when weight is being transferred from one foot to another. A horse moves its head and neck in a slight up and down motion that helps maintain balance.[38]		Ideally, the advancing rear hoof oversteps the spot where the previously advancing front hoof touched the ground. The more the rear hoof oversteps, the smoother and more comfortable the walk becomes. Individual horses and different breeds vary in the smoothness of their walk. However, a rider will almost always feel some degree of gentle side-to-side motion in the horse's hips as each hind leg reaches forward.		The fastest "walks" with a four-beat footfall pattern are actually the lateral forms of ambling gaits such as the running walk, singlefoot, and similar rapid but smooth intermediate speed gaits. If a horse begins to speed up and lose a regular four-beat cadence to its gait, the horse is no longer walking, but is beginning to either trot or pace.		Elephants can move both forwards and backwards, but cannot trot, jump, or gallop. They use only two gaits when moving on land, the walk and a faster gait similar to running.[39] In walking, the legs act as pendulums, with the hips and shoulders rising and falling while the foot is planted on the ground. With no "aerial phase", the fast gait does not meet all the criteria of running, although the elephant uses its legs much like other running animals, with the hips and shoulders falling and then rising while the feet are on the ground.[40] Fast-moving elephants appear to 'run' with their front legs, but 'walk' with their hind legs and can reach a top speed of 18 km/h (11 mph).[41] At this speed, most other quadrupeds are well into a gallop, even accounting for leg length.		Walking fish, sometimes called ambulatory fish, is a general term that refers to fish that are able to travel over land for extended periods of time. The term may also be used for some other cases of nonstandard fish locomotion, e.g., when describing fish "walking" along the sea floor, as the handfish or frogfish.		|group2 = See also |list2 =		|below = }}		
Middle-distance running events are track races longer than sprints, up to 3000 metres. The standard middle distances are the 800 metres, 1500 metres and mile run, although the 3000 metres may also be classified as a middle-distance event.[1] The 1500 m came about as a result of running three laps of a 500 m outdoor track or six laps of a 250m indoor track,[2] which were commonplace in continental Europe in the 20th century.[3]						A very uncommon middle-distance event that is sometimes run by sprinters for muscle stamina training.		This was a popular distance, particularly indoors, when imperial distances were common. In 1882, American Lon Myers set what was then a world record at 600 yards (548.64 metres), running it in 1:11.4.[4] The event was a common event for most American students because it was one of the standardized test events as part of the President's Award on Physical Fitness.[5] In the early 1970s, Martin McGrady was unsuccessful at longer or shorter races, but made his reputation, set world records and drew many fans to arenas to watch him race elite Olympians at this odd distance.		This middle distance length is rather uncommon, and is mainly run by sprinters wishing to test their endurances at a longer distance. Like other middle distance races, it evolved from the 600 yard race. The 600 m is also used as an early season stepping stone by 800 m runners before they have reached full race fitness.		Johnny Gray (United States) holds the record for men: 1:12.81, Santa Monica, 24 May 1986.		Ana Fidelia Quirot (Cuba) holds the women's record: 1:22.63, Guadalajara, 25 July 1997.		The 800 m consists of two laps around a standard 400 m track, and has always been an Olympic event. It was included in the first women's track programme in 1928, but suspended until 1960 because of shock and the exhaustion it caused the competitors. Without the benefits of modern training, men of the era were, in contrast, expected to run themselves to complete exhaustion during competitions[citation needed].		David Rudisha (Kenya) is the current recordholder: 1:40.91, London, 9 August 2012. Jarmila Kratochvílová (Czechoslovakia) set the current women's record: 1:53.28, Munich, 26 July 1983.[6]		The 880 yard run, or half mile, was the forebear to the 800 m distance and has its roots in competitions in the United Kingdom in the 1830s.[7]		This distance is not commonly raced, though it is more common than the 500 m event is for sprinters. This is commonly raced as an indoor men's heptathlon event, or as an indoor high school event. In 1881, Lon Myers set what was then a world record at 1000 yards, running it in 2:13.0.[4]		The men's record is held by Noah Ngeny (Kenya) (2:11.96, Rieti, 5 September 1999), while Svetlana Masterkova (Russia) set the women's record (2:28.98, Brussels, 23 August 1996).[6]		See also 1000 metres world record progression.		Three laps. A distance seldom raced on its own, but commonly raced as part of the distance medley relay.		There is no recorded world records or world bests. However, Hicham El Guerrouj (Morocco) is believed to be the fastest man at this distance: 2:44.75, Rieti, 2002.[8]		Also known as the metric mile, this is a premier middle-distance race, covering three and three-quarter laps around a standard Olympic-sized track. In recent years, races over this distance have become more of a prolonged sprint, with each lap averaging 55 seconds for the world record performance by Hicham El Guerrouj of Morocco: 3:26.00 on 14 July 1998 at Rome (two 1:50 min 800 m performances back to back).[6] Thus, speed is necessary, and it seems that the more aerobic conditioning, the better. Genzebe Dibaba from Ethiopia holds the women's world record: 3:50.07 set in Monaco on 17 July 2015.[6]		This is a difficult distance at which to compete mentally, in addition to being one of the more tactical middle-distance track events. The distance is often witness to some of the most tactical, physical races in the sport, as many championship races are won in the final few metres.		At exactly four laps of a normal 400 m track, this distance is raced as a near replacement for the mile (it is, in fact, 9.344 m, about 30.6 feet, shorter; however, it is still colloquially referred to as "the mile"). The 1600 meters is the official distance for this range of races in US high schools. While this race is rarely run outside high school and collegiate invitational competition, it has been held at the international level. The 1500 m, however, is the most common distance run at the college and international levels. The final leg of a distance medley relay is 1600 metres.		An accurate way to run an actual mile on a metric track is to run the additional 9.344 meters before starting the first marked 400 meter lap. Many tracks, especially high-level tracks, will have a waterfall starting line drawn 9.344 meters back for this purpose. Otherwise, on a metric track, there will be a relay zone 10 meters before the common start/finish line, frequently marked by a triangle pointed toward the finish. In many configurations, that triangle is about half a meter wide, making its point extremely close to the mile start line, which would be slightly less than two feet from the marked relay zone (the widest part of the triangle, or line).[9]		This length of middle-distance race, 1760 yards, (1609.344 metres), is very common in countries that do not use the metric system, and is still often referred to as the "Blue Riband" of the track. When the International Amateur Athletic Federation decided in 1976 to recognize only world records for metric distances, it made an exception for the mile and records are kept to this day.		Historically, the mile took the place that the 1500 m has today. It is still raced on the world class level, but usually only at select occasions, like the famous Wanamaker Mile, held annually at the Millrose Games. Running a mile in less than four minutes is a famously difficult achievement, long thought impossible by the scientific community. The first man to break the four-minute barrier was Englishman Roger Bannister at Oxford in 1954.		The current record holders are Hicham El Guerrouj (Morocco) (3:43.13, Rome, 7 July 1999) and Svetlana Masterkova (Russia) (4:12.56, Zürich, 14 August 1996).[6]		Another event that is rarely run, a miler's speed will generally allow him/her to prevail at this distance over less balanced challengers.		Hicham El Guerrouj (Morocco) (4:44.79, Berlin, 7 September 1999) and Sonia O'Sullivan (Ireland) (5:25.36, Edinburgh, 8 July 1994) are currently the fastest at this distance outdoors.[6] On February 7, 2017, Genzebe Dibaba (Ethiopia) ran 5:23.75 indoors. Although the 2000m isn't an official world record event indoors, Dibaba’s performance can be classed as an outright world record as it is faster than Sonia O'Sullivan's outdoor mark.[10]		Truly on the borderline between middle and longer distances, the 3000 m (7.5 laps) is a standard race in the United States, though it is not raced at the outdoor IAAF World Championships. This race requires decent speed, but a lack of natural quickness can be made up for with superior aerobic conditioning and race tactics. The records at this distance were set by Daniel Komen (Kenya) (7:20.67, Rieti, 1 September 1996) and Junxia Wang (China) (8:06.11, Beijing, 13 September 1993).[6]		At exactly eight laps on a standard 400 m track, this event is typically run only in American high schools, along with the 1600 m. It is colloquially called the "two-mile", as the distance is only 18.688 metres shorter. In college, the typical runner of this event would convert to the 5,000 metre run (or potentially the 3,000 metre run during indoor season). It should be noted that in most eastern American high schools, colleges, and middle schools, this event is usually considered a long-distance event, depending on the region. It is the longest track distance run in most high school competitions.[11]		This length of long middle-distance or short long-distance race was 3520 yards (3218.688 metres).		Historically, the two mile took the place that the 3000 m and the 3200 m have today. The first man to break the four-minute barrier for both miles was Daniel Komen (Kenya) at Hechtel, Belgium on 19 July 1997, and his time of 7:58.61 remains a world record. Meseret Defar (Ethiopia) is the fastest woman: 8:58.58, Brussels, Belgium, 14 September 2007.		Another race only run in high school or Masters meets. The typical specialist in this event would move up to the 3000 metre steeplechase in college.		The 3,000 metre steeplechase is a distance event requiring greater strength, stamina, and agility than the flat 3,000 metre event. This is because athletes are required to jump over five barriers per lap, after a flat first 200 m to allow for settling in. One barrier per lap is placed in front of a water pit, meaning that runners are also forced to deal with the chafing of wet shoes as they race. The world records are held by Saif Saeed Shaheen (Qatar) (7:53.63, Brussels. 3 September 2004) and Gulnara Samitova (Russia) (8:58.81, Beijing, 17 August 2008).[6]		In the United States, the 3000 m is more common at the high school and collegiate levels (along with the US two mile). In Japan, the 800, 1500 and 3000 metre events are competed in both genders for junior high school and high school, except that high school boys jump to 5000 metres. Both 3000 and 5000 metre distances are sometimes described as long distance[12] but also frequently as middle distance,[13][14][15] depending on the context. From the perspective of a longer race like a half marathon, marathon or relays such as the ekiden relay, the 5000 metre race might be viewed as middle distance.		The tables below do not focus on record times but rather on the performance of many runners in a given year (in this case, 2007 and 2008). These are the top 100 (or even 500) junior high school and high school runners in Japan and the USA.		A few states of the USA use this distance, among them Oregon, Florida, Massachusetts, and Rhode Island.		A few states of the USA use this distance, among them Oregon, Massachusetts, Florida, and Rhode Island.		In the USA, the steeplechase is still relatively uncommon in high school.[20]		Rarely run in youth competition, though some high school competitions include the 3000m steeplechase, for example, it is an event competed in championships and larger meets in New York State.		(Not a middle distance event)		Japanese secondary school boys regularly run 5000 metres on the track rather than 3000 meters. USA high school boys rarely run this distance except during cross country.		Media related to Middle-distance running at Wikimedia Commons		
Diseases of affluence is a term sometimes given to selected diseases and other health conditions which are commonly thought to be a result of increasing wealth in a society.[1] Psychologists' research is probing why affluent people enjoy better health. Epidemiological studies have confirmed the relationship between income, education and occupation on the one hand and health outcomes on the other. Studies have found that relative risk of death increased significantly as rank decreased. The message is simple: the lower a person's socioeconomic status, the greater their risk of both physical and psychological health problems.[2]		Also referred to as the "Western disease" paradigm, these diseases are in contrast to so-called "diseases of poverty", which largely result from and contribute to human impoverishment. The modern diet and sedentary lifestyle is argued to be the blame for current levels of obesity,[3] cardiovascular disease,[4] high blood pressure,[5] type 2 diabetes,[6] osteoporosis,[7] colorectal cancer,[8] acne,[9] gout,[10] depression, and diseases related to vitamin and mineral deficiencies.[11] These diseases of affluence have vastly increased in prevalence since the end of World War II.		Examples of diseases of affluence include mostly chronic non-communicable diseases (NCDs) and other physical health conditions for which personal lifestyles and societal conditions associated with economic development are believed to be an important risk factor — such as type 2 diabetes, asthma,[12] coronary heart disease, cerebrovascular disease, peripheral vascular disease, obesity, hypertension, cancer, alcoholism, gout, and some types of allergy.[1][13][14]		They may also be considered to include depression and other mental health conditions associated with increased social isolation and lower levels of psychological well being observed in many developed countries.[15][16] Many of these conditions are interrelated, for example obesity is thought to be a partial cause of many other illnesses.[citation needed]		In contrast, the diseases of poverty tend to be largely infectious diseases, or the result of poor living conditions. These include tuberculosis, asthma, and intestinal diseases.[17] Increasingly, research is finding that diseases thought to be diseases of affluence also appear in large part in the poor. These diseases include obesity and cardiovascular disease and, coupled with infectious diseases, these further increase global health inequalities.[1]		Diseases of affluence are predicted to become more prevalent in developing countries as diseases of poverty decline, longevity increases, and lifestyles change.[1][13] In 2008, nearly 80% of deaths due to NCDs — including heart disease, strokes, chronic lung diseases, cancers and diabetes — occurred in low- and middle-income countries.[18]						Factors associated with the increase of these conditions and illnesses ironically appear to be things that are a direct result of technological advances. They include:		General:		
Diabetes mellitus type 2 (also known as type 2 diabetes) is a long-term metabolic disorder that is characterized by high blood sugar, insulin resistance, and relative lack of insulin.[6] Common symptoms include increased thirst, frequent urination, and unexplained weight loss. Symptoms may also include increased hunger, feeling tired, and sores that do not heal.[3] Often symptoms come on slowly.[6] Long-term complications from high blood sugar include heart disease, strokes, diabetic retinopathy which can result in blindness, kidney failure, and poor blood flow in the limbs which may lead to amputations.[1] The sudden onset of hyperosmolar hyperglycemic state may occur; however, ketoacidosis is uncommon.[4][5]		Type 2 diabetes primarily occurs as a result of obesity and lack of exercise.[1] Some people are more genetically at risk than others.[6] Type 2 diabetes makes up about 90% of cases of diabetes, with the other 10% due primarily to diabetes mellitus type 1 and gestational diabetes.[1] In diabetes mellitus type 1 there is a lower total level of insulin to control blood glucose, due to an autoimmune induced loss of insulin-producing beta cells in the pancreas.[12][13] Diagnosis of diabetes is by blood tests such as fasting plasma glucose, oral glucose tolerance test, or glycated hemoglobin (A1C).[3]		Type 2 diabetes is partly preventable by staying a normal weight, exercising regularly, and eating properly. Treatment involves exercise and dietary changes.[1] If blood sugar levels are not adequately lowered, the medication metformin is typically recommended.[7][14] Many people may eventually also require insulin injections.[9] In those on insulin, routinely checking blood sugar levels is advised; however, this may not be needed in those taking pills.[15] Bariatric surgery often improves diabetes in those who are obese.[8][16]		Rates of type 2 diabetes have increased markedly since 1960 in parallel with obesity.[17] As of 2015 there were approximately 392 million people diagnosed with the disease compared to around 30 million in 1985.[11][18] Typically it begins in middle or older age,[6] although rates of type 2 diabetes are increasing in young people.[19][20] Type 2 diabetes is associated with a ten-year-shorter life expectancy.[10] Diabetes was one of the first diseases described.[21] The importance of insulin in the disease was determined in the 1920s.[22]		The classic symptoms of diabetes are polyuria (frequent urination), polydipsia (increased thirst), polyphagia (increased hunger), and weight loss.[23] Other symptoms that are commonly present at diagnosis include a history of blurred vision, itchiness, peripheral neuropathy, recurrent vaginal infections, and fatigue.[13] Many people, however, have no symptoms during the first few years and are diagnosed on routine testing.[13] A small number of people with type 2 diabetes mellitus can develop a hyperosmolar hyperglycemic state (a condition of very high blood sugar associated with a decreased level of consciousness and low blood pressure).[13]		Type 2 diabetes is typically a chronic disease associated with a ten-year-shorter life expectancy.[10] This is partly due to a number of complications with which it is associated, including: two to four times the risk of cardiovascular disease, including ischemic heart disease and stroke; a 20-fold increase in lower limb amputations, and increased rates of hospitalizations.[10] In the developed world, and increasingly elsewhere, type 2 diabetes is the largest cause of nontraumatic blindness and kidney failure.[24] It has also been associated with an increased risk of cognitive dysfunction and dementia through disease processes such as Alzheimer's disease and vascular dementia.[25] Other complications include acanthosis nigricans, sexual dysfunction, and frequent infections.[23]		The development of type 2 diabetes is caused by a combination of lifestyle and genetic factors.[24][26] While some of these factors are under personal control, such as diet and obesity, other factors are not, such as increasing age, female gender, and genetics.[10] A lack of sleep has been linked to type 2 diabetes.[27] This is believed to act through its effect on metabolism.[27] The nutritional status of a mother during fetal development may also play a role, with one proposed mechanism being that of altered DNA methylation.[28] The intestinal bacteriæ Prevotella copri and Bacteroides vulgatus have been connected with type 2 diabetes.[29]		Lifestyle factors are important to the development of type 2 diabetes, including obesity and being overweight (defined by a body mass index of greater than 25), lack of physical activity, poor diet, stress, and urbanization.[10][30] Excess body fat is associated with 30% of cases in those of Chinese and Japanese descent, 60–80% of cases in those of European and African descent, and 100% of cases in Pima Indians and Pacific Islanders.[13] Among those who are not obese, a high waist–hip ratio is often present.[13] Smoking appears to increase the risk of type 2 diabetes mellitus.[31]		Dietary factors also influence the risk of developing type 2 diabetes. Consumption of sugar-sweetened drinks in excess is associated with an increased risk.[32][33] The type of fats in the diet are important, with saturated fats and trans fatty acids increasing the risk, and polyunsaturated and monounsaturated fat decreasing the risk.[26] Eating a lot of white rice appears to play a role in increasing risk.[34] A lack of exercise is believed to cause 7% of cases.[35] Persistent organic pollutants may play a role.[36]		Most cases of diabetes involve many genes, with each being a small contributor to an increased probability of becoming a type 2 diabetic.[10] If one identical twin has diabetes, the chance of the other developing diabetes within his lifetime is greater than 90%, while the rate for nonidentical siblings is 25–50%.[13] As of 2011, more than 36 genes had been found that contribute to the risk of type 2 diabetes.[37] All of these genes together still only account for 10% of the total heritable component of the disease.[37] The TCF7L2 allele, for example, increases the risk of developing diabetes by 1.5 times and is the greatest risk of the common genetic variants.[13] Most of the genes linked to diabetes are involved in beta cell functions.[13]		There are a number of rare cases of diabetes that arise due to an abnormality in a single gene (known as monogenic forms of diabetes or "other specific types of diabetes").[10][13] These include maturity onset diabetes of the young (MODY), Donohue syndrome, and Rabson–Mendenhall syndrome, among others.[10] Maturity onset diabetes of the young constitute 1–5% of all cases of diabetes in young people.[38]		There are a number of medications and other health problems that can predispose to diabetes.[39] Some of the medications include: glucocorticoids, thiazides, beta blockers, atypical antipsychotics,[40] and statins.[41] Those who have previously had gestational diabetes are at a higher risk of developing type 2 diabetes.[23] Other health problems that are associated include: acromegaly, Cushing's syndrome, hyperthyroidism, pheochromocytoma, and certain cancers such as glucagonomas.[39] Testosterone deficiency is also associated with type 2 diabetes.[42][43]		Type 2 diabetes is due to insufficient insulin production from beta cells in the setting of insulin resistance.[13] Insulin resistance, which is the inability of cells to respond adequately to normal levels of insulin, occurs primarily within the muscles, liver, and fat tissue.[44] In the liver, insulin normally suppresses glucose release. However, in the setting of insulin resistance, the liver inappropriately releases glucose into the blood.[10] The proportion of insulin resistance versus beta cell dysfunction differs among individuals, with some having primarily insulin resistance and only a minor defect in insulin secretion and others with slight insulin resistance and primarily a lack of insulin secretion.[13]		Other potentially important mechanisms associated with type 2 diabetes and insulin resistance include: increased breakdown of lipids within fat cells, resistance to and lack of incretin, high glucagon levels in the blood, increased retention of salt and water by the kidneys, and inappropriate regulation of metabolism by the central nervous system.[10] However, not all people with insulin resistance develop diabetes, since an impairment of insulin secretion by pancreatic beta cells is also required.[13]		The World Health Organization definition of diabetes (both type 1 and type 2) is for a single raised glucose reading with symptoms, otherwise raised values on two occasions, of either:[47]		A random blood sugar of greater than 11.1 mmol/l (200 mg/dL) in association with typical symptoms[23] or a glycated hemoglobin (HbA1c) of ≥ 48 mmol/mol (≥ 6.5 DCCT %) is another method of diagnosing diabetes.[10] In 2009 an International Expert Committee that included representatives of the American Diabetes Association (ADA), the International Diabetes Federation (IDF), and the European Association for the Study of Diabetes (EASD) recommended that a threshold of ≥ 48 mmol/mol (≥ 6.5 DCCT %) should be used to diagnose diabetes.[48] This recommendation was adopted by the American Diabetes Association in 2010.[49] Positive tests should be repeated unless the person presents with typical symptoms and blood sugars >11.1 mmol/l (>200 mg/dl).[48]		Threshold for diagnosis of diabetes is based on the relationship between results of glucose tolerance tests, fasting glucose or HbA1c and complications such as retinal problems.[10] A fasting or random blood sugar is preferred over the glucose tolerance test, as they are more convenient for people.[10] HbA1c has the advantages that fasting is not required and results are more stable but has the disadvantage that the test is more costly than measurement of blood glucose.[50] It is estimated that 20% of people with diabetes in the United States do not realize that they have the disease.[10]		Diabetes mellitus type 2 is characterized by high blood glucose in the context of insulin resistance and relative insulin deficiency.[51] This is in contrast to diabetes mellitus type 1 in which there is an absolute insulin deficiency due to destruction of islet cells in the pancreas and gestational diabetes mellitus that is a new onset of high blood sugars associated with pregnancy.[13] Type 1 and type 2 diabetes can typically be distinguished based on the presenting circumstances.[48] If the diagnosis is in doubt antibody testing may be useful to confirm type 1 diabetes and C-peptide levels may be useful to confirm type 2 diabetes,[52] with C-peptide levels normal or high in type 2 diabetes, but low in type 1 diabetes.[53]		No major organization recommends universal screening for diabetes as there is no evidence that such a program improve outcomes.[54][55] Screening is recommended by the United States Preventive Services Task Force (USPSTF) in adults without symptoms whose blood pressure is greater than 135/80 mmHg.[56] For those whose blood pressure is less, the evidence is insufficient to recommend for or against screening.[56] There is no evidence that it changes the risk of death in this group of people.[55] They also recommend screening among those who are overweight and between the ages of 40 and 70.[57]		The World Health Organization recommends testing those groups at high risk[54] and in 2014 the USPSTF is considering a similar recommendation.[58] High-risk groups in the United States include: those over 45 years old; those with a first degree relative with diabetes; some ethnic groups, including Hispanics, African-Americans, and Native-Americans; a history of gestational diabetes; polycystic ovary syndrome; excess weight; and conditions associated with metabolic syndrome.[23] The American Diabetes Association recommends screening those who have a BMI over 25 (in people of Asian descent screening is recommending for a BMI over 23.[59]		Onset of type 2 diabetes can be delayed or prevented through proper nutrition and regular exercise.[60][61] Intensive lifestyle measures may reduce the risk by over half.[24][62] The benefit of exercise occurs regardless of the person's initial weight or subsequent weight loss.[63] High levels of physical activity reduce the risk of diabetes by about 28%.[64] Evidence for the benefit of dietary changes alone, however, is limited,[65] with some evidence for a diet high in green leafy vegetables[66] and some for limiting the intake of sugary drinks.[32] In those with impaired glucose tolerance, diet and exercise either alone or in combination with metformin or acarbose may decrease the risk of developing diabetes.[24][67] Lifestyle interventions are more effective than metformin.[24] While low vitamin D levels are associated with an increased risk of diabetes, correcting the levels by supplementing vitamin D3 does not improve that risk.[68]		Management of type 2 diabetes focuses on lifestyle interventions, lowering other cardiovascular risk factors, and maintaining blood glucose levels in the normal range.[24] Self-monitoring of blood glucose for people with newly diagnosed type 2 diabetes may be used in combination with education,[69] however the benefit of self monitoring in those not using multi-dose insulin is questionable.[24][70] In those who do not want to measure blood levels, measuring urine levels may be done.[69] Managing other cardiovascular risk factors, such as hypertension, high cholesterol, and microalbuminuria, improves a person's life expectancy.[24] Decreasing the systolic blood pressure to less than 140 mmHg is associated with a lower risk of death and better outcomes.[71] Intensive blood pressure management (less than 130/80 mmHg) as opposed to standard blood pressure management (less than 140/85–100 mmHg) results in a slight decrease in stroke risk but no effect on overall risk of death.[72]		Intensive blood sugar lowering (HbA1c<6%) as opposed to standard blood sugar lowering (HbA1c of 7–7.9%) does not appear to change mortality.[73][74] The goal of treatment is typically an HbA1c of around 7% or a fasting glucose of less than 7.2 mmol/L (130 mg/dL); however these goals may be changed after professional clinical consultation, taking into account particular risks of hypoglycemia and life expectancy.[59][75] Despite guidelines recommending that intensive blood sugar control be based on balancing immediate harms with long-term benefits, many people—for example people with a life expectancy of less than nine years who will not benefit, are over-treated.[76]		It is recommended that all people with type 2 diabetes get regular eye examination.[13] There is weak evidence suggesting that treating gum disease by scaling and root planing may result in a small short-term improvement in blood sugar levels for people with diabetes.[77] There is no evidence to suggest that this improvement in blood sugar levels is maintained longer than 4 months.[77] There is also not enough evidence to determine if medications to treat gum disease are effective at lowering blood sugar levels.[77]		A proper diet and exercise are the foundations of diabetic care,[23] with a greater amount of exercise yielding better results.[78] Aerobic exercise leads to a decrease in HbA1c and improved insulin sensitivity.[79] Resistance training is also useful and the combination of both types of exercise may be most effective.[79] A diabetic diet that promotes weight loss is important.[80] While the best diet type to achieve this is controversial,[80] a low glycemic index diet or low carbohydrate diet has been found to improve blood sugar control.[81][82] Culturally appropriate education may help people with type 2 diabetes control their blood sugar levels, for up to 24 months.[83] If changes in lifestyle in those with mild diabetes has not resulted in improved blood sugars within six weeks, medications should then be considered.[23] There is not enough evidence to determine if lifestyle interventions affect mortality in those who already have DM2.[62] Vegetarian diets in general have been related to lower diabetes risk, but do not offer advantages compared with diets which allow moderate amounts of animal products.[84] There is not enough evidence to suggest that cinnamon improves blood sugar levels in people with type 2 diabetes.[85]		There are several classes of anti-diabetic medications available. Metformin is generally recommended as a first line treatment as there is some evidence that it decreases mortality;[7][24][86] however, this conclusion is questioned.[87] Metformin should not be used in those with severe kidney or liver problems.[23]		A second oral agent of another class or insulin may be added if metformin is not sufficient after three months.[75] Other classes of medications include: sulfonylureas, thiazolidinediones, dipeptidyl peptidase-4 inhibitors, SGLT2 inhibitors, and glucagon-like peptide-1 analogs.[75] There is no significant difference between these agents.[75] Rosiglitazone, a thiazolidinedione, has not been found to improve long-term outcomes even though it improves blood sugar levels.[88] Additionally it is associated with increased rates of heart disease and death.[89] Angiotensin-converting enzyme inhibitors (ACEIs) prevent kidney disease and improve outcomes in those with diabetes.[90][91] The similar medications angiotensin receptor blockers (ARBs) do not.[91] A 2016 review recommended treating to a systolic blood pressure of 140 to 150 mmHg.[92]		Injections of insulin may either be added to oral medication or used alone.[24] Most people do not initially need insulin.[13] When it is used, a long-acting formulation is typically added at night, with oral medications being continued.[23][24] Doses are then increased to effect (blood sugar levels being well controlled).[24] When nightly insulin is insufficient, twice daily insulin may achieve better control.[23] The long acting insulins glargine and detemir are equally safe and effective,[93] and do not appear much better than neutral protamine Hagedorn (NPH) insulin, but as they are significantly more expensive, they are not cost effective as of 2010.[94] In those who are pregnant insulin is generally the treatment of choice.[23]		Weight loss surgery in those who are obese is an effective measure to treat diabetes.[95] Many are able to maintain normal blood sugar levels with little or no medication following surgery[96] and long-term mortality is decreased.[97] There however is some short-term mortality risk of less than 1% from the surgery.[98] The body mass index cutoffs for when surgery is appropriate are not yet clear.[97] It is recommended that this option be considered in those who are unable to get both their weight and blood sugar under control.[99][100]		Globally as of 2015 it was estimated that there were 392 million people with type 2 diabetes making up about 90% of diabetes cases.[10][11] This is equivalent to about 6% of the world's population.[11] Diabetes is common both in the developed and the developing world.[10] It remains uncommon, however, in the underdeveloped world.[13]		Women seem to be at a greater risk as do certain ethnic groups,[10][101] such as South Asians, Pacific Islanders, Latinos, and Native Americans.[23] This may be due to enhanced sensitivity to a Western lifestyle in certain ethnic groups.[102] Traditionally considered a disease of adults, type 2 diabetes is increasingly diagnosed in children in parallel with rising obesity rates.[10] Type 2 diabetes is now diagnosed as frequently as type 1 diabetes in teenagers in the United States.[13]		Rates of diabetes in 1985 were estimated at 30 million, increasing to 135 million in 1995 and 217 million in 2005.[18] This increase is believed to be primarily due to the global population aging, a decrease in exercise, and increasing rates of obesity.[18] The five countries with the greatest number of people with diabetes as of 2000 are India having 31.7 million, China 20.8 million, the United States 17.7 million, Indonesia 8.4 million, and Japan 6.8 million.[103] It is recognized as a global epidemic by the World Health Organization.[1]		Diabetes is one of the first diseases described[21] with an Egyptian manuscript from c. 1500 BCE mentioning "too great emptying of the urine."[104] The first described cases are believed to be of type 1 diabetes.[104] Indian physicians around the same time identified the disease and classified it as madhumeha or honey urine noting that the urine would attract ants.[104] The term "diabetes" or "to pass through" was first used in 230 BCE by the Greek Apollonius Of Memphis.[104] The disease was rare during the time of the Roman empire with Galen commenting that he had only seen two cases during his career.[104]		Type 1 and type 2 diabetes were identified as separate conditions for the first time by the Indian physicians Sushruta and Charaka in 400–500 AD with type 1 associated with youth and type 2 with being overweight.[104] The term "mellitus" or "from honey" was added by the Briton John Rolle in the late 1700s to separate the condition from diabetes insipidus which is also associated with frequent urination.[104] Effective treatment was not developed until the early part of the 20th century when the Canadians Frederick Banting and Charles Best discovered insulin in 1921 and 1922.[104] This was followed by the development of the long acting NPH insulin in the 1940s.[104]						
Cardiorespiratory fitness (CRF) refers to the ability of the circulatory and respiratory systems to supply oxygen to skeletal muscles during sustained physical activity. The primary measure of CRF is VO2max.[1] In 2016, the American Heart Association published an official scientific statement advocating that CRF be categorized as a clinical vital sign and should be routinely assessed as part of clinical practice. [1]		Regular exercise makes these systems more efficient by enlarging the heart muscle, enabling more blood to be pumped with each stroke, and increasing the number of small arteries in trained skeletal muscles, which supply more blood to working muscles. Exercise improves not just the respiratory system but the heart by increasing the amount of oxygen that is inhaled and distributed to body tissue.[2] A 2005 Cochrane review demonstrated that physical activity interventions are effective for increasing cardiovascular fitness.[3]		There are many benefits of cardiorespiratory fitness. It can reduce the risk of heart disease, lung cancer, type 2 diabetes, stroke, and other diseases. Cardiorespiratory fitness helps improve lung and heart condition, and increases feelings of wellbeing.[2] Additionally, there is mounting evidence that CRF is potentially a stronger predictor of mortality than other established risk factors such as smoking, hypertension, high cholesterol, and type 2 diabetes. Significantly, CRF can be added to these traditional risk factors to improve risk prediction validity.[1]		The American College of Sports Medicine recommends aerobic exercise 3–5 times per week for 30–60 minutes per session, at a moderate intensity, that maintains the heart rate between 65–85% of the maximum heart rate.[4]						The cardiovascular system responds to changing demands on the body by adjusting cardiac output, blood flow, and blood pressure. Cardiac output is defined as the product of heart rate and stroke volume which represents the volume of blood being pumped by the heart each minute. Cardiac output increases during physical activity due to an increase in both the heart rate and stroke volume.[5] At the beginning of exercise, the cardiovascular adaptations are very rapid: “Within a second after muscular contraction, there is a withdrawal of vagal outflow to the heart, which is followed by an increase in sympathetic stimulation of the heart. This results in an increase in cardiac output to ensure that blood flow to the muscle is matched to the metabolic needs”.[6] Both heart rate and stroke volume vary directly with the intensity of the exercise performed and many improvements can be made through continuous training.		Another important issue is the regulation of blood flow during exercise. Blood flow must increase in order to provide the working muscle with more oxygenated blood which can be accomplished through neural and chemical regulation. Blood vessels are under sympathetic tone, therefore the release of noradrenaline and adrenaline will cause vasoconstriction of non-essential tissues such as the liver, intestines, and kidneys, and decrease neurotransmitter release to the active muscles promoting vasodilatation. Also, chemical factors such as a decrease in oxygen concentration and an increase in carbon dioxide or lactic acid concentration in the blood promote vasodilatation to increase blood flow.[7] As a result of increased vascular resistance, blood pressure rises throughout exercise and stimulates baroreceptors in the carotid arteries and aortic arch. “These pressure receptors are important since they regulate arterial blood pressure around an elevated systemic pressure during exercise”.[6]		Although all of the described adaptations in the body to maintain homeostatic balance during exercise are very important, the most essential factor is the involvement of the respiratory system. The respiratory system allows for the proper exchange and transport of gases to and from the lungs while being able to control the ventilation rate through neural and chemical impulses. In addition, the body is able to efficiently use the three energy systems which include the phosphagen system, the glycolytic system, and the oxidative system.[5]		In most cases, as the body is exposed to physical activity, the core temperature of the body tends to rise as heat gain becomes larger than the amount of heat lost. “The factors that contribute to heat gain during exercise include anything that stimulate metabolic rate, anything from the external environment that causes heat gain, and the ability of the body to dissipate heat under any given set of circumstances”.[5] In response to an increase in core temperature, there are a variety of factors which adapt in order to help restore heat balance. The main physiological response to an increase in body temperature is mediated by the thermal regulatory center located in the hypothalamus of the brain which connects to thermal receptors and effectors. There are numerous thermal effectors including sweat glands, smooth muscles of blood vessels, some endocrine glands, and skeletal muscle. With an increase in the core temperature, the thermal regulatory center will stimulate the arterioles supplying blood to the skin to dilate along with the release of sweat on the skin surface to reduce temperature through evaporation.[5] In addition to the involuntary regulation of temperature, the hypothalamus is able to communicate with the cerebral cortex to initiate voluntary control such as removing clothing or drinking cold water. With all regulations taken into account, the body is able to maintain core temperature within about two or three degrees Celsius during exercise.[6]		
A treadmill is a device generally for walking or running or climbing while staying in the same place. Treadmills were introduced before the development of powered machines, to harness the power of animals or humans to do work, often a type of mill that was operated by a person or animal treading steps of a treadwheel to grind grain. In later times, treadmills were used as punishment devices for people sentenced to hard labour in prisons. The terms treadmill and treadwheel were used interchangeably for the power and punishment mechanisms.		More recently, treadmills are not used to harness power, but as exercise machines for running or walking in one place. Rather than the user powering the mill, the machine provides a moving platform with a wide conveyor belt driven by an electric motor or a flywheel. The belt moves to the rear, requiring the user to walk or run at a speed matching that of the belt. The rate at which the belt moves is the rate of walking or running. Thus, the speed of running may be controlled and measured. The more expensive, heavy-duty versions are motor-driven (usually by an electric motor). The simpler, lighter, and less expensive versions passively resist the motion, moving only when walkers push the belt with their feet. The latter are known as manual treadmills.		According to Sports & Fitness Industry Association, treadmills continue to be the largest selling exercise equipment category by a large margin. [1] As a result, the treadmill industry counts with hundreds of manufacturers throughout the World.[2]						The first consumer treadmill for home use was developed by William Staub, a mechanical engineer.[3] Staub developed his treadmill after reading the 1968 book, Aerobics, by Dr. Kenneth H. Cooper.[3] Cooper's book noted that individuals who ran for eight minutes four-to-five times a week would be in better physical condition.[3] Staub noticed that there were no affordable household treadmills at the time and decided to develop a treadmill for his own use during the late 1960s.[3] He called his first treadmill the PaceMaster 600.[3] Once finished, Staub sent his prototype treadmill to Cooper, who found the machine's first customers, which included sellers of fitness equipment.[3]		Staub began producing the first home treadmills at his plant in Clifton, New Jersey, before moving production to Little Falls, New Jersey.[3]		Treadmills as power sources originated in antiquity.[4] These ancient machines came in three major designs.[5] The first was to have a horizontal bar jutting out of a vertical shaft. It rotated around a vertical axis, driven by an ox or other animal walking in a circle pushing the bar. Even humans were used to power them. The second design was a vertical wheel that was powered through climbing in place instead of walking in circles. This is similar to what we know today as the hamster wheel. The third design also required climbing but used a sloped, moving platform instead.		Treadmills as muscle powered engines originated roughly 4000 years ago.[6] Their primary use was to lift buckets of water. This same technology was later adapted to create rotary grain mills and the treadwheel crane. It was also used to pump water and power dough-kneading machines and bellows.		Treadmills were invented in 1818 by an English engineer named Sir William Cubitt, son of a miller. Noting idle prisoners at Bury St Edmundsgaol, he proposed using their muscle power to both cure their idleness and produce useful work.[7]		Cubitt's treadmills for punishment usually rotated around a horizontal axis, requiring the user to step upwards, like walking up an endless staircase. Those punished walked around the outside of the wheel holding a horizontal handrail for stability. Earlier treadwheels include either horizontal or inclined-axis devices designed for a single user as well as a horizontal-axis design with the user inside and using the shaft as a handrail, in a manner similar to the familiar toys for small pet animals such as hamsters.		They remained in use until the second half of the 19th century; they were like twenty-foot long paddle wheels with twenty-four steps around a six-foot cylinder. Several prisoners stood side-by-side on a wheel, and had to work six or more hours a day, effectively climbing 5,000 to 14,000 vertical feet (1,5 to 4 km). While the purpose was mainly punitive, the most infamous mill at Brixton Prison was installed in 1821 and used to grind grain to supplement an existing windmill which Cubitt had previously installed nearby. It gained notoriety for the cruelty with which it was used, which then became a popular satirical metaphor for early-19th century prisons.		The machines could also be used to pump water or power ventilators in mines.[8][9]		The US patent of treadmill "training machine" (#1,064,968) was issued on June 17, 1913.[10]		The forerunner of exercise treadmills was designed to diagnose heart and lung disease, and was invented by Dr. Robert Bruce and Wayne Quinton at the University of Washington in 1952.[11][12] Dr. Kenneth H. Cooper's research on the benefits of aerobic exercise, published in 1968, provided a medical argument to support the commercial development of the home treadmill and exercise bike.		Among the users of treadmills today are medical facilities (hospitals, rehabilitation centers, medical and physiotherapy clinics, institutes of higher education), sports clubs, Biomechanics Institute, orthopedic shoe shops, running shops, Olympic training centers, universities, fire-training centers, NASA, test facilities and training rooms of police and army, gyms and even home users.		Treadmill ergometers are now mainly motor driven. Most treadmills have a running table with sliding plate. Before and after the race table, there are two shafts. The running belt is stretched between the shafts and the running deck. Safety standards for treadmills are the IEC EN 957-1 and IEC EN 957-6.		For medical treadmills applicable norms, standards and guidelines are the Medical Device Directive (MDD), European Guideline 93/42 EEC, European Guideline 2007/47 EEC, IEC EN 60601-1, EN 62304, EN 14971 and the machinery directive 2006/42/EC.		Medical treadmills are class IIb active therapeutic devices and also active devices for diagnosis. With their very powerful (e.g. 3.3 kW = 4.5 HP) electric motor powered drive system treadmills deliver mechanical energy to the human body through the moving running belt of the treadmill. The subject is not changing his horizontal position and is passively moved and forced to catch up with the running belt underneath his feet. The subject can also be fixed in safety harnesses, unweighting systems, various supports or even fixed in and moved with a robotic orthotic system utilizing the treadmill.		Medical treadmills are also active measuring devices. When connected through an interface with ECG, ergospirometry, blood pressure monitor (BPM), or EMG, they become a new medical system (e.g., stress test system or cardiopulmonary rehabilitation system) and can also be equipped to measure VO2max and various other vital functions.		Most treadmills have a “cardio mode”, where a target heart rate is defined and the speed and elevation (load) is controlled automatically until the subject is in “heart rate steady state”. So the treadmill is delivering mechanical energy to the human body based on the vital function (heart rate) of the subject.		A medical treadmill which is also used for ergometry and cardiopulmonary stress test as well as performance diagnostics is always a class IIb medical device either when used as stand-alone device in a medical environment or when used in connection with an ECG, EMG, ergospirometry, or blood pressure monitoring device.		On the running deck the subject is moving, who adapts to the adjustable speed of the belt. The running deck is usually mounted on damping elements, so the running deck has shock absorbing characteristics. By a lifting element, the entire frame including treadmill running deck will be raised and thus simulates a pitch angle for uphill running. Some treadmills also have the reversing of a running belt for the purpose of downhill loads. Most treadmills for professionals in the fitness area, run for table sizes of about 150 cm long and 50 cm width, a speed range of about 0 ... 20 km/h and slope angle of 0 ... 20%.		For athletes, larger and more stable treadmills are necessary. Sprinters reach with some weight relief temporarily speeds of up to 45 km/h must therefore run on a large deck of up to 300 cm in length and have up to 100 cm width. At high physical exertion and increased risk of falling a fall stop unit is required to prevent a fall of the subjects or patients. This fall stop device is usually implemented by a safety arch on which a rope is attached to an electrical switch. A harness bears the subject preventing from falling and shuts down the running belt.		In some offices, employees are provided with treadmill desks so that employees can walk while working on a computer or speaking on the phone.[13]		In treatment centers treadmills are used with built-in seats left and right for therapists, for example, so the therapists then can move the legs of a stroke patient in order to simulate walking movements and learn to walk again. This is called manual locomotion therapy.		Oversized treadmills are also used for cycling at speeds up to 80 km/h, for wheelchair users and in special applications with thick running belt for cross-country skiing and biathlon, where athletes perform training and testing exercise with roller ski on a running deck of up to sizes of 450 x 300 cm.		As a cardiovascular exercise:		As an indoor activity:		As a machine:		As it is basically a conveyor belt, the treadmill can be used for activities other than running. If horses are being tested (especially in jockey racing) they will be put on a specially constructed treadmill. Large treadmills can also accommodate cars. Treadmills can also be used to exercise dogs that are accustomed to running on a conveyor; however tying the leash to the treadmill should be avoided as it can cause serious injury.		Dog/Pet and underwater pet treatment treadmills are available for both home and clinical use. A variety of makes and models are available, but key features of treadmills designed for pet use include a longer running surface, open front and back entries and side rails to prevent the pet from falling off the treadmill. None are designed to be used without human supervision. Many veterinary and animal rehabilitation clinics also offer underwater treadmill therapy as part of their services provided to clients' pets.		Advanced applications are so called omnidirectional treadmills. They are designed to move in two dimensions and are intended as the base for a "holodeck". There are several solutions which were proposed and research continues because some issues remain unsolved, such as large size, noise and vibration. There are parallel developments being conducted by researchers working on projects sponsored by the Department of Veterans Affairs to create virtual reality environments for a wheelchair trainer in order to promote therapeutic exercise.[17]		
The Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, improved efficiency of water power, the increasing use of steam power, the development of machine tools and the rise of the factory system. Textiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested; the textile industry was also the first to use modern production methods.[1]		The Industrial Revolution began in Great Britain and most of the important technological innovations were British. Laws also shaped the revolution, such as courts ruling in favor of property rights. An entrepreneurial spirit and consumer revolution helped drive industrialisation in Britain which after 1800 was emulated in Belgium, the United States, and France.[2]		The Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.[3][4][5] About the same time the Industrial Revolution was occurring, Britain was undergoing an agricultural revolution, which also helped to improve living standards and provided surplus labour available for industry.		Mechanised textile production spread from Great Britain to continental Europe in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium, and later in France. Since then industrialisation has spread throughout much of the world.[1] The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes.[6][7][8][9] GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy,[10] while the Industrial Revolution began an era of per-capita economic growth in capitalist economies.[11] Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants.[12]		The First Industrial Revolution evolved into the Second Industrial Revolution in the transition years between 1840 and 1870, when technological and economic progress continued with the increasing adoption of steam transport (steam-powered railways, boats and ships), the large-scale manufacture of machine tools and the increasing use of machinery in steam-powered factories.[13][14][15]		The earliest recorded use of the term "Industrial Revolution" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise.[16] In his 1976 book Keywords: A Vocabulary of Culture and Society, Raymond Williams states in the entry for "Industry": "The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century." The term Industrial Revolution applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of la révolution industrielle.[17] Friedrich Engels in The Condition of the Working Class in England in 1844 spoke of "an industrial revolution, a revolution which at the same time changed the whole of civil society". However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.[18]		Some historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term revolution is a misnomer. This is still a subject of debate among some historians.		The commencement of the Industrial Revolution is closely linked to a small number of innovations,[19] beginning in the second half of the 18th century. By the 1830s the following gains had been made in important technologies:		Arguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, since the silk industry there was a closely guarded secret, the state of the industry there is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.[28][29]		In the late 17th and early 18th centuries the British government passed a series of Calico Acts in order to protect the domestic woollen industry from the increasing amounts of cotton fabric imported from its competitors in India.[22][30]		The demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.[30]		On the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system. Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off season the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.[21][30][31]:823 The flying shuttle patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box.[31]:821–22		Lewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham. Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.		In 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles.[32] The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting.[33] It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792,[34] and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.[31]:825–27		The spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright.[31]:827–30 For each spindle, the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather-covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.		Samuel Crompton's Spinning Mule, introduced in 1779, was a combination of the spinning jenny and the water frame in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating.[31]:832 Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive cloth in large quantities.[31]:832		Realising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. In 1776 he patented a two-man operated loom which was more conventional.[31]:834 Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.[35]		The demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin. With a cotton gin a man could remove seed from as much upland cotton in one day as would have previously taken a woman working two months to process at one pound per day.[14]		Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. This in turn fed a weaving industry that advanced with improvements to shuttles and the loom or 'frame'. The output of an individual labourer increased dramatically, with the effect that the new machines threatened the bargaining power of the labourers, and early innovators were attacked and their inventions destroyed.[36]		These advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power – first horse power and then water power—which made cotton manufacture a mechanised industry. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.[37]		A major change in the metal industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal,[38] and coal was more abundant than wood.[21]		Use of coal in smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (The foundry cupola is a different (and later) innovation.)		This was followed by Abraham Darby, who made great strides using coke to fuel his blast furnaces at Coalbrookdale in 1709. However, the coke pig iron he made was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs. Coke pig iron was hardly used to produce wrought iron in forges until the mid-1750s, when his son Abraham Darby II built Horsehay and Ketley furnaces (not far from Coalbrookdale). By then, coke pig iron was cheaper than charcoal pig iron. Since cast iron was becoming cheaper and more plentiful, it began being a structural material following the building of the innovative Iron Bridge in 1778 by Abraham Darby III.		Wrought iron for smiths to forge into consumer goods was still made in finery forges, as it long had been. However, new processes were adopted in the ensuing years. The first is referred to today as potting and stamping, but this was superseded by Henry Cort's puddling process. Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784.[27] Rolling replaced hammering for consolidating wrought iron and expelling some of the dross. Rolling was 15 times faster than hammering with a trip hammer. Roller mills were first used for making sheets, but also were developed for rolling structural shapes such as angles and rails.		Puddling produced a structural grade iron at a relatively low cost. Puddling was a means of decarburizing pig iron by slow oxidation, with iron ore as the oxygen source, as the iron was manually stirred using a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40.[39] Puddling was done in a reverberatory furnace, allowing coal or coke to be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised.		Up to that time, British iron manufacturers had used considerable amounts of imported iron to supplement native supplies. This came principally from Sweden from the mid-17th century and later also from Russia from the end of the 1720s. However, from 1785, imports decreased because of the new iron making technology, and Britain became an exporter of bar iron as well as manufactured wrought iron consumer goods.		Hot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. By using waste exhaust heat to preheat combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coal or two-thirds using coke;[40] however, the efficiency gains continued as the technology improved.[41] Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive;[42] however, by the end of the 19th century transportation costs fell considerably.		Two decades before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.		The supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.		The development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, the majority of industrial power was supplied by water and wind. In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp.[43] Small industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century. These included crank-, treadle-powered and horse-powered workshop and light industrial machinery.[44]		The first commercially successful industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its "brand name", The Miner's Friend). Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.		The first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were successfully put to use in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a lot of capital to build, and produced about 5 hp (3.7 kW). They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.[45]		A fundamental change in working principles was brought about by Scotsman James Watt. In close collaboration with Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton & Watts engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.		By 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from 5 to 10 hp (3.7 to 7.5 kW).		The development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.		Until about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats.		The Industrial Revolution created a demand for metal parts used in machinery. This led to the development of several machine tools for cutting metal parts. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.		Before the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal was kept to a minimum. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Hand methods of production were very laborious and costly and precision was difficult to achieve. Pre-industrial machinery was built by various craftsmen – millwrights built water and wind mills, carpenters made wooden framing, and smiths and turners made metal parts.		The first large machine tool was the cylinder boring machine used for boring the large-diameter cylinders on early steam engines. The planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.		Henry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He was hired away by Joseph Bramah for the production of high security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe. Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template.[14][31]:392–95 The slide rest lathe was called one of history's most important inventions, although not entirely Maudslay's idea.[14]:31, 36		Maudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.		James Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.		The impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century.[46]		In the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the economy, by value added, in the U.S.[47]		The large scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around 100 pounds (50 kg) in each of the chambers, at least a tenfold increase.		The production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulphuric acid with sodium chloride to give sodium sulphate and hydrochloric acid. The sodium sulphate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulphide. Adding water separated the soluble sodium carbonate from the calcium sulphide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulphide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash,[48] and also to potash (potassium carbonate) derived from hardwood ashes.		These two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulphuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.		The development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.		After 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry.[49] Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead the practice was to hire German-trained chemists.[50]		In 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about 1,400 °C (2,552 °F), then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel.[51] Cement was used on a large scale in the construction of the London sewerage system a generation later.		Another major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.		A new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure.		A machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.		The method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.[52]		The British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy.[53]		Industrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.		Jethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because the yield of seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact. Good quality seed drills were not produced until the mid 18th century.[54]		Joseph Foljambe's Rotherham plough of 1730 was the first commercially successful iron plough.[55][56][57][58] The threshing machine, invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour.[59]:286 It took several decades to diffuse[60] and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.		Machine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.[46]		Coal mining in Britain, particularly in South Wales, started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable.		Coal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.		Other developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton[61] the beginnings of a machine industry[14][62] and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years.[63]		At the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagon ways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century.		The Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.		Canals were the first technology to allow bulk materials to be economically transported long distances inland. This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.[31][64]		Building of canals dates to ancient times. The Grand Canal in China, "the world's largest artificial waterway and oldest canal still in existence," parts of which were started between the 6th and 4th centuries BC, is 1,121 miles (1,804 km) long and links Hangzhou with Beijing.[65]		In the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£22,589,130 as of 2013[update]),[66][67] but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half.[68] This success helped inspire a period of intense canal building, known as Canal Mania.[69] New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.		By the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world,[70] and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.		Britain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.		Much of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816.[72] The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stage coaches carried the rich, and the less wealthy could pay to ride on carriers carts.		Reducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England.		“ A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”[73]		Railways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high pressure steam engine also around 1800.		Wagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs. See: Metallurgy		Steam locomotives began being built after the introduction of high pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.		The rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of Hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity the blast furnace.		On 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington.[74] The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.		Construction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.		Prior to the Industrial Revolution most of the workforce was employed in agriculture, either as self-employed farmers as land owners or tenants, or as landless agricultural labourers. By the time of the Industrial Revolution the putting-out system whereby farmers and townspeople produced goods in their homes, often described as cottage industry, was the standard. Typical putting out system goods included spinning and weaving. Merchant capitalist provided the raw materials, typically paid workers by the piece, and were responsible for the sale of the goods. Embezzlement of supplies by workers and poor quality were common problems. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system.[75]		Some early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers.[76] Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories. Many workers, who had nothing but their labour to sell, became factory workers out of necessity.		The change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx, however, he recognized the increase in productivity made possible by technology.[77]		Women's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women.[78][79] Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.[80]		In a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation.[81] Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the "family wage economy" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the "family consumer economy," in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.[82]		The effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s.[84] A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards.[84] During 1813–1913, there was a significant increase in worker wages.[85][86][87]		Some economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that "for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility."[3] Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s.[4][5]		Chronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years, and only slightly higher in Britain. The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years.[88]		In Britain and the Netherlands, food supply had been increasing and prices falling before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus.[21][59][89][90] Before the Industrial Revolution, advances in agriculture or technology soon led to an increase in population, which again strained food and other resources, limiting increases in per capita income. This condition is called the Malthusian trap, and it was finally overcome by industrialisation.[59]		Transportation improvements, such as canals and improved roads, also lowered food costs. Railroads were introduced near the end of the Industrial Revolution.		The very rapid growth in population in the 19th century in the cities included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London.[91] The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms.[92][93] Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants.[94] People moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such as devastated Ireland in the 1840s.[95][96][97]		A large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the Socialist movement, The Condition of the Working Class in England in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. These shanty towns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high.[98] Not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions.		Conditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.		Consumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating.		According to Robert Hughes in The Fatal Shore, the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million.[99] Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s.[100][101] Europe's population increased from about 100 million in 1700 to 400 million by 1900.[102]		The Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.[103]		In terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life.[104] However, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.[105]		Industrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed "Cottonopolis", and the world's first industrial city.[106] Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.[107]		For much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.		The transition to industrialisation was not without difficulty. For example, a group of English workers known as Luddites formed to protest against industrialisation and sometimes sabotaged factories.		In other industries the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.		By 1746 an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.		The Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although infant mortality rates were reduced markedly.[109][110] There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.[111]		Child labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders,[112] 10–20% of an adult male's wage.[113] Children as young as four were employed.[113] Beatings and long hours were common, with some child coal miners and hurriers working from 4 am until 5 pm.[113] Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions.[113] Many children developed lung cancer and other diseases and died before the age of 25.[113] Workhouses would sell orphans and abandoned children as "pauper apprentices", working without wages for board and lodging.[113] Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape.[113] Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week. Some lost hands or limbs, others were crushed under the machines, and some were decapitated.[113] Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw.[113] Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust.[113]		Reports were written detailing some of the abuses, particularly in the coal mines[114] and textile factories,[115] and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.		Politicians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult.[113] About ten years later, the employment of children and women in mining was forbidden. These laws decreased the number of child labourers, however child labour remained in Europe and the United States up to the 20th century.[116]		The Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of combinations or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.		The main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted.		In 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its Charter of reforms received over three million signatures but was rejected by Parliament without consideration.		Working people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.		Unions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.[117]		Eventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party.		The rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.		Unrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.		The origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste.[118] The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.		The manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity.[119] The industry reached the US around 1850 causing pollution and lawsuits.[120]		In industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms.[121] Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash and gritty particles and to empower local authorities to impose their own regulations.[122]		The application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which reinforced rising literacy and demands for mass political participation.		During the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.[109]		The growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities,[123] compared to nearly 50% today (the beginning of the 21st century).[124] Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.[125]		Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s,[6] while T. S. Ashton held that it occurred roughly between 1760 and 1830.[7] The Industrial Revolution on Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Russian and Belgian governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.		Belgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.[126]		Wallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word "houille" was coined in Wallonia),[127] the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its Sillon industriel, 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, (...) there was a huge industrial development based on coal-mining and iron-making...'.[128] Philippe Raxhon wrote about the period after 1830: "It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain."[129] "The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent."[130] Michel De Coster, Professor at the Université de Liège wrote also: "The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory (...) But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated." [131]		Wallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the Sillon industriel, which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:		The industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres (...) at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent (...) Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.[132]		The industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear take-off.[133] Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:		Based on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.[134]		Germany's political disunity – with three dozen states – and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France[135]		During the period 1790–1815 Sweden experienced two parallel economic movements: an agricultural revolution with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a protoindustrialisation, with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a consumption revolution starting in the 1820s.		During 1815–1850 the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.		During 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873.		During 1890–1930, Sweden experienced the second industrial revolution. New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.		The industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).		In 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882,[136] used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.		Modern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.[137]		During the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy.[138] The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.[139][140]		Important American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US. The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century.		Oliver Evans invented an automated flour mill in the mid 1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon. This is considered to be the first modern materials handling system an important advance in the progress toward mass production.[46]		The United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.		Thomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era,[141] and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.[142]		In 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills.[143] Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of "America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than 45 miles (72 km) from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.		Merchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using 5.6 miles (9.0 km) of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.		A major U.S. contribution to industrialization was the development of techniques to make interchangeable parts from metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms. The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory. Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. The milling machine, a fundamental machine tool, is believed to have been invented by Ely Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented by Thomas Blanchard. The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardized and interchangeable parts became known as the American system of manufacturing[46]		Precision manufacturing techniques made it possible to build machines that mechanized the shoe industry.[144] and the watch industry. The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.		Steel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a "Second Industrial Revolution", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality.[31][145] Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.		This Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.		The increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.		A new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.		By the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.		During the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes.[146] Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of "nature" in art and language, in contrast to "monstrous" machines and factories; the "Dark satanic mills" of Blake's poem "And did those feet in ancient time". Mary Shelley's novel Frankenstein reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.[147]		The causes of the Industrial Revolution were complicated and remain a topic for debate, with some historians believing the Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century. The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the surplus population who could no longer find employment in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories.[148] The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century.[149] A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.[150]		Until the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine.[151] However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.[152]		Lewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates.[153] He explains that the model for standardised mass production was the printing press and that "the archetypal model for the industrial era was the clock". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.		The presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them.[154] Internal tariffs were abolished by Henry VIII of England, they survived in Russia till 1753, 1789 in France and 1839 in Spain.		Governments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors.[155] Watt's monopoly may have prevented other inventors, such as Richard Trevithick, William Murdoch or Jonathan Hornblower, from introducing improved steam engines, thereby retarding the industrial revolution by about 16 years.[156][157]		One question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East, or at other times like in Classical Antiquity[158] or the Middle Ages.[159] Numerous factors have been suggested, including education, technological changes[160] (see Scientific Revolution in Europe), "modern" government, "modern" work attitudes, ecology, and culture.[161] However, most historians contest the assertion that Europe and China were roughly equal because modern estimates of per capita income on Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars[162]) whereas China, by comparison, had only 450 dollars.		Some historians such as David Landes and Max Weber credit the different belief systems in Asia and Europe with dictating where the revolution occurred.[163] The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews.[164] Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.[165]		Regarding India, the Marxist historian Rajani Palme Dutt said: "The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain."[166] In contrast to China, India was split up into many competing kingdoms, with the three major ones being the Marathas, Sikhs and the Mughals. In addition, the economy was highly dependent on two sectors – agriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over.		Great Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the industrial revolution.[167] Key factors fostering this environment were: (1) The period of peace and stability which followed the unification of England and Scotland; (2) no trade barriers between England and Scotland;(3) the rule of law (respecting the sanctity of contracts); (4) a straightforward legal system which allowed the formation of joint-stock companies (corporations); (5) absence of tolls, which had largely disappeared from Britain by the 15th century, but were an extreme burden on goods elsewhere in the world, and (6) a free market (capitalism).[21]		Geographical and natural resource advantages of Great Britain were the fact that it had extensive coast lines and many navigable rivers in an age where water was the easiest means of transportation and having the highest quality coal in Europe.[21]		There were two main values that really drove the Industrial Revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution.[83] These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.[2]		The debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution.[168] Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.[169]		Instead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy[170]). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position – an island separated from the rest of mainland Europe.		Another theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.		The stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism.[172] (This point is also made in Hilaire Belloc's The Servile State.)		The French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, Letters on the English (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. "Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker’s word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another’s throats; but as there are such a multitude, they all live happy and in peace."[173]		Britain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs 3 to 5 acres (1.21 to 2.02 ha) for fodder while even early steam engines produced four times more mechanical energy.		In 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s.[174]		Knowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.		Another means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' (i.e. science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, "They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution".[175] Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual Transactions.		There were publications describing technology. Encyclopaedias such as Harris's Lexicon Technicum (1704) and Abraham Rees's Cyclopaedia (1802–1819) contain much of value. Cyclopaedia contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the Descriptions des Arts et Métiers and Diderot's Encyclopédie explained foreign methods with fine engraved plates.		Periodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the Annales des Mines, published accounts of travels made by French engineers who observed British methods on study tours.		Another theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work.[176] The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.		Dissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.		Historians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.		
PubMed Central (PMC) is a free digital repository that archives publicly accessible full-text scholarly articles that have been published within the biomedical and life sciences journal literature. As one of the major research databases within the suite of resources that have been developed by the National Center for Biotechnology Information (NCBI), PubMed Central is much more than just a document repository. Submissions into PMC undergo an indexing and formatting procedure which results in enhanced metadata, medical ontology, and unique identifiers which all enrich the XML structured data for each article on deposit.[1] Content within PMC can easily be interlinked to many other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to freely discover, read and build upon this portfolio of biomedical knowledge.[2]		PubMed Central should not be confused with PubMed. These are two very different services at their core.[3] While PubMed is a searchable database of biomedical citations and abstracts, the full-text article referenced in the PubMed record will physically reside elsewhere. (Sometimes in print, sometimes online, sometimes free, sometimes behind a toll-wall accessible only to paying subscribers). PubMed Central is a free digital archive of articles, accessible to anyone from anywhere via a basic web browser. The full text of all PubMed Central articles is free to read, with varying provisions for reuse.		As of February 2014[update], the PMC archive contained over 2.9 million articles, with contributions coming directly from publishers or authors depositing their own manuscripts into the repository per the NIH Public Access Policy. Recent data shows that in the past year (Jan 2013 – Jan 2014) author-initiated deposits exceeded 103,000 papers during just this 12-month period.[4] PMC also identifies about 4,000 journals which now participate in some capacity to automatically deposit their published content into the PMC repository.[5] Some participating publishers will delay the release of their articles on PubMed Central for a set time after publication, this is often referred to as an "embargo period", and can range from a few months to a few years depending on the journal. (Embargoes of six to twelve months are the most common).						Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central.[6]		A UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009.		The National Library of Medicine "NLM Journal Publishing Tag Set" journal article markup language is freely available.[7] The Association of Learned and Professional Society Publishers comments that "it is likely to become the standard for preparing scholarly content for both books and journals".[8] A related DTD is available for books.[9] The Library of Congress and the British Library have announced support for the NLM DTD.[10] It has also been popular with journal service providers.[11]		With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles.[12] This includes NASA content, with the interface branded as "PubSpace".[13][14]		Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above).		Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change.[15]		Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet available at one of these sources, are tracked in the database and automatically come "live" when the resources become available.		An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts.		When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats.		In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD.		Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some,[16] to cautious concern by others.[17] While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that same truth causes others to worry about traffic being diverted from the published version-of-record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies.[18] Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support.[19]		The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact.[20] A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication.[21]		The change in procedure has received criticism.[22] The American Physiological Society has expressed reservations about the implementation of the policy.[23]		The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of "PMC" followed by a string of 7 numbers. The format is[24]		Authors applying for NIH awards must include the PMCID in their application.		
Running is a method of terrestrial locomotion allowing humans and other animals to move rapidly on foot. Running is a type of gait characterized by an aerial phase in which all feet are above the ground (though there are exceptions[1]). This is in contrast to walking, where one foot is always in contact with the ground, the legs are kept mostly straight and the center of gravity vaults over the stance leg or legs in an inverted pendulum fashion.[2] A characteristic feature of a running body from the viewpoint of spring-mass mechanics is that changes in kinetic and potential energy within a stride occur simultaneously, with energy storage accomplished by springy tendons and passive muscle elasticity.[3] The term running can refer to any of a variety of speeds ranging from jogging to sprinting.		It is assumed that the ancestors of mankind developed the ability to run for long distances about 2.6 million years ago, probably in order to hunt animals.[4] Competitive running grew out of religious festivals in various areas. Records of competitive racing date back to the Tailteann Games in Ireland in 1829 BCE,[5][citation needed] while the first recorded Olympic Games took place in 776 BCE. Running has been described as the world's most accessible sport.[6]						It is thought that human running evolved at least four and a half million years ago out of the ability of the ape-like Australopithecus, an early ancestor of humans, to walk upright on two legs.[7]		The theory proposed considered to be the most likely evolution of running is of early humans' developing as endurance runners from the practice of persistence hunting of animals, the activity of following and chasing until a prey is too exhausted to flee, succumbing to "chase myopathy" (Sears 2001), and that human features such as the nuchal ligament, abundant sweat glands, the Achilles tendons, big knee joints and muscular glutei maximi, were changes caused by this type of activity (Bramble & Lieberman 2004, et al.).[8][9][10] The theory as first proposed used comparative physiological evidence and the natural habits of animals when running, indicating the likelihood of this activity as a successful hunting method. Further evidence from observation of modern-day hunting practice also indicated this likelihood (Carrier et al. 1984). [10][11] According to Sears (p. 12) scientific investigation (Walker & Leakey 1993) of the Nariokotome Skeleton provided further evidence for the Carrier theory.[12]		Competitive running grew out of religious festivals in various areas such as Greece, Egypt, Asia, and the East African Rift in Africa. The Tailteann Games, an Irish sporting festival in honor of the goddess Tailtiu, dates back to 1829 BCE, and is one of the earliest records of competitive running.[5][citation needed] The origins of the Olympics and Marathon running are shrouded by myth and legend, though the first recorded games took place in 776 BCE.[13] Running in Ancient Greece can be traced back to these games of 776 BCE.		...I suspect that the sun, moon, earth, stars, and heaven, which are still the Gods of many barbarians, were the only Gods known to the aboriginal Hellenes. Seeing that they were always moving and running, from their running nature they were called Gods or runners (Thus, Theontas)...		Running gait can be divided into two phases in regard to the lower extremity: stance and swing.[15][16][17][18] These can be further divided into absorption, propulsion, initial swing and terminal swing. Due to the continuous nature of running gait, no certain point is assumed to be the beginning. However, for simplicity it will be assumed that absorption and footstrike mark the beginning of the running cycle in a body already in motion.		Footstrike occurs when a plantar portion of the foot makes initial contact with the ground. Common footstrike types include forefoot, midfoot and heel strike types.[19][20][21] These are characterized by initial contact of the ball of the foot, ball and heel of the foot simultaneously and heel of the foot respectively. During this time the hip joint is undergoing extension from being in maximal flexion from the previous swing phase. For proper force absorption, the knee joint should be flexed upon footstrike and the ankle should be slightly in front of the body.[22] Footstrike begins the absorption phase as forces from initial contact are attenuated throughout the lower extremity. Absorption of forces continues as the body moves from footstrike to midstance due to vertical propulsion from the toe-off during a previous gait cycle.		Midstance is defined as the time at which the lower extremity limb of focus is in knee flexion directly underneath the trunk, pelvis and hips. It is at this point that propulsion begins to occur as the hips undergo hip extension, the knee joint undergoes extension and the ankle undergoes plantar flexion. Propulsion continues until the leg is extended behind the body and toe off occurs. This involves maximal hip extension, knee extension and plantar flexion for the subject, resulting in the body being pushed forward from this motion and the ankle/foot leaves the ground as initial swing begins.		Most recent research, particularly regarding the footstrike debate, has focused solely on the absorption phases for injury identification and prevention purposes. The propulsion phase of running involves the movement beginning at midstance until toe off.[16][17][23] From a full stride length model however, components of the terminal swing and footstrike can aid in propulsion.[18][24] Set up for propulsion begins at the end of terminal swing as the hip joint flexes, creating the maximal range of motion for the hip extensors to accelerate through and produce force. As the hip extensors change from reciporatory inhibitors to primary muscle movers, the lower extremity is brought back toward the ground, although aided greatly by the stretch reflex and gravity.[18] Footstrike and absorption phases occur next with two types of outcomes. This phase can be only a continuation of momentum from the stretch reflex reaction to hip flexion, gravity and light hip extension with a heel strike, which does little to provide force absorption through the ankle joint.[23][25][26] With a mid/forefoot strike, loading of the gastro-soleus complex from shock absorption will serve to aid in plantar flexion from midstance to toe-off.[26][27] As the lower extremity enters midstance, true propulsion begins.[23] The hip extensors continue contracting along with help from the acceleration of gravity and the stretch reflex left over from maximal hip flexion during the terminal swing phase. Hip extension pulls the ground underneath the body, thereby pulling the runner forward. During midstance, the knee should be in some degree of knee flexion due to elastic loading from the absorption and footstrike phases to preserve forward momentum.[28][29][30] The ankle joint is in dorsiflexion at this point underneath the body, either elastically loaded from a mid/forefoot strike or preparing for stand-alone concentric plantar flexion. All three joints perform the final propulsive movements during toe-off.[23][25][26][27] The plantar flexors plantar flex, pushing off from the ground and returning from dorsiflexion in midstance. This can either occur by releasing the elastic load from an earlier mid/forefoot strike or concentrically contracting from a heel strike. With a forefoot strike, both the ankle and knee joints will release their stored elastic energy from the footstrike/absorption phase.[28][29][30] The quadriceps group/knee extensors go into full knee extension, pushing the body off of the ground. At the same time, the knee flexors and stretch reflex pull the knee back into flexion, adding to a pulling motion on the ground and beginning the initial swing phase. The hip extensors extend to maximum, adding the forces pulling and pushing off of the ground. The movement and momentum generated by the hip extensors also contributes to knee flexion and the beginning of the initial swing phase.		Initial swing is the response of both stretch reflexes and concentric movements to the propulsion movements of the body. Hip flexion and knee flexion occur beginning the return of the limb to the starting position and setting up for another footstrike. Initial swing ends at midswing, when the limb is again directly underneath the trunk, pelvis and hip with the knee joint flexed and hip flexion continuing. Terminal swing then begins as hip flexion continues to the point of activation of the stretch reflex of the hip extensors. The knee begins to extend slightly as it swings to the anterior portion of the body. The foot then makes contact with the ground with footstrike, completing the running cycle of one side of the lower extremity. Each limb of the lower extremity works opposite to the other. When one side is in toe-off/propulsion, the other hand is in the swing/recovery phase preparing for footstrike.[15][16][17][18] Following toe-off and the beginning of the initial swing of one side, there is a flight phase where neither extremity is in contact with the ground due to the opposite side finishing terminal swing. As the footstrike of the one hand occurs, initial swing continues. The opposing limbs meet with one in midstance and midswing, beginning the propulsion and terminal swing phases.		Upper extremity function serves mainly in providing balance in conjunction with the opposing side of the lower extremity.[16] The movement of each leg is paired with the opposite arm which serves to counterbalance the body, particularly during the stance phase.[23] The arms move most effectively (as seen in elite athletes) with the elbow joint at an approximately 90 degrees or less, the hands swinging from the hips up to mid chest level with the opposite leg, the Humerus moving from being parallel with the trunk to approximately 45 degrees shoulder extension (never passing the trunk in flexion) and with as little movement in the transverse plane as possible.[31] The trunk also rotates in conjunction with arm swing. It mainly serves as a balance point from which the limbs are anchored. Thus trunk motion should remain mostly stable with little motion except for slight rotation as excessive movement would contribute to transverse motion and wasted energy. Mechanics of Propulsion		Recent research into various forms of running has focused on the differences, in the potential injury risks and shock absorption capabilities between heel and mid/forefoot footstrikes. It has been shown that heel striking is generally associated with higher rates of injury and impact due to inefficient shock absorption and inefficient biomechanical compensations for these forces.[19] This is due to forces from a heel strike traveling through bones for shock absorption rather than being absorbed by muscles. Since bones cannot disperse forces easily, the forces transmitted to other parts of the body, including ligaments, joints and bones in the rest of the lower extremity all the way up to the lower back.[32] This causes the body to use abnormal compensatory motions in an attempt to avoid serious bone injuries.[33] These compensations include internal rotation of the tibia, knee and hip joints. Excessive amounts of compensation over time have been linked to higher risk of injuries in those joints as well as the muscles involved in those motions.[25] Conversely, a mid/forefoot strike has been associated with greater efficiency and lower injury risk due to the triceps surae being used as a lever system to absorb forces with the muscles eccentrically rather than through the bone.[19] Landing with a mid/forefoot strike has also been shown to not only properly attenuate shock but allows the triceps surae to aid in propulsion via reflexive plantarflexion after stretching to absorb ground contact forces.[24][34] Thus a mid/forefoot strike may aid in propulsion. However, even among elite athletes there are variations in self selected footstrike types.[35] This is especially true in longer distance events, where there is a prevalence of heel strikers.[36] There does tend however to be a greater percentage of mid/forefoot striking runners in the elite fields, particularly in the faster racers and the winning individuals or groups.[31] While one could attribute the faster speeds of elite runners compared to recreational runners with similar footstrikes to physiological differences, the hip and joints have been left out of the equation for proper propulsion. This brings up the question as to how heel striking elite distance runners are able to keep up such high paces with a supposedly inefficient and injurious foot strike technique.		Biomechanical factors associated with elite runners include increased hip function, use and stride length over recreational runners.[31][37] An increase in running speeds causes increased ground reaction forces and elite distance runners must compensate for this to maintain their pace over long distances.[38] These forces are attenuated through increased stride length via increased hip flexion and extension through decreased ground contact time and more force being used in propulsion.[38][39][40] With increased propulsion in the horizontal plane, less impact occurs from decreased force in the vertical plane.[41] Increased hip flexion allows for increased use of the hip extensors through midstance and toe-off, allowing for more force production.[23] The difference even between world class and national level distance runners has been associated with more efficient hip joint function.[42] The increase in velocity likely comes from the increased range of motion in hip flexion and extension, allowing for greater acceleration and velocity. The hip extensors and hip extension have been linked to more powerful knee extension during toe-off, which contributes to propulsion.[31] Stride length must be properly increased with some degree of knee flexion maintained through the terminal swing phases, as excessive knee extension during this phase along with footstrike has been associated with higher impact forces due to braking and an increased prevalence of heel striking.[43] Elite runners tend to exhibit some degree of knee flexion at footstrike and midstance, which first serves to eccentrically absorb impact forces in the quadriceps muscle group.[42][44][45] Secondly it allows for the knee joint to concentrically contract and provides major aid in propulsion during toe-off as the quadriceps group is capable of produce large amounts of force.[23] Recreational runners have been shown to increase stride length through increased knee extension rather than increased hip flexion as exhibited by elite runners, which serves instead to provide an intense breaking motion with each step and decrease the rate and efficiency of knee extension during toe-off, slowing down speed.[37] Knee extension however contributes to additional stride length and propulsion during toe-off and is seen more frequently in elite runners as well.[31]		Leaning forward places a runner's center of mass on the front part of the foot, which avoids landing on the heel and facilitates the use of the spring mechanism of the foot. It also makes it easier for the runner to avoid landing the foot in front of the center of mass and the resultant braking effect. While upright posture is essential, a runner should maintain a relaxed frame and use his/her core to keep posture upright and stable. This helps prevent injury as long as the body is neither rigid nor tense. The most common running mistakes are tilting the chin up and scrunching shoulders.[46]		Exercise physiologists have found that the stride rates are extremely consistent across professional runners, between 185 and 200 steps per minute. The main difference between long- and short-distance runners is the length of stride rather than the rate of stride.[47][48]		During running, the speed at which the runner moves may be calculated by multiplying the cadence (steps per second) by the stride length. Running is often measured in terms of pace[49] in minutes per mile or kilometer. Fast stride rates coincide with the rate one pumps one's arms. The faster one's arms move up and down, parallel with the body, the faster the rate of stride. Different types of stride are necessary for different types of running. When sprinting, runners stay on their toes bringing their legs up, using shorter and faster strides. Long distance runners tend to have more relaxed strides that vary.		While there exists the potential for injury while running (just as there is in any sport), there are many benefits. Some of these benefits include potential weight loss, improved cardiovascular and respiratory health (reducing the risk of cardiovascular and respiratory diseases), improved cardiovascular fitness, reduced total blood cholesterol, strengthening of bones (and potentially increased bone density), possible strengthening of the immune system and an improved self-esteem and emotional state.[50] Running, like all forms of regular exercise, can effectively slow[51] or reverse[52] the effects of aging.		Whereby an optimal amount of vigorous aerobic exercise such as running might bring benefits related to lower cardiovascular disease and life extension, it should be noted that in an excessive dose (e.g., marathons) it might have an opposite effect associated with cardiotoxicity.[53]		Running can assist people in losing weight, staying in shape and improving body composition. Research suggests that for the person of average weight, they will burn approximately 100 calories per mile they run.[54] Running increases your metabolism even after you have finished running. You will continue to burn an increased level of calories for a short time after the run.[55] Different speeds and distances are appropriate for different individual health and fitness levels. For new runners, it takes time to get into shape. The key is consistency and a slow increase in speed and distance.[54] While running, it is best to pay attention to how one's body feels. If a runner is gasping for breath or feels exhausted while running, it may be beneficial to slow down or try a shorter distance for a few weeks. If a runner feels that the pace or distance is no longer challenging, then the runner may want to speed up or run farther.[56]		Running can also have psychological benefits, as many participants in the sport report feeling an elated, euphoric state, often referred to as a "runner's high".[57] Running is frequently recommended as therapy for people with clinical depression and people coping with addiction.[58] A possible benefit may be the enjoyment of nature and scenery, which also improves psychological well-being[59] (see Ecopsychology § Practical benefits).		In animal models, running has been shown to increase the number of newly born neurons within the brain.[60] This finding could have significant implications in aging as well as learning and memory. A recent study published in Cell Metabolism has also linked running with improved memory and learning skills.[61]		Many injuries are associated with running because of its high-impact nature. Change in running volume may lead to development of patellofemoral pain syndrome, iliotibial band syndrome, patellar tendinopathy, plica syndrome, and medial tibial stress syndrome. Change in running pace may cause Achilles Tendinitis, gastrocnemius injuries, and plantar fasciitis.[62] Repetitive stress on the same tissues without enough time for recovery or running with improper form can lead to many of the above. Runners generally attempt to minimize these injuries by warming up before exercise,[22] focusing on proper running form, performing strength training exercises, eating a well balanced diet, allowing time for recovery, and "icing" (applying ice to sore muscles or taking an ice bath).[63]		Some runners may experience injuries when running on concrete surfaces. The problem with running on concrete is that the body adjusts to this flat surface running, and some of the muscles will become weaker, along with the added impact of running on a harder surface. Therefore, it is advised to change terrain occasionally – such as trail, beach, or grass running. This is more unstable ground and allows the legs to strengthen different muscles. Runners should be wary of twisting their ankles on such terrain. Running downhill also increases knee stress and should, therefore, be avoided. Reducing the frequency and duration can also prevent injury.		Barefoot running has been promoted as a means of reducing running related injuries,[64] but this remains controversial and a majority of professionals advocate the wearing of appropriate shoes as the best method for avoiding injury.[65] However, a study in 2013 concluded that wearing neutral shoes is not associated with increased injuries.[66]		Another common, running-related injury is chafing, caused by repetitive rubbing of one piece of skin against another, or against an article of clothing. One common location for chafe to occur is the runner's upper thighs. The skin feels coarse and develops a rash-like look. A variety of deodorants and special anti-chafing creams are available to treat such problems. Chafe is also likely to occur on the nipple. There are a variety of home remedies that runners use to deal with chafing while running such as band-aids and using grease to reduce friction. Prevention is key which is why form fitting cloths are important.[67]		Running is both a competition and a type of training for sports that have running or endurance components. As a sport, it is split into events divided by distance and sometimes includes permutations such as the obstacles in steeplechase and hurdles. Running races are contests to determine which of the competitors is able to run a certain distance in the shortest time. Today, competitive running events make up the core of the sport of athletics. Events are usually grouped into several classes, each requiring substantially different athletic strengths and involving different tactics, training methods, and types of competitors.		Running competitions have probably existed for most of humanity's history and were a key part of the ancient Olympic Games as well as the modern Olympics. The activity of running went through a period of widespread popularity in the United States during the running boom of the 1970s. Over the next two decades, as many as 25 million Americans were doing some form of running or jogging – accounting for roughly one tenth of the population.[68] Today, road racing is a popular sport among non-professional athletes, who included over 7.7 million people in America alone in 2002.[69]		Footspeed, or sprint speed, is the maximum speed at which a human can run. It is affected by many factors, varies greatly throughout the population, and is important in athletics and many sports.		The fastest human footspeed on record is 44.7 km/h (12.4 m/s, 27.8 mph), seen during a 100-meter sprint (average speed between the 60th and the 80th meter) by Usain Bolt.[70]		(see Category:Athletics (track and field) record progressions)		Track running events are individual or relay events with athletes racing over specified distances on an oval running track. The events are categorised as sprints, middle and long-distance, and hurdling.		Road running takes place on a measured course over an established road (as opposed to track and cross country running). These events normally range from distances of 5 kilometers to longer distances such as half marathons and marathons, and they may involve scores of runners or wheelchair entrants.		Cross country running takes place over the open or rough terrain. The courses used for these events may include grass, mud, woodlands, hills, flat ground and water. It is a popular participatory sport and is one of the events which, along with track and field, road running, and racewalking, makes up the umbrella sport of athletics.		Sprints are short running events in athletics and track and field. Races over short distances are among the oldest running competitions. The first 13 editions of the Ancient Olympic Games featured only one event – the stadion race, which was a race from one end of the stadium to the other.[71] There are three sprinting events which are currently held at the Olympics and outdoor World Championships: the 100 metres, 200 metres, and 400 metres. These events have their roots in races of imperial measurements which were later altered to metric: the 100 m evolved from the 100-yard dash,[72] the 200 m distances came from the furlong (or 1/8 of a mile),[73] and the 400 m was the successor to the 440 yard dash or quarter-mile race.[74]		At the professional level, sprinters begin the race by assuming a crouching position in the starting blocks before leaning forward and gradually moving into an upright position as the contest progresses and momentum is gained.[75] Athletes remain in the same lane on the running track throughout all sprinting events,[74] with the sole exception of the 400 m indoors. Races up to 100 m are largely focused upon acceleration to an athlete's maximum speed.[75] All sprints beyond this distance increasingly incorporate an element of endurance.[76] Human physiology dictates that a runner's near-top speed cannot be maintained for more than thirty seconds or so as lactic acid builds up, and leg muscles begin to be deprived of oxygen.[74]		The 60 metres is a common indoor event and it an indoor world championship event. Other less-common events include the 50 metres, 55 metres, 300 metres and 500 metres which are used in some high and collegiate competitions in the United States. The 150 metres, though rarely competed, has a star-studded history: Pietro Mennea set a world best in 1983,[77] Olympic champions Michael Johnson and Donovan Bailey went head-to-head over the distance in 1997,[78] and Usain Bolt improved Mennea's record in 2009.[77]		Middle distance running events are track races longer than sprints up to 3000 metres. The standard middle distances are the 800 metres, 1500 metres and mile run, although the 3000 metres may also be classified as a middle distance event.[79] The 880 yard run, or half mile, was the forebear to the 800 m distance and it has its roots in competitions in the United Kingdom in the 1830s.[80] The 1500 m came about as a result of running three laps of a 500 m track, which was commonplace in continental Europe in the 1900s.[81]		Examples of longer-distance running events are long distance track races, marathons, ultramarathons, and multiday races.				
The American Institute for Cancer Research (AICR) is a large American cancer research organization associated with the World Cancer Research Fund umbrella organization.		Its stated mission is:		As of 2015, the charity has a "one star" rating from Charity Navigator, with a score of 65.32 out of 100.[1] At the time of their rating, over 50% of the charity's income turned around into fundraising expenses.		In November 2007, the AICR, in conjunction with WCRF, published 'Food, Nutrition, Physical Activity, and the Prevention of Cancer: a Global Perspective', ISBN 978-0-9722522-2-5, and freely downloadable from its official website.[2] This 537-page analysis is designed to be an authoritative compendium on the origins of cancer. The AICR sees it as leading to concrete recommendations for public policy and it will be publishing a policy report in 2008.		The report is the result of a five-year process that has seen teams of scientists around the world sifting through 500,000 studies on the link between cancer and diet, physical activity and weight, before identifying the 7,000 most relevant. A panel of 21 world-renowned experts, chaired by Professor Sir Michael Marmot, then made recommendations.				
Rich Froning Jr. (born July 21, 1987) is an American professional CrossFit athlete known for his achievements in the 2010, 2011, 2012, 2013, 2014, 2015, and 2016 CrossFit Games. He became the first person to win the title of "Fittest Man on Earth" four times with his first-place finish in the 2011, 2012, 2013, and 2014 CrossFit Games. In 2015 and 2016, he led team CrossFit Mayhem Freedom to the first-place finish in the Team category in the CrossFit Games and claimed the Affiliate Cup.		Froning has not only been successful in the sport; he has been a pioneer in making a living off of the sport of CrossFit. Froning has won over $1,050,000 in prize money for winning the CrossFit Games four times,[2] and has accumulated significant sponsorships from brands like Reebok,[3] Oakley,[4] and Rogue Fitness.[5] In early 2015, Reebok released a shoe designed in part by Froning which bore the name of the Bible scripture tattooed to his side, the Reebok CrossFit Compete 6:14.[6]		Froning owns and operates the affiliate gym CrossFit Mayhem, located in Cookeville, Tennessee, and is a member of the CrossFit Level 1 Seminar Staff.						Froning was born in Mount Clemens, Michigan. He moved to Cookeville, Tennessee, where he currently resides. There he attended Cookeville High School where he played baseball and was an all-district, all-region second baseman. He also participated in football. Upon graduating in 2005, Froning received a baseball scholarship to Walters State Community College. Soon after, Froning decided to end his baseball career, and began working at the Cookeville Fire Department, while continuing his studies at Tennessee Technological University.[7] While working out with the fire department, Froning became interested in CrossFit, which he soon developed a passion for and in 2010, he began coaching and competing.[8]		Froning has competed in the CrossFit Games since 2010. Froning won the 2011 CrossFit Games, and followed that up by winning every stage of the 2012-2014 Games season including the worldwide Open, Regionals, and the Games. That winning streak ended after he retired from Individual Competition in 2015, and he subsequently took second place in the world in the 2015 Open, behind Mathew Fraser. However, as part of his new team, CrossFit Mayhem Freedom, he began a new winning streak by earning first place at both the 2015 Central East Regional and at the Games.		Froning has said "It's not necessarily that I like to win, but I hate losing more."[6]		In 2010, Froning qualified for the 2010 Southeast Regional competition through the Sectional Qualifiers, a precursor to the online-based Open. Froning won the Regional, thereby qualifying for the fourth CrossFit Games at the Home Depot Center in Carson, California.		At the 2010 Games, Froning earned five top 5 finishes including two event wins, and never dropped below 13th. He was leading heading into the final event, however the rope climb portion of the final event exposed "a chink in his armor" in terms of technique. Froning did not know how to wrap his legs and feet around the rope, so he was forced to attempt the rope climbs only using his arms. Fatigued, Froning fell from the rope multiple times.[9][10] His 12th-place finish on the final event opened up the space necessary for Graham Holmberg to move up to the top spot. The 2010 Games podium had Holmberg in first, Froning in second, and Chris Spealler in third.		In 2012, Froning and Annie Thorisdottir were the first athletes to win the CrossFit Games twice. In the years following, Froning set the standard for the "Fittest Man on Earth" with four consecutive CrossFit Games wins. In a documentary released in the summer of 2015, "Froning," documentary filmmakers from CrossFit, Inc., argued that Froning is the Fittest Man in History.[11] In 2015 and 2016, Froning took first place in the Central regional event, qualifying him for the CrossFit Games as an individual; however, he chose to participate in the team competition instead, as part of the "Mayhem Freedom" team, which placed first both years.[12]		Froning does not adhere to the Paleolithic diet nor Zone diet, both of which are popular in the CrossFit community.[16] He doesn't adhere to a specific diet plan, preferring to listen to his body. He eats a lot of peanut butter and drinks whole milk. During the day he typically doesn't eat an excessive amount, instead eating a large meal at night and drinking multiple protein shakes.[17]		He reports working out multiple times a day[18] and prefers not to take days off from training, unlike many other athletes. Froning does not have a coach nor programmer who writes his workouts. Instead, Froning often finds an exceptional athlete to be his training partner, such as Games athletes Dan Bailey and James Hobart.		Some of Froning's methods are discussed in his 2013 memoir, First: What It Takes to Win.[19]		
Allied victory		Asia and the Pacific Pacific War		Mediterranean and Middle East		Other campaigns		Contemporaneous wars		World War II (often abbreviated to WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945, although related conflicts began earlier. It involved the vast majority of the world's countries—including all of the great powers—eventually forming two opposing military alliances: the Allies and the Axis. It was the most widespread war in history, and directly involved more than 100 million people from over 30 countries. In a state of total war, the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, erasing the distinction between civilian and military resources. Marked by mass deaths of civilians, including the Holocaust (in which approximately 6 million Jews were killed)[1] and the strategic bombing of industrial and population centres (in which approximately one million were killed, and which included the atomic bombings of Hiroshima and Nagasaki),[2] it resulted in an estimated 50 million to 85 million fatalities. These made World War II the deadliest conflict in human history.[3]		The Empire of Japan aimed to dominate Asia and the Pacific and was already at war with the Republic of China in 1937,[4] but the world war is generally said to have begun on 1 September 1939[5] with the invasion of Poland by Nazi Germany and subsequent declarations of war on Germany by France and the United Kingdom. Supplied by the Soviet Union, from late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. Under the Molotov–Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. The war continued primarily between the European Axis powers and the coalition of the United Kingdom and the British Commonwealth, with campaigns including the North Africa and East Africa campaigns, the aerial Battle of Britain, the Blitz bombing campaign, the Balkan Campaign as well as the long-running Battle of the Atlantic. On 22 June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history, which trapped the major part of the Axis military forces into a war of attrition. In December 1941, Japan attacked the United States and European colonies in the Pacific Ocean, and quickly conquered much of the Western Pacific.		The Axis advance halted in 1942 when Japan lost the critical Battle of Midway, near Hawaii, and Germany was defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union. In 1943, with a series of German defeats on the Eastern Front, the Allied invasion of Sicily and the Allied invasion of Italy which brought about Italian surrender, and Allied victories in the Pacific, the Axis lost the initiative and undertook strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained all of its territorial losses and invaded Germany and its allies. During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in South Central China and Burma, while the Allies crippled the Japanese Navy and captured key Western Pacific islands.		The war in Europe concluded with an invasion of Germany by the Western Allies and the Soviet Union, culminating in the capture of Berlin by Soviet troops, the suicide of Adolf Hitler and the subsequent German unconditional surrender on 8 May 1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 August and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings and the Soviet invasion of Manchuria, Japan surrendered on 15 August 1945. Thus ended the war in Asia, cementing the total victory of the Allies.		World War II altered the political alignment and social structure of the world. The United Nations (UN) was established to foster international co-operation and prevent future conflicts. The victorious great powers—the United States, the Soviet Union, China, the United Kingdom, and France—became the permanent members of the United Nations Security Council.[6] The Soviet Union and the United States emerged as rival superpowers, setting the stage for the Cold War, which lasted for the next 46 years. Meanwhile, the influence of European great powers waned, while the decolonisation of Asia and Africa began. Most countries whose industries had been damaged moved towards economic recovery. Political integration, especially in Europe, emerged as an effort to end pre-war enmities and to create a common identity.[7]		The start of the war in Europe is generally held to be 1 September 1939,[8][9] beginning with the German invasion of Poland; Britain and France declared war on Germany two days later. The dates for the beginning of war in the Pacific include the start of the Second Sino-Japanese War on 7 July 1937,[10][11] or even the Japanese invasion of Manchuria on 19 September 1931.[12][13]		Others follow the British historian A. J. P. Taylor, who held that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously and the two wars merged in 1941. This article uses the conventional dating. Other starting dates sometimes used for World War II include the Italian invasion of Abyssinia on 3 October 1935.[14] The British historian Antony Beevor views the beginning of World War II as the Battles of Khalkhin Gol fought between Japan and the forces of Mongolia and the Soviet Union from May to September 1939.[15]		The exact date of the war's end is also not universally agreed upon. It was generally accepted at the time that the war ended with the armistice of 14 August 1945 (V-J Day), rather than the formal surrender of Japan (2 September 1945). A peace treaty with Japan was signed in 1951.[16] A treaty regarding Germany's future allowed the reunification of East and West Germany to take place in 1990 and resolved other post-World War II issues.[17]		World War I had radically altered the political European map, with the defeat of the Central Powers—including Austria-Hungary, Germany, Bulgaria and the Ottoman Empire—and the 1917 Bolshevik seizure of power in Russia, which eventually led to the founding of the Soviet Union. Meanwhile, the victorious Allies of World War I, such as France, Belgium, Italy, Romania and Greece gained territory, and new nation-states were created out of the collapse of Austria-Hungary and the Ottoman and Russian Empires.		To prevent a future world war, the League of Nations was created during the 1919 Paris Peace Conference. The organisation's primary goals were to prevent armed conflict through collective security, military and naval disarmament, and settling international disputes through peaceful negotiations and arbitration.		Despite strong pacifist sentiment after World War I,[18] its aftermath still caused irredentist and revanchist nationalism in several European states. These sentiments were especially marked in Germany because of the significant territorial, colonial, and financial losses incurred by the Treaty of Versailles. Under the treaty, Germany lost around 13 per cent of its home territory and all of its overseas possessions, while German annexation of other states was prohibited, reparations were imposed, and limits were placed on the size and capability of the country's armed forces.[19]		The German Empire was dissolved in the German Revolution of 1918–1919, and a democratic government, later known as the Weimar Republic, was created. The interwar period saw strife between supporters of the new republic and hardline opponents on both the right and left. Italy, as an Entente ally, had made some post-war territorial gains; however, Italian nationalists were angered that the promises made by Britain and France to secure Italian entrance into the war were not fulfilled in the peace settlement. From 1922 to 1925, the Fascist movement led by Benito Mussolini seized power in Italy with a nationalist, totalitarian, and class collaborationist agenda that abolished representative democracy, repressed socialist, left-wing and liberal forces, and pursued an aggressive expansionist foreign policy aimed at making Italy a world power, promising the creation of a "New Roman Empire".[20]		Adolf Hitler, after an unsuccessful attempt to overthrow the German government in 1923, eventually became the Chancellor of Germany in 1933. He abolished democracy, espousing a radical, racially motivated revision of the world order, and soon began a massive rearmament campaign.[21] Meanwhile, France, to secure its alliance, allowed Italy a free hand in Ethiopia, which Italy desired as a colonial possession. The situation was aggravated in early 1935 when the Territory of the Saar Basin was legally reunited with Germany and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme, and introduced conscription.[22]		To contain Germany, the United Kingdom, France and Italy formed the Stresa Front in April 1935; however, that June, the United Kingdom made an independent naval agreement with Germany, easing prior restrictions. The Soviet Union, concerned by Germany's goals of capturing vast areas of Eastern Europe, drafted a treaty of mutual assistance with France. Before taking effect though, the Franco-Soviet pact was required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless.[23] The United States, concerned with events in Europe and Asia, passed the Neutrality Act in August of the same year.[24]		Hitler defied the Versailles and Locarno treaties by remilitarising the Rhineland in March 1936, encountering little opposition.[25] In October 1936, Germany and Italy formed the Rome–Berlin Axis. A month later, Germany and Japan signed the Anti-Comintern Pact, which Italy would join in the following year.		The Kuomintang (KMT) party in China launched a unification campaign against regional warlords and nominally unified China in the mid-1920s, but was soon embroiled in a civil war against its former Chinese Communist Party allies[26] and new regional warlords . In 1931, an increasingly militaristic Empire of Japan, which had long sought influence in China[27] as the first step of what its government saw as the country's right to rule Asia, used the Mukden Incident as a pretext to launch an invasion of Manchuria and establish the puppet state of Manchukuo.[28]		Too weak to resist Japan, China appealed to the League of Nations for help. Japan withdrew from the League of Nations after being condemned for its incursion into Manchuria. The two nations then fought several battles, in Shanghai, Rehe and Hebei, until the Tanggu Truce was signed in 1933. Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression in Manchuria, and Chahar and Suiyuan.[29] After the 1936 Xi'an Incident, the Kuomintang and communist forces agreed on a ceasefire to present a united front to oppose Japan.[30]		The Second Italo–Abyssinian War was a brief colonial war that began in October 1935 and ended in May 1936. The war began with the invasion of the Ethiopian Empire (also known as Abyssinia) by the armed forces of the Kingdom of Italy (Regno d'Italia), which was launched from Italian Somaliland and Eritrea.[31] The war resulted in the military occupation of Ethiopia and its annexation into the newly created colony of Italian East Africa (Africa Orientale Italiana, or AOI); in addition, it exposed the weakness of the League of Nations as a force to preserve peace. Both Italy and Ethiopia were member nations, but the League did nothing when the former clearly violated the League's Article X.[32] Germany was the only major European nation to support the invasion. Italy subsequently dropped its objections to Germany's goal of absorbing Austria.[33]		When civil war broke out in Spain, Hitler and Mussolini lent military support to the Nationalist rebels, led by General Francisco Franco. The Soviet Union supported the existing government, the Spanish Republic. Over 30,000 foreign volunteers, known as the International Brigades, also fought against the Nationalists. Both Germany and the USSR used this proxy war as an opportunity to test in combat their most advanced weapons and tactics. The Nationalists won the civil war in April 1939; Franco, now dictator, bargained with both sides during the Second World War, but never concluded any major agreements. He did send volunteers to fight on the Eastern Front under German command but Spain remained neutral and did not allow either side to use its territory.[34][page needed]		In July 1937, Japan captured the former Chinese imperial capital of Beijing after instigating the Marco Polo Bridge Incident, which culminated in the Japanese campaign to invade all of China.[35] The Soviets quickly signed a non-aggression pact with China to lend materiel support, effectively ending China's prior co-operation with Germany. Generalissimo Chiang Kai-shek deployed his best army to defend Shanghai, but, after three months of fighting, Shanghai fell. The Japanese continued to push the Chinese forces back, capturing the capital Nanking in December 1937. After the fall of Nanking, tens of thousands if not hundreds of thousands of Chinese civilians and disarmed combatants were murdered by the Japanese.[36][37]		In March 1938, Nationalist Chinese forces won their first major victory at Taierzhuang but then the city of Xuzhou was taken by Japanese in May.[38] In June 1938, Chinese forces stalled the Japanese advance by flooding the Yellow River; this manoeuvre bought time for the Chinese to prepare their defences at Wuhan, but the city was taken by October.[39] Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve; instead the Chinese government relocated inland to Chongqing and continued the war.[40][41]		In the mid-to-late 1930s, Japanese forces in Manchukuo had sporadic border clashes with the Soviet Union and the Mongolian People's Republic. The Japanese doctrine of Hokushin-ron, which emphasised Japan's expansion northward, was favoured by the Imperial Army during this time. With the Japanese defeat at Khalkin Gol in 1939, the ongoing Second Sino-Japanese War[42] and ally Nazi Germany pursuing neutrality with the Soviets, this policy would prove difficult to maintain. Japan and the Soviet Union eventually signed a Neutrality Pact in April 1941, and Japan adopted the doctrine of Nanshin-ron, promoted by the Navy, which took its focus southward, eventually leading to its war with the United States and the Western Allies.[43][44]		In Europe, Germany and Italy were becoming more aggressive. In March 1938, Germany annexed Austria, again provoking little response from other European powers.[45] Encouraged, Hitler began pressing German claims on the Sudetenland, an area of Czechoslovakia with a predominantly ethnic German population; and soon Britain and France followed the counsel of British Prime Minister Neville Chamberlain and conceded this territory to Germany in the Munich Agreement, which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands.[46] Soon afterwards, Germany and Italy forced Czechoslovakia to cede additional territory to Hungary and Poland annexed Czechoslovakia's Zaolzie region.[47]		Although all of Germany's stated demands had been satisfied by the agreement, privately Hitler was furious that British interference had prevented him from seizing all of Czechoslovakia in one operation. In subsequent speeches Hitler attacked British and Jewish "war-mongers" and in January 1939 secretly ordered a major build-up of the German navy to challenge British naval supremacy. In March 1939, Germany invaded the remainder of Czechoslovakia and subsequently split it into the German Protectorate of Bohemia and Moravia and a pro-German client state, the Slovak Republic.[48] Hitler also delivered an ultimatum to Lithuania, forcing the concession of the Klaipėda Region.		Greatly alarmed and with Hitler making further demands on the Free City of Danzig, Britain and France guaranteed their support for Polish independence; when Italy conquered Albania in April 1939, the same guarantee was extended to Romania and Greece.[49] Shortly after the Franco-British pledge to Poland, Germany and Italy formalised their own alliance with the Pact of Steel.[50] Hitler accused Britain and Poland of trying to "encircle" Germany and renounced the Anglo-German Naval Agreement and the German–Polish Non-Aggression Pact.		In August 1939, Germany and the Soviet Union signed the Molotov–Ribbentrop Pact,[51] a non-aggression treaty with a secret protocol. The parties gave each other rights to "spheres of influence" (western Poland and Lithuania for Germany; eastern Poland, Finland, Estonia, Latvia and Bessarabia for the USSR). It also raised the question of continuing Polish independence.[52] The agreement was crucial to Hitler because it assured that Germany would not have to face the prospect of a two-front war, as it had in World War I, after it defeated Poland.		The situation reached a general crisis in late August as German troops continued to mobilise against the Polish border. In a private meeting with the Italian foreign minister, Count Ciano, Hitler asserted that Poland was a "doubtful neutral" that needed to either yield to his demands or be "liquidated" to prevent it from drawing off German troops in the future "unavoidable" war with the Western democracies. He did not believe Britain or France would intervene in the conflict.[53] On 23 August Hitler ordered the attack to proceed on 26 August, but upon hearing that Britain had concluded a formal mutual assistance pact with Poland and that Italy would maintain neutrality, he decided to delay it.[54]		In response to British requests for direct negotiations to avoid war, Germany made demands on Poland, which only served as a pretext to worsen relations.[55] On 29 August, Hitler demanded that a Polish plenipotentiary immediately travel to Berlin to negotiate the handover of Danzig, and to allow a plebiscite in the Polish Corridor in which the German minority would vote on secession.[55] The Poles refused to comply with the German demands and on the night of 30–31 August in a violent meeting with the British ambassador Neville Henderson, Ribbentrop declared that Germany considered its claims rejected.[56]		On 1 September 1939, Germany invaded Poland under the false pretext that the Poles had carried out a series of sabotage operations against German targets near the border.[57] Two days later, on 3 September, after a British ultimatum to Germany to cease military operations was ignored, Britain and France, followed by the fully independent Dominions[58] of the British Commonwealth[59]—Australia (3 September), Canada (10 September), New Zealand (3 September), and South Africa (6 September)—declared war on Germany. However, initially the alliance provided limited direct military support to Poland, consisting of a cautious, half-hearted French probe into the Saarland.[60] The Western Allies also began a naval blockade of Germany, which aimed to damage the country's economy and war effort.[61] Germany responded by ordering U-boat warfare against Allied merchant and warships, which was to later escalate into the Battle of the Atlantic.		On 17 September 1939, after signing a cease-fire with Japan, the Soviets invaded Poland from the east.[62] The Polish army was defeated and Warsaw surrendered to the Germans on 27 September, with final pockets of resistance surrendering on 6 October. Poland's territory was divided between Germany and the Soviet Union, with Lithuania and Slovakia also receiving small shares. After the defeat of Poland's armed forces, the Polish resistance established an Underground State and a partisan Home Army.[63] About 100,000 Polish military personnel were evacuated to Romania and the Baltic countries; many of these soldiers later fought against the Germans in other theatres of the war.[64] Poland's Enigma codebreakers were also evacuated to France.[65]		On 6 October, Hitler made a public peace overture to Britain and France, but said that the future of Poland was to be determined exclusively by Germany and the Soviet Union. Chamberlain rejected this on 12 October, saying "Past experience has shown that no reliance can be placed upon the promises of the present German Government."[56] After this rejection Hitler ordered an immediate offensive against France,[66] but bad weather forced repeated postponements until the spring of 1940.[67][68][69]		After signing the German–Soviet Treaty of Friendship, Cooperation and Demarcation, the Soviet Union forced the Baltic countries—Estonia, Latvia and Lithuania—to allow it to station Soviet troops in their countries under pacts of "mutual assistance".[70][71][72] Finland rejected territorial demands, prompting a Soviet invasion in November 1939.[73] The resulting Winter War ended in March 1940 with Finnish concessions.[74] Britain and France, treating the Soviet attack on Finland as tantamount to its entering the war on the side of the Germans, responded to the Soviet invasion by supporting the USSR's expulsion from the League of Nations.[72]		In June 1940, the Soviet Union forcibly annexed Estonia, Latvia and Lithuania,[71] and the disputed Romanian regions of Bessarabia, Northern Bukovina and Hertza. Meanwhile, Nazi-Soviet political rapprochement and economic co-operation[75][76] gradually stalled,[77][page needed][78] and both states began preparations for war.[79]		In April 1940, Germany invaded Denmark and Norway to protect shipments of iron ore from Sweden, which the Allies were attempting to cut off by unilaterally mining neutral Norwegian waters.[80] Denmark capitulated after a few hours, and despite Allied support, during which the important harbour of Narvik temporarily was recaptured from the Germans, Norway was conquered within two months.[81] British discontent over the Norwegian campaign led to the replacement of the British Prime Minister, Neville Chamberlain, with Winston Churchill on 10 May 1940.[82]		Germany launched an offensive against France and, adhering to the Manstein Plan also attacked the neutral nations of Belgium, the Netherlands, and Luxembourg on 10 May 1940.[83] That same day British forces landed in Iceland and the Faroes to preempt a possible German invasion of the islands.[84] The U.S., in close co-operation with the Danish envoy to Washington D.C., agreed to protect Greenland, laying the political framework for the formal establishment of bases in April 1941. The Netherlands and Belgium were overrun using blitzkrieg tactics in a few days and weeks, respectively.[85] The French-fortified Maginot Line and the main body of the Allied forces which had moved into Belgium were circumvented by a flanking movement through the thickly wooded Ardennes region,[86] mistakenly perceived by Allied planners as an impenetrable natural barrier against armoured vehicles.[87][88] As a result, the bulk of the Allied armies found themselves trapped in an encirclement and were beaten. The majority were taken prisoner, whilst over 300,000, mostly British and French, were evacuated from the continent at Dunkirk by early June, although abandoning almost all of their equipment.[89]		On 10 June, Italy invaded France, declaring war on both France and the United Kingdom.[90] Paris fell to the Germans on 14 June and eight days later France signed an armistice with Germany and was soon divided into German and Italian occupation zones,[91] and an unoccupied rump state under the Vichy Regime, which, though officially neutral, was generally aligned with Germany. France kept its fleet but the British feared the Germans would seize it, so on 3 July, the British attacked it.[92]		The Battle of Britain[93] began in early July with Luftwaffe attacks on shipping and harbours.[94] On 19 July, Hitler again publicly offered to end the war, saying he had no desire to destroy the British Empire. The United Kingdom rejected this ultimatum.[95] The main German air superiority campaign started in August but failed to defeat RAF Fighter Command, and a proposed invasion was postponed indefinitely on 17 September. The German strategic bombing offensive intensified as night attacks on London and other cities in the Blitz, but largely failed to disrupt the British war effort.[94]		Using newly captured French ports, the German Navy enjoyed success against an over-extended Royal Navy, using U-boats against British shipping in the Atlantic.[96] The British scored a significant victory on 27 May 1941 by sinking the German battleship Bismarck.[97] Perhaps most importantly, during the Battle of Britain the Royal Air Force had successfully resisted the Luftwaffe's assault, and the German bombing campaign largely ended in May 1941.[98]		Throughout this period, the neutral United States took measures to assist China and the Western Allies. In November 1939, the American Neutrality Act was amended to allow "cash and carry" purchases by the Allies.[99] In 1940, following the German capture of Paris, the size of the United States Navy was significantly increased. In September, the United States further agreed to a trade of American destroyers for British bases.[100] Still, a large majority of the American public continued to oppose any direct military intervention into the conflict well into 1941.[101]		Although Roosevelt had promised to keep the United States out of the war, he nevertheless took concrete steps to prepare for war. In December 1940 he accused Hitler of planning world conquest and ruled out negotiations as useless, calling for the US to become an "arsenal of democracy" and promoted the passage of Lend-Lease aid to support the British war effort.[95] In January 1941 secret high level staff talks with the British began for the purposes of determining how to defeat Germany should the US enter the war. They decided on a number of offensive policies, including an air offensive, the "early elimination" of Italy, raids, support of resistance groups, and the capture of positions to launch an offensive against Germany.[102]		At the end of September 1940, the Tripartite Pact united Japan, Italy and Germany to formalise the Axis Powers. The Tripartite Pact stipulated that any country, with the exception of the Soviet Union, not in the war which attacked any Axis Power would be forced to go to war against all three.[103] The Axis expanded in November 1940 when Hungary, Slovakia and Romania joined the Tripartite Pact.[104] Romania would make a major contribution (as did Hungary) to the Axis war against the USSR, partially to recapture territory ceded to the USSR, partially to pursue its leader Ion Antonescu's desire to combat communism.[105]		Italy began operations in the Mediterranean, initiating a siege of Malta in June, conquering British Somaliland in August, and making an incursion into British-held Egypt in September 1940. In October 1940, Italy started the Greco-Italian War because of Mussolini's jealousy of Hitler's success but within days was repulsed and pushed back into Albania, where a stalemate soon occurred.[106] The United Kingdom responded to Greek requests for assistance by sending troops to Crete and providing air support to Greece. Hitler decided that when the weather improved he would take action against Greece to assist the Italians and prevent the British from gaining a foothold in the Balkans, to strike against the British naval dominance of the Mediterranean, and to secure his hold on Romanian oil.[107]		In December 1940, British Commonwealth forces began counter-offensives against Italian forces in Egypt and Italian East Africa.[108] The offensive in North Africa was highly successful and by early February 1941 Italy had lost control of eastern Libya and large numbers of Italian troops had been taken prisoner. The Italian Navy also suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission by a carrier attack at Taranto, and neutralising several more warships at the Battle of Cape Matapan.[109]		The Germans soon intervened to assist Italy. Hitler sent German forces to Libya in February, and by the end of March they had launched an offensive which drove back the Commonwealth forces which had been weakened to support Greece.[110] In under a month, Commonwealth forces were pushed back into Egypt with the exception of the besieged port of Tobruk.[111] The Commonwealth attempted to dislodge Axis forces in May and again in June, but failed on both occasions.[112]		By late March 1941, following Bulgaria's signing of the Tripartite Pact, the Germans were in position to intervene in Greece. Plans were changed, however, because of developments in neighbouring Yugoslavia. The Yugoslav government had signed the Tripartite Pact on 25 March, only to be overthrown two days later by a British-encouraged coup. Hitler viewed the new regime as hostile and immediately decided to eliminate it. On 6 April Germany simultaneously invaded both Yugoslavia and Greece, making rapid progress and forcing both nations to surrender within the month. The British were driven from the Balkans after Germany conquered the Greek island of Crete by the end of May.[113] Although the Axis victory was swift, bitter partisan warfare subsequently broke out against the Axis occupation of Yugoslavia, which continued until the end of the war.		The Allies did have some successes during this time. In the Middle East, Commonwealth forces first quashed an uprising in Iraq which had been supported by German aircraft from bases within Vichy-controlled Syria,[114] then, with the assistance of the Free French, invaded Syria and Lebanon to prevent further such occurrences.[115]		With the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations. With the Soviets wary of mounting tensions with Germany and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions in Southeast Asia, the two powers signed the Soviet–Japanese Neutrality Pact in April 1941.[116] By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, massing forces on the Soviet border.[117]		Hitler believed that Britain's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later.[118] He therefore decided to try to strengthen Germany's relations with the Soviets, or failing that, to attack and eliminate them as a factor. In November 1940, negotiations took place to determine if the Soviet Union would join the Tripartite Pact. The Soviets showed some interest, but asked for concessions from Finland, Bulgaria, Turkey, and Japan that Germany considered unacceptable. On 18 December 1940, Hitler issued the directive to prepare for an invasion of the Soviet Union.		On 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet Union in Operation Barbarossa, with Germany accusing the Soviets of plotting against them. They were joined shortly by Finland and Hungary.[119] The primary targets of this surprise offensive[120][page needed] were the Baltic region, Moscow and Ukraine, with the ultimate goal of ending the 1941 campaign near the Arkhangelsk-Astrakhan line, from the Caspian to the White Seas. Hitler's objectives were to eliminate the Soviet Union as a military power, exterminate Communism, generate Lebensraum ("living space")[121] by dispossessing the native population[122][page needed] and guarantee access to the strategic resources needed to defeat Germany's remaining rivals.[123][page needed]		Although the Red Army was preparing for strategic counter-offensives before the war,[124][page needed] Barbarossa forced the Soviet supreme command to adopt a strategic defence. During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel. By the middle of August, however, the German Army High Command decided to suspend the offensive of a considerably depleted Army Group Centre, and to divert the 2nd Panzer Group to reinforce troops advancing towards central Ukraine and Leningrad.[125][page needed] The Kiev offensive was overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made possible further advance into Crimea and industrially developed Eastern Ukraine (the First Battle of Kharkov).[126]		The diversion of three quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to the Eastern Front[127] prompted Britain to reconsider its grand strategy.[128][page needed] In July, the UK and the Soviet Union formed a military alliance against Germany[129] The British and Soviets invaded Iran to secure the Persian Corridor and Iran's oil fields.[130] In August, the United Kingdom and the United States jointly issued the Atlantic Charter.[131]		By October Axis operational objectives in Ukraine and the Baltic region were achieved, with only the sieges of Leningrad[132][page needed] and Sevastopol continuing.[133] A major offensive against Moscow was renewed; after two months of fierce battles in increasingly harsh weather the German army almost reached the outer suburbs of Moscow, where the exhausted troops[134] were forced to suspend their offensive.[135] Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Soviet capability to resist was not broken, and the Soviet Union retained a considerable part of its military potential. The blitzkrieg phase of the war in Europe had ended.[136][page needed]		By early December, freshly mobilised reserves[137][page needed] allowed the Soviets to achieve numerical parity with Axis troops.[138] This, as well as intelligence data which established that a minimal number of Soviet troops in the East would be sufficient to deter any attack by the Japanese Kwantung Army,[139][page needed] allowed the Soviets to begin a massive counter-offensive that started on 5 December all along the front and pushed German troops 100–250 kilometres (62–155 mi) west.[140]		In 1939, the United States had renounced its trade treaty with Japan; and, beginning with an aviation gasoline ban in July 1940, Japan became subject to increasing economic pressure.[95] During this time, Japan launched its first attack against Changsha, a strategically important Chinese city, but was repulsed by late September.[141] Despite several offensives by both sides, the war between China and Japan was stalemated by 1940. To increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan invaded and occupied northern Indochina.[142] Afterwards, the United States embargoed iron, steel and mechanical parts against Japan.[143] Other sanctions soon followed.		In August of that year, Chinese communists launched an offensive in Central China; in retaliation, Japan instituted harsh measures in occupied areas to reduce human and material resources for the communists.[144] Continued antipathy between Chinese communist and nationalist forces culminated in armed clashes in January 1941, effectively ending their co-operation.[145] In March, the Japanese 11th army attacked the headquarters of the Chinese 19th army but was repulsed during Battle of Shanggao.[146] In September, Japan attempted to take the city of Changsha again and clashed with Chinese nationalist forces.[147]		German successes in Europe encouraged Japan to increase pressure on European governments in Southeast Asia. The Dutch government agreed to provide Japan some oil supplies from the Dutch East Indies, but negotiations for additional access to their resources ended in failure in June 1941.[148] In July 1941 Japan sent troops to southern Indochina, thus threatening British and Dutch possessions in the Far East. The United States, United Kingdom and other Western governments reacted to this move with a freeze on Japanese assets and a total oil embargo.[149][150] Japan was also planning an invasion of the Soviet Far East, intending to capitalize off the German invasion in the west, but abandoned the operation after the sanctions.[151]		Since early 1941 the United States and Japan had been engaged in negotiations in an attempt to improve their strained relations and end the war in China. During these negotiations Japan advanced a number of proposals which were dismissed by the Americans as inadequate.[152] At the same time the US, Britain, and the Netherlands engaged in secret discussions for the joint defence of their territories, in the event of a Japanese attack against any of them.[153] Roosevelt reinforced the Philippines (an American protectorate scheduled for independence in 1946) and warned Japan that the US would react to Japanese attacks against any "neighboring countries".[153]		Frustrated at the lack of progress and feeling the pinch of the American-British-Dutch sanctions, Japan prepared for war. On 20 November a new government under Hideki Tojo presented an interim proposal as its final offer. It called for the end of American aid to China and for the supply of oil and other resources to Japan. In exchange Japan promised not to launch any attacks in Southeast Asia and to withdraw its forces from southern Indochina.[152] The American counter-proposal of 26 November required that Japan evacuate all of China without conditions and conclude non-aggression pacts with all Pacific powers.[154] That meant Japan was essentially forced to choose between abandoning its ambitions in China, or seizing the natural resources it needed in the Dutch East Indies by force;[155] the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war.[156]		Japan planned to rapidly seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific; the Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war.[157] To prevent American intervention while securing the perimeter it was further planned to neutralise the United States Pacific Fleet and the American military presence in the Philippines from the outset.[158] On 7 December 1941 (8 December in Asian time zones), Japan attacked British and American holdings with near-simultaneous offensives against Southeast Asia and the Central Pacific.[159] These included an attack on the American fleet at Pearl Harbor, the Philippines, landings in Thailand and Malaya[159] and the battle of Hong Kong.		These attacks led the United States, Britain, China, Australia and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large-scale hostilities with European Axis countries, maintained its neutrality agreement with Japan.[160] Germany, followed by the other Axis states, declared war on the United States[161] in solidarity with Japan, citing as justification the American attacks on German war vessels that had been ordered by Roosevelt.[119][162]		On 1 January 1942, the Allied Big Four[163]—the Soviet Union, China, Britain and the United States—and 22 smaller or exiled governments issued the Declaration by United Nations, thereby affirming the Atlantic Charter,[164] and agreeing to not to sign a separate peace with the Axis powers.		During 1942, Allied officials debated on the appropriate grand strategy to pursue. All agreed that defeating Germany was the primary objective. The Americans favoured a straightforward, large-scale attack on Germany through France. The Soviets were also demanding a second front. The British, on the other hand, argued that military operations should target peripheral areas to wear out German strength, leading to increasing demoralisation, and bolster resistance forces. Germany itself would be subject to a heavy bombing campaign. An offensive against Germany would then be launched primarily by Allied armour without using large-scale armies.[165] Eventually, the British persuaded the Americans that a landing in France was infeasible in 1942 and they should instead focus on driving the Axis out of North Africa.[166]		At the Casablanca Conference in early 1943, the Allies reiterated the statements issued in the 1942 Declaration by the United Nations, and demanded the unconditional surrender of their enemies. The British and Americans agreed to continue to press the initiative in the Mediterranean by invading Sicily to fully secure the Mediterranean supply routes.[167] Although the British argued for further operations in the Balkans to bring Turkey into the war, in May 1943, the Americans extracted a British commitment to limit Allied operations in the Mediterranean to an invasion of the Italian mainland and to invade France in 1944.[168]		By the end of April 1942, Japan and its ally Thailand had almost fully conquered Burma, Malaya, the Dutch East Indies, Singapore, and Rabaul, inflicting severe losses on Allied troops and taking a large number of prisoners.[169] Despite stubborn resistance by Filipino and US forces, the Philippine Commonwealth was eventually captured in May 1942, forcing its government into exile.[170] On 16 April, in Burma, 7,000 British soldiers were encircled by the Japanese 33rd Division during the Battle of Yenangyaung and rescued by the Chinese 38th Division.[171] Japanese forces also achieved naval victories in the South China Sea, Java Sea and Indian Ocean,[172] and bombed the Allied naval base at Darwin, Australia. In January 1942, the only Allied success against Japan was a Chinese victory at Changsha.[173] These easy victories over unprepared US and European opponents left Japan overconfident, as well as overextended.[174]		In early May 1942, Japan initiated operations to capture Port Moresby by amphibious assault and thus sever communications and supply lines between the United States and Australia. The planned invasion was thwarted when an Allied task force, centred on two American fleet carriers, fought Japanese naval forces to a draw in the Battle of the Coral Sea.[175] Japan's next plan, motivated by the earlier Doolittle Raid, was to seize Midway Atoll and lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces to occupy the Aleutian Islands in Alaska.[176] In mid-May, Japan started the Zhejiang-Jiangxi Campaign in China, with the goal of inflicting retribution on the Chinese who aided the surviving American airmen in the Doolittle Raid by destroying air bases and fighting against the Chinese 23rd and 32nd Army Groups.[177][178] In early June, Japan put its operations into action but the Americans, having broken Japanese naval codes in late May, were fully aware of plans and order of battle, and used this knowledge to achieve a decisive victory at Midway over the Imperial Japanese Navy.[179]		With its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan chose to focus on a belated attempt to capture Port Moresby by an overland campaign in the Territory of Papua.[180] The Americans planned a counter-attack against Japanese positions in the southern Solomon Islands, primarily Guadalcanal, as a first step towards capturing Rabaul, the main Japanese base in Southeast Asia.[181]		Both plans started in July, but by mid-September, the Battle for Guadalcanal took priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to the northern part of the island, where they faced Australian and United States troops in the Battle of Buna-Gona.[182] Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal. By the start of 1943, the Japanese were defeated on the island and withdrew their troops.[183] In Burma, Commonwealth forces mounted two operations. The first, an offensive into the Arakan region in late 1942, went disastrously, forcing a retreat back to India by May 1943.[184] The second was the insertion of irregular forces behind Japanese front-lines in February which, by the end of April, had achieved mixed results.[185]		Despite considerable losses, in early 1942 Germany and its allies stopped a major Soviet offensive in central and southern Russia, keeping most territorial gains they had achieved during the previous year.[186] In May the Germans defeated Soviet offensives in the Kerch Peninsula and at Kharkov,[187] and then launched their main summer offensive against southern Russia in June 1942, to seize the oil fields of the Caucasus and occupy Kuban steppe, while maintaining positions on the northern and central areas of the front. The Germans split Army Group South into two groups: Army Group A advanced to the lower Don River and struck south-east to the Caucasus, while Army Group B headed towards the Volga River. The Soviets decided to make their stand at Stalingrad on the Volga.[188]		By mid-November, the Germans had nearly taken Stalingrad in bitter street fighting when the Soviets began their second winter counter-offensive, starting with an encirclement of German forces at Stalingrad[189] and an assault on the Rzhev salient near Moscow, though the latter failed disastrously.[190] By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been forced to surrender,[191] and the front-line had been pushed back beyond its position before the summer offensive. In mid-February, after the Soviet push had tapered off, the Germans launched another attack on Kharkov, creating a salient in their front line around the Russian city of Kursk.[192]		Exploiting poor American naval command decisions, the German navy ravaged Allied shipping off the American Atlantic coast.[193] By November 1941, Commonwealth forces had launched a counter-offensive, Operation Crusader, in North Africa, and reclaimed all the gains the Germans and Italians had made.[194] In North Africa, the Germans launched an offensive in January, pushing the British back to positions at the Gazala Line by early February,[195] followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives.[196] Concerns the Japanese might use bases in Vichy-held Madagascar caused the British to invade the island in early May 1942.[197] An Axis offensive in Libya forced an Allied retreat deep inside Egypt until Axis forces were stopped at El Alamein.[198] On the Continent, raids of Allied commandos on strategic targets, culminating in the disastrous Dieppe Raid,[199] demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security.[200][page needed]		In August 1942, the Allies succeeded in repelling a second attack against El Alamein[201] and, at a high cost, managed to deliver desperately needed supplies to the besieged Malta.[202] A few months later, the Allies commenced an attack of their own in Egypt, dislodging the Axis forces and beginning a drive west across Libya.[203] This attack was followed up shortly after by Anglo-American landings in French North Africa, which resulted in the region joining the Allies.[204] Hitler responded to the French colony's defection by ordering the occupation of Vichy France;[204] although Vichy forces did not resist this violation of the armistice, they managed to scuttle their fleet to prevent its capture by German forces.[204][205] The now pincered Axis forces in Africa withdrew into Tunisia, which was conquered by the Allies in May 1943.[204][206]		In early 1943 the British and Americans began the Combined Bomber Offensive, a strategic bombing campaign against Germany. The goals were to disrupt the German war economy, reduce German morale, and "de-house" the civilian population.[207]		After the Guadalcanal Campaign, the Allies initiated several operations against Japan in the Pacific. In May 1943, Canadian and U.S. forces were sent to eliminate Japanese forces from the Aleutians.[208] Soon after, the U.S., with support from Australian and New Zealand forces, began major operations to isolate Rabaul by capturing surrounding islands, and breach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands.[209] By the end of March 1944, the Allies had completed both of these objectives, and had also neutralised the major Japanese base at Truk in the Caroline Islands. In April, the Allies launched an operation to retake Western New Guinea.[210] In the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 preparing for large offensives in central Russia. On 4 July 1943, Germany attacked Soviet forces around the Kursk Bulge. Within a week, German forces had exhausted themselves against the Soviets' deeply echeloned and well-constructed defences[211] and, for the first time in the war, Hitler cancelled the operation before it had achieved tactical or operational success.[212] This decision was partially affected by the Western Allies' invasion of Sicily launched on 9 July which, combined with previous Italian failures, resulted in the ousting and arrest of Mussolini later that month.[213] Also, in July 1943 the British firebombed Hamburg killing over 40,000 people.[214]		On 12 July 1943, the Soviets launched their own counter-offensives, thereby dispelling any chance of German victory or even stalemate in the east. The Soviet victory at Kursk marked the end of German superiority,[215] giving the Soviet Union the initiative on the Eastern Front.[216][217] The Germans tried to stabilise their eastern front along the hastily fortified Panther–Wotan line, but the Soviets broke through it at Smolensk and by the Lower Dnieper Offensives.[218]		On 3 September 1943, the Western Allies invaded the Italian mainland, following Italy's armistice with the Allies.[219] Germany responded by disarming Italian forces, seizing military control of Italian areas,[220] and creating a series of defensive lines.[221] German special forces then rescued Mussolini, who then soon established a new client state in German-occupied Italy named the Italian Social Republic,[222] causing an Italian civil war. The Western Allies fought through several lines until reaching the main German defensive line in mid-November.[223]		German operations in the Atlantic also suffered. By May 1943, as Allied counter-measures became increasingly effective, the resulting sizeable German submarine losses forced a temporary halt of the German Atlantic naval campaign.[224] In November 1943, Franklin D. Roosevelt and Winston Churchill met with Chiang Kai-shek in Cairo and then with Joseph Stalin in Tehran.[225] The former conference determined the post-war return of Japanese territory[226] and the military planning for the Burma Campaign,[227] while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat.[228]		From November 1943, during the seven-week Battle of Changde, the Chinese forced Japan to fight a costly war of attrition, while awaiting Allied relief.[229][230][231] In January 1944, the Allies launched a series of attacks in Italy against the line at Monte Cassino and tried to outflank it with landings at Anzio.[232] By the end of January, a major Soviet offensive expelled German forces from the Leningrad region,[233] ending the longest and most lethal siege in history.		The following Soviet offensive was halted on the pre-war Estonian border by the German Army Group North aided by Estonians hoping to re-establish national independence. This delay slowed subsequent Soviet operations in the Baltic Sea region.[234] By late May 1944, the Soviets had liberated Crimea, largely expelled Axis forces from Ukraine, and made incursions into Romania, which were repulsed by the Axis troops.[235] The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, on 4 June, Rome was captured.[236]		The Allies had mixed success in mainland Asia. In March 1944, the Japanese launched the first of two invasions, an operation against British positions in Assam, India,[237] and soon besieged Commonwealth positions at Imphal and Kohima.[238] In May 1944, British forces mounted a counter-offensive that drove Japanese troops back to Burma,[238] and Chinese forces that had invaded northern Burma in late 1943 besieged Japanese troops in Myitkyina.[239] The second Japanese invasion of China aimed to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields.[240] By June, the Japanese had conquered the province of Henan and begun a new attack on Changsha in the Hunan province.[241]		On 6 June 1944 (known as D-Day), after three years of Soviet pressure,[242] the Western Allies invaded northern France. After reassigning several Allied divisions from Italy, they also attacked southern France.[243] These landings were successful, and led to the defeat of the German Army units in France. Paris was liberated by the local resistance assisted by the Free French Forces, both led by General Charles de Gaulle, on 25 August[244] and the Western Allies continued to push back German forces in western Europe during the latter part of the year. An attempt to advance into northern Germany spearheaded by a major airborne operation in the Netherlands failed.[245] After that, the Western Allies slowly pushed into Germany, but failed to cross the Ruhr river in a large offensive. In Italy, Allied advance also slowed due to the last major German defensive line.[246]		On 22 June, the Soviets launched a strategic offensive in Belarus ("Operation Bagration") that destroyed the German Army Group Centre almost completely.[247] Soon after that another Soviet strategic offensive forced German troops from Western Ukraine and Eastern Poland. The Soviet advance prompted resistance forces in Poland to initiate several uprisings against the German occupation. However, the largest of these in Warsaw, where German soldiers massacred 200,000 civilians, and a national uprising in Slovakia, did not receive Soviet support and were subsequently suppressed by the Germans.[248] The Red Army's strategic offensive in eastern Romania cut off and destroyed the considerable German troops there and triggered a successful coup d'état in Romania and in Bulgaria, followed by those countries' shift to the Allied side.[249]		In September 1944, Soviet troops advanced into Yugoslavia and forced the rapid withdrawal of German Army Groups E and F in Greece, Albania and Yugoslavia to rescue them from being cut off.[250] By this point, the Communist-led Partisans under Marshal Josip Broz Tito, who had led an increasingly successful guerrilla campaign against the occupation since 1941, controlled much of the territory of Yugoslavia and engaged in delaying efforts against German forces further south. In northern Serbia, the Red Army, with limited support from Bulgarian forces, assisted the Partisans in a joint liberation of the capital city of Belgrade on 20 October. A few days later, the Soviets launched a massive assault against German-occupied Hungary that lasted until the fall of Budapest in February 1945.[251] Unlike impressive Soviet victories in the Balkans, bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus denied the Soviets occupation of Finland and led to a Soviet-Finnish armistice on relatively mild conditions,[252][253] although Finland was forced to fight their former allies.		By the start of July 1944, Commonwealth forces in Southeast Asia had repelled the Japanese sieges in Assam, pushing the Japanese back to the Chindwin River[254] while the Chinese captured Myitkyina. In September 1944, Chinese force captured the Mount Song to reopen the Burma Road.[255] In China, the Japanese had more successes, having finally captured Changsha in mid-June and the city of Hengyang by early August.[256] Soon after, they invaded the province of Guangxi, winning major engagements against Chinese forces at Guilin and Liuzhou by the end of November[257] and successfully linking up their forces in China and Indochina by mid-December.[258]		In the Pacific, US forces continued to press back the Japanese perimeter. In mid-June 1944, they began their offensive against the Mariana and Palau islands, and decisively defeated Japanese forces in the Battle of the Philippine Sea. These defeats led to the resignation of the Japanese Prime Minister, Hideki Tojo, and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands. In late October, American forces invaded the Filipino island of Leyte; soon after, Allied naval forces scored another large victory in the Battle of Leyte Gulf, one of the largest naval battles in history.[259]		On 16 December 1944, Germany made a last attempt on the Western Front by using most of its remaining reserves to launch a massive counter-offensive in the Ardennes to split the Western Allies, encircle large portions of Western Allied troops and capture their primary supply port at Antwerp to prompt a political settlement.[260] By January, the offensive had been repulsed with no strategic objectives fulfilled.[260] In Italy, the Western Allies remained stalemated at the German defensive line. In mid-January 1945, the Soviets and Poles attacked in Poland, pushing from the Vistula to the Oder river in Germany, and overran East Prussia.[261] On 4 February, Soviet, British and US leaders met for the Yalta Conference. They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan.[262]		In February, the Soviets entered Silesia and Pomerania, while Western Allies entered western Germany and closed to the Rhine river. By March, the Western Allies crossed the Rhine north and south of the Ruhr, encircling the German Army Group B,[263] while the Soviets advanced to Vienna. In early April, the Western Allies finally pushed forward in Italy and swept across western Germany, while Soviet and Polish forces stormed Berlin in late April. American and Soviet forces met at the Elbe river on 25 April. On 30 April 1945, the Reichstag was captured, signalling the military defeat of Nazi Germany.[264]		Several changes in leadership occurred during this period. On 12 April, President Roosevelt died and was succeeded by Harry S. Truman. Benito Mussolini was killed by Italian partisans on 28 April.[265] Two days later, Hitler committed suicide, and was succeeded by Grand Admiral Karl Dönitz.[266]		German forces surrendered in Italy on 29 April. Total and unconditional surrender was signed on 7 May, to be effective by the end of 8 May.[267] German Army Group Centre resisted in Prague until 11 May.[268]		In the Pacific theatre, American forces accompanied by the forces of the Philippine Commonwealth advanced in the Philippines, clearing Leyte by the end of April 1945. They landed on Luzon in January 1945 and recaptured Manila in March following a battle which reduced the city to ruins. Fighting continued on Luzon, Mindanao, and other islands of the Philippines until the end of the war.[269] Meanwhile, the United States Army Air Forces (USAAF) were destroying strategic and populated cities and towns in Japan in an effort to destroy Japanese war industry and civilian morale. On the night of 9–10 March, USAAF B-29 bombers struck Tokyo with thousands of incendiary bombs, which killed 100,000 civilians and destroyed 16 square miles (41 km2) within a few hours. Over the next five months, the USAAF firebombed a total of 67 Japanese cities, killing 393,000 civilians and destroying 65% of built-up areas.[270]		In May 1945, Australian troops landed in Borneo, overrunning the oilfields there. British, American, and Chinese forces defeated the Japanese in northern Burma in March, and the British pushed on to reach Rangoon by 3 May.[271] Chinese forces started to counterattack in Battle of West Hunan that occurred between 6 April and 7 June 1945. American naval and amphibious forces also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of June.[272] At the same time, American submarines cut off Japanese imports, drastically reducing Japan's ability to supply its overseas forces.[273]		On 11 July, Allied leaders met in Potsdam, Germany. They confirmed earlier agreements about Germany,[274] and reiterated the demand for unconditional surrender of all Japanese forces by Japan, specifically stating that "the alternative for Japan is prompt and utter destruction".[275] During this conference, the United Kingdom held its general election, and Clement Attlee replaced Churchill as Prime Minister.[276]		The Allies called for unconditional Japanese surrender in the Potsdam Declaration of 27 July, but the Japanese government rejected the call. In early August, the USAAF dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki. The Allies justified the atomic bombings as a military necessity to avoid invading the Japanese home islands which would cost the lives of between 250,000 and 500,000 Allied servicemen and millions of Japanese troops and civilians.[277] Between the two bombings, the Soviets, pursuant to the Yalta agreement, invaded Japanese-held Manchuria, and quickly defeated the Kwantung Army, which was the largest Japanese fighting force.[278][279] The Red Army also captured Sakhalin Island and the Kuril Islands. On 15 August 1945, Japan surrendered, with the surrender documents finally signed at Tokyo Bay on the deck of the American battleship USS Missouri on 2 September 1945, ending the war.[280]		The Allies established occupation administrations in Austria and Germany. The former became a neutral state, non-aligned with any political bloc. The latter was divided into western and eastern occupation zones controlled by the Western Allies and the USSR, accordingly. A denazification programme in Germany led to the prosecution of Nazi war criminals and the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society.[281]		Germany lost a quarter of its pre-war (1937) territory. Among the eastern territories, Silesia, Neumark and most of Pomerania were taken over by Poland, East Prussia was divided between Poland and the USSR, followed by the expulsion of the 9 million Germans from these provinces, as well as the expulsion of 3 million Germans from the Sudetenland in Czechoslovakia to Germany. By the 1950s, every fifth West German was a refugee from the east. The Soviet Union also took over the Polish provinces east of the Curzon line, from which 2 million Poles were expelled;[282] north-east Romania,[283][284] parts of eastern Finland,[285] and the three Baltic states were also incorporated into the USSR.[286][287]		In an effort to maintain world peace,[288] the Allies formed the United Nations, which officially came into existence on 24 October 1945,[289] and adopted the Universal Declaration of Human Rights in 1948, as a common standard for all member nations.[290] The great powers that were the victors of the war—France, China, Britain, the Soviet Union and the United States—became the permanent members of the UN's Security Council.[6] The five permanent members remain so to the present, although there have been two seat changes, between the Republic of China and the People's Republic of China in 1971, and between the Soviet Union and its successor state, the Russian Federation, following the dissolution of the Soviet Union in 1991. The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over.[291]		Germany had been de facto divided, and two independent states, the Federal Republic of Germany and the German Democratic Republic[292] were created within the borders of Allied and Soviet occupation zones, accordingly. The rest of Europe was also divided into Western and Soviet spheres of influence.[293] Most eastern and central European countries fell into the Soviet sphere, which led to establishment of Communist-led regimes, with full or partial support of the Soviet occupation authorities. As a result, Poland, Hungary, East Germany,[294] Czechoslovakia, Romania, and Albania[295] became Soviet satellite states. Communist Yugoslavia conducted a fully independent policy, causing tension with the USSR.[296]		Post-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact;[297] the long period of political tensions and military competition between them, the Cold War, would be accompanied by an unprecedented arms race and proxy wars.[298]		In Asia, the United States led the occupation of Japan and administrated Japan's former islands in the Western Pacific, while the Soviets annexed Sakhalin and the Kuril Islands.[299] Korea, formerly under Japanese rule, was divided and occupied by the Soviet Union in the North and the US in the South between 1945 and 1948. Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to the Korean War.[300]		In China, nationalist and communist forces resumed the civil war in June 1946. Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated to Taiwan in 1949.[301] In the Middle East, the Arab rejection of the United Nations Partition Plan for Palestine and the creation of Israel marked the escalation of the Arab–Israeli conflict. While European powers attempted to retain some or all of their colonial empires, their losses of prestige and resources during the war rendered this unsuccessful, leading to decolonisation.[302][303]		The global economy suffered heavily from the war, although participating nations were affected differently. The US emerged much richer than any other nation; it had a baby boom and by 1950 its gross domestic product per person was much higher than that of any of the other powers and it dominated the world economy.[304] The UK and US pursued a policy of industrial disarmament in Western Germany in the years 1945–1948.[305] Because of international trade interdependencies this led to European economic stagnation and delayed European recovery for several years.[306][307]		Recovery began with the mid-1948 currency reform in Western Germany, and was sped up by the liberalisation of European economic policy that the Marshall Plan (1948–1951) both directly and indirectly caused.[308][309] The post-1948 West German recovery has been called the German economic miracle.[310] Italy also experienced an economic boom[311] and the French economy rebounded.[312] By contrast, the United Kingdom was in a state of economic ruin,[313] and although it received a quarter of the total Marshall Plan assistance, more than any other European country,[314] continued relative economic decline for decades.[315]		The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era.[316] Japan experienced incredibly rapid economic growth, becoming one of the most powerful economies in the world by the 1980s.[317] China returned to its pre-war industrial production by 1952.[318]		Estimates for the total number of casualties in the war vary, because many deaths went unrecorded. Most suggest that some 60 million people died in the war, including about 20 million military personnel and 40 million civilians.[319][320][321] Many of the civilians died because of deliberate genocide, massacres, mass-bombings, disease, and starvation.		The Soviet Union lost around 27 million people during the war,[322] including 8.7 million military and 19 million civilian deaths. The largest portion of military dead were 5.7 million ethnic Russians, followed by 1.3 million ethnic Ukrainians.[323] A quarter of the people in the Soviet Union were wounded or killed.[324] Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany.[325]		Of the total number of deaths in World War II, approximately 85 per cent—mostly Soviet and Chinese—were on the Allied side and 15 per cent were on the Axis side. Many of these deaths were caused by war crimes committed by German and Japanese forces in occupied territories. An estimated 11[326] to 17 million[327] civilians died either as a direct or as an indirect result of Nazi ideological policies, including the systematic genocide of around 6 million Jews during the Holocaust, along with a further 5 to 6 million ethnic Poles and other Slavs (including Ukrainians and Belarusians)[328]—Roma, homosexuals, and other ethnic and minority groups.[327] Hundreds of thousands (varying estimates) of ethnic Serbs, along with gypsies and Jews, were murdered by the Axis-aligned Croatian Ustaše in Yugoslavia,[329] and retribution-related killings were committed just after the war ended.		In Asia and the Pacific, between 3 million and more than 10 million civilians, mostly Chinese (estimated at 7.5 million[330]), were killed by the Japanese occupation forces.[331] The best-known Japanese atrocity was the Nanking Massacre, in which fifty to three hundred thousand Chinese civilians were raped and murdered.[332] Mitsuyoshi Himeta reported that 2.7 million casualties occurred during the Sankō Sakusen. General Yasuji Okamura implemented the policy in Heipei and Shantung.[333]		Axis forces employed biological and chemical weapons. The Imperial Japanese Army used a variety of such weapons during its invasion and occupation of China (see Unit 731)[334][335] and in early conflicts against the Soviets.[336] Both the Germans and Japanese tested such weapons against civilians[337] and, sometimes on prisoners of war.[338]		The Soviet Union was responsible for the Katyn massacre of 22,000 Polish officers,[339] and the imprisonment or execution of thousands of political prisoners by the NKVD,[340] in the Baltic states, and eastern Poland annexed by the Red Army.		The mass-bombing of cities in Europe and Asia has often been called a war crime. However, no positive or specific customary international humanitarian law with respect to aerial warfare existed before or during World War II.[341]		The German government led by Adolf Hitler and the Nazi Party was responsible for the Holocaust, the killing of approximately 6 million Jews, as well as 2.7 million ethnic Poles,[342] and 4 million others who were deemed "unworthy of life" (including the disabled and mentally ill, Soviet prisoners of war, homosexuals, Freemasons, Jehovah's Witnesses, and Romani) as part of a programme of deliberate extermination. About 12 million, most of whom were Eastern Europeans, were employed in the German war economy as forced labourers.[343]		In addition to Nazi concentration camps, the Soviet gulags (labour camps) led to the death of citizens of occupied countries such as Poland, Lithuania, Latvia, and Estonia, as well as German prisoners of war (POWs) and even Soviet citizens who had been or were thought to be supporters of the Nazis.[344] Sixty per cent of Soviet POWs of the Germans died during the war.[345] Richard Overy gives the number of 5.7 million Soviet POWs. Of those, 57 per cent died or were killed, a total of 3.6 million.[346] Soviet ex-POWs and repatriated civilians were treated with great suspicion as potential Nazi collaborators, and some of them were sent to the Gulag upon being checked by the NKVD.[347]		Japanese prisoner-of-war camps, many of which were used as labour camps, also had high death rates. The International Military Tribunal for the Far East found the death rate of Western prisoners was 27.1 per cent (for American POWs, 37 per cent),[348] seven times that of POWs under the Germans and Italians.[349] While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from the United States were released after the surrender of Japan, the number of Chinese released was only 56.[350]		According to historian Zhifen Ju, at least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by the East Asia Development Board, or Kōain, for work in mines and war industries. After 1942, the number reached 10 million.[351] The US Library of Congress estimates that in Java, between 4 and 10 million rōmusha (Japanese: "manual labourers"), were forced to work by the Japanese military. About 270,000 of these Javanese labourers were sent to other Japanese-held areas in South East Asia, and only 52,000 were repatriated to Java.[352]		On 19 February 1942, Roosevelt signed Executive Order 9066, interning about 100,000 Japanese living on the West Coast. Canada had a similar programme.[353][354] In addition, 14,000 German and Italian citizens who had been assessed as being security risks were also interned.[355]		In accordance with the Allied agreement made at the Yalta Conference millions of POWs and civilians were used as forced labour by the Soviet Union.[356] In Hungary's case, Hungarians were forced to work for the Soviet Union until 1955.[357]		In Europe, occupation came under two forms. In Western, Northern, and Central Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of Czechoslovakia) Germany established economic policies through which it collected roughly 69.5 billion reichmarks (27.8 billion US Dollars) by the end of the war, this figure does not include the sizeable plunder of industrial products, military equipment, raw materials and other goods.[358] Thus, the income from occupied nations was over 40 per cent of the income Germany collected from taxation, a figure which increased to nearly 40 per cent of total German income as the war went on.[359]		In the East, the intended gains of Lebensraum were never attained as fluctuating front-lines and Soviet scorched earth policies denied resources to the German invaders.[360] Unlike in the West, the Nazi racial policy encouraged extreme brutality against what it considered to be the "inferior people" of Slavic descent; most German advances were thus followed by mass executions.[361] Although resistance groups formed in most occupied territories, they did not significantly hamper German operations in either the East[362] or the West[363] until late 1943.		In Asia, Japan termed nations under its occupation as being part of the Greater East Asia Co-Prosperity Sphere, essentially a Japanese hegemony which it claimed was for purposes of liberating colonised peoples.[364] Although Japanese forces were originally welcomed as liberators from European domination in some territories, their excessive brutality turned local public opinion against them within weeks.[365] During Japan's initial conquest it captured 4,000,000 barrels (640,000 m3) of oil (~5.5×105 tonnes) left behind by retreating Allied forces, and by 1943 was able to get production in the Dutch East Indies up to 50 million barrels (~6.8×10^6 t), 76 per cent of its 1940 output rate.[365]		In Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics. In 1938, the Western Allies (United Kingdom, France, Poland and British Dominions) had a 30 per cent larger population and a 30 per cent higher gross domestic product than the European Axis powers (Germany and Italy); if colonies are included, it then gives the Allies more than a 5:1 advantage in population and nearly 2:1 advantage in GDP.[366] In Asia at the same time, China had roughly six times the population of Japan, but only an 89 per cent higher GDP; this is reduced to three times the population and only a 38 per cent higher GDP if Japanese colonies are included.[366]		Though the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies, as the war largely settled into one of attrition.[367] While the Allies' ability to out-produce the Axis is often attributed to the Allies having more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in the labour force,[368] Allied strategic bombing,[369] and Germany's late shift to a war economy[370] contributed significantly. Additionally, neither Germany nor Japan planned to fight a protracted war, and were not equipped to do so.[371] To improve their production, Germany and Japan used millions of slave labourers;[372] Germany used about 12 million people, mostly from Eastern Europe,[343] while Japan used more than 18 million people in Far East Asia.[351][352]		Aircraft were used for reconnaissance, as fighters, bombers, and ground-support, and each role was advanced considerably. Innovation included airlift (the capability to quickly move limited high-priority supplies, equipment, and personnel);[373] and of strategic bombing (the bombing of enemy industrial and population centres to destroy the enemy's ability to wage war).[374] Anti-aircraft weaponry also advanced, including defences such as radar and surface-to-air artillery, such as the German 88 mm gun. The use of the jet aircraft was pioneered and, though late introduction meant it had little impact, it led to jets becoming standard in air forces worldwide.[375]		Advances were made in nearly every aspect of naval warfare, most notably with aircraft carriers and submarines. Although aeronautical warfare had relatively little success at the start of the war, actions at Taranto, Pearl Harbor, and the Coral Sea established the carrier as the dominant capital ship in place of the battleship.[376][377][378]		In the Atlantic, escort carriers proved to be a vital part of Allied convoys, increasing the effective protection radius and helping to close the Mid-Atlantic gap.[379] Carriers were also more economical than battleships because of the relatively low cost of aircraft[380] and their not requiring to be as heavily armoured.[381] Submarines, which had proved to be an effective weapon during the First World War,[382] were anticipated by all sides to be important in the second. The British focused development on anti-submarine weaponry and tactics, such as sonar and convoys, while Germany focused on improving its offensive capability, with designs such as the Type VII submarine and wolfpack tactics.[383] Gradually, improving Allied technologies such as the Leigh light, hedgehog, squid, and homing torpedoes proved victorious.		Land warfare changed from the static front lines of World War I to increased mobility and combined arms. The tank, which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon.[384] In the late 1930s, tank design was considerably more advanced than it had been during World War I,[385] and advances continued throughout the war with increases in speed, armour and firepower.		At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications.[386] This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat. This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France.[384] Many means of destroying tanks, including indirect artillery, anti-tank guns (both towed and self-propelled), mines, short-ranged infantry antitank weapons, and other tanks were utilised.[386] Even with large-scale mechanisation, infantry remained the backbone of all forces,[387] and throughout the war, most infantry were equipped similarly to World War I.[388]		The portable machine gun spread, a notable example being the German MG34, and various submachine guns which were suited to close combat in urban and jungle settings.[388] The assault rifle, a late war development incorporating many features of the rifle and submachine gun, became the standard postwar infantry weapon for most armed forces.[389][390]		Most major belligerents attempted to solve the problems of complexity and security involved in using large codebooks for cryptography by designing ciphering machines, the most well known being the German Enigma machine.[391] Development of SIGINT (signals intelligence) and cryptanalysis enabled the countering process of decryption. Notable examples were the Allied decryption of Japanese naval codes[392] and British Ultra, a pioneering method for decoding Enigma benefiting from information given to Britain by the Polish Cipher Bureau, which had been decoding early versions of Enigma before the war.[393] Another aspect of military intelligence was the use of deception, which the Allies used to great effect, such as in operations Mincemeat and Bodyguard.[392][394] Other technological and engineering feats achieved during, or as a result of, the war include the world's first programmable computers (Z3, Colossus, and ENIAC), guided missiles and modern rockets, the Manhattan Project's development of nuclear weapons, operations research and the development of artificial harbours and oil pipelines under the English Channel.		Gilbert, Martin (2001). "Final Solution". In Dear, Ian; Foot, Richard D. The Oxford Companion to World War II. Oxford, UK: Oxford University Press. pp. 285–292. ISBN 0-19-280670-X. 		
The Army Physical Fitness Test (APFT) is designed to test the muscular strength, endurance, and cardiovascular respiratory fitness of soldiers in the Army. Soldiers are scored based on their performance in three events consisting of the push-up, sit-up, and a two-mile run, ranging from 0 to 100 points in each event. A minimum score of 60 in each event is required to pass the test. The soldier's overall score is the sum of the points from the three events. If a soldier passes all three events, the total may range from 180 to 300.		Active component and reserve component soldiers on active duty are required to take a "record" (meaning for official records) APFT at least twice each calendar year. Reservists not on active duty must take a "record" test once per calendar year.[1] FM 7-22 covers the administration of the APFT, as well as ways to conduct individual, squad and unit level physical training sessions		If, due to a diagnosed medical condition, a soldier is temporarily unable to conduct one or more of the events in the record APFT, the soldier can be granted an extension to allow him or her to overcome his or her injury and return to an acceptable level of physical fitness. If a soldier has a permanent medical condition that keeps him or her from conducting the two mile run, an alternative aerobic event consisting of either a 2.5 mile walk, an 800-yard swim, or 6.2 mile cycle ride is taken. There are no alternate events for the push-up or sit-up.						The physical fitness assessments for the U.S. Army were first developed in 1858 at the United States Military Academy. Over the years, the athletics for soldiers have been revised repeatedly. According to a US Army abstract, the calisthenics and events of "push-ups, sit-ups, and a 2-mile run was introduced in 1980."[2]		The testing events are conducted in accordance with standards detailed in Army FM 7-22, Army Physical Readiness Training. Prior to the start of each event, the standard is read aloud, followed by a demonstration in which an individual demonstrates both the correct exercise and any disqualifying behaviors which would make the exercise incorrect.		Quoted from Army FM 7-22:		Scoring on the APFT is based on gender, age category, number of repetitions performed of the push-up and sit-up, and run time. Score tables are found in Army FM 7-22 and on Department of the Army Form 705, Army Physical Fitness Test Scorecard. The score for each event ranges from 0 to 100 points; a minimum score of 60 in each event is required to pass the test. The soldier's overall score is the sum of the points from the three events. If a soldier passes all three events, the total may range from 180 to 300. APFT standards may be more rigorous for some special purpose units, such as for special operations soldiers which are usually required to score 70 points or better in each event.		Soldiers who score 270 or above on the APFT, with a minimum score of 90 in each event, are awarded the Physical Fitness Badge, which can be worn on the physical training uniform of enlisted soldiers.[3] The APFT score also converts to promotion points which are used to in part to determine the eligibility of soldiers for promotion to a higher rank. Effective with the latest change to the Army's enlisted promotion doctrine, the number of promotion points awarded to Soldiers with a "promotable" status was changed.[4] Soldiers seeking promotion from Specialist (E-4) to Sergeant (E-5) can achieve a maximum of 180 promotion points whereas a Soldier seeking promotion from Sergeant (E-5) to Staff Sergeant (E-6) can only achieve a maximum of 145 promotion points.		Prior to May 2013, the scoring algorithm also included an extended scale, by which soldiers could earn more than 100 points in an event by performing better than the 100-point standard. However, use of such a scale is specifically forbidden in the current Army Field Manual.[5]		Under previous versions, in order for a soldier to earn a score of over 300, he or she must have obtained 100 points in each event, meaning that a soldier could not begin to use the extended scale for any one event until 100 points were reached in all three events. Scores above maximum could only be used locally in an unofficial capacity; official record scores could never exceed 300 points. Beyond the 100-point level, each additional push-up was one additional point, each additional sit-up was one additional point, and each six-second reduction in the run time was one additional point.		For soldiers who have a medical or physical condition which prevents them from being able to successfully participate in the two mile run, alternate aerobic events are authorized. Scoring for alternate aerobic events is either GO or NO-GO (pass or fail) and is based on the gender and age of the individual. For the purposes of promotion, a soldier's score on an alternate event equals the average of their push-up and sit-up scores.[6] Restrictions applicable to the APFT for soldiers with medical conditions (including pregnancy) or on physical profiles are stipulated in Army Regulation 40-501.[7] For example, one stipulation is that "Once the [temporary] profile is lifted, the soldier must be given twice the time of the profile (but not more than 90 days) to train for the [next] APFT.[8]		Failure to pass two or more consecutive record APFTs can lead to separation from the Army, although this is not always the case. Soldiers who have failed an APFT are often put into a "remedial program" first, which includes additional physical training. An APFT failure also results in the soldier being "flagged" which make them ineligible for promotion and attendance to military training and/or schools. However, a soldier cannot be denied an award or decoration due to an APFT failure.[9]		source: APFT Standards source: APFT Standards		In rare occurrences, soldiers have died during or right after the physical fitness test.[10][11] These deaths have been attributed to chronic or sudden health conditions and not due to the test itself.		In 2009, a male soldier died during the 2-mile run.[12]		In 2016, a soldier collapsed right after the 2-mile run; afterwards, he was pronounced dead.[13] He was later identified as Clayton Z. Hughes.[14]		For soldiers attending the first phase of Ranger School, a special Ranger Physical Fitness Test is conducted for all age groups, which is separate from the Army Physical Fitness Test. The test is pass/fail and involves push-ups, sit-ups, chin-ups, and a five-mile run. Push-ups and sit-ups are to be performed within 2 minutes.[15][16]		Canadian Armed Forces: https://www.cfmws.com/en/AboutUs/PSP/DFIT/Fitness/FORCEprogram/Pages/About-the-FORCE-Program.aspx		
Dopamine (DA, contracted from 3,4-dihydroxyphenethylamine) is an organic chemical of the catecholamine and phenethylamine families that plays several important roles in the brain and body. It is an amine synthesized by removing a carboxyl group from a molecule of its precursor chemical L-DOPA, which is synthesized in the brain and kidneys. Dopamine is also synthesized in plants and most animals.		In the brain, dopamine functions as a neurotransmitter—a chemical released by neurons (nerve cells) to send signals to other nerve cells. The brain includes several distinct dopamine pathways, one of which plays a major role in reward-motivated behavior. Most types of rewards increase the level of dopamine in the brain, and many addictive drugs increase dopamine neuronal activity. Other brain dopamine pathways are involved in motor control and in controlling the release of various hormones. These pathways and cell groups form a dopamine system which is neuromodulatory.		Outside the central nervous system, dopamine functions primarily as a local chemical messenger. In blood vessels, it inhibits norepinephrine release and acts as a vasodilator (at normal concentrations); in the kidneys, it increases sodium excretion and urine output; in the pancreas, it reduces insulin production; in the digestive system, it reduces gastrointestinal motility and protects intestinal mucosa; and in the immune system, it reduces the activity of lymphocytes. With the exception of the blood vessels, dopamine in each of these peripheral systems is synthesized locally and exerts its effects near the cells that release it.		Several important diseases of the nervous system are associated with dysfunctions of the dopamine system, and some of the key medications used to treat them work by altering the effects of dopamine. Parkinson's disease, a degenerative condition causing tremor and motor impairment, is caused by a loss of dopamine-secreting neurons in an area of the midbrain called the substantia nigra. Its metabolic precursor L-DOPA can be manufactured, and in its pure form marketed as Levodopa is the most widely used treatment for the condition. There is evidence that schizophrenia involves altered levels of dopamine activity, and most antipsychotic drugs used to treat this are dopamine antagonists which reduce dopamine activity.[2] Similar dopamine antagonist drugs are also some of the most effective anti-nausea agents. Restless legs syndrome and attention deficit hyperactivity disorder (ADHD) are associated with decreased dopamine activity.[3] Dopaminergic stimulants can be addictive in high doses, but some are used at lower doses to treat ADHD. Dopamine itself is available as a manufactured medication for intravenous injection: although it cannot reach the brain from the bloodstream, its peripheral effects make it useful in the treatment of heart failure or shock, especially in newborn babies.		A dopamine molecule consists of a catechol structure (a benzene ring with two hydroxyl side groups) with one amine group attached via an ethyl chain.[4] As such, dopamine is the simplest possible catecholamine, a family that also includes the neurotransmitters norepinephrine and epinephrine.[5] The presence of a benzene ring with this amine attachment makes it a substituted phenethylamine, a family that includes numerous psychoactive drugs.[6]		Like most amines, dopamine is an organic base.[7] As a base, it is generally protonated in acidic environments (in an acid-base reaction).[7] The protonated form is highly water-soluble and relatively stable, but can become oxidized if exposed to oxygen or other oxidants.[7] In basic environments, dopamine is not protonated.[7] In this free base form, it is less water-soluble and also more highly reactive.[7] Because of the increased stability and water-solubility of the protonated form, dopamine is supplied for chemical or pharmaceutical use as dopamine hydrochloride—that is, the hydrochloride salt that is created when dopamine is combined with hydrochloric acid.[7] In dry form, dopamine hydrochloride is a fine colorless powder.[7]		Dopamine is synthesized in a restricted set of cell types, mainly neurons and cells in the medulla of the adrenal glands.[11] The primary and minor metabolic pathways respectively are:		The direct precursor of dopamine, L-DOPA, can be synthesized indirectly from the essential amino acid phenylalanine or directly from the non-essential amino acid tyrosine.[14] These amino acids are found in nearly every protein and so are readily available in food, with tyrosine being the most common. Although dopamine is also found in many types of food, it is incapable of crossing the blood–brain barrier that surrounds and protects the brain.[15] It must therefore be synthesized inside the brain to perform its neuronal activity.[15]		L-Phenylalanine is converted into L-tyrosine by the enzyme phenylalanine hydroxylase, with molecular oxygen (O2) and tetrahydrobiopterin as cofactors. L-Tyrosine is converted into L-DOPA by the enzyme tyrosine hydroxylase, with tetrahydrobiopterin, O2, and iron (Fe2+) as cofactors.[14] L-DOPA is converted into dopamine by the enzyme aromatic L-amino acid decarboxylase (also known as DOPA decarboxylase), with pyridoxal phosphate as the cofactor.[14]		Dopamine itself is used as precursor in the synthesis of the neurotransmitters norepinephrine and epinephrine.[14] Dopamine is converted into norepinephrine by the enzyme dopamine β-hydroxylase, with O2 and L-ascorbic acid as cofactors.[14] Norepinephrine is converted into epinephrine by the enzyme phenylethanolamine N-methyltransferase with S-adenosyl-L-methionine as the cofactor.[14]		Some of the cofactors also require their own synthesis.[14] Deficiency in any required amino acid or cofactor can impair the synthesis of dopamine, norepinephrine, and epinephrine.[14]		Dopamine is broken down into inactive metabolites by a set of enzymes—monoamine oxidase (MAO), catechol-O-methyl transferase (COMT), and aldehyde dehydrogenase (ALDH), acting in sequence.[16] Both isoforms of monoamine oxidase, MAO-A and MAO-B, effectively metabolize dopamine.[14] Different breakdown pathways exist but the main end-product is homovanillic acid (HVA), which has no known biological activity.[16] From the bloodstream, homovanillic acid is filtered out by the kidneys and then excreted in the urine.[16] The two primary metabolic routes that convert dopamine into HVA are:		In clinical research on schizophrenia, measurements of homovanillic acid in plasma have been used to estimate levels of dopamine activity in the brain. A difficulty in this approach however, is separating the high level of plasma homovanillic acid contributed by the metabolism of norepinephrine.[17][18]		Although dopamine is normally broken down by an oxidoreductase enzyme, it is also susceptible to oxidation by direct reaction with oxygen, yielding quinones plus various free radicals as products.[19] The rate of oxidation can be increased by the presence of ferric iron or other factors. Quinones and free radicals produced by autoxidation of dopamine can poison cells, and there is evidence that this mechanism may contribute to the cell loss that occurs in Parkinson's disease and other conditions.[20]		Dopamine exerts its effects by binding to and activating cell surface receptors.[11] In humans, dopamine has a high binding affinity at dopamine receptors and trace amine-associated receptor 1 (TAAR1).[1][21] In mammals, five subtypes of dopamine receptors have been identified, labeled from D1 to D5.[11] All of them function as metabotropic, G protein-coupled receptors, meaning that they exert their effects via a complex second messenger system.[22] These receptors can be divided into two families, known as D1-like and D2-like.[11] For receptors located on neurons in the nervous system, the ultimate effect of D1-like activation (D1 and D5) can be excitation (via opening of sodium channels) or inhibition (via opening of potassium channels); the ultimate effect of D2-like activation (D2, D3, and D4) is usually inhibition of the target neuron.[22] Consequently, it is incorrect to describe dopamine itself as either excitatory or inhibitory: its effect on a target neuron depends on which types of receptors are present on the membrane of that neuron and on the internal responses of that neuron to the second messenger cAMP.[22] D1 receptors are the most numerous dopamine receptors in the human nervous system; D2 receptors are next; D3, D4, and D5 receptors are present at significantly lower levels.[22]		Inside the brain, dopamine functions as a neurotransmitter and neuromodulator, and is controlled by a set of mechanisms common to all monoamine neurotransmitters.[11] After synthesis, dopamine is transported from the cytosol into synaptic vesicles by a solute carrier—a vesicular monoamine transporter, VMAT2.[23] Dopamine is stored in these vesicles until it is ejected into the synaptic cleft. In most cases, the release of dopamine occurs through a process called exocytosis which is caused by action potentials, but it can also be caused by the activity of an intracellular trace amine-associated receptor, TAAR1.[21] TAAR1 is a high-affinity receptor for dopamine, trace amines, and certain substituted amphetamines that is located along membranes in the intracellular milieu of the presynaptic cell;[21] activation of the receptor can regulate dopamine signaling by inducing dopamine reuptake inhibition and efflux as well as by inhibiting neuronal firing through a diverse set of mechanisms.[21][24]		Once in the synapse, dopamine binds to and activates dopamine receptors.[25] These can be postsynaptic dopamine receptors, which are located on dendrites (the postsynaptic neuron), or presynaptic autoreceptors (e.g., the D2sh and presynaptic D3 receptors), which are located on the membrane of an axon terminal (the presynaptic neuron).[11][25] After the postsynaptic neuron elicits an action potential, dopamine molecules quickly become unbound from their receptors. They are then absorbed back into the presynaptic cell, via reuptake mediated either by the dopamine transporter or by the plasma membrane monoamine transporter.[26] Once back in the cytosol, dopamine can either be broken down by a monoamine oxidase or repackaged into vesicles by VMAT2, making it available for future release.[23]		In the brain the level of extracellular dopamine is modulated by two mechanisms: phasic and tonic transmission.[27] Phasic dopamine release, like most neurotransmitter release in the nervous system, is driven directly by action potentials in the dopamine-containing cells.[27] Tonic dopamine transmission occurs when small amounts of dopamine are released without being preceded by presynaptic action potentials.[27] Tonic transmission is regulated by a variety of factors, including the activity of other neurons and neurotransmitter reuptake.[27]		Inside the brain, dopamine plays important roles in executive functions, motor control, motivation, arousal, reinforcement, and reward, as well as lower-level functions including lactation, sexual gratification, and nausea. The dopaminergic cell groups and pathways make up the dopamine system which is neuromodulatory.		Dopaminergic neurons (dopamine-producing nerve cells) are comparatively few in number—a total of around 400,000 in the human brain[28]—and their cell bodies are confined in groups to a few relatively small brain areas.[29] However their axons project to many other brain areas, and they exert powerful effects on their targets.[29] These dopaminergic cell groups were first mapped in 1964 by Annica Dahlström and Kjell Fuxe, who assigned them labels starting with the letter "A" (for "aminergic").[30] In their scheme, areas A1 through A7 contain the neurotransmitter norepinephrine, whereas A8 through A14 contain dopamine. The dopaminergic areas they identified are the substantia nigra (groups 8 and 9); the ventral tegmental area (group 10); the posterior hypothalamus (group 11); the arcuate nucleus (group 12); the zona incerta (group 13) and the periventricular nucleus (group 14).[30]		The substantia nigra is a small midbrain area that forms a component of the basal ganglia. This has two parts—an input area called the pars compacta and an output area the pars reticulata. The dopaminergic neurons are found mainly in the pars compacta (cell group A8) and nearby (group A9).[29] In humans, the projection of dopaminergic neurons from the substantia nigra pars compacta to the dorsal striatum, termed the nigrostriatal pathway, plays a significant role in the control of motor function and in learning new motor skills.[31] These neurons are especially vulnerable to damage, and when a large number of them die, the result is a parkinsonian syndrome.[32]		The ventral tegmental area (VTA) is another midbrain area. The most prominent group of VTA dopaminergic neurons projects to the prefrontal cortex via the mesocortical pathway and another smaller group projects to the nucleus accumbens via the mesolimbic pathway. Together, these two pathways are collectively termed the mesocorticolimbic projection.[29][31] The VTA also sends dopaminergic projections to the amygdala, cingulate gyrus, hippocampus, and olfactory bulb.[29][31] Mesocorticolimbic neurons play a central role in reward and other aspects of motivation.[31]		The posterior hypothalamus has dopamine neurons that project to the spinal cord, but their function is not well established.[33] There is some evidence that pathology in this area plays a role in restless legs syndrome, a condition in which people have difficulty sleeping due to an overwhelming compulsion to constantly move parts of the body, especially the legs.[33]		The arcuate nucleus and the periventricular nucleus of the hypothalamus have dopamine neurons that form an important projection—the tuberoinfundibular pathway which goes to the pituitary gland, where it influences the secretion of the hormone prolactin.[34] Dopamine is the primary neuroendocrine inhibitor of the secretion of prolactin from the anterior pituitary gland.[34] Dopamine produced by neurons in the arcuate nucleus is secreted into the hypophyseal portal system of the median eminence, which supplies the pituitary gland.[34] The prolactin cells that produce prolactin, in the absence of dopamine, secrete prolactin continuously; dopamine inhibits this secretion.[34] In the context of regulating prolactin secretion, dopamine is occasionally called prolactin-inhibiting factor, prolactin-inhibiting hormone, or prolactostatin.[34]		The zona incerta, grouped between the arcuate and periventricular nuclei, projects to several areas of the hypothalamus, and participates in the control of gonadotropin-releasing hormone, which is necessary to activate the development of the male and female reproductive systems, following puberty.[34]		An additional group of dopamine-secreting neurons is found in the retina of the eye.[35] These neurons are amacrine cells, meaning that they have no axons.[35] They release dopamine into the extracellular medium, and are specifically active during daylight hours, becoming silent at night.[35] This retinal dopamine acts to enhance the activity of cone cells in the retina while suppressing rod cells—the result is to increase sensitivity to color and contrast during bright light conditions, at the cost of reduced sensitivity when the light is dim.[35]		The largest and most important sources of dopamine in the vertebrate brain are the substantia nigra and ventral tegmental area.[29] These structures are closely related to each other and functionally similar in many respects.[29] Both are components of the basal ganglia, a complex network of structures located mainly at the base of the forebrain.[29] The largest component of the basal ganglia is the striatum.[36] The substantia nigra sends a dopaminergic projection to the dorsal striatum, while the ventral tegmental area sends a similar type of dopaminergic projection to the ventral striatum.[29]		Progress in understanding the functions of the basal ganglia has been slow.[36] The most popular hypotheses, broadly stated, propose that the basal ganglia play a central role in action selection.[37] The action selection theory in its simplest form proposes that when a person or animal is in a situation where several behaviors are possible, activity in the basal ganglia determines which of them is executed, by releasing that response from inhibition while continuing to inhibit other motor systems that if activated would generate competing behaviors.[38] Thus the basal ganglia, in this concept, are responsible for initiating behaviors, but not for determining the details of how they are carried out. In other words, they essentially form a decision-making system.[38]		The basal ganglia can be divided into several sectors, and each is involved in controlling particular types of actions.[39] The ventral sector of the basal ganglia (containing the ventral striatum and ventral tegmental area) operates at the highest level of the hierarchy, selecting actions at the whole-organism level.[38] The dorsal sectors (containing the dorsal striatum and substantia nigra) operate at lower levels, selecting the specific muscles and movements that are used to implement a given behavior pattern.[39]		Dopamine contributes to the action selection process in at least two important ways. First, it sets the "threshold" for initiating actions.[37] The higher the level of dopamine activity, the lower the impetus required to evoke a given behavior.[37] As a consequence, high levels of dopamine lead to high levels of motor activity and impulsive behavior; low levels of dopamine lead to torpor and slowed reactions.[37] Parkinson's disease, in which dopamine levels in the substantia nigra circuit are greatly reduced, is characterized by stiffness and difficulty initiating movement—however, when people with the disease are confronted with strong stimuli such as a serious threat, their reactions can be as vigorous as those of a healthy person.[40] In the opposite direction, drugs that increase dopamine release, such as cocaine or amphetamine, can produce heightened levels of activity, including at the extreme, psychomotor agitation and stereotyped movements.[41]		The second important effect of dopamine is as a "teaching" signal.[37] When an action is followed by an increase in dopamine activity, the basal ganglia circuit is altered in a way that makes the same response easier to evoke when similar situations arise in the future.[37] This is a form of operant conditioning, in which dopamine plays the role of a reward signal.[38]		In the reward system, reward is the attractive and motivational property of a stimulus that induces appetitive behavior (also known as approach behavior) – and consummatory behavior.[42] A rewarding stimulus is one that has the potential to cause an approach to it and a choice to be made to consume it or not.[42] Pleasure, learning (e.g., classical and operant conditioning), and approach behavior are the three main functions of reward.[42] As an aspect of reward, pleasure provides a definition of reward;[42] however, while all pleasurable stimuli are rewarding, not all rewarding stimuli are pleasurable (e.g., extrinstic rewards like money).[42][43] The motivational or desirable aspect of rewarding stimuli is reflected by the approach behavior that they induce, whereas the pleasurable component of intrinstic rewards is derived from the consummatory behavior that ensues upon acquiring them.[42] A neuropsychological model which distinguishes these two components of an intrinsically rewarding stimulus is the incentive salience model, where "wanting" or desire (less commonly, "seeking"[44]) corresponds to appetitive or approach behavior while "liking" or pleasure corresponds to consummatory behavior.[42][45][46] In human drug addicts, "wanting" becomes dissociated with "liking" as the desire to use an addictive drug increases, while the pleasure obtained from consuming it decreases due to drug tolerance.[45]		Within the brain, dopamine functions partly as a "global reward signal", where an initial phasic dopamine response to a rewarding stimulus encodes information about the salience, value, and context of a reward.[42] In the context of reward-related learning, dopamine also functions as a reward prediction error signal, that is, the degree to which the value of a reward is unexpected.[42] According to this hypothesis of Wolfram Schultz, rewards that are expected do not produce a second phasic dopamine response in certain dopaminergic cells, but rewards that are unexpected, or greater than expected, produce a short-lasting increase in synaptic dopamine, whereas the omission of an expected reward actually causes dopamine release to drop below its background level.[42] The "prediction error" hypothesis has drawn particular interest from computational neuroscientists, because an influential computational-learning method known as temporal difference learning makes heavy use of a signal that encodes prediction error.[42] This confluence of theory and data has led to a fertile interaction between neuroscientists and computer scientists interested in machine learning.[42]		Evidence from microelectrode recordings from the brains of animals shows that dopamine neurons in the ventral tegmental area (VTA) and substantia nigra are strongly activated by a wide variety of rewarding events.[42] These reward-responsive dopamine neurons in the VTA and substantia nigra are crucial for reward-related cognition and serve as the central component of the reward system.[45][47][48] The function of dopamine varies in each axonal projection from the VTA and substantia nigra;[45] for example, the VTA–nucleus accumbens shell projection assigns incentive salience ("want") to rewarding stimuli and its associated cues, the VTA–orbitofrontal cortex projection updates the value of different goals in accordance with their incentive salience, the VTA–amygdala and VTA–hippocampus projections mediate the consolidation of reward-related memories, and both the VTA–nucleus accumbens core and substantia nigra–dorsal striatum pathways are involved in learning motor responses that facilitate the acquisition of rewarding stimuli.[45][49] Some activity within the VTA dopaminergic projections appears to be associated with reward prediction as well.[45][49]		While dopamine has a central role in mediating "wanting" — associated with the appetitive or approach behavioral responses to rewarding stimuli, detailed studies have shown that dopamine cannot simply be equated with hedonic "liking" or pleasure, as reflected in the consummatory behavioral response.[43] Dopamine neurotransmission is involved in some but not all aspects of pleasure-related cognition, since pleasure centers have been identified both within the dopamine system (i.e., nucleus accumbens shell) and outside the dopamine system (i.e., ventral pallidum and parabrachial nucleus).[43][46][50] For example, direct electrical stimulation of dopamine pathways, using electrodes implanted in the brain, is experienced as pleasurable, and many types of animals are willing to work to obtain it.[51] Antipsychotic drugs used to treat psychosis reduce dopamine levels and tend to cause anhedonia, a diminished ability to experience pleasure.[52] Many types of pleasurable experiences—such as sex, enjoying food, or playing video games—increase dopamine release.[53] All addictive drugs directly or indirectly affect dopamine neurotransmission in the nucleus accumbens;[45][51] these drugs increase drug "wanting", leading to compulsive drug use, when repeatedly taken in high doses, presumably through the sensitization of incentive-salience.[46] Drugs that increase synaptic dopamine concentrations include psychostimulants such as methamphetamine and cocaine. These produce increases in "wanting" behaviors, but do not greatly alter expressions of pleasure or change levels of satiation.[46][51] However, opiate drugs such as heroin or morphine produce increases in expressions of "liking" and "wanting" behaviors.[46] Moreover, animals in which the ventral tegmental dopamine system has been rendered inactive do not seek food, and will starve to death if left to themselves, but if food is placed in their mouths they will consume it and show expressions indicative of pleasure.[54]		Dopamine does not cross the blood–brain barrier, so its synthesis and functions in peripheral areas are to a large degree independent of its synthesis and functions in the brain.[15] A substantial amount of dopamine circulates in the bloodstream, but its functions there are not entirely clear.[16] Dopamine is found in blood plasma at levels comparable to those of epinephrine, but in humans, over 95% of the dopamine in the plasma is in the form of dopamine sulfate, a conjugate produced by the enzyme sulfotransferase 1A3/1A4 acting on free dopamine.[16] The bulk of this dopamine sulfate is produced in the mesentery that surrounds parts of the digestive system.[16] The production of dopamine sulfate is thought to be a mechanism for detoxifying dopamine that is ingested as food or produced by the digestive process—levels in the plasma typically rise more than fifty-fold after a meal.[16] Dopamine sulfate has no known biological functions and is excreted in urine.[16]		The relatively small quantity of unconjugated dopamine in the bloodstream may be produced by the sympathetic nervous system, the digestive system, or possibly other organs.[16] It may act on dopamine receptors in peripheral tissues, or be metabolized, or be converted to norepinephrine by the enzyme dopamine beta hydroxylase, which is released into the bloodstream by the adrenal medulla.[16] Some dopamine receptors are located in the walls of arteries, where they act as a vasodilator and an inhibitor of norepinephrine release.[55] These responses might be activated by dopamine released from the carotid body under conditions of low oxygen, but whether arterial dopamine receptors perform other biologically useful functions is not known.[55]		Beyond its role in modulating blood flow, there are several peripheral systems in which dopamine circulates within a limited area and performs an exocrine or paracrine function.[16] The peripheral systems in which dopamine plays an important role include the immune system, the kidneys and the pancreas.		In the immune system dopamine acts upon receptors present on immune cells, especially lymphocytes.[56] Dopamine can also affect immune cells in the spleen, bone marrow, and circulatory system.[57] In addition, dopamine can be synthesized and released by immune cells themselves.[56] The main effect of dopamine on lymphocytes is to reduce their activation level. The functional significance of this system is unclear, but it affords a possible route for interactions between the nervous system and immune system, and may be relevant to some autoimmune disorders.[57]		The renal dopaminergic system is located in the cells of the nephron in the kidney, where all subtypes of dopamine receptors are present.[58] Dopamine is also synthesized there, by tubule cells, and discharged into the tubular fluid. Its actions include increasing the blood supply to the kidneys, increasing the glomerular filtration rate, and increasing the excretion of sodium in the urine. Hence, defects in renal dopamine function can lead to reduced sodium excretion and consequently result in the development of high blood pressure. There is strong evidence that faults in the production of dopamine or in the receptors can result in a number of pathologies including oxidative stress, edema, and either genetic or essential hypertension. Oxidative stress can itself cause hypertension.[59] Defects in the system can also be caused by genetic factors or high blood pressure.[60]		In the pancreas the role of dopamine is somewhat complex. The pancreas consists of two parts, an exocrine and an endocrine component. The exocrine part synthesizes and secretes digestive enzymes and other substances, including dopamine, into the small intestine.[61] The function of this secreted dopamine after it enters the small intestine is not clearly established—the possibilities include protecting the intestinal mucosa from damage and reducing gastrointestinal motility (the rate at which content moves through the digestive system).[61]		The pancreatic islets make up the endocrine part of the pancreas, and synthesize and secrete hormones including insulin into the bloodstream.[61] There is evidence that the beta cells in the islets that synthesize insulin contain dopamine receptors, and that dopamine acts to reduce the amount of insulin they release.[61] The source of their dopamine input is not clearly established—it may come from dopamine that circulates in the bloodstream and derives from the sympathetic nervous system, or it may be synthesized locally by other types of pancreatic cells.[61]		Dopamine as a manufactured medication is sold under the trade names Intropin, Dopastat, and Revimine, among others. It is on the World Health Organization's List of Essential Medicines.[62] It is most commonly used as a stimulant drug in the treatment of severe low blood pressure, slow heart rate, and cardiac arrest. It is especially important in treating these in newborn infants.[63] It is given intravenously. Since the half-life of dopamine in plasma is very short—approximately one minute in adults, two minutes in newborn infants and up to five minutes in preterm infants—it is usually given in a continuous intravenous drip rather than a single injection.[64]		Its effects, depending on dosage, include an increase in sodium excretion by the kidneys, an increase in urine output, an increase in heart rate, and an increase in blood pressure.[64] At low doses it acts through the sympathetic nervous system to increase heart muscle contraction force and heart rate, thereby increasing cardiac output and blood pressure.[65] Higher doses also cause vasoconstriction that further increases blood pressure.[65][66] Older literature also describes very low doses thought to improve kidney function without other consequences, but recent reviews have concluded that doses at such low levels are not effective and may sometimes be harmful.[67] While some effects result from stimulation of dopamine receptors, the prominent cardiovascular effects result from dopamine acting at α1, β1, and β2 adrenergic receptors.[68][69]		Side effects of dopamine include negative effects on kidney function and irregular heartbeats.[65] The LD50, or lethal dose which is expected to prove fatal in 50% of the population, has been found to be: 59 mg/kg (mouse; administered intravenously); 95 mg/kg (mouse; administered intraperitoneally); 163 mg/kg (rat; administered intraperitoneally); 79 mg/kg (dog; administered intravenously).[70]		A fluorinated form of L-DOPA known as fluorodopa is available for use in positron emission tomography to assess the function of the nigrostriatal pathway.[71]		The dopamine system plays a central role in several significant medical conditions, including Parkinson's disease, attention deficit hyperactivity disorder, schizophrenia, and addiction. Aside from dopamine itself, there are many other important drugs that act on dopamine systems in various parts of the brain or body. Some are used for medical or recreational purposes, but neurochemists have also developed a variety of research drugs, some of which bind with high affinity to specific types of dopamine receptors and either agonize or antagonize their effects, and many that affect other aspects of dopamine physiology,[72] including dopamine transporter inhibitors, VMAT inhibitors, and enzyme inhibitors.		A number of studies have reported an age-related decline in dopamine synthesis and dopamine receptor density (i.e., the number of receptors) in the brain.[73] This decline has been shown to occur in the striatum and extrastriatal regions.[74] Decreases in the D1, D2, and D3 receptors are well documented.[75][76][77] The reduction of dopamine with aging is thought to be responsible for many neurological symptoms that increase in frequency with age, such as decreased arm swing and increased rigidity.[78] Changes in dopamine levels may also cause age-related changes in cognitive flexibility.[78]		Other neurotransmitters, such as serotonin and glutamate also show a decline in output with aging.[77][79]		Parkinson's disease is an age-related disorder characterized by movement disorders such as stiffness of the body, slowing of movement, and trembling of limbs when they are not in use.[40] In advanced stages it progresses to dementia and eventually death.[40] The main symptoms are caused by the loss of dopamine-secreting cells in the substantia nigra.[80] These dopamine cells are especially vulnerable to damage, and a variety of insults, including encephalitis (as depicted in the book and movie "Awakenings"), repeated sports-related concussions, and some forms of chemical poisoning such as MPTP, can lead to substantial cell loss, producing a parkinsonian syndrome that is similar in its main features to Parkinson's disease.[81] Most cases of Parkinson's disease, however, are idiopathic, meaning that the cause of cell death cannot be identified.[81]		The most widely used treatment for parkinsonism is administration of L-DOPA, the metabolic precursor for dopamine.[15] L-DOPA is converted to dopamine in the brain and various parts of the body by the enzyme DOPA decarboxylase.[14] L-DOPA is used rather than dopamine itself because, unlike dopamine, it is capable of crossing the blood-brain barrier.[15] It is often co-administered with an enzyme inhibitor of peripheral decarboxylation such as carbidopa or benserazide, to reduce the amount converted to dopamine in the periphery and thereby increase the amount of L-DOPA that enters the brain.[15] When L-DOPA is administered regularly over a long time period, a variety of unpleasant side effects such as dyskinesia often begin to appear; even so, it is considered the best available long-term treatment option for most cases of Parkinson's disease.[15]		L-DOPA treatment cannot restore the dopamine cells that have been lost, but it causes the remaining cells to produce more dopamine, thereby compensating for the loss to at least some degree.[15] In advanced stages the treatment begins to fail because the cell loss is so severe that the remaining ones cannot produce enough dopamine regardless of L-DOPA levels.[15] Other drugs that enhance dopamine function, such as bromocriptine and pergolide, are also sometimes used to treat Parkinsonism, but in most cases L-DOPA appears to give the best trade-off between positive effects and negative side-effects.[15]		Dopaminergic medications that are used to treat Parkinson's disease are sometimes associated with the development of a dopamine dysregulation syndrome, which involves the overuse of dopaminergic medication and medication-induced compulsive engagement in natural rewards like gambling and sexual activity.[82][83] The latter behaviors are similar to those observed in individuals with a behavioral addiction.[82]		Cocaine, substituted amphetamines (including methamphetamine), Adderall, methylphenidate (marketed as Ritalin or Concerta), MDMA (ecstasy) and other psychostimulants exert their effects primarily or partly by increasing dopamine levels in the brain by a variety of mechanisms.[84] Cocaine and methylphenidate are dopamine transporter blockers or reuptake inhibitors; they non-competitively inhibit dopamine reuptake, resulting in increased dopamine concentrations in the synaptic cleft.[85][86]:54–58 Like cocaine, substituted amphetamines and amphetamine also increase the concentration of dopamine in the synaptic cleft, but by different mechanisms.[24][86]:147–150		The effects of psychostimulants include increases in heart rate, body temperature, and sweating; improvements in alertness, attention, and endurance; increases in pleasure produced by rewarding events; but at higher doses agitation, anxiety, or even loss of contact with reality.[84] Drugs in this group can have a high addiction potential, due to their activating effects on the dopamine-mediated reward system in the brain.[84] However some can also be useful, at lower doses, for treating attention deficit hyperactivity disorder (ADHD) and narcolepsy.[87][88] An important differentiating factor is the onset and duration of action.[84] Cocaine can take effect in seconds if it is injected or inhaled in free base form; the effects last from 5 to 90 minutes.[89] This rapid and brief action makes its effects easily perceived and consequently gives it high addiction potential.[84] Methylphenidate taken in pill form, in contrast, can take two hours to reach peak levels in the bloodstream,[87] and depending on formulation the effects can last for up to 12 hours.[citation needed] These slow and sustained actions reduce the potential for abuse and make it more useful for treating ADHD.[87][not in citation given]		A variety of addictive drugs produce an increase in reward-related dopamine activity.[84] Stimulants such as nicotine, cocaine and methamphetamine promote increased levels of dopamine which appear to be the primary factor in causing addiction. For other addictive drugs such as the opioid heroin, the increased levels of dopamine in the reward system may only play a minor role in addiction.[90] When people addicted to stimulants go through withdrawal, they do not experience the physical suffering associated with alcohol withdrawal or withdrawal from opiates; instead they experience craving, an intense desire for the drug characterized by irritability, restlessness, and other arousal symptoms,[91] brought about by psychological dependence.		The dopamine system plays a crucial role in several aspects of addiction. At the earliest stage, genetic differences that alter the expression of dopamine receptors in the brain can predict whether a person will find stimulants appealing or aversive.[92] Consumption of stimulants produces increases in brain dopamine levels that last from minutes to hours.[84] Finally, the chronic elevation in dopamine that comes with repetitive high-dose stimulant consumption triggers a wide-ranging set of structural changes in the brain that are responsible for the behavioral abnormalities which characterize an addiction.[93] Treatment of stimulant addiction is very difficult, because even if consumption ceases, the craving that comes with psychological withdrawal does not.[91] Even when the craving seems to be extinct, it may re-emerge when faced with stimuli that are associated with the drug, such as friends, locations and situations.[91] Association networks in the brain are greatly interlinked.[94]		Psychiatrists in the early 1950s discovered that a class of drugs known as typical antipsychotics (also known as major tranquilizers), were often effective at reducing the psychotic symptoms of schizophrenia.[95] The introduction of the first widely used antipsychotic, chlorpromazine (Thorazine), in the 1950s, led to the release of many patients with schizophrenia from institutions in the years that followed.[95] By the 1970s researchers understood that these typical antipsychotics worked as antagonists on the D2 receptors.[95][96] This realization led to the so-called dopamine hypothesis of schizophrenia, which postulates that schizophrenia is largely caused by hyperactivity of brain dopamine systems.[97] The dopamine hypothesis drew additional support from the observation that psychotic symptoms were often intensified by dopamine-enhancing stimulants such as methamphetamine, and that these drugs could also produce psychosis in healthy people if taken in large enough doses.[97] In the following decades other atypical antipsychotics that had fewer serious side effects were developed.[95] Many of these newer drugs do not act directly on dopamine receptors, but instead produce alterations in dopamine activity indirectly.[98] These drugs were also used to treat other psychoses.[95] Antipsychotic drugs have a broadly suppressive effect on most types of active behavior, and particularly reduce the delusional and agitated behavior characteristic of overt psychosis.[96] There remains substantial dispute, however, about how much of an improvement the patient experiences on these drugs.[99]		Later observations, however, have caused the dopamine hypothesis to lose popularity, at least in its simple original form.[97] For one thing, patients with schizophrenia do not typically show measurably increased levels of brain dopamine activity.[97] Also, other dissociative drugs, notably ketamine and phencyclidine that act on glutamate NMDA receptors (and not on dopamine receptors) can produce psychotic symptoms.[97] Perhaps most importantly, those drugs that do reduce dopamine activity are a very imperfect treatment for schizophrenia: they only reduce a subset of symptoms, while producing severe short-term and long-term side effects.[100] Even so, many psychiatrists and neuroscientists continue to believe that schizophrenia involves some sort of dopamine system dysfunction.[95] As the "dopamine hypothesis" has evolved over time, however, the sorts of dysfunctions it postulates have tended to become increasingly subtle and complex.[95]		However, the widespread use of antipsychotic drugs has long been controversial.[99] There are several reasons for this. First, antipsychotic drugs are perceived as very aversive by people who have to take them, because they produce a general dullness of thought and suppress the ability to experience pleasure.[101] Second, it is difficult to show that they act specifically against psychotic behaviors rather than merely suppressing all types of active behavior.[99] Third, they can produce a range of serious side effects, including weight gain, diabetes, fatigue, sexual dysfunction, hormonal changes, and a type of serious movement disorder known as tardive dyskinesia.[100] Some of these side effects may continue long after the cessation of drug use, or even permanently.[100]		Altered dopamine neurotransmission is implicated in attention deficit hyperactivity disorder (ADHD), a condition associated with impaired cognitive control, in turn leading to problems with regulating attention (attentional control), inhibiting behaviors (inhibitory control), and forgetting things or missing details (working memory), among other problems.[102] There are genetic links between dopamine receptors, the dopamine transporter, and ADHD, in addition to links to other neurotransmitter receptors and transporters.[103] The most important relationship between dopamine and ADHD involves the drugs that are used to treat ADHD.[104] Some of the most effective therapeutic agents for ADHD are psychostimulants such as methylphenidate (Ritalin, Concerta) and amphetamine (Adderall, Dexedrine), drugs that increase both dopamine and norepinephrine levels in the brain.[104] The clinical effects of these psychostimulants in treating ADHD are mediated through the indirect activation of dopamine and norepinephrine receptors, specifically dopamine receptor D1 and adrenoceptor A2, in the prefrontal cortex.[102][105][106]		Dopamine plays a role in pain processing in multiple levels of the central nervous system including the spinal cord, periaqueductal gray, thalamus, basal ganglia, and cingulate cortex.[107] Decreased levels of dopamine have been associated with painful symptoms that frequently occur in Parkinson's disease.[107] Abnormalities in dopaminergic neurotransmission also occur in several painful clinical conditions, including burning mouth syndrome, fibromyalgia, and restless legs syndrome.[107]		Nausea and vomiting are largely determined by activity in the area postrema in the medulla of the brainstem, in a region known as the chemoreceptor trigger zone.[108] This area contains a large population of type D2 dopamine receptors.[108] Consequently, drugs that activate D2 receptors have a high potential to cause nausea.[108] This group includes some medications that are administered for Parkinson's disease, as well as other dopamine agonists such as apomorphine.[109] In some cases, D2-receptor antagonists such as metoclopramide are useful as anti-nausea drugs.[108]		There are no reports of dopamine in archaea, but it has been detected in some types of bacteria and in the protozoan called Tetrahymena.[110] Perhaps more importantly, there are types of bacteria that contain homologs of all the enzymes that animals use to synthesize dopamine.[111] It has been proposed that animals derived their dopamine-synthesizing machinery from bacteria, via horizontal gene transfer that may have occurred relatively late in evolutionary time, perhaps as a result of the symbiotic incorporation of bacteria into eukaryotic cells that gave rise to mitochondria.[111]		Dopamine is used as a neurotransmitter in most multicellular animals.[112] In sponges there is only a single report of the presence of dopamine, with no indication of its function;[113] however, dopamine has been reported in the nervous systems of many other radially symmetric species, including the cnidarian jellyfish, hydra and some corals.[114] This dates the emergence of dopamine as a neurotransmitter back to the earliest appearance of the nervous system, over 500 million years ago in the Cambrian era. Dopamine functions as a neurotransmitter in vertebrates, echinoderms, arthropods, molluscs, and several types of worm.[115][116]		In every type of animal that has been examined, dopamine has been seen to modify motor behavior.[112] In the model organism, nematode Caenorhabditis elegans, it reduces locomotion and increases food-exploratory movements; in flatworms it produces "screw-like" movements; in leeches it inhibits swimming and promotes crawling. Across a wide range of vertebrates, dopamine has an "activating" effect on behavior-switching and response selection, comparable to its effect in mammals.[112]		Dopamine has also consistently been shown to play a role in reward learning, in all animal groups.[112] As in all vertebrates – invertebrates such as roundworms, flatworms, molluscs and common fruit flies can all be trained to repeat an action if it is consistently followed by an increase in dopamine levels.[112]		It had long been believed that arthropods were an exception to this with dopamine being seen as having an adverse effect. Reward was seen to be mediated instead by octopamine, a neurotransmitter closely related to norepinephrine.[117] More recent studies however have shown that dopamine does play a part in reward learning in fruit flies. Also it has been found that the rewarding effect of octopamine is due to its activating a set of dopaminergic neurons not previously accessed in the research.[117]		Many plants, including a variety of food plants, synthesize dopamine to varying degrees.[118] The highest concentrations have been observed in bananas—the fruit pulp of red and yellow bananas contains dopamine at levels of 40 to 50 parts per million by weight.[118] Potatoes, avocados, broccoli, and Brussels sprouts may also contain dopamine at levels of 1 part per million or more; oranges, tomatoes, spinach, beans, and other plants contain measurable concentrations less than 1 part per million.[118] The dopamine in plants is synthesized from the amino acid tyrosine, by biochemical mechanisms similar to those that animals use.[118] It can be metabolized in a variety of ways, producing melanin and a variety of alkaloids as byproducts.[118] The functions of plant catecholamines have not been clearly established, but there is evidence that they play a role in the response to stressors such as bacterial infection, act as growth-promoting factors in some situations, and modify the way that sugars are metabolized. The receptors that mediate these actions have not yet been identified, nor have the intracellular mechanisms that they activate.[118]		Dopamine consumed in food cannot act on the brain, because it cannot cross the blood–brain barrier.[15] However, there are also a variety of plants that contain L-DOPA, the metabolic precursor of dopamine.[119] The highest concentrations are found in the leaves and bean pods of plants of the genus Mucuna, especially in Mucuna pruriens (velvet beans), which have been used as a source for L-DOPA as a drug.[120] Another plant containing substantial amounts of L-DOPA is Vicia faba, the plant that produces fava beans (also known as "broad beans"). The level of L-DOPA in the beans, however, is much lower than in the pod shells and other parts of the plant.[121] The seeds of Cassia and Bauhinia trees also contain substantial amounts of L-DOPA.[119]		In a species of marine green algae Ulvaria obscura, a major component of some algal blooms, dopamine is present in very high concentrations, estimated at 4.4% of dry weight. There is evidence that this dopamine functions as an anti-herbivore defense, reducing consumption by snails and isopods.[122]				Melanins are a family of dark-pigmented substances found in a wide range of organisms.[123] Chemically they are closely related to dopamine, and there is a type of melanin, known as dopamine-melanin, that can be synthesized by oxidation of dopamine via the enzyme tyrosinase.[123] The melanin that darkens human skin is not of this type: it is synthesized by a pathway that uses L-DOPA as a precursor but not dopamine.[123] However, there is substantial evidence that the neuromelanin that gives a dark color to the brain's substantia nigra is at least in part dopamine-melanin.[124]		Dopamine-derived melanin probably appears in at least some other biological systems as well. Some of the dopamine in plants is likely to be used as a precursor for dopamine-melanin.[125] The complex patterns that appear on butterfly wings, as well as black-and-white stripes on the bodies of insect larvae, are also thought to be caused by spatially structured accumulations of dopamine-melanin.[126]		Dopamine was first synthesized in 1910 by George Barger and James Ewens at Wellcome Laboratories in London, England[127] and first identified in the human brain by Kathleen Montagu in 1957. It was named dopamine because it is a monoamine whose precursor in the Barger-Ewens synthesis is 3,4-dihydroxyphenylalanine (levodopa or L-DOPA). Dopamine's function as a neurotransmitter was first recognized in 1958 by Arvid Carlsson and Nils-Åke Hillarp at the Laboratory for Chemical Pharmacology of the National Heart Institute of Sweden.[128] Carlsson was awarded the 2000 Nobel Prize in Physiology or Medicine for showing that dopamine is not only a precursor of norepinephrine (noradrenaline) and epinephrine (adrenaline), but is also itself a neurotransmitter.[129]		Research motivated by adhesive polyphenolic proteins in mussels led to the discovery in 2007 that a wide variety of materials, if placed in a solution of dopamine at slightly basic pH, will become coated with a layer of polymerized dopamine, often referred to as polydopamine.[130][131] This polymerized dopamine forms by a spontaneous oxidation reaction, and is formally a type of melanin.[132] Synthesis usually involves reaction of dopamine hydrochloride with Tris as a base in water. The structure of polydopamine is unknown.[131]		Polydopamine coatings can form on objects ranging in size from nanoparticles to large surfaces.[132] Polydopamine layers have chemical properties that have the potential to be extremely useful, and numerous studies have examined their possible applications.[132] At the simplest level, they can be used for protection against damage by light, or to form capsules for drug delivery.[132] At a more sophisticated level, their adhesive properties may make them useful as substrates for biosensors or other biologically active macromolecules.[132]		
Physical fitness is a state of health and well-being and, more specifically, the ability to perform aspects of sports, occupations and daily activities. Physical fitness is generally achieved through proper nutrition,[1] moderate-vigorous physical exercise,[2] and sufficient rest.[3]		Before the industrial revolution, fitness was defined as the capacity to carry out the day’s activities without undue fatigue. However, with automation and changes in lifestyles physical fitness is now considered a measure of the body's ability to function efficiently and effectively in work and leisure activities, to be healthy, to resist hypokinetic diseases, and to meet emergency situations.[4]						Fitness is defined[5] as the quality or state of being fit. Around 1950, perhaps consistent with the Industrial Revolution and the treatise of World War II, the term "fitness" increased in western vernacular by a factor of ten.[6] Modern definition of fitness describe either a person or machine's ability to perform a specific function or a holistic definition of human adaptability to cope with various situations. This has led to an interrelation of human fitness and attractiveness which has mobilized global fitness and fitness equipment industries. Regarding specific function, fitness is attributed to person who possess significant aerobic or anaerobic ability, i.e. strength or endurance. A well rounded fitness program will improve a person in all aspects of fitness, rather than one, such as only cardio/respiratory endurance or only weight training.		A comprehensive fitness program tailored to an individual typically focuses on one or more specific skills,[7] and on age-[8] or health-related needs such as bone health.[9] Many sources[10] also cite mental, social and emotional health as an important part of overall fitness. This is often presented in textbooks as a triangle made up of three points, which represent physical, emotional, and mental fitness. Physical fitness can also prevent or treat many chronic health conditions brought on by unhealthy lifestyle or aging.[11] Working out can also help some people sleep better and possibly alleviate some mood disorders in certain individuals.[12]		Developing research has demonstrated that many of the benefits of exercise are mediated through the role of skeletal muscle as an endocrine organ. That is, contracting muscles release multiple substances known as myokines which promote the growth of new tissue, tissue repair, and various anti-inflammatory functions, which in turn reduce the risk of developing various inflammatory diseases.[13]		The Physical Activity Guidelines for Americans was created by the Office of Disease Prevention and Health Promotion. This publication suggests that all adults should avoid inactivity to promote good health mentally and physically. For substantial health benefits, adults should participate in at least 150 minutes (two hours and 30 minutes) a week of moderate-intensity, or 75 minutes (1 hour and 15 minutes) a week of vigorous-intensity aerobic physical activity, or an equivalent combination of moderate- and vigorous-intensity aerobic activity. Aerobic activity should be performed in episodes of at least 10 minutes, and preferably, it should be spread throughout the week. For additional and more extensive health benefits, adults should increase their aerobic physical activity to 300 minutes (5 hours) a week of moderate-intensity, or 150 minutes a week of vigorous-intensity aerobic physical activity, or an equivalent combination of moderate- and vigorous-intensity activity. Additional health benefits are gained by engaging in physical activity beyond this amount. Adults should also do muscle-strengthening activities that are moderate or high intensity and involve all major muscle groups on 2 or more days a week, as these activities provide additional health benefits.[14]				Specific or task-oriented fitness is a person's ability to perform in a specific activity with a reasonable efficiency: for example, sports or military service. Specific training prepares athletes to perform well in their sport.		Examples are:		In order for physical fitness to benefit the health of an individual, an unknown response in the person called a stimulus will be triggered by the exertion. When exercise is performed with the correct amount of intensity, duration and frequency, a significant amount of improvement can occur. The person may overall feel better but the physical effects on the human body take weeks or months to notice and possibly years for full development. For training purposes, exercise must provide a stress or demand on either a function or tissue. To continue improvements, this demand must eventually increase little over an extended period of time. This sort of exercise training has three basic principles: overload, specificity, and progression. These principles are related to health but also enhancement of physical working capacity.[21]		High Intensity Interval Training consists of repeated, short bursts of exercise, completed at a high level of intensity. These sets of intense activity are followed by a predetermined time of rest or low intensity activity.[22] Studies have shown that exercising at a higher intensity has increased cardiac benefits for humans, compared to when exercising at a low or moderate level.[23] When your workout consists of an HIIT session, your body has to work harder to replace the oxygen it lost. Research into the benefits of HIIT have revealed that it can be very successful for reducing fat, especially around the abdominal region. Furthermore, when compared to continuous moderate exercise, HIIT proves to burn more calories and increase the amount of fat burned post- HIIT session.[24] Lack of time is one of the main reasons stated for not exercising; HIIT is a great alternative for those people because the duration of an HIIT session can be as short as 10 minutes, making it much quicker than conventional workouts.[25]		Cardiorespiratory fitness can be measured using VO2 max, a measure of the amount of oxygen the body can uptake and utilize.[26][27] Aerobic exercise, which improves cardiorespiratory fitness, involves movement that increases the heart rate to improve the body's oxygen consumption. This form of exercise is an important part of all training regiments ranging from professional athletes to the everyday person. Also, it helps increase stamina.		Examples are:		Physical fitness has proven to result in positive effects on the body's blood pressure because staying active and exercising regularly builds up a stronger heart. The heart is the main organ in charge of systolic blood pressure and diastolic blood pressure. Engaging in a physical activity will create a rise in blood pressure, once the activity is stopped, however, the individual’s blood pressure will return to normal. The more physical activity that one engages in, the easier this process becomes, resulting in a more ‘fit’ individual.[29] Through regular physical fitness, the heart does not have to work as hard to create a rise in blood pressure, which lowers the force on the arteries, and lowers the over all blood pressure.[30]		Centers for disease control and prevention provide lifestyle guidelines of maintaining a balanced diet and engaging in physical activity to reduce the risk of disease. The WCRF/ American Institute for Cancer Research (AICR) published a list of recommendations that reflect the evidence they have found through consistency in fitness and dietary factors that directly relate to Cancer prevention.		The WCRF/AICR recommendations include the following:		These recommendations are also widely supported by the American Cancer Society. The guidelines have been evaluated and individuals that have higher guideline adherence scores substantially reduce cancer risk as well as help towards control with a multitude of chronic health problems. Regular physical activity is a factor that helps reduce an individual’s blood pressure and improves cholesterol levels, two key components that correlate with heart disease and Type 2 Diabetes.[32] The American Cancer Society encourages the public to "adopt a physically active lifestyle" by meeting the criteria in a variety of physical activities such as hiking, swimming, circuit training, resistance raining, lifting, etc. It is understood that cancer is not a disease that can be cured by physical fitness alone, however because it is a multifactorial disease, physical fitness is a controllable prevention. The large associations tied with being physically fit and reduced cancer risk are enough to provide a strategy to reduce cancer risk.[31] The American Cancer Society assorts different levels of activity ranging from moderate to vigorous to clarify the recommended time spent on a physical activity. These classifications of physical activity consider the intentional exercise and basic activities done on a daily basis and give the public a greater understanding by what fitness levels suffice as future disease prevention.		Studies have shown an association between increased physical activity and reduced inflammation.[33] It produces both a short-term inflammatory response and a long-term anti-inflammatory effect.[34] Physical activity reduces inflammation in conjunction with or independent of changes in body weight.[35] However, the mechanisms linking physical activity to inflammation are unknown.		Physical activity boosts the immune system. This is dependent on the concentration of endogenous factors (such as sex hormones, metabolic hormones and growth hormones), body temperature, blood flow, hydration status and body position.[36] Physical activity has shown to increase the levels of natural killer (NK) cells, NK T cells, macrophages, neutrophils and eosinophils, complements, cytokines, antibodies and T cytotoxic cells.[37][38] However, the mechanism linking physical activity to immune system is not fully understood.		Physical activity affects one’s blood pressure, cholesterol levels, blood lipid levels, blood clotting factors and the strength of blood vessels. All factors that directly correlate to cardiovascular disease. It also improves the body’s use of insulin. People who are at risk for diabetes, Type 2 (insulin resistant) especially, benefit greatly from physical activity because it activates a better usage of insulin and protects the heart. Those who develop diabetes have an increased risk of developing cardiovascular disease. In a study where a sample of around ten thousand adults from the Third National Health and Nutrition Examination Survey, physical activity and metabolic risk factors such as insulin resistance, inflammation, dyslipidemia were assessed. The study adjusted basic confounders with moderate/vigorous physical activity and the relation with CVD mortality. The results displayed physical activity being associated with a lower risk of CVD mortality that was independent of traditional metabolic risk factors.		The American Heart Association recommendations include the same findings as provided in the WCRF/ AICR recommendations list for people who are healthy. In regards to people with lower blood pressure or cholesterol, the association recommends that these individuals aim for around forty minutes of moderate to vigorous physical activity around three or four times a week.[39]		Achieving resilience through physical fitness promotes a vast and complex range of health related benefits. Individuals who keep up physical fitness levels generally regulate their distribution of body fat and stay away from obesity. Abdominal fat, specifically visceral fat, is most directly affected by engaging in aerobic exercise. Strength training has been known to increase the amount of muscle in the body, however it can also reduce body fat.[40] Sex steroid hormones, insulin, and an appropriate immune response are factors that mediate metabolism in relation to the abdominal fat. Therefore, physical fitness provides weight control through regulation of these bodily functions.[41]		Menopause is the term that is used to refer to the stretch of both before and after a woman's last menstrual cycle. There are an instrumental amount of symptoms connected to menopause, most of which can affect the quality of life of the women involved in this stage of her life. One way to reduce the severity of the symptoms is exercise and keeping a healthy level of fitness. Prior to and during menopause as the female body changes there can be physical, physiological or internal changes to the body. These changes can be prevented or even reduced with the use of regular exercise. These changes include;[42]		The Melbourne Women's Midlife Health Project provided evidence that showed over an eight-year time period 438 were followed. Even though the physical activity was not associated with VMS in this cohort at the beginning. Women who reported they were physically active everyday at the beginning were 49% less likely to have reported bothersome hot flushes. This is in contrast to women whose level of activity decreased and were more likely to experience bothersome hot flushes.[44]		Physical fitness can enhance your mood through the release of dopamine, neurotransmitters, endorphins and endocannabinoids. A short workout session or walk can cause your body to release this chemical which causes individuals to feel better. Physical fitness increases your bodies temperature which results in a calming effect toward the individual.[45][46][47]		|group2 = See also |list2 =		|below = }}		
The International Sports Sciences Association is an organization that operates as a teaching institution and certification agency for fitness trainers, aerobic instructors, and medical professionals.		The ISSA offers a general fitness certification course for personal training and six specialized fitness certification courses, including fitness nutrition, sports nutrition, strength and conditioning, exercise therapy, senior fitness and youth fitness. The student obtains the status of "Elite Trainer" after completion of three courses, and "Master Trainer" after completion of six courses.		Instruction is based on exercise assessment, nutritional planning, fitness instruction, sports medicine practice, and post-rehabilitation training. The school has enrolled over 130,000 students, both in fitness education and continuing education courses.[1]						No official requirements or standards of physical training existed until the fitness boom of the 1980s. The International Sports Sciences Association was founded in 1988, when, "recognizing the need for standardization and credibility, Dr. Sal Arria and Dr. Fredrick Hatfield created a personal fitness training program to merge gym experience with practical and applied sciences."[2] In keeping with this standard, the ISSA became a Provisional Affiliate of the National Board of Fitness Examiners in 2004.		The National Board of Fitness Examiners was founded to develop nationally based and uniform standards of practice for personal fitness trainers to be used to administer an NBFE provided written and practical examination followed by registration of those who successfully pass the test process. Under NBFE’s procedure, preparation for the NBFE examination can take several forms and includes training through a number of affiliated organizations who themselves include certifying organizations for fitness professionals,[3] including the ISSA. In 2009, the ISSA became the first fitness organization to earn accreditation by a federally recognized accrediting agency—the Distance Education and Training Council[4][5] (an accrediting agency recognized by the United States Department of Education and the Council for Higher Education Accreditation).		As a result, the ISSA was added to the list of GI Jobs' Military Friendly Schools in 2010, being approved by the Department of Defense for the Defense Activity for Non-Traditional Education Support (DANTES) to accept U.S. Armed Forces Tuition Assistance and the Military Spouse Career Advancement Account (MyCAA) Financial Assistance program.[6][7]		The ISSA is accredited by the Accrediting Commission of the Distance Education and Training Council (DETC).[13] The DETC is listed by the U.S. Department of Education as a nationally recognized agency,[14] is recognized by the Council for Higher Education Accreditation (CHEA),[15] and by the International Health, Racquet & Sportsclub Association (IHRSA).[16]		ISSA is registered with the Better Business Bureau and has been awarded the highest company rating of A+.[17] The ISSA is approved by the New York Chiropractic College for Postgraduate and Continuing Education.[18] The ISSA is also a Provisional Affiliate of the National Board of Fitness Examiners (NFBE).[3][19]		The ISSA is recognized by the Defense Activity for Non-traditional Education Support (DANTES) as a Nationally Accredited Distance Learning Program for Military Service Members. It is also approved for Armed Forces Tuition Assistance and Military Spouse Financial Assistance (MyCAA).[20] "The ISSA is also GI Bill approved by the Bureau for Private and Postsecondary Vocational Education (BPPVE) under contract with the Veterans Administration. Active military, veterans, and eligible spouses and dependents can receive reimbursement from the Veterans Administration for testing fees for any ISSA fitness certification courses."[6]		
Strength training is a type of physical exercise specializing in the use of resistance to induce muscular contraction which builds the strength, anaerobic endurance, and size of skeletal muscles.		When properly performed, strength training can provide significant functional benefits and improvement in overall health and well-being, including increased bone, muscle, tendon, and ligament strength and toughness, improved joint function, reduced potential for injury,[1] increased bone density, increased metabolism, increased fitness,[2][3] improved cardiac function, and improved lipoprotein lipid profiles, including elevated HDL ("good") cholesterol.[4] Training commonly uses the technique of progressively increasing the force output of the muscle through incremental weight increases and uses a variety of exercises and types of equipment to target specific muscle groups. Strength training is primarily an anaerobic activity, although some proponents have adapted it to provide the benefits of aerobic exercise through circuit training.		Strength training is typically associated with the production of lactate, which is a limiting factor of exercise performance. Regular endurance exercise leads to adaptations in skeletal muscle which can prevent lactate levels from rising during strength training. This is mediated via activation of PGC-1alpha which alter the LDH (lactate dehydrogenase) isoenzyme complex composition and decreases the activity of the lactate generating enzyme LDHA, while increasing the activity of the lactate metabolizing enzyme LDHB.[5]		Sports where strength training is central are bodybuilding, weightlifting, powerlifting, strongman, Highland games, shotput, discus throw, and javelin throw. Many other sports use strength training as part of their training regimen, notably tennis, American football, wrestling, track and field, rowing, lacrosse, basketball, pole dancing, hockey, professional wrestling, rugby union, rugby league, and soccer. Strength training for other sports and physical activities is becoming increasingly popular.						The benefits of weight training include greater muscular strength, improved muscle tone and appearance, increased endurance and enhanced bone density.		Many people take up weight training to improve their physical attractiveness. There is evidence that a body type consisting of broad shoulders and a narrow waist, attainable through strength training, is the most physically attractive male attribute according to women participating in the research.[6] Most men can develop substantial muscles; most women lack the testosterone to do it, but they can develop a firm, "toned" (see below) physique, and they can increase their strength by the same proportion as that achieved by men (but usually from a significantly lower starting point). An individual's genetic make-up dictates the response to weight training stimuli to a significant extent, training can not exceed a muscle's intrinsic genetically determined qualities, but clearly polymorphic expression of Myosin heavy chains is possible.[7]		Workouts elevate metabolism for up to 14 hours following 45-minutes of vigorous exercise.[8][9]		Strength training also provides functional benefits. Stronger muscles improve posture, provide better support for joints, and reduce the risk of injury from everyday activities. Older people who take up weight training can prevent some of the loss of muscle tissue that normally accompanies aging—and even regain some functional strength—and by doing so become less frail.[10] They may be able to avoid some types of physical disability. Weight-bearing exercise also helps to prevent osteoporosis and to improve bone strength in those with osteoporosis.[11] The benefits of weight training for older people have been confirmed by studies of people who began engaging in it even in their 80s and 90s.		Though strength training can stimulate the cardiovascular system, many exercise physiologists, based on their observation of maximal oxygen uptake, argue that aerobics training is a better cardiovascular stimulus. Central catheter monitoring during resistance training reveals increased cardiac output, suggesting that strength training shows potential for cardiovascular exercise. However, a 2007 meta-analysis found that, though aerobic training is an effective therapy for heart failure patients, combined aerobic and strength training is ineffective.[12]		Strength training may be important to metabolic and cardiovascular health. Recent evidence suggests that resistance training may reduce metabolic and cardiovascular disease risk. Overweight individuals with high strength fitness exhibit metabolic/cardiovascular risk profiles similar to normal-weight, fit individuals rather than overweight unfit individuals.[13]		For many people in rehabilitation or with an acquired disability, such as following stroke or orthopaedic surgery, strength training for weak muscles is a key factor to optimise recovery.[14] For people with such a health condition, their strength training is likely to need to be designed by an appropriate health professional, such as a physiotherapist or an occupational therapist.		Stronger muscles improve performance in a variety of sports. Sport-specific training routines are used by many competitors. These often specify that the speed of muscle contraction during weight training should be the same as that of the particular sport.[15]		One side effect of intense exercise is increased levels of dopamine, serotonin, and norepinephrine, which can help to improve mood and counter feelings of depression (It should be noted that dopamine and serotonin were not found to be increased by resistance training).[16][17]		Developing research has demonstrated that many of the benefits of exercise are mediated through the role of skeletal muscle as an endocrine organ. That is, contracting muscles release multiple substances known as myokines which promote the growth of new tissue, tissue repair, and various anti-inflammatory functions, which in turn reduce the risk of developing various inflammatory diseases.[18]		The basic principles of strength training involve a manipulation of the number of repetitions (reps), sets, tempo, exercises and force to cause desired changes in strength, endurance or size by overloading of a group of muscles. The specific combinations of reps, sets, exercises, resistance and force depend on the purpose of the individual performing the exercise: to gain size and strength multiple (4+) sets with fewer reps must be performed using more force.[19] A wide spectrum of regimens can be adopted to achieve different results, but the classic formula recommended by the American College of Sports Medicine reads as follows:		Typically failure to use good form during a training set can result in injury or an inability to meet training goals – since the desired muscle group is not challenged sufficiently, the threshold of overload is never reached and the muscle does not gain in strength. There are cases when cheating is beneficial, as is the case where weaker groups become the weak link in the chain and the target muscles are never fully exercised as a result.		The benefits of strength training include increased muscle, tendon and ligament strength, bone density, flexibility, tone, metabolic rate and postural support.		Strength training has a variety of specialized terms used to describe parameters of strength training:		For developing endurance, gradual increases in volume and gradual decreases in intensity is the most effective program.[21][22] Sets of thirteen to twenty repetitions develop anaerobic endurance, with some increases to muscle size and limited impact on strength.[23]		It has been shown that for beginners, multiple-set training offers minimal benefits over single-set training with respect to either strength gain or muscle mass increase, but for the experienced athlete multiple-set systems are required for optimal progress.[23][24][25] However, one study shows that for leg muscles, three sets are more effective than one set.[26]		Beginning weight-trainers are in the process of training the neurological aspects of strength,[citation needed] the ability of the brain to generate a rate of neuronal action potentials that will produce a muscular contraction that is close to the maximum of the muscle's potential.		Weights for each exercise should be chosen so that the desired number of repetitions can just be achieved.		In one common method, weight training uses the principle of progressive overload, in which the muscles are overloaded by attempting to lift at least as much weight as they are capable. They respond by growing larger and stronger.[28] This procedure is repeated with progressively heavier weights as the practitioner gains strength and endurance.		However, performing exercises at the absolute limit of one's strength (known as one rep max lifts) is considered too risky for all but the most experienced practitioners. Moreover, most individuals wish to develop a combination of strength, endurance and muscle size. One repetition sets are not well suited to these aims. Practitioners therefore lift lighter (sub-maximal) weights, with more repetitions, to fatigue the muscle and all fibres within that muscle as required by the progressive overload principle.		Commonly, each exercise is continued to the point of momentary muscular failure. Contrary to widespread belief, this is not the point at which the individual thinks they cannot complete any more repetitions, but rather the first repetition that fails due to inadequate muscular strength. Training to failure is a controversial topic with some advocating training to failure on all sets while others believe that this will lead to overtraining, and suggest training to failure only on the last set of an exercise.[29] Some practitioners recommend finishing a set of repetitions just before reaching a personal maximum at a given time. Adrenaline and other hormones may promote additional intensity by stimulating the body to lift additional weight (as well as the neuro-muscular stimulations that happen when in "fight-or-flight" mode, as the body activates more muscle fibres), so getting "psyched up" before a workout can increase the maximum weight lifted.		Weight training can be a very effective form of strength training because exercises can be chosen, and weights precisely adjusted, to safely exhaust each individual muscle group after the specific numbers of sets and repetitions that have been found to be the most effective for the individual. Other strength training exercises lack the flexibility and precision that weights offer.		Split training involves working no more than three muscle groups or body parts per day, instead spreading the training of specific body parts throughout a training cycle of several days. It is commonly used by more advanced practitioners due to the logistics involved in training all muscle groups maximally. Training all the muscles in the body individually through their full range of motion in a single day is generally not considered possible due to caloric and time constraints. Split training involves fully exhausting individual muscle groups during a workout, then allowing several days for the muscle to fully recover. Muscles are worked roughly twice per week and allowed roughly 72 hours to recover. Recovery of certain muscle groups is usually achieved on days while training other groups, i.e. a 7-day week can consist of a practitioner training trapezius, side shoulders and upper shoulders to exhaustion on one day, the following day the arms to exhaustion, the day after that the rear, front shoulders and back, the day after that the chest. In this way all mentioned muscle groups are allowed the necessary recovery.[30]		Three important variables of strength training are intensity, volume, and frequency. Intensity refers to the amount of work required to achieve the activity, and is proportional to the mass of the weights being lifted. Volume refers to the number of muscles worked, exercises, sets and reps during a single session. Frequency refers to how many training sessions are performed per week.		These variables are important because they are all mutually conflicting, as the muscle only has so much strength and endurance, and takes time to recover due to microtrauma. Increasing one by any significant amount necessitates the decrease of the other two, e.g. increasing weight means a reduction of reps, and will require more recovery time and therefore fewer workouts per week. Trying to push too much intensity, volume and frequency will result in overtraining, and eventually lead to injury and other health issues such as chronic soreness and general lethargy, illness or even acute trauma such as avulsion fractures. A high-medium-low formula can be used to avoid overtraining, with either intensity, volume, or frequency being high, one of the others being medium, and the other being low. One example of this training strategy can be found in the following chart:		A common training strategy is to set the volume and frequency the same each week (e.g. training 3 times per week, with 2 sets of 12 reps each workout), and steadily increase the intensity (weight) on a weekly basis. However, to maximize progress to specific goals, individual programs may require different manipulations, such as decreasing the weight, and increase volume or frequency.[31]		Making program alterations on a daily basis (daily undulating periodization) seems to be more efficient in eliciting strength gains than doing so every 4 weeks (linear periodization),[32] but for beginners there are no differences between different periodization models.[33]		There are many complicated definitions for periodization, but the term simply means the division of the overall training program into periods which accomplish different goals.		Periodization is the modulating of volume, intensity, and frequency over time, to both stimulate gains and allow recovery.		In some programs for example; volume is decreased during a training cycle while intensity is increased. In this template, a lifter would begin a training cycle with a higher rep range than he will finish with.		For this example, the lifter has a 1 rep max of 225 lb:		This is an example of periodization where the number of repetitions decreases while the weight increases.		There are many methods of strength training. Examples include weight training, circuit training, isometric exercise, gymnastics, plyometrics, Parkour, yoga, Pilates, Super Slow.		Strength training may be done with minimal or no equipment, for instance bodyweight exercises. Equipment used for strength training includes barbells and dumbbells, weight machines and other exercise machines, weighted clothing, resistance bands, gymnastics apparatus, Swiss balls, wobble boards, indian clubs, pneumatic exercise equipment, hydraulic exercise equipment.		Strength training exercise is primarily anaerobic.[34] Even while training at a lower intensity (training loads of ~20-RM), anaerobic glycolysis is still the major source of power, although aerobic metabolism makes a small contribution.[35] Weight training is commonly perceived as anaerobic exercise, because one of the more common goals is to increase strength by lifting heavy weights. Other goals such as rehabilitation, weight loss, body shaping, and bodybuilding often use lower weights, adding aerobic character to the exercise.		Except in the extremes, a muscle will fire fibres of both the aerobic or anaerobic types on any given exercise, in varying ratio depending on the load on the intensity of the contraction.[25] This is known as the energy system continuum. At higher loads, the muscle will recruit all muscle fibres possible, both anaerobic ("fast-twitch") and aerobic ("slow-twitch"), in order to generate the most force. However, at maximum load, the anaerobic processes contract so forcefully that the aerobic fibers are completely shut out, and all work is done by the anaerobic processes. Because the anaerobic muscle fibre uses its fuel faster than the blood and intracellular restorative cycles can resupply it, the maximum number of repetitions is limited.[36] In the aerobic regime, the blood and intracellular processes can maintain a supply of fuel and oxygen, and continual repetition of the motion will not cause the muscle to fail.		Circuit weight training is a form of exercise that uses a number of weight training exercise sets separated by short intervals. The cardiovascular effort to recover from each set serves a function similar to an aerobic exercise, but this is not the same as saying that a weight training set is itself an aerobic process.		Weight trainers commonly divide the body's individual muscles into ten major muscle groups. These do not include the hip, neck and forearm muscles, which are rarely trained in isolation. The most common exercises for these muscle groups are listed below.		The sequence shown below is one possible way to order the exercises. The large muscles of the lower body are normally trained before the smaller muscles of the upper body, because these first exercises require more mental and physical energy. The core muscles of the torso are trained before the shoulder and arm muscles that assist them. Exercises often alternate between "pushing" and "pulling" movements to allow their specific supporting muscles time to recover. The stabilizing muscles in the waist should be trained last.		A number of techniques have been developed to make weight training exercises more intense, and thereby potentially increase the rate of progress. Many weight lifters use these techniques to bring themselves past a plateau, a duration where a weightlifter may be unable to do more lifting repetitions, sets, or use higher weight resistance.		Drop sets do not end at the point of momentary muscular failure, but continue with progressively lighter weights.		Pyramid sets are weight training sets in which the progression is from lighter weights with a greater number of repetitions in the first set, to heavier weights with fewer repetitions in subsequent sets.		A reverse pyramid is the opposite in which the heavier weights are used at the beginning and progressively lightened.		Burnouts combine pyramids and drop sets, working up to higher weights with low reps and then back down to lower weights and high reps.		The diminishing set method is where a weight is chosen that can be lifted for 20 reps in one set, and then 70 repetitions are performed in as few sets as possible.[37]		Rest-pause are performed at or near 1RM, with ten to twenty seconds of rest between each lift.[38] The lift is repeated six to eight times. It is generally recommended to use this method infrequently.		The Giant set, is a form of training that targets one muscle group (e.g. the triceps) with four separate exercises performed in quick succession, often to failure and sometimes with the reduction of weight halfway through a set once muscle fatigue sets in. This form of intense training 'shocks' the muscles and as such, is usually performed by experienced trainers and should be used infrequently.[39]		Strength training is a safe form of exercise when the movements are controlled, and carefully defined. Or some safety measures can also be taken before the training. However, as with any form of exercise, improper execution and the failure to take appropriate precautions can result in injury. A helmet, boots, gloves, and back belt can aide in injury prevention. Principles of weight training safety apply to strength training.		Bodybuilding is a sport in which the goal is to increase muscle size and definition. Bodybuilding increases the endurance of muscles, as well as strength, though not as much as if it were the primary goal. Bodybuilders compete in bodybuilding competitions, and use specific principles and methods of strength training to maximize muscular size and develop extremely low levels of body fat. In contrast, most strength trainers train to improve their strength and endurance while not giving special attention to reducing body fat below normal. Strength trainers tend to focus on compound exercises to build basic strength, whereas bodybuilders often use isolation exercises to visually separate their muscles, and to improve muscular symmetry. Pre-contest training for bodybuilders is different again, in that they attempt to retain as much muscular tissue as possible while undergoing severe dieting. However, the bodybuilding community has been the source of many strength training principles, techniques, vocabulary, and customs.		It is widely accepted that strength training must be matched by changes in diet in order to be effective. Although aerobic exercise has been proven to have an effect on the dietary intake of macronutrients, strength training has not [43] and an increase in dietary protein is generally believed to be required for building skeletal muscle with popular sources advising weight trainers to consume a high-protein diet which delivers 1.4 to 1.8 g of protein per kg of body weight per day (0.6 to 0.8 g per pound).[44] Protein that is neither needed for cell growth and repair nor consumed for energy is converted into urea mainly through the deamination process and is excreted by the kidneys. It was once thought that a high-protein diet entails risk of kidney damage, but studies have shown that kidney problems only occur in people with previous kidney disease. However failure to properly hydrate can put an increased strain on the kidney's ability to function.[45][46] An adequate supply of carbohydrates (5–7 g per kg) is also needed as a source of energy and for the body to restore glycogen levels in muscles.[47]		A light, balanced meal prior to the workout (usually one to two hours beforehand) ensures that adequate energy and amino acids are available for the intense bout of exercise. The type of nutrients consumed affects the response of the body, and nutrient timing whereby protein and carbohydrates are consumed prior to and after workout has a beneficial impact on muscle growth.[48] Water is consumed throughout the course of the workout to prevent poor performance due to dehydration. A protein shake is often consumed immediately[49] following the workout, because both protein uptake and protein usage are increased at this time.[50] Glucose (or another simple sugar) is often consumed as well since this quickly replenishes any glycogen lost during the exercise period. To maximise muscle protein anabolism, recovery drink should contain glucose (dextrose), protein (usually whey) hydrolysate containing mainly dipeptides and tripeptides, and leucine.[51] Some weight trainers also take ergogenic aids such as creatine or steroids to aid muscle growth. However, the effectiveness of some products is disputed and others are potentially harmful.		Due to the androgenic hormonal differences between males and females, the latter are generally unable to develop large muscles regardless of the training program used.[52] Normally the most that can be achieved is a look similar to that of a fitness model. Muscle is denser than fat, so someone who builds muscle while keeping the same body weight will occupy less volume; if two people weigh the same (and are the same height) but have different lean body mass percentages, the one with more muscle will appear thinner.[53]		In addition, though bodybuilding uses the same principles as strength training, it is with a goal of gaining muscle bulk. Strength trainers with different goals and programs will not gain the same mass as a professional bodybuilder.		Some weight trainers perform light, high-repetition exercises in an attempt to "tone" their muscles without increasing their size.		The word tone derives from the Latin "tonus" (meaning "tension"). In anatomy and physiology, as well as medicine, the term "muscle tone" refers to the continuous and passive partial contraction of the muscles, or the muscles' resistance to passive stretching during resting state as determined by a deep tendon reflex. Muscle tonus is dependent on neurological input into the muscle. In medicine, observations of changes in muscle tonus can be used to determine normal or abnormal states which can be indicative of pathology. The common strength training term "tone" is derived from this use.		What muscle builders refer to as a toned physique or "muscle firmness" is one that combines reasonable muscular size with moderate levels of body fat, qualities that may result from a combination of diet and exercise.[54]		Muscle tone or firmness is derived from the increase in actin and myosin cross filaments in the sarcomere. When this occurs the same amount of neurological input creates a greater firmness or tone in the resting continuous and passive partial contraction in the muscle.		Exercises of 6–12 reps cause hypertrophy of the sarcoplasm in slow-twitch and high-twitch muscle fibers, contributing to overall increased muscle bulk. This is not to be confused with myofibril hypertrophy which leads to strength gains. Both however can occur to an extent during this rep range. Even though most are of the opinion that higher repetitions are best for producing the desired effect of muscle firmness or tone, it is not.[citation needed] Low volume strength training of 5 repetitions or fewer will increase strength by increasing actin and myosin cross filaments thereby increasing muscle firmness or tone. The low volume of this training will inhibit the hypertrophy effect.[55]		Lowered-calorie diets have no positive effect on muscle hypertrophy for muscle of any fiber type. They may, however, decrease the thickness of subcutaneous fat (fat between muscle and skin), through an overall reduction in body fat, thus making muscle striations more visible.		Exercises like sit-ups, or abdominal crunches, performs less work than whole-body aerobic exercises[56] thereby expending fewer calories during exercise than jogging, for example.		Hypertrophy serves to maintain muscle mass, for an elevated basal metabolic rate, which has the potential to burn more calories in a given period compared to aerobics. This helps to maintain a higher metabolic rate which would otherwise diminish after metabolic adaption to dieting, or upon completion of an aerobic routine.[57]		Weight loss also depends on the type of strength training used. Weight training is generally used for bulking, but the bulking method will more than likely not increase weight because of the diet involved. However, when resistance or circuit training is used, because they are not geared towards bulking, women tend to lose weight more quickly. Lean muscles require calories to maintain themselves at rest, which will help reduce fat through an increase in the basal metabolic rate.		Until the 20th century, the history of strength training was very similar to the history of weight training. With the advent of modern technology, materials and knowledge, the methods that can be used for strength training have multiplied significantly.		Hippocrates explained the principle behind strength training when he wrote "that which is used develops, and that which is not used wastes away", referring to muscular hypertrophy and atrophy. Progressive resistance training dates back at least to Ancient Greece, when legend has it that wrestler Milo of Croton trained by carrying a newborn calf on his back every day until it was fully grown. Another Greek, the physician Galen, described strength training exercises using the halteres (an early form of dumbbell) in the 2nd century. Ancient Persians used the meels, which became popular during the 19th century as the Indian club, and has recently made a comeback in the form of the clubbell.		The dumbbell was joined by the barbell in the latter half of the 19th century. Early barbells had hollow globes that could be filled with sand or lead shot, but by the end of the century these were replaced by the plate-loading barbell commonly used today.[58]		Strength training with isometric exercise was popularised by Charles Atlas from the 1930s onwards. The 1960s saw the gradual introduction of exercise machines into the still-rare strength training gyms of the time. Strength training became increasingly popular in the 1980s following the release of the bodybuilding movie Pumping Iron and the subsequent popularity of Arnold Schwarzenegger.[59]		Orthopaedic specialists used to recommend that children avoid weight training because the growth plates on their bones might be at risk. The very rare reports of growth plate fractures in children who trained with weights occurred as a result of inadequate supervision, improper form or excess weight, and there have been no reports of injuries to growth plates in youth training programs that followed established guidelines.[60][61] The position of the National Strength and Conditioning Association is that strength training is safe for children if properly designed and supervised.[62]		Younger children are at greater risk of injury than adults if they drop a weight on themselves or perform an exercise incorrectly; further, they may lack understanding of, or ignore the safety precautions around weight training equipment. As a result, supervision of minors is considered vital to ensuring the safety of any youth engaging in strength training.[60][61]		Strength training is the fourth most popular form of fitness in Australia.[63] Due to its popularity amongst all ages, there is great scepticism on what the appropriate age to commence strength training in young athletes is. Some points of the opposing view of strength training in young adolescence are stunted growth, health and bone problems in later stages of life and unhealthy eating habits.[64] Studies by Australian experts that have been recognised by the Australian Institute of Sport (AIS) have debunked these myths. There is no link between any prolonged health risks and strength training in pre-adolescence if the procedures of strength training are followed correctly and under suitable supervision. Strength training for pre-adolescents should focus on skills and techniques. Children should only work on strengthening all the big muscle groups, using free weight and body weight movements with relatively light loads. The benefits of these practices include increased strength performance, injury prevention and learning good training principles.[65]		Older adults are prone to loss of muscle strength.[66][67] With more strength older adults have better health, better quality of life, better physical function[67] and fewer falls.[67] In cases in which an older person begins strength training, their doctor or health care provider may neglect to emphasize a strength training program which results in muscle gains. Under-dosed strength training programs should be avoided in favor of a program which matches the abilities and goals of the person exercising.[68]		In setting up an exercise program for an older adult, they should go through a baseline fitness assessment to determine their current limits. Any exercise program for older adults should match the intensity, frequency, and duration of exercise that the person can perform. The program should have a goal of increased strength as compared to the baseline measurement.[68]		Recommended training for older adults is three times a week of light strength training exercises. Exercise machines are a commonly used equipment in a gym setting, including treadmills with exercises such as walking or light jogging. Home-based exercises should usually consist of body weight or elastic band exercises that maintain a low level of impact on the muscles. Weights can also be used by older adults if they maintain a lighter weight load with an average amount of repetitions (10–12 REPS) with suitable supervision. It is important for older adults to maintain a light level of strength training with low levels of impact to avoid injuries.[69]		Older people who exercise against a resistance or force become stronger .[67] Progressive resistance training (PRT) also improves physical functioning in older people, including the performance of simple (e.g.: walking, climbing stairs, rising from a chair more quickly) and complex daily activities (e.g.: bathing, cooking).[67] Caution is recommended when transferring PRT exercises for clinical populations, as adverse effects are unclear.[67]		|group2 = See also |list2 =		|below = }}		
The menstrual cycle is the regular natural change that occurs in the female reproductive system (specifically the uterus and ovaries) that makes pregnancy possible.[1][2] The cycle is required for the production of ovocytes, and for the preparation of the uterus for pregnancy.[1] Up to 80% of women report having some symptoms during the one to two weeks prior to menstruation.[3] Common symptoms include acne, tender breasts, bloating, feeling tired, irritability and mood changes.[4] These symptoms interfere with normal life and therefore qualify as premenstrual syndrome in 20 to 30% of women. In 3 to 8%, they are severe.[3]		The first period usually begins between twelve and fifteen years of age, a point in time known as menarche.[5] They may occasionally start as early as eight, and this onset may still be normal.[6] The average age of the first period is generally later in the developing world and earlier in developed world. The typical length of time between the first day of one period and the first day of the next is 21 to 45 days in young women and 21 to 35 days in adults (an average of 28 days).[6][7] Menstruation stops occurring after menopause which usually occurs between 45 and 55 years of age.[8] Bleeding usually lasts around 2 to 7 days.[6]		The menstrual cycle is governed by hormonal changes.[6] These changes can be altered by using hormonal birth control to prevent pregnancy.[9] Each cycle can be divided into three phases based on events in the ovary (ovarian cycle) or in the uterus (uterine cycle).[1] The ovarian cycle consists of the follicular phase, ovulation, and luteal phase whereas the uterine cycle is divided into menstruation, proliferative phase, and secretory phase.		Stimulated by gradually increasing amounts of estrogen in the follicular phase, discharges of blood (menses) flow stop, and the lining of the uterus thickens. Follicles in the ovary begin developing under the influence of a complex interplay of hormones, and after several days one or occasionally two become dominant (non-dominant follicles shrink and die). Approximately mid-cycle, 24–36 hours after the luteinizing hormone (LH) surges, the dominant follicle releases an ovocyte, in an event called ovulation. After ovulation, the ovocyte only lives for 24 hours or less without fertilization while the remains of the dominant follicle in the ovary become a corpus luteum; this body has a primary function of producing large amounts of progesterone. Under the influence of progesterone, the uterine lining changes to prepare for potential implantation of an embryo to establish a pregnancy. If implantation does not occur within approximately two weeks, the corpus luteum will involute, causing a sharp drop in levels of both progesterone and estrogen. The hormone drop causes the uterus to shed its lining in a process termed menstruation. Menstruation also occurs in some other animals including shrews, bats, and other primates such as apes and monkeys.[10]		The average age of menarche is 12–15.[5][11] They may occasionally start as early as eight, and this onset may still be normal.[6] This first period often occurs later in the developing world than the developed world.[7]		The average age of menarche is approximately 12.5 years in the United States,[12] 12.7 in Canada,[13] 12.9 in the UK[14] and 13.1 years in Iceland.[15] Factors such as genetics, diet and overall health can affect timing.[16]		The cessation of menstrual cycles at the end of a woman's reproductive period is termed menopause. The average age of menopause in women is 52 years, with anywhere between 45 and 55 being common. Menopause before age 45 is considered premature in industrialised countries.[17] Like the age of menarche, the age of menopause is largely a result of cultural and biological factors;[18] however, illnesses, certain surgeries, or medical treatments may cause menopause to occur earlier than it might have otherwise.[19]		The length of a woman's menstrual cycle typically varies somewhat, with some shorter cycles and some longer cycles. A woman who experiences variations of less than eight days between her longest cycles and shortest cycles is considered to have regular menstrual cycles. It is unusual for a woman to experience cycle length variations of less than four days. Length variation between eight and 20 days is considered as moderately irregular cycles. Variation of 21 days or more between a woman's shortest and longest cycle lengths is considered very irregular. [20]		The average menstrual cycle lasts 28 days. The variability of menstrual cycle lengths is highest for women under 25 years of age and is lowest, that is, most regular, for ages 25 to 39.[21] Subsequently, the variability increases slightly for women aged 40 to 44.[21]		The luteal phase of the menstrual cycle is about the same length in most individuals (mean 14.13 days, SD 1.41 days)[22] whereas the follicular phase tends to show much more variability (log-normally distributed with 95% of individuals having follicular phases between 10.3 and 16.3 days).[23] The follicular phase also seems to get significantly shorter with age (geometric mean 14.2 days in women aged 18–24 vs. 10.4 days in women aged 40–44).[23]		Some women with neurological conditions experience increased activity of their conditions at about the same time during each menstrual cycle. For example, drops in estrogen levels have been known to trigger migraines,[24] especially when the woman who suffers migraines is also taking the birth control pill. Many women with epilepsy have more seizures in a pattern linked to the menstrual cycle; this is called "catamenial epilepsy".[25] Different patterns seem to exist (such as seizures coinciding with the time of menstruation, or coinciding with the time of ovulation), and the frequency with which they occur has not been firmly established. Using one particular definition, one group of scientists found that around one-third of women with intractable partial epilepsy has catamenial epilepsy.[25][26][27] An effect of hormones has been proposed, in which progesterone declines and estrogen increases would trigger seizures.[28] Recently, studies have shown that high doses of estrogen can cause or worsen seizures, whereas high doses of progesterone can act like an antiepileptic drug.[29] Studies by medical journals have found that women experiencing menses are 1.68 times more likely to commit suicide.[30]		Mice have been used as an experimental system to investigate possible mechanisms by which levels of sex steroid hormones might regulate nervous system function. During the part of the mouse estrous cycle when progesterone is highest, the level of nerve-cell GABA receptor subtype delta was high. Since these GABA receptors are inhibitory, nerve cells with more delta receptors are less likely to fire than cells with lower numbers of delta receptors. During the part of the mouse estrous cycle when estrogen levels are higher than progesterone levels, the number of delta receptors decrease, increasing nerve cell activity, in turn increasing anxiety and seizure susceptibility.[31]		Estrogen levels may affect thyroid behavior.[32] For example, during the luteal phase (when estrogen levels are lower), the velocity of blood flow in the thyroid is lower than during the follicular phase (when estrogen levels are higher).[33]		Among women living closely together, it was once thought that the onsets of menstruation tend to synchronize. This effect was first described in 1971, and possibly explained by the action of pheromones in 1998.[34] Subsequent research has called this hypothesis into question.[35]		As a side note, research indicates that women have a significantly higher likelihood of anterior cruciate ligament injuries in the pre-ovulatory stage, than post-ovulatory stage.[36]		The most fertile period (the time with the highest likelihood of pregnancy resulting from sexual intercourse) covers the time from some 5 days before until 1 to 2 days after ovulation.[37] In a 28‑day cycle with a 14‑day luteal phase, this corresponds to the second and the beginning of the third week. A variety of methods have been developed to help individual women estimate the relatively fertile and the relatively infertile days in the cycle; these systems are called fertility awareness.		Fertility awareness methods that rely on cycle length records alone are called calendar-based methods.[38] Methods that require observation of one or more of the three primary fertility signs (basal body temperature, cervical mucus, and cervical position)[39] are known as symptoms-based methods.[38] Urine test kits are available that detect the LH surge that occurs 24 to 36 hours before ovulation; these are known as ovulation predictor kits (OPKs).[40] Computerized devices that interpret basal body temperatures, urinary test results, or changes in saliva are called fertility monitors.		A woman's fertility is also affected by her age.[41] As a woman's total egg supply is formed in fetal life,[42] to be ovulated decades later, it has been suggested that this long lifetime may make the chromatin of eggs more vulnerable to division problems, breakage, and mutation than the chromatin of sperm, which are produced continuously during a man's reproductive life. However, despite this hypothesis, a similar paternal age effect has also been observed.		As measured on women undergoing in vitro fertilization, a longer menstrual cycle length is associated with higher pregnancy and delivery rates, even after age adjustment.[43] Delivery rates after IVF have been estimated to be almost doubled for women with a menstrual cycle length of more than 34 days compared with women with a menstrual cycle length shorter than 26 days.[43] A longer menstrual cycle length is also significantly associated with better ovarian response to gonadotropin stimulation and embryo quality.[43]		The different phases of the menstrual cycle correlate with women’s moods. In some cases, hormones released during the menstrual cycle can cause behavioral changes in females; mild to severe mood changes can occur.[44] The menstrual cycle phase and ovarian hormones may contribute to increased empathy in women. The natural shift of hormone levels during the different phases of the menstrual cycle has been studied in conjunction with test scores. When completing empathy exercises, women in the follicular stage of their menstrual cycle performed better than women in their midluteal phase. A significant correlation between progesterone levels and the ability to accurately recognize emotion was found. Performances on emotion recognition tasks were better when women had lower progesterone levels. Women in the follicular stage showed higher emotion recognition accuracy than their midluteal phase counterparts. Women were found to react more to negative stimuli when in midluteal stage over the women in the follicular stage, perhaps indicating more reactivity to social stress during that menstrual cycle phase.[45] Overall, it has been found that women in the follicular phase demonstrated better performance in tasks that contain empathetic traits.		Fear response in women during two different points in the menstrual cycle has been examined. When estrogen is highest in the preovulatory stage, women are significantly better at identifying expressions of fear than women who were menstruating, which is when estrogen levels are lowest. The women were equally able to identify happy faces, demonstrating that the fear response was a more powerful response. To summarize, menstrual cycle phase and the estrogen levels correlates with women’s fear processing.[46]		However, the examination of daily moods in women with measuring ovarian hormones may indicate a less powerful connection. In comparison to levels of stress or physical health, the ovarian hormones had less of an impact on overall mood.[47] This indicates that while changes of ovarian hormones may influence mood, on a day-to-day level it does not influence mood more than other stressors do.		Females have been found to experience different eating habits at different stages of their menstrual cycle, with food intake being higher during the luteal phase than the follicular phase.[48][49] Food intake increases by approximately 10% during the luteal phase compared to the follicular phase.[49]		Various studies have shown that during the luteal phase woman consume more carbohydrates, proteins and fats and that 24-hour energy expenditure shows increases between 2.5-11.5%.[50] The increasing intake during the luteal phase may be related to higher preferences for sweet and fatty foods, which occurs naturally and is enhanced during the luteal phases of the menstrual cycle.[50] This is due to the higher metabolic demand during this phase.[51] In particular, women tend to show a cravings for chocolate, with higher cravings during the luteal phase.[50]		Females with premenstrual syndrome (PMS) report changes in appetite across the menstrual cycle more than non-sufferers of PMS, possibly due to their oversensitivity to changes in hormone levels.[49] In women with PMS, food intake is higher in the luteal phase than follicular.[52] The remaining symptoms of PMS, including mood changes and physical symptoms, also occur during the luteal phase. No difference for preference of food types has been found between PMS sufferers and non-sufferers.[48]		The different levels of ovarian hormones at different stages of the cycle have been used to explain eating behaviour changes. Progesterone has been shown to promote fat storage, causing a higher intake of fatty foods during the luteal phase when progesterone levels are higher.[49] Additionally, with a high estrogen level dopamine is ineffective in converting to noradrenaline, a hormone which promotes eating, therefore decreasing appetite.[49] In humans, the level of these ovarian hormones during the menstrual cycle have been found to influence binge eating.[53]		It is theorized that the use of birth control pills should affect eating behaviour as they minimise or remove the fluctuations in hormone levels.[48] The neurotransmitter serotonin is also thought to play a role in food intake. Serotonin is responsible for inhibiting eating and controlling meal size,[54] among other things, and is modulated in part by ovarian hormones.[55]		A number of factors affect whether dieting will affect these menstrual processes: age, weight loss and the diet itself. First, younger women are likely to experience menstrual irregularities due to their diet. Second, menstrual abnormalities are more likely with more weight loss. For example, anovulatory cycles can occur as a result of adopting a restricted diet, as well as engaging in a high amount of exercise.[49] Finally, the cycle is affected more by a vegetarian diet compared to a non-vegetarian diet.[56]		Studies investigating effects of the menstrual cycle on alcohol consumption have found mixed evidence.[57] However, some evidence suggests that individuals consume more alcohol during the luteal stage, especially if these individuals are heavy drinkers or have a family history of alcohol abuse.[51]		The level of substance abuse increases with PMS, mostly with addictive substances such as nicotine, tobacco and cocaine.[51] One theory behind this suggests this higher level of substance abuse is due to decreased self-control as a result of the higher metabolic demands during the luteal phase.[51]		Infrequent or irregular ovulation is called oligoovulation.[58] The absence of ovulation is called anovulation. Normal menstrual flow can occur without ovulation preceding it: an anovulatory cycle. In some cycles, follicular development may start but not be completed; nevertheless, estrogens will be formed and stimulate the uterine lining. Anovulatory flow resulting from a very thick endometrium caused by prolonged, continued high estrogen levels is called estrogen breakthrough bleeding. Anovulatory bleeding triggered by a sudden drop in estrogen levels is called withdrawal bleeding.[59] Anovulatory cycles commonly occur before menopause (perimenopause) and in women with polycystic ovary syndrome.[60]		Very little flow (less than 10 ml) is called hypomenorrhea. Regular cycles with intervals of 21 days or fewer are polymenorrhea; frequent but irregular menstruation is known as metrorrhagia. Sudden heavy flows or amounts greater than 80 ml are termed menorrhagia.[61] Heavy menstruation that occurs frequently and irregularly is menometrorrhagia. The term for cycles with intervals exceeding 35 days is oligomenorrhea.[62] Amenorrhea refers to more than three[61] to six[62] months without menses (while not being pregnant) during a woman's reproductive years.		The menstrual cycle can be described by the ovarian or uterine cycle. The ovarian cycle describes changes that occur in the follicles of the ovary whereas the uterine cycle describes changes in the endometrial lining of the uterus. Both cycles can be divided into three phases. The ovarian cycle consists of the follicular phase, ovulation, and the luteal phase whereas the uterine cycle consists of menstruation, proliferative phase, and secretory phase.[1]		The follicular phase is the first part of the ovarian cycle. During this phase, the ovarian follicles mature and get ready to release an egg.[1] The latter part of this phase overlaps with the proliferative phase of the uterine cycle.		Through the influence of a rise in follicle stimulating hormone (FSH) during the first days of the cycle, a few ovarian follicles are stimulated.[63] These follicles, which were present at birth[63] and have been developing for the better part of a year in a process known as folliculogenesis, compete with each other for dominance. Under the influence of several hormones, all but one of these follicles will stop growing, while one dominant follicle in the ovary will continue to maturity. The follicle that reaches maturity is called a tertiary, or Graafian, follicle, and it contains the ovum.[63]		Ovulation is the second phase of the ovarian cycle in which a mature egg is released from the ovarian follicles into the oviduct.[64] During the follicular phase, estradiol suppresses production of luteinizing hormone (LH) from the anterior pituitary gland. When the egg has nearly matured, levels of estradiol reach a threshold above which this effect is reversed and estrogen stimulates the production of a large amount of LH. This process, known as the LH surge, starts around day 12 of the average cycle and may last 48 hours.[65]		The exact mechanism of these opposite responses of LH levels to estradiol is not well understood.[66] In animals, a gonadotropin-releasing hormone (GnRH) surge has been shown to precede the LH surge, suggesting that estrogen's main effect is on the hypothalamus, which controls GnRH secretion.[66] This may be enabled by the presence of two different estrogen receptors in the hypothalamus: estrogen receptor alpha, which is responsible for the negative feedback estradiol-LH loop, and estrogen receptor beta, which is responsible for the positive estradiol-LH relationship.[67] However, in humans it has been shown that high levels of estradiol can provoke abrupt increases in LH, even when GnRH levels and pulse frequencies are held constant,[66] suggesting that estrogen acts directly on the pituitary to provoke the LH surge.		The release of LH matures the egg and weakens the wall of the follicle in the ovary, causing the fully developed follicle to release its secondary oocyte.[63] The secondary oocyte promptly matures into an ootid and then becomes a mature ovum. The mature ovum has a diameter of about 0.2 mm.[68]		Which of the two ovaries—left or right—ovulates appears essentially random; no known left and right co-ordination exists.[69] Occasionally, both ovaries will release an egg;[69] if both eggs are fertilized, the result is fraternal twins.[70]		After being released from the ovary, the egg is swept into the fallopian tube by the fimbria, which is a fringe of tissue at the end of each fallopian tube. After about a day, an unfertilized egg will disintegrate or dissolve in the fallopian tube.[63]		Fertilization by a spermatozoon, when it occurs, usually takes place in the ampulla, the widest section of the fallopian tubes. A fertilized egg immediately begins the process of embryogenesis, or development. The developing embryo takes about three days to reach the uterus and another three days to implant into the endometrium.[63] It has usually reached the blastocyst stage at the time of implantation.		In some women, ovulation features a characteristic pain called mittelschmerz (German term meaning middle pain).[71] The sudden change in hormones at the time of ovulation sometimes also causes light mid-cycle blood flow.[72]		The luteal phase is the final phase of the ovarian cycle and it corresponds to the secretory phase of the uterine cycle. During the luteal phase, the pituitary hormones FSH and LH cause the remaining parts of the dominant follicle to transform into the corpus luteum, which produces progesterone. The increased progesterone in the adrenals starts to induce the production of estrogen. The hormones produced by the corpus luteum also suppress production of the FSH and LH that the corpus luteum needs to maintain itself. Consequently, the level of FSH and LH fall quickly over time, and the corpus luteum subsequently atrophies.[63] Falling levels of progesterone trigger menstruation and the beginning of the next cycle. From the time of ovulation until progesterone withdrawal has caused menstruation to begin, the process typically takes about two weeks, with 14 days considered normal. For an individual woman, the follicular phase often varies in length from cycle to cycle; by contrast, the length of her luteal phase will be fairly consistent from cycle to cycle.[73]		The loss of the corpus luteum is prevented by fertilization of the egg. The syncytiotrophoblast, which is the outer layer of the resulting embryo-containing structure (the blastocyst) and later also becomes the outer layer of the placenta, produces human chorionic gonadotropin (hCG), which is very similar to LH and which preserves the corpus luteum. The corpus luteum can then continue to secrete progesterone to maintain the new pregnancy. Most pregnancy tests look for the presence of hCG.[63]		The uterine cycle has three phases: proliferative, secretory, and menses.[74]		Menstruation (also called menstrual bleeding, menses, catamenia or a period) is the first phase of the uterine cycle. The flow of menses normally serves as a sign that a woman has not become pregnant. (However, this cannot be taken as certainty, as a number of factors can cause bleeding during pregnancy; some factors are specific to early pregnancy, and some can cause heavy flow.)[75][76][77]		Eumenorrhea denotes normal, regular menstruation that lasts for a few days (usually 3 to 5 days, but anywhere from 2 to 7 days is considered normal).[71][78] The average blood loss during menstruation is 35 milliliters with 10–80 ml considered normal.[79] Women who experience Menorrhagia are more susceptible to iron deficiency than the average person.[80] An enzyme called plasmin inhibits clotting in the menstrual fluid.[81]		Painful cramping in the abdomen, back, or upper thighs is common during the first few days of menstruation. Severe uterine pain during menstruation is known as dysmenorrhea, and it is most common among adolescents and younger women (affecting about 67.2% of adolescent females).[82] When menstruation begins, symptoms of premenstrual syndrome (PMS) such as breast tenderness and irritability generally decrease.[71] Many sanitary products are marketed to women for use during their menstruation.		The proliferative phase is the second phase of the uterine cycle when estrogen causes the lining of the uterus to grow, or proliferate, during this time.[63] As they mature, the ovarian follicles secrete increasing amounts of estradiol, and estrogen. The estrogens initiate the formation of a new layer of endometrium in the uterus, histologically identified as the proliferative endometrium. The estrogen also stimulates crypts in the cervix to produce fertile cervical mucus, which may be noticed by women practicing fertility awareness.[83]		The secretory phase is the final phase of the uterine cycle and it corresponds to the luteal phase of the ovarian cycle. During the secretory phase, the corpus luteum produces progesterone, which plays a vital role in making the endometrium receptive to implantation of the blastocyst and supportive of the early pregnancy, by increasing blood flow and uterine secretions and reducing the contractility of the smooth muscle in the uterus;[84] it also has the side effect of raising the woman's basal body temperature.[85]		While some forms of birth control do not affect the menstrual cycle, hormonal contraceptives work by disrupting it. Progestogen negative feedback decreases the pulse frequency of gonadotropin-releasing hormone (GnRH) release by the hypothalamus, which decreases the release of follicle-stimulating hormone (FSH) and luteinizing hormone (LH) by the anterior pituitary. Decreased levels of FSH inhibit follicular development, preventing an increase in estradiol levels. Progestogen negative feedback and the lack of estrogen positive feedback on LH release prevent a mid-cycle LH surge. Inhibition of follicular development and the absence of a LH surge prevent ovulation.[86][87][88]		The degree of ovulation suppression in progestogen-only contraceptives depends on the progestogen activity and dose. Low dose progestogen-only contraceptives—traditional progestogen only pills, subdermal implants Norplant and Jadelle, and intrauterine system Mirena—inhibit ovulation in about 50% of cycles and rely mainly on other effects, such as thickening of cervical mucus, for their contraceptive effectiveness.[89] Intermediate dose progestogen-only contraceptives—the progestogen-only pill Cerazette and the subdermal implant Nexplanon—allow some follicular development but more consistently inhibit ovulation in 97–99% of cycles. The same cervical mucus changes occur as with very low-dose progestogens. High-dose, progestogen-only contraceptives—the injectables Depo-Provera and Noristerat—completely inhibit follicular development and ovulation.[89]		Combined hormonal contraceptives include both an estrogen and a progestogen. Estrogen negative feedback on the anterior pituitary greatly decreases the release of FSH, which makes combined hormonal contraceptives more effective at inhibiting follicular development and preventing ovulation. Estrogen also reduces the incidence of irregular breakthrough bleeding.[86][87][88] Several combined hormonal contraceptives—the pill, NuvaRing, and the contraceptive patch—are usually used in a way that causes regular withdrawal bleeding. In a normal cycle, menstruation occurs when estrogen and progesterone levels drop rapidly.[85] Temporarily discontinuing use of combined hormonal contraceptives (a placebo week, not using patch or ring for a week) has a similar effect of causing the uterine lining to shed. If withdrawal bleeding is not desired, combined hormonal contraceptives may be taken continuously, although this increases the risk of breakthrough bleeding.		Breastfeeding causes negative feedback to occur on pulse secretion of gonadotropin-releasing hormone (GnRH) and luteinizing hormone (LH). Depending on the strength of the negative feedback, breastfeeding women may experience complete suppression of follicular development, but no ovulation, or normal menstrual cycle may resume.[90] Suppression of ovulation is more likely when suckling occurs more frequently.[91] The production of prolactin in response to suckling is important to maintaining lactational amenorrhea.[92] On average, women who are fully breastfeeding whose infants suckle frequently experience a return of menstruation at fourteen and a half months postpartum. There is a wide range of response among individual breastfeeding women, however, with some experiencing return of menstruation at two months and others remaining amenorrheic for up to 42 months postpartum.[93]		The word "menstruation" is etymologically related to "moon". The terms "menstruation" and "menses" are derived from the Latin mensis (month), which in turn relates to the Greek mene (moon) and to the roots of the English words month and moon.[94]		Even though the average length of the human menstrual cycle is similar to that of the lunar cycle, in modern humans there is no relation between the two.[95] The relationship is believed to be a coincidence.[96][97] Light exposure does not appear to affect the menstrual cycle in humans.[10] A meta-analysis of studies from 1996 showed no correlation between the human menstrual cycle and the lunar cycle.[98][99][100]		Dogon villagers did not have electric lighting and spent most nights outdoors, talking and sleeping; so they were apparently an ideal population for detecting a lunar influence; none was found.[101] Other scholars counter, however, that the Dogon — unlike ancestral African hunter-gatherer populations — are polygamous, meaning that reproductive synchrony would not be expected on theoretical grounds.[102]		In a number of countries, mainly in Asia, legislation or corporate practice has introduced formal menstrual leave to provide women with either paid or unpaid leave of absence from their employment while they are menstruating.[103] Countries with policies include Japan, Taiwan, Indonesia, and South Korea.[103] The practice is controversial due to concerns that it bolsters the perception of women as weak, inefficient workers,[103] as well as concerns that it is unfair to men.[104][105]		
The CrossFit Games is an athletic competition sponsored by Crossfit Inc.[1] and Reebok.[2] The competition has been held every summer since 2007. Athletes at the Games compete in workouts that they learn about hours or days beforehand, consisting mostly of an assortment of standard aerobic, weightlifting, and gymnastics movements, as well as some additional surprise elements that are not part of the typical CrossFit regimen such as obstacle courses, ocean swimming, softball throwing, or ascending a pegboard.[3][4] The Games are styled as a venue for determining the "Fittest on Earth," where competitors should be "ready for anything."[citation needed]						In 2007, the first annual CrossFit Games were contested in Aromas, California, on a small ranch owned by the family of Games director Dave Castro.[5] Interest and participation in the event grew in the following years, and in 2010, the Games moved to a new venue: the StubHub Center (known at that time as the Home Depot Center) in Carson, California.[6] Following seven years at that site, the Games moved to the Alliant Energy Center in Madison, Wisconsin, where the first Games outside California will take place in summer 2017.[7]		The CrossFit Games season comprises three stages of competition: the Open, Regionals, and the Games themselves.		The Open, introduced in 2011 and so called because participation is open to anyone,[8] is held over five weeks in February-March; a new workout is released on each Thursday night (Pacific Time) and competitors complete the workout and submit their scores online by Monday evening, with either a video or validation by a CrossFit affiliate. Since 2013, Open workout announcements have been broadcast live, and featured two or more past CrossFit Games athletes competing head-to-head immediately following the workout description.		Each Open competitor is categorized into one of 17 regions according to primary training location; North America is divided into 12 regions, with the remaining regions roughly corresponding the five other populated continents. After all five Open workouts, the overall performance of competitors within each region is ranked, and the top few athletes (currently 10, 20, or 30 depending on the region) advance to the next stage: Regionals.[9] In 2015, the format changed from 17 regional events (one for each region) to eight. Each "super-regional" event includes qualifiers from two or three regions, totaling 40 or 50 athletes. Regional events last three days and are held two or three per week over three consecutive weekends in late (boreal) spring; the workouts are the same for all regional events.		The top five athletes from each regional event advance to the CrossFit Games, which are held over three days in July or August. The men's and women's events each consist of 40 competitors vying for 3 podium positions and approximately $750,000 in total prize money (the overall prize purse of $2.2 million, as of 2016, includes payouts to other divisions).		The marquee events at the CrossFit Games are the men's and women's individual competitions. The first place prize for each currently stands at $285,000.		Teams consist of three men and three women, who must all primarily train at the same facility. Teams are subject to a similar qualification process as the individuals.		The Games include age-based divisions for younger and older competitors. Masters divisions were introduced at the 2010 Games. There are currently six divisions each for women and men: 35–39, 40–44, 45–49, 50–54, 55–59, and 60+. Divisions for teenagers were introduced in 2015: the age ranges are 14–15 and 16–17, for both boys and girls. Rather than regional events, masters and teen athletes qualify for the games by a second online competition following the Open. The top 200 athletes in each division worldwide are invited to compete in this qualifier, of which the top 20 advance to the Games.[10] Prior to the introduction of these secondary online qualifiers, masters and teens competitors qualified for the Games directly from the Open.		Due to CrossFit's official partnership with Reebok, competitors at the 2015 Games were banned from wearing Nike footwear.[11] Nike arranged for several trucks to be parked near the main entrance to the arena, which served as mobile billboards with the slogan "Don't ban our shoe, beat our shoe".[12] The partnership also prohibits Nike from labeling its Metcon shoes as intended for CrossFit - the brand uses the term "high intensity training" instead.[11]		CrossFit's decision to award winners of the 2016 Games with handguns resulted in widespread criticism from members and sponsors.[13] Resulting protests forced the temporary closure of two CrossFit locations in New York City.[14]		Participation and sponsorship have grown rapidly since the inception of the Games. The prize money awarded to each first-place male and female increased from $500 at the inaugural Games to $275,000 in 2013-2016. The largest jump in prize money came from the first Games sponsored by Reebok in 2011 when first place went from $25,000 in 2010 to $250,000 in 2011.[15] The total prize payout in 2016 was $2,200,000.[16]		In 2011, 26,000 athletes signed up to compete in the "Open". In 2012–2017, participation was 69,000, 138,000, 209,000, 273,000, 324,307, and 380,000 respectively.[17][18][19][20][21] In 2016, 175 countries were represented by registered participants.		Individual and Team Champions[22]		Masters Men's Champions[22]		Masters Women's Champions[22]		Teens Champions[22]		
Human swimming is the self-propulsion of a person through water or another liquid, usually for recreation, sport, exercise, or survival. Locomotion is achieved through coordinated movement of the limbs, the body, or both. Humans can hold their breath underwater and undertake rudimentary locomotive swimming within weeks of birth, as an evolutionary response.[1]		Swimming is consistently among top public recreational activities,[2][3][4][5] and in some countries, swimming lessons are a compulsory part of the educational curriculum.[6] As a formalized sport, swimming features in a range of local, national, and international competitions, including every modern summer Olympics, which takes place every four years.						Swimming relies on the natural buoyancy of the human body. On average, the body has a relative density of 0.98 compared to water, which causes the body to float. However, buoyancy varies on the basis of both body composition and the salinity of the water. Higher levels of body fat and saltier water both lower the relative density of the body and increase its buoyancy.		Since the human body is only slightly less dense than water, water supports the weight of the body during swimming. As a result, swimming is “low-impact” compared to land activities such as running. The density and viscosity of water also create resistance for objects moving through the water. Swimming strokes use this resistance to create propulsion, but this same resistance also generates drag on the body.		Hydrodynamics is important to stroke technique for swimming faster, and swimmers who want to swim faster or tire less try to reduce the drag of the body's motion through the water. To be more hydrodynamic, swimmers can either increase the power of their strokes or reduce water resistance, though power must increase by a factor of three to achieve the same effect as reducing resistance.[7] Efficient swimming by reducing water resistance involves a horizontal water position, rolling the body to reduce the breadth of the body in the water, and extending the arms as far as possible to reduce wave resistance.[7]		Just before plunging into the pool, swimmers may perform exercises such as squatting. Squatting helps in enhancing a swimmer’s start by warming up the thigh muscles.[8]		Human babies demonstrate an innate swimming or diving reflex from newborn until the age of approximately 6 months.[9] Other mammals also demonstrate this phenomenon (see mammalian diving reflex). The diving response involves apnea, reflex bradycardia, and peripheral vasoconstriction; in other words, babies immersed in water spontaneously hold their breath, slow their heart rate, and reduce blood circulation to the extremities (fingers and toes).[9]		Swimming can be undertaken using a wide range of styles, known as 'strokes,' and these strokes are used for different purposes, or to distinguish between classes in competitive swimming. It is not necessary to use a defined stroke for propulsion through the water, and untrained swimmers may use a 'doggy paddle' of arm and leg movements, similar to the way four-legged animals swim.		There are four main strokes used in competition and recreation swimming: the front crawl, also known as freestyle, the breaststroke, the backstroke and the butterfly. Competitive swimming in Europe started around 1800, mostly using the breaststroke. In 1873, John Arthur Trudgen introduced the trudgen to Western swimming competitions.[10] The butterfly stroke developed in the 1930s, and was considered a variant of the breaststroke until accepted as a separate style in 1953.[11] Butterfly is also known as the hardest stroke to many, but it burns the most calories compared to the other 3 strokes.[citation needed]		Other strokes exist for specific purposes, such as training or rescue, and it is also possible to adapt strokes to avoid using parts of the body, either to isolate certain body parts, such as swimming with arms only or legs only to train them harder, or for use by amputees or those affected by paralysis .		Swimming has been recorded since prehistoric times, and the earliest records of swimming date back to Stone Age paintings from around 7,000 years ago. Written references date from 2000 BC. Some of the earliest references include the Epic of Gilgamesh, the Iliad, the Odyssey, the Bible (Ezekiel 47:5, Acts 27:42, Isaiah 25:11), Beowulf, and other sagas.		The coastal tribes living in the volatile Low Countries were known as excellent swimmers by the Romans. Men and horses of the Batavi tribe could cross the Rhine without losing formation, according to Tacitus. Dio Cassius describes one surprise tactic employed by Aulus Plautius against the Celts at the Battle of the Medway:[12]		The [British Celts] thought that Romans would not be able to cross it without a bridge, and consequently bivouacked in rather careless fashion on the opposite bank; but he sent across a detachment of [Batavii], who were accustomed to swim easily in full armour across the most turbulent streams. . . . Thence the Britons retired to the river Thames at a point near where it empties into the ocean and at flood-tide forms a lake. This they easily crossed because they knew where the firm ground and the easy passages in this region were to be found; but the Romans in attempting to follow them were not so successful. However, the [Batavii] swam across again and some others got over by a bridge a little way up-stream, after which they assailed the barbarians from several sides at once and cut down many of them."		In 1538, Nikolaus Wynmann, a German professor of languages, wrote the first swimming book, The Swimmer or A Dialogue on the Art of Swimming (Der Schwimmer oder ein Zweigespräch über die Schwimmkunst).		There are many reasons why people swim, from swimming as a recreational pursuit to swimming as a necessary part of a job or other activity. Swimming may also be used to rehabilitate injuries, especially various cardiovascular injuries and muscle injuries.		Many swimmers swim for recreation, with swimming consistently ranking as one of the physical activities people are most likely to take part in. Recreational swimming can also be used for exercise, relaxation, or rehabilitation.[13] The support of the water, and the reduction in impact, makes swimming accessible for people who are unable to undertake activities such as running.		Swimming is primarily a cardiovascular/aerobic exercise[14] due to the long exercise time, requiring a constant oxygen supply to the muscles, except for short sprints where the muscles work anaerobically. As with most aerobic exercise, swimming is believed to reduce the harmful effects of stress. Swimming is also effective in improving health for people with cardiovascular problems and chronic illnesses. It is proven to positively impact the mental health of pregnant women and mothers. Swimming can even improve mood.[15]		As of 2013, the Americans with Disabilities Act requires that swimming pools in the United States be accessible to disabled swimmers.[16]		"Water-based exercise can benefit older adults by improving quality of life and decreasing disability. It also improves or maintains the bone health of post-menopausal women."[17]		Swimming as a sport predominantly involves participants competing to be the fastest over a given distance. Competitors swim different distances in different levels of competition. For example, swimming has been an Olympic sport since 1896, and the current program includes events from 50 m to 1500 m in length, across all four main strokes and medley.		The sport is governed internationally by the Fédération Internationale de Natation (FINA), and competition pools for FINA events are 25 or 50 meters in length. In the United States, a pool 25 yards in length is commonly used for competition.		Other swimming and water-related sporting disciplines include diving, synchronized swimming, water polo, triathlon, and the modern pentathlon.		Some occupations require workers to swim. For example, abalone and pearl diving, and spearfishing.		Swimming is used to rescue people in the water who are in distress, including exhausted swimmers, non-swimmers who have accidentally entered the water, and others who have come to harm on the water. Lifeguards or volunteer lifesavers are deployed at many pools and beaches worldwide to fulfill this purpose, and they, as well as rescue swimmers, may use specific swimming styles for rescue purposes.		Swimming is also used in marine biology to observe plants and animals in their natural habitat. Other sciences use swimming, for example Konrad Lorenz swam with geese as part of his studies of animal behavior.		Swimming also has military purposes. Military swimming is usually done by special operation forces, such as Navy SEALs and US Army Special Forces. Swimming is used to approach a location, gather intelligence, engage in sabotage or combat, and subsequently depart. This may also include airborne insertion into water or exiting a submarine while it is submerged. Due to regular exposure to large bodies of water, all recruits in the United States Navy, Marine Corps, and Coast Guard are required to complete basic swimming or water survival training.		Swimming is also a professional sport. Companies sponsor swimmers who have the skills to compete at the international level. Many swimmers compete competitively to represent their home country in the Olympics. Professional swimmers may also earn a living as entertainers, performing in water ballets.		Locomotion by swimming over brief distances is frequent when alternatives are precluded. There have been cases of political refugees swimming in the Baltic Sea[18] and of people jumping in the water and swimming ashore from vessels not intended to reach land where they planned to go.[19] Swimming travel is central to the plot of the motion picture "Welcome". US president John F. Kennedy led his sailors swimming island to island after his torpedo boat was sunk in World War II, and his senator brother Ted Kennedy claimed to have left Chappaquiddick Island by swimming.[citation needed]		There are many risks associated with voluntary or involuntary human presence in water, which may result in death directly or through drowning asphyxiation. Swimming is both the goal of much voluntary presence, and the prime means of regaining land in accidental situations.		Most recorded water deaths fall into these categories:		Adverse effects of swimming can include:		Around any pool area, safety equipment is often important,[21] and is a zoning requirement for most residential pools in the United States.[22] Supervision by personnel trained in rescue techniques is required at most competitive swimming meets and public pools.		Traditionally, children were considered not able to swim independently until 4 years of age,[23] although now infant swimming lessons are recommended to prevent drowning.[24]		In Sweden, Denmark, Norway, Estonia and Finland, the curriculum for the fifth grade (fourth grade in Estonia) states that all children should learn how to swim as well as how to handle emergencies near water. Most commonly, children are expected to be able to swim 200 metres (660 ft)—of which at least 50 metres (160 ft) on their back – after first falling into deep water and getting their head under water. Even though about 95 percent of Swedish school children know how to swim, drowning remains the third most common cause of death among children.[citation needed]		In both the Netherlands and Belgium swimming lessons under school time (schoolzwemmen, school swimming) are supported by the government. Most schools provide swimming lessons. There is a long tradition of swimming lessons in the Netherlands and Belgium, the Dutch translation for the breaststroke swimming style is even schoolslag (schoolstroke). In France, swimming is a compulsory part of the curriculum for primary schools. Children usually spend one semester per year learning swimming during CE1/CE2/CM1 (2nd, 3rd and 4th grade).		In many places, swimming lessons are provided by local swimming pools, both those run by the local authority and by private leisure companies. Many schools also include swimming lessons into their Physical Education curricula, provided either in the schools' own pool, or in the nearest public pool.		In the UK, the "Top-ups scheme" calls for school children who cannot swim by the age of 11 to receive intensive daily lessons. Children who have not reached Great Britain's National Curriculum standard of swimming 25 metres by the time they leave primary school receive a half-hour lesson every day for two weeks during term-time.[25]		In Canada and Mexico there has been a call to include swimming in public school curriculum.[26]		In the United States there is the Infant Swimming Resource (ISR)[27] initiative that provides lessons for infant children, to cope with an emergency where they have fallen into water. They are taught how to roll-back-to-float (hold their breath underwater, to roll onto their back, to float unassisted, rest and breathe until help arrives).		Standard everyday clothing is usually impractical for swimming and is unsafe under some circumstances. Most cultures today expect swimmers to wear swimsuits.		Men's swimsuits commonly resemble shorts, or briefs. Casual men's swimsuits (for example, boardshorts) are rarely skintight, unlike competitive swimwear, like jammers or diveskins. In most cases, boys and men swim with their upper body exposed, except in countries where custom or law prohibits it in a public setting, or for practical reasons such as sun protection.		Modern women's swimsuits are generally skintight, covering the pubic region and the breasts (See bikini). Women's swimwear may also cover the midriff as well. Women's swimwear is often a fashion statement, and whether it is modest or not is a subject of debate by many groups, religious and secular.		Competitive swimwear is built so that the wearer can swim faster and more efficiently. Modern competitive swimwear is skintight and lightweight. There are many kinds of competitive swimwear for each gender. It is used in aquatic competitions, such as water polo, swim racing, diving, and rowing.		Wetsuits provide both thermal insulation and flotation. Many swimmers lack buoyancy in the leg. The wetsuit reduces density, and therefore improves buoyancy while swimming. It provides insulation by absorbing some of the surrounding water, which then heats up when in direct contact with skin. The wetsuit is the usual choice for those who swim in cold water for long periods of time, as it reduces susceptibility to hypothermia.		Some people also choose to wear no clothing while swimming. This is known as skinny dipping. It was common for males to swim naked in a public setting up to the early 20th century. Today, skinny dipping can be a rebellious activity, or merely a casual one.		|group2 = See also |list2 =		|below = }}		
Aerobic exercise (also known as cardio) is physical exercise of low to high intensity that depends primarily on the aerobic energy-generating process.[1] Aerobic literally means "relating to, involving, or requiring free oxygen",[2] and refers to the use of oxygen to adequately meet energy demands during exercise via aerobic metabolism.[3] Generally, light-to-moderate intensity activities that are sufficiently supported by aerobic metabolism can be performed for extended periods of time.[1]		When practiced in this way, examples of cardiovascular/aerobic exercise are medium to long distance running/jogging, swimming, cycling, and walking, according to the first extensive research on aerobic exercise, conducted in the 1960s on over 5,000 U.S. Air Force personnel by Dr. Kenneth H. Cooper.[4][5]						Kenneth Cooper was the first person to introduce the concept of aerobic exercise. In the 1960s, Cooper started research into preventive medicine. He became intrigued by the belief that exercise can preserve one's health. In 1970 he created his own institute (the Cooper Institute) for non-profit research and education devoted to preventive medicine. He sparked millions into becoming active and is now known as the "father of aerobics".[citation needed]		Aerobic exercise and fitness can be contrasted with anaerobic exercise, of which strength training and short-distance running are the most salient examples. The two types of exercise differ by the duration and intensity of muscular contractions involved, as well as by how energy is generated within the muscle.[6]		New research on the endocrine functions of contracting muscles has shown that both aerobic and anaerobic exercise promote the secretion of myokines, with attendant benefits including growth of new tissue, tissue repair, and various anti-inflammatory functions, which in turn reduce the risk of developing various inflammatory diseases. Myokine secretion in turn is dependent on the amount of muscle contracted, and the duration and intensity of contraction. As such, both types of exercise produce endocrine benefits.[7]		In almost all conditions, anaerobic exercise is accompanied by aerobic exercises because the less efficient anaerobic metabolism must supplement the aerobic system due to energy demands that exceed the aerobic system's capacity. What is generally called aerobic exercise might be better termed "solely aerobic", because it is designed to be low-intensity enough not to generate lactate via pyruvate fermentation, so that all carbohydrate is aerobically turned into energy.		Initially during increased exertion, muscle glycogen is broken down to produce glucose, which undergoes glycolysis producing pyruvate which then reacts with oxygen (Krebs cycle, Chemiosmosis) to produce carbon dioxide and water and releases energy. If there is a shortage of oxygen (anaerobic exercise, explosive movements), carbohydrate is consumed more rapidly because the pyruvate ferments into lactate. If the intensity of the exercise exceeds the rate with which the cardiovascular system can supply muscles with oxygen, it results in buildup of lactate and quickly makes it impossible to continue the exercise. Unpleasant effects of lactate buildup initially include the burning sensation in the muscles, and may eventually include nausea and even vomiting if the exercise is continued without allowing lactate to clear from the bloodstream.		As glycogen levels in the muscle begin to fall, glucose is released into the bloodstream by the liver, and fat metabolism is increased so that it can fuel the aerobic pathways. Aerobic exercise may be fueled by glycogen reserves, fat reserves, or a combination of both, depending on the intensity. Prolonged moderate-level aerobic exercise at 65% VO2 max (the heart rate of 150 bpm for a 30-year-old human) results in the maximum contribution of fat to the total energy expenditure. At this level, fat may contribute 40% to 60% of total, depending on the duration of the exercise. Vigorous exercise above 75% VO2max (160 bpm) primarily burns glycogen.[8][9]		Major muscles in a rested, untrained human typically contain enough energy for about 2 hours of vigorous exercise. Exhaustion of glycogen is a major cause of what marathon runners call "hitting the wall". Training, lower intensity levels, and carbohydrate loading may allow postponement of the onset of exhaustion beyond 4 hours.[9]		Aerobic exercise comprises innumerable forms. In general, it is performed at a moderate level of intensity over a relatively long period of time. For example, running a long distance at a moderate pace is an aerobic exercise, but sprinting is not. Playing singles tennis, with near-continuous motion, is generally considered aerobic activity, while golf or two person team tennis, with brief bursts of activity punctuated by more frequent breaks, may not be predominantly aerobic. Some sports are thus inherently "aerobic", while other aerobic exercises, such as fartlek training or aerobic dance classes, are designed specifically to improve aerobic capacity and fitness. It is most common for aerobic exercises to involve the leg muscles, primarily or exclusively. There are some exceptions. For example, rowing to distances of 2,000 m or more is an aerobic sport that exercises several major muscle groups, including those of the legs, abdominals, chest, and arms. Common kettlebell exercises combine aerobic and anaerobic aspects.		Among the recognized benefits of doing regular aerobic exercise are:[10]		As a result, aerobic exercise can reduce the risk of death due to cardiovascular problems. In addition, high-impact aerobic activities (such as jogging or using a skipping rope) can stimulate bone growth, as well as reduce the risk of osteoporosis for both men and women.		In addition to the health benefits of aerobic exercise, there are numerous performance benefits:		Some drawbacks of aerobic exercise include:		Both the health benefits and the performance benefits, or "training effect", require a minimum duration and frequency of exercise. Most authorities suggest at least twenty minutes performed at least three times per week.[13]		Cooper himself defines aerobic exercise as the ability to utilise the maximum amount of oxygen during exhaustive work. Cooper describes some of the major health benefits of aerobic exercise, such as gaining more efficient lungs by maximising breathing capacity, thereby increasing ability to ventilate more air in a shorter period of time. As breathing capacity increases, one is able to extract oxygen more quickly into the blood stream, increasing elimination of carbon dioxide. With aerobic exercise the heart becomes more efficient at functioning, and blood volume, hemoglobin and red blood cells increase, enhancing the ability of the body to transport oxygen from the lungs into the blood and muscles. Metabolism will change and enable consumption of more calories without putting on weight. Aerobic exercise can delay osteoporosis as there is an increase in muscle mass, a loss of fat and an increase in bone density. With these variables increasing, there is a decrease in likelihood of diabetes as muscles use sugars better than fat. One of the major benefits of aerobic exercise is that body weight may decrease slowly; it will only decrease at a rapid pace if there is a calorie restriction, therefore reducing obesity rates.[14]		Aerobic capacity describes the functional capacity of the cardiorespiratory system, (the heart, lungs and blood vessels). Aerobic capacity refers to the maximum amount of oxygen consumed by the body during intense exercises, in a given time frame.[15] It is a function both of cardiorespiratory performance and the maximum ability to remove and utilize oxygen from circulating blood. To measure maximal aerobic capacity, an exercise physiologist or physician will perform a VO2 max test, in which a subject will undergo progressively more strenuous exercise on a treadmill, from an easy walk through to exhaustion. The individual is typically connected to a respirometer to measure oxygen consumption, and the speed is increased incrementally over a fixed duration of time. The higher the measured cardiorespiratory endurance level, the more oxygen has been transported to and used by exercising muscles, and the higher the level of intensity at which the individual can exercise. More simply put, the higher the aerobic capacity, the higher the level of aerobic fitness. The Cooper and multi-stage fitness tests can also be used to assess functional aerobic capacity for particular jobs or activities.		The degree to which aerobic capacity can be improved by exercise varies very widely in the human population: while the average response to training is an approximately 17% increase in VO2max, in any population there are "high responders" who may as much as double their capacity, and "low responders" who will see little or no benefit from training.[16] Studies indicate that approximately 10% of otherwise healthy individuals cannot improve their aerobic capacity with exercise at all.[17] The degree of an individual's responsiveness is highly heritable, suggesting that this trait is genetically determined.[16]		Obesity in Australia is becoming a huge issue. With one in four Australians over weight. Obesity can be deadly as it increases the risk of coronary heart disease, type 2 diabetes and stroke. In Australia it is proven that nearly 40% of males and 60% of females do not do enough physical activity a day. Introducing aerobic exercise to a daily routine would benefit the body and reduce the risk of cardiovascular disease. The Australian Heart Foundation guidelines outline that exercise to reduce fat should involve continuous moderate aerobic exercise. Continuous moderate exercise is easily accessible and should be performed for at least 30 minutes five times a week. This will reduce obesity by 19% versus no activity at all.[18]		Higher intensity exercise, such as High-intensity interval training (HIIT), increases the resting metabolic rate (RMR) in the 24 hours following high intensity exercise,[19] ultimately burning more calories than lower intensity exercise; low intensity exercise burns more calories during the exercise, due to the increased duration, but fewer afterwards.		Aerobic exercise has long been a popular approach to achieving weight loss and physical fitness, often taking a commercial form.		
Cycling, also called bicycling or biking, is the use of bicycles for transport, recreation, exercise or sport.[1] Persons engaged in cycling are referred to as "cyclists",[2] "bikers",[3] or less commonly, as "bicyclists".[4] Apart from two-wheeled bicycles, "cycling" also includes the riding of unicycles, tricycles, quadracycles, recumbent and similar human-powered vehicles (HPVs).		Bicycles were introduced in the 19th century and now number approximately one billion worldwide.[5] They are the principal means of transportation in many parts of the world.		Cycling is widely regarded as a very effective and efficient mode of transportation[6][7] optimal for short to moderate distances.		Bicycles provide numerous benefits in comparison with motor vehicles, including the sustained physical exercise involved in cycling, easier parking, increased maneuverability, and access to roads, bike paths and rural trails. Cycling also offers a reduced consumption of fossil fuels, less air or noise pollution, and much reduced traffic congestion. These lead to less financial cost to the user as well as to society at large (negligible damage to roads, less road area required).[8] By fitting bicycle racks on the front of buses, transit agencies can significantly increase the areas they can serve.[9]		Among the disadvantages of cycling are the requirement of bicycles (excepting tricycles or quadracycles) to be balanced by the rider in order to remain upright, the reduced protection in crashes in comparison to motor vehicles,[10] often longer travel time (except in densely populated areas), vulnerability to weather conditions, difficulty in transporting passengers, and the fact that a basic level of fitness is required for cycling moderate to long distances.						In many countries, the most commonly used vehicle for road transport is a utility bicycle. These have frames with relaxed geometry, protecting the rider from shocks of the road and easing steering at low speeds. Utility bicycles tend to be equipped with accessories such as mudguards, pannier racks and lights, which extends their usefulness on a daily basis. As the bicycle is so effective as a means of transportation various companies have developed methods of carrying anything from the weekly shop to children on bicycles. Certain countries rely heavily on bicycles and their culture has developed around the bicycle as a primary form of transport. In Europe, Denmark and the Netherlands have the most bicycles per capita and most often use bicycles for everyday transport.[11][12]		Road bikes tend to have a more upright shape and a shorter wheelbase, which make the bike more mobile but harder to ride slowly. The design, coupled with low or dropped handlebars, requires the rider to bend forward more, making use of stronger muscles (particularly the gluteus maximus) and reducing air resistance at high speed.		The price of a new bicycle can range from US$50 to more than US$20,000 (the highest priced bike in the world is the custom Madone by Damien Hirst, sold at $500,000 USD[13]),[14] depending on quality, type and weight (the most exotic road bicycles can weigh as little as 3.2 kg (7 lb)[15]). However, UCI regulations stipulate a legal race bike cannot weigh less than 6.8 kg (14.99 lbs). Being measured for a bike and taking it for a test ride are recommended before buying.		The drivetrain components of the bike should also be considered. A middle grade dérailleur is sufficient for a beginner, although many utility bikes are equipped with hub gears. If the rider plans a significant amount of hillclimbing a triple-chainrings crankset gear system may be preferred. Otherwise, the relatively lighter and less expensive double chainring may be better. Much simpler fixed wheel bikes are also available.		Many road bikes, along with mountain bikes, include clipless pedals to which special shoes attach, via a cleat, enabling the rider to pull on the pedals as well as push. Other possible accessories for the bicycle include front and rear lights, bells or horns, child carrying seats, cycling computers with GPS, locks, bar tape, fenders (mud-guards), baggage racks, baggage carriers and pannier bags, water bottles and bottle cages.		For basic maintenance and repairs cyclists can carry a pump (or a CO2 cartridge), a puncture repair kit, a spare inner tube, and tire levers and a set of allen keys. Cycling can be more efficient and comfortable with special shoes, gloves, and shorts. In wet weather, riding can be more tolerable with waterproof clothes, such as cape, jacket, trousers (pants) and overshoes and high-visibility clothing is advisable to reduce the risk from motor vehicle users.		Items legally required in some jurisdictions, or voluntarily adopted for safety reasons, include bicycle helmets, generator or battery operated lights, reflectors, and audible signalling devices such as a bell or horn. Extras include studded tires and a bicycle computer.		Bikes can also be heavily customized, with different seat designs and handle bars, for example.		Many schools and police departments run educational programs to instruct children in bicycle handling skills and introduce them to the rules of the road as they apply to cyclists. In different countries these may be known as bicycle rodeos or operated as schemes such as Bikeability. Education for adult cyclists is available from organizations such as the League of American Bicyclists.		Beyond simply riding, another skill is riding efficiently and safely in traffic. One popular approach to riding in motor vehicle traffic is vehicular cycling, occupying road space as car does. Alternately, in countries such as Denmark and the Netherlands, where cycling is popular, cyclists are sometimes segregated into bike lanes at the side of, or separate from, main highways and roads. Many primary schools participate in the national road test in which children individually complete a circuit on roads near the school while being observed by testers.		Cyclists, pedestrians and motorists make different demands on road design which may lead to conflicts. Some jurisdictions give priority to motorized traffic, for example setting up one-way street systems, free-right turns, high capacity roundabouts, and slip roads. Others share priority with cyclists so as to encourage more cycling by applying varying combinations of traffic calming measures to limit the impact of motorized transport, and by building bike lanes, bike paths and cycle tracks.		In jurisdictions where motor vehicles were given priority, cycling has tended to decline while in jurisdictions where cycling infrastructure was built, cycling rates have remained steady or increased. Occasionally, extreme measures against cycling may occur. In Shanghai, where bicycles were once the dominant mode of transport, bicycle travel on a few city roads was banned temporarily in December 2003.[16]		In areas in which cycling is popular and encouraged, cycle-parking facilities using bicycle stands, lockable mini-garages, and patrolled cycle parks are used in order to reduce theft. Local governments promote cycling by permitting bicycles to be carried on public transport or by providing external attachment devices on public transport vehicles. Conversely, an absence of secure cycle-parking is a recurring complaint by cyclists from cities with low modal share of cycling.		Extensive cycling infrastructure may be found in some cities. Such dedicated paths in some cities often have to be shared with in-line skaters, scooters, skateboarders, and pedestrians. Dedicated cycling infrastructure is treated differently in the law of every jurisdiction, including the question of liability of users in a collision. There is also some debate about the safety of the various types of separated facilities.		Bicycles are considered a sustainable mode of transport, especially suited for urban use and relatively shorter distances when used for transport (compared to recreation). Case studies and good practices (from European cities and some worldwide examples) that promote and stimulate this kind of functional cycling in cities can be found at Eltis, Europe's portal for local transport.		A number of cities, including Paris, London and Barcelona, now have successful bike hire schemes designed to help people cycle in the city. Typically these feature utilitarian city bikes which lock into docking stations, released on payment for set time periods. Costs vary from city to city. In London, initial hire access costs £2 per day. The first 30 minutes of each trip is free, with £2 for each additional 30 minutes until the bicycle is returned.[17]		Utility cycling refers both to cycling as a mode of daily commuting transport as well as the use of a bicycle in a commercial activity, mainly to transport goods, mostly accomplished in an urban environment.		The postal services of many countries have long relied on bicycles. The British Royal Mail first started using bicycles in 1880; now bicycle delivery fleets include 37,000 in the UK, 25,700 in Germany, 10,500 in Hungary and 7000 in Sweden. In Australia, Australia Post has also reintroduced bicycle postal deliveries on some routes due to an inability to recruit sufficient licensed riders willing to use their uncomfortable motorbikes. The London Ambulance Service has recently introduced bicycling paramedics, who can often get to the scene of an incident in Central London more quickly than a motorized ambulance.[18]		The use of bicycles by police has been increasing, since they provide greater accessibility to bicycle and pedestrian zones and allow access when roads are congested.[19]		Bicycles enjoy substantial use as general delivery vehicles in many countries. In the UK and North America, as their first jobs, generations of teenagers have worked at delivering newspapers by bicycle. London has many delivery companies that use bicycles with trailers. Most cities in the West, and many outside it, support a sizeable and visible industry of cycle couriers who deliver documents and small packages. In India, many of Mumbai's Dabbawalas use bicycles to deliver home cooked lunches to the city’s workers. In Bogotá, Colombia the city’s largest bakery recently replaced most of its delivery trucks with bicycles. Even the car industry uses bicycles. At the huge Mercedes-Benz factory in Sindelfingen, Germany workers use bicycles, color-coded by department, to move around the factory.[citation needed]		Bicycles are used for recreation at all ages. Bicycle touring, also known as cyclotourism, involves touring and exploration or sightseeing by bicycle for leisure. A brevet or randonnée is an organized long-distance ride.		One popular Dutch pleasure is the enjoyment of relaxed cycling in the countryside of the Netherlands. The land is very flat and full of public bicycle trails and cycle tracks where cyclists are not bothered by cars and other traffic, which makes it ideal for cycling recreation. Many Dutch people subscribe every year to an event called fietsvierdaagse — four days of organised cycling through the local environment. Paris–Brest–Paris (PBP), which began in 1891, is the oldest bicycling event still run on a regular basis on the open road, covers over 1,200 km (746 mi) and imposes a 90-hour time limit. Similar if smaller institutions exist in many countries.		Many cycling clubs hold organized rides in which bicyclists of all levels participate. The typical organized ride starts with a large group of riders, called the mass, bunch or even peloton. This will thin out over the course of the ride. Many riders choose to ride together in groups of the same skill level to take advantage of drafting.		Most organized rides, for example cyclosportives (or gran fondos), Challenge Rides or reliability trials, and hill climbs include registration requirements and will provide information either through the mail or online concerning start times and other requirements. Rides usually consist of several different routes, sorted by mileage, and with a certain number of rest stops that usually include refreshments, first aid and maintenance tools. Routes can vary by as much as 100 miles (160 km).		Mountain biking began in the 1970s, originally as a downhill sport, practised on customized cruiser bicycles around Mount Tamalpais.[20] Most mountain biking takes place on dirt roads, trails and in purpose-built parks. Downhill mountain biking has just evolved in the recent years and is performed at places such as Whistler Mountain Bike Park. Slopestyle, a form of downhill, is when riders do tricks such as tailwhips, 360s, backflips and front flips. There are several disciplines of mountain biking besides downhill. Cross country, often referred to as XC, all mountain, trail, free ride, and newly popular enduro.		The Marching and Cycling Band HHK from Haarlem (the Netherlands) is one of the few marching bands around the world which also performs on bicycles.		Shortly after the introduction of bicycles, competitions developed independently in many parts of the world. Early races involving boneshaker style bicycles were predictably fraught with injuries. Large races became popular during the 1890s "Golden Age of Cycling", with events across Europe, and in the U.S. and Japan as well. At one point, almost every major city in the US had a velodrome or two for track racing events, however since the middle of the 20th century cycling has become a minority sport in the US whilst in Continental Europe it continues to be a major sport, particularly in the United Kingdom, France, Belgium, Italy and Spain. The most famous of all bicycle races is the Tour de France. This began in 1903, and continues to capture the attention of the sporting world.		In 1899, Charles Minthorn Murphy became the first man to ride his bicycle a mile in under a minute (hence his nickname, Mile-a-Minute Murphy), which he did by drafting a locomotive at New York's Long Island.		As the bicycle evolved its various forms, different racing formats developed. Road races may involve both team and individual competition, and are contested in various ways. They range from the one-day road race, criterium, and time trial to multi-stage events like the Tour de France and its sister events which make up cycling's Grand Tours. Recumbent bicycles were banned from bike races in 1934 after Marcel Berthet set a new hour record in his Velodyne streamliner (49.992 km on November 18, 1933). Track bicycles are used for track cycling in Velodromes, while cyclo-cross races are held on outdoor terrain, including pavement, grass, and mud. Cyclocross races feature man-made features such as small barriers which riders either bunny hop over or dismount and walk over. Time trial races, another form of road racing require a rider to ride against the clock. Time trials can be performed as a team or as a single rider. Bikes are changed for time trial races, using aero bars. In the past decade, mountain bike racing has also reached international popularity and is even an Olympic sport.		Professional racing organizations place limitations on the bicycles that can be used in the races that they sanction. For example, the Union Cycliste Internationale, the governing body of international cycle sport (which sanctions races such as the Tour de France), decided in the late 1990s to create additional rules which prohibit racing bicycles weighing less than 6.8 kilograms (14.96 pounds). The UCI rules also effectively ban some bicycle frame innovations (such as the recumbent bicycle) by requiring a double triangle structure.[21]		The bicycle has been used as a method of reconnaissance as well as transporting soldiers and supplies to combat zones. In this it has taken over many of the functions of horses in warfare. In the Second Boer War, both sides used bicycles for scouting. In World War I, France, Germany, Australia and New Zealand used bicycles to move troops. In its 1937 invasion of China, Japan employed some 50,000 bicycle troops, and similar forces were instrumental in Japan's march or "roll" through Malaya in World War II. Germany used bicycles again in World War II, while the British employed airborne "Cycle-commandos" with folding bikes.		In the Vietnam War, communist forces used bicycles extensively as cargo carriers along the Ho Chi Minh Trail.		The last country known to maintain a regiment of bicycle troops was Switzerland, which disbanded its last unit in 2003.		Two broad and correlated themes run in bicycle activism: one is about advocating the bicycle as an alternative mode of transport, and the other is about the creation of conditions to permit and/or encourage bicycle use, both for utility and recreational cycling.[22] Although the first, which emphasizes the potential for energy and resource conservation and health benefits gained from cycling versus automobile use, is relatively undisputed, the second is the subject of much debate.		It is generally agreed that improved local and inter-city rail services and other methods of mass transportation (including greater provision for cycle carriage on such services) create conditions to encourage bicycle use. However, there are different opinions on the role of various types of cycling infrastructure in building bicycle-friendly cities and roads.		Some bicycle activists (including some traffic management advisers) seek the construction of bike paths, cycle tracks and bike lanes for journeys of all lengths and point to their success in promoting safety and encouraging more people to cycle. Some activists, especially those from the vehicular cycling tradition, view the safety, practicality, and intent of such facilities with suspicion. They favor a more holistic approach based on the 4 'E's; education (of everyone involved), encouragement (to apply the education), enforcement (to protect the rights of others), and engineering (to facilitate travel while respecting every person's equal right to do so). Some groups offer training courses to help cyclists integrate themselves with other traffic.		Critical Mass is an event typically held on the last Friday of every month in cities around the world where bicyclists take to the streets en masse. While the ride was founded with the idea of drawing attention to how unfriendly the city was to bicyclists, the leaderless structure of Critical Mass makes it impossible to assign it any one specific goal. In fact, the purpose of Critical Mass is not formalized beyond the direct action of meeting at a set location and time and traveling as a group through city streets.		There is a long-running cycle helmet debate among activists. The most heated controversy surrounds the topic of compulsory helmet use.		Cyclists form associations, both for specific interests (trails development, road maintenance, bike maintenance, urban design, racing clubs, touring clubs, etc.) and for more global goals (energy conservation, pollution reduction, promotion of fitness). Some bicycle clubs and national associations became prominent advocates for improvements to roads and highways. In the United States, the League of American Wheelmen lobbied for the improvement of roads in the last part of the 19th century, founding and leading the national Good Roads Movement. Their model for political organization, as well as the paved roads for which they argued, facilitated the growth of the automobile.		As a sport, cycling is governed internationally by the Union Cycliste Internationale in Switzerland, USA Cycling (merged with the United States Cycling Federation in 1995) in the United States, (for upright bicycles) and by the International Human Powered Vehicle Association (for other HPVs, or human-powered vehicles). Cycling for transport and touring is promoted on a European level by the European Cyclists' Federation, with associated members from Great Britain, Japan and elsewhere. Regular conferences on cycling as transport are held under the auspices of Velo City; global conferences are coordinated by Velo Mondial.[23]		The health benefits of cycling outweigh the risks, when cycling is compared to a sedentary lifestyle. A Dutch study found that cycling can extend lifespans by up to 14 months, but the risks equated to a reduced lifespan of 40 days or less.[24] Cycling in the Netherlands is often safer than in other parts of the world, so the risk-benefit ratio will be different in other regions.[25] Overall, benefits of cycling or walking have been shown to exceed risks by ratios of 9:1 to 96:1 when compared with no exercise at all.[26] However these studies did not compare cycling to other forms of exercise that can involve less risk.		The physical exercise gained from cycling is generally linked with increased health and well-being. According to the World Health Organization, physical inactivity is second only to tobacco smoking as a health risk in developed countries,[27] and this is associated with many tens of billions of dollars of healthcare costs.[28] The WHO's report[27] suggests that increasing physical activity is a public health 'best buy', and that cycling is a 'highly suitable activity' for this purpose. The charity Sustrans reports that investment in cycling provision can give a 20:1 return from health and other benefits.[29] It has been estimated that, on average, approximately 20 life-years are gained from the health benefits of road bicycling for every life-year lost through injury.[30]		Bicycles are often used by people seeking to improve their fitness and cardiovascular health. In this regard, cycling is especially helpful for those with arthritis of the lower limbs who are unable to pursue sports that cause impact to the knees and other joints. Since cycling can be used for the practical purpose of transportation, there can be less need for self-discipline to exercise.		Cycling while seated is a relatively non-weight bearing exercise that, like swimming, does little to promote bone density.[31] Cycling up and out of the saddle, on the other hand, does a better job by transferring more of the rider's body weight to the legs. However, excessive cycling while standing can cause knee damage[32][not in citation given] It used to be thought that cycling while standing was less energy efficient, but recent research has proven this not to be true. Other than air resistance, there is no wasted energy from cycling while standing, if it is done correctly.[33]		Cycling on a stationary cycle is frequently advocated as a suitable exercise for rehabilitation, particularly for lower limb injury, owing to the low impact which it has on the joints. In particular, cycling is commonly used within knee rehabilitation programs.[34]		As a response to the increased global sedentarity and consequent overweight and obesity, one response that has been adopted by many organizations concerned with health and environment is the promotion of Active travel, which seeks to promote walking and cycling as safe and attractive alternatives to motorized transport. Given that many journeys are for relatively short distances, there is considerable scope to replace car use with walking or cycling, though in many settings this may require some infrastructure modification, particularly to attract the less experienced and confident.		Cycling suffers from a perception that it is unsafe.[35][36] This perception is not always backed by hard numbers, because of under reporting of accidents and lack of bicycle use data (amount of cycling, kilometers cycled) which make it hard to assess the risk and monitor changes in risks.[37] In the UK, fatality rates per mile or kilometre are slightly less than those for walking.[38] In the US, bicycling fatality rates are less than 2/3 of those walking the same distance.[39][40] However, in the UK for example the fatality and serious injury rates per hour of travel are just over double for cycling than those for walking.[38] Thus if a person is, for example, about to undertake a ten kilometre journey to a given destination it may on average be safer to undertake this journey by bicycle than on foot. However, if a person is intending, for example, to undertake an hour's exercise it is likely to be considerably more dangerous to take that exercise by cycling rather than by walking.		Despite the risk factors associated with bicycling, cyclists have a lower overall mortality rate when compared to other groups. A Danish study in 2000 found that even after adjustment for other risk factors, including leisure time physical activity, those who did not cycle to work experienced a 39% higher mortality rate than those who did.[41]		Injuries (to cyclists, from cycling) can be divided into two types:		Acute physical trauma includes injuries to the head and extremities resulting from falls and collisions. Most cycle deaths result from a collision with a car or heavy goods vehicle, both motorist and cyclist having been found responsible for collisions.[42][43][44] A third of collisions between motorists and cyclists are caused by car dooring.[45] However, around 16% of serious cyclist injuries reported to police in the UK in 2014 did not involve any other person or vehicle.[46]		Although a majority of bicycle collisions occur during the day,[46] bicycle lighting is recommended for safety when bicycling at night to increase visibility.[47]		Of a study of 518 cyclists, a large majority reported at least one overuse injury, with over one third requiring medical treatment. The most common injury sites were the neck (48.8%) and the knees (41.7%), as well as the groin/buttocks (36.1%), hands (31.1%), and back (30.3%). Women were more likely to suffer from neck and shoulder pain than men.[48]		Many cyclists suffer from overuse injuries to the knees, affecting cyclists at all levels. These are caused by many factors:[49]		Excessive saddle height can cause posterior knee pain, while setting the saddle too low can cause pain in the anterior of the knee. An incorrectly fitted saddle may eventually lead to muscle imbalance. A 25 to 35 degree knee angle is recommended to avoid an overuse injury.[50]		Overuse injuries, including chronic nerve damage at weight bearing locations, can occur as a result of repeatedly riding a bicycle for extended periods of time. Damage to the ulnar nerve in the palm, carpal tunnel in the wrist, the genitourinary tract[51] or bicycle seat neuropathy[52] may result from overuse. Recumbent bicycles are designed on different ergonomic principles and eliminate pressure from the saddle and handlebars, due to the relaxed riding position.		Note that overuse is a relative term, and capacity varies greatly between individuals. Someone starting out in cycling must be careful to increase length and frequency of cycling sessions slowly, starting for example at an hour or two per day, or a hundred miles or kilometers per week. Bilateral muscular pain is a normal by-product of the training process, whereas unilateral pain may reveal "exercise-induced arterial endofibrosis".[53] Joint pain and numbness are also early signs of overuse injury.		Cycling has been linked to sexual impotence due to pressure on the perineum from the seat, but fitting a proper sized seat prevents this effect.[54][55][56][57] In extreme cases, pudendal nerve entrapment can be a source of intractable perineal pain.[58] Some cyclists with induced pudendal nerve pressure neuropathy gained relief from improvements in saddle position and riding techniques.[59]		The National Institute for Occupational Safety and Health (NIOSH) has investigated the potential health effects of prolonged bicycling in police bicycle patrol units, including the possibility that some bicycle saddles exert excessive pressure on the urogenital area of cyclists, restricting blood flow to the genitals.[60] Their study found that using bicycle seats without protruding noses reduced pressure on the groin by at least 65% and significantly reduced the number of cases of urogenital paresthesia. A follow-up found that 90% of bicycle officers who tried the no-nose seat were using it six months later. NIOSH recommends that riders use a no-nose bicycle seat for workplace bicycling.[57][61]		A Spanish study of top triathletes found those who cover more than 186 miles (300 km) a week on their bikes have less than 4% normal looking sperm, where normal adult males would be expected to have from 15% to 20%.[54][62]		Despite rumors to the contrary, there is no scientific evidence linking cycling with testicular cancer.[63]		One concern is that riding in traffic may expose the cyclist to higher levels of air pollution, especially if he or she travels on or along busy roads. Some authors have claimed this to be untrue, showing that the pollutant and irritant count within cars is consistently higher,[64] presumably because of limited circulation of air within the car and due to the air intake being directly in the stream of other traffic. Other authors have found small or inconsistent differences in concentrations but claim that exposure of cyclists is higher due to increased minute ventilation[65] and is associated with minor biological changes.[66] The significance of the associated health effect, if any, is unclear but probably much smaller than the health impacts associated with accidents and the health benefits derived from additional physical activity.		|group2 = See also |list2 =		|below = }}		
Sugar is the generic name for sweet, soluble carbohydrates, many of which are used in food. There are various types of sugar derived from different sources. Simple sugars are called monosaccharides and include glucose (also known as dextrose), fructose, and galactose. The "table sugar" or "granulated sugar" most customarily used as food is sucrose, a disaccharide of glucose and fructose. Sugar is used in prepared foods (e.g., cookies and cakes) and it is added to some foods and beverages (e.g., coffee and tea). In the body, sucrose is hydrolysed into the simple sugars fructose and glucose. Other disaccharides include maltose from malted grain, and lactose from milk. Longer chains of sugars are called oligosaccharides or polysaccharides. Some other chemical substances, such as glycerol may also have a sweet taste, but are not classified as sugars. Diet food substitutes for sugar, include aspartame and sucralose, a chlorinated derivative of sucrose.		Sugars are found in the tissues of most plants and are present in sugarcane and sugar beet in sufficient concentrations for efficient commercial extraction. The world production of sugar in 2011 was about 168 million tonnes. The average person consumes about 24 kilograms (53 lb) of sugar each year (33.1 kg in developed countries), equivalent to over 260 food calories per person, per day. Since the latter part of the twentieth century, it has been questioned whether a diet high in sugars, especially refined sugars, is good for human health. Over-consumption of sugar has been implicated in the occurrence of obesity, diabetes, cardiovascular disease, dementia, and tooth decay. Numerous studies have been undertaken to try to clarify the position, but with varying results, mainly because of the difficulty of finding populations for use as controls that do not consume or are largely free of any sugar consumption.						The etymology reflects the spread of the commodity. The English word "sugar" ultimately originates from the Sanskrit शर्करा (śarkarā), via Arabic سكر (sukkar) as granular or candied sugar, which is cognate with the Greek word, kroke, or "pebble".[1] The contemporary Italian word is zucchero, whereas the Spanish and Portuguese words, azúcar and açúcar, respectively, have kept a trace of the Arabic definite article. The Old French word is zuchre and the contemporary French, sucre. The earliest Greek word attested is σάκχαρις (sákkʰaris).		The English word jaggery, a coarse brown sugar made from date palm sap or sugarcane juice, has a similar etymological origin – Portuguese jagara from the Sanskrit शर्करा (śarkarā).[2]		Sugar has been produced in the Indian subcontinent[3] since ancient times. It was not plentiful or cheap in early times and honey was more often used for sweetening in most parts of the world. Originally, people chewed raw sugarcane to extract its sweetness. Sugarcane was a native of tropical South Asia and Southeast Asia.[4] Different species seem to have originated from different locations with Saccharum barberi originating in India and S. edule and S. officinarum coming from New Guinea.[4][5] One of the earliest historical references to sugarcane is in Chinese manuscripts dating back to 8th century BC that state that the use of sugarcane originated in India.[6]		Sugar was found in Europe by the 1st century CE, but only as an imported medicine, and not as a food.[7][8] The Greek physician Pedanius Dioscorides in the 1st century CE described sugar in his medical treatise De Materia Medica,[9] and Pliny the Elder, a 1st-century CE Roman, described sugar in his Natural History: "Sugar is made in Arabia as well, but Indian sugar is better. It is a kind of honey found in cane, white as gum, and it crunches between the teeth. It comes in lumps the size of a hazelnut. Sugar is used only for medical purposes."[8]		Sugar remained relatively unimportant until the Indians discovered methods of turning sugarcane juice into granulated crystals that were easier to store and to transport.[10] Crystallized sugar was discovered by the time of the Imperial Guptas, around the 5th century CE.[10] In the local Indian language, these crystals were called khanda (Devanagari: खण्ड, Khaṇḍa), which is the source of the word candy.[11] Indian sailors, who carried clarified butter and sugar as supplies, introduced knowledge of sugar on the various trade routes they travelled.[10] Buddhist monks, as they travelled around, brought sugar crystallization methods to China.[12] During the reign of Harsha (r. 606–647) in North India, Indian envoys in Tang China taught methods of cultivating sugarcane after Emperor Taizong of Tang (r. 626–649) made known his interest in sugar. China then established its first sugarcane plantations in the seventh century.[13] Chinese documents confirm at least two missions to India, initiated in 647 CE, to obtain technology for sugar refining.[14] In South Asia, the Middle East and China, sugar became a staple of cooking and desserts.		Crusaders brought sugar home with them to Europe after their campaigns in the Holy Land, where they encountered caravans carrying "sweet salt". Early in the 12th century, Venice acquired some villages near Tyre and set up estates to produce sugar for export to Europe, where it supplemented honey, which had previously been the only available sweetener.[15] Crusade chronicler William of Tyre, writing in the late 12th century, described sugar as "very necessary for the use and health of mankind".[16] In the 15th century, Venice was the chief sugar refining and distribution centre in Europe.[6]		In August 1492, Christopher Columbus stopped at La Gomera in the Canary Islands, for wine and water, intending to stay only four days. He became romantically involved with the governor of the island, Beatriz de Bobadilla y Ossorio, and stayed a month. When he finally sailed, she gave him cuttings of sugarcane, which became the first to reach the New World.[17] The first sugar cane harvest was conducted in Hispaniola in 1501, and many sugar mills had been constructed in Cuba and Jamaica by the 1520s.[18] The Portuguese took sugar cane to Brazil. By 1540, there were 800 cane sugar mills in Santa Catarina Island and another 2,000 on the north coast of Brazil, Demarara, and Surinam.		Sugar was a luxury in Europe until the 18th century, when it became more widely available. It then became popular and by the 19th century, sugar came to be considered a necessity. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes.[19] It drove, in part, colonization of tropical islands and nations where labor-intensive sugarcane plantations and sugar manufacturing could thrive. The demand for cheap labor to perform the hard work involved in its cultivation and processing increased the demand for the slave trade from Africa (in particular West Africa). After slavery was abolished, there was high demand for indentured laborers from South Asia (in particular India).[20][21][22] Millions of slave and indentured laborers were brought into the Caribbean and the Americas, Indian Ocean colonies, southeast Asia, Pacific Islands, and East Africa and Natal. The modern ethnic mix of many nations that have been settled in the last two centuries has been influenced by the demand for sugar.[23][24][25]		Sugar also led to some industrialization of areas where sugar cane was grown. For example, Lieutenant J. Paterson, of the Bengal establishment, persuaded the British Government that sugar cane could be cultivated in British India with many advantages and at less expense than in the West Indies; as a result, sugar factories were established in Bihar in eastern India.[26] During the Napoleonic Wars, sugar beet production increased in continental Europe because of the difficulty of importing sugar when shipping was subject to blockade. By 1880, the sugar beet was the main source of sugar in Europe. It was cultivated in Lincolnshire and other parts of England, although the United Kingdom continued to import the main part of its sugar from its colonies.[27]		Until the late nineteenth century, sugar was purchased in loaves, which had to be cut using implements called sugar nips.[28] In later years, granulated sugar was more usually sold in bags. Sugar cubes were produced in the nineteenth century. The first inventor of a process to make sugar in cube form was the Moravian Jakub Kryštof Rad, director of a sugar company in Dačice. He began sugar cube production after being granted a five-year patent for the process on January 23, 1843. Henry Tate of Tate & Lyle was another early manufacturer of sugar cubes at his refineries in Liverpool and London. Tate purchased a patent for sugar cube manufacture from German Eugen Langen, who in 1872 had invented a different method of processing of sugar cubes.[29]		Scientifically, sugar loosely refers to a number of carbohydrates, such as monosaccharides, disaccharides, or oligosaccharides. Monosaccharides are also called "simple sugars," the most important being glucose. Most monosaccharides have a formula that conforms to C nH 2nO n with n between 3 and 7 (deoxyribose being an exception). Glucose has the molecular formula C 6H 12O 6. The names of typical sugars end with -ose, as in "glucose" and "fructose". Sometimes such words may also refer to any types of carbohydrates soluble in water. The acyclic mono- and disaccharides contain either aldehyde groups or ketone groups. These carbon-oxygen double bonds (C=O) are the reactive centers. All saccharides with more than one ring in their structure result from two or more monosaccharides joined by glycosidic bonds with the resultant loss of a molecule of water (H 2O) per bond.[30]		Monosaccharides in a closed-chain form can form glycosidic bonds with other monosaccharides, creating disaccharides (such as sucrose) and polysaccharides (such as starch). Enzymes must hydrolyze or otherwise break these glycosidic bonds before such compounds become metabolized. After digestion and absorption the principal monosaccharides present in the blood and internal tissues include glucose, fructose, and galactose. Many pentoses and hexoses can form ring structures. In these closed-chain forms, the aldehyde or ketone group remains non-free, so many of the reactions typical of these groups cannot occur. Glucose in solution exists mostly in the ring form at equilibrium, with less than 0.1% of the molecules in the open-chain form.[30]		Biopolymers of sugars are common in nature. Through photosynthesis, plants produce glyceraldehyde-3-phosphate (G3P), a phosphated 3-carbon sugar that is used by the cell to make monosaccharides such as glucose (C 6H 12O 6) or (as in cane and beet) sucrose (C 12H 22O 11). Monosaccharides may be further converted into structural polysaccharides such as cellulose and pectin for cell wall construction or into energy reserves in the form of storage polysaccharides such as starch or inulin. Starch, consisting of two different polymers of glucose, is a readily degradable form of chemical energy stored by cells, and can be converted to other types of energy.[30] Another polymer of glucose is cellulose, which is a linear chain composed of several hundred or thousand glucose units. It is used by plants as a structural component in their cell walls. Humans can digest cellulose only to a very limited extent, though ruminants can do so with the help of symbiotic bacteria in their gut.[31] DNA and RNA are built up of the monosaccharides deoxyribose and ribose, respectively. Deoxyribose has the formula C 5H 10O 4 and ribose the formula C 5H 10O 5.[32]		Because sugars burn easily when exposed to flame, the handling of sugars risks dust explosion. The 2008 Georgia sugar refinery explosion, which killed 14 people and injured 40, and destroyed most of the refinery, was caused by the ignition of sugar dust. In its culinary use, sugar that is exposed to a heat source causes caramelization to take place. As the process occurs, volatile chemicals such as diacetyl are released, producing the characteristic caramel flavor.		Fructose, galactose, and glucose are all simple sugars, monosaccharides, with the general formula C6H12O6. They have five hydroxyl groups (−OH) and a carbonyl group (C=O) and are cyclic when dissolved in water. They each exist as several isomers with dextro- and laevo-rotatory forms that cause polarized light to diverge to the right or the left.[33]		Lactose, maltose, and sucrose are all compound sugars, disaccharides, with the general formula C12H22O11. They are formed by the combination of two monosaccharide molecules with the exclusion of a molecule of water.[33]		The sugar contents of common fruits and vegetables are presented in Table 1. All data with a unit of g (gram) are based on 100 g of a food item. The fructose/glucose ratio is calculated by dividing the sum of free fructose plus half sucrose by the sum of free glucose plus half sucrose.		Sugar beet became a major source of sugar in the 19th century when methods for extracting the sugar became available. It is a biennial plant,[41] a cultivated variety of Beta vulgaris in the family Amaranthaceae, the tuberous root of which contains a high proportion of sucrose. It is cultivated as a root crop in temperate regions with adequate rainfall and requires a fertile soil. The crop is harvested mechanically in the autumn and the crown of leaves and excess soil removed. The roots do not deteriorate rapidly and may be left in a clamp in the field for some weeks before being transported to the processing plant. Here the crop is washed and sliced and the sugar extracted by diffusion. Milk of lime is added to the raw juice and carbonatated in a number of stages in order to purify it. Water is evaporated by boiling the syrup under a vacuum. The syrup is then cooled and seeded with sugar crystals. The white sugar that crystallizes out can be separated in a centrifuge and dried. It requires no further refining.[42]		Sugarcane refers to any of several species, or their hybrids, of giant grasses in the genus Saccharum in the family Poaceae. They have been cultivated in tropical climates in South Asia and Southeast Asia since ancient times for the sucrose that is found in their stems. A great expansion in sugarcane production took place in the 18th century with the establishment of slave plantations in the Americas. The use of slavery meant that this was the first time that sugar became cheap enough for most people, who previously had to rely on honey to sweeten foods. It requires a frost-free climate with sufficient rainfall during the growing season to make full use of the plant's great growth potential. The crop is harvested mechanically or by hand, chopped into lengths and conveyed rapidly to the processing plant (commonly known as a sugar mill). Here, it is either milled and the juice extracted with water or extracted by diffusion. The juice is then clarified with lime and heated to destroy enzymes. The resulting thin syrup is concentrated in a series of evaporators, after which further water is removed by evaporation in vacuum containers. The resulting supersaturated solution is seeded with sugar crystals and the sugar crystallizes out and is separated from the fluid and dried. Molasses is a by-product of the process and the fiber from the stems, known as bagasse, is burned to provide energy for the sugar extraction process. The crystals of raw sugar have a sticky brown coating and either can be used as they are or can be bleached by sulfur dioxide or can be treated in a carbonatation process to produce a whiter product.[43] About 2,500 litres (660 US gal) of irrigation water is needed for every one kilogram of sugar produced.[44]		Refined sugar is made from raw sugar that has undergone a refining process to remove the molasses.[45][46] Raw sugar is sucrose which is extracted from sugarcane or sugar beet. While raw sugar can be consumed, the refining process removes unwanted tastes and results in refined sugar or white sugar.[47][48]		The sugar may be transported in bulk to the country where it will be used and the refining process often takes place there. The first stage is known as affination and involves immersing the sugar crystals in a concentrated syrup that softens and removes the sticky brown coating without dissolving them. The crystals are then separated from the liquor and dissolved in water. The resulting syrup is treated either by a carbonatation or by a phosphatation process. Both involve the precipitation of a fine solid in the syrup and when this is filtered out, many of the impurities are removed at the same time. Removal of color is achieved by using either a granular activated carbon or an ion-exchange resin. The sugar syrup is concentrated by boiling and then cooled and seeded with sugar crystals, causing the sugar to crystallize out. The liquor is spun off in a centrifuge and the white crystals are dried in hot air and ready to be packaged or used. The surplus liquor is made into refiners' molasses.[49] The International Commission for Uniform Methods of Sugar Analysis sets standards for the measurement of the purity of refined sugar, known as ICUMSA numbers; lower numbers indicate a higher level of purity in the refined sugar.[50]		Refined sugar is widely used for industrial needs for higher quality. Refined sugar is purer (ICUMSA below 300) than raw sugar (ICUMSA over 1,500).[51] The level of purity associated with the colors of sugar, expressed by standard number ICUMSA, the smaller ICUMSA numbers indicate the higher purity of sugar.[51]		The five largest producers of sugar in 2011 were Brazil, India, the European Union, China and Thailand. In the same year, the largest exporter of sugar was Brazil, distantly followed by Thailand, Australia and India. The largest importers were the European Union, United States and Indonesia. At present, Brazil has the highest per capita consumption of sugar, followed by Australia, Thailand, and the European Union.[52][53]		World sugar production (1000 metric tons)[52]		In most parts of the world, sugar is an important part of the human diet, making food more palatable and providing food energy. After cereals and vegetable oils, sugar derived from sugarcane and beet provided more kilocalories per capita per day on average than other food groups.[59] According to the FAO, an average of 24 kilograms (53 lb) of sugar, equivalent to over 260 food calories per day, was consumed annually per person of all ages in the world in 1999. Even with rising human populations, sugar consumption is expected to increase to 25.1 kilograms (55 lb) per person per year by 2015.[60]		Data collected in multiple U.S. surveys between 1999 and 2008 show that the intake of added sugars has declined by 24 percent with declines occurring in all age, ethnic and income groups.[61]		The per capita consumption of refined sugar in the United States has varied between 27 and 46 kilograms (60 and 101 lb) in the last 40 years. In 2008, American per capita total consumption of sugar and sweeteners, exclusive of artificial sweeteners, equalled 61.9 kg (136 lb) per year. This consisted of 29.65 kg (65.4 lb) pounds of refined sugar and 31 kg (68.3 lb) pounds of corn-derived sweeteners per person.[63][64]		Brown and white granulated sugar are 97% to nearly 100% carbohydrates, respectively, with 2% or no water, and no dietary fiber, protein or fat (tables). While brown sugar contains a moderate amount of iron (15% of the Reference Daily Intake in a 100 gram amount), both sugars are devoid of essential nutrients (table). In a typical serving size of 4 grams (one teaspoon), sugar supplies 15 calories.[65]		Because brown sugar contains a small amount (5–10%) of molasses reintroduced during processing, its value to some consumers is a richer flavor than white sugar.[66]		Studies examining the health impact of sugars are inconclusive. The United Nations meta-analysis and WHO studies showed contrasting impacts of sugar in refined and unrefined forms.[67] Other studies indicated variable results between health effects, particularly on obesity, and whether the research was funded by the sugar industry or those by independent sponsors.[68] The 'empty calories' argument is that a diet high in added sugar will reduce consumption of foods that contain essential nutrients.[69]		Sugar addiction is the term for the relationship between sugar and the various aspects of food addiction including "bingeing, withdrawal, craving and cross-sensitization". Some scientists assert that consumption of sweets or sugar could have a heroin addiction-like effect,[70] but recent reviews suggest that sugar addiction does not occur in humans.[71][72]		Claims have been made of a sugar–Alzheimer's disease connection, but debate continues over whether cognitive decline is attributable to dietary fructose or to overall energy intake.[73][74]		Carbohydrates are classified according to their glycemic index, a system for measuring how quickly a food that is eaten raises blood sugar levels, and glycemic load, which takes into account both the glycemic index and the amount of carbohydrate in the food.[75] This has led to carbohydrate counting, a method used by diabetics for planning their meals.[76]		Studies in animals have suggested that chronic consumption of refined sugars can contribute to metabolic and cardiovascular dysfunction. Some experts have suggested that refined fructose is more damaging than refined glucose in terms of cardiovascular risk.[77] Cardiac performance has been shown to be impaired by switching from a carbohydrate diet including fiber to a high-carbohydrate diet.[78] Switching from saturated fatty acids to carbohydrates with high glycemic index values shows a statistically-significant increase in the risk of myocardial infarction.[79] Other studies have shown that the risk of developing coronary heart disease is decreased by adopting a diet high in polyunsaturated fatty acids but low in sugar, whereas a low-fat, high-carbohydrate diet brings no reduction. This suggests that consuming a diet with a high glycemic load typical of the "junk food" diet is strongly associated with an increased risk of developing coronary heart disease.[80]		The consumption of added sugars has been positively associated with multiple measures known to increase cardiovascular disease risk amongst adolescents as well as adults.[81] Studies are suggesting that the impact of refined carbohydrates or high glycemic load carbohydrates are more significant than the impact of saturated fatty acids on cardiovascular disease.[82][83] A high dietary intake of sugar (in this case, sucrose or disaccharide) can substantially increase the risk of heart and vascular diseases. According to a Swedish study of 4301 people undertaken by Lund University and Malmö University College, sugar was associated with higher levels of bad blood lipids, causing a high level of small and medium low-density lipoprotein (LDL) and reduced high-density lipoprotein (HDL). In contrast, the amount of fat eaten did not affect the level of blood fats. Incidentally quantities of alcohol and protein were linked to an increase in the good HDL blood fat.[84]		There is a common notion that sugar leads to hyperactivity, in particular in children, but studies and meta-studies question or address this issue.[85] Some articles and studies do refer to the increasing evidence supporting the links between refined sugar and hyperactivity.[86][87][88] The WHO FAO meta-study suggests that such inconclusive results are to be expected when some studies do not effectively segregate or control for free sugars as opposed to sugars still in their natural form (entirely unrefined) while others do.[67] One study followed thirty-five 5-to-7-year-old boys who were reported by their mothers to be behaviorally "sugar-sensitive." They were randomly assigned to experimental and control groups. In the experimental group, mothers were told that their children were fed sugar, and, in the control group, mothers were told that their children received a placebo. In fact, all children received the placebo, but mothers in the sugar expectancy condition rated their children as significantly more hyperactive.[89] This result suggests that the real effect of sugar is that it increases worrying among parents with preconceived notions.		Controlled trials have now shown unequivocally that consumption of sugar-sweetened beverages increases body weight and body fat, and that replacement of sugar by artificial sweeteners reduces weight.[90][91][92] Studies on the link between sugars and diabetes are inconclusive, with some suggesting that eating excessive amounts of sugar does not increase the risk of diabetes, although the extra calories from consuming large amounts of sugar can lead to obesity, which may itself increase the risk of developing this metabolic disorder.[93][94][95][96][97][98] Other studies show correlation between refined sugar (also called "free sugar") consumption and the onset of diabetes, and negative correlation with the consumption of fiber.[99][100][101][102] These included a 2010 meta-analysis of eleven studies involving 310,819 participants and 15,043 cases of type 2 diabetes.[103]		This found that "SSBs (sugar-sweetened beverages) may increase the risk of metabolic syndrome and type 2 diabetes not only through obesity but also by increasing dietary glycemic load, leading to insulin resistance, β-cell dysfunction, and inflammation". As an overview to consumption related to chronic disease and obesity, the World Health Organization's independent meta-studies specifically distinguish free sugars ("all monosaccharides and disaccharides added to foods by the manufacturer, cook or consumer, plus sugars naturally present in honey, syrups and fruit juices") from sugars occurring naturally in food. The reports prior to 2000 set the limits for free sugars at a maximum of 10% of carbohydrate intake, measured by energy, rather than mass, and since 2002 have aimed for a level across the entire population of less than 10%.[67] The consultation committee recognized that this goal is "controversial. However, the Consultation considered that the studies showing no effect of free sugars on excess weight have limitations".[67] A 2015 New York Times report noted that "a review of beverage studies, published in the journal PLOS Medicine, found that those funded by Coca-Cola, PepsiCo, the American Beverage Association and the sugar industry were five times more likely to find no link between sugary drinks and weight gain than studies whose authors reported no financial conflicts."[68]		In regard to contributions to tooth decay, the role of free sugars is also recommended to be below an absolute maximum of 10% of energy intake, with a minimum of zero. There is "convincing evidence from human intervention studies, epidemiological studies, animal studies and experimental studies, for an association between the amount and frequency of free sugars intake and dental caries" while other sugars (complex carbohydrate) consumption is normally associated with a lower rate of dental caries.[104] Lower rates of tooth decay have been seen in individuals with hereditary fructose intolerance.[105] Also, studies have shown that the consumption of sugar and starch have different impacts on oral health with the ingestion of starchy foods and fresh fruit being associated with low levels of dental caries.[104]		The World Health Organization recommends[106] that both adults and children reduce the intake of free sugars to less than 10% of total energy intake. A reduction to below 5% of total energy intake brings additional health benefits, especially in what regards dental caries (cavities in the teeth). These recommendations were based on the totality of available evidence reviewed regarding the relationship between free sugars intake and body weight and dental caries. Free sugars include monosaccharides and disaccharides added to foods and beverages by the manufacturer, cook or consumer, and sugars naturally present in honey, syrups, fruit juices and fruit juice concentrates.[106]		On May 20, 2016 the U.S. Food and Drug Administration announced changes to the Nutrition Facts panel displayed on all foods, to be effective by July 2018. New to the panel is a requirement to list "Added sugars" by weight and as a percent of Daily Value (DV). For vitamins and minerals the intent of DVs is to indicate how much should be consumed. For added sugars, the guidance is that 100% DV should not be exceeded. 100% DV is defined as 50 grams. For a person consuming 2000 calories a day, 50 grams, the amount to not exceed, is the same as 200 calories, and thus 10% of total calories - same guidance as the World Health Organization.[107] To put this into context, most 12 ounce (335 mL) cans of soda contain 39 grams of sugar. In the United States, a recently published government survey on food consumption reported that for men and women ages 20 and older the average total sugar intakes - naturally occurring in foods and added - were, respectively, 125 and 99 g/day.[108]		Various culinary sugars have different densities due to differences in particle size and inclusion of moisture.		Domino Sugar gives the following weight to volume conversions (in United States customary units):[109]		The "Engineering Resources – Bulk Density Chart" published in Powder and Bulk gives different values for the bulk densities:[110]						
The American Heart Association (AHA) is a non-profit organization in the United States that fosters appropriate cardiac care in an effort to reduce disability and deaths caused by cardiovascular disease and stroke. Originally formed in New York City in 1924 as the Association for the Prevention and Relief of Heart Disease,[1] it is currently headquartered in Dallas, Texas. The American Heart Association is a national voluntary health agency.		They are known for publishing standards on basic life support and advanced cardiac life support (ACLS), and in 2014 issued its first guidelines for preventing strokes in women.[2] They are known also for operating a number of highly visible public service campaigns starting in the 1970s, and also operate a number of fundraising events. In 1994, the Chronicle of Philanthropy, an industry publication, released a study that showed the American Heart Association was ranked as the 5th "most popular charity/non-profit in America."[3] John Warner, M.D., MBA is president of the American Heart Association for its 2017–18 fiscal year[4].						The American Heart Association grew out of a set of smaller precursor groups. The primary precursor was the Association for the Prevention and Relief of Heart Disease, formed in New York City in 1915, to study whether patients with heart disease could safely return to work. Several similar organizations formed or evolved in Boston, Philadelphia, and Chicago in the 1920s. Recognizing the need for a national organization to share research and promote findings, the American Heart Association was formed in 1924 by six cardiologists representing several of these precursor groups.[1]		The AHA remained small until the 1940s when it was selected for support by Procter & Gamble, via their PR firm, from a list of applicant charities. Procter & Gamble gave $1.5 million from its radio show, Truth or Consequences, allowing the organization to go national.[5]		Recommendations regarding limiting saturated fats and cholesterol emerged from a series of scientific studies in the 1950s, and related American Heart Association dietary guidelines emerged between 1957 and 1961. The 1957 AHA report included: (1) Diet may play an important role in the pathogenesis of atherosclerosis, (2) The fat content and total calories in the diet are probably important factors, (3) The ratio between saturated and unsaturated fat may be the basic determinant, and (4) A wide variety of other factors beside fat, both dietary and non-dietary, may be important. By 1961, these finding had been strengthened, leading to the new 1961 AHA recommendations: (1) Maintain a correct body weight, (2) Engage in moderate exercise, e.g., walking to aid in weight reduction, (3) Reduce intake of total fat, saturated fat, and cholesterol. Increase intake of polyunsaturated fat, (4) Men with a strong family history of atherosclerosis should pay particular attention to diet modification, and (5) Dietary changes should be carried out under medical supervision. These recommendations continued to become more precise from 1957 to 1980, but maintained "a general coherence among them".[6]		In 1994, the Chronicle of Philanthropy, an industry publication, released the results of the largest study of charitable and non-profit organization popularity and credibility. The study showed that the American Heart Association was ranked as the 5th "most popular charity/non-profit in America" of over 100 charities researched with 95% of Americans over the age of 12 choosing Love and Like A lot description category for the American Heart Association.[3]		On October 28, 2009 The American Heart Association and the Ad Council launched a hands-only CPR public service announcement and website.[7] On November 30, 2009, The American Heart Association announced a new cardiac arrest awareness campaign called Be the Beat.[8] The campaign's aim is to teach 12- to 15-year-olds fun ways to learn the basics of cardiopulmonary resuscitation and how to use an automated external defibrillator.		In 2010 the AHA launched the "Go Red for Women" campaign. Historically men have been the primary subjects of heart disease and stroke research. "Go Red for Women" specifically targets women with information about risks and action they can take to protect their health. All revenues from the local and national campaigns goes to support awareness, research, education and community programs to benefit women.[9]		In May 2010, the AHA endorsed the Nintendo Wii Console. The AHA Heart Icon is featured on the box of the console as well as on the Wii Fit Plus and Wii Sports Resort titles.		It also carried out a campaign in 2012 to educate more people on how to carry out hands-only CPR.[10][11][12] The 2012 campaign, which began in New York City, had Jennifer Coolidge as the spokesperson.[13]		In 2012, singer-songwriter and actress Michelle Williams became an ambassador for the Power to End Stroke campaign. In speaking of her role she said "I am honored to partner with the campaign […] My father had a stroke in 2005 due to smoking, diabetes and an unhealthy diet, and my grandmother was diagnosed with having a stroke in 2006 when she went to her doctor for a simple outpatient procedure. I am bringing awareness to people so that strokes can be prevented. Let's take care of ourselves…the first step is knowledge about your health."[14]		In 2014, the American Heart Association issued its first guidelines for preventing strokes in women.[2]		In 2015, the American Heart Association officially endorsed the Tobacco 21 campaign, urging local, state and national governments to raise the tobacco and nicotine sales age from 18 to 21.[15]		
Breast cancer is cancer that develops from breast tissue.[8] Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, fluid coming from the nipple, or a red scaly patch of skin.[1] In those with distant spread of the disease, there may be bone pain, swollen lymph nodes, shortness of breath, or yellow skin.[9]		Risk factors for developing breast cancer include being female, obesity, lack of physical exercise, drinking alcohol, hormone replacement therapy during menopause, ionizing radiation, early age at first menstruation, having children late or not at all, older age, and family history.[1][2] About 5–10% of cases are due to genes inherited from a person's parents, including BRCA1 and BRCA2 among others. Breast cancer most commonly develops in cells from the lining of milk ducts and the lobules that supply the ducts with milk. Cancers developing from the ducts are known as ductal carcinomas, while those developing from lobules are known as lobular carcinomas.[1] In addition, there are more than 18 other sub-types of breast cancer. Some cancers, such as ductal carcinoma in situ, develop from pre-invasive lesions.[2] The diagnosis of breast cancer is confirmed by taking a biopsy of the concerning lump. Once the diagnosis is made, further tests are done to determine if the cancer has spread beyond the breast and which treatments it may respond to.[1]		The balance of benefits versus harms of breast cancer screening is controversial. A 2013 Cochrane review stated that it is unclear if mammographic screening does more good or harm.[10] A 2009 review for the US Preventive Services Task Force found evidence of benefit in those 40 to 70 years of age,[11] and the organization recommends screening every two years in women 50 to 74 years old.[12] The medications tamoxifen or raloxifene may be used in an effort to prevent breast cancer in those who are at high risk of developing it.[2] Surgical removal of both breasts is another preventative measure in some high risk women.[2] In those who have been diagnosed with cancer, a number of treatments may be used, including surgery, radiation therapy, chemotherapy, hormonal therapy and targeted therapy.[1] Types of surgery vary from breast-conserving surgery to mastectomy.[13][14] Breast reconstruction may take place at the time of surgery or at a later date. In those in whom the cancer has spread to other parts of the body, treatments are mostly aimed at improving quality of life and comfort.[14]		Outcomes for breast cancer vary depending on the cancer type, extent of disease, and person's age.[14] Survival rates in the developed world are high,[15] with between 80% and 90% of those in England and the United States alive for at least 5 years.[5][4] In developing countries survival rates are poorer.[2] Worldwide, breast cancer is the leading type of cancer in women, accounting for 25% of all cases.[16] In 2012 it resulted in 1.68 million new cases and 522,000 deaths.[16] It is more common in developed countries[2] and is more than 100 times more common in women than in men.[15][17]		The first noticeable symptom of breast cancer is typically a lump that feels different from the rest of the breast tissue. More than 80% of breast cancer cases are discovered when the woman feels a lump.[18] The earliest breast cancers are detected by a mammogram.[19] Lumps found in lymph nodes located in the armpits[18] can also indicate breast cancer.		Indications of breast cancer other than a lump may include thickening different from the other breast tissue, one breast becoming larger or lower, a nipple changing position or shape or becoming inverted, skin puckering or dimpling, a rash on or around a nipple, discharge from nipple/s, constant pain in part of the breast or armpit, and swelling beneath the armpit or around the collarbone.[20] Pain ("mastodynia") is an unreliable tool in determining the presence or absence of breast cancer, but may be indicative of other breast health issues.[18][19][21]		Inflammatory breast cancer is a particular type of breast cancer which can pose a substantial diagnostic challenge. Symptoms may resemble a breast inflammation and may include itching, pain, swelling, nipple inversion, warmth and redness throughout the breast, as well as an orange-peel texture to the skin referred to as peau d'orange.[18] As inflammatory breast cancer does not present as a lump there can sometimes be a delay in diagnosis.		Another reported symptom complex of breast cancer is Paget's disease of the breast. This syndrome presents as skin changes resembling eczema, such as redness, discoloration, or mild flaking of the nipple skin. As Paget's disease of the breast advances, symptoms may include tingling, itching, increased sensitivity, burning, and pain. There may also be discharge from the nipple. Approximately half of women diagnosed with Paget's disease of the breast also have a lump in the breast.[22]		In rare cases, what initially appears as a fibroadenoma (hard, movable non-cancerous lump) could in fact be a phyllodes tumor. Phyllodes tumors are formed within the stroma (connective tissue) of the breast and contain glandular as well as stromal tissue. Phyllodes tumors are not staged in the usual sense; they are classified on the basis of their appearance under the microscope as benign, borderline, or malignant.[23]		Occasionally, breast cancer presents as metastatic disease—that is, cancer that has spread beyond the original organ. The symptoms caused by metastatic breast cancer will depend on the location of metastasis. Common sites of metastasis include bone, liver, lung and brain.[24] Unexplained weight loss can occasionally signal breast cancer, as can symptoms of fevers or chills. Bone or joint pains can sometimes be manifestations of metastatic breast cancer, as can jaundice or neurological symptoms. These symptoms are called non-specific, meaning they could be manifestations of many other illnesses.[25]		Most symptoms of breast disorders, including most lumps, do not turn out to represent underlying breast cancer. Fewer than 20% of lumps, for example, are cancerous,[26] and benign breast diseases such as mastitis and fibroadenoma of the breast are more common causes of breast disorder symptoms. Nevertheless, the appearance of a new symptom should be taken seriously by both patients and their doctors, because of the possibility of an underlying breast cancer at almost any age.[27]		Risk factors can be divided into two categories:		The primary risk factors for breast cancer are being female and older age.[29] Other potential risk factors include genetics,[30] lack of childbearing or lack of breastfeeding,[31] higher levels of certain hormones,[32][33] certain dietary patterns, and obesity. Recent studies have indicated that exposure to light pollution is a risk factor for the development of breast cancer.[34]		Smoking tobacco appears to increase the risk of breast cancer, with the greater the amount smoked and the earlier in life that smoking began, the higher the risk.[35] In those who are long-term smokers, the risk is increased 35% to 50%.[35] A lack of physical activity has been linked to about 10% of cases.[36] Sitting regularly for prolonged periods is associated with higher mortality from breast cancer. The risk is not negated by regular exercise, though it is lowered.[37]		There is an association between use of hormonal birth control and the development of premenopausal breast cancer,[28][38] but whether oral contraceptives use may actually cause premenopausal breast cancer is a matter of debate.[39] If there is indeed a link, the absolute effect is small.[39][40] Additionally, it is not clear if the association exists with newer hormonal birth controls.[40] In those with mutations in the breast cancer susceptibility genes BRCA1 or BRCA2, or who have a family history of breast cancer, use of modern oral contraceptives does not appear to affect the risk of breast cancer.[41][42]		The association between breast feeding and breast cancer has not been clearly determined; some studies have found support for an association while others have not.[43] In the 1980s, the abortion–breast cancer hypothesis posited that induced abortion increased the risk of developing breast cancer.[44] This hypothesis was the subject of extensive scientific inquiry, which concluded that neither miscarriages nor abortions are associated with a heightened risk for breast cancer.[45]		A number of dietary factors have been linked to the risk for breast cancer. Dietary factors which may increase risk include a high fat diet,[46] high alcohol intake,[47] and obesity-related high cholesterol levels.[48][49] Dietary iodine deficiency may also play a role.[50] Evidence for fiber is unclear. A 2015 review found that studies trying to link fiber intake with breast cancer produced mixed results.[51] In 2016 a tentative association between low fiber intake during adolescence and breast cancer was observed.[52]		Other risk factors include radiation[53] and shift-work.[54] A number of chemicals have also been linked, including polychlorinated biphenyls, polycyclic aromatic hydrocarbons, and organic solvents[55] Although the radiation from mammography is a low dose, it is estimated that yearly screening from 40 to 80 years of age will cause approximately 225 cases of fatal breast cancer per million women screened.[56]		Some genetic susceptibility may play a minor role in most cases.[57] Overall, however, genetics is believed to be the primary cause of 5–10% of all cases.[58] Women whose mother was diagnosed before 50 have an increased risk of 1.7 and those whose mother was diagnosed at age 50 or after has an increased risk of 1.4.[59] In those with zero, one or two affected relatives, the risk of breast cancer before the age of 80 is 7.8%, 13.3%, and 21.1% with a subsequent mortality from the disease of 2.3%, 4.2%, and 7.6% respectively.[60] In those with a first degree relative with the disease the risk of breast cancer between the age of 40 and 50 is double that of the general population.[61]		In less than 5% of cases, genetics plays a more significant role by causing a hereditary breast–ovarian cancer syndrome.[57] This includes those who carry the BRCA1 and BRCA2 gene mutation.[57] These mutations account for up to 90% of the total genetic influence with a risk of breast cancer of 60–80% in those affected.[58] Other significant mutations include p53 (Li–Fraumeni syndrome), PTEN (Cowden syndrome), and STK11 (Peutz–Jeghers syndrome), CHEK2, ATM, BRIP1, and PALB2.[58] In 2012, researchers said that there are four genetically distinct types of the breast cancer and that in each type, hallmark genetic changes lead to many cancers.[62]		Breast changes like atypical ductal hyperplasia[63] and lobular carcinoma in situ,[64][65][66] found in benign breast conditions such as fibrocystic breast changes, are correlated with an increased breast cancer risk. Diabetes mellitus might also increase the risk of breast cancer.[67] Autoimmune diseases such as lupus erythematosus seem also to increase the risk for the acquisition of breast cancer.[68]		Breast cancer, like other cancers, occurs because of an interaction between an environmental (external) factor and a genetically susceptible host. Normal cells divide as many times as needed and stop. They attach to other cells and stay in place in tissues. Cells become cancerous when they lose their ability to stop dividing, to attach to other cells, to stay where they belong, and to die at the proper time.		Normal cells will commit cell suicide (programmed cell death) when they are no longer needed. Until then, they are protected from cell suicide by several protein clusters and pathways. One of the protective pathways is the PI3K/AKT pathway; another is the RAS/MEK/ERK pathway. Sometimes the genes along these protective pathways are mutated in a way that turns them permanently "on", rendering the cell incapable of committing suicide when it is no longer needed. This is one of the steps that causes cancer in combination with other mutations. Normally, the PTEN protein turns off the PI3K/AKT pathway when the cell is ready for programmed cell death. In some breast cancers, the gene for the PTEN protein is mutated, so the PI3K/AKT pathway is stuck in the "on" position, and the cancer cell does not commit suicide.[69]		Mutations that can lead to breast cancer have been experimentally linked to estrogen exposure.[70]		Abnormal growth factor signaling in the interaction between stromal cells and epithelial cells can facilitate malignant cell growth.[71][72] In breast adipose tissue, overexpression of leptin leads to increased cell proliferation and cancer.[73]		In the United States, 10 to 20 percent of people with breast cancer and people with ovarian cancer have a first- or second-degree relative with one of these diseases. The familial tendency to develop these cancers is called hereditary breast–ovarian cancer syndrome. The best known of these, the BRCA mutations, confer a lifetime risk of breast cancer of between 60 and 85 percent and a lifetime risk of ovarian cancer of between 15 and 40 percent. Some mutations associated with cancer, such as p53, BRCA1 and BRCA2, occur in mechanisms to correct errors in DNA. These mutations are either inherited or acquired after birth. Presumably, they allow further mutations, which allow uncontrolled division, lack of attachment, and metastasis to distant organs.[53][74] However, there is strong evidence of residual risk variation that goes well beyond hereditary BRCA gene mutations between carrier families. This is caused by unobserved risk factors.[75] This implicates environmental and other causes as triggers for breast cancers. The inherited mutation in BRCA1 or BRCA2 genes can interfere with repair of DNA cross links and DNA double strand breaks (known functions of the encoded protein).[76] These carcinogens cause DNA damage such as DNA cross links and double strand breaks that often require repairs by pathways containing BRCA1 and BRCA2.[77][78] However, mutations in BRCA genes account for only 2 to 3 percent of all breast cancers.[79] Levin et al. say that cancer may not be inevitable for all carriers of BRCA1 and BRCA2 mutations.[80] About half of hereditary breast–ovarian cancer syndromes involve unknown genes.		GATA-3 directly controls the expression of estrogen receptor (ER) and other genes associated with epithelial differentiation, and the loss of GATA-3 leads to loss of differentiation and poor prognosis due to cancer cell invasion and metastasis.[81]		Most types of breast cancer are easy to diagnose by microscopic analysis of a sample—or biopsy—of the affected area of the breast. Also, there are types of breast cancer that require specialized lab exams.		The two most commonly used screening methods, physical examination of the breasts by a healthcare provider and mammography, can offer an approximate likelihood that a lump is cancer, and may also detect some other lesions, such as a simple cyst.[82] When these examinations are inconclusive, a healthcare provider can remove a sample of the fluid in the lump for microscopic analysis (a procedure known as fine needle aspiration, or fine needle aspiration and cytology—FNAC) to help establish the diagnosis. The needle aspiration may be performed in a healthcare provider's office or clinic using local anaesthetic if required.[clarification needed] A finding of clear fluid makes the lump highly unlikely to be cancerous, but bloody fluid may be sent off for inspection under a microscope for cancerous cells. Together, physical examination of the breasts, mammography, and FNAC can be used to diagnose breast cancer with a good degree of accuracy.		Other options for biopsy include a core biopsy or vacuum-assisted breast biopsy,[83] which are procedures in which a section of the breast lump is removed; or an excisional biopsy, in which the entire lump is removed. Very often the results of physical examination by a healthcare provider, mammography, and additional tests that may be performed in special circumstances (such as imaging by ultrasound or MRI) are sufficient to warrant excisional biopsy as the definitive diagnostic and primary treatment method.		MRI showing breast cancer		Excised human breast tissue, showing an irregular, dense, white stellate area of cancer 2 cm in diameter, within yellow fatty tissue.		High-grade invasive ductal carcinoma, with minimal tubule formation, marked pleomorphism, and prominent mitoses, 40x field.		Micrograph showing a lymph node invaded by ductal breast carcinoma, with an extension of the tumor beyond the lymph node.		Neuropilin-2 expression in normal breast and breast carcinoma tissue.		F-18 FDG PET/CT: A breast cancer metastasis to the right scapula		Needle breast biopsy.		Elastography shows stiff cancer tissue on ultrasound imaging.		Ultrasound image shows irregularly shaped mass of breast cancer.		Infiltrating (Invasive) breast carcinoma.		Breast cancers are classified by several grading systems. Each of these influences the prognosis and can affect treatment response. Description of a breast cancer optimally includes all of these factors.		Stage T1 breast cancer		Stage T2 breast cancer		Stage T3 breast cancer		Stage 1A breast cancer		Stage 1B breast cancer		Stage 2A breast cancer		Stage 2A breast cancer		Stage 2B breast cancer		Stage 2B breast cancer		Stage 2B breast cancer		Stage 3A breast cancer		Stage 3A breast cancer		Stage 3A breast cancer		Stage 3B breast cancer		Stage 3B breast cancer		Stage 4 breast cancer		Women may reduce their risk of breast cancer by maintaining a healthy weight, drinking less alcohol, being physically active and breastfeeding their children.[90] These modifications might prevent 38% of breast cancers in the US, 42% in the UK, 28% in Brazil and 20% in China.[90] The benefits with moderate exercise such as brisk walking are seen at all age groups including postmenopausal women.[90][91] High levels of physical activity reduce the risk of breast cancer by about 14%.[92] Strategies that encourage regular physical activity and reduce obesity could also have other benefits, such as reduced risks of cardiovascular disease and diabetes.[28]		High intake of citrus fruit has been associated with a 10% reduction in the risk of breast cancer.[93]		Marine omega-3 polyunsaturated fatty acids appear to reduce the risk.[94] High consumption of soy-based foods may reduce risk.[95]		Removal of both breasts before any cancer has been diagnosed or any suspicious lump or other lesion has appeared (a procedure known as prophylactic bilateral mastectomy) may be considered in people with BRCA1 and BRCA2 mutations, which are associated with a substantially heightened risk for an eventual diagnosis of breast cancer.[96][97] Evidence is not strong enough to support this procedure in anyone but those at the highest risk.[98] BRCA testing is recommended in those with a high family risk after genetic counseling. It is not recommended routinely.[99] This is because there are many forms of changes in BRCA genes, ranging from harmless polymorphisms to obviously dangerous frameshift mutations. The effect of most of the identifiable changes in the genes is uncertain. Testing in an average-risk person is particularly likely to return one of these indeterminate, useless results. It is unclear if removing the second breast in those who have breast cancer in one is beneficial.[98]		The selective estrogen receptor modulators (such as tamoxifen) reduce the risk of breast cancer but increase the risk of thromboembolism and endometrial cancer.[100][100] There is no overall change in the risk of death.[100][101] They are thus not recommended for the prevention of breast cancer in women at average risk but may be offered for those at high risk.[102] The benefit of breast cancer reduction continues for at least five years after stopping a course of treatment with these medications.[103]		Breast cancer screening refers to testing otherwise-healthy women for breast cancer in an attempt to achieve an earlier diagnosis under the assumption that early detection will improve outcomes. A number of screening tests have been employed including clinical and self breast exams, mammography, genetic screening, ultrasound, and magnetic resonance imaging.		A clinical or self breast exam involves feeling the breast for lumps or other abnormalities. Clinical breast exams are performed by health care providers, while self-breast exams are performed by the person themselves.[104] Evidence does not support the effectiveness of either type of breast exam, as by the time a lump is large enough to be found it is likely to have been growing for several years and thus soon be large enough to be found without an exam.[105][106] Mammographic screening for breast cancer uses X-rays to examine the breast for any uncharacteristic masses or lumps. During a screening, the breast is compressed and a technician takes photos from multiple angles. A general mammogram takes photos of the entire breast, while a diagnostic mammogram focuses on a specific lump or area of concern.[107]		A number of national bodies recommend breast cancer screening. For the average woman, the U.S. Preventive Services Task Force recommends mammography every two years in women between the ages of 50 and 74,[12] the Council of Europe recommends mammography between 50 and 69 with most programs using a 2-year frequency,[108] and in Canada screening is recommended between the ages of 50 and 74 at a frequency of 2 to 3 years.[109] These task force reports point out that in addition to unnecessary surgery and anxiety, the risks of more frequent mammograms include a small but significant increase in breast cancer induced by radiation.[110]		The Cochrane collaboration (2013) states that the best quality evidence neither demonstrates a reduction in cancer specific, nor a reduction in all cause mortality from screening mammography.[10] When less rigorous trials are added to the analysis there is a reduction in mortality due to breast cancer of 0.05% (a decrease of 1 in 2000 deaths from breast cancer over 10 years or a relative decrease of 15% from breast cancer).[10] Screening over 10 years results in a 30% increase in rates of over-diagnosis and over-treatment (3 to 14 per 1000) and more than half will have at least one falsely positive test.[10][111][112][needs update] This has resulted in the view that it is not clear whether mammography screening does more good or harm.[10] Cochrane states that, due to recent improvements in breast cancer treatment, and the risks of false positives from breast cancer screening leading to unnecessary treatment, "it therefore no longer seems beneficial to attend for breast cancer screening" at any age.[113] Whether MRI as a screening method has greater harms or benefits when compared to standard mammography is not known.[114]		The management of breast cancer depends on various factors, including the stage of the cancer and the age of the patient. Increasingly aggressive treatments are employed in accordance with the poorer the patient's prognosis and the higher the risk of recurrence of the cancer following treatment.		Breast cancer is usually treated with surgery, which may be followed by chemotherapy or radiation therapy, or both. A multidisciplinary approach is preferable.[115] Hormone receptor-positive cancers are often treated with hormone-blocking therapy over courses of several years. Monoclonal antibodies, or other immune-modulating treatments, may be administered in certain cases of metastatic and other advanced stages of breast cancer.		Surgery involves the physical removal of the tumor, typically along with some of the surrounding tissue. One or more lymph nodes may be biopsied during the surgery; increasingly the lymph node sampling is performed by a sentinel lymph node biopsy.		Standard surgeries include:		Once the tumor has been removed, if the patient desires, breast reconstruction surgery, a type of plastic surgery, may then be performed to improve the aesthetic appearance of the treated site. Alternatively, women use breast prostheses to simulate a breast under clothing, or choose a flat chest. Nipple prosthesis can be used at any time following the mastectomy.		Drugs used after and in addition to surgery are called adjuvant therapy. Chemotherapy or other types of therapy prior to surgery are called neoadjuvant therapy. Aspirin may reduce mortality from breast cancer.[116]		There are currently three main groups of medications used for adjuvant breast cancer treatment: hormone-blocking agents, chemotherapy, and monoclonal antibodies.		Hormone blocking therapy		Chemotherapy		Monoclonal antibodies		Radiotherapy is given after surgery to the region of the tumor bed and regional lymph nodes, to destroy microscopic tumor cells that may have escaped surgery. It may also have a beneficial effect on tumor microenvironment.[125][126] Radiation therapy can be delivered as external beam radiotherapy or as brachytherapy (internal radiotherapy). Conventionally radiotherapy is given after the operation for breast cancer. Radiation can also be given at the time of operation on the breast cancer. Radiation can reduce the risk of recurrence by 50–66% (1/2 – 2/3 reduction of risk) when delivered in the correct dose[127] and is considered essential when breast cancer is treated by removing only the lump (Lumpectomy or Wide local excision).		Prognosis is usually given for the probability of progression-free survival (PFS) or disease-free survival (DFS). These predictions are based on experience with breast cancer patients with similar classification. A prognosis is an estimate, as patients with the same classification will survive a different amount of time, and classifications are not always precise. Survival is usually calculated as an average number of months (or years) that 50% of patients survive, or the percentage of patients that are alive after 1, 5, 15, and 20 years. Prognosis is important for treatment decisions because patients with a good prognosis are usually offered less invasive treatments, such as lumpectomy and radiation or hormone therapy, while patients with poor prognosis are usually offered more aggressive treatment, such as more extensive mastectomy and one or more chemotherapy drugs.		Prognostic factors are reflected in the classification scheme for breast cancer including stage, (i.e., tumor size, location, whether disease has spread to lymph nodes and other parts of the body), grade, recurrence of the disease, and the age and health of the patient. The Nottingham Prognostic Index is a commonly used prognostic tool.		The stage of the breast cancer is the most important component of traditional classification methods of breast cancer, because it has a greater effect on the prognosis than the other considerations. Staging takes into consideration size, local involvement, lymph node status and whether metastatic disease is present. The higher the stage at diagnosis, the poorer the prognosis. The stage is raised by the invasiveness of disease to lymph nodes, chest wall, skin or beyond, and the aggressiveness of the cancer cells. The stage is lowered by the presence of cancer-free zones and close-to-normal cell behaviour (grading). Size is not a factor in staging unless the cancer is invasive. For example, Ductal Carcinoma In Situ (DCIS) involving the entire breast will still be stage zero and consequently an excellent prognosis with a 10-year disease free survival of about 98%.[128]		The breast cancer grade is assessed by comparison of the breast cancer cells to normal breast cells. The closer to normal the cancer cells are, the slower their growth and the better the prognosis. If cells are not well differentiated, they will appear immature, will divide more rapidly, and will tend to spread. Well differentiated is given a grade of 1, moderate is grade 2, while poor or undifferentiated is given a higher grade of 3 or 4 (depending upon the scale used). The most widely used grading system is the Nottingham scheme;[132] details are provided in the discussion of breast cancer grade.		The presence of estrogen and progesterone receptors in the cancer cell is important in guiding treatment. Those who do not test positive for these specific receptors will not be able to respond to hormone therapy, and this can affect their chance of survival depending upon what treatment options remain, the exact type of cancer, and how advanced the disease is.		In addition to hormone receptors, there are other cell surface proteins that may affect prognosis and treatment. HER2 status directs the course of treatment. Patients whose cancer cells are positive for HER2 have a more aggressive disease and may be treated with the 'targeted therapy', trastuzumab (Herceptin), a monoclonal antibody that targets this protein and improves the prognosis significantly.		Younger women with an age of less than 40 years or women over 80 years tend to have a poorer prognosis than post-menopausal women due to several factors. Their breasts may change with their menstrual cycles, they may be nursing infants, and they may be unaware of changes in their breasts. Therefore, younger women are usually at a more advanced stage when diagnosed. There may also be biologic factors contributing to a higher risk of disease recurrence for younger women with breast cancer.[133][134]		High mammographic breast density, which is a marker of increased risk of developing breast cancer, may not mean an increased risk of death among breast cancer patients, according to a 2012 report of a study involving 9232 women by the National Cancer Institute (NCI).[135] On the other hand, more recent research has shown that women with extremely low mammographic densities (<10%) hold a significantly worse prognosis compared to women with other densities, irrespective of all possible confounding factors.[136]		Since breast cancer in males is usually detected at later stages, outcomes are typically worse.[137]		The emotional impact of cancer diagnosis, symptoms, treatment, and related issues can be severe. Most larger hospitals are associated with cancer support groups which provide a supportive environment to help patients cope and gain perspective from cancer survivors.		Not all breast cancer patients experience their illness in the same manner. Factors such as age can have a significant impact on the way a patient copes with a breast cancer diagnosis. Premenopausal women with estrogen-receptor positive breast cancer must confront the issues of early menopause induced by many of the chemotherapy regimens used to treat their breast cancer, especially those that use hormones to counteract ovarian function.[138]		On the other hand, a small 2007 study conducted by researchers at the College of Public Health of the University of Georgia suggested a need for greater attention to promoting functioning and psychological well-being among older cancer survivors, even when they may not have obvious cancer-related medical complications.[139] The study found that older breast cancer survivors showed multiple indications of decrements in their health-related quality of life, and lower psychosocial well-being than a comparison group. Survivors reported no more depressive symptoms or anxious mood than the comparison group, however, they did score lower on measures of positive psychosocial well-being and reported more depressed mood and days affected by fatigue. As the incidence of breast cancer in women over 50 rises and survival rates increase, breast cancer is increasingly becoming a geriatric issue that warrants both further research and the expansion of specialized cancer support services tailored for specific age groups.[139]		Worldwide, breast cancer is the most common invasive cancer in women.[141] It affects about 12% of women worldwide.[141] (The most common form of cancer is non-invasive non-melanoma skin cancer; non-invasive cancers are generally easily cured, cause very few deaths, and are routinely excluded from cancer statistics.) Breast cancer comprises 22.9% of invasive cancers in women[142] and 16% of all female cancers.[143] In 2012, it comprised 25.2% of cancers diagnosed in women, making it the most common female cancer.[144]		In 2008, breast cancer caused 458,503 deaths worldwide (13.7% of cancer deaths in women and 6.0% of all cancer deaths for men and women together).[142] Lung cancer, the second most common cause of cancer-related death in women, caused 12.8% of cancer deaths in women (18.2% of all cancer deaths for men and women together).[142]		The incidence of breast cancer varies greatly around the world: it is lowest in less-developed countries and greatest in the more-developed countries. In the twelve world regions, the annual age-standardized incidence rates per 100,000 women are as follows: in Eastern Asia, 18; South Central Asia, 22; sub-Saharan Africa, 22; South-Eastern Asia, 26; North Africa and Western Asia, 28; South and Central America, 42; Eastern Europe, 49; Southern Europe, 56; Northern Europe, 73; Oceania, 74; Western Europe, 78; and in North America, 90.[145]		The number of cases worldwide has significantly increased since the 1970s, a phenomenon partly attributed to the modern lifestyles.[146][147] Breast cancer is strongly related to age with only 5% of all breast cancers occurring in women under 40 years old.[148] There were more than 41,000 newly diagnosed cases of breast cancer registered in England in 2011, around 80% of these cases were in women age 50 or older [149] Based on U.S. statistics in 2015 there were 2.8 million women affected by breast cancer.[141]		Because of its visibility, breast cancer was the form of cancer most often described in ancient documents.[150] Because autopsies were rare, cancers of the internal organs were essentially invisible to ancient medicine. Breast cancer, however, could be felt through the skin, and in its advanced state often developed into fungating lesions: the tumor would become necrotic (die from the inside, causing the tumor to appear to break up) and ulcerate through the skin, weeping fetid, dark fluid.[150]		The oldest evidence of breast cancer was discovered in Egypt in 2015 and dates back to the Sixth Dynasty.[151] The study of a woman's remains from the necropolis of Qubbet el-Hawa showed the typical destructive damage due to metastatic spread.[151] The Edwin Smith Papyrus describes 8 cases of tumors or ulcers of the breast that were treated by cauterization. The writing says about the disease, "There is no treatment."[152] For centuries, physicians described similar cases in their practices, with the same conclusion. Ancient medicine, from the time of the Greeks through the 17th century, was based on humoralism, and thus believed that breast cancer was generally caused by imbalances in the fundamental fluids that controlled the body, especially an excess of black bile.[153] Alternatively, patients often saw it as divine punishment.[154] In the 18th century, a wide variety of medical explanations were proposed, including a lack of sexual activity, too much sexual activity, physical injuries to the breast, curdled breast milk, and various forms of lymphatic blockages, either internal or due to restrictive clothing.[153][155] In the 19th century, the Scottish surgeon John Rodman said that fear of cancer caused cancer, and that this anxiety, learned by example from the mother, accounted for breast cancer's tendency to run in families.[155]		Although breast cancer was known in ancient times, it was uncommon until the 19th century, when improvements in sanitation and control of deadly infectious diseases resulted in dramatic increases in lifespan. Previously, most women had died too young to have developed breast cancer.[155] Additionally, early and frequent childbearing and breastfeeding probably reduced the rate of breast cancer development in those women who did survive to middle age.[155]		Because ancient medicine believed that the cause was systemic, rather than local, and because surgery carried a high mortality rate, the preferred treatments tended to be pharmacological rather than surgical. Herbal and mineral preparations, especially involving the poison arsenic, were relatively common.		Mastectomy for breast cancer was performed at least as early as AD 548, when it was proposed by the court physician Aetios of Amida to Theodora.[150] It was not until doctors achieved greater understanding of the circulatory system in the 17th century that they could link breast cancer's spread to the lymph nodes in the armpit. The French surgeon Jean Louis Petit (1674–1750) and later the Scottish surgeon Benjamin Bell (1749–1806) were the first to remove the lymph nodes, breast tissue, and underlying chest muscle.[156]		Their successful work was carried on by William Stewart Halsted who started performing radical mastectomies in 1882, helped greatly by advances in general surgical technology, such as aseptic technique and anesthesia. The Halsted radical mastectomy often involved removing both breasts, associated lymph nodes, and the underlying chest muscles. This often led to long-term pain and disability, but was seen as necessary in order to prevent the cancer from recurring.[157] Before the advent of the Halsted radical mastectomy, 20-year survival rates were only 10%; Halsted's surgery raised that rate to 50%.[158] Extending Halsted's work, Jerome Urban promoted superradical mastectomies, taking even more tissue, until 1963, when the ten-year survival rates proved equal to the less-damaging radical mastectomy.[157]		Radical mastectomies remained the standard of care in America until the 1970s, but in Europe, breast-sparing procedures, often followed radiation therapy, were generally adopted in the 1950s.[157] One reason for this striking difference in approach may be the structure of the medical professions: European surgeons, descended from the barber surgeon, were held in less esteem than physicians; in America, the surgeon was the king of the medical profession.[157] Additionally, there were far more European women surgeons: Less than one percent of American surgical oncologists were female, but some European breast cancer wards boasted a medical staff that was half female.[157] American health insurance companies also paid surgeons more to perform radical mastectomies than they did to perform more intricate breast-sparing surgeries.[157]		Breast cancer staging systems were developed in the 1920s and 1930s.[157]		During the 1970s, a new understanding of metastasis led to perceiving cancer as a systemic illness as well as a localized one, and more sparing procedures were developed that proved equally effective. Modern chemotherapy developed after World War II.[159]		The French surgeon Bernard Peyrilhe (1737–1804) realized the first experimental transmission of cancer by injecting extracts of breast cancer into an animal.		Prominent women who died of breast cancer include Anne of Austria, the mother of Louis XIV of France; Mary Washington, mother of George, and Rachel Carson, the environmentalist.[160]		The first case-controlled study on breast cancer epidemiology was done by Janet Lane-Claypon, who published a comparative study in 1926 of 500 breast cancer cases and 500 control patients of the same background and lifestyle for the British Ministry of Health.[161]		In the 1980s and 1990s, thousands of women who had successfully completed standard treatment then demanded and received high-dose bone marrow transplants, thinking this would lead to better long-term survival. However, it proved completely ineffective, and 15–20% of women died because of the brutal treatment.[162]		The 1995 reports from the Nurses' Health Study and the 2002 conclusions of the Women's Health Initiative trial conclusively proved that hormone replacement therapy significantly increased the incidence of breast cancer.[162]		Before the 20th century, breast cancer was feared and discussed in hushed tones, as if it were shameful. As little could be safely done with primitive surgical techniques, women tended to suffer silently rather than seeking care. When surgery advanced, and long-term survival rates improved, women began raising awareness of the disease and the possibility of successful treatment. The "Women's Field Army", run by the American Society for the Control of Cancer (later the American Cancer Society) during the 1930s and 1940s was one of the first organized campaigns. In 1952, the first peer-to-peer support group, called "Reach to Recovery", began providing post-mastectomy, in-hospital visits from women who had survived breast cancer.[163]		The breast cancer movement of the 1980s and 1990s developed out of the larger feminist movements and women's health movement of the 20th century.[164] This series of political and educational campaigns, partly inspired by the politically and socially effective AIDS awareness campaigns, resulted in the widespread acceptance of second opinions before surgery, less invasive surgical procedures, support groups, and other advances in patient care.[165]		A pink ribbon is the most prominent symbol of breast cancer awareness. Pink ribbons, which can be made inexpensively, are sometimes sold as fundraisers, much like poppies on Remembrance Day. They may be worn to honor those who have been diagnosed with breast cancer, or to identify products that the manufacturer would like to sell to consumers that are interested in breast cancer.[166]		The pink ribbon is associated with individual generosity, faith in scientific progress, and a "can-do" attitude. It encourages consumers to focus on the emotionally appealing ultimate vision of a cure for breast cancer, rather than on the fraught path between current knowledge and any future cures.[167]		Wearing or displaying a pink ribbon has been criticized by the opponents of this practice as a kind of slacktivism, because it has no practical positive effect. It has also been criticized as hypocrisy, because some people wear the pink ribbon to show good will towards women with breast cancer, but then oppose these women's practical goals, like patient rights and anti-pollution legislation.[168][169] Critics say that the feel-good nature of pink ribbons and pink consumption distracts society from the lack of progress on preventing and curing breast cancer.[170] It is also criticized for reinforcing gender stereotypes and objectifying women and their breasts.[171] Breast Cancer Action launched the "Think Before You Pink" campaign, and said that businesses have co-opted the pink campaign to promote products that cause breast cancer, such as alcoholic beverages.[172]		Breast cancer culture, or pink ribbon culture, is the set of activities, attitudes, and values that surround and shape breast cancer in public. The dominant values are selflessness, cheerfulness, unity, and optimism. Appearing to have suffered bravely is the passport into the culture.		The woman with breast cancer is given a cultural template that constrains her emotional and social responses into a socially acceptable discourse: She is to use the emotional trauma of being diagnosed with breast cancer and the suffering of extended treatment to transform herself into a stronger, happier and more sensitive person who is grateful for the opportunity to become a better person. Breast cancer therapy becomes a rite of passage rather than a disease.[173] To fit into this mold, the woman with breast cancer needs to normalize and feminize her appearance, and minimize the disruption that her health issues cause anyone else. Anger, sadness, and negativity must be silenced.[173]		As with most cultural models, people who conform to the model are given social status, in this case as cancer survivors. Women who reject the model are shunned, punished and shamed.[173]		The culture is criticized for treating adult women like little girls, as evidenced by "baby" toys such as pink teddy bears given to adult women.[173]		The primary purposes or goals of breast cancer culture are to maintain breast cancer's dominance as the preëminent women's health issue, to promote the appearance that society is "doing something" effective about breast cancer, and to sustain and expand the social, political, and financial power of breast cancer activists.[174]		Compared to other diseases or other cancers, breast cancer receives a proportionately greater share of resources and attention. In 2001 MP Ian Gibson, chairman of the House of Commons of the United Kingdom all party group on cancer stated "The treatment has been skewed by the lobbying, there is no doubt about that. Breast cancer sufferers get better treatment in terms of bed spaces, facilities and doctors and nurses."[175] Breast cancer also receives significantly more media coverage than other, equally prevalent cancers, with a study by Prostate Coalition showing 2.6 breast cancer stories for each one covering cancer of the prostate.[176] Ultimately there is a concern that favoring sufferers of breast cancer with disproportionate funding and research on their behalf may well be costing lives elsewhere.[175] Partly because of its relatively high prevalence and long-term survival rates, research is biased towards breast cancer. Some subjects, such as cancer-related fatigue, have been studied little except in women with breast cancer.		One result of breast cancer's high visibility is that statistical results can sometimes be misinterpreted, such as the claim that one in eight women will be diagnosed with breast cancer during their lives—a claim that depends on the unrealistic assumption that no woman will die of any other disease before the age of 95.[177] This obscures the reality, which is that about ten times as many women will die from heart disease or stroke than from breast cancer.[178]		The emphasis on breast cancer screening may be harming women by subjecting them to unnecessary radiation, biopsies, and surgery. One-third of diagnosed breast cancers might recede on their own.[179] Screening mammography efficiently finds non-life-threatening, asymptomatic breast cancers and pre-cancers, even while overlooking serious cancers. According to H. Gilbert Welch of the Dartmouth Institute for Health Policy and Clinical Practice, research on screening mammography has taken the "brain-dead approach that says the best test is the one that finds the most cancers" rather than the one that finds dangerous cancers.[179]		Breast cancers occur during pregnancy at the same rate as breast cancers in non-pregnant women of the same age. Breast cancer then becomes more common in the 5 or 10 years following pregnancy but then becomes less common than among the general population.[180] These cancers are known as postpartum breast cancer and have worse outcomes including an increased risk of distant spread of disease and mortality.[181] Other cancers found during or shortly after pregnancy appear at approximately the same rate as other cancers in women of a similar age.[182]		Diagnosing new cancer in a pregnant woman is difficult, in part because any symptoms are commonly assumed to be a normal discomfort associated with pregnancy.[182] As a result, cancer is typically discovered at a somewhat later stage than average in many pregnant or recently pregnant women. Some imaging procedures, such as MRIs (magnetic resonance imaging), CT scans, ultrasounds, and mammograms with fetal shielding are considered safe during pregnancy; some others, such as PET scans are not.[182]		Treatment is generally the same as for non-pregnant women.[182] However, radiation is normally avoided during pregnancy, especially if the fetal dose might exceed 100 cGy. In some cases, some or all treatments are postponed until after birth if the cancer is diagnosed late in the pregnancy. Early deliveries to speed the start of treatment are not uncommon. Surgery is generally considered safe during pregnancy, but some other treatments, especially certain chemotherapy drugs given during the first trimester, increase the risk of birth defects and pregnancy loss (spontaneous abortions and stillbirths).[182] Elective abortions are not required and do not improve the likelihood of the mother surviving or being cured.[182]		Radiation treatments may interfere with the mother's ability to breastfeed her baby because it reduces the ability of that breast to produce milk and increases the risk of mastitis. Also, when chemotherapy is being given after birth, many of the drugs pass through breast milk to the baby, which could harm the baby.[182]		Regarding future pregnancy among breast cancer survivors, there is often fear of cancer recurrence.[183] On the other hand, many still regard pregnancy and parenthood to represent normalcy, happiness and life fulfillment.[183]		In breast cancer survivors, non-hormonal birth control methods should be used as first-line options. Progestogen-based methods such as depot medroxyprogesterone acetate, IUD with progestogen or progestogen only pills have a poorly investigated but possible increased risk of cancer recurrence, but may be used if positive effects outweigh this possible risk.[184]		In breast cancer survivors, it is recommended to first consider non-hormonal options for menopausal effects, such as bisphosphonates or selective estrogen receptor modulators (SERMs) for osteoporosis, and vaginal estrogen for local symptoms. Observational studies of systemic hormone replacement therapy after breast cancer are generally reassuring. If hormone replacement is necessary after breast cancer, estrogen-only therapy or estrogen therapy with an intrauterine device with progestogen may be safer options than combined systemic therapy.[185]		Treatments are being evaluated in trials. This includes individual drugs, combinations of drugs, and surgical and radiation techniques Investigations include new types of targeted therapy,[186] cancer vaccines, oncolytic virotherapy,[187] and immunotherapy.[188]		The latest research is reported annually at scientific meetings such as that of the American Society of Clinical Oncology, San Antonio Breast Cancer Symposium,[189] and the St. Gallen Oncology Conference in St. Gallen, Switzerland.[190] These studies are reviewed by professional societies and other organizations, and formulated into guidelines for specific treatment groups and risk category.		Fenretinide, a retinoid, is also being studied as a way to reduce the risk of breast cancer (retinoids are drugs related to vitamin A).[191][192]		As of 2014 cryoablation is being studied to see if it could be a substitute for a lumpectomy in small cancers.[193] There is tentative evidence in those with tumors less than 2 centimeters.[194] It may also be used in those in who surgery is not possible.[194] Another review states that cryoablation looks promising for early breast cancer of small size.[195]		A considerable part of the current knowledge on breast carcinomas is based on in vivo and in vitro studies performed with cell lines derived from breast cancers. These provide an unlimited source of homogenous self-replicating material, free of contaminating stromal cells, and often easily cultured in simple standard media. The first breast cancer cell line described, BT-20, was established in 1958. Since then, and despite sustained work in this area, the number of permanent lines obtained has been strikingly low (about 100). Indeed, attempts to culture breast cancer cell lines from primary tumors have been largely unsuccessful. This poor efficiency was often due to technical difficulties associated with the extraction of viable tumor cells from their surrounding stroma. Most of the available breast cancer cell lines issued from metastatic tumors, mainly from pleural effusions. Effusions provided generally large numbers of dissociated, viable tumor cells with little or no contamination by fibroblasts and other tumor stroma cells. Many of the currently used BCC lines were established in the late 1970s. A very few of them, namely MCF-7, T-47D, and MDA-MB-231, account for more than two-thirds of all abstracts reporting studies on mentioned breast cancer cell lines, as concluded from a Medline-based survey.		NFAT transcription factors are implicated in breast cancer, more specifically in the process of cell motility at the basis of metastasis formation. Indeed, NFAT1 (NFATC2) and NFAT5 are pro-invasive and pro-migratory in breast carcinoma[196][197] and NFAT3 (NFATc4) is an inhibitor of cell motility.[198] NFAT1 regulates the expression of the TWEAKR and its ligand TWEAK with the Lipocalin 2 to increase breast cancer cell invasion[199] and NFAT3 inhibits Lipocalin 2 expression to blunt the cell invasion.[198]		Clinically, the most useful metabolic markers in breast cancer are the estrogen and progesterone receptors that are used to predict response to hormone therapy. New or potentially new markers for breast cancer include BRCA1 and BRCA2[200] to identify patients at high risk of developing breast cancer, HER-2[201] and SCD1[202] for predicting response to therapeutic regimens, and urokinase plasminogen activator, PA1-1[203] and SCD1[204] for assessing prognosis.										
Endurance training is the act of exercising to increase endurance. The term endurance training generally refers to training the aerobic system as opposed to the anaerobic system. The need for endurance in sports is often predicated as the need of cardiovascular and simple muscular endurance, but the issue of endurance is far more complex. Endurance can be divided into two categories including: general endurance and specific endurance. It can be shown that endurance in sport is closely tied to the execution of skill and technique. A well conditioned athlete can be defined as, the athlete who executes his or her technique consistently and effectively with the least effort.[1]						Endurance training is essential for a variety of endurance sports. A notable example is distance running events (800 meters upwards to marathon and ultra-marathon) with the required degree of endurance training increasing with race distance. Two other popular examples are cycling (particularly road cycling) and competitive swimming. These three endurance sports are combined in triathlon. Other sports for which extensive amounts of endurance training are required include rowing and cross country skiing. Athletes can also undergo endurance training when their sport may not necessarily be an endurance sport in the whole sense but may still demand some endurance. For instance aerobic endurance is necessary (to varying extents) in racket sports, football, rugby, martial arts, basketball and cricket. Endurance exercise tends to be popular with non-athletes for the purpose of increasing general fitness or burning more calories to increase weight loss potential.		Long-term endurance training induces many physiological adaptations both centrally and peripherally mediated. Central cardiovascular adaptations include decreased heart rate, increased stroke volume of the heart,[2] increased blood plasma, without any major changes in red blood cell count, which reduces blood viscosity and increased cardiac output as well as total mitochondrial volume in the muscle fibers used in the training (i.e. the thigh muscles in runners will have more mitochondria than the thigh muscles of swimmers). Mitochondria increase in both number and size and there are similar increases in myoglobin and oxidative enzymes. Adaptations of the peripheral include capillarization, that is an increase in the surface area that both the venous and arterial capillaries supply. This also allows for increased heat dissipation during strenuous exercise. The muscles heighten their glycogen and fat storing capabilities in endurance athletes in order to increase the length in time in which they can perform work. Endurance training primarily work the slow twitch (type 1) fibers and develop such fibers in their efficiency and resistance to fatigue. Catabolism also improves increasing the athletes capacity to use fat and glycogen stores as an energy source. These metabolic processes are known as glycogenolysis, glycolysis and lipolysis. There is higher efficiency in oxygen transport and distribution. In recent years it has been recognized that oxidative enzymes such as succinate dehydrogenase (SDH) that enable mitochondria to break down nutrients to form ATP increase by 2.5 times in well trained endurance athletes[2] In addition to SDH, myoglobin increases by 75-80% in well trained endurance athletes.[2]		The potential for negative health effects from long-term, high-volume endurance training have begun to emerge in the scientific literature in recent years.[3][4][5] The known risks are primarily associated with training for and participation in extreme endurance events, and affect the cardiovascular system through adverse structural remodeling of the heart and the associated arteries, with heart-rhythm abnormalities perhaps being the most common resulting symptom.[6] Endurance exercise can also reduce testosterone levels.[7][8]		Common methods for training include periodization, intervals, hard easy, long slow distance, and in recent years high-intensity interval training. The periodization method is very common and was accredited to Tudor Bompa[9] and consists of blocks of time, generally 4–12 weeks each. The blocks are called preparation, base, build and race.[10] The goal of a structured training program with periodization is to bring the athlete into peak fitness at the time of a big race or event. Preparation as the name suggests lays the groundwork for heavier work to follow. For a runner contemplating a competitive marathon the preparation phase might consist of easier runs of 1–4 miles 3-4 times per week and including 2–3 days of core strengthening. In the base phase the athlete now works on building cardiovascular endurance by having several long runs staying in heart rate zone 1-2[clarification needed] every week and each week adding slightly more mileage (using 10% rule for safely increasing the mileage). Core strengthening is continued in the base period. Once the base phase is complete and the athlete has sufficient endurance, the build period is needed to give the athlete the ability to hold a faster pace for the race duration. The build phase is where duration of runs is traded for intensity or heart rate zones 3-5. An easy method to obtain intensity is interval training and interval training starts to happen in the build phase. Through interval training during the build phase the athlete can achieve higher lactate threshold and in some athletes VO2 max is increased. Because interval training is demanding on the body, a professional coach should be consulted. In the very least the athlete should do a warm up and active stretching before the interval session and static stretch or yoga after hard interval sessions. It is also advisable to have days of rest or easy workouts the day after interval sessions.[11] Finally the race phase of the periodization approach is where the duration of the workouts decreases but intense workouts remain so as to keep the high lactate threshold that was gained in the build phase. In Ironman training, the race phase is where a long "taper" occurs of up to 4 weeks for highly trained Ironman racers. A final phase is designated transition and is a period of time, where the body is allowed to recover from the hard race effort and some maintenance endurance training is performed so the high fitness level attained in the previous periods will not be lost.		Traditionally, strength training (the performance of exercises with resistance or added weight) was not deemed appropriate for endurance athletes due to potential interference in the adaptive response to the endurance elements of an athlete's training plan. There were also misconceptions regarding the addition of excess body mass through muscle hypertrophy (growth) associated with strength training, which could negatively effect endurance performance by increasing the amount of work required to be completed by the athlete. However, more recent and comprehensive research has proved that short-term (8 weeks) strength training in addition to endurance training is beneficial for endurance performance, particularly long-distance running.[12]		The heart rate monitor is one of the relatively easy methods to assess fitness in endurance athletes. By comparing heart rate over time fitness gains can be observed when the heart rate decreases for running or cycling at a given speed. In cycling the effect of wind on the cyclists speed is difficult to subtract out and so many cyclists now use power meters built into their bicycles. The power meter allows the athlete to actually measure power output over a set duration or course and allows direct comparison of fitness progression.[13] In the 2008 Olympics Michael Phelps was aided by repeated lactate threshold measurement. This allowed his coaches to fine tune his training program so that he could recover between swim events that were sometimes several minutes apart.[14] Much similar to blood glucose for diabetes, lower priced lactate measurement devices are now available but in general the lactate measurement approach is still the domain of the professional coach and elite athlete.		|group2 = See also |list2 =		|below = }}		
Mental health is a level of psychological well-being, or an absence of mental illness. It is the "psychological state of someone who is functioning at a satisfactory level of emotional and behavioral adjustment".[1] From the perspective of positive psychology or holism, mental health may include an individual's ability to enjoy life, and create a balance between life activities and efforts to achieve psychological resilience.		According to the World Health Organization (WHO), mental health includes "subjective well-being, perceived self-efficacy, autonomy, competence, inter-generational dependence, and self-actualization of one's intellectual and emotional potential, among others."[2] The WHO further states that the well-being of an individual is encompassed in the realization of their abilities, coping with normal stresses of life, productive work and contribution to their community.[3] Cultural differences, subjective assessments, and competing professional theories all affect how "mental health" is defined.[2] A widely accepted definition of health by mental health specialists is psychoanalyst Sigmund Freud's definition: the capacity "to work and to love".[4]						According to the U.S. surgeon general (1999), mental health is the successful performance of mental function, resulting in productive activities, fulfilling relationships with other people, and providing the ability to adapt to change and cope with adversity. The term mental illness refers collectively to all diagnosable mental disorders—health conditions characterized by alterations in thinking, mood, or behavior associated with distress or impaired functioning.[5]		A person struggling with their mental health may experience stress, depression, anxiety, relationship problems, grief, addiction, ADHD or learning disabilities, mood disorders, or other mental illnesses of varying degrees.[6][7] Therapists, psychiatrists, psychologists, social workers, nurse practitioners or physicians can help manage mental illness with treatments such as therapy, counseling, or medication.		In the mid-19th century, William Sweetser was the first to coin the term "mental hygiene", which can be seen as the precursor to contemporary approaches to work on promoting positive mental health.[8][9] Isaac Ray, one of the thirteen[citation needed] founders of the American Psychiatric Association, further defined mental hygiene as "the art of preserving the mind against all incidents and influences calculated to deteriorate its qualities, impair its energies, or derange its movements."[9]		Dorothea Dix (1802–1887) was an important figure in the development of "mental hygiene" movement. Dix was a school teacher who endeavored throughout her life to help people with mental disorders, and to bring to light the deplorable conditions into which they were put.[10] This was known as the "mental hygiene movement".[10] Before this movement, it was not uncommon that people affected by mental illness in the 19th century would be considerably neglected, often left alone in deplorable conditions, barely even having sufficient clothing.[10] Dix's efforts were so great that there was a rise in the number of patients in mental health facilities, which sadly resulted in these patients receiving less attention and care, as these institutions were largely understaffed.[10]		Emil Kraepelin in 1896 developed the taxonomy mental disorders which has dominated the field for nearly 80 years. Later the proposed disease model of abnormality was subjected to analysis and considered normality to be relative to the physical, geographical and cultural aspects of the defining group.		At the beginning of the 20th century, Clifford Beers founded the Mental Health America – National Committee for Mental Hygiene after publication of his accounts from lived experience in lunatic asylums "A mind that found itself" in 1908[11] and opened the first outpatient mental health clinic in the United States.[12]		The mental hygiene movement, related to the social hygiene movement, had at times been associated with advocating eugenics and sterilisation of those considered too mentally deficient to be assisted into productive work and contented family life.[13][14] In the post-WWII years, references to mental hygiene were gradually replaced by the term 'mental health' due to its positive aspect that evolves from the treatment of illness to preventive and promotive areas of healthcare.[15]		Mental illnesses are more common than cancer, diabetes, or heart disease. Over 26 percent of all Americans over the age of 18 meet the criteria for having a mental illness. Serious mental disorders affect an estimated 6 percent of the adult population, or approximately 1 in 17 people. A little more than half receive treatment.[16] A WHO report estimates the global cost of mental illness at nearly $2.5 trillion (two-thirds in indirect costs) in 2010, with a projected increase to over $6 trillion by 2030.		Evidence from the World Health Organization suggests that nearly half of the world's population are affected by mental illness with an impact on their self-esteem, relationships and ability to function in everyday life.[17] An individual's emotional health can also impact physical health and poor mental health can lead to problems such as substance abuse.[18]		Maintaining good mental health is crucial to living a long and healthy life. Good mental health can enhance one's life, while poor mental health can prevent someone from living an enriching life. According to Richards, Campania, & Muse-Burke, "There is growing evidence that is showing emotional abilities are associated with prosocial behaviors such as stress management and physical health."[18] Their research also concluded that people who lack emotional expression are inclined to anti-social behaviors (e.g., drug and alcohol abuse, physical fights, vandalism), which are a direct reflection of their mental health and suppress emotions.[18]		Mental health can be seen as an unstable continuum, where an individual's mental health may have many different possible values.[19] Mental wellness is generally viewed as a positive attribute, even if the person does not have any diagnosed mental health condition. This definition of mental health highlights emotional well-being, the capacity to live a full and creative life, and the flexibility to deal with life's inevitable challenges. Some discussions are formulated in terms of contentment or happiness.[20] Many therapeutic systems and self-help books offer methods and philosophies espousing strategies and techniques vaunted as effective for further improving the mental wellness. Positive psychology is increasingly prominent in mental health.		A holistic model of mental health generally includes concepts based upon anthropological, educational, psychological, religious and sociological perspectives, as well as theoretical perspectives from personality, social, clinical, health and developmental psychology.[21][22]		An example of a wellness model includes one developed by Myers, Sweeney and Witmer. It includes five life tasks—essence or spirituality, work and leisure, friendship, love and self-direction—and twelve sub tasks—sense of worth, sense of control, realistic beliefs, emotional awareness and coping, problem solving and creativity, sense of humor, nutrition, exercise, self care, stress management, gender identity, and cultural identity—which are identified as characteristics of healthy functioning and a major component of wellness. The components provide a means of responding to the circumstances of life in a manner that promotes healthy functioning.		The tripartite model of mental well-being[19][23] views mental well-being as encompassing three components of emotional well-being, social well-being, and psychological well-being. Emotional well-being is defined as having high levels of positive emotions, whereas social and psychological well-being are defined as the presence of psychological and social skills and abilities that contribute to optimal functioning in daily life. The model has received empirical support across cultures.[23][24][25] The Mental Health Continuum-Short Form (MHC-SF) is the most widely used scale to measure the tripartite model of mental well-being.[26][27][28]		Mental health and stability is a very important factor in a person’s everyday life. Social skills, behavioural skills, and someone’s way of thinking are just some of the things that the human brain develops at an early age. Learning how to interact with others and how to focus on certain subjects are essential lessons to learn from the time we can talk all the way to when we are so old that we can barely walk. However, there are some people out there who have difficulty with these kind of skills and behaving like an average person. This is a most likely the cause of having a mental illness. A mental illness is a wide range of conditions that affect a person’s mood, thinking, and behavior. About 26% of people in the United States, ages 18 and older, have been diagnosed with some kind of mental disorder. However, not much is said about children with mental illnesses even though there are many that will develop one, even as early as age three.		The most common mental illnesses in children include, but are not limited to, ADHD, autism and anxiety disorder, as well as depression in older children and teens. Having a mental illness at a younger age is much different from having one in your thirties. Children's brains are still developing and will continue to develop until around the age of twenty-five.[29] When a mental illness is thrown into the mix, it becomes significantly harder for a child to acquire the necessary skills and habits that people use throughout the day. For example, behavioral skills don’t develop as fast as motor or sensory skills do.[29] So when a child has an anxiety disorder, they begin to lack proper social interaction and associate many ordinary things with intense fear.[30] This can be scary for the child because they don’t necessarily understand why they act and think the way that they do. Many researchers say that parents should keep an eye on their child if they have any reason to believe that something is slightly off.[29] If the children are evaluated earlier, they become more acquainted to their disorder and treating it becomes part of their daily routine.[29] This is opposed to adults who might not recover as quickly because it is more difficult for them to adapt.		Mental illness affects not only the person themselves, but the people around them. Friends and family also play an important role in the child’s mental health stability and treatment. If the child is young, parents are the ones who evaluate their child and decide whether or not they need some form of help.[31] Friends are a support system for the child and family as a whole. Living with a mental disorder is never easy, so it’s always important to have people around to make the days a little easier. However, there are negative factors that come with the social aspect of mental illness as well. Parents are sometimes held responsible for their child’s own illness.[31] People also say that the parents raised their children in a certain way or they acquired their behavior from them. Family and friends are sometimes so ashamed of the idea of being close to someone with a disorder that the child feels isolated and thinks that they have to hide their illness from others.[31] When in reality, hiding it from people prevents the child from getting the right amount of social interaction and treatment in order to thrive in today’s society.		Stigma is also a well-known factor in mental illness. Stigma is defined as “a mark of disgrace associated with a particular circumstance, quality, or person.” Stigma is used especially when it comes to the mentally disabled. People have this assumption that everyone with a mental problem, no matter how mild or severe, is automatically considered destructive or a criminal person. Thanks to the media, this idea has been planted in our brains from a young age.[32] Watching movies about teens with depression or children with Autism makes us think that all of the people that have a mental illness are like the ones on TV. In reality, the media displays an exaggerated version of most illnesses. Unfortunately, not many people know that, so they continue to belittle those with disorders. In a recent study, a majority of young people associate mental illness with extreme sadness or violence.[33] Now that children are becoming more and more open to technology and the media itself, future generations will then continue to pair mental illness with negative thoughts. The media should be explaining that many people with disorders like ADHD and anxiety, with the right treatment, can live ordinary lives and should not be punished for something they cannot help.		Sueki, (2013) carried out a study titled “The effect of suicide–related internet use on users’ mental health: A longitudinal Study”. This study investigated the effects of suicide-related internet use on user’s suicidal thoughts, predisposition to depression and anxiety and loneliness. The study consisted of 850 internet users; the data was obtained by carrying out a questionnaire amongst the participants. This study found that browsing websites related to suicide, and methods used to commit suicide, had a negative effect on suicidal thoughts and increased depression and anxiety tendencies. The study concluded that as suicide-related internet use adversely affected the mental health of certain age groups it may be prudent to reduce or control their exposure to these websites. These findings certainly suggest that the internet can indeed have a profoundly negative impact on our mental health.[34]		Psychiatrist Thomas Szasz compared that 50 years ago children were either categorized as good or bad, and today "all children are good, but some are mentally healthy and others are mentally ill". The social control and forced identity creation is the cause of many mental health problems among today's children.[35] A behaviour or misbehaviour can not be an illness but exercise of their free will and today's immediacy in drug administration for every problem along with the legal over-guarding and disregard of a child's status as a dependent shakes their personal self and invades their internal growth.		Mental health is conventionally defined as a hybrid of absence of a mental disorder and presence of well-being. Focus is increasing on preventing mental disorders. Prevention is beginning to appear in mental health strategies, including the 2004 WHO report "Prevention of Mental Disorders", the 2008 EU "Pact for Mental Health" and the 2011 US National Prevention Strategy.[36][page needed] Prevention of a disorder at a young age may significantly decrease the chances that a child will suffer from a disorder later in life, and shall be the most efficient and effective measure from a public health perspective.[37] Prevention may require the regular consultation of a physician for at least twice a year to detect any signs that reveal any mental health concerns.[38][unreliable medical source?]		Mental health is a socially constructed and socially defined concept; that is, different societies, groups, cultures, institutions and professions have very different ways of conceptualizing its nature and causes, determining what is mentally healthy, and deciding what interventions, if any, are appropriate.[39] Thus, different professionals will have different cultural, class, political and religious backgrounds, which will impact the methodology applied during treatment.		Research has shown that there is stigma attached to mental illness.[40] In the United Kingdom, the Royal College of Psychiatrists organized the campaign Changing Minds (1998–2003) to help reduce stigma.[41] Due to this stigma, responses to a positive diagnosis may be a display of denialism.[42]		Many mental health professionals are beginning to, or already understand, the importance of competency in religious diversity and spirituality. The American Psychological Association explicitly states that religion must be respected. Education in spiritual and religious matters is also required by the American Psychiatric Association.[43]		Unemployment has been shown to have a negative impact on an individual's emotional well-being, self-esteem and more broadly their mental health. Increasing unemployment has been show to have a significant impact on mental health, predominantly depressive disorders.[citation needed] This is an important consideration when reviewing the triggers for mental health disorders in any population survey.[44] In order to improve your emotional mental health, the root of the issue has to be resolved. "Prevention emphasizes the avoidance of risk factors; promotion aims to enhance an individual's ability to achieve a positive sense of self-esteem, mastery, well-being, and social inclusion."[45] It is very important to improve your emotional mental health by surrounding yourself with positive relationships. We as humans, feed off companionships and interaction with other people. Another way to improve your emotional mental health is participating in activities that can allow you to relax and take time for yourself. Yoga is a great example of an activity that calms your entire body and nerves. According to a study on well-being by Richards, Campania and Muse-Burke, "mindfulness is considered to be a purposeful state, it may be that those who practice it believe in its importance and value being mindful, so that valuing of self-care activities may influence the intentional component of mindfulness."[18]		Mental health care navigation helps to guide patients and families through the fragmented, often confusing mental health industries. Care navigators work closely with patients and families through discussion and collaboration to provide information on best therapies as well as referrals to practitioners and facilities specializing in particular forms of emotional improvement. The difference between therapy and care navigation is that the care navigation process provides information and directs patients to therapy rather than providing therapy. Still, care navigators may offer diagnosis and treatment planning. Though many care navigators are also trained therapists and doctors. Care navigation is the link between the patient and the below therapies. A clear recognition that mental health requires medical intervention was demonstrated in a study by Kessler et al. of the prevalence and treatment of mental disorders from 1990 to 2003 in the United States. Despite the prevalence of mental health disorders remaining unchanged during this period, the number of patients seeking treatment for mental disorders increased threefold.[46]		Emotional mental disorders are a leading cause of disabilities worldwide. Investigating the degree and severity of untreated emotional mental disorders throughout the world is a top priority of the World Mental Health (WMH) survey initiative,[47] which was created in 1998 by the World Health Organization (WHO).[48] "Neuropsychiatric disorders are the leading causes of disability worldwide, accounting for 37% of all healthy life years lost through disease.These disorders are most destructive to low and middle-income countries due to their inability to provide their citizens with proper aid. Despite modern treatment and rehabilitation for emotional mental health disorders, "even economically advantaged societies have competing priorities and budgetary constraints".		The World Mental Health survey initiative has suggested a plan for countries to redesign their mental health care systems to best allocate resources. "A first step is documentation of services being used and the extent and nature of unmet needs for treatment. A second step could be to do a cross-national comparison of service use and unmet needs in countries with different mental health care systems. Such comparisons can help to uncover optimum financing, national policies, and delivery systems for mental health care."		Knowledge of how to provide effective emotional mental health care has become imperative worldwide. Unfortunately, most countries have insufficient data to guide decisions, absent or competing visions for resources, and near constant pressures to cut insurance and entitlements. WMH surveys were done in Africa (Nigeria, South Africa), the Americas (Colombia, Mexico, United States), Asia and the Pacific (Japan, New Zealand, Beijing and Shanghai in the People's Republic of China), Europe (Belgium, France, Germany, Italy, Netherlands, Spain, Ukraine), and the middle east (Israel, Lebanon). Countries were classified with World Bank criteria as low-income (Nigeria), lower middle-income (China, Colombia, South Africa, Ukraine), higher middle-income (Lebanon, Mexico), and high-income.		The coordinated surveys on emotional mental health disorders, their severity, and treatments were implemented in the aforementioned countries. These surveys assessed the frequency, types, and adequacy of mental health service use in 17 countries in which WMH surveys are complete. The WMH also examined unmet needs for treatment in strata defined by the seriousness of mental disorders. Their research showed that "the number of respondents using any 12-month mental health service was generally lower in developing than in developed countries, and the proportion receiving services tended to correspond to countries' percentages of gross domestic product spent on health care". "High levels of unmet need worldwide are not surprising, since WHO Project ATLAS' findings of much lower mental health expenditures than was suggested by the magnitude of burdens from mental illnesses. Generally, unmet needs in low-income and middle-income countries might be attributable to these nations spending reduced amounts (usually <1%) of already diminished health budgets on mental health care, and they rely heavily on out-of-pocket spending by citizens who are ill equipped for it".		Activity therapies, also called recreation therapy and occupational therapy, promote healing through active engagement. Making crafts can be a part of occupational therapy. Walks can be a part of recreation therapy.		Biofeedback is a process of gaining control of physical processes and brainwaves. It can be used to decrease anxiety, increase well-being, increase relaxation, and other methods of mind-over-body control.[citation needed]		Expressive therapies are a form of psychotherapy that involves the arts or art-making. These therapies include music therapy, art therapy, dance therapy, drama therapy, and poetry therapy.		Group therapy involves any type of therapy that takes place in a setting involving multiple people. It can include psychodynamic groups, activity groups for expressive therapy, support groups (including the Twelve-step program), problem-solving and psychoeducation groups.		Psychotherapy is the general term for scientific based treatment of mental health issues based on modern medicine. It includes a number of schools, such as gestalt therapy, psychoanalysis, cognitive behavioral therapy and dialectical behavioral therapy.		The practice of mindfulness meditation has several mental health benefits, such as bringing about reductions in depression, anxiety and stress.[49][50][51][52] Mindfulness meditation may also be effective in treating substance use disorders.[53][54] Further, mindfulness meditation appears to bring about favorable structural changes in the brain.[55][56][57]		Spiritual counselors meet with people in need to offer comfort and support and to help them gain a better understanding of their issues and develop a problem-solving relation with spirituality. These types of counselors deliver care based on spiritual, psychological and theological principles.[58][unreliable source?]		Social work in mental health, also called psychiatric social work, is a process where an individual in a setting is helped to attain freedom from overlapping internal and external problems (social and economic situations, family and other relationships, the physical and organizational environment, psychiatric symptoms, etc.). It aims for harmony, quality of life, self-actualization and personal adaptation across all systems. Psychiatric social workers are mental health professionals that can assist patients and their family members in coping with both mental health issues and various economic or social problems caused by mental illness or psychiatric dysfunctions and to attain improved mental health and well-being. They are vital members of the treatment teams in Departments of Psychiatry and Behavioral Sciences in hospitals. They are employed in both outpatient and inpatient settings of a hospital, nursing homes, state and local governments, substance abuse clinics, correctional facilities, health care services...etc.[59]		In psychiatric social work there are three distinct groups. One made up of the social workers in psychiatric organizations and hospitals. The second group consists members interested with mental hygiene education and holding designations that involve functioning in various mental health services and the third group consist of individuals involved directly with treatment and recovery process.[60]		In the United States, social workers provide most of the mental health services. According to government sources, 60 percent of mental health professionals are clinically trained social workers, compared to 10 percent of psychiatrists, 23 percent of psychologists, and 5 percent of psychiatric nurses.[61]		Mental health social workers in Japan have professional knowledge of health and welfare and skills essential for person's well-being. Their social work training enables them as a professional to carry out Consultation assistance for mental disabilities and their social reintegration; Consultation regarding the rehabilitation of the victims; Advice and guidance for post-discharge residence and re-employment after hospitalized care, for major life events in regular life, money and self-management and in other relevant matters in order to equip them to adapt in daily life. Social workers provide individual home visits for mentally ill and do welfare services available, with specialized training a range of procedural services are coordinated for home, workplace and school. In an administrative relationship, Psychiatric social workers provides consultation, leadership, conflict management and work direction. Psychiatric social workers who provides assessment and psychosocial interventions function as a clinician, counselor and municipal staff of the health centers.[62]		Social workers play many roles in mental health settings, including those of case manager, advocate, administrator, and therapist. The major functions of a psychiatric social worker are promotion and prevention, treatment, and rehabilitation. Social workers may also practice:		Psychiatric social workers conduct psychosocial assessments of the patients and work to enhance patient and family communications with the medical team members and ensure the inter-professional cordiality in the team to secure patients with the best possible care and to be active partners in their care planning. Depending upon the requirement, social workers are often involved in illness education, counseling and psychotherapy. In all areas, they are pivotal to the aftercare process to facilitate a careful transition back to family and community. [63]		During the 1840s, Dorothea Lynde Dix, a retired Boston teacher who is considered the founder of the mental health movement, began a crusade that would change the way people with mental disorders were viewed and treated. Dix was not a social worker; the profession was not established until after her death in 1887. However, her life and work were embraced by early psychiatric social workers, and she is considered one of the pioneers of psychiatric social work along with Elizabeth Horton, who in 1907 was the first psychiatric social worker in the New York hospital system, and others.[64] The early twentieth century was a time of progressive change in attitudes towards mental illness. Community Mental Health Centers Act was passed in 1963. This policy encouraged the deinstitutionalisation of people with mental illness. Later mental health consumer movement came by 1980s. A consumer was defined as a person who has received or is currently receiving services for a psychiatric condition. People with mental disorders and their families became advocates for better care. Building public understanding and awareness through consumer advocacy helped bring mental illness and its treatment into mainstream medicine and social services.[65] In the 2000s focus was on Managed care movement which aimed at a health care delivery system to eliminate unnecessary and inappropriate care in order to reduce costs & Recovery movement in which by principle acknowledges that many people with serious mental illness spontaneously recover and others recover and improve with proper treatment.[66]		Role of social workers made an impact with 2003 invasion of Iraq and War in Afghanistan (2001–14) social workers worked out of the NATO hospital in Afghanistan and Iraq bases. They made visits to provide counseling services at forward operating bases. Twenty-two percent of the clients were diagnosed with post-traumatic stress disorder, 17 percent with depression, and 7 percent with alcohol abuse.[67] In 2009, a high level of suicides was reached among active-duty soldiers: 160 confirmed or suspected Army suicides. In 2008, the Marine Corps had a record 52 suicides.[68] The stress of long and repeated deployments to war zones, the dangerous and confusing nature of both wars, wavering public support for the wars, and reduced troop morale have all contributed to the escalating mental health issues.[69] Military and civilian social workers are primary service providers in the veterans’ health care system.		Mental health services, is a loose network of services ranging from highly structured inpatient psychiatric units to informal support groups, where psychiatric social workers indulges in the diverse approaches in multiple settings along with other paraprofessional workers.		A role for psychiatric social workers was established early in Canada’s history of service delivery in the field of population health. Native North Americans understood mental trouble as an indication of an individual who had lost their equilibrium with the sense of place and belonging in general, and with the rest of the group in particular. In native healing beliefs, health and mental health were inseparable, so similar combinations of natural and spiritual remedies were often employed to try to relieve both mental and physical illness. These communities and families greatly valued holistic approaches for preventative health care. Indigenous peoples in Canada have faced cultural oppression and social marginalization through the actions of European colonizers and their institutions since the earliest periods of contact. Culture contact brought with it many forms of depredation. Economic, political, and religious institutions of the European settlers all contributed to the displacement and oppression of indigenous people.[70][page needed] The officially recorded treatment practices started in 1714, when Quebec opened wards for the mentally ill. Asylums for the insane were opened in 1835 in Saint John, New Brunswick, and in 1841 in Toronto, when care for the mentally ill became institutionally based. Canada became a self-governing dominion in 1867, retaining its ties to the British crown. During this period age of industrial capitalism began, which lead to a social and economic dislocation in many forms. By 1887 asylums were converted to hospitals and nurses and attendants were employed for the care of the mentally ill. In 1918 Clarence Hincks & Clifford Beers founded the Canadian National Committee for Mental Hygiene, which later became the Canadian Mental Health Association. In 1930s Dr. Clarence Hincks promoted prevention and of treating sufferers of mental illness before they were incapacitated/early detection. World War II profoundly affected attitudes towards mental health. The medical examinations of recruits revealed that thousands of apparently healthy adults suffered mental difficulties. This knowledge changed public attitudes towards mental health, and stimulated research into preventive measures and methods of treatment.[71] In 1951 Mental Health Week was introduced across Canada. For the first half of the twentieth century, with a period of deinstitutionalisation beginning in the late 1960s psychiatric social work succeeded to the current emphasis on community-based care, Psychiatric Social Work focused beyond the medical model’s aspects on individual diagnosis to identify and address social inequities and structural issues. In the 1980s Mental Health Act was amended to give consumers the right to choose treatment alternatives. Later the focus shifted to workforce mental health issues and environment.[72]		The earliest citing of Mental disorders in India are from Vedic Era (2000 BC – AD 600).[73] Charaka Samhita, an ayurvedic textbook believed to be from 400–200 BC describes various factors of mental stability. It also has instructions regarding how to set up a care delivery system.[74] In the same era In south India Siddha was a medical system, the great sage Agastya, one of the 18 siddhas contributing to a system of medicine has included the Agastiyar Kirigai Nool, a compendium of psychiatric disorders and their recommended treatments.[75] In Atharva Veda too there are descriptions and resolutions about mental health afflictions. In the Mughal period Unani system of medicine was introduced by an Indian physician Unhammad in 1222.[76] Then existed form of psychotherapy was known then as ilaj-i-nafsani in Unani medicine.		The 18th century was a very unstable period in Indian history, which contributed to psychological and social chaos in the Indian subcontinent. In 1745 of lunatic asylums were developed in Bombay (Mumbai) followed by Calcutta (Kolkata) in 1784, and Madras (Chennai) in 1794. The need to establish hospitals became more acute, first to treat and manage Englishmen and Indian ‘sepoys’ (military men) employed by the British East India Company.[77] The First Lunacy Act (also called Act No. 36) that came into effect in 1858 was later modified by a committee appointed in Bengal in 1888. Later, the Indian Lunacy Act, 1912 was brought under this legislation. A rehabilitation programme was initiated between 1870s and 1890s for persons with mental illness at the Mysore Lunatic Asylum, and then an occupational therapy department was established during this period in almost each of the lunatic asylums. The programme in the asylum was called ‘work therapy’. In this programme, persons with mental illness were involved in the field of agriculture for all activities. This programme is considered as the seed of origin of psychosocial rehabilitation in India.		Berkeley-Hill, superintendent of the European Hospital (now known as the Central Institute of Psychiatry (CIP), established in 1918), was deeply concerned about the improvement of mental hospitals in those days. The sustained efforts of Berkeley-Hill helped to raise the standard of treatment and care and he also persuaded the government to change the term ‘asylum’ to ‘hospital’ in 1920.[78] Techniques similar to the current token-economy were first started in 1920 and called by the name ‘habit formation chart’ at the CIP, Ranchi. In 1937, the first post of psychiatric social worker was created in the child guidance clinic run by the Dhorabji Tata School of Social Work (established in 1936), It is considered as the first documented evidence of social work practice in Indian mental health field.		After Independence in 1947, general hospital psychiatry units (GHPUs) where established to improve conditions in existing hospitals, while at the same time encouraging outpatient care through these units. In Amritsar a Dr. Vidyasagar, instituted active involvement of families in the care of persons with mental illness. This was advanced practice ahead of its times regarding treatment and care. This methodology had a greater impact on social work practice in the mental health field especially in reducing the stigmatisation. In 1948 Gauri Rani Banerjee, trained in the United States, started a master’s course in medical and psychiatric social work at the Dhorabji Tata School of Social Work (Now TISS). Later the first trained psychiatric social worker was appointed in 1949 at the adult psychiatry unit of Yervada mental hospital, Pune.		In various parts of the country, in mental health service settings, social workers were employed—in 1956 at a mental hospital in Amritsar, in 1958 at a child guidance clinic of the college of nursing, and in Delhi in 1960 at the All India Institute of Medical Sciences and in 1962 at the Ram Manohar Lohia Hospital. In 1960, the Madras Mental Hospital (Now Institute of Mental Health), employed social workers to bridge the gap between doctors and patients. In 1961 the social work post was created at the NIMHANS. In these settings they took care of the psychosocial aspect of treatment. This had long-term greater impact of social work practice in mental health.[79]		In 1966 by the recommendation Mental Health Advisory Committee, Ministry of Health, Government of India, NIMHANS commenced Department of Psychiatric Social Work in and started a two-year Postgraduate Diploma in Psychiatric Social Work was introduced in 1968. In 1978, the nomenclature of the course was changed to MPhil in Psychiatric Social Work. Subsequently, a PhD Programme was introduced. By the recommendations Mudaliar committee in 1962, Diploma in Psychiatric Social Work was started in 1970 at the European Mental Hospital at Ranchi (now CIP), upgraded the program and added other higher training courses subsequently.		A new initiative to integrate mental health with general health services started in 1975 in India. The Ministry of Health, Government of India formulated the National Mental Health Programme (NMHP) and launched it in 1982. The same was reviewed in 1995 and based on that, the District Mental Health Program (DMHP) launched in 1996 and sought to integrate mental health care with public health care.[80] This model has been implemented in all the states and currently there are 125 DMHP sites in India.		National Human Rights Commission (NHRC) in 1998 and 2008 carried out systematic, intensive and critical examinations of mental hospitals in India. This resulted in recognition of the human rights of the persons with mental illness by the NHRC. From the NHRC's report as part of the NMHP, funds were provided for upgrading the facilities of mental hospitals. This is studied to result in positive changes over the past 10 years than in the preceding five decades by the 2008 report of the NHRC and NIMHANS.[81] In 2016 Mental Health Care Bill was passed which ensures and legally entitles access to treatments with coverage from insurance, safeguarding dignity of the afflicted person, improving legal and healthcare access and allows for free medications.[82][83][84] In December 2016, Disabilities Act 1995 was repealed with Rights of Persons with Disabilities Act (RPWD), 2016 from the 2014 Bill which ensures benefits to a wider population with disabilities. The Bill before becoming an Act was pushed for amendments by stakeholders mainly against alarming clauses in the "Equality and Non discrimination" section that diminishes the power of the act and allows establishments to overlook or discriminate against persons with disabilities and against the general lack of directives that requires to ensure the proper implementation of the Act.[85][86]		Lack of any universally accepted single licensing authority compared to foreign countries puts Social Workers at general in risk. But general bodies/councils accepts automatically a university qualified Social Worker as a professional licensed to practice or as a qualified clinician. Lack of a centralized council in tie-up with Schools of Social Work also makes a decline in promotion for the scope of social workers as mental health professionals. Though in this midst the service of Social Workers has given a facelift of the mental health sector in the country with other allied professionals.[87]		Evidence suggests that 450 million people worldwide are impacted by mental health, major depression ranks fourth among the top 10 leading causes of disease worldwide. Within 20 years, mental illness is predicted to become the leading cause of disease worldwide. Women are more likely to have a mental illness than men. One million people commit suicide every year and 10 to 20 million attempt it.[88]		A survey conducted by Australian Bureau of Statistics in 2008 regarding adults with manageable to severe neurosis reveals almost half of the population had a mental disorder at some point of their life and one in five people had sustaining disorder preceding 12 months. In Neurotic disorders, 14% of the population experienced anxiety disorders, comorbidity disorders were the next common mental disorder with vulnerability to substance abuse and relapses. There were distinct gender differences in disposition to mental health illness. Women were found to have high rate of mental health disorders and Men had higher propensity of risk for substance abuse. The SMHWB survey showed low socioeconomic status and high dysfunctional pattern in the family was proportional to greater risk for mental health disorders. A 2010 survey regarding adults with psychosis revealed 5 persons per 1000 in the population seeks professional mental health services for psychotic disorders and the most common psychotic disorder was schizophrenia.[89][90]		According to statistics released by the Centre of Addiction and Mental Health one in five people in Ontario experience a mental health or addiction problem. Young people ages 15 to 25 are particularly vulnerable. Major depression is found to affect 8% and anxiety disorder 12% of the population. Women are 1.5 times more likely to suffer from mood and anxiety disorders. WHO points out that there are distinct gender differences in patterns of mental health and illness. The lack of power and control over their socioeconomic status, gender based violence; low social position and responsibility for the care of others render women vulnerable to mental health risks. Since more women than men seek help regarding a mental health problem, this has led to not only gender stereotyping but also reinforcing social stigma. WHO has found that this stereotyping has led doctors to diagnose depression more often in women than in men even when they display identical symptoms. Often communication between health care providers and women is authoritarian leading to either the under-treatment or over-treatment of these women.[3]		Firstly, Women's College Hospital is specifically dedicated to women's health in Canada. This hospital is located at the heart of downtown, Toronto where there are several locations available for specific medical conditions. WCH is a great organization that helps educate women on mental illness due to its specialization with women and mental health. Women's College Hospital helps women who have symptoms of mental illnesses such as depression, anxiety, menstruation, pregnancy, childbirth, and menopause. They also focus on psychological issues, abuse, neglect and mental health issues from various medications.[91]		The countless aspect about this organization is that WCH is open to women of all ages, including pregnant women that experience poor mental health. WCH not only provides care for good mental health, but they also have a program called the "Women's Mental Health Program" where doctors and nurses help treat and educate women regarding mental health collaboratively, individually, and online by answering questions from the public.[91]		The second organization is the Centre for Addiction and Mental Health (CAMH). CAMH is one of Canada's largest and most well-known health and addiction facilities, and it has received international recognitions from the Pan American Health Organization and World Health Organization Collaborating Centre. They practice in doing research in areas of addiction and mental health in both men and women. In order to help both men and women, CAMH provides "clinical care, research, education, policy development and health promotion to help transform the lives of people affected by mental health and addiction issues."[92] CAMH is different from Women's College Hospital due to its widely known rehab centre for women who have minor addiction issues, to severe ones. This organization provides care for mental health issues by assessments, interventions, residential programs, treatments, and doctor and family support.[92]		According to the World Health Organization in 2004, depression is the leading cause of disability in the United States for individuals ages 15 to 44.[93] Absence from work in the U.S. due to depression is estimated to be in excess of $31 billion per year. Depression frequently co-occurs with a variety of medical illnesses such as heart disease, cancer, and chronic pain and is associated with poorer health status and prognosis.[94] Each year, roughly 30,000 Americans take their lives, while hundreds of thousands make suicide attempts (Centers for Disease Control and Prevention).[95] In 2004, suicide was the 11th leading cause of death in the United States (Centers for Disease Control and Prevention), third among individuals ages 15–24. Despite the increasingly availability of effectual depression treatment, the level of unmet need for treatment remains high.[citation needed] By way of comparison, a study conducted in Australia during 2006 to 2007 reported that one-third (34.9%) of patients diagnosed with a mental health disorder had presented to medical health services for treatment.[96]		There are many factors that influence mental health including:		Emotional mental illnesses should be a particular concern in the United States since the U.S. has the highest annual prevalence rates (26 percent) for mental illnesses among a comparison of 14 developing and developed countries.[97] While approximately 80 percent of all people in the United States with a mental disorder eventually receive some form of treatment, on the average persons do not access care until nearly a decade following the development of their illness, and less than one-third of people who seek help receive minimally adequate care.[98]		The mental health policies in the United States have experienced four major reforms: the American asylum movement led by Dorothea Dix in 1843; the "mental hygiene" movement inspired by Clifford Beers in 1908; the deinstitutionalization started by Action for Mental Health in 1961; and the community support movement called for by The CMCH Act Amendments of 1975.[99]		In 1843, Dorothea Dix submitted a Memorial to the Legislature of Massachusetts, describing the abusive treatment and horrible conditions received by the mentally ill patients in jails, cages, and almshouses. She revealed in her Memorial: "I proceed, gentlemen, briefly to call your attention to the present state of insane persons confined within this Commonwealth, in cages, closets, cellars, stalls, pens! Chained, naked, beaten with rods, and lashed into obedience…."[100] Many asylums were built in that period, with high fences or walls separating the patients from other community members and strict rules regarding the entrance and exit. In those asylums, traditional treatments were well implemented: drugs were not used as a cure for a disease, but a way to reset equilibrium in a person's body, along with other essential elements such as healthy diets, fresh air, middle class culture, and the visits by their neighboring residents.[citation needed] In 1866, a recommendation came to the New York State Legislature to establish a separate asylum for chronic mentally ill patients. Some hospitals placed the chronic patients into separate wings or wards, or different buildings.[101]		In A Mind That Found Itself (1908) Clifford Whittingham Beers described the humiliating treatment he received and the deplorable conditions in the mental hospital.[102] One year later, the National Committee for Mental Hygiene (NCMH) was founded by a small group of reform-minded scholars and scientists – including Beer himself – which marked the beginning of the "mental hygiene" movement. The movement emphasized the importance of childhood prevention. World War I catalyzed this idea with an additional emphasis on the impact of maladjustment, which convinced the hygienists that prevention was the only practical approach to handle mental health issues.[103] However, prevention was not successful, especially for chronic illness; the condemnable conditions in the hospitals were even more prevalent, especially under the pressure of the increasing number of chronically ill and the influence of the Depression.[99]		In 1961, the Joint Commission on Mental Health published a report called Action for Mental Health, whose goal was for community clinic care to take on the burden of prevention and early intervention of the mental illness, therefore to leave space in the hospitals for severe and chronic patients. The court started to rule in favor of the patients' will on whether they should be forced to treatment. By 1977, 650 community mental health centers were built to cover 43 percent of the population and serve 1.9 million individuals a year, and the lengths of treatment decreased from 6 months to only 23 days.[104] However, issues still existed. Due to inflation, especially in the 1970s, the community nursing homes received less money to support the care and treatment provided. Fewer than half of the planned centers were created, and new methods did not fully replace the old approaches to carry out its full capacity of treating power.[104] Besides, the community helping system was not fully established to support the patients' housing, vocational opportunities, income supports, and other benefits.[99] Many patients returned to welfare and criminal justice institutions, and more became homeless. The movement of deinstitutionalization was facing great challenges.[105]		After realizing that simply changing the location of mental health care from the state hospitals to nursing houses was insufficient to implement the idea of deinstitutionalization, the National Institute of Mental Health in 1975 created the Community Support Program (CSP) to provide funds for communities to set up a comprehensive mental health service and supports to help the mentally ill patients integrate successfully in the society. The program stressed the importance of other supports in addition to medical care, including housing, living expenses, employment, transportation, and education; and set up new national priority for people with serious mental disorders. In addition, the Congress enacted the Mental Health Systems Act of 1980 to prioritize the service to the mentally ill and emphasize the expansion of services beyond just clinical care alone.[106] Later in the 1980s, under the influence from the Congress and the Supreme Court, many programs started to help the patients regain their benefits. A new Medicaid service was also established to serve people who were suffering from a "chronic mental illness." People who were temporally hospitalized were also provided aid and care and a pre-release program was created to enable people to apply for reinstatement prior to discharge.[104] Not until 1990, around 35 years after the start of the deinstitutionalization, did the first state hospital begin to close. The number of hospitals dropped from around 300 by over 40 in the 1990s, and finally a Report on Mental Health showed the efficacy of mental health treatment, giving a range of treatments available for patients to choose.[106]		However, several critics maintain that deinstitutionalization has, from a mental health point of view, been a thoroughgoing failure. The seriously mentally ill are either homeless, or in prison; in either case (especially the latter), they are getting little or no mental health care. This failure is attributed to a number of reasons over which there is some degree of contention, although there is general agreement that community support programs have been ineffective at best, due to a lack of funding.[105]		The 2011 National Prevention Strategy included mental and emotional well-being, with recommendations including better parenting and early intervention programs, which increase the likelihood of prevention programs being included in future US mental health policies.[107][page needed] The NIMH is researching only suicide and HIV/AIDS prevention, but the National Prevention Strategy could lead to it focusing more broadly on longitudinal prevention studies.[108][not in citation given]		In 2013, United States Representative Tim Murphy introduced the Helping Families in Mental Health Crisis Act, HR2646. The bipartisan bill went through substantial revision and was reintroduced in 2015 by Murphy and Congresswoman Eddie Bernice Johnson. In November 2015, it passed the Health Subcommittee by an 18–12 vote.[citation needed]		
A test or examination (informally, exam or evaluation) is an assessment intended to measure a test-taker's knowledge, skill, aptitude, physical fitness, or classification in many other topics (e.g., beliefs).[1] A test may be administered verbally, on paper, on a computer, or in a confined area that requires a test taker to physically perform a set of skills. Tests vary in style, rigor and requirements. For example, in a closed book test, a test taker is often required to rely upon memory to respond to specific items whereas in an open book test, a test taker may use one or more supplementary tools such as a reference book or calculator when responding to an item. A test may be administered formally or informally. An example of an informal test would be a reading test administered by a parent to a child. An example of a formal test would be a final examination administered by a teacher in a classroom or an I.Q. test administered by a psychologist in a clinic. Formal testing often results in a grade or a test score.[2] A test score may be interpreted with regards to a norm or criterion, or occasionally both. The norm may be established independently, or by statistical analysis of a large number of participants. An exam is meant to test a child's knowledge or willingness to give time to manipulate that subject.		A standardized test is any test that is administered and scored in a consistent manner to ensure legal defensibility.[3] Standardized tests are often used in education, professional certification, psychology (e.g., MMPI), the military, and many other fields.		A non-standardized test is usually flexible in scope and format, variable in difficulty and significance. Since these tests are usually developed by individual instructors, the format and difficulty of these tests may not be widely adopted or used by other instructors or institutions. A non-standardized test may be used to determine the proficiency level of students, to motivate students to study, and to provide feedback to students. In some instances, a teacher may develop non-standardized tests that resemble standardized tests in scope, format, and difficulty for the purpose of preparing their students for an upcoming standardized test.[4] Finally, the frequency and setting by which a non-standardized tests are administered are highly variable and are usually constrained by the duration of the class period. A class instructor may for example, administer a test on a weekly basis or just twice a semester. Depending on the policy of the instructor or institution, the duration of each test itself may last for only five minutes to an entire class period.		In contrasts to non-standardized tests, standardized tests are widely used, fixed in terms of scope, difficulty and format, and are usually significant in consequences. Standardized tests are usually held on fixed dates as determined by the test developer, educational institution, or governing body, which may or may not be administered by the instructor, held within the classroom, or constrained by the classroom period. Although there is little variability between different copies of the same type of standardized test (e.g., SAT or GRE), there is variability between different types of standardized tests.		Any test with important consequences for the individual test taker is referred to as a high-stakes test.		A test may be developed and administered by an instructor, a clinician, a governing body, or a test provider. In some instances, the developer of the test may not be directly responsible for its administration. For example, Educational Testing Service (ETS), a nonprofit educational testing and assessment organization, develops standardized tests such as the SAT but may not directly be involved in the administration or proctoring of these tests. As with the development and administration of educational tests, the format and level of difficulty of the tests themselves are highly variable and there is no general consensus or invariable standard for test formats and difficulty. Often, the format and difficulty of the test is dependent upon the educational philosophy of the instructor, subject matter, class size, policy of the educational institution, and requirements of accreditation or governing bodies. In general, tests developed and administered by individual instructors are non-standardized whereas tests developed by testing organizations are standardized.						Ancient China was the first country in the world that implemented a nationwide standardized test, which was called the imperial examination. The main purpose of this examination was to select able candidates for specific governmental positions.[5] The imperial examination was established by the Sui dynasty in 605 AD and was later abolished by the Qing dynasty 1300 years later in 1905. England had adopted this examination system in 1806 to select specific candidates for positions in Her Majesty's Civil Service, modeled on the Chinese imperial examination.[6] This examination system was later applied to education and it started to influence other parts of the world as it became a prominent standard (e.g. regulations to prevent the markers from knowing the identity of candidates), of delivering standardized tests.		As the profession transitioned to the modern mass-education system, the style of examination became fixed, with the stress on standardized papers to be sat by large numbers of students. Leading the way in this regard was the burgeoning Civil Service that began to move toward a meritocratic basis for selection in the mid 19th century in England.		British civil service was influenced by the imperial examinations system and meritocratic system of China. Thomas Taylor Meadows, Britain's consul in Guangzhou, China argued in his Desultory Notes on the Government and People of China, published in 1847, that "the long duration of the Chinese empire is solely and altogether owing to the good government which consists in the advancement of men of talent and merit only," and that the British must reform their civil service by making the institution meritocratic.[7] As early as in 1806, the Honourable East India Company established a college near London to train and examine administrators of the Company's territories in India.[8] Examinations for the Indian 'civil service'- a term coined by the Company – were introduced in 1829.[9]		In 1853 the Chancellor of the Exchequer William Gladstone, commissioned Sir Stafford Northcote and Charles Trevelyan to look into the operation and organisation of the Civil Service. Influenced by the ancient Chinese Imperial Examination, the Northcote–Trevelyan Report of 1854 made four principal recommendations: that recruitment should be on the basis of merit determined through standardized written examination, that candidates should have a solid general education to enable inter-departmental transfers, that recruits should be graded into a hierarchy and that promotion should be through achievement, rather than 'preferment, patronage or purchase'.[10] A Civil Service Commission was also set up in 1855 to oversee open recruitment and end patronage, and most of the other Northcote–Trevelyan recommendations were implemented over some years.[11]		The Northcote–Trevelyan model of meritocratic examination remained essentially stable for a hundred years. This was a tribute to its success in removing corruption, delivering public services (even under the stress of two world wars), and responding effectively to political change. It also had a great international influence and was adapted by members of the Commonwealth. The Pendleton Civil Service Reform Act established a similar system in the United States.		Written examinations had been unheard of before 1702 for European education. "The Chinese examinations were described repeatedly in Western literature on China of the seventeenth and eighteenth centuries."[12] Standardized testing began to influence the method of examination in British universities from the 1850s, where oral examination had been the norm since the Middle Ages. In the US, the transition happened under the influence of the educational reformer Horace Mann. This shift decisively helped to move education into the modern era, by standardizing expanding curricula in the sciences and humanities, creating a rationalized method for the evaluation of teachers and institutions and creating a basis for the streaming of students according to ability.[13]		Both World War I and World War II demonstrated the necessity of standardized testing and the benefits associated with these tests. Tests were used to determine the mental aptitude of recruits to the military. The US Army used the Stanford–Binet Intelligence Scale to test the IQ of the soldiers.[14]		After the War, industry began using tests to evaluate applicants for various jobs based on performance. In 1952, the first Advanced Placement (AP) test was administered to begin closing the gap between high schools and colleges.[15]		Some countries such as the United Kingdom and France require all their secondary school students to take a standardized test on individual subjects such as the General Certificate of Secondary Education (GCSE) (in England) and Baccalauréat respectively as a requirement for graduation.[16] These tests are used primarily to assess a student's proficiency in specific subjects such as mathematics, science, or literature. In contrasts, high school students in other countries such as the United States may not be required to take a standardized test to graduate. Moreover, students in these countries usually take standardized tests only to apply for a position in a university program and are typically given the option of taking different standardized tests such as the ACT or SAT, which are used primarily to measure a student's reasoning skill.[17][18] High school students in the United States may also take Advanced Placement tests on specific subjects to fulfill university-level credit. Depending on the policies of the test maker or country, administration of standardized tests may be done in a large hall, classroom, or testing center. A proctor or invigilator may also be present during the testing period to provide instructions, to answer questions, or to prevent cheating.		Grades or test scores from standardized test may also be used by universities to determine if a student applicant should be admitted into one of its academic or professional programs. For example, universities in the United Kingdom admit applicants into their undergraduate programs based primarily or solely on an applicant's grades on pre-university qualifications such as the GCE A-levels or Cambridge Pre-U.[19][20] In contrast, universities in the United States use an applicant's test score on the SAT or ACT as just one of their many admission criteria to determine if an applicant should be admitted into one of its undergraduate programs. The other criteria in this case may include the applicant's grades from high school, extracurricular activities, personal statement, and letters of recommendations.[21] Once admitted, undergraduate students in the United Kingdom or United States may be required by their respective programs to take a comprehensive examination as a requirement for passing their courses or for graduating from their respective programs.		Standardized tests are sometimes used by certain countries to manage the quality of their educational institutions. For example, the No Child Left Behind Act in the United States requires individual states to develop assessments for students in certain grades. In practice, these assessments typically appear in the form of standardized tests. Test scores of students in specific grades of an educational institution are then used to determine the status of that educational institution, i.e., whether it should be allowed to continue to operate in the same way or to receive funding.		Finally, standardized tests are sometimes used to compare proficiencies of students from different institutions or countries. For example, the Organisation for Economic Co-operation and Development (OECD) uses Programme for International Student Assessment (PISA) to evaluate certain skills and knowledge of students from different participating countries.[22]		Standardized tests are sometimes used by certain governing bodies to determine if a test taker is allowed to practice a profession, to use a specific job title, or to claim competency in a specific set of skills. For example, a test taker who intends to become a lawyer is usually required by a governing body such as a governmental bar licensing agency to pass a bar exam.		Standardized tests are also used in certain countries to regulate immigration. For example, intended immigrants to Australia are legally required to pass a citizenship test as part of that country's naturalization process.[23]		Tests are sometimes used as a tool to select for participants that have potential to succeed in a competition such as a sporting event. For example, serious skaters who wish to participate in figure skating competitions in the United States must pass official U.S. Figure Skating tests just to qualify.[24]		Tests are sometimes used by a group to select for certain types of individuals to join the group. For example, Mensa International is a high I.Q. society that requires individuals to score at the 98th percentile or higher on a standardized, supervised IQ test.[25]		Written tests are tests that are administered on paper or on a computer (as an eExam). A test taker who takes a written test could respond to specific items by writing or typing within a given space of the test or on a separate form or document.		In some tests; where knowledge of many constants or technical terms is required to effectively answer questions, like Chemistry or Biology – the test developer may allow every test taker to bring with them a cheat sheet.		A test developer's choice of which style or format to use when developing a written test is usually arbitrary given that there is no single invariant standard for testing. Be that as it may, certain test styles and format have become more widely used than others. Below is a list of those formats of test items that are widely used by educators and test developers to construct paper or computer-based tests. As a result, these tests may consist of only one type of test item format (e.g., multiple choice test, essay test) or may have a combination of different test item formats (e.g., a test that has multiple choice and essay items).		In a test that has items formatted as multiple choice questions, a candidate would be given a number of set answers for each question, and the candidate must choose which answer or group of answers is correct. There are two families of multiple choice questions.[26] The first family is known as the True/False question and it requires a test taker to choose all answers that are appropriate. The second family is known as One-Best-Answer question and it requires a test taker to answer only one from a list of answers.		There are several reasons to using multiple choice questions in tests. In terms of administration, multiple choice questions usually requires less time for test takers to answer, are easy to score and grade, provide greater coverage of material, allows for a wide range of difficulty, and can easily diagnose a test taker's difficulty with certain concepts.[27] As an educational tool, multiple choice items test many levels of learning as well as a test taker's ability to integrate information, and it provides feedback to the test taker about why distractors were wrong and why correct answers were right. Nevertheless, there are difficulties associated with the use of multiple choice questions. In administrative terms, multiple choice items that are effective usually take a great time to construct.[27] As an educational tool, multiple choice items do not allow test takers to demonstrate knowledge beyond the choices provided and may even encourage guessing or approximation due to the presence of at least one correct answer. For instance, a test taker might not work out explicitly that 6.14 ⋅ 7.95 = 48.813 {\displaystyle 6.14\cdot 7.95=48.813} , but knowing that 6 ⋅ 8 = 48 {\displaystyle 6\cdot 8=48} , they would choose an answer close to 48. Moreover, test takers may misinterpret these items and in the process, perceive these items to be tricky or picky. Finally, multiple choice items do not test a test taker's attitudes towards learning because correct responses can be easily faked.		True/False questions present candidates with a binary choice – a statement is either true or false. This method presents problems, as depending on the number of questions, a significant number of candidates could get 100% just by guesswork, and should on average get 50%.		A matching item is an item that provides a defined term and requires a test taker to match identifying characteristics to the correct term.[28]		A fill-in-the-blank item provides a test taker with identifying characteristics and requires the test taker to recall the correct term.[28] There are two types of fill-in-the-blank tests. The easier version provides a word bank of possible words that will fill in the blanks. For some exams all words in the word bank are used exactly once. If a teacher wanted to create a test of medium difficulty, they would provide a test with a word bank, but some words may be used more than once and others not at all. The hardest variety of such a test is a fill-in-the-blank test in which no word bank is provided at all. This generally requires a higher level of understanding and memory than a multiple choice test. Because of this, fill-in-the-blank tests[with no word bank] are often feared by students.		Items such as short answer or essay typically require a test taker to write a response to fulfill the requirements of the item. In administrative terms, essay items take less time to construct.[27] As an assessment tool, essay items can test complex learning objectives as well as processes used to answer the question. The items can also provide a more realistic and generalizable task for test. Finally, these items make it difficult for test takers to guess the correct answers and require test takers to demonstrate their writing skills as well as correct spelling and grammar.		The difficulties with essay items is primarily administrative. For one, these items take more time for test takers to answer.[27] When these questions are answered, the answers themselves are usually poorly written because test takers may not have time to organize and proofread their answers. In turn, it takes more time to score or grade these items. When these items are being scored or graded, the grading process itself becomes subjective as non-test related information may influence the process. Thus, considerable effort is required to minimize the subjectivity of the grading process. Finally, as an assessment tool, essay questions may potentially be unreliable in assessing the entire content of a subject matter.		Most mathematics questions, or calculation questions from subjects such as chemistry, physics or economics employ a style which does not fall into any of the above categories, although some papers, notably the Maths Challenge papers in the United Kingdom employ multiple choice. Instead, most mathematics questions state a mathematical problem or exercise that requires a student to write a freehand response. Marks are given more for the steps taken than for the correct answer. If the question has multiple parts, later parts may use answers from previous sections, and marks may be granted if an earlier incorrect answer was used but the correct method was followed, and an answer which is correct (given the incorrect input) is returned.		Higher level mathematical papers may include variations on true/false, where the candidate is given a statement and asked to verify its validity by direct proof or stating a counterexample.		A physical fitness test is a test designed to measure physical strength, agility, and endurance. They are commonly employed in educational institutions as part of the physical education curriculum, in medicine as part of diagnostic testing, and as eligibility requirements in fields that focus on physical ability such as military or police. Throughout the 20th century, scientific evidence emerged demonstrating the usefulness of strength training and aerobic exercise in maintaining overall health, and more agencies began to incorporate standardized fitness testing. In the United States, the President's Council on Youth Fitness was established in 1956 as a way to encourage and monitor fitness in schoolchildren.		Common tests[29][30][31] include timed running or the multi-stage fitness test (commonly known as the "beep test), and numbers of push-ups, sit-ups/abdominal crunches and pull-ups that the individual can perform. More specialised tests may be used to test ability to perform a particular job or role. Many gyms, private organisations and event organisers have their own fitness tests. Using military techniques developed by the British Army and modern test like Illinois Agility Run and Cooper Test.[32]		A performance test is an assessment that requires an examinee to actually perform a task or activity, rather than simply answering questions referring to specific parts. The purpose is to ensure greater fidelity to what is being tested.		An example is a behind-the-wheel driving test to obtain a driver's license. Rather than only answering simple multiple-choice items regarding the driving of an automobile, a student is required to actually drive one while being evaluated.		Performance tests are commonly used in workplace and professional applications, such as professional certification and licensure. When used for personnel selection, the tests might be referred to as a work sample. A licensure example would be cosmetologists being required to demonstrate a haircut or manicure on a live person. The Group-Bourdon test is one of a number of psychometric tests which trainee train drivers in the UK are required to pass.[33]		Some performance tests are simulations. For instance, the assessment to become certified as an ophthalmic technician includes two components, a multiple-choice examination and a computerized skill simulation. The examinee must demonstrate the ability to complete seven tasks commonly performed on the job, such as retinoscopy, that are simulated on a computer.		From the perspective of a test developer, there is great variability with respect to time and effort needed to prepare a test. Likewise, from the perspective of a test taker, there is also great variability with respect to the time and needed to obtain a desired grade or score on any given test. When a test developer constructs a test, the amount of time and effort is dependent upon the significance of the test itself, the proficiency of the test taker, the format of the test, class size, deadline of test, and experience of the test developer.		The process of test construction has been aided in several ways. For one, many test developers were themselves students at one time, and therefore are able to modify or outright adopt questions from their previous tests. In some countries, book publishers often provide teaching packages that include test banks to university instructors who adopt their published books for their courses.[34] These test banks may contain up to four thousand sample test questions that have been peer-reviewed and time-tested. The instructor who chooses to use this testbank would only have to select a fixed number of test questions from this test bank to construct a test.		As with test constructions, the time needed for a test taker to prepare for a test is dependent upon the frequency of the test, the test developer, and the significance of the test. In general, nonstandardized tests that are short, frequent, and do not constitute a major portion of the test taker's overall course grade or score do not require the test taker to spend much time preparing for the test.[35] Conversely, nonstandardized tests that are long, infrequent, and do constitute a major portion of the test taker's overall course grade or score usually require the test taker to spend great amounts of time preparing for the test. To prepare for a nonstandardized test, test takers may rely upon their reference books, class or lecture notes, Internet, and past experience. Test takers may also use various learning aids to study for tests such as flashcards and mnemonics.[36] Test takers may even hire tutors to coach them through the process so that they may increase the probability of obtaining a desired test grade or score. In countries such as the United Kingdom, demand for private tuition has increased significantly in recent years.[37] Finally, test takers may rely upon past copies of a test from previous years or semesters to study for a future test. These past tests may be provided by a friend or a group that has copies of previous tests or by instructors and their institutions, or by the test provider (such as an examination board) itself.[38][39]		Unlike a nonstandardized test, the time needed by test takers to prepare for standardized tests is less variable and usually considerable. This is because standardized tests are usually uniform in scope, format, and difficulty and often have important consequences with respect to a test taker's future such as a test taker's eligibility to attend a specific university program or to enter a desired profession. It is not unusual for test takers to prepare for standardized tests by relying upon commercially available books that provide in-depth coverage of the standardized test or compilations of previous tests (e.g., 10 year series in Singapore). In many countries, test takers even enroll in test preparation centers or cram schools that provide extensive or supplementary instructions to test takers to help them better prepare for a standardized test. In Hong Kong, it has been suggested that the tutors running such centers are celebrities in their own right.[40] This has led to private tuition being a popular career choice for new graduates in developed economies.[41][42] Finally, in some countries, instructors and their institutions have also played a significant role in preparing test takers for a standardized test.		Cheating on a test is the process of using unauthorized means or methods for the purpose of obtaining a desired test score or grade. This may range from bringing and using notes during a closed book examination, to copying another test taker's answer or choice of answers during an individual test, to sending a paid proxy to take the test.[43]		Several common methods have been employed to combat cheating. They include the use of multiple proctors or invigilators during a testing period to monitor test takers. Test developers may construct multiple variants of the same test to be administered to different test takers at the same time, or write tests with few multiple-choice options, based on the theory that fully worked answers are difficult to imitate.[44] In some cases, instructors themselves may not administer their own tests but will leave the task to other instructors or invigilators, which may mean that the invigilators do not know the candidates, and thus some form of identification may be required. Finally, instructors or test providers may compare the answers of suspected cheaters on the test themselves to determine if cheating did occur.		Despite their widespread use, the validity, quality, or use of tests, particularly standardized tests in education have continued to be widely supported or criticized. Like the tests themselves, supports and criticisms of tests are often varied and may come from a variety of sources such as parents, test takers, instructors, business groups, universities, or governmental watchdogs.		Supporters of standardized tests in education often provide the following reasons for promoting testing in education:		Critics of standardized tests in education often provide the following reasons for revising or removing standardized tests in education:		
VO2 max (also maximal oxygen consumption, maximal oxygen uptake, peak oxygen uptake or maximal aerobic capacity) is the maximum rate of oxygen consumption as measured during incremental exercise, most typically on a motorized treadmill.[1][2] Maximal oxygen consumption reflects the aerobic physical fitness of the individual, and is an important determinant of their endurance capacity during prolonged, sub-maximal exercise. The name is derived from V - volume, O2 - oxygen, max - maximum.		VO2 max is expressed either as an absolute rate in (for example) litres of oxygen per minute (L/min) or as a relative rate in (for example) millilitres of oxygen per kilogram of body mass per minute (e.g., mL/(kg·min)). The latter expression is often used to compare the performance of endurance sports athletes. However, VO2 max generally does not vary linearly with body mass, either among individuals within a species or among species,[1][2] so comparisons of the performance capacities of individuals or species that differ in body size must be done with appropriate statistical procedures, such as analysis of covariance.						Accurately measuring VO2 max involves a physical effort sufficient in duration and intensity to fully tax the aerobic energy system. In general clinical and athletic testing, this usually involves a graded exercise test (either on a treadmill or on a cycle ergometer) in which exercise intensity is progressively increased while measuring:		VO2 max is reached when oxygen consumption remains at a steady state despite an increase in workload.		VO2 max is properly defined by the Fick equation:		Tests measuring VO2 max can be dangerous in individuals who are not considered normal healthy subjects, as any problems with the respiratory and cardiovascular systems will be greatly exacerbated in clinically ill patients. Thus, many protocols for estimating VO2 max have been developed for those for whom a traditional VO2 max test would be too risky. These generally are similar to a VO2 max test, but do not reach the maximum of the respiratory and cardiovascular systems and are called sub-maximal tests.		Another estimate of VO2 max, based on maximum and resting heart rates, was created by a group of researchers from Denmark.[4] It is given by:		This equation uses the ratio of maximum heart rate (HRmax) to resting heart rate (HRrest) to predict VO2 max. The researchers cautioned that the conversion rule was based on measurements on well-trained men aged 21 to 51 only, and may not be reliable when applied to other sub-groups. They also advised that the formula is most reliable when based on actual measurement of maximum heart rate, rather than an age-related estimate.		Kenneth H. Cooper conducted a study for the United States Air Force in the late 1960s. One of the results of this was the Cooper test in which the distance covered running in 12 minutes is measured. Based on the measured distance, an estimate of VO2 max [in mL/(kg·min)] is:[5]		where d12 is distance (in metres) covered in 12 minutes		An alternative equation is:		where dmiles12 is distance (in miles) covered in 12 minutes,		There are several other reliable tests and VO2 max calculators to estimate VO2 max, most notably the multi-stage fitness test (or beep test).[6]		Estimation of VO2 max from a timed one-mile track walk with duration t, incorporating gender, age, body weight in pounds (BW), and heart rate (HR) at the end of the mile.[7] The factor x is 6.3150 for males, 0 for females. BW is in lbs, time is in minutes.		The Firstbeat method of VO2 max estimation, of which a patent application was filed in 2012,[8] is widely licensed and used in consumer technology applications.[9] The first consumer fitness device utilizing the Firstbeat method of VO2 max estimation was the Garmin Forerunner 620, released in 2013.[10] Since then, Suunto, Jabra, Huawei, and PulseOn[11] have also introduced products that utilize the Firstbeat method.[12]		The method relies on an analysis of the linear relationship between oxygen consumption and running speed, meaning that the oxygen cost of running increases when running speed increases. To facilitate analysis and enhance accuracy, timed segments of recorded activity data are identified on the basis of heart rate ranges and reliability; and only the most reliable segments are utilized. This allows the method to be applied to freely performed running, walking and cycling activities and diminishes the need for dedicated fitness testing protocols. The calculation requires user basic anthropometric data (age, gender, height, weight, etc.), heartbeat data (internal workload), and a measure of external workload.[13]		VO2 max estimates provided by the Firstbeat method are most accurate during running activities that utilize GPS to capture external workload data. This combination has been validated to be 95% accurate compared to laboratory testing.[13] Because the Firstbeat estimation method is sub-maximal in nature, accuracy of the estimate is strongly tied to validity of the HRmax value used in the calculation.[13]		The average untrained healthy male will have a VO2 max of approximately 35–40 mL/(kg·min).[14][15] The average untrained healthy female will score a VO2 max of approximately 27–31 mL/(kg·min).[14] These scores can improve with training and decrease with age, though the degree of trainability also varies very widely: conditioning may double VO2 max in some individuals, and will never improve it in others. In one study, 10% of participants showed no benefit after completing a 20-week conditioning program, although the other 90% of the test subjects all showed substantial improvements in fitness to varying degree.[16][17]		In sports where endurance is an important component in performance, such as cycling, rowing, cross-country skiing, swimming and running, world-class athletes typically have high VO2 maxima. Elite male runners can consume up to 85 mL/(kg·min), and female elite runners can consume about 77 mL/(kg·min).[18] Five time Tour de France winner Miguel Indurain is reported to have had a VO2 max of 88.0 at his peak, while cross-country skier Bjørn Dæhlie measured at 96 mL/(kg·min).[19] Dæhlie's result was achieved out of season, and physiologist Erlend Hem who was responsible for the testing stated that he would not discount the possibility of the skier passing 100 mL/(kg·min) at his absolute peak. Norwegian cyclist Oskar Svendsen is thought to have recorded the highest VO2 max of 97.5 mL/(kg·min), when aged 18.[20]		The highest values in absolute terms for humans are often found in rowers, as their much greater bulk makes up for a slightly lower VO2 max per kg. Elite oarsmen measured in 1984 had VO2 max values of 6.1±0.6 L/min and oarswomen 4.1±0.4 L/min.[21] Rowers are interested in both absolute values of VO2 max and in lung capacity, and the fact that they are measured in similar units means that the two are often confused. British rower Sir Matthew Pinsent is variously reported to have had a VO2 of 7.5 L/min[22] or 8.5 L/min, although the latter may represent confusion with his lung capacity of 8.5 litres.[23] New Zealand sculler Rob Waddell has one of the highest absolute VO2 max levels ever tested.[24]		Values have been measured in some other animal species: thoroughbred horses have a VO2 max of around 180 mL/(kg·min). Siberian dogs running in the Iditarod Trail Sled Dog Race have VO2 max values as high as 240 mL/(kg·min).[25]		The factors affecting VO2 are often divided into supply and demand.[26] Supply is the transport of oxygen from the lungs to the mitochondria (including lung diffusion, stroke volume, blood volume, and capillary density of the skeletal muscle) while demand is the rate at which the mitochondria can reduce oxygen in the process of oxidative phosphorylation.[26] Of these, the supply factor is often considered to be the limiting one.[26][27] However, it has also been argued that while trained subjects probably are supply limited, untrained subjects can indeed have a demand limitation.[28]		Tim Noakes, a professor of exercise and sports science at the University of Cape Town, describes a number of factors that may affect VO2 max: age, sex, fitness and training, changes in altitude, and action of the ventilatory muscles.[29] Noakes also asserts that VO2 max is a relatively poor predictor of performance in runners due to variations in running economy and fatigue resistance during prolonged exercise.[29]		Cardiac output, pulmonary diffusion capacity, oxygen carrying capacity, and other peripheral limitations like muscle diffusion capacity, mitochondrial enzymes, and capillary density are all examples of VO2 max determinants. The body works as a system. If one of these factor is sub-par, then the whole system loses its normal capacity to function properly.[28]		The drug erythropoietin (EPO) can boost VO2 max by a significant amount in both humans and other mammals.[30] This makes EPO attractive to athletes in endurance sports, such as professional cycling. By 1998 it had become widespread in cycling and led to the Festina affair[31][32] as well as being mentioned ubiquitously in the USADA 2012 report on the US Postal team.[33] Greg LeMond has suggested establishing a baseline for riders' VO2 max (and other attributes) to detect abnormal performance increases.[34]		
The following outline is provided as an overview of and topical guide to exercise:		Exercise – any bodily activity that enhances or maintains physical fitness and overall health and wellness. It is performed for various reasons including strengthening muscles and the cardiovascular system, honing athletic skills, weight loss or maintenance, as well as for the purpose of enjoyment. Frequent and regular physical exercise boosts the immune system, and helps prevent the "diseases of affluence" such as heart disease, cardiovascular disease, Type 2 diabetes and obesity.[1][2]		Aerobic exercise –		Anaerobic exercise –		Strength training (by muscle to be strengthened; (c) = compound exercise, (i) = isolated exercise)		Calisthenics		Stretching –		List of exercise equipment		Exercise trends		Exercise physiology		Remote physiological monitoring		|group2 = See also |list2 =		|below = }}		
Nutrition is the science that interprets the interaction of nutrients and other substances in food in relation to maintenance, growth, reproduction, health and disease of an organism. It includes food intake, absorption, assimilation, biosynthesis, catabolism and excretion.[1]		The diet of an organism is what it eats, which is largely determined by the availability, the processing and palatability of foods. A healthy diet includes preparation of food and storage methods that preserve nutrients from oxidation, heat or leaching, and that reduce risk of foodborne illness.		A poor diet can cause deficiency diseases such as blindness, anemia, scurvy, preterm birth, stillbirth and cretinism;[2] health-threatening conditions like obesity[3][4] and metabolic syndrome;[5] and such common chronic systemic diseases as cardiovascular disease,[6] diabetes,[7][8] and osteoporosis.[9][10][11] A poor diet can cause the wasting of kwashiorkor in acute cases, and the stunting of marasmus in chronic cases of malnutrition.[2]						The first recorded dietary advice, carved into a Babylonian stone tablet in about 2500 BC, cautioned those with pain inside to avoid eating onions for three days. Scurvy, later found to be a vitamin C deficiency, was first described in 1500 BC in the Ebers Papyrus.[12]		According to Walter Gratzer, the study of nutrition probably began during the 6th century BC. In China, the concept of Qi developed, a spirit or "wind" similar to what Western Europeans later called pneuma.[13] Food was classified into "hot" (for example, meats, blood, ginger, and hot spices) and "cold" (green vegetables) in China, India, Malaya, and Persia.[14] Humours developed perhaps first in China alongside qi.[13] Ho the Physician concluded that diseases are caused by deficiencies of elements (Wu Xing: fire, water, earth, wood, and metal), and he classified diseases as well as prescribed diets.[14] About the same time in Italy, Alcmaeon of Croton (a Greek) wrote of the importance of equilibrium between what goes in and what goes out, and warned that imbalance would result in disease marked by obesity or emaciation.[15]		The first recorded nutritional experiment with human subjects is found in the Bible's Book of Daniel. Daniel and his friends were captured by the king of Babylon during an invasion of Israel. Selected as court servants, they were to share in the king's fine foods and wine. But they objected, preferring vegetables (pulses) and water in accordance with their Jewish dietary restrictions. The king's chief steward reluctantly agreed to a trial. Daniel and his friends received their diet for ten days and were then compared to the king's men. Appearing healthier, they were allowed to continue with their diet.[16][17]		Around 475 BC, Anaxagoras stated that food is absorbed by the human body and, therefore, contains "homeomerics" (generative components), suggesting the existence of nutrients.[18] Around 400 BC, Hippocrates, who recognized and was concerned with obesity, which may have been common in southern Europe at the time,[15] said, "Let food be your medicine and medicine be your food."[19] The works that are still attributed to him, Corpus Hippocraticum, called for moderation and emphasized exercise.[15]		Salt, pepper and other spices were prescribed for various ailments in various preparations for example mixed with vinegar. In the 2nd century BC, Cato the Elder believed that cabbage (or the urine of cabbage-eaters) could cure digestive diseases, ulcers, warts, and intoxication. Living about the turn of the millennium, Aulus Celsus, an ancient Roman doctor, believed in "strong" and "weak" foods (bread for example was strong, as were older animals and vegetables).[20]		One mustn't overlook the doctrines of Galen: In use from his life in the 1st century AD until the 17th century, it was heresy to disagree with him for 1500 years.[21] Galen was physician to gladiators in Pergamon, and in Rome, physician to Marcus Aurelius and the three emperors who succeeded him.[22] Most of Galen's teachings were gathered and enhanced in the late 11th century by Benedictine monks at the School of Salerno in Regimen sanitatis Salernitanum, which still had users in the 17th century.[23] Galen believed in the bodily humours of Hippocrates, and he taught that pneuma is the source of life. Four elements (earth, air, fire and water) combine into "complexion", which combines into states (the four temperaments: sanguine, phlegmatic, choleric, and melancholic). The states are made up of pairs of attributes (hot and moist, cold and moist, hot and dry, and cold and dry), which are made of four humours: blood, phlegm, green (or yellow) bile, and black bile (the bodily form of the elements). Galen thought that for a person to have gout, kidney stones, or arthritis was scandalous, which Gratzer likens to Samuel Butler's Erehwon (1872) where sickness is a crime.[21]		In the 1500s, Paracelsus was probably the first to criticize Galen publicly.[21] Also in the 16th century, scientist and artist Leonardo da Vinci compared metabolism to a burning candle. Leonardo did not publish his works on this subject, but he was not afraid of thinking for himself and he definitely disagreed with Galen.[14] Ultimately, 16th century works of Andreas Vesalius, sometimes called the father of modern medicine, overturned Galen's ideas.[25] He was followed by piercing thought amalgamated with the era's mysticism and religion sometimes fueled by the mechanics of Newton and Galileo. Jan Baptist van Helmont, who discovered several gases such as carbon dioxide, performed the first quantitative experiment. Robert Boyle advanced chemistry. Sanctorius measured body weight. Physician Herman Boerhaave modeled the digestive process. Physiologist Albrecht von Haller worked out the difference between nerves and muscles.[26]		Sometimes forgotten during his life, James Lind, a physician in the British navy, performed the first scientific nutrition experiment in 1747. Lind discovered that lime juice saved sailors that had been at sea for years from scurvy, a deadly and painful bleeding disorder. Between 1500 and 1800, an estimated two million sailors had died of scurvy.[27] The discovery was ignored for forty years, after which British sailors became known as "limeys."[28] The essential vitamin C within citrus fruits would not be identified by scientists until 1932.[27]		Around 1770, Antoine Lavoisier discovered the details of metabolism, demonstrating that the oxidation of food is the source of body heat. Called the most fundamental chemical discovery of the 18th century,[30] Lavoisier discovered the principle of conservation of mass. His ideas made the phlogiston theory of combustion obsolete.[31]		In 1790, George Fordyce recognized calcium as necessary for the survival of fowl. In the early 19th century, the elements carbon, nitrogen, hydrogen, and oxygen were recognized as the primary components of food, and methods to measure their proportions were developed.[32]		In 1816, François Magendie discovered that dogs fed only carbohydrates (sugar), fat (olive oil), and water died evidently of starvation, but dogs also fed protein survived, identifying protein as an essential dietary component.[33] William Prout in 1827 was the first person to divide foods into carbohydrates, fat, and protein.[34] During the 19th century, Jean-Baptiste Dumas and Justus von Liebig quarrelled over their shared belief that animals get their protein directly from plants (animal and plant protein are the same and that humans do not create organic compounds).[35] With a reputation as the leading organic chemist of his day but with no credentials in animal physiology,[36] Liebig grew rich making food extracts like beef bouillon and infant formula that were later found to be of questionable nutritious value.[37] In the 1860s, Claude Bernard discovered that body fat can be synthesized from carbohydrate and protein, showing that the energy in blood glucose can be stored as fat or as glycogen.[38]		In the early 1880s, Kanehiro Takaki observed that Japanese sailors (whose diets consisted almost entirely of white rice) developed beriberi (or endemic neuritis, a disease causing heart problems and paralysis), but British sailors and Japanese naval officers did not. Adding various types of vegetables and meats to the diets of Japanese sailors prevented the disease, (not because of the increased protein as Takaki supposed but because it introduced a few parts per million of thiamine to the diet, later understood as a cure[39]).		In 1896, Eugen Baumann observed iodine in thyroid glands. In 1897, Christiaan Eijkman worked with natives of Java, who also suffered from beriberi. Eijkman observed that chickens fed the native diet of white rice developed the symptoms of beriberi but remained healthy when fed unprocessed brown rice with the outer bran intact. Eijkman cured the natives by feeding them brown rice, discovering that food can cure disease. Over two decades later, nutritionists learned that the outer rice bran contains vitamin B1, also known as thiamine.		In the early 20th century, Carl von Voit and Max Rubner independently measured caloric energy expenditure in different species of animals, applying principles of physics in nutrition. In 1906, Edith G. Willcock and Frederick Hopkins showed that the amino acid tryptophan aids the well-being of mice but it did not assure their growth.[40] In the middle of twelve years of attempts to isolate them,[41] Hopkins said in a 1906 lecture that "unsuspected dietetic factors," other than calories, protein, and minerals, are needed to prevent deficiency diseases.[42] In 1907, Stephen M. Babcock and Edwin B. Hart started the cow feeding, single-grain experiment, which took nearly four years to complete.		In 1912, Casimir Funk coined the term vitamin, a vital factor in the diet, from the words "vital" and "amine," because these unknown substances preventing scurvy, beriberi, and pellagra, were thought then to be derived from ammonia. The vitamins were studied in the first half of the 20th century.		In 1913, Elmer McCollum and Marguerite Davis discovered the first vitamin, fat-soluble vitamin A, then water-soluble vitamin B (in 1915; now known to be a complex of several water-soluble vitamins) and named vitamin C as the then-unknown substance preventing scurvy. Lafayette Mendel and Thomas Osborne also performed pioneering work on vitamins A and B. In 1919, Sir Edward Mellanby incorrectly identified rickets as a vitamin A deficiency because he could cure it in dogs with cod liver oil.[45] In 1922, McCollum destroyed the vitamin A in cod liver oil, but found that it still cured rickets.[45] Also in 1922, H.M. Evans and L.S. Bishop discover vitamin E as essential for rat pregnancy, originally calling it "food factor X" until 1925.		In 1925, Hart discovered that trace amounts of copper are necessary for iron absorption. In 1927, Adolf Otto Reinhold Windaus synthesized vitamin D, and was awarded the Nobel Prize in Chemistry in 1928. In 1928, Albert Szent-Györgyi isolated ascorbic acid, and in 1932 proved that it is vitamin C by preventing scurvy. In 1935, he synthesized it, and in 1937, he won a Nobel Prize for his efforts. Szent-Györgyi concurrently elucidated much of the citric acid cycle.		In the 1930s, William Cumming Rose identified essential amino acids, necessary protein components that the body cannot synthesize. In 1935, Underwood and Marston independently discovered the necessity of cobalt. In 1936, Eugene Floyd DuBois showed that work and school performance are related to caloric intake. In 1938, Erhard Fernholz discovered the chemical structure of vitamin E and then he tragically disappeared.[46][47] It was synthesised the same year by Paul Karrer.[46]		In 1940, rationing in the United Kingdom during and after World War II took place according to nutritional principles drawn up by Elsie Widdowson and others. In 1941, the first Recommended Dietary Allowances (RDAs) were established by the National Research Council.		In 1992, The U.S. Department of Agriculture introduced the Food Guide Pyramid.[48] This replaced the Four Food Groups (1956-1992) and was superseded by the concept of MyPlate (2011-present).		A 2014 meta-analysis concluded that adenovirus 36 (Ad36) infection is associated with an increased risk of obesity development.[49]		The list of nutrients that people are known to require is, in the words of Marion Nestle, "almost certainly incomplete".[50] As of 2014, nutrients are thought to be of two types: macro-nutrients which are needed in relatively large amounts, and micronutrients which are needed in smaller quantities.[51] A type of carbohydrate, dietary fiber, i.e. non-digestible material such as cellulose, is required,[52] for both mechanical and biochemical reasons, although the exact reasons remain unclear. Some nutrients can be stored - the fat-soluble vitamins - while others are required more or less continuously. Poor health can be caused by a lack of required nutrients, or for some vitamins and minerals, too much of a required nutrient.		The macronutrients are carbohydrates, fiber, fats, protein, and water.[51] The macronutrients (excluding fiber and water) provide structural material (amino acids from which proteins are built, and lipids from which cell membranes and some signaling molecules are built) and energy. Some of the structural material can be used to generate energy internally, and in either case it is measured in Joules or kilocalories (often called "Calories" and written with a capital C to distinguish them from little 'c' calories). Carbohydrates and proteins provide 17 kJ approximately (4 kcal) of energy per gram, while fats provide 37 kJ (9 kcal) per gram,[53] though the net energy from either depends on such factors as absorption and digestive effort, which vary substantially from instance to instance. Vitamins, minerals, fiber, and water do not provide energy, but are required for other reasons.		Molecules of carbohydrates and fats consist of carbon, hydrogen, and oxygen atoms. Carbohydrates range from simple monosaccharides (glucose, fructose, galactose) to complex polysaccharides (starch). Fats are triglycerides, made of assorted fatty acid monomers bound to a glycerol backbone. Some fatty acids, but not all, are essential in the diet: they cannot be synthesized in the body. Protein molecules contain nitrogen atoms in addition to carbon, oxygen, and hydrogen. The fundamental components of protein are nitrogen-containing amino acids, some of which are essential in the sense that humans cannot make them internally. Some of the amino acids are convertible (with the expenditure of energy) to glucose and can be used for energy production, just as ordinary glucose, in a process known as gluconeogenesis. By breaking down existing protein, the carbon skeleton of the various amino acids can be metabolized to intermediates in cellular respiration; the remaining ammonia is discarded primarily as urea in urine.		Carbohydrates may be classified as monosaccharides, disaccharides, or polysaccharides depending on the number of monomer (sugar) units they contain. They constitute a large part of foods such as rice, noodles, bread, and other grain-based products, also potatoes, yams, beans, fruits, fruit juices and vegetables. Monosaccharides, disaccharides, and polysaccharides contain one, two, and three or more sugar units, respectively. Polysaccharides are often referred to as complex carbohydrates because they are typically long, multiple branched chains of sugar units.		Traditionally, simple carbohydrates are believed to be absorbed quickly, and therefore to raise blood-glucose levels more rapidly than complex carbohydrates. This, however, is not accurate.[54][55][56][57] Some simple carbohydrates (e.g., fructose) follow different metabolic pathways (e.g., fructolysis) that result in only a partial catabolism to glucose, while, in essence, many complex carbohydrates may be digested at the same rate as simple carbohydrates.[58] The World Health Organization (WHO) recommends that added sugars should represent no more than 10% of total energy intake.[59]		Dietary fiber is a carbohydrate that is incompletely absorbed in humans and in some animals. Like all carbohydrates, when it is metabolized it can produce four Calories (kilocalories) of energy per gram. However, in most circumstances it accounts for less than that because of its limited absorption and digestibility. Dietary fiber consists mainly of cellulose, a large carbohydrate polymer which is indigestible as humans do not have the required enzymes to disassemble it. There are two subcategories: soluble and insoluble fiber. Whole grains, fruits (especially plums, prunes, and figs), and vegetables are good sources of dietary fiber. There are many health benefits of a high-fiber diet. Dietary fiber helps reduce the chance of gastrointestinal problems such as constipation and diarrhea by increasing the weight and size of stool and softening it. Insoluble fiber, found in whole wheat flour, nuts and vegetables, especially stimulates peristalsis;– the rhythmic muscular contractions of the intestines, which move digest along the digestive tract. Soluble fiber, found in oats, peas, beans, and many fruits, dissolves in water in the intestinal tract to produce a gel that slows the movement of food through the intestines. This may help lower blood glucose levels because it can slow the absorption of sugar. Additionally, fiber, perhaps especially that from whole grains, is thought to possibly help lessen insulin spikes, and therefore reduce the risk of type 2 diabetes. The link between increased fiber consumption and a decreased risk of colorectal cancer is still uncertain.		A molecule of dietary fat typically consists of several fatty acids (containing long chains of carbon and hydrogen atoms), bonded to a glycerol. They are typically found as triglycerides (three fatty acids attached to one glycerol backbone). Fats may be classified as saturated or unsaturated depending on the detailed structure of the fatty acids involved. Saturated fats have all of the carbon atoms in their fatty acid chains bonded to hydrogen atoms, whereas unsaturated fats have some of these carbon atoms double-bonded, so their molecules have relatively fewer hydrogen atoms than a saturated fatty acid of the same length. Unsaturated fats may be further classified as monounsaturated (one double-bond) or polyunsaturated (many double-bonds). Furthermore, depending on the location of the double-bond in the fatty acid chain, unsaturated fatty acids are classified as omega-3 or omega-6 fatty acids. Trans fats are a type of unsaturated fat with trans-isomer bonds; these are rare in nature and in foods from natural sources; they are typically created in an industrial process called (partial) hydrogenation. There are nine kilocalories in each gram of fat. Fatty acids such as conjugated linoleic acid, catalpic acid, eleostearic acid and punicic acid, in addition to providing energy, represent potent immune modulatory molecules.		Saturated fats (typically from animal sources) have been a staple in many world cultures for millennia. Unsaturated fats (e. g., vegetable oil) are considered healthier, while trans fats are to be avoided. Saturated and some trans fats are typically solid at room temperature (such as butter or lard), while unsaturated fats are typically liquids (such as olive oil or flaxseed oil). Trans fats are very rare in nature, and have been shown to be highly detrimental to human health, but have properties useful in the food processing industry, such as rancidity resistance.[60]		Most fatty acids are non-essential, meaning the body can produce them as needed, generally from other fatty acids and always by expending energy to do so. However, in humans, at least two fatty acids are essential and must be included in the diet. An appropriate balance of essential fatty acids—omega-3 and omega-6 fatty acids—seems also important for health, although definitive experimental demonstration has been elusive. Both of these "omega" long-chain polyunsaturated fatty acids are substrates for a class of eicosanoids known as prostaglandins, which have roles throughout the human body. They are hormones, in some respects. The omega-3 eicosapentaenoic acid (EPA), which can be made in the human body from the omega-3 essential fatty acid alpha-linolenic acid (ALA), or taken in through marine food sources, serves as a building block for series 3 prostaglandins (e.g., weakly inflammatory PGE3). The omega-6 dihomo-gamma-linolenic acid (DGLA) serves as a building block for series 1 prostaglandins (e.g. anti-inflammatory PGE1), whereas arachidonic acid (AA) serves as a building block for series 2 prostaglandins (e.g. pro-inflammatory PGE 2). Both DGLA and AA can be made from the omega-6 linoleic acid (LA) in the human body, or can be taken in directly through food. An appropriately balanced intake of omega-3 and omega-6 partly determines the relative production of different prostaglandins, which is one reason why a balance between omega-3 and omega-6 is believed important for cardiovascular health. In industrialized societies, people typically consume large amounts of processed vegetable oils, which have reduced amounts of the essential fatty acids along with too much of omega-6 fatty acids relative to omega-3 fatty acids.		The conversion rate of omega-6 DGLA to AA largely determines the production of the prostaglandins PGE1 and PGE2. Omega-3 EPA prevents AA from being released from membranes, thereby skewing prostaglandin balance away from pro-inflammatory PGE2 (made from AA) toward anti-inflammatory PGE1 (made from DGLA). Moreover, the conversion (desaturation) of DGLA to AA is controlled by the enzyme delta-5-desaturase, which in turn is controlled by hormones such as insulin (up-regulation) and glucagon (down-regulation). The amount and type of carbohydrates consumed, along with some types of amino acid, can influence processes involving insulin, glucagon, and other hormones; therefore, the ratio of omega-3 versus omega-6 has wide effects on general health, and specific effects on immune function and inflammation, and mitosis (i.e., cell division).		Proteins are structural materials in much of the animal body (e.g. muscles, skin, and hair). They also form the enzymes that control chemical reactions throughout the body. Each protein molecule is composed of amino acids, which are characterized by inclusion of nitrogen and sometimes sulphur (these components are responsible for the distinctive smell of burning protein, such as the keratin in hair). The body requires amino acids to produce new proteins (protein retention) and to replace damaged proteins (maintenance). As there is no protein or amino acid storage provision, amino acids must be present in the diet. Excess amino acids are discarded, typically in the urine. For all animals, some amino acids are essential (an animal cannot produce them internally) and some are non-essential (the animal can produce them from other nitrogen-containing compounds). About twenty amino acids are found in the human body, and about ten of these are essential and, therefore, must be included in the diet. A diet that contains adequate amounts of amino acids (especially those that are essential) is particularly important in some situations: during early development and maturation, pregnancy, lactation, or injury (a burn, for instance). A complete protein source contains all the essential amino acids; an incomplete protein source lacks one or more of the essential amino acids.		It is possible with protein combinations of two incomplete protein sources (e.g., rice and beans) to make a complete protein source, and characteristic combinations are the basis of distinct cultural cooking traditions. However, complementary sources of protein do not need to be eaten at the same meal to be used together by the body.[61] Excess amino acids from protein can be converted into glucose and used for fuel through a process called gluconeogenesis.		Water is excreted from the body in multiple forms; including urine and feces, sweating, and by water vapour in the exhaled breath. Therefore, it is necessary to adequately rehydrate to replace lost fluids.		Early recommendations for the quantity of water required for maintenance of good health suggested that 6–8 glasses of water daily is the minimum to maintain proper hydration.[62] However the notion that a person should consume eight glasses of water per day cannot be traced to a credible scientific source.[63] The original water intake recommendation in 1945 by the Food and Nutrition Board of the National Research Council read: "An ordinary standard for diverse persons is 1 milliliter for each calorie of food. Most of this quantity is contained in prepared foods."[64] More recent comparisons of well-known recommendations on fluid intake have revealed large discrepancies in the volumes of water we need to consume for good health.[65] Therefore, to help standardize guidelines, recommendations for water consumption are included in two recent European Food Safety Authority (EFSA) documents (2010): (i) Food-based dietary guidelines and (ii) Dietary reference values for water or adequate daily intakes (ADI).[66] These specifications were provided by calculating adequate intakes from measured intakes in populations of individuals with “desirable osmolarity values of urine and desirable water volumes per energy unit consumed.”[66]		For healthful hydration, the current EFSA guidelines recommend total water intakes of 2.0 L/day for adult females and 2.5 L/day for adult males. These reference values include water from drinking water, other beverages, and from food. About 80% of our daily water requirement comes from the beverages we drink, with the remaining 20% coming from food.[67] Water content varies depending on the type of food consumed, with fruit and vegetables containing more than cereals, for example.[68] These values are estimated using country-specific food balance sheets published by the Food and Agriculture Organisation of the United Nations.[68]		The EFSA panel also determined intakes for different populations. Recommended intake volumes in the elderly are the same as for adults as despite lower energy consumption, the water requirement of this group is increased due to a reduction in renal concentrating capacity.[66] Pregnant and breastfeeding women require additional fluids to stay hydrated. The EFSA panel proposes that pregnant women should consume the same volume of water as non-pregnant women, plus an increase in proportion to the higher energy requirement, equal to 300 mL/day.[66] To compensate for additional fluid output, breastfeeding women require an additional 700 mL/day above the recommended intake values for non-lactating women. Dehydration and over-hydration - too little and too much water, respectively - can have harmful consequences.[66][69]		The micronutrients are minerals, vitamins, and others.[51]		Dietary minerals are inorganic chemical elements required by living organisms,[70] other than the four elements carbon, hydrogen, nitrogen, and oxygen that are present in nearly all organic molecules. The term "mineral" is archaic, since the intent is to describe simply the less common elements in the diet. Some are heavier than the four just mentioned, including several metals, which often occur as ions in the body. Some dietitians recommend that these be supplied from foods in which they occur naturally, or at least as complex compounds, or sometimes even from natural inorganic sources (such as calcium carbonate from ground oyster shells). Some minerals are absorbed much more readily in the ionic forms found in such sources. On the other hand, minerals are often artificially added to the diet as supplements; the most famous is likely iodine in iodized salt which prevents goiter.		Many elements are essential in relative quantity; they are usually called "bulk minerals". Some are structural, but many play a role as electrolytes.[71] Elements with recommended dietary allowance (RDA) greater than 150  mg/day are, in alphabetical order (with informal or folk-medicine perspectives in parentheses):		Many elements are required in trace amounts, usually because they play a catalytic role in enzymes.[73] Some trace mineral elements (RDA < 200 mg/day) are, in alphabetical order:		Vitamins are essential nutrients,[70] necessary in the diet for good health. (Vitamin D is the exception, as it can be synthesized in the skin in the presence of UVB radiation.) Vitamin deficiencies may result in disease conditions, including goitre, scurvy, osteoporosis, impaired immune system, disorders of cell metabolism, certain forms of cancer, symptoms of premature aging, and poor psychological health, among many others.[74] Excess levels of some vitamins are also dangerous to health. The Food and Nutrition Board of the Institute of Medicine has established Tolerable Upper Intake Levels (ULs) for seven vitamins. [75]		Heart disease, cancer, obesity, and diabetes are commonly called "Western" diseases because these maladies were once rarely seen in developing countries. An international study in China found some regions had virtually no cancer or heart disease, while in other areas they reflected "up to a 100-fold increase" coincident with shifts from diets that were found to be entirely plant-based to heavily animal-based, respectively.[76] In contrast, diseases of affluence like cancer and heart disease are common throughout the developed world, including the United States. Adjusted for age and exercise, large regional clusters of people in China rarely suffered from these "Western" diseases possibly because their diets are rich in vegetables, fruits, and whole grains, and have little dairy and meat products.[76] There are arguments for and against this controversial issue.		The French paradox was an observation from the 1980s that the French suffer a relatively low incidence of coronary heart disease, despite having a diet relatively rich in saturated fats. A number of explanations were suggested:		However, statistics collected by the World Health Organization from 1990–2000 show that the incidence of heart disease in France may have been underestimated and, in fact, may be similar to that of neighboring countries.[81]		Phytochemicals such as polyphenols are compounds produced naturally in plants (phyto means "plant" in Greek). In general, the term is used to refer to those chemicals under research to assess whether they have biological significance. To date, there is no evidence in humans that polyphenols or other non-nutrient compounds from plants have health effects.[82][83]		While initial studies sought to reveal if nutrient antioxidant supplements might promote health, one meta-analysis concluded that supplementation with vitamins A and E and beta-carotene did not convey any benefits and may in fact increase risk of death. Vitamin C and selenium supplements did not impact mortality rate. Health effects of non-nutrient phytochemicals such as polyphenols were not assessed.[84][85]		Animal intestines contain a large population of gut flora. In humans, the four dominant phyla are Firmicutes, Bacteroidetes, Actinobacteria, and Proteobacteria.[86] They are essential to digestion and are also affected by food that is consumed. Bacteria in the large intestine perform many important functions for humans, including breaking down and aiding in the absorption of soluble fiber, stimulating cell growth, repressing the growth of harmful bacteria, training the immune system to respond only to pathogens, producing vitamin B12, and defending against some infectious diseases.[87] "Probiotics" refers to the idea of deliberately consuming live bacteria in an attempt to change the bacterial population in the large intestine, to the health benefit of the host human or animal. "Prebiotic (nutrition)" refers to the idea that consuming a bacterial energy source such as soluble fiber could support the population of health-beneficial bacteria in the large intestine. There is not yet a scientific consensus as to health benefits accruing from probiotics or prebiotics.		Carnivore and herbivore diets are contrasting, with basic nitrogen and carbon proportions vary for their particular foods. Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize.[88]		Plant nutrition is the study of the chemical elements that are necessary for plant growth.[89] There are several principles that apply to plant nutrition. Some elements are directly involved in plant metabolism. However, this principle does not account for the so-called beneficial elements, whose presence, while not required, has clear positive effects on plant growth.		A nutrient that is able to limit plant growth according to Liebig's law of the minimum is considered an essential plant nutrient if the plant cannot complete its full life cycle without it. There are 16 essential plant soil nutrients, besides the three major elemental nutrients carbon and oxygen that are obtained by photosynthetic plants from carbon dioxide in air, and hydrogen, which is obtained from water.		Plants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Green plants obtain their carbohydrate supply from the carbon dioxide in the air by the process of photosynthesis. Carbon and oxygen are absorbed from the air, while other nutrients are absorbed from the soil. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H+) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen. The carbon dioxide molecules are used as the carbon source in photosynthesis.		Although nitrogen is plentiful in the Earth's atmosphere, very few plants can use this directly. Most plants, therefore, require nitrogen compounds to be present in the soil in which they grow. This is made possible by the fact that largely inert atmospheric nitrogen is changed in a nitrogen fixation process to biologically usable forms in the soil by bacteria.[90]		Plant nutrition is a difficult subject to understand completely, partially because of the variation between different plants and even between different species or individuals of a given clone. Elements present at low levels may cause deficiency symptoms, and toxicity is possible at levels that are too high. Furthermore, deficiency of one element may present as symptoms of toxicity from another element, and vice versa.		Research in the field of nutrition has greatly contributed in finding out the essential facts about how environmental depletion can lead to crucial nutrition-related health problems like contamination, spread of contagious diseases, malnutrition, etc. Moreover, environmental contamination due to discharge of agricultural as well as industrial chemicals like organocholrines, heavy metal, and radionucleotides may adversely affect the human and the ecosystem as a whole. As far as safety of the human health is concerned, then these environmental contaminants can reduce people's nutritional status and health. This could directly or indirectly cause drastic changes in their diet habits. Hence, food-based remedial as well as preventive strategies are essential to address global issues like hunger and malnutrition and to enable the susceptible people to adapt themselves to all these environmental as well as socio-economic alterations.[91]		Canada's Food Guide is another example government-run nutrition program. Produced by Health Canada, the guide advises food quantities, provides education on balanced nutrition, and promotes physical activity in accordance with government-mandated nutrient needs. Like other nutrition programs around the world, Canada's Food Guide divides nutrition into four main food groups: vegetables and fruit, grain products, milk and alternatives, and meat and alternatives.[92] It is interesting to note that, unlike its American counterpart, the Canadian guide references and provides alternative to meat and dairy, which can be attributed to the growing vegan and vegetarian movements.		In the US, nutritional standards and recommendations are established jointly by the US Department of Agriculture and US Department of Health and Human Services. Dietary and physical activity guidelines from the USDA are presented in the concept of MyPlate, which superseded the food pyramid, which replaced the Four Food Groups. The Senate committee currently responsible for oversight of the USDA is the Agriculture, Nutrition and Forestry Committee. Committee hearings are often televised on C-SPAN. The U.S. Department of Health and Human Services provides a sample week-long menu that fulfills the nutritional recommendations of the government.[93]		Governmental organisations have been working on nutrition literacy interventions in non-primary health care settings to address the nutrition information problem in the U.S. Some programs include:		The Family Nutrition Program (FNP) is a free nutrition education program serving low-income adults around the U.S. This program is funded by the Food Nutrition Service’s (FNS) branch of the United States Department of Agriculture (USDA) usually through a local state academic institution that runs the program. The FNP has developed a series of tools to help families participating in the Food Stamp Program stretch their food dollar and form healthful eating habits including nutrition education.[citation needed]		Expanded Food and Nutrition Education Program (ENFEP) is a unique program that currently operates in all 50 states and in American Samoa, Guam, Micronesia, Northern Marianas, Puerto Rico, and the Virgin Islands. It is designed to assist limited-resource audiences in acquiring the knowledge, skills, attitudes, and changed behavior necessary for nutritionally sound diets, and to contribute to their personal development and the improvement of the total family diet and nutritional well-being.		An example of a state initiative to promote nutrition literacy is Smart Bodies, a public-private partnership between the state’s largest university system and largest health insurer, Louisiana State Agricultural Center and Blue Cross and Blue Shield of Louisiana Foundation. Launched in 2005, this program promotes lifelong healthful eating patterns and physically active lifestyles for children and their families. It is an interactive educational program designed to help prevent childhood obesity through classroom activities that teach children healthful eating habits and physical exercise.		Nutrition is taught in schools in many countries. In England and Wales, the Personal and Social Education and Food Technology curricula include nutrition, stressing the importance of a balanced diet and teaching how to read nutrition labels on packaging. In many schools, a Nutrition class will fall within the Family and Consumer Science or Health departments. In some American schools, students are required to take a certain number of FCS or Health related classes. Nutrition is offered at many schools, and, if it is not a class of its own, nutrition is included in other FCS or Health classes such as: Life Skills, Independent Living, Single Survival, Freshmen Connection, Health etc. In many Nutrition classes, students learn about the food groups, the food pyramid, Daily Recommended Allowances, calories, vitamins, minerals, malnutrition, physical activity, healthful food choices, portion sizes, and how to live a healthy life.		A 1985, US National Research Council report entitled Nutrition Education in US Medical Schools concluded that nutrition education in medical schools was inadequate.[94] Only 20% of the schools surveyed taught nutrition as a separate, required course. A 2006 survey found that this number had risen to 30%.[95] Membership by physicians in leading professional nutrition societies such as the American Society for Nutrition has generally declined from the 1990s.[96]		In the US, Registered dietitian nutritionists (RDs or RDNs)[97] are health professionals qualified to provide safe, evidence-based dietary advice which includes a review of what is eaten, a thorough review of nutritional health, and a personalized nutritional treatment plan. They also provide preventive and therapeutic programs at work places, schools and similar institutions. Certified Clinical Nutritionists or CCNs, are trained health professionals who also offer dietary advice on the role of nutrition in chronic disease, including possible prevention or remediation by addressing nutritional deficiencies before resorting to drugs.[98] Government regulation especially in terms of licensing, is currently less universal for the CCN than that of RD or RDN. Another advanced Nutrition Professional is a Certified Nutrition Specialist or CNS. These Board Certified Nutritionists typically specialize in obesity and chronic disease. In order to become board certified, potential CNS candidate must pass an examination, much like Registered Dieticians. This exam covers specific domains within the health sphere including; Clinical Intervention and Human Health.[99]		At the time of this entry, we were not able to identify any specific nutrition literacy studies in the U.S. at a national level. However, the findings of the 2003 National Assessment of Adult Literacy (NAAL) provide a basis upon which to frame the nutrition literacy problem in the U.S. NAAL introduced the first ever measure of "the degree to which individuals have the capacity to obtain, process and understand basic health information and services needed to make appropriate health decisions" – an objective of Healthy People 2010[100] and of which nutrition literacy might be considered an important subset. On a scale of below basic, basic, intermediate and proficient, NAAL found 13 percent of adult Americans have proficient health literacy, 44% have intermediate literacy, 29 percent have basic literacy and 14 percent have below basic health literacy. The study found that health literacy increases with education and people living below the level of poverty have lower health literacy than those above it.		Another study examining the health and nutrition literacy status of residents of the lower Mississippi Delta found that 52 percent of participants had a high likelihood of limited literacy skills.[101] While a precise comparison between the NAAL and Delta studies is difficult, primarily because of methodological differences, Zoellner et al. suggest that health literacy rates in the Mississippi Delta region are different from the U.S. general population and that they help establish the scope of the problem of health literacy among adults in the Delta region. For example, only 12 percent of study participants identified the My Pyramid graphic two years after it had been launched by the USDA. The study also found significant relationships between nutrition literacy and income level and nutrition literacy and educational attainment[101] further delineating priorities for the region.		These statistics point to the complexities surrounding the lack of health/nutrition literacy and reveal the degree to which they are embedded in the social structure and interconnected with other problems. Among these problems are the lack of information about food choices, a lack of understanding of nutritional information and its application to individual circumstances, limited or difficult access to healthful foods, and a range of cultural influences and socioeconomic constraints such as low levels of education and high levels of poverty that decrease opportunities for healthful eating and living.		The links between low health literacy and poor health outcomes has been widely documented[102] and there is evidence that some interventions to improve health literacy have produced successful results in the primary care setting. More must be done to further our understanding of nutrition literacy specific interventions in non-primary care settings[101] in order to achieve better health outcomes.		Malnutrition refers to insufficient, excessive, or imbalanced consumption of nutrients by an organism. In developed countries, the diseases of malnutrition are most often associated with nutritional imbalances or excessive consumption. In developing countries, malnutrition is more likely to be caused by poor access to a range of nutritious foods or inadequate knowledge. In Mali the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) and the Aga Khan Foundation, trained women's groups to make equinut, a healthy and nutritional version of the traditional recipe di-dèguè (comprising peanut paste, honey and millet or rice flour). The aim was to boost nutrition and livelihoods by producing a product that women could make and sell, and which would be accepted by the local community because of its local heritage.[103]		Although there are more organisms in the world who are malnourished due to insufficient consumption, increasingly more organisms suffer from excessive over-nutrition; a problem caused by an over abundance of sustenance coupled with the instinctual desire (by animals in particular) to consume all that it can.		Nutritionism is the view that excessive reliance on food science and the study of nutrition can lead to poor nutrition and to ill health. It was originally credited to Gyorgy Scrinis,[104] and was popularized by Michael Pollan. Since nutrients are invisible, policy makers rely on nutrition experts to advise on food choices. Because science has an incomplete understanding of how food affects the human body, Pollan argues, nutritionism can be blamed for many of the health problems relating to diet in the Western World today.[105][106]		The U.S. Food and Nutrition Board sets Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for vitamins and minerals. EARs and RDAs are part of Dietary Reference Intakes.[107] The DRI documents describe nutrient deficiency signs and symptoms.		The U.S. Food and Nutrition Board sets Tolerable Upper Intake Levels (known as ULs) for vitamins and minerals when evidence is sufficient. ULs are set a safe fraction below amounts shown to cause health problems. ULs are part of Dietary Reference Intakes.[108] The European Food Safety Authority also reviews the same safety questions and set its own ULs.[109]		When too much of one or more nutrients is present in the diet to the exclusion of the proper amount of other nutrients, the diet is said to be unbalanced.		Research indicates that improving the awareness of nutritious meal choices and establishing long-term habits of healthy eating have a positive effect on cognitive and spatial memory capacity, with potential to increase a student's ability to process and retain academic information.		Some organizations have begun working with teachers, policymakers, and managed foodservice contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university level institutions. Health and nutrition have been proven to have close links with overall educational success.[110] Currently, less than 10% of American college students report that they eat the recommended five servings of fruit and vegetables daily.[111] Better nutrition has been shown to affect both cognitive and spatial memory performance; a study showed those with higher blood sugar levels performed better on certain memory tests.[112] In another study, those who consumed yogurt performed better on thinking tasks when compared to those that consumed caffeine-free diet soda or confections.[113] Nutritional deficiencies have been shown to have a negative effect on learning behavior in mice as far back as 1951.[114]		There is limited research available that directly links a student's Grade Point Average (G.P.A.) to their overall nutritional health. Additional substantive data is needed to prove that overall intellectual health is closely linked to a person's diet, rather than just another correlation fallacy.		Nutritional supplement treatment may be appropriate for major depression, bipolar disorder, schizophrenia, and obsessive compulsive disorder, the four most common mental disorders in developed countries.[120] Supplements that have been studied most for mood elevation and stabilization include eicosapentaenoic acid and docosahexaenoic acid (each of which an omega-3 fatty acid contained in fish oil but not in flaxseed oil), vitamin B12, folic acid, and inositol.		Cancer is now common in developing countries. According to a study by the International Agency for Research on Cancer, "In the developing world, cancers of the liver, stomach and esophagus were more common, often linked to consumption of carcinogenic preserved foods, such as smoked or salted food, and parasitic infections that attack organs." Lung cancer rates are rising rapidly in poorer nations because of increased use of tobacco. Developed countries "tended to have cancers linked to affluence or a 'Western lifestyle' — cancers of the colon, rectum, breast and prostate — that can be caused by obesity, lack of exercise, diet and age."[121]		Several lines of evidence indicate lifestyle-induced hyperinsulinemia and reduced insulin function (i.e., insulin resistance) as a decisive factor in many disease states. For example, hyperinsulinemia and insulin resistance are strongly linked to chronic inflammation, which in turn is strongly linked to a variety of adverse developments such as arterial microinjuries and clot formation (i.e., heart disease) and exaggerated cell division (i.e., cancer). Hyperinsulinemia and insulin resistance (the so-called metabolic syndrome) are characterized by a combination of abdominal obesity, elevated blood sugar, elevated blood pressure, elevated blood triglycerides, and reduced HDL cholesterol. The negative effect of hyperinsulinemia on prostaglandin PGE1/PGE2 balance may be significant.		The state of obesity clearly contributes to insulin resistance, which in turn can cause type 2 diabetes. Virtually all obese and most type 2 diabetic individuals have marked insulin resistance. Although the association between overweight and insulin resistance is clear, the exact (likely multifarious) causes of insulin resistance remain less clear. It is important to note that it has been demonstrated that appropriate exercise, more regular food intake, and reducing glycemic load (see below) all can reverse insulin resistance in overweight individuals (and thereby lower blood sugar levels in those with type 2 diabetes).		Obesity can unfavourably alter hormonal and metabolic status via resistance to the hormone leptin, and a vicious cycle may occur in which insulin/leptin resistance and obesity aggravate one another. The vicious cycle is putatively fuelled by continuously high insulin/leptin stimulation and fat storage, as a result of high intake of strongly insulin/leptin stimulating foods and energy. Both insulin and leptin normally function as satiety signals to the hypothalamus in the brain; however, insulin/leptin resistance may reduce this signal and therefore allow continued overfeeding despite large body fat stores. In addition, reduced leptin signalling to the brain may reduce leptin's normal effect to maintain an appropriately high metabolic rate.		There is a debate about how and to what extent different dietary factors— such as intake of processed carbohydrates, total protein, fat, and carbohydrate intake, intake of saturated and trans fatty acids, and low intake of vitamins/minerals—contribute to the development of insulin and leptin resistance. In any case, analogous to the way modern man-made pollution may possess the potential to overwhelm the environment's ability to maintain homeostasis, the recent explosive introduction of high glycemic index and processed foods into the human diet may possess the potential to overwhelm the body's ability to maintain homeostasis and health (as evidenced by the metabolic syndrome epidemic).		Excess water intake, without replenishment of sodium and potassium salts, leads to hyponatremia, which can further lead to water intoxication at more dangerous levels. A well-publicized case occurred in 2007, when Jennifer Strange died while participating in a water-drinking contest.[122] More usually, the condition occurs in long-distance endurance events (such as marathon or triathlon competition and training) and causes gradual mental dulling, headache, drowsiness, weakness, and confusion; extreme cases may result in coma, convulsions, and death. The primary damage comes from swelling of the brain, caused by increased osmosis as blood salinity decreases. Fluid replacement techniques which can increase risk of over-hydration include water aid stations during running/cycling races, trainers providing water during team sports practices and games, and devices such as water-containing backpacks with a built-in drinking tube.		Antinutrients are natural or synthetic compounds that interfere with the absorption of nutrients. Nutrition studies focus on antinutrients commonly found in food sources and beverages.		Sugar consumption in the United States		The relatively recent increased consumption of sugar has been linked to the rise of some afflictions such as diabetes, obesity, and more recently heart disease. Increased consumption of sugar has been tied to these three, among others. Obesity levels have more than doubled in the last 30 years among adults, going from 15% to 35% in the United States.[123] Obesity and diet also happen to be high risk factors for diabetes. In the same time span that obesity doubled, diabetes numbers quadrupled in America. Increased weight, especially in the form of belly fat, and high sugar intake are also high risk factors for heart disease.[124] Both sugar intake and fatty tissue increase the probability of elevated LDL cholesterol in the bloodstream. Elevated amounts of Low-density lipoprotein (LDL) cholesterol, is the primary factor in heart disease. In order to avoid all the dangers of sugar, moderate consumption is paramount.		Since the Industrial Revolution some two hundred years ago, the food processing industry has invented many technologies that both help keep foods fresh longer and alter the fresh state of food as they appear in nature. Cooling is the primary technology used to maintain freshness, whereas many more technologies have been invented to allow foods to last longer without becoming spoiled. These latter technologies include pasteurisation, autoclavation, drying, salting, and separation of various components, all of which appearing to alter the original nutritional contents of food. Pasteurisation and autoclavation (heating techniques) have no doubt improved the safety of many common foods, preventing epidemics of bacterial infection. But some of the (new) food processing technologies have downfalls as well.		Modern separation techniques such as milling, centrifugation, and pressing have enabled concentration of particular components of food, yielding flour, oils, juices, and so on, and even separate fatty acids, amino acids, vitamins, and minerals. Inevitably, such large-scale concentration changes the nutritional content of food, saving certain nutrients while removing others. Heating techniques may also reduce food's content of many heat-labile nutrients such as certain vitamins and phytochemicals, and possibly other yet-to-be-discovered substances.[125] Because of reduced nutritional value, processed foods are often 'enriched' or 'fortified' with some of the most critical nutrients (usually certain vitamins) that were lost during processing. Nonetheless, processed foods tend to have an inferior nutritional profile compared to whole, fresh foods, regarding content of both sugar and high GI starches, potassium/sodium, vitamins, fiber, and of intact, unoxidized (essential) fatty acids. In addition, processed foods often contain potentially harmful substances such as oxidized fats and trans fatty acids.		A dramatic example of the effect of food processing on a population's health is the history of epidemics of beri-beri in people subsisting on polished rice. Removing the outer layer of rice by polishing it removes with it the essential vitamin thiamine, causing beri-beri. Another example is the development of scurvy among infants in the late 19th century in the United States. It turned out that the vast majority of sufferers were being fed milk that had been heat-treated (as suggested by Pasteur) to control bacterial disease. Pasteurisation was effective against bacteria, but it destroyed the vitamin C.		As mentioned, lifestyle- and obesity-related diseases are becoming increasingly prevalent all around the world. There is little doubt that the increasingly widespread application of some modern food processing technologies has contributed to this development. The food processing industry is a major part of modern economy, and as such it is influential in political decisions (e.g., nutritional recommendations, agricultural subsidising). In any known profit-driven economy, health considerations are hardly a priority; effective production of cheap foods with a long shelf-life is more the trend. In general, whole, fresh foods have a relatively short shelf-life and are less profitable to produce and sell than are more processed foods. Thus, the consumer is left with the choice between more expensive, but nutritionally superior, whole, fresh foods, and cheap, usually nutritionally inferior, processed foods. Because processed foods are often cheaper, more convenient (in both purchasing, storage, and preparation), and more available, the consumption of nutritionally inferior foods has been increasing throughout the world along with many nutrition-related health complications.		Biology:		Dangers of poor nutrition		Food:		Healthy diet:		Lists:		Nutrients:		Profession:		Tools:		Organizations:		Related topics		
Stretching is a form of physical exercise in which a specific muscle or tendon (or muscle group) is deliberately flexed or stretched in order to improve the muscle's felt elasticity and achieve comfortable muscle tone.[1] The result is a feeling of increased muscle control, flexibility, and range of motion. Stretching is also used therapeutically to alleviate cramps.[2]		In its most basic form, stretching is a natural and instinctive activity; it is performed by humans and many other animals. It can be accompanied by yawning. Stretching often occurs instinctively after waking from sleep, after long periods of inactivity, or after exiting confined spaces and areas.		Increasing flexibility through stretching is one of the basic tenets of physical fitness. It is common for athletes to stretch before and after exercise in an attempt to reduce risk of injury and increase performance,[3] though these practices are not always based on scientific evidence of effectiveness.		Stretching can be dangerous when performed incorrectly. There are many techniques for stretching in general, but depending on which muscle group is being stretched, some techniques may be ineffective or detrimental, even to the point of causing tears, hypermobility, instability, or permanent damage to the tendons, ligaments, and muscle fiber.[4] The physiological nature of stretching and theories about the effect of various techniques are therefore subject to heavy inquiry.		Although stretching is part of some warm-up routines, a study in 2013 indicates that it weakens muscles in that situation.[5]						Studies have shed light on the function, in stretching, of a large protein within the myofibrils of skeletal muscles named titin.[6] A study performed by Magid and Law demonstrated that the origin of passive muscle tension (which occurs during stretching) is actually within the myofibrils, not extracellularly as had previously been supposed.[7] Due to neurological safeguards against injury, it is normally impossible for adults to stretch most muscle groups to their fullest length without training due to the activation of muscle antagonists as the muscle reaches the limit of its normal range of motion.[4]		There are five different types of stretching: ballistic, dynamic, SMF stretching, PNF stretching, and static stretching. Ballistic stretching is a rapid bouncing stretch in which a body part is moving with momentum that stretches the muscles to a maximum. Muscles respond to this type of stretching by contracting to protect itself from over extending. Dynamic stretching is a walking or movement stretch. By performing slow controlled movements through full range of motion, a person reduces risk of injury. Proprioceptive neuromuscular facilitation (PNF) is a type of stretch for a particular muscle and its specific job, so resistance should be applied, then the muscle should be relaxed. Static stretching is a type of stretch whereby a person stretches the muscle until a gentle tension is felt and then holds the stretch for thirty seconds or until a muscle release is felt, without any movement or bouncing.[3]		Although many people engage in stretching before or after exercise, the medical evidence has shown this has no meaningful benefit in preventing muscle soreness.[8]		Stretching does not appear to reduce the risk of injury during exercise, except perhaps for runners.[9] There is some evidence that pre-exercise stretching may increase athletes' range of movement.[9][10]		Some people are more flexible than others as defined by individual body flexibility score; this includes sex differences where females are generally more flexible than males.[medical citation needed]		|group2 = See also |list2 =		|below = }}		
The American Cancer Society (ACS) is a nationwide voluntary health organization dedicated to eliminating cancer. Established in 1913, the society is organized into eleven geographical divisions of both medical and lay volunteers operating in more than 900 offices throughout the United States.[2][3] Its home office is located in the American Cancer Society Center in Atlanta, Georgia. The ACS publishes the journals Cancer, CA: A Cancer Journal for Clinicians and Cancer Cytopathology.[4]						The society was founded on May 23, 1913 by 15 physicians and businessmen in New York City under the name American Society for the Control of Cancer (ASCC). The current name was adopted in 1944.[3][5] According to Charity Navigator the ACS is one of the oldest and largest volunteer health organizations.[3]		At the time of founding, it was not considered appropriate to mention the word ‘cancer’ in public. Information concerning this illness was cloaked in a climate of fear and denial. Over 75,000 people died each year of cancer in just the United States. The top item on the founders’ agenda was to raise awareness of cancer, before any other progress could be made in funding research. Therefore, a frenetic writing campaign was undertaken to educate doctors, nurses, patients and family members about cancer. Articles were written for popular magazines and professional journals. The ASCC undertook to publish their own journal, Campaign Notes, which was a monthly bulletin with information about cancer. They began recruiting doctors from all over the United States to help educate the public about cancer.		In 1936, Marjorie Illig, an ASCC field representative, suggested the creation of a network consisting of new volunteers for the purpose of waging “war on cancer.” From 1935 to 1938 the number of people involved in cancer control in the US grew from 15,000 to 150,000. According to Working to Give, The Women’s Field Army, a group of volunteers working for the ASCC was primarily responsible for this increase.[6]		The sword symbol, adopted by the American Cancer Society in 1928, was designed by George E. Durant of Brooklyn, New York. According to Durant, the two serpents forming the handle represent the scientific and medical focus of the society’s mission and the blade expresses the “crusading spirit of the cancer control movement."[7]		In 2013 the American Cancer Society embarked on a nationwide reorganization. The organization centralized its operations and consolidated, merging previous regional affiliates into the parent organization. It also required all employees to reapply for their jobs.[8][9]		Its activities include providing grants to researchers, including funding 47 Nobel Laureate researchers, discovering the link between smoking and cancer, and serving one million callers every year through its National Cancer Information Center. The 47 Nobel Prize laureates include James D. Watson, Mario Capecchi, Oliver Smithies, Paul Berg, E. Donnall Thomas, and Walter Gilbert.[10] The American Cancer Society's website contained a chronological listing of specific accomplishments in the fight against cancer, for example the unipod technological device of UTD, that the ACS had a hand in, including the funding of various scientists who went on to discover life-saving cancer treatments, and advocating for increased use of preventative techniques.[11] More than two million people volunteer with the ACS which has over 3,400 local offices.[3]		It also runs public health advertising campaigns, and organizes projects such as the Relay For Life and the Great American Smokeout. It operates a series of thrift stores to raise money for its operations. The ACS participates in the Hopkins 4K for Cancer, a 4000-mile bike ride from Baltimore to San Francisco to raise money for the society's Hope Lodge.[12][13]		The society’s allocation of funds for the fiscal year ending December 31, 2015, lists 75% of funds for Program Services (Patient Support 37%, Research 16%, Prevention 13.1%, Detection and Treatment 9.2%). The remaining 25% are allocated for supporting services (Fundraising 19.1%, and Management, General administration 5.5%).[14] This meets the Better Business Bureau's Standards for Charity Accountability: Standard 8 (Program Service Expense Ratio) of at least 65% of total expenses spent on program activities.[15]		In 2012 the American Cancer Society raised $934 million and spent $943 million prompting a national consolidation and cost-cutting reorganization.[8]		John R. Seffrin, former CEO of the American Cancer Society, received $2,401,112 salary/compensation from the charity for the 2009-2010 fiscal year.[15] This is the second most money given by any charity to the head of that charity, according to Charity Watch. The money included $1.5 million in a retention benefit approved in 2001, “to preserve management stability.”[16] Mr. Seffrin's compensation for the fiscal year ending August 31, 2012 was $832,355.[17]		In 1994, the Chronicle of Philanthropy, a nonprofit industry publication, released the results of the largest study of charitable and non-profit organization popularity and credibility conducted by Nye Lavalle & Associates. The study showed that the American Cancer Society was ranked as the 10th "most popular charity/non-profit in America" of over 100 charities researched with 38% of Americans over the age of 12 choosing "love" and "like a lot" for the American Cancer Society.[6][18] [19]		The Better Business Bureau lists American Cancer Society as an accredited charity meeting all of its Standards for Charity Accountability as of January 2012.[15] Charity Navigator rates the society two of four stars for fiscal year 2011.[20] According to Charity Navigator the society is directed to "eliminating cancer" and destroying it.[3] Charity Watch rates American Cancer Society a "C", stating that the Society devotes 40% of its annual expenditures to administration, fundraising, etc., with the other 60% going to fund programs.[16]		In 1995, the Arizona chapter of the American Cancer Society was targeted for its extremely high overhead. Two economists, James Bennett and Thomas DiLorenzo, issued a report analyzing the chapter's financial statements and demonstrating that the Arizona chapter used about 95% of its donations for paying salaries and other overhead costs, resulting in a 22 to 1 ratio of overhead to actual money spent on the cause. The report also asserted that the Arizona chapter's annual report had grossly misrepresented the amount of money spent on patient services, inflating it by more than a factor of 10. The American Cancer Society responded by alleging that the two economists issuing the report were working for a group funded by the tobacco industry.[21]		The American Cancer Society was criticized in 2011 for turning down participation from the Foundation Beyond Belief in its Relay For Life "National Team" program.[22][23]				
An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSN are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2]		The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975.[3] ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.		When a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN), respectively.[citation needed] Conversely, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.[4]						The format of the ISSN is an eight digit code, divided by a hyphen into two four-digit numbers.[1] As an integer number, it can be represented by the first seven digits.[5] The last code digit, which may be 0-9 or an X, is a check digit. Formally, the general form of the ISSN code (also named "ISSN structure" or "ISSN syntax") can be expressed as follows:[6]		or by a PCRE regular expression:[7]		The ISSN of the journal Hearing Research, for example, is 0378-5955, where the final 5 is the check digit, that is C=5. To calculate the check digit, the following algorithm may be used:		To confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by its position in the number, counting from the right (if the check digit is X, then add 10 to the sum). The modulus 11 of the sum must be 0.		There is an online ISSN checker that can validate an ISSN, based on the above algorithm.[8][9]		ISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris. The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government. The International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System) otherwise known as the ISSN Register. At the end of 2016[update], the ISSN Register contained records for 1,943,572 items.[10]		ISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books. An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location. For this reason a new ISSN is assigned to a serial each time it undergoes a major title change.		Since the ISSN applies to an entire serial a new identifier, the Serial Item and Contribution Identifier (SICI), was built on top of it to allow references to specific volumes, articles, or other identifiable components (like the table of contents).		Separate ISSNs are needed for serials in different media (except reproduction microforms). Thus, the print and electronic media versions of a serial need separate ISSNs.[11] Also, a CD-ROM version and a web version of a serial require different ISSNs since two different media are involved. However, the same ISSN can be used for different file formats (e.g. PDF and HTML) of the same online serial.		This "media-oriented identification" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content, independent of media. This "content-oriented identification" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), as ISSN-independent initiative, consolidated in the 2000s.		Only later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an "ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media".[12]		The ISSN Register is not freely available for interrogation on the web, but is available by subscription. There are several routes to the identification and verification of ISSN codes for the public:		An ISSN can be encoded as a uniform resource name (URN) by prefixing it with "urn:ISSN:".[13] For example, Rail could be referred to as "urn:ISSN:0953-4563". URN namespaces are case-sensitive, and the ISSN namespace is all caps.[14] If the checksum digit is "X" then it is always encoded in uppercase in a URN.		The util URNs are content-oriented, but ISSN is media-oriented:		A unique URN for serials simplifies the search, recovery and delivery of data for various services including, in particular, search systems and knowledge databases.[12] ISSN-L (see Linking ISSN below) was created to fill this gap.		There are two most popular media types that adopted special labels (indicating below in italics), and one in fact ISSN-variant, with also an optional label. All are used in standard metadata context like JATS, and the labels also, frequently, as abbreviations.		p-ISSN is a standard label for "Print ISSN", the ISSN for the print media (paper) version of a serial. Usually it is the "default media", so the "default ISSN".		e-ISSN (or eISSN) is a standard label for "Electronic ISSN", the ISSN for the electronic media (online) version of a serial.		ISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007, the "linking ISSN (ISSN-L)" provides a mechanism for collocation or linking among the different media versions of the same continuing resource.		The ISSN-L is one ISSN number among the existing ISSNs, so, does not change the use or assignment of "ordinary" ISSNs;[16] it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L.		With ISSN-L is possible to designate one single ISSN for all those media versions of the title. The use of ISSN-L facilitates search, retrieval and delivery across all media versions for services like OpenURL, library catalogues, search engines or knowledge bases.[17]		
The International Standard Book Number (ISBN) is a unique[a][b] numeric commercial book identifier.		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.		The initial ISBN configuration of recognition was generated in 1967 based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten digit ISBN by prefixing it with a zero).		Occasionally, a book may appear without a printed ISBN if it is printed privately or the author does not follow the usual ISBN procedure; however, this can be rectified later.[1]		Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.						The Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin,[2] for the booksellers and stationers WHSmith and others in 1965.[3] The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker[4] (regarded as the "Father of the ISBN"[5]) and in 1968 in the US by Emery Koltay[4] (who later became director of the U.S. ISBN agency R.R. Bowker).[5][6][7]		The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108.[3][4] The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.[8]		An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has "SBN 340 01381 8" – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated.		Since 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with "Bookland" European Article Number EAN-13s.[9]		An ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN.[10] The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10 digit ISBN) or 5 parts (for a 13 digit ISBN):		A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN number is complicated, because most of the parts do not use a fixed number of digits.[13]		ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. In Canada, ISBNs are issued at no cost with the stated purpose of encouraging Canadian culture.[14] In the United Kingdom, United States, and some other countries, where the service is provided by non-government-funded organisations, the issuing of ISBNs requires payment of a fee.		Australia: ISBNs are issued by the commercial library services agency Thorpe-Bowker,[15] and prices range from $42 for a single ISBN (plus a $55 registration fee for new publishers) to $2,890 for a block of 1,000 ISBNs. Access is immediate when requested via their website.[16]		Brazil: National Library of Brazil, a government agency, is responsible for issuing ISBNs, and there is a cost of R$16 [17]		Canada: Library and Archives Canada, a government agency, is responsible for issuing ISBNs, and there is no cost. Works in French are issued an ISBN by the Bibliothèque et Archives nationales du Québec.		Colombia: Cámara Colombiana del Libro, a NGO, is responsible for issuing ISBNs. Cost of issuing an ISBN is about USD 20.		Hong Kong: The Books Registration Office (BRO), under the Hong Kong Public Libraries, issues ISBNs in Hong Kong. There is no fee.[18]		India: The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development, is responsible for registration of Indian publishers, authors, universities, institutions, and government departments that are responsible for publishing books.[19] There is no fee associated in getting ISBN in India.[20]		Italy: The privately held company EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association) is responsible for issuing ISBNs.[21] The original national prefix 978-88 is reserved for publishing companies, starting at €49 for a ten-codes block[22] while a new prefix 979-12 is dedicated to self-publishing authors, at a fixed price of €25 for a single code.		Maldives: The National Bureau of Classification (NBC) is responsible for ISBN registrations for publishers who are publishing in the Maldives.[citation needed]		Malta: The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb) issues ISBN registrations in Malta.[23][24][25]		Morocco: The National Library of Morocco is responsible for ISBN registrations for publishing in Morocco and Moroccan-occupied portion of Western Sahara.		New Zealand: The National Library of New Zealand is responsible for ISBN registrations for publishers who are publishing in New Zealand.[26]		Pakistan: The National Library of Pakistan is responsible for ISBN registrations for Pakistani publishers, authors, universities, institutions, and government departments that are responsible for publishing books.		South Africa: The National Library of South Africa is responsible for ISBN issuance for South African publishing institutions and authors.		United Kingdom and Republic of Ireland: The privately held company Nielsen Book Services Ltd, part of Nielsen Holdings N.V., is responsible for issuing ISBNs in blocks of 10, 100 or 1000. Prices start from £120 (plus VAT) for the smallest block on a standard turnaround of ten days.[27]		United States: In the United States, the privately held company R.R. Bowker issues ISBNs.[4] There is a charge that varies depending upon the number of ISBNs purchased, with prices starting at $125.00 for a single number. Access is immediate when requested via their website.[28]		Publishers and authors in other countries obtain ISBNs from their respective national ISBN registration agency. A directory of ISBN agencies is available on the International ISBN Agency website.		The registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979).[11] Registration group identifiers have primarily been allocated within the 978 prefix element.[29] The single-digit group identifiers within the 978 prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–621, 7, 80–94, 950–989, 9926–9989, and 99901–99976.[30] Books published in rare languages typically have longer group identifiers.[31]		Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN.[11] The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.[32]		The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.		The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.[citation needed]		A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes.[33] Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.		Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.		By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements.[34] Here are some sample ISBN-10 codes, illustrating block length variations.		English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:[35]		A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with "0" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen digit codes is not compatible and will, in general, give a different check digit from the corresponding 10 digit ISBN, and does not provide the same protection against transposition. This is because the thirteen digit code was required to be compatible with the EAN format, and hence could not contain an "X".		The 2001 edition of the official manual of the International ISBN Agency says that the ISBN-10 check digit[36] – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.		For example, for an ISBN-10 of 0-306-40615-2:		Formally, using modular arithmetic, we can say:		It is also true for ISBN-10's that the sum of all the ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:		Formally, we can say:		The two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proved that all possible valid ISBN-10's have at least two digits different from each other. It can also be proved that there are no pairs of valid ISBN-10's with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e. if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.[37]		In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN number (although it is still unlikely).		Modular arithmetic is convenient for calculating the check digit using modulus 11. Each of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by a number in a sequence from 10 to 2, and the remainder of the sum, with respect to 11, is computed. The resulting remainder, plus the check digit, must equal a multiple of 11 (either 0 or 11). Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the first "modulo 11" is unneeded, but it may be considered to simplify the calculation.)		For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:		Thus the check digit is 2, and the complete sequence is ISBN 0-306-40615-2. The value x 10 {\displaystyle x_{10}} required to satisfy this condition might be 10; if so, an 'X' should be used.		It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:		The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.		The 2005 edition of the International ISBN Agency's official manual[38] describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.		Formally, using modular arithmetic, we can say:		The calculation of an ISBN-13 check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.		For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:		Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.		In general, the ISBN-13 check digit is calculated as follows.		Let		Then		This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.		Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).		The conversion is quite simple as one only needs to prefix "978" to the existing number and calculate the new checksum using the ISBN-13 algorithm.		Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers.[39] For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden® : a novel based on the best-selling game by Tecmo (1990) and Wacky Laws (1997), both published by Scholastic.		Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN".[40] However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine.[citation needed] OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.		Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.[41]		Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits for the currency and the recommended retail price.[42] For 10 digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternate digits).		Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (ISBN-13). The process began 1 January 2005 and was planned to conclude 1 January 2007.[43] As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. 10 digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.		Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.[44]		Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.		
Physical therapy (PT), mostly known as Physiotherapy, is a primary care specialty in western medicine that, by using mechanical force and movements [Bio-mechanics or Kinesiology], Manual therapy, exercise therapy, electrotherapy and various physical therapies who practice evidence based treatments, remediates impairments and promotes mobility, function, and quality of life through examination, diagnosis, prognosis, and physical intervention. It is performed by physical therapists (known as physiotherapists in many countries).		In addition to clinical practice, other activities encompassed in the physical therapy profession include research, education, consultation, and administration. Physical therapy services may be provided as primary care treatment or alongside, or in conjunction with, other medical services.		Physical therapy attempts to address the illnesses, or injuries that limit a person's abilities to move and perform functional activities in their daily lives.[1] PTs use an individual's history and physical examination to arrive at a diagnosis and establish a management plan and, when necessary, incorporate the results of laboratory and imaging studies like X-rays, CT-scan, or MRI findings. Electrodiagnostic testing (e.g., electromyograms and nerve conduction velocity testing) may also be used.[2] PT management commonly includes prescription of or assistance with specific exercises, manual therapy and manipulation, mechanical devices such as traction, education, physical agents which includes heat, cold, electricity, sound waves, radiation, assistive devices, prostheses, orthoses and other interventions. In addition, PTs work with individuals to prevent the loss of mobility before it occurs by developing fitness and wellness-oriented programs for healthier and more active lifestyles, providing services to individuals and populations to develop, maintain and restore maximum movement and functional ability throughout the lifespan. This includes providing therapeutic treatment in circumstances where movement and function are threatened by aging, injury, disease or environmental factors. Functional movement is central to what it means to be healthy.		Physical therapy is a professional career which has many specialties including musculoskeletal, sports, neurology, wound care, EMG, cardiopulmonary, geriatrics, orthopedics, women's health, and pediatrics. Neurological rehabilitation is in particular a rapidly emerging field. PTs practice in many settings, such as private-owned physical therapy clinics, outpatient clinics or offices, health and wellness clinics, rehabilitation hospitals facilities, skilled nursing facilities, extended care facilities, private homes, education and research centers, schools, hospices, industrial and this workplaces or other occupational environments, fitness centers and sports training facilities.[3]		Physical therapists also practise in the non-patient care roles such as health policy,[4][5][6][7] health insurance, health care administration and as health care executives.[8][9] Physical therapists are involved in the medical-legal field serving as experts, performing peer review and independent medical examinations.		Education varies greatly by country. The span of education ranges from some countries having little formal education to others having doctoral degrees and post doctoral residencies and fellowships.		Physicians like Hippocrates and later Galen are believed to have been the first practitioners of physical therapy, advocating massage, manual therapy techniques and hydrotherapy to treat people in 460 BC.[10] After the development of orthopedics in the eighteenth century, machines like the Gymnasticon were developed to treat gout and similar diseases by systematic exercise of the joints, similar to later developments in physical therapy.[11]		The earliest documented origins of actual physical therapy as a professional group date back to Per Henrik Ling, "Father of Swedish Gymnastics," who founded the Royal Central Institute of Gymnastics (RCIG) in 1813 for manipulation, and exercise. The Swedish word for physical therapist is sjukgymnast = someone involved in gymnastics for those who are ill. In 1887, PTs were given official registration by Sweden's National Board of Health and Welfare. Other countries soon followed. In 1894, four nurses in Great Britain formed the Chartered Society of Physiotherapy.[12] The School of Physiotherapy at the University of Otago in New Zealand in 1913,[13] and the United States' 1914 Reed College in Portland, Oregon, which graduated "reconstruction aides."[14] Since the profession's inception, spinal manipulative therapy has been a component of the physical therapist practice.[15]		Modern physical therapy was established towards the end of the 19th century due to events that had an effect on a global scale, which called for rapid advances in physical therapy. Soon following American orthopedic surgeons began treating children with disabilities and began employing women trained in physical education, and remedial exercise. These treatments were applied and promoted further during the Polio outbreak of 1916. During the First World War women were recruited to work with and restore physical function to injured soldiers, and the field of physical therapy was institutionalized. In 1918 the term "Reconstruction Aide" was used to refer to individuals practicing physical therapy. The first school of physical therapy was established at Walter Reed Army Hospital in Washington, D.C., following the outbreak of World War I.[16] Research catalyzed the physical therapy movement. The first physical therapy research was published in the United States in March 1921 in "The PT Review." In the same year, Mary McMillan organized the American Women's Physical Therapeutic Association (now called the American Physical Therapy Association (APTA). In 1924, the Georgia Warm Springs Foundation promoted the field by touting physical therapy as a treatment for polio.[17] Treatment through the 1940s primarily consisted of exercise, massage, and traction. Manipulative procedures to the spine and extremity joints began to be practiced, especially in the British Commonwealth countries, in the early 1950s.[18][19] Around this time when polio vaccines were developed, physical therapists have become a normal occurrence in hospitals throughout North America and Europe.[20] In the late 1950s, physical therapists started to move beyond hospital-based practice to outpatient orthopedic clinics, public schools, colleges/universities health-centres, geriatric settings (skilled nursing facilities), rehabilitation centers and medical centers. Specialization for physical therapy in the U.S. occurred in 1974, with the Orthopaedic Section of the APTA being formed for those physical therapists specializing in orthopaedics. In the same year, the International Federation of Orthopaedic Manipulative Physical Therapists was formed,[21] which has ever since played an important role in advancing manual therapy worldwide.		Educational criteria for physical therapy providers vary from state to state and from country to country, and among various levels of professional responsibility. Most U.S. states have physical therapy practice acts that recognize both physical therapists (PT) and physical therapist assistants (PTA) and some jurisdictions also recognize physical therapy technicians (PT Techs) or aides. Most countries have licensing bodies that require physical therapists to be a member of before they can start practicing as independent professionals.		Canadian Physiotherapy programs are offered at 15 Universities, often through the university's respective college of medicine. In the past decade, each of Canada's physical therapy schools has transitioned from 3-year Bachelor of Science in Physical Therapy (BScPT) programs that required 2 years of pre-requisites university courses (5-year bachelor's degree) to 2-year Master's of Physical Therapy (MPT) programs that require pre-requisite bachelor's degrees. The last Canadian university to follow suit was the University of Manitoba who transitioned to the MPT program in 2012, making the MPT credential the new entry to practice standard across Canada. Existing practitioners with BScPT credentials are not required to upgrade their qualifications.		In the province of Quebec, prospective physiotherapists are required to have completed a college diploma in either health sciences, which lasts on average two years, or physical rehabilitation technology, which lasts at least three years, to apply to a physiotherapy program or program in university. Following admission, physical therapy students work on a bachelor of science with a major in physical therapy and rehabilitation. The B.Sc. usually requires three years to complete. Students must then enter graduate school to complete a master's degree in physical therapy, which normally requires one and a half to two years of study. Graduates who obtain their M.Sc. must successfully pass the membership examination to become member of the Ordre professionnel de la physiothérapie du Québec (OPPQ). Physiotherapists can pursue their education in such fields as rehabilitation sciences, sports medicine, kinesiology, and physiology.		In the province of Quebec, physical rehabilitation therapists are health care professionals who are required to complete a three-year college diploma program in physical rehabilitation therapy and be member of the Ordre professionnel de la physiothérapie du Québec (OPPQ) in order to practise legally in the country.		Most physical rehabilitation therapists complete their college diploma at Collège Montmorency, Dawson College, or Cégep Marie-Victorin, all situated in and around the Montreal area.		After completing their technical college diploma, graduates have the opportunity to pursue their studies at the university level to perhaps obtain a bachelor's degree in physiotherapy, kinesiology, exercise science, or occupational therapy. The Université de Montréal, the Université Laval and the Université de Sherbrooke are among the Québécois universities that admit physical rehabilitation therapists in their programs of study related to health sciences and rehabilitation in order to credit courses that were completed in college.		To date, there are no bridging programs available to facilitate upgrading from the BScPT to the MPT credential. However, research Master's of Science (MSc) and Doctor of Philosophy (PhD) programs are available at every university. Aside from academic research, practitioners can upgrade their skills and qualifications through continuing education courses and curriculums. Continuing education is a requirement of the provincial regulatory bodies.		The Canadian Alliance of Physiotherapy Regulators (CAPR), or simply known as The Alliance, offers eligible program graduates to apply for the national Physiotherapy Competency Examination (PCE). Passing the PCE is one of the requirements in most provinces and territories to work as a licensed physiotherapist in Canada.[22] The Alliance has members which are physiotherapy regulatory organizations recognized in their respective provinces and territories:		The Canadian Physiotherapy Association offers a curriculum of continuing education courses in orthopaedics and manual therapy. The program consists of 5 levels (7 courses) of training with ongoing mentorship and evaluation at each level. The orthopaedic curriculum and examinations takes a minimum of 4 years to complete. However, upon completion of level 2, physiotherapists can apply to a unique 1-year course-based Master's program in advanced orthopaedics and manipulation at the University of Western Ontario to complete their training. This program accepts only 16 physiotherapists annually since 2007. Successful completion of either of these education streams and their respective examinations allows physiotherapists the opportunity to apply to the Canadian Academy of Manipulative Physiotherapy (CAMPT) for fellowship. Fellows of the Canadian Academy of manipulative Physiotherapists (FCAMPT) are considered leaders in the field, having extensive post-graduate education in orthopaedics and manual therapy. FCAMPT is an internationally recognized credential, as CAMPT is a member of the International Federation of Manipulative Physiotherapists (IFOMPT), a branch of the World Confederation of Physical Therapy (WCPT) and the World Health Organization (WHO).		Physiotherapy degrees are offered at three universities: Robert Gordon University in Aberdeen, Glasgow Caledonian University in Glasgow and Queen Margaret University in Edinburgh. Students can qualify as physiotherapists by completing a four-year Bachelor of Science degree or a two-year master's degree (if they already have an undergraduate degree in a related field).		In order to use the title 'Physiotherapist', a student must register with the Health and Care Professions Council, a UK wide regulatory body, on qualifying. Many physiotherapists are also members of the Chartered Society of Physiotherapists (CSP), who provides insurance and professional support.		The primary physical therapy practitioner is the Physical Therapist (PT) who is trained and licensed to examine, evaluate, diagnose and treat impairment, functional limitations and disabilities in patients or clients. Physical therapist education curricula in the United States culminate in a Doctor of Physical Therapy (DPT) degree,[24] but many currently practising PTs hold a Master of Physical Therapy degree, and some still hold a Bachelor's degree. Currently the education programs for physical therapy have changed. The Master of Physical Therapy and Master of Science in Physical Therapy degrees are no longer offered, and the entry-level degree is the Doctor of Physical Therapy degree, which typically takes 3 years after completing bachelor's degree.[25] PTs who hold a Masters or bachelors in PT are encouraged to get their DPT because APTA's goal is for all PT's to be on a doctoral level.[26] WCPT recommends physical therapist entry-level educational programs be based on university or university-level studies, of a minimum of four years, independently validated and accredited.[27] Curricula in the United States are accredited by the Commission on Accreditation in Physical Therapy Education (CAPTE). According to CAPTE, as of 2017 there are 31,380 students currently enrolled in 227 accredited PT programs in the United States while 12,945 PTA students are currently enrolled in 331 PTA programs in the United States.[28] (Updated CAPTE statistics list that for 2015–2016, there were 30,419 students enrolled in 233 accredited PT programs in the United States.)[29]		The physical therapist professional curriculum includes content in the clinical sciences (e.g., content about the cardiovascular, pulmonary, endocrine, metabolic, gastrointestinal, genitourinary, integumentary, musculoskeletal, and neuromuscular systems and the medical and surgical conditions frequently seen by physical therapists). Current training is specifically aimed to enable physical therapists to appropriately recognize and refer non-musculoskeletal diagnoses that may presently similarly to those caused by systems not appropriate for physical therapy intervention, which has resulted in direct access to physical therapists in many states.[30]		Post-doctoral residency and fellowship education prevalence is increasing steadily with 219 residency, and 42 fellowship programs accredited in 2016. Residencies are aimed to train physical therapists in a specialty such as acute care, cardiovascular & pulmonary, clinical electrophysiology, faculty[disambiguation needed], geriatrics, neurology, orthopaedics, pediatrics, sports, women's health, and wound care, whereas fellowships train specialists in a subspecialty (e.g. critical care, hand therapy, and division 1 sports), similar to the medical model. Residency programs offer eligibility to sit for the specialist certification in their respective area of practice. For example, completion of an orthopaedic physical therapy residency, allows its graduates to apply and sit for the clinical specialist examination in orthopaedics, achieving the OCS designation upon passing the examination.[31] Board certification of physical therapy specialists is aimed to recognize individuals with advanced clinical knowledge and skill training in their respective area of practice, and exemplifies the trend toward greater education to optimally treat individuals with movement dysfunction.[32]		Physical therapist assistants may deliver treatment and physical interventions for patients and clients under a care plan established by and under the supervision of a physical therapist. Physical therapist assistants in the United States are currently trained under associate of applied sciences curricula specific to the profession, as outlined and accredited by CAPTE. As of August 2011, there were 276 accredited two-year (Associate degree) programs for physical therapist assistants In the United States of America.[33] According to CAPTE, as of 2012 there are 10,598 students currently enrolled in 280 accredited PTA programs in the United States.[28] Updated CAPTE statistics list that for 2015–2016, there are 12,726 students enrolled in 340 accredited PTA programs in the United States.[29]		Curricula for the physical therapist assistant associate degree include:[34]		Job duties and education requirements for Physical Therapy Technicians or Aides may vary depending on the employer, but education requirements range from high school diploma or equivalent to completion of a 2-year degree program.[35] O-Net reports that 64% of PT Aides/Techs have a high school diploma or equivalent, 21% have completed some college but do not hold a degree, and 10% hold an associate degree.[36]		Some jurisdictions allow physical therapists to employ technicians or aides or therapy assistants to perform designated routine tasks related to physical therapy under the direct supervision of a physical therapist. Some jurisdictions require physical therapy technicians or aides to be certified, and education and certification requirements vary among jurisdictions.		Physical therapy-related jobs in North America have shown rapid growth in recent years, but employment rates and average wages may vary significantly between different countries, states, provinces or regions. A study from 2013 states that 56.4% of physical therapists were globally satisfied with their jobs.[37] Salary, interest in work, and fulfillment in job are important predictors of job satisfaction.[38] In a Polish study, job burnout among the physical therapists was manifested by increased emotional exhaustion and decreased sense of personal achievement.[39] Emotional exhaustion is significantly higher among physical therapists working with adults and employed in hospitals. Other factors that increased burnout include working in a hospital setting and having seniority from 15 to 19 years.[39]		According to the United States Department of Labor's Bureau of Labor Statistics, there were approximately 210,900 physical therapists employed in the United States in 2014, earning an average $84,020 annually in 2015, or $40.40 per hour, with 34% growth in employment projected by the year 2024.[40] The Bureau of Labor Statistics also reports that there were approximately 128,700 Physical Therapist Assistants and Aides employed in the United States in 2014, earning an average $42,980 annually, or $20.66 per hour, with 40% growth in employment projected by the year 2024. To meet their needs, many healthcare and physical therapy facilities hire "travel physical therapists", who work temporary assignments between 8 and 26 weeks for much higher wages; about $113,500 a year.[41] Bureau of Labor Statistics data on PTAs and Techs can be difficult to decipher, due to their tendency to report data on these job fields collectively rather than separately. O-Net reports that in 2015, PTAs in the United States earned a median wage of $55,170 annually or $26.52 hourly, and that Aides/Techs earned a median wage of $25,120 annually or $12.08 hourly in 2015.[36][42] The American Physical Therapy Association reports vacancy rates for physical therapists as 11.2% in outpatient private practice, 10% in acute care settings, and 12.1% in skilled nursing facilities. The APTA also reports turnover rates for physical therapists as 10.7% in outpatient private practice, 11.9% in acute care settings, 27.6% in skilled nursing facilities.[43][44][45]		The body of knowledge of physical therapy is large, and therefore physical therapists may specialize in a specific clinical area. While there are many different types of physical therapy, the American Board of Physical Therapy Specialties lists nine current specialist certifications, the ninth, Oncology, pending for its first examination in 2019.[46] Most Physical Therapists practicing in a specialty will have undergone further training, such as an accredited residency program, although individuals are currently able to sit for their specialist examination after 2,000 hours of focused practice in their respective specialty population, in addition to requirements set by each respective specialty board.		Cardiovascular and pulmonary rehabilitation respiratory practitioners and physical therapists offer therapy for a wide variety of cardiopulmonary disorders or pre and post cardiac or pulmonary surgery. An example of cardiac surgery is coronary bypass surgery. Primary goals of this specialty include increasing endurance and functional independence. Manual therapy is used in this field to assist in clearing lung secretions experienced with cystic fibrosis. Pulmonary disorders, heart attacks, post coronary bypass surgery, chronic obstructive pulmonary disease, and pulmonary fibrosis, treatments can benefit[citation needed] from cardiovascular and pulmonary specialized physical therapists.[47][verification needed]		This specialty area includes electrotherapy/physical agents, electrophysiological evaluation (EMG/NCV), physical agents, and wound management.		Geriatric physical therapy covers a wide area of issues concerning people as they go through normal adult aging but is usually focused on the older adult. There are many conditions that affect many people as they grow older and include but are not limited to the following: arthritis, osteoporosis, cancer, Alzheimer's disease, hip and joint replacement, balance disorders, incontinence, etc. Geriatric physical therapists specialize in providing therapy for such conditions in older adults.		Integumentary physical therapy includes the treatment of conditions involving the skin and all its related organs. Common conditions managed include wounds and burns. Physical therapists may utilize surgical instruments, wound irrigations, dressings and topical agents to remove the damaged or contaminated tissue and promote tissue healing.[48] Other commonly used interventions include exercise, edema control, splinting, and compression garments. The work done by physical therapists in the integumentary specialty do work similar to what would be done by medical doctors or nurses in the emergency room or triage.		Neurological physical therapy is a field focused on working with individuals who have a neurological disorder or disease. These can include stroke, chronic back pain, Alzheimer's disease, Charcot-Marie-Tooth disease (CMT), ALS, brain injury, cerebral palsy, l.g.b. syndrome[clarification needed], multiple sclerosis, Parkinson's disease, facial palsy and spinal cord injury. Common impairments associated with neurologic conditions include impairments of vision, balance, ambulation, activities of daily living, movement, muscle strength and loss of functional independence.[47] The techniques involve in neurological physical therapy are wide-ranging and often require specialized training.		Neurological physiotherapy is also called neurophysiotherapy or neurological rehabilitation.		Orthopedic physical therapists diagnose, manage, and treat disorders and injuries of the musculoskeletal system including rehabilitation after orthopedic surgery. acute trauma such as sprains, strains,injuries of insidious onset such as tendinopathy, bursitis and deformities like scoliosis. This speciality of physical therapy is most often found in the out-patient clinical setting. Orthopedic therapists are trained in the treatment of post-operative orthopedic procedures, fractures, acute sports injuries, arthritis, sprains, strains, back and neck pain, spinal conditions, and amputations.		Joint and spine mobilization/manipulation, dry needling (similar to acupuncture), therapeutic exercise, neuromuscular techniques, muscle reeducation, hot/cold packs, and electrical muscle stimulation (e.g., cryotherapy, iontophoresis, electrotherapy) are modalities employed to expedite recovery in the orthopedic setting.[49][verification needed] Additionally, an emerging adjunct to diagnosis and treatment is the use of sonography for diagnosis and to guide treatments such as muscle retraining.[50][51][52] Those who have suffered injury or disease affecting the muscles, bones, ligaments, or tendons will benefit from assessment by a physical therapist specialized in orthopedics.		Pediatric physical therapy assists in early detection of health problems and uses a variety of modalities to provide physical therapy for disorders in the pediatric population. These therapists are specialized in the diagnosis, treatment, and management of infants, children, and adolescents with a variety of congenital, developmental, neuromuscular, skeletal, or acquired disorders/diseases. Treatments focus mainly on improving gross and fine motor skills, balance and coordination, strength and endurance as well as cognitive and sensory processing/integration.		Physical therapists are closely involved in the care and wellbeing of athletes including recreational, semi-professional (paid) and professional (full-time employment) participants. This area of practice encompasses athletic injury management under 5 main categories:		Physical therapists who work for professional sport teams often have a specialized sports certification issued through their national registering organisation. Most Physical therapists who practice in a sporting environment are also active in collaborative sports medicine programs too (See also: athletic trainers).[53]		At present community based Physiotherapy rehabilitation are the main areas where specially trained candidates of physiotherapists intervening disabled conditions and rehabilitating them.[54]		They act as agents of change in Community setups by educating and transferring the basic skills and knowledge and giving treatments in the management of chronic and acute diseases and disabilities and rehabilitating them and coordinating group efforts taking administrative roles in Community Based Rehabilitation.Community Physiotherapy promotes concept of community responsibility of health and healthy living.		Community physiotherapy is practiced by specially trained and specialized physiotherapists.		Women's health physical therapy mostly addresses women's issues related to the female reproductive system, child birth, and post-partum. These conditions include lymphedema, osteoporosis, pelvic pain, prenatal and post-partum periods, and urinary incontinence. It also addresses incontinence, pelvic pain, and other disorders associated with pelvic floor dysfunction.[55] Manual physical therapy has been demonstrated in multiple studies to increase rates of conception in women with infertility.[56][57][58][59]		Physiotherapy in the field of oncology and palliative care is a continuously evolving and developing specialty, both in malignant and non-malignant diseases. Rehabilitation for both groups of patients is now recognized as an essential part of the clinical pathway, as early diagnoses and new treatments are enabling patients to live longer. it is generally accepted that patients should have access to an appropriate level of rehabilitation, so that they can function at a minimum level of dependency and optimize their quality of life, regardless of their life expectancy.		Physiotherapy is scientifically proven to be one of the most effective ways to treat and prevent pain and injury.It strengthens muscles and improves function.		It not only reduces or removes pain for a short time, but also reduces the risk for future back-pain re-occurrence. Based on the particular diagnosis, varied methods are practiced by physiotherapists to treat patients. They may follow pain management program, which helps get rid of inflammation and swelling for sometimes.[citation needed]		A systematic review that included patients with brain injury, musculoskeletal conditions, cardiac conditions, or multiple pathologies found that the alliance between patient and therapist positively correlates with treatment outcome. Outcomes includes: ability to perform activities of daily living, manage pain, complete specific physical function tasks, depression, global assessment of physical health, treatment adherence, and treatment satisfaction.[60]		Studies have explored four themes that may influence patient–therapist interactions: interpersonal and communication skills, practical skills, individualized patient-centered care, and organizational and environmental factors.[61] Physical therapists need to be able to effectively communicate with their patients on a variety of levels. Patients have varying levels of health literacy so it is important for physical therapists to take that into account when discussing the patient's ailments as well as planned treatment. Research has shown that using communication tools tailored to the patient's health literacy leads to improved engagement with their practitioner and their clinical care. In addition, patients reported that shared decision-making will yield a positive relationship.[62] Practical skills such as the ability to educate patients about their conditions, and professional expertise are perceived as valuable factors in patient care. Patients value the ability of a clinician to provide clear and simple explanations about their problems. Furthermore, patients value when physical therapists possess excellent technical skills that improve the patient effectively.[61]		Environmental factors such as the location, equipment used, and parking are less important to the patient than the physical therapy clinical encounter itself.[63]		Based on the current understanding, the most important factors that contribute to the patient–therapist interactions include that the physical therapist: spends an adequate amount of time with the patient, possesses strong listening and communication skills, treats the patient with respect, provides clear explanations of the treatment, and allows the patient to be involved in the treatment decisions.[63]		A 2012 systematic review found evidence to support the use of spine manipulation by physical therapists as a safe option to improve outcomes for low back pain.[64]		According to Randomized control trial a combination of manual therapy and supervised exercise therapy by physiotherapists give functional benefits for patients with osteoarthritis of the knee and may delay or prevent the need for surgery for the knee.[65]		Another Randomized controlled study has shown that Surgical decompression treatment for lumbar spinal stenosis is no better than physiotherapy in improving symptoms and function.So physiotherapy is as effective as surgery in lumbar spinal stenosis.[66]		The study, published recently in the Journal of Orthopaedic & Sports Physical Therapy, suggests that physical therapy—particularly a combination of manual therapy of the neck and median and stretching exercises—may be preferable to surgery in Carpal tunnel syndrome.		A systemic review in 2012 about effectiveness of physiotherapy treatment in asthma patients concluded that physiotherapy treatment may/can improve Quality of Life of patients with asthma, can improve cardiopulmonary fitness and inspiratory pressure and reduce symptoms and medication use .[67]		A 2015 systematic review suggested that spine manipulation and therapeutic massage are effective interventions for neck pain; it also suggested, however, that electroacupuncture, strain-counterstrain, relaxation massage, heat therapy and ultrasound therapy are not effective and thus not recommended for the treatment of neck pain.[68]		Telehealth (or telerehabilitation) is a developing form of physical therapy to respond to the increasing demand and high cost of typical physical therapy treatment.[69] Telehealth is online communication between the clinician and patient either live or in prerecorded sessions.[70] The benefits of telehealth include improved accessibility in remote areas, more cost efficient services, improved convenience for the home-bound and physically disabled.[70] Some considerations for telehealth are limited evidence to prove effectiveness and compliance more than in-person therapy, licensure and payment policy issues, compromised privacy.[71] Studies are controversial as to the effectiveness of telehealth in patients with various conditions, such as stroke, multiple sclerosis and lower back pain.[72]		Definitions and licensing requirements in the United States vary among jurisdictions, as each state has enacted its own physical therapy practice act defining the profession within its jurisdiction, but the American Physical Therapy Association (APTA) has also drafted a model definition in order to limit this variation, and the APTA is also responsible for accrediting physical therapy education curricula throughout the United States of America.		
